<doc id="21490" url="https://en.wikipedia.org/wiki?curid=21490" title="Nylon">
Nylon

Nylon is a generic designation for a family of synthetic polymers, more specifically aliphatic or semi-aromatic polyamides. They can be melt-processed into fibers, films or shapes. The first example of nylon (nylon 66) was produced on February 28, 1935, by Wallace Carothers at DuPont's research facility at the DuPont Experimental Station. Nylon polymers have found significant commercial applications in fibers (apparel, flooring and rubber reinforcement), in shapes (molded parts for cars, electrical equipment, etc.), and in films (mostly for food packaging).
Overview.
Nylon is a thermoplastic, silky material, first used commercially in a nylon-bristled toothbrush (1938), followed more famously by women's stockings ("nylons"; 1940) after being introduced as a fabric at the 1939 New York World's Fair. Nylon is made of repeating units linked by peptide bonds and is a type of "polyamide" and is frequently referred to as such. Nylon was the first commercially successful synthetic thermoplastic polymer. Commercially, nylon polymer is made by reacting monomers which are either lactams, acid/amines or stoichiometric mixtures of diamines (-NH2) and diacids (-COOH). Mixtures of these can be polymerized together to make copolymers. Nylon polymers can be mixed with a wide variety of additives to achieve many different property variations.
Nylon was intended to be a synthetic replacement for silk and substituted for it in many different products after silk became scarce during World War II. It replaced silk in military applications such as parachutes and flak vests, and was used in many types of vehicle tires.
After initial commercialization of nylon as a fiber, applications in the form of shapes and films were also developed. The main market for nylon shapes now is in auto components, but there are many others.
Chemistry.
Nylons are condensation copolymers, formed by reacting difunctional monomers containing equal parts of amine and carboxylic acid, so that amides are formed at both ends of each monomer in a process analogous to polypeptide biopolymers. Most nylons are made from the reaction of a dicarboxylic acid with a diamine (e.g. PA66) or a lactam or amino acid with itself (e.g. PA6). In the first case, the structure is so-called ABAB similar to polyesters and polyurethanes: the "repeating unit" consists of one of each monomer, so that they alternate in the chain. Since each monomer in this copolymer has the same reactive group on both ends, the direction of the amide bond reverses between each monomer, unlike natural polyamide proteins, which have overall directionality: C terminal → N terminal. In the second case (so called AA), the repeating unit corresponds to the single monomer.
It is difficult to get the proportions exactly correct, and deviations can lead to chain termination at molecular weights less than a desirable 10,000 daltons (u). To overcome this problem, a crystalline, solid "nylon salt" can be formed at room temperature, using an exact 1:1 ratio of the acid and the base to neutralize each other. The salt is crystallized to purify it and obtain the desired precise stoichiometry. Heated to 285 °C (545 °F), the salt reacts to form nylon polymer with the production of water.
Wallace Carothers at DuPont patented nylon 66, but overlooked the possibility to use lactams. That synthetic route was developed by Paul Schlack at IG Farben, leading to nylon 6, or polycaprolactam — formed by a ring-opening polymerization. The peptide bond within the caprolactam is broken with the exposed active groups on each side being incorporated into two new bonds as the monomer becomes part of the polymer backbone.
The 428 °F (220 °C) melting point of nylon 6 is lower than the 509 °F (265 °C) melting point of nylon 66.
Nylon 510, made from pentamethylene diamine and sebacic acid, was studied by Carothers even before nylon 66 and has superior properties, but is more expensive to make. In keeping with this naming convention, "nylon 6,12" or "PA 612" is a copolymer of a 6C diamine and a 12C diacid. Similarly for PA 510 PA 611; PA 1012, etc. Other nylons include copolymerized dicarboxylic acid/diamine products that are "not" based upon the monomers listed above. For example, some fully aromatic nylons (known as "aramids") are polymerized with the addition of diacids like terephthalic acid (→ Kevlar, Twaron) or isophthalic acid (→ Nomex), more commonly associated with polyesters. There are copolymers of PA 66/6; copolymers of PA 66/6/12; and others. In general linear polymers are the most useful, but it is possible to introduce branches in nylon by the condensation of dicarboxylic acids with polyamines having three or more amino groups.
The general reaction is:
Two molecules of water are given off and the nylon is formed. Its properties are determined by the R and R' groups in the monomers. In nylon 6,6, R = 4C and R' = 6C alkanes, but one also has to include the two carboxyl carbons in the diacid to get the number it donates to the chain. In Kevlar, both R and R' are benzene rings.
Industrial synthesis is usually done by heating the acids, amines or lactams to remove water, but in the laboratory, diacid chlorides can be reacted with diamines. For example, a popular demonstration of interfacial polymerization (the "nylon rope trick") is the synthesis of nylon 66 from adipoyl chloride and hexamethylene diamine
Nomenclature.
The nomenclature used for nylon polymers was devised during the synthesis of the first simple aliphatic nylons and uses numbers to describe the number of carbons between acid and amine functions (including the carbon of the carboxylic acid). Subsequent use of cyclic and aromatic monomers required the use of letters or sets of letters. One number after "PA" for a homopolymer based on one monomer, and two numbers or sets of letters where there are two monomers. For copolymers the comonomers or pairs of comonomers are separated by slashes, as shown in the examples below.
In common usage, the prefix 'PA' or the name 'Nylon' are used interchangeably and are equivalent in meaning.
The term polyphthalamide (abbreviated to PPA) is used when 60% or more moles of the carboxylic acid portion of the repeating unit in the polymer chain is composed of a combination of terephthalic (TPA) and isophthalic (IPA) acids
Monomers.
Nylon monomers are manufactured by a variety routes, starting in most cases from crude oil but sometimes from biomass. Those in current production are described below.
Diamines.
2-methyl pentamethylene diamine is a by product of HMD production
With increasing interest in biobased materials other monomers are being investigated
1,5-pentanediamine (cadaverine) (PMD): starch (e.g. cassava) → glucose → lysine → PMD 
Polymers.
Due to the large number of diamines, diacids and aminoacids that can be synthesized, many nylon polymers have been made experimentally and characterized to varying degrees. A smaller number have been scaled up and offered commercially, and these are detailed below.
Homopolymers.
Homopolymer nylons derived from one monomer
Examples of these polymers that are or were commercially available
Homopolymer polyamides derived from pairs of diamines and diacids (or diacid derivatives). Shown in the table below are polymers which are or have been offered commercially either as homopolymers or as a part of a copolymer.
Examples of these polymers that are or were commercially available
Copolymers.
It is easy to make mixtures of the monomers or sets of monomers used to make nylons to obtain copolymers. This lowers crystallinity and can therefore lower the melting point.
Some copolymers that have been or are commercially available are listed below: 
Blends.
Most nylon polymers are miscible with each other allowing a range of blends to be made. The two polymers can react with one another by transamidation to form random copolymers. 
According to their crystallinity, polyamides can be:
According to this classification, PA66, for example, is an aliphatic semi-crystalline homopolyamide.
Flammability.
Nylon is a material that ignites easily and burns rapidly upon exposure to an open flame. The 1967 Accident of the Apollo 1 Command Module was caused by an electrical short which created a fire that quickly consumed the interior of the cabin due to the pure oxygen environment and excessive amounts of flammable materials on board, most of which were nylon and velcro.
Bulk properties.
Above their melting temperatures, "T"m, thermoplastics like nylon are amorphous solids or viscous fluids in which the chains approximate random coils. Below "T"m, amorphous regions alternate with regions which are lamellar crystals. The amorphous regions contribute elasticity and the crystalline regions contribute strength and rigidity. The planar amide (-CO-NH-) groups are very polar, so nylon forms multiple hydrogen bonds among adjacent strands. Because the nylon backbone is so regular and symmetrical, especially if all the amide bonds are in the "trans" configuration, nylons often have high crystallinity and make excellent fibers. The amount of crystallinity depends on the details of formation, as well as on the kind of nylon. Apparently it can never be quenched from a melt as a completely amorphous solid.
Nylon 66 can have multiple parallel strands aligned with their neighboring peptide bonds at coordinated separations of exactly 6 and 4 carbons for considerable lengths, so the carbonyl oxygens and amide hydrogens can line up to form interchain hydrogen bonds repeatedly, without interruption (see the figure opposite). Nylon 510 can have coordinated runs of 5 and 8 carbons. Thus parallel (but not antiparallel) strands can participate in extended, unbroken, multi-chain β-pleated sheets, a strong and tough supermolecular structure similar to that found in natural silk fibroin and the β-keratins in feathers. (Proteins have only an amino acid α-carbon separating sequential -CO-NH- groups.) Nylon 6 will form uninterrupted H-bonded sheets with mixed directionalities, but the β-sheet wrinkling is somewhat different. The three-dimensional disposition of each alkane hydrocarbon chain depends on rotations about the 109.47° tetrahedral bonds of singly bonded carbon atoms.
When extruded into fibers through pores in an industry spinneret, the individual polymer chains tend to align because of viscous flow. If subjected to cold drawing afterwards, the fibers align further, increasing their crystallinity, and the material acquires additional tensile strength. In practice, nylon fibers are most often drawn using heated rolls at high speeds.
Block nylon tends to be less crystalline, except near the surfaces due to shearing stresses during formation. Nylon is clear and colorless, or milky, but is easily dyed. Multistranded nylon cord and rope is slippery and tends to unravel. The ends can be melted and fused with a heat source such as a flame or electrode to prevent this.
Nylons are hygroscopic, and will absorb or desorb moisture as a function of the ambient humidity. Variations in moisture content have several effects on the polymer. Firstly, the dimensions will change, but more importantly moisture acts as a plasticizer, lowering the glass transition temperature ("T"g), and consequently the elastic modulus at temperatures below the "T"g 
When dry, polyamide is a good electrical insulator. However, polyamide is hygroscopic. The absorption of water will change some of the material's properties such as its electrical resistance. Nylon is less absorbent than wool or cotton.
Characteristics.
The characteristic features of nylon 66 include:
On the other hand, nylon 6 is easy to dye, more readily fades; it has a higher impact resistance, a more rapid moisture absorption, greater elasticity and elastic recovery.
Uses.
Fibers.
Bill Pittendreigh, DuPont, and other individuals and corporations worked diligently during the first few months of World War II to find a way to replace Asian silk and hemp with nylon in parachutes. It was also used to make tires, tents, ropes, ponchos, and other military supplies. It was even used in the production of a high-grade paper for U.S. currency. At the outset of the war, cotton accounted for more than 80% of all fibers used and manufactured, and wool fibers accounted for nearly all of the rest. By August 1945, manufactured fibers had taken a market share of 25%, at the expense of cotton. After the war, because of shortages of both silk and nylon, nylon parachute material was sometimes repurposed to make dresses.
Nylon 6 and 66 fibers are used in carpet manufacture.
Nylon is one kind of fiber used in tire cord.
Shapes.
Nylon resins are widely used in the automobile industry especially in the engine compartment.
Solid nylon is used in hair combs and mechanical parts such as machine screws, gears and other low- to medium-stress components previously cast in metal. Engineering-grade nylon is processed by extrusion, casting, and injection molding. Type 6,6 Nylon 101 is the most common commercial grade of nylon, and Nylon 6 is the most common commercial grade of molded nylon. For use in tools such as spudgers, nylon is available in glass-filled variants which increase structural and impact strength and rigidity, and molybdenum disulfide-filled variants which increase lubricity. Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers. Nylon can be used as the matrix material in composite materials, with reinforcing fibers like glass or carbon fiber; such a composite has a higher density than pure nylon. Such thermoplastic composites (25% to 30% glass fiber) are frequently used in car components next to the engine, such as intake manifolds, where the good heat resistance of such materials makes them feasible competitors to metals.
Nylon was used to make the stock of the Remington Nylon 66 rifle. The frame of the modern Glock pistol is made of a nylon composite.
Food packaging.
Nylon resins are used as a component of food packaging films where an oxygen barrier is needed. The high temperature resistance of nylon makes it useful for oven bags.
Filaments.
Nylon filaments are primarily used in brushes especially toothbrushes and 'strimmers'. They are also used as monofilaments in fishing line. Nylon 610 and 612 are the most used polymers for filaments.
Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers.
Other forms.
Extruded Profiles.
Nylon resins can be extruded into rods, tubes and sheets.
Powder Coating.
Nylon powders are used to powder coat metals. Nylon 11 and nylon 12 are the most widely used.
Instrument strings.
In the mid-1940s, classical guitarist Andrés Segovia mentioned the shortage of good guitar strings in the United States, particularly his favorite Pirastro catgut strings, to a number of foreign diplomats at a party, including General Lindeman of the British Embassy. A month later, the General presented Segovia with some nylon strings which he had obtained via some members of the DuPont family. Segovia found that although the strings produced a clear sound, they had a faint metallic timbre which he hoped could be eliminated.
Nylon strings were first tried on stage by Olga Coelho in New York in January, 1944.
In 1946, Segovia and string maker Albert Augustine were introduced by their mutual friend Vladimir Bobri, editor of Guitar Review. On the basis of Segovia's interest and Augustine's past experiments, they decided to pursue the development of nylon strings. DuPont, skeptical of the idea, agreed to supply the nylon if Augustine would endeavor to develop and produce the actual strings. After three years of development, Augustine demonstrated a nylon first string whose quality impressed guitarists, including Segovia, in addition to DuPont.
Wound strings, however, were more problematic. Eventually, however, after experimenting with various types of metal and smoothing and polishing techniques, Augustine was also able to produce high quality nylon wound strings.
Hydrolysis and degradation.
All nylons are susceptible to hydrolysis, especially by strong acids, a reaction essentially the reverse of the synthetic reaction shown above. The molecular weight of nylon products so attacked drops, and cracks form quickly at the affected zones. Lower members of the nylons (such as nylon 6) are affected more than higher members such as nylon 12. This means that nylon parts cannot be used in contact with sulfuric acid for example, such as the electrolyte used in lead–acid batteries.
When being molded, nylon must be dried to prevent hydrolysis in the molding machine barrel since water at high temperatures can also degrade the polymer. The reaction is of the type:
Environmental impact, incineration and recycling.
Berners-Lee reckons the average greenhouse gas footprint of nylon in manufacturing carpets at 5.43 kg CO2 equivalent per kg, when produced in Europe. This gives it almost the same carbon footprint as wool, but with greater durability and therefore a lower overall carbon footprint.
Data published by PlasticsEurope indicates for nylon 66 a greenhouse gas footprint of 6.4 kg CO2 equivalent per kg, and an energy consumption of 138 kJ/kg. When considering the environmental impact of nylon, it is important to consider the use phase. In particular when cars are lightweighted, significant savings in fuel consumption and CO2 emissions are reduced.
Various nylons break down in fire and form hazardous smoke, and toxic fumes or ash, typically containing hydrogen cyanide. Incinerating nylons to recover the high energy used to create them is usually expensive, so most nylons reach the garbage dumps, decaying very slowly. Nylon is a robust polymer and lends itself well to recycling. Much nylon resin is recycled directly in a closed loop at the injection molding machine, by grinding sprues and runners and mixing them with the virgin granules being consumed by the molding machine.
Current market and forecast.
As one of the largest engineering polymer families, the global demand of nylon resins and compounds was valued at roughly US$20.5 billion in 2013. The market is expected to reach US$30 billion by 2020 by following an average annual growth of 5.5%.
Etymology.
In 1940, John W. Eckelberry of DuPont stated that the letters "nyl" were arbitrary and the "on" was copied from the suffixes of other fibers such as cotton and rayon. A later publication by DuPont ("Context", vol. 7, no. 2, 1978) explained that the name was originally intended to be "No-Run" ("run" meaning "unravel"), but was modified to avoid making such an unjustified claim. Since the products were not really run-proof, the vowels were swapped to produce "nuron", which was changed to "nilon" "to make it sound less like a nerve tonic". For clarity in pronunciation, the "i" was changed to "y".
An alternative but apocryphal explanation for the name is that it is a combination of New York and London: NY-Lon.
External links.
For historical perspectives on nylon, see the Documents List of "The Stocking Story: You Be The Historian" at the Smithsonian website, by The Lemelson Center for the Study of Invention and Innovation, National Museum of American History, Smithsonian Institution.

</doc>
<doc id="21491" url="https://en.wikipedia.org/wiki?curid=21491" title="Nucleus">
Nucleus

Nucleus (pl: "nuclei") is a Latin word for the seed inside a fruit. It may refer to:

</doc>
<doc id="21494" url="https://en.wikipedia.org/wiki?curid=21494" title="Nerd">
Nerd

Nerd (adjective: nerdy) is a descriptive term, often used pejoratively, indicating that a person is overly intellectual, obsessive, or lacking social skills. They may spend inordinate amounts of time on unpopular, obscure, or non-mainstream activities, which are generally either highly technical or relating to topics of fiction or fantasy, to the exclusion of more mainstream activities. Additionally, many nerds are described as being shy, quirky, and unattractive, and may have difficulty participating in, or even following, sports. Though originally derogatory, "Nerd" is a stereotypical term, but as with other pejoratives, it has been reclaimed and redefined by some as a term of pride and group identity.
Etymology.
The first documented appearance of the word "nerd" is as the name of a creature in Dr. Seuss's book "If I Ran the Zoo" (1950), in which the narrator Gerald McGrew claims that he would collect "a Nerkle, a Nerd, and a Seersucker too" for his imaginary zoo. The slang meaning of the term dates to the next year, 1951, when "Newsweek" magazine reported on its popular use as a synonym for "drip" or "square" in Detroit, Michigan. By the early 1960s, usage of the term had spread throughout the United States, and even as far as Scotland. At some point, the word took on connotations of bookishness and social ineptitude.
An alternate spelling, as "nurd" or "gnurd", also began to appear in the mid-1960s or early 1970s. Author Philip K. Dick claimed to have coined the nurd spelling in 1973, but its first recorded use appeared in a 1965 student publication at Rensselaer Polytechnic Institute. Oral tradition there holds that the word is derived from "knurd" ("drunk" spelled backward), which was used to describe people who studied rather than partied. The term "gnurd " (spelled with the "g") was in use at the Massachusetts Institute of Technology by 1965. The term "nurd "was also in use at the Massachusetts Institute of Technology as early as 1971 but was used in the context for the proper name of a fictional character in a satirical "news" article.
The Online Etymology Dictionary speculates that the word is an alteration of the 1940s term "nert" (meaning "stupid or crazy person"), which is itself an alteration of "nut".
The term was popularized in the 1970s by its heavy use in the sitcom "Happy Days".
Typical stereotype.
Nerds can be described either by their hobbies and interests, or by abstract qualities such as personality, status, social skills, and physical appearance.
"Nerdy" interests.
Some interests and activities that are likely to be described as nerdy are:
American satirist "Weird Al" Yankovic's song "White and Nerdy" states many other stereotypical nerd interests, including the Segway, ten-pin bowling, A.V. Club, the Renaissance Fair, editing Wikipedia, and Dungeons and Dragons. 
An interest can also be nerdy because of its association with "nerdy" people. For example, the stereotype of a "Band nerd" comes from the opinion that many high school band students are goofy or socially inept (except with other band students), things that would brand a person as a nerd. But, it has been applied to all students that are in band or orchestra, even the ones with little involvement (see School band#Stereotypes and popular culture).
Over time, an activity or subject can become less nerdy. This may be because of availability, because of better applications for the general public, or because of a shifting image of the majority of people taking that interest. Examples of such activities include computers, video games, the internet, books, movies, and television.
Personality and physical appearance.
Stereotypical nerds are commonly seen as intelligent but socially and physically awkward. They would typically be perceived as either lacking confidence or being indifferent or oblivious to the negative perceptions held of them by others, with the result that they become frequent objects of scorn, ridicule, bullying, and social isolation. However, many nerds may eventually find a group of similar people to associate with.
Because of the nerd stereotype, many smart people are often thought of as nerdy. This belief can be harmful, as it can cause high-school students to "switch off their lights" out of fear of being branded as one of them, and cause otherwise appealing people to be nerdy simply for their intellect. It was once thought that intellectuals were nerdy because they were envied. However, Paul Graham stated in his essay, "Why Nerds are Unpopular", that intellect is neutral, meaning that you are neither loved or despised for it. He also states that it is only the correlation that makes smart teens automatically seem nerdy, and that a nerd is someone that is not socially adept enough. Additionally, he says that the reason why many smart kids are unpopular is that they "don't have time for the activities required for popularity."
Stereotypical "nerd" appearance, often lampooned in caricatures, includes very large glasses, braces, severe acne and pants worn high at the waist. In the media, many nerds are white males, portrayed as being physically unfit, either overweight or skinny due to lack of physical exercise . It has been suggested by some, such as linguist Mary Bucholtz, that being a nerd may be a state of being "hyperwhite" and rejecting African-American culture and slang that "cool" white children use. However, after the "Revenge of the Nerds" movie franchise (with multicultural nerds), and the introduction of the Steve Urkel character on the television series "Family Matters", nerds have been seen in all races and colors as well as more recently being a frequent young Asian or Indian male stereotype in North America. Portrayal of "nerd girls", in films such as "She's Out of Control", "Welcome to the Dollhouse" and "She's All That" depicts that smart but nerdy women might suffer later in life if they do not focus on improving their physical attractiveness.
In the United States, a 2010 study published in the "Journal of International and Intercultural Communication" indicated that Asian Americans are perceived as most likely to be nerds, followed by White Americans, while non-White Hispanics and Black Americans were perceived as least likely to be nerds. These stereotypes stem from concepts of Orientalism and Primitivism, as discussed in Ron Eglash's essay "Race, Sex, and Nerds": "From Black Geeks to Asian American Hipsters". Among Whites, Jews are perceived as the most nerdy and are stereotyped in similar ways to Asians.
Medical and mental disorders.
Nerdiness is often compared to one or more medical disorders.
Nerd pride.
The rise of Silicon Valley and the American computer industry at large has allowed many "nerdy" people to accumulate large fortunes. Many stereotypically "nerdy" interests, such as superhero and science fiction works, are now popular culture hits. Some measures of nerdiness are now allegedly considered desirable, as, to some, it suggests a person who is intelligent, respectful, interesting, and able to earn a large salary. Stereotypical nerd qualities are evolving, going from awkwardness and social ostracism to an allegedly more widespread acceptance and sometimes even celebration of their differences.
In the 1984 film "Revenge of the Nerds" Robert Carradine worked to embody the nerd stereotype; in doing so, he helped create a definitive image of nerds. Additionally, the storyline presaged, and may have helped inspire, the "nerd pride" that emerged in the 1990s. "American Splendor" regular Toby Radloff claims this was the movie that inspired him to become "The Genuine Nerd from Cleveland, Ohio." In the "American Splendor" film, Toby's friend, "American Splendor" author Harvey Pekar, was less receptive to the movie, believing it to be hopelessly idealistic, explaining that Toby, an adult low income file clerk, had nothing in common with the middle class kids in the film who would eventually attain college degrees, success, and cease being perceived as nerds. Many, however, seem to share Radloff's view, as "nerd pride" has become more widespread in the years since. MIT professor Gerald Sussman, for example, seeks to instill pride in nerds:
Bryan Caplan, a professor of economics at George Mason University, refers to himself as "an openly nerdy man" and has written of a "Jock/Nerd Theory of History". He believes that income redistribution is a tactic by Jocks to prevent Nerds from gaining power over them.
The popular computer-related news website Slashdot uses the tagline "News for nerds. Stuff that matters." The Charles J. Sykes quote "Be nice to nerds. Chances are you'll end up working for one" has been popularized on the Internet and incorrectly attributed to Bill Gates. In Spain, Nerd Pride Day has been observed on May 25 since 2006, the same day as Towel Day, another somewhat nerdy holiday. The date was picked because it's the anniversary of the release of "".
The Green brothers, John Green and Hank Green of the popular YouTube account vlogbrothers have commonly referred to themselves as nerds, and much of their online personas are that of nerdy appeal. In fact, the name their fans have adapted reflects the popularity of this nerdy subculture, "Nerdfighters" or "Nerdfighteria."
An episode from the animated series "Freakazoid", titled "Nerdator", includes the use of nerds to power the mind of a Predator-like enemy. Towards the middle of the show, he gave this speech. :
The Danish reality TV show "FC Zulu", known in the internationally franchised format as "FC Nerds", established a format wherein a team of nerds, after two or three months of training, competes with a professional soccer team.
Some commentators consider that the word is devalued when applied to people who adopt a sub-cultural pattern of behaviour, rather than being reserved for people with a marked ability.

</doc>
<doc id="21496" url="https://en.wikipedia.org/wiki?curid=21496" title="Nucleic acid">
Nucleic acid

Nucleic acids are biopolymers, or large biomolecules, essential for all known forms of life. Nucleic acids, which include DNA (deoxyribonucleic acid) and RNA (ribonucleic acid), are made from monomers known as nucleotides. Each nucleotide has three components: a 5-carbon sugar, a phosphate group, and a nitrogenous base. If the sugar is deoxyribose, the polymer is DNA. If the sugar is ribose, the polymer is RNA. When all three components are combined, they form a nucleic acid. Nucleotides are also known as phosphate nucleotides.
Nucleic acids are among the most important biological macromolecules (others being amino acids/proteins, sugars/carbohydrates, and lipids/fats). They are found in abundance in all living things, where they function in encoding, transmitting and expressing genetic information—in other words, information is conveyed through the nucleic acid sequence, or the order of nucleotides within a DNA or RNA molecule. Strings of nucleotides strung together in a specific sequence are the mechanism for storing and transmitting hereditary, or genetic information via protein synthesis.
Nucleic acids were discovered by Friedrich Miescher in 1869. Experimental studies of nucleic acids constitute a major part of modern biological and medical research, and form a foundation for genome and forensic science, as well as the biotechnology and pharmaceutical industries.
Occurrence and nomenclature.
The term "nucleic acid" is the overall name for DNA and RNA, members of a family of biopolymers, and is synonymous with "polynucleotide". Nucleic acids were named for their initial discovery within the nucleus, and for the presence of phosphate groups (related to phosphoric acid). Although first discovered within the nucleus of eukaryotic cells, nucleic acids are now known to be found in all life forms including within bacteria, archaea, mitochondria, chloroplasts, viruses, and viroids. (note: there is debate as to whether viruses are living or non-living). All living cells contain both DNA and RNA (except some cells such as mature red blood cells), while viruses contain either DNA or RNA, but usually not both.
The basic component of biological nucleic acids is the nucleotide, each of which contains a pentose sugar (ribose or deoxyribose), a phosphate group, and a nucleobase.
Nucleic acids are also generated within the laboratory, through the use of enzymes (DNA and RNA polymerases) and by solid-phase chemical synthesis. The chemical methods also enable the generation of altered nucleic acids that are not found in nature, for example peptide nucleic acids.
Molecular composition and size.
Nucleic acids are generally very large molecules. Indeed, DNA molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs).
In most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however—some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form.
Nucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a purine or pyrimidine nucleobase (sometimes termed "nitrogenous base" or simply "base"), a pentose sugar, and a phosphate group. The substructure consisting of a nucleobase plus sugar is termed a nucleoside. Nucleic acid types differ in the structure of the sugar in their nucleotides–DNA contains 2'-deoxyribose while RNA contains ribose (where the only difference is the presence of a hydroxyl group). Also, the nucleobases found in the two nucleic acid types are different: adenine, cytosine, and guanine are found in both RNA and DNA, while thymine occurs in DNA and uracil occurs in RNA.
The sugars and phosphates in nucleic acids are connected to each other in an alternating chain (sugar-phosphate backbone) through phosphodiester linkages. In conventional nomenclature, the carbons to which the phosphate groups attach are the 3'-end and the 5'-end carbons of the sugar. This gives nucleic acids directionality, and the ends of nucleic acid molecules are referred to as 5'-end and 3'-end. The nucleobases are joined to the sugars via an N-glycosidic linkage involving a nucleobase ring nitrogen (N-1 for pyrimidines and N-9 for purines) and the 1' carbon of the pentose sugar ring.
Non-standard nucleosides are also found in both RNA and DNA and usually arise from modification of the standard nucleosides within the DNA molecule or the primary (initial) RNA transcript. Transfer RNA (tRNA) molecules contain a particularly large number of modified nucleosides.
Topology.
Double-stranded nucleic acids are made up of complementary sequences, in which extensive Watson-Crick base pairing results in a highly repeated and quite uniform double-helical three-dimensional structure. In contrast, single-stranded RNA and DNA molecules are not constrained to a regular double helix, and can adopt highly complex three-dimensional structures that are based on short stretches of intramolecular base-paired sequences that include both Watson-Crick and noncanonical base pairs, as well as a wide range of complex tertiary interactions.
Nucleic acid molecules are usually unbranched, and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions.
Nucleic acid sequences.
One DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of great importance in biology since they carry the ultimate instructions that encode all biological molecules, molecular assemblies, subcellular and cellular structures, organs, and organisms, and directly enable cognition, memory, and behavior (See: Genetics). Enormous efforts have gone into the development of experimental methods to determine the nucleotide sequence of biological DNA and RNA molecules, and today hundreds of millions of nucleotides are sequenced daily at genome centers and smaller laboratories worldwide. In addition to maintaining the GenBank nucleic acid sequence database, the National Center for Biotechnology Information (NCBI, http://www.ncbi.nlm.nih.gov) provides analysis and retrieval resources for the data in GenBank and other biological data made available through the NCBI Web site 
Types of nucleic acids.
Deoxyribonucleic acid.
Deoxyribonucleic acid (DNA) is a nucleic acid containing the genetic instructions used in the development and functioning of all known living organisms. The DNA segments carrying this genetic information are called genes. Likewise, other DNA sequences have structural purposes, or are involved in regulating the use of this genetic information. Along with RNA and proteins, DNA is one of the three major macromolecules that are essential for all known forms of life.
DNA consists of two long polymers of simple units called nucleotides, with backbones made of sugars and phosphate groups joined by ester bonds. These two strands run in opposite directions to each other and are, therefore, anti-parallel. Attached to each sugar is one of four types of molecules called nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes information. This information is read using the genetic code, which specifies the sequence of the amino acids within proteins. The code is read by copying stretches of DNA into the related nucleic acid RNA in a process called transcription.
Within cells DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
Ribonucleic acid.
Ribonucleic acid (RNA) functions in converting genetic information from genes into the amino acid sequences of proteins. The three universal types of RNA include transfer RNA (tRNA), messenger RNA (mRNA), and ribosomal RNA (rRNA). Messenger RNA acts to carry genetic sequence information between DNA and ribosomes, directing protein synthesis. Ribosomal RNA is a major component of the ribosome, and catalyzes peptide bond formation. Transfer RNA serves as the carrier molecule for amino acids to be used in protein synthesis, and is responsible for decoding the mRNA. In addition, many other classes of RNA are now known.
Artificial nucleic acid analogs.
Artificial nucleic acid analogues have been designed and synthesized by chemists, and include peptide nucleic acid, morpholino- and locked nucleic acid, as well as glycol nucleic acid and threose nucleic acid. Each of these is distinguished from naturally occurring DNA or RNA by changes to the backbone of the molecule.

</doc>
<doc id="21497" url="https://en.wikipedia.org/wiki?curid=21497" title="Nitrate">
Nitrate

Nitrate is a polyatomic ion with the molecular formula and a molecular mass of 62.0049 g/mol. Nitrates also describe the organic functional group RONO2. These nitrate esters are a specialized class of explosives.
Structure.
The anion is the conjugate base of nitric acid, consisting of one central nitrogen atom surrounded by three identically bonded oxygen atoms in a trigonal planar arrangement. The nitrate ion carries a formal charge of −1. This results from a combination formal charge in which each of the three oxygens carries a − charge, whereas the nitrogen carries a +1 charge, all these adding up to formal charge of the polyatomic nitrate ion. This arrangement is commonly used as an example of resonance. Like the isoelectronic carbonate ion, the nitrate ion can be represented by resonance structures:
Properties.
Almost all inorganic nitrate salts are soluble in water at standard temperature and pressure. A common example of an inorganic nitrate salt is potassium nitrate (saltpeter). A rich source of inorganic nitrate in the human body comes from diets rich in leafy green foods, such as spinach and arugula.
It is now believed that dietary nitrate in the form of plant-based foods is converted in the body to nitrite. Most nitrite ingestion in the U.S. diet is from conversion of nitrate in plants, not from nitrite intake from cured meats. Nitrite and water are converted to nitric oxide which reduces hypertension. Anti-hypertensive diets, such as the DASH diet, typically elevates nitric oxide levels which is first reduced to nitrite in the saliva, as detected in saliva testing, prior to forming nitric oxide.
Occurrence.
Nitrate salts are found naturally on earth as large deposits, particularly of nitratine, a major source of sodium nitrate.
Nitrites are produced by a number of species of nitrifying bacteria, and the nitrate compounds for gunpowder (see this topic for more) were historically produced, in the absence of mineral nitrate sources, by means of various fermentation processes using urine and dung.
Nitrates are found in man-made fertilizers.
As a byproduct of lightning strikes in earth's nitrogen-oxygen rich atmosphere, nitric acid is produced when nitrogen dioxide reacts with water vapor.
Uses.
Nitrates are mainly produced for use as fertilizers in agriculture because of their high solubility and biodegradability. The main nitrate fertilizers are ammonium, sodium, potassium, and calcium salts. Several million kilograms are produced annually for this purpose.
The second major application of nitrates is as oxidizing agents, most notably in explosives where the rapid oxidation of carbon compounds liberates large volumes of gases (see Gunpowder for an example). Sodium nitrate is used to remove air bubbles from molten glass and some ceramics. Mixtures of the molten salt are used to harden some metals.
Explosives and table tennis balls are made from celluloid.
Although nitrites are the nitrogen compound chiefly used in meat curing, nitrates are used in certain specialty curing processes where a long release of nitrite from parent nitrate stores is needed. The use of nitrates in food preservation is controversial. This is due to the potential for the formation of nitrosamines when nitrates are present in high concentrations and the product is cooked at high temperatures. The effect is seen for red or processed meat, but not for white meat or fish. The production of carcinogenic nitrosamines can be potently inhibited by the use of the antioxidants Vitamin C and the alpha-tocopherol form of Vitamin E during curing. Under simulated gastric conditions, nitrosothiols rather than nitrosamines are the main nitroso species being formed. The usage of either compound is therefore regulated; for example, in the United States, the concentration of nitrates and nitrites is generally limited to 200 ppm or lower. They are considered irreplaceable in the prevention of botulinum poisoning from consumption of cured dry sausages by preventing spore germination.
Detection.
The historical standard method of testing for nitrate is the Cadmium Reduction Method, which is reliable and accurate although it is dependent on a toxic metal cadmium and thus not suitable for all applications. An alternative method for nitrate and nitrite analysis is enzymatic reduction using nitrate reductase, which has recently been proposed by the US Environmental Protection Agency as an alternate test procedure for determining nitrate. An open source photometer as been developed for this method to accurately detect nitrate in water, soils, forage, etc. According to Hackaday this device can be built for US$65 to use this method to quantify nitrate accurately.
Free nitrate ions in solution can be detected by a nitrate ion selective electrode. Such electrodes function analogously to the pH selective electrode. This response is partially described by the Nernst equation.
Toxicity.
Toxicosis.
Nitrate toxicosis can occur through enterohepatic metabolism of nitrate due to nitrite being an intermediate. Nitrites oxidize the iron atoms in hemoglobin from ferrous iron(II) to ferric iron(III), rendering it unable to carry oxygen. This process can lead to generalized lack of oxygen in organ tissue and a dangerous condition called methemoglobinemia. Although nitrite converts to ammonia, if there is more nitrite than can be converted, the animal slowly suffers from a lack of oxygen.
Human health effects.
Humans are subject to nitrate toxicity, with infants being especially vulnerable to methemoglobinemia due to nitrate metabolizing triglycerides present at higher concentrations than at other stages of development. Methemoglobinemia in infants is known as blue baby syndrome. Although nitrates in drinking water were once thought to be a contributing factor, there are now significant scientific doubts as to whether there is a causal link. Blue baby syndrome is now thought to be the product of a number of factors, which can include any factor that causes gastric upset, such as diarrhoeal infection, protein intolerance, heavy metal toxicity etc., with nitrates playing a minor role. Nitrates, if a factor in a specific case, would most often be ingested by infants in high nitrate drinking water. However, nitrate exposure may also occur if eating, for instance, vegetables containing high levels of nitrate. Lettuce may contain elevated nitrate under growth conditions such as reduced sunlight, undersupply of the essential micronutrients molybdenum (Mo) and iron (Fe), or high concentrations of nitrate due to reduced assimilation of nitrate in the plant. High levels of nitrate fertilization also contribute to elevated levels of nitrate in the harvested plant.
Some adults can be more susceptible to the effects of nitrates than others. The methemoglobin reductase enzyme may be under-produced or absent in certain people that have an inherited mutation. Such individuals cannot break down methemoglobin as rapidly as those that do have the enzyme, leading to increased circulating levels of methemoglobin (the implication being that their blood is not as oxygen-rich). Those with insufficient stomach acid (including some vegetarians and vegans) may also be at risk. It is the increased consumption of green, leafy vegetables that typically accompany these types of diets may lead to increased nitrate intake. A wide variety of medical conditions, including food allergies, asthma, hepatitis, and gallstones may be linked with low stomach acid; these individuals may also be highly sensitive to the effects of nitrate.
Methemoglobinemia can be treated with methylene blue, which reduces ferric iron(III) in affected blood cells back to ferrous iron(II).
Marine toxicity.
In freshwater or estuarine systems close to land, nitrate can reach high levels that can potentially cause the death of fish. While nitrate is much less toxic than ammonia, levels over 30 ppm of nitrate can inhibit growth, impair the immune system and cause stress in some aquatic species. However, in light of inherent problems with past protocols on acute nitrate toxicity experiments, the extent of nitrate toxicity has been the subject of recent debate.
In most cases of excess nitrate concentrations in aquatic systems, the primary source is surface runoff from agricultural or landscaped areas that have received excess nitrate fertilizer. This is called eutrophication and can lead to algae blooms. As well as leading to water anoxia and dead zones, these blooms may cause other changes to ecosystem function, favouring some groups of organisms over others. As a consequence, as nitrate forms a component of total dissolved solids, they are widely used as an indicator of water quality.
Symptoms of nitrate poisoning include increased heart rate and respiration; in advanced cases blood and tissue may turn a blue or brown color. Feed can be tested for nitrate; treatment consists of supplementing or substituting existing supplies with lower nitrate material. Safe levels of nitrate for various types of livestock are as follows:
The values above are on a dry (moisture-free) basis.
Nitrate overview.
Nitrate formation with elements of the periodic table.

</doc>
<doc id="21502" url="https://en.wikipedia.org/wiki?curid=21502" title="Nike">
Nike

Nike may refer to:

</doc>
<doc id="21503" url="https://en.wikipedia.org/wiki?curid=21503" title="Nevis">
Nevis

Nevis is a small island in the Caribbean Sea that forms part of the inner arc of the Leeward Islands chain of the West Indies. Nevis and the neighbouring island of Saint Kitts constitute one country: the Federation of Saint Kitts and Nevis. Nevis is located near the northern end of the Lesser Antilles archipelago, about 350 km east-southeast of Puerto Rico and 80 km west of Antigua. Its area is and the capital is Charlestown.
Saint Kitts and Nevis are separated by a shallow channel known as "The Narrows". Nevis is roughly conical in shape with a volcano known as Nevis Peak at its centre. The island is fringed on its western and northern coastlines by sandy beaches which are composed of a mixture of white coral sand with brown and black sand which is eroded and washed down from the volcanic rocks that make up the island. The gently-sloping coastal plain ( wide) has natural freshwater springs as well as non-potable volcanic hot springs, especially along the western coast.
The island was named "Oualie" ("Land of Beautiful Waters") by the Caribs and "Dulcina" ("Sweet Island") by the early British settlers. The name, "Nevis", is derived from the Spanish, "Nuestra Señora de las Nieves" (which means Our Lady of the Snows); the name first appears on maps in the 16th century. Nevis is also known by the sobriquet "Queen of the Caribees", which it earned in the 18th century, when its sugar plantations created much wealth for the British.
Nevis is of particular historical significance to Americans because it was the birthplace and early childhood home of Alexander Hamilton. For the British, Nevis is the place where Horatio Nelson was stationed as a young sea captain, and is where he met and married a Nevisian, Frances Nisbet, the young widow of a plantation-owner.
The majority of the approximately 12,000 citizens of Nevis are of primarily African descent. English is the official language, and the literacy rate, 98 percent, is one of the highest in the Western Hemisphere.
Etymology.
In 1498, Christopher Columbus gave the island the name "San Martin" (Saint Martin). However, the confusion of numerous poorly-charted small islands in the Leeward Island chain meant that this name ended up being accidentally transferred to another island, which is still known as Saint-Martin/Sint Maarten.
The current name "Nevis" was derived from a Spanish name "Nuestra Señora de las Nieves" by a process of abbreviation and anglicisation. The Spanish name means Our Lady of the Snows. It is not known who chose this name for the island, but it is a reference to the story of a 4th-century Catholic miracle: a snowfall on the Esquiline Hill in Rome. Presumably the white clouds that usually cover the top of Nevis Peak reminded someone of this story of a miraculous snowfall in a hot climate.
Nevis was part of the Spanish claim to the Caribbean islands, a claim pursued until the Treaty of Madrid (1670), even though there were no Spanish settlements on the island. According to Vincent Hubbard, author of "Swords, Ships & Sugar: History of Nevis", the Spanish ruling caused many of the Arawak groups who were not ethnically Caribs to "be redefined as Caribs overnight". Records indicate that the Spanish enslaved large numbers of the native inhabitants on the more accessible of the Leeward Islands and sent them to Cubagua, Venezuela to dive for pearls. Hubbard suggests that the reason the first European settlers found so few "Caribs" on Nevis is that they had already been rounded up by the Spanish and shipped off to be used as slaves.
History.
Amerindians.
Nevis was first sighted by Columbus in 1493; an island settled for more than two thousand years by Amerindian people. The indigenous people of Nevis during these periods belonged to the Leeward Island Amerindian groups popularly referred to as Arawaks and Caribs, a complex mosaic of ethnic groups with similar culture and language. Lennox Honychurch (D. Phil. in Anthropology) from Dominica, a leading scholar in the history and culture of Caribs, traces the European use of the term "Carib" to refer to the Leeward Island aborigines to Columbus, who picked it up from the Taínos on Hispaniola. It was not a name the Caribs called themselves. "Carib Indians" was the generic name used for all groups believed involved in cannibalistic war rituals, more particularly, the consumption of parts of a killed enemy's body.
The Amerindian name for Nevis was "Oualie", land of beautiful waters. The structure of the Island Carib language has been linguistically identified as Arawakan.
Colonial era.
In spite of the Spanish claim, Nevis continued to be a popular stop-over point for English and Dutch ships on their way to the North American continent. Captain Bartholomew Gilbert of Plymouth visited the island in 1603, spending two weeks to cut twenty tons of lignum vitae wood. Gilbert sailed on to Virginia to seek out survivors of the Roanoke settlement in what is now North Carolina. Captain John Smith visited Nevis also on his way to Virginia in 1607. This was the voyage which founded Jamestown, the first permanent English settlement in the New World.
On 30 August 1620, James I of England asserted sovereignty over Nevis by giving a Royal Patent for colonisation to the Earl of Carlisle. However, actual European settlement did not happen until 1628 when Anthony Hilton moved from nearby Saint Kitts following a murder plot against him. He was accompanied by 80 other settlers, soon to be boosted by a further 100 settlers from London who had originally hoped to settle Barbuda. Hilton became the first Governor of Nevis. After the Treaty of Madrid (1670) between Spain and England, Nevis became the seat of the British colony and the Admiralty Court also sat in Nevis. Between 1675 and 1730, the island was the headquarters for the slave trade for the Leeward Islands, with approximately 6,000-7,000 enslaved West Africans passing through en route to other islands each year. The Royal African Company brought all its ships through Nevis. A 1678 census shows a community of Irish people – 22% of the population – existing as either indentured servants or freemen.
Due to the profitable Triangular trade and the high quality of Nevisian sugar cane, the island soon became a dominant source of wealth for Great Britain and the slave-owning British plantocracy. When the Leeward Islands were separated from Barbados in 1671, Nevis became the seat of the Leeward Islands colony and was given the nickname "Queen of the Caribees". It remained colonial capital for the Leeward Islands until the seat was transferred to Antigua for military reasons in 1698. During this period, Nevis was the richest of the British Leeward Islands. The island outranked larger islands like Jamaica in sugar production in the late 17th century. The wealth of the planters on the island is evident in the tax records preserved at the Calendar State Papers in the British Colonial Office Public Records, where the amount of tax collected on the Leeward Islands was recorded. The sums recorded for 1676 as "head tax on slaves", a tax payable in sugar, amounted to 384,600 pounds in Nevis, as opposed to 67,000 each in Antigua and Saint Kitts, 62,500 in Montserrat, and 5,500 total in the other five islands. The profits on sugar cultivation in Nevis was enhanced by the fact that the cane juice from Nevis yielded an unusually high amount of sugar. A gallon (3.79 litres) of cane juice from Nevis yielded 24 ounces (0.71 litres) of sugar, whereas a gallon from Saint Kitts yielded 16 ounces (0.47 litres). Twenty percent of the British Empire's total sugar production in 1700 was derived from Nevisian plantations. Exports from West Indian colonies like Nevis were worth more than all the exports from all the mainland Thirteen Colonies of North America combined at the time of the American Revolution.
The enslaved families formed the large labour force required to work the sugar plantations. After the 1650s the supply of white indentured servants began to dry up due to increased wages in England and less incentive to migrate to the colonies. By the end of the 17th century, the population of Nevis consisted of a small, rich planter elite in control, a marginal population of poor whites, a great majority of African-descended slaves, and an unknown number of maroons, escaped slaves living in the mountains. In 1780, 90 percent of the 10,000 people living on Nevis were black. Some of the maroons joined with the few remaining Caribs in Nevis to form a resistance force. Memories of the Nevisian maroons' struggle under the plantation system are preserved in place names such as Maroon Hill, an early centre of resistance.
The great wealth generated by the colonies of the West Indies led to wars between Spain, Britain, and France. The formation of the United States can be said to be a partial by-product of these wars and the strategic trade aims that often ignored North America. Three privateers (William Kidd being one of them) were employed by the British Crown to help protect ships in Nevis' waters.
During the 17th century, the French, based on Saint Kitts, launched many attacks on Nevis, sometimes assisted by the Island Caribs, who in 1667 sent a large fleet of canoes along in support. In the same year a Franco-Dutch invasion fleet was repelled off Nevis by an English fleet. Letters and other records from the era indicate that the English on Nevis hated and feared the Amerindians. In 1674 and 1683 they participated in attacks on Carib villages in Dominica and St. Vincent, in spite of a lack of official approval from the Crown for the attack.
On Nevis, the English built Fort Charles and a series of smaller fortifications to aid in defending the island against Carib attacks.
Emancipation.
In 1706, Pierre Le Moyne d'Iberville, the French Canadian founder of Louisiana in North America, decided to drive the English out of Nevis and thus also stop pirate attacks on French ships; he considered Nevis the region's headquarters for piracy against French trade. During d'Iberville's invasion of Nevis, French buccaneers were used in the front line, infamous for being ruthless killers after the pillaging during the wars with Spain where they gained a reputation for torturing and murdering non-combatants. In the face of the invading force, the English militiamen of Nevis fled. Some planters burned the plantations, rather than letting the French have them, and hid in the mountains. It was the enslaved Africans who held the French at bay by taking up arms to defend their families and the island. The slave quarters had been looted and burned as well, as the main reward promised the men fighting on the French side in the attack was the right to capture as many slaves as possible and resell them in Martinique.
During the fighting, 3,400 enslaved Nevisians were captured and sent off to Martinique, but about 1,000 more, poorly armed and militarily untrained, held the French troops at bay, by "murderous fire" according to an eyewitness account by an English militiaman. He wrote that "the slaves' brave behaviour and defence there shamed what some of their masters did, and they do not shrink to tell us so." After 18 days of fighting, the French were driven off the island. Among the Nevisian men, women and children carried away on d'Iberville's ships, six ended up in Louisiana, the first persons of African descent to arrive there.
One consequence of the French attack was a collapsed sugar industry and during the ensuing hardship on Nevis, small plots of land on the plantations were made available to the enslaved families in order to control the loss of life due to starvation. With less profitability for the absentee plantation owners, the import of food supplies for the plantation workers dwindled. Between 1776 and 1783, when the food supplies failed to arrive altogether due to the rebellion in North America, 300–400 enslaved Nevisians starved to death. On 1 August 1834, slavery was abolished in the British Empire. In Nevis, 8,815 slaves were freed. The first Monday in August is celebrated as Emancipation Day and is part of the annual Nevis Culturama festival.
A four-year apprenticeship programme followed the abolishment of slavery on the plantations. In spite of the continued use of the labour force, the Nevisian slave owners were paid over £150,000 in compensation from the British Government for the loss of property, whereas the enslaved families received nothing for 200 years of labour. One of the wealthiest planter families in Nevis, the Pinneys of Montravers Plantation, claimed £36,396 (worth close to £1,800,000 today) in compensation for the slaves on the family-owned plantations around the Caribbean.
Because of the early distribution of plots and because many of the planters departed from the island when sugar cultivation became unprofitable, a relatively large percentage of Nevisians already owned or controlled land at emancipation. Others settled on crown land. This early development of a society with a majority of small, landowning farmers and entrepreneurs created a stronger middleclass in Nevis than in Saint Kitts where the sugar industry continued until 2006. Even though the 15 families in the wealthy planter elite no longer control the arable land, Saint Kitts still has a large, landless working class population.
1800 to the present day.
Nevis was united with Saint Kitts and Anguilla in 1882, and they became an associated state with full internal autonomy in 1967, though Anguilla seceded in 1971. Together, Saint Kitts and Nevis became independent on 19 September 1983. On 10 August 1998, a referendum on Nevis to separate from Saint Kitts had 2,427 votes in favour and 1,498 against, falling short of the two-thirds majority needed.
Before 1967, the local government of Saint Kitts was also the government of Nevis and Anguilla. Nevis had two seats and Anguilla one seat in the government. The economic and infrastructural development of the two smaller islands was not a priority to the colonial federal government.
When the hospital in Charlestown was destroyed in a hurricane in 1899, planting of trees in the squares of Saint Kitts and refurbishing of government buildings, also in Saint Kitts, took precedence over the rebuilding of the only hospital in Nevis. After five years without any proper medical facilities, the leaders in Nevis initiated a campaign, threatening to seek independence from Saint Kitts. The British Administrator in Saint Kitts, Charles Cox, was unmoved. He stated that Nevis did not need a hospital since there had been no significant rise in the number of deaths during the time Nevisians had been without a hospital. Therefore, no action was needed on behalf of the government, and besides, Cox continued, the Legislative Council regarded "Nevis and Anguilla as a drag on St. Kitts and would willingly see a separation". Finally, a letter of complaint to the metropolitan British Foreign Office gave result and the federal government in Saint Kitts was ordered by their superiors in London to take speedy action. The Legislative Council took another five years to consider their options. The final decision by the federal government was to not rebuild the old hospital after all, but to instead convert the old Government House in Nevis into a hospital, named Alexandra Hospital after Queen Alexandra, wife of King Edward VII. A majority of the funds assigned for the hospital could thus spent on the construction of a new official residence in Nevis.
Electricity was introduced in Nevis in 1954, when two generators were shipped in to provide electricity to the area around Charlestown. In this regard, Nevis fared better than Anguilla, where there were no paved roads, no electricity and no telephones up until 1967. However, electricity did not become available island-wide on Nevis until 1971.
An ambitious infrastructure development programme has been introduced during the last 10 years, including a transformation of the Charlestown port, construction of a new deep-water harbour, resurfacing and widening the Island Main Road, a new airport terminal and control tower, and a major airport expansion, which required the relocation of an entire village in order to make room for the runway extension.
Modernised classrooms and better equipped schools, as well as improvements in the educational system, have contributed to a leap in academic performance on the island. The pass rate among the Nevisian students sitting for the Caribbean Examination Council (CXC) exams, the Cambridge General Certificate of Education Examination (GCE) and the Caribbean Advance Proficiency Examinations is now consistently among the highest in the English-speaking Caribbean.
Economy.
The official currency is the Eastern Caribbean dollar (EC$), which is shared by eight other territories in the region.
Historical.
After d'Iberville's invasion in 1704, records show Nevis’ sugar industry in ruins and a decimated population begging the English Parliament and relatives for loans and monetary assistance to stave off island-wide starvation. The sugar industry on the island never fully recovered and during the general depression that followed the loss of the West Indian sugar monopoly, Nevis fell on hard times and the island became one of the poorest in the region. The island remained poorer than Saint Kitts until 1991, when the fiscal performance of Nevis edged ahead of the fiscal performance of Saint Kitts for the first time since the French invasion. The European Commission's Delegation in Barbados and the Eastern Caribbean estimates the annual per capita Gross Domestic Product (GDP) on Nevis to be about 10 percent higher than on St. Kitts.
Tourism.
The major source of revenue for Nevis today is tourism. During the 2003–2004 season, approximately 40,000 tourists visited Nevis. A five star hotel "(The Four Seasons Resort Nevis, West Indies)", four exclusive restored plantation inns, and several smaller hotels, are currently in operation. Larger developments along the west coast have recently been approved and are in the process of being developed.
Offshore accounting.
The introduction of new legislation has made offshore financial services a rapidly growing economic sector in Nevis. Incorporation of companies, international insurance and reinsurance, as well as several international banks, trust companies, asset management firms, have created a boost in the economy. During 2005, the Nevis Island Treasury collected $94.6 million in annual revenue, compared to $59.8 million during 2001. In 1998, 17,500 international banking companies were registered in Nevis. Registration and annual filing fees paid in 1999 by these entities amounted to over 10 percent of Nevis’ revenues. The offshore financial industry gained importance during the financial disaster of 1999 when Hurricane Lenny damaged the major resort on the island, causing the hotel to be closed down for a year and 400 of the 700 employees to be laid off.
In 2000, the Financial Action Task Force, part of the Organisation for Economic Co-operation and Development (OECD), issued a blacklist of 35 nations which were said to be non-cooperative in the campaign against tax evasion and money laundering. The list included the Federation of Saint Kitts and Nevis, as well as Liechtenstein, Monaco, Luxembourg, the British Channel Islands, Israel, and Russia. No alleged misconduct had taken place on Nevis, but the island was included in the blanket action against all offshore financial business centres, as such centres cause a considerable loss of tax revenue for the G7 countries. With new regulations in place, Saint Kitts and Nevis were removed from the list in 2002.
Politics.
The political structure for the Federation of Saint Kitts and Nevis is based on the Westminster Parliamentary system, but it is a unique structure in that Nevis has its own unicameral legislature, consisting of Her Majesty's representative (the Deputy Governor General) and members of the Nevis Island Assembly. Nevis has considerable autonomy in its legislative branch. The constitution actually empowers the Nevis Island Legislature to make laws that cannot be abrogated by the National Assembly. In addition, Nevis has a constitutionally protected right to secede from the federation, should a two-third majority of the island's population vote for independence in a local referendum. Section 113.(1) of the constitution states: "The Nevis Island Legislature may provide that the island of Nevis shall cease to be federated with the island of Saint Christopher and accordingly that this Constitution shall no longer have effect in the island of Nevis."
Nevis has its own premier and its own government, the Nevis Island Administration. It collects its own taxes and has a separate budget, with a current account surplus. According to a statement released by the Nevis Ministry of Finance in 2005, Nevis had one of the highest growth rates in gross national product and per capita income in the Caribbean at that point.
The federal prime minister, Denzil Douglas, is the leader of the majority party of the federal House of Representatives in Saint Kitts, and his cabinet conducts the affairs of state. The Federation of Saint Kitts and Nevis has a 14 or 15-member unicameral legislature or parliament (the Senate and House of Representatives sit and vote together): A Senate, with three or four members appointed by the governor general on the advice of the prime minister and the leader of the opposition; and a popularly elected House of Representatives with 11 members, eight Saint Kitts seats and three Nevis seats. The prime minister and the cabinet are responsible to the Parliament.
Elections.
Nevis elections are scheduled every five years. The Nevis elections of 2013, called on 23 January 2013, was won by the party in opposition, the Concerned Citizens Movement (CCM), led by Vance Amory. The CCM won three of the five seats in the Nevis Island Assembly, while the incumbent party, the Nevis Reformation Party (NRP), won two.
In the federal elections of 2010, the CCM won two of the three Nevis assigned Federal seats, while the NRP won one. Of the eight Saint Kitts assigned federal seats, the St Kitts-Nevis Labour Party won six and the People's Action Movement (PAM) two.
Movement for constitutional reform.
Joseph Parry, leader of the opposition, has indicated that he favours constitutional reform over secession for Nevis. His party, the NRP, has historically been the strongest and most ardent proponent for Nevis independence; the party came to power with secession as the main campaign issue. In 1975, the NRP manifesto declared that: "The Nevis Reformation Party will strive at all costs to gain secession for Nevis from St. Kitts – a privilege enjoyed by the island of Nevis prior to 1882."
A cursory proposal for constitutional reform was presented by the NRP in 1999, but the issue was not prominent in the 2006 election campaign and it appears a detailed proposal has yet to be worked out and agreed upon within the party.
In "Handbook of Federal Countries" published by Forum of Federations, the authors consider the constitution problematic because it does not "specifically outline" the federal financial arrangements or the means by which the central government and Nevis Island Administration can raise revenue: "In terms of the NIA, the constitution only states (in s. 108(1)) that 'all revenues...raised or received by the Administration...shall be paid into and form a fund styled the Nevis Island Consolidated Fund.' [...] Section 110(1) states that the proceeds of all 'takes' collected in St. Kitts and Nevis under any law are to be shared between the federal government and the Nevis Island Administration based on population. The share going to the NIA, however, is subject to deductions (s. 110(2)), such as the cost of common services and debt charges, as determined by the Governor-General (s.110(3)) on the advice of the Prime Minister who can also take advice from the Premier of Nevis (s.110(4))."
According to a 1995 report by the Commonwealth Observer Group of the Commonwealth Secretariat, "the federal government is also the local government of St Kitts and this has resulted in a perception among the political parties in Nevis that the interests of the people of Nevis are being neglected by the federal government which is more concerned with the administration of St Kitts than with the federal administration."
Secession movement.
Simeon Daniel, Nevis' first Premier and former leader of the Nevis Reformation Party (NRP) and Vance Amory, Premier and leader of the Concerned Citizens Movement (CCM), made sovereign independence for Nevis from the Federation of Saint Kitts and Nevis part of their parties' agenda. Since independence from the United Kingdom in 1983, the Nevis Island Administration and the Federal Government have been involved in several conflicts over the interpretation of the new constitution which came into effect at independence. During an interview on Voice of America in March 1998, repeated in a government issued press release headlined "PM Douglas Maintains 1983 Constitution is Flawed", Prime Minister Denzil Douglas called the constitution a "recipe for disaster and disharmony among the people of both islands".
A crisis developed in 1984 when the People's Action Movement (PAM) won a majority in the Federal elections and temporarily ceased honouring the Federal Government's financial obligations to Nevis. Consequently, cheques issued by the Nevis Administration were not honoured by the Bank, public servants in Nevis were not paid on time and the Nevis Island Administration experienced difficulties in meeting its financial obligations.
There is also substantial support in Nevis for British Overseas Territory status similarly to Anguilla, which was formerly the third of the tri-state Saint Christopher-Nevis-Anguilla colony.
Legislative motivation for secession.
In 1996, four new bills were introduced in the National Assembly in Saint Kitts, one of which made provisions to have revenue derived from activities in Nevis paid directly to the treasury in Saint Kitts instead of to the treasury in Nevis. Another bill, The Financial Services Committee Act, contained provisions that all investments in Saint Kitts and Nevis would require approval by an investment committee in Saint Kitts. This was controversial, because ever since 1983 the Nevis Island Administration had approved all investments for Nevis, on the basis that the constitution vests legislative authority for industries, trades and businesses and economic development in Nevis to the Nevis Island Administration.
All three representatives from Nevis, including the leader of the opposition in the Nevis Island Assembly, objected to the introduction of these bills into the National Assembly in Saint Kitts, arguing that the bills would affect the ability of Nevis to develop its offshore financial services sector and that the bills would be detrimental to the Nevis economy. All the representatives in opposition in the National Assembly shared the conviction that the bills, if passed into law, would be unconstitutional and undermine the constitutional and legislative authority of the Nevis Island Administration, as well as result in the destruction of the economy of Nevis.
The constitutional crisis initially developed when the newly appointed Attorney General refused to grant permission for the Nevis Island Administration to assert its legal right in the Courts. After a decision of the High Court in favour of the Nevis Island Administration, the Prime Minister gave newspaper interviews stating that he "refused to accept the decision of the High Court". Due to the deteriorating relationship between the Nevis Island Administration and the Federal Government, a Constitutional Committee was appointed in April 1996 to advise on whether or not the present constitutional arrangement between the islands should continue. The committee recommended constitutional reform and the establishment of an island administration for Saint Kitts, separate from the Federal Government.
The Federal Government in Saint Kitts fills both functions today and Saint Kitts does not have an equivalent to the Nevis Island Administration. Disagreements between the political parties in Nevis and between the Nevis Island Administration and the Federal Government have prevented the recommendations by the electoral committee from being implemented. The problematic political arrangement between the two islands therefore continues to date.
Nevis has continued developing its own legislation, such as The Nevis International Insurance Ordinance and the Nevis International Mutual Funds Ordinance of 2004, but calls for secession are often based on concerns that the legislative authority of the Nevis Island Administration might be challenged again in the future.
Fiscal motivation for secession.
The issues of political dissension between Saint Kitts and Nevis are often centred around perceptions of imbalance in the economic structure. As noted by many scholars, Nevisians have often referred to a structural imbalance in Saint Kitts' favour in how funds are distributed between the two islands and this issue has made the movement for Nevis secession a constant presence in the island's political arena, with many articles appearing in the local press expressing concerns such as those compiled by Everton Powell in "What Motivates Our Call for Independence": 
1998 referendum.
A referendum on secession from the Federation of St. Kitts and Nevis was held in 1998. Although 62% voted in favor of a secession, a two-thirds majority would have been necessary for the referendum to succeed.
Parishes.
The island of Nevis is divided into five administrative subdivisions called parishes, each of which has an elected representative in the Nevis Island Assembly. The division of this almost round island into parishes was done in a circular sector pattern, so each parish is shaped like a pie slice, reaching from the highest point of Nevis Peak down to the coastline.
The parishes have double names, for example Saint George Gingerland. The first part of the name is the name of the patron saint of the parish church, and the second part of the name is the traditional common name of the parish. Often the parishes are referred to simply by their common names. The religious part of a parish name is sometimes written or pronounced in the possessive: Saint George's Gingerland.
The five parishes of Nevis are:
Geography.
The formation of the island began in mid-Pliocene times, approximately 3.45 million years ago. Nine distinct eruptive centres from different geological ages, ranging from mid-Pliocene to Pleistocene, have contributed to the formation. No single model of the island's geological evolution can therefore be ascertained.
Nevis Peak (985 m /3,232 ft) is the dormant remnant of one of these ancient stratovolcanoes. The last activity took place about 100,000 years ago, but active fumaroles and hot springs are still found on the island, the most recent formed in 1953. The composite cone of Nevis volcano has two overlapping summit craters that are partially filled by a lava dome, created in recent, pre-Columbian time. Pyroclastic flows and mudflows were deposited on the lower slopes of the cone simultaneously. Nevis Peak is located on the outer crater rim. Four other lava domes were constructed on the flanks of the volcano, one on the northeast flank (Madden's Mount), one on the eastern flank (Butlers Mountain), one on the northwest coast (Mount Lily) and one on the south coast (Saddle Hill, with a height of 375 metres).
During the last ice age, when the sea level was 60 m lower, the three islands of Saint Kitts, Nevis and Sint Eustatius (also known as Statia) were connected as one island. Saba, however, is separated from these three by a deeper channel.
There are visible wave-breaking reefs along the northern and eastern shorelines. To the south and west, the reefs are located in deeper water and are suitable for scuba diving. The most developed beach on Nevis is the 6.5 km long Pinney's Beach, on the western or Caribbean coast. There are sheltered swimming beaches in Oualie Bay and Cades Bay. The eastern coast of the island faces into the Atlantic Ocean, and can have strong surf in parts of the shore which are unprotected by fringing coral reefs. The colour of the sand on the beaches of Nevis is variable: on a lot of the bigger beaches the sand is a yellow grey in colour, but some beaches on the southern coast have darker, reddish, or even black sand. Under a microscope it becomes clear that Nevis sand is a mixture of tiny fragments of coral, many foraminifera, and small crystals of the various mineral constituents of the volcanic rock of which the island is made.
Colonial deforestation.
During the 17th and 18th centuries, massive deforestation was undertaken by the planters as the land was initially cleared for sugar cultivation. This intense land exploitation by the sugar and cotton industry lasted almost 300 years, and greatly changed the island's ecosystem.
In some places along the windswept southeast or "Windward" coast of the island, the landscape is radically altered compared with how it used to be in pre-colonial times. Due to extreme land erosion, the top soil was swept away, and in some places at the coast, sheer cliffs as high as 25 metres (82 ft) have developed.
Thick forest once covered the eastern coastal plain, where the Amerindians built their first settlements during the Aceramic period, complimenting the ecosystem surrounding the coral reef just offshore. It was the easy access to fresh water on the island and the rich food source represented by the ocean life sheltered by the reef that made it feasible for the Amerindians to settle this area around 600 BC. With the loss of the natural vegetation, the balance in runoff nutrients to the reef was disturbed, eventually causing as much as 80 percent of the large eastern fringing reef to become inactive. As the reef broke apart, it in turn provided less protection for the coastline.
During times of maximum cultivation, sugar cane fields stretched from the coastline of Nevis up to an altitude at which the mountain slopes were too steep and rocky to farm. Nonetheless, once the sugar industry was finally abandoned, vegetation on the leeward side of the island regrew reasonably well, as scrub and secondary forest.
Water resources.
Nevis has several natural freshwater springs (including Nelson's Spring). The island also has numerous non-potable volcanic hot springs, including most notably the Bath Spring near Bath village, just south of the capital Charlestown.
After heavy rains, powerful rivers of rainwater pour down the numerous ravines (known as ghauts). When the water reaches the coastline, the corresponding coastal ponds, both freshwater and brackish, fill to capacity and beyond, spilling over into the sea.
With modern development, the existing freshwater springs are no longer enough to supply water to the whole island. The water supply now comes mostly from Government wells. The major source of potable water for the island is groundwater, obtained from 14 active wells. Water is pumped from the wells, stored and allowed to flow by gravity to the various locations.
Climate.
The climate is tropical with little variation, tempered all year round (but particularly from December through February) by the steady north-easterly winds, called the trade winds. There is a slightly hotter and somewhat rainier season from May to November.
Nevis lies within the track area of tropical storms and occasional hurricanes. These storms can develop between August and October. This time of year has the heaviest rainfalls.
Culture.
"Culturama", the annual cultural festival of Nevis, is celebrated during the Emancipation Day weekend, the first week of August. The festivities include many traditional folk dances, such as the masquerade, the Moko jumbies on stilts, Cowboys and Indians, and Plait the Ribbon, a May pole dance. The celebration was given a more organised form in 1974, including a Miss Culture Show and a Calypso Competition, as well as drama performances, old fashion Troupes (including Johnny Walkers, Giant and Spear, Bulls, Red Cross and Blue Ribbon), arts and crafts exhibitions and recipe competitions. According to the Nevis Department of Culture, the aim is to protect and encourage indigenous folklore, in order to make sure that the uniquely Caribbean culture can "reassert itself and flourish".
Music, theatre and dance.
Nevisian culture has since the 17th century incorporated African, European and East Indian cultural elements, creating a distinct Afro-Caribbean culture. Several historical anthropologists have done field research Nevis and in Nevisian migrant communities in order to trace the creation and constitution of a Nevisian cultural community. Karen Fog Olwig published her research about Nevis in 1993, writing that the areas where the Afro-Caribbean traditions were especially strong and flourishing relate to kinship and subsistence farming. However, she adds, Afro-Caribbean cultural impulses were not recognised or valued in the colonial society and were therefore often expressed through Euro-Caribbean cultural forms. Examples of European forms appropriated to express Afro-Caribbean culture are the Nevisian and Kittitian "Tea Meetings" and "Christmas Sports". According to anthropologist Roger D. Abrahams, these traditional performance art forms are "Nevisian approximation of British performance codes, techniques, and patterns". He writes that the Tea Meetings were staged as theatrical "battles between decorum and chaos", decorum represented by the ceremony chairmen and chaos the hecklers in the audience, with a diplomatic King or a Queen presiding over the battle to ensure fairness.
The Christmas Sports included a form of comedy and satire based on local events and gossip. They were historically an important part of the Christmas celebrations in Nevis, performed on Christmas Eve by small troupes consisting of five or six men accompanied by string bands from different parts of the island. One of the men in the troupe was dressed as a woman, playing all the female parts in the dramatisations. The troupes moved from yard to yard to perform their skits, using props, face paint and costumes to play the roles of well-known personalities in the community. Examples of gossip about undesired behaviour that could surface in the skits for comic effect were querulous neighbours, adulterous affairs, planters mistreating workers, domestic disputes or abuse, crooked politicians and any form of stealing or cheating experienced in the society. Even though no names were mentioned in these skits, the audience would usually be able to guess who the heckling message in the troupe's dramatised portrayals was aimed at, as it was played out right on the person's own front yard. The acts thus functioned as social and moral commentaries on current events and behaviours in Nevisian society. This particular form is called "Bazzarding" by many locals. Abrahams theorises that Christmas Sports are rooted in the pre-emancipation Christmas and New Year holiday celebrations, when the enslaved population had several days off.
American folklorist and musicologist Alan Lomax visited Nevis in 1962 in order to conduct long-term research into the black folk culture of the island. His field trip to Nevis and surrounding islands resulted in the anthology "Lomax Caribbean Voyage" series. 
Among the Nevisians recorded were chantey-singing fishermen in a session organised in a rum shop in Newcastle; Santoy, the Calypsonian, performing calypsos by Nevisian ballader and local legend Charles Walters to guitar and cuatro; and string bands, fife players and drummers from Gingerland, performing quadrilles.
The island is also known for "Jamband music", which is the kind of music performed by local bands during the "Culturama Festival" and is key to "Jouvert" dancing. The sounds of the so-called "Iron Band" are also popular within the culture; many locals come together using any old pans, sinks, or other kits of any sort; which they use to create sounds and music. This form of music is played throughout the villages during the Christmas and carnival seasons.
Architecture.
A series of earthquakes during the 18th century severely damaged most of the colonial-era stone buildings of Charlestown. The Georgian stone buildings in Charlestown that are visible today had to be partially rebuilt after the earthquakes, and this led to the development of a new architectural style, consisting of a wooden upper floor over a stone ground floor; the new style resisted earthquake damage much more effectively.
Two famous Nevisian buildings from the 18th century are Hermitage Plantation, built of lignum vitae wood in 1740, the oldest surviving wooden house still in use in the Caribbean today, and the Bath Hotel, the first hotel in the Caribbean, a luxury hotel and spa built by John Huggins in 1778. The soothing waters of the hotel's hot spring and the lively social life on Nevis attracted many famous Europeans including Antigua-based Admiral Nelson, and Prince William Henry, Duke of Clarence, (future William IV of the United Kingdom), who attended balls and private parties at the Bath Hotel. Today, the building serves as government offices, and there are two outdoor hot-spring bathing spots which were specially constructed in recent years for public use.
An often repeated legend appears to suggest that a massive 1690 earthquake and tsunami destroyed the buildings of the original capital Jamestown on the west coast. Folk tales say that the town sank beneath the ocean. However, archaeologists from the University of Southampton who have done excavations in the area, have found no evidence to indicate that the story is true. They state that this story may originate with an over-excited Victorian letter writer sharing somewhat exaggerated accounts of his exotic life in the tropical colony with a British audience back home. One such letter recounts that so much damage was done to the town that it was completely evacuated, and was engulfed by the sea. Early maps do not, however, actually show a settlement called "Jamestown", only "Morton's Bay", and later maps show that all that was left of Jamestown/Morton's Bay in 1818 was a building labelled "Pleasure House". Very old bricks that wash up on Pinney's Beach after storms may have contributed to this legend of a sunken town; however these bricks are thought to be dumped ballast from 17th and 18th century sailing ships.
References.
Notes
Further reading

</doc>
<doc id="21504" url="https://en.wikipedia.org/wiki?curid=21504" title="Nicole Kidman">
Nicole Kidman

Nicole Mary Kidman, AC (born 20 June 1967) is an Australian actress and film producer. Kidman's breakthrough roles were in the 1989 feature film thriller "Dead Calm" and television thriller miniseries "Bangkok Hilton". Appearing in several films in the early 1990s, she came to worldwide recognition for her performances in the stock-car racing film "Days of Thunder" (1990), the romance-drama "Far and Away" (1992), and the superhero film "Batman Forever" (1995). Other successful films followed in the late 1990s. Her performance in the musical "Moulin Rouge!" (2001) earned her a second Golden Globe Award for Best Actress – Motion Picture Musical or Comedy and her first nomination for the Academy Award for Best Actress. Kidman's performance as Virginia Woolf in the drama film "The Hours" (2002) received critical acclaim and earned her the Academy Award for Best Actress, the BAFTA Award for Best Actress in a Leading Role, the Golden Globe Award for Best Actress – Motion Picture Drama and the Silver Bear for Best Actress at the Berlin International Film Festival.
Kidman's other notable films include the crime comedy-drama "To Die For" (1995), for which she won her first Golden Globe Award for Best Actress – Motion Picture Musical or Comedy, the erotic thriller "Eyes Wide Shut" (1999), the horror-thriller "The Others" (2001), the epic war drama film "Cold Mountain" (2003), the drama "Dogville" (2003), the political thriller "The Interpreter" (2005), and the epic historical romantic drama "Australia" (2008). Her performances in the drama "Birth" (2004) and the thriller "The Paperboy" (2012) earned her Golden Globe nominations for Best Actress and Supporting Actress respectively. Her performance in the 2010 drama "Rabbit Hole", which she also produced, earned Kidman further accolades, including a third nomination for the Academy Award for Best Actress. In 2012, she earned her first Primetime Emmy Award nomination for Outstanding Lead Actress in a Miniseries or a Movie for her role in the biopic "Hemingway & Gellhorn".
Kidman has been a Goodwill Ambassador for UNICEF since 1994. and for UNIFEM since 2006. In 2006, Kidman was made an Order of Australia, and was the highest-paid actress in the motion picture industry in that year. As a result of being born to Australian parents in Hawaiʻi, Kidman has dual citizenship in Australia and the United States. Kidman founded and owns the production company Blossom Films.
Early life.
Kidman was born in Honolulu, Hawaiʻi, while her Australian parents were temporarily in the United States on educational visas. Her father was Antony David Kidman (1938–2014), a biochemist, clinical psychologist and author, who died of a heart attack in Singapore aged 75. Her mother, Janelle Ann (née Glenny), is a nursing instructor who edited her husband's books and was a member of the Women's Electoral Lobby. Kidman's ancestry includes Irish, Scottish, and English heritage.
Being born in Hawaiʻi, she was given the Hawaiian name Hōkūlani. The inspiration for the name came from a baby elephant born around the same time at the Honolulu Zoo. but it is a commonly used Hawaiian name for girls, meaning 'Heavenly Star'. 
At the time of Kidman's birth, her father was a graduate student at the University of Hawaiʻi at Mānoa. He became a visiting fellow at the National Institute of Mental Health of the United States. Opposed to the war in Vietnam, Kidman's parents participated in anti-war protests while living in Washington, D.C. The family returned to Australia when Kidman was four and her mother now lives on Sydney's North Shore. Kidman has a younger sister, Antonia Kidman, a journalist and TV presenter.
Kidman attended Lane Cove Public School and North Sydney Girls' High School. She was enrolled in ballet at three and showed her natural talent for acting in her primary and high school years. She says that she was first inspired to become an actress upon seeing Margaret Hamilton's performance as the Wicked Witch of the West in "The Wizard of Oz". Kidman has revealed that she was timid as a child, saying, "I am very shy – really shy – I even had a stutter as a kid, which I slowly got over, but I still regress into that shyness. So I don't like walking into a crowded restaurant by myself; I don't like going to a party by myself."
She studied at the Victorian College of the Arts in Melbourne, and at the Phillip Street Theatre in Sydney, with actress and friend Naomi Watts who had attended the same high school.
Career.
1983–1994: Career beginnings.
In 1983, aged 16, Kidman made film debut in a remake of the Australian holiday season favourite "Bush Christmas". By the end of 1983, she had a supporting role in the television series "Five Mile Creek". In 1984, her mother was diagnosed with breast cancer, which caused Kidman to halt her acting to work temporarily while she studied massage so she could help her mother with physical therapy. She began gaining popularity in the mid-1980s after appearing in several film roles, including "BMX Bandits", "Watch the Shadows Dance", and the romantic comedy "Windrider" (1986), which earned Kidman attention due to her racy scenes. Also during the decade, she appeared in several Australian productions, including the soap opera "A Country Practice" and the miniseries "Vietnam" (1986). She also made guest appearances on Australian television programs and TV movies. She also appeared in Sesame Street.
In 1988, Kidman appeared in "Emerald City", based on the play of the same name. The Australian film earned her an Australian Film Institute for Best Supporting Actress. Kidman next starred with Sam Neill in "Dead Calm" (1989) as Rae Ingram, playing the wife of a naval officer. The thriller brought Kidman to international recognition; "Variety" commented: "Throughout the film, Kidman is excellent. She gives the character of Rae real tenacity and energy." Meanwhile, critic Roger Ebert noted the excellent chemistry between the leads, stating, "Kidman and Zane do generate real, palpable hatred in their scenes together." She followed that up with the Australian miniseries "Bangkok Hilton". She next moved on to star alongside her then-boyfriend and future husband, Tom Cruise, in the 1990 auto racing film "Days of Thunder", as a young doctor who falls in love with a NASCAR driver. It is Kidman's American debut and was among the highest-grossing films of the year.
In 1991, she co-starred with former classmate and friend Naomi Watts and Thandie Newton in the Australian independent film "Flirting". Kidman and Watts portrayed two high school girls in this coming of age story, which won the Australian Film Institute Award for Best Film. That same year, her work in the film "Billy Bathgate" earned Kidman her first Golden Globe Award nomination, for Best Supporting Actress. "The New York Times", in its film review, called her "a beauty with, it seems, a sense of humor". The following year, she and Cruise re-teamed for Ron Howard's Irish epic "Far and Away" (1992), which was a modest critical and commercial success. In 1993, she starred in the thriller "Malice" opposite Alec Baldwin and the drama "My Life" opposite Michael Keaton.
1995–2003: Worldwide recognition.
In 1995, Kidman appeared in her highest-grossing live-action film (as of ), playing Dr. Chase Meridian, the damsel in distress, in the superhero film "Batman Forever", opposite Val Kilmer as the film's title character. The same year Kidman appeared in Gus Van Sant's critically acclaimed "To Die For", earning praise, including winning her first Golden Globe for her portrayal of murderous newscaster Suzanne Stone Maretto. Kidman next appeared in "The Portrait of a Lady" (1996), based on the novel the same name, alongside Barbara Hershey, John Malkovich and Mary-Louise Parker. The following year, she starred in the action-thriller "The Peacemaker" (1997) as White House nuclear expert Dr. Julia Kelly, opposite George Clooney. The film grossed $110,000,000 worldwide. In 1998, she co-starred with Sandra Bullock in the poorly received fantasy "Practical Magic" as a modern-day witch. Kidman returned to her work on stage the same year in the David Hare play "The Blue Room", which opened in London.
In 1999, Kidman reunited with then husband, Tom Cruise, to portray a married couple in "Eyes Wide Shut", the final film of director Stanley Kubrick. The film was subject to censorship controversies due to the explicit nature of its sex scenes. The film received further attention following Kubrick's death shortly before its release. After brief hiatus and a highly publicized divorce from Cruise, Kidman returned to the screen to play a mail-order bride in the British-American drama "Birthday Girl". In 2001, Kidman played the cabaret actress and courtesan Satine in Baz Luhrmann's musical "Moulin Rouge!", opposite Ewan McGregor. Subsequently, Kidman received her second Golden Globe Award, for Best Actress in a Motion Picture Musical or Comedy, as well as other acting awards. She also received her first Academy Award nomination, for Best Actress. Also in 2001, she had a starring role in Alejandro Amenábar's Spanish horror film "The Others" as Grace Stewart. Grossing over $210,947,037 worldwide, the film also earned several Goya Awards award nominations, including a Best Actress nomination for Kidman. She received her second BAFTA and fifth Golden Globe nominations. Kidman was named the World's Most Beautiful Person by People magazine
In 2002, Kidman won critical praise for her portrayal of Virginia Woolf in Stephen Daldry's "The Hours", which stars Meryl Streep and Julianne Moore. Kidman wore prosthetics that were applied to her nose making her almost unrecognisable playing the author during her time in 1920s England, and her bouts with depression and mental illness while trying to write her novel, "Mrs. Dalloway". The film earned positive notices and several nominations, including for an Academy Award for Best Picture. "The New York Times" wrote that, "Kidman tunnels like a ferret into the soul of a woman besieged by excruciating bouts of mental illness. As you watch her wrestle with the demon of depression, it is as if its torment has never been shown on the screen before. Directing her desperate, furious stare into the void, her eyes not really focusing, Ms. Kidman, in a performance of astounding bravery, evokes the savage inner war waged by a brilliant mind against a system of faulty wiring that transmits a searing, crazy static into her brain". Kidman won numerous critics' awards, including her first BAFTA, third Golden Globe, and the Academy Award for Best Actress. As the first Australian actress to win an Academy Award, Kidman made a teary acceptance speech about the importance of art, even during times of war, saying, "Why do you come to the Academy Awards when the world is in such turmoil? Because art is important. And because you believe in what you do and you want to honour that, and it is a tradition that needs to be upheld."
Following her Oscar win, Kidman appeared in three very different films in 2003. The first, a leading role in "Dogville", by Danish director Lars von Trier, was an experimental film set on a bare soundstage. The second was an adaptation of Philip Roth's novel "The Human Stain", opposite Anthony Hopkins. Her third film was Anthony Minghella's war drama "Cold Mountain". Kidman appeared opposite Jude Law and Renée Zellweger, playing Southerner Ada Monroe, who is in love with Law's character and separated by the Civil War. "TIME" magazine wrote, "Kidman takes strength from Ada's plight and grows steadily, literally luminous. Her sculptural pallor gives way to warm radiance in the firelight". The film garnered several award nominations and wins for its actors; Kidman received her sixth Golden Globe nomination at the 61st Golden Globe Awards for Best Actress.
2004–2009: Continued success.
In 2004 she appeared in the film, "Birth", which received controversy over a scene in which Kidman shares a bath with her co-star, 10-year-old Cameron Bright. At a press conference at the Venice Film Festival, Kidman addressed the controversy saying, "It wasn't that I wanted to make a film where I kiss a 10-year-old boy. I wanted to make a film where you understand love". Kidman earned her seventh Golden Globe nomination, for Best Actress – Motion Picture Drama. That same year she appeared in the black comedy-science-fiction film "The Stepford Wives", a remake of the 1975 film of the same name. Kidman appeared in the lead role as Joanna Eberhart, a successful producer. The film, directed by Frank Oz, was critically panned and a commercial failure. The following year, Kidman appeared opposite Sean Penn in the Sydney Pollack thriller "The Interpreter", playing UN translator Silvia Broome. Also that year, she starred in "Bewitched", based on the 1960s TV sitcom of the same name, opposite Will Ferrell. Both Kidman and Ferrell earned that year's Razzie Award for "Worst Screen Couple". Neither film fared well in the United States, with box office sales falling well short of the production costs, but both films performed well internationally.
In conjunction with her success in the film industry, Kidman became the face of the "Chanel No. 5" perfume brand. She starred in a campaign of television and print ads with Rodrigo Santoro, directed by "Moulin Rouge!" director Baz Luhrmann, to promote the fragrance during the holiday seasons of 2004, 2005, 2006, and 2008. The three-minute commercial produced for "Chanel No. 5" made Kidman the record holder for the most money paid per minute to an actor after she reportedly earned US$12million for the three-minute advert. During this time, Kidman was also listed as the 45th Most Powerful Celebrity on the 2005 "Forbes" Celebrity 100 List. She made a reported US$14.5 million in 2004–2005. On "People" magazine's list of 2005's highest paid actresses, Kidman was second behind Julia Roberts, with US$16–17 million per-film price tag. Nintendo in 2007 announced that Kidman would be the new face of Nintendo's advertising campaign for the Nintendo DS game More Brain Training in its European market.
Kidman portrayed photographer Diane Arbus in the biography "Fur" (2006), opposite Robert Downey, Jr.. Both Kidman and Downey Jr. received praise for their performances. She also lent her voice to the animated film "Happy Feet" (2006), which grossed over US$384 million worldwide. In 2007, she starred in the science-fiction movie "The Invasion" directed by Oliver Hirschbiegel, a remake of the 1956 "Invasion of the Body Snatchers" that proved a critical and commercial failure. She also played opposite Jennifer Jason Leigh and Jack Black in Noah Baumbach's comedy-drama "Margot at the Wedding", which earned Kidman a Satellite Award nomination for Best Actress – Musical or Comedy. She then starred in the fantasy-adventure, "The Golden Compass" (2007), playing the villainous Marisa Coulter.
In 2008, she reunited with "Moulin Rouge!" director Baz Luhrmann in the Australian period film "Australia", set in the remote Northern Territory during the Japanese attack on Darwin during World War II. Kidman played opposite Hugh Jackman as an Englishwoman feeling overwhelmed by the continent. The acting was praised and the movie was a box office success worldwide. Kidman was originally set to star in the post-World War II German drama, "The Reader", working with previous collaborators Sydney Pollack and Anthony Minghella, but due to her pregnancy prior to filming she had to back out. The role went to Kate Winslet, who ultimately won the Oscar for Best Actress, which Kidman presented to her during the 81st Academy Awards. Kidman appeared in the 2009 Rob Marshall musical "Nine", portraying the Federico Fellini-like character's muse, Claudia Jenssen. She was featured alongside fellow Oscar winners Daniel Day-Lewis, Judi Dench, Marion Cotillard, Penélope Cruz and Sophia Loren. Kidman, whose screen time was brief compared to the other actresses, performed the musical number "Unusual Way" alongside Day-Lewis. The film received several Golden Globe and Academy Award nominations, and earned Kidman a fourth Screen Actors Guild Award nomination, as part of the Outstanding Cast.
2010–14: Independent films and biopics.
In 2010, she starred with Aaron Eckhart in the film adaptation of the Pulitzer Prize-winning play "Rabbit Hole", for which she vacated her role in the Woody Allen picture "You Will Meet a Tall Dark Stranger". Her work on "Rabbit Hole" earned her critical acclaim, and received nominations for the Academy Awards, Golden Globe Awards and Screen Actors Guild Awards, Kidman also produced this film. She lent her voice to a promotional video that Australia used to support its bid to host the 2018 FIFA World Cup. "TV Guide" reported in 2008 that Kidman will star in "The Danish Girl", a film adaptation of the novel of the same name, playing Lili Elbe, the world's first postoperative transsexual. "Screen Daily" reported that shooting would begin in Germany in July 2011. However the project has been delayed following the exit of the director, Lasse Hallström and Kidman's co-star Rachel Weisz. In 2009, "Variety" said that she would produce and star in a film adaptation of the Chris Cleave novel "Little Bee", in association with BBC Films.
In June 2010, "TV Guide" announced that Kidman and Clive Owen will star in an HBO film about Ernest Hemingway and his relationship with Martha Gellhorn. entitled "Hemingway & Gellhorn". The film, directed by Philip Kaufman, began shooting in March 2011, with an air date scheduled for 2012. She also starred alongside Nicolas Cage in director Joel Schumacher's action-thriller "Trespass", with the stars playing a married couple taken hostage.
On 17 September 2010, ContactMusic. com said Kidman would return to Broadway to portray Alexandra Del Lago in David Cromer's revival of Tennessee Williams' "Sweet Bird of Youth", with Scott Rudin producing. On 30 August 2011, Cromer spoke to "The New York Times" and explained that the production would not meet its original fall 2011 revival date but that it remains an active project.
In June 2011, Kidman was cast in Lee Daniels' adaptation of the Pete Dexter novel, "The Paperboy"; she began filming on the thriller on 1 August 2011, and "The Paperboy" was released in 2012. In the film, she portrayed death row groupie Charlotte Bless, and performed sex scenes that she claims not to have remembered until seeing the finished film. "I was like okay, so that's what I did," she said. The film competed in the 2012 Cannes Film Festival, and Kidman's performance drew critical acclaim and among nominations for the SAG and the Saturn Award for Best Supporting Actress, gave Kidman her second Golden Globe nomination for Best Supporting Actress and her tenth nomination overall. In 2012, Kidman's audiobook recording of Virginia Woolf's "To the Lighthouse" was released at Audible.com. Kidman also co-starred in Park Chan-wook's "Stoker" (2013) to positive critical response and a Saturn Award nomination for Best Supporting Actress. In April 2013 she was selected as a member of the main competition jury at the 2013 Cannes Film Festival.
In 2014, Kidman starred in the biopic, "Grace of Monaco" in the title role that chronicles the 1962 crisis, in which Charles de Gaulle blockaded the tiny principality, angered by Monaco's status as a tax haven for wealthy French subjects and Kelly's contemplating a Hollywood return to star in Alfred Hitchcock's "Marnie". Opening out of competition at the 2014 Cannes Film Festival, the film received largely negative reviews. Kidman also starred in two films with Colin Firth, the first, the British-Australian historical drama, "The Railway Man" in which Kidman played officer's wife Patti Lomax received positive critical reviews. Katherine Monk of the Montreal Gazette said of Kidman's performance, "It's a truly masterful piece of acting that transcends Teplitzky's store-bought framing, but it's Kidman who delivers the biggest surprise: For the first time since her eyebrows turned into solid marble arches, the Australian Oscar winner is truly terrific". The second, the British thriller film "Before I Go To Sleep" drew positive critical response of Kidman's performance, as Christine Lucas, a car crash survivor with brain damage. Kidman also appeared in the family film "Paddington" (2014) as a villain.
2015 to present.
Upcoming films include the Australian-Irish drama-thriller, "Strangerland", which opened at the 2015 Sundance Film Festival to a 'rapturous' audience response to Kidman's performance. Kidman also co-starred in the Jason Bateman-directed "The Family Fang", produced by Kidman's production company, Blossom Films which premiered at the 2015 Toronto International Film Festival. Other projects include the biographical drama, "Queen of the Desert", with Kidman portraying the lead role of traveller, writer, archaeologist, explorer, cartographer and political officer Gertrude Bell and "Genius" alongside Colin Firth and Guy Pearce. Kidman played a lead role in the 2015 thriller "Secret in Their Eyes", directed by Billy Ray and costarring Julia Roberts and Chiwetel Ejiofor.
After more than 15 years, Kidman returned in the London's West End in UK premiere of "Photograph 51" at the Noel Coward Theatre in 2015. She starred as British scientist Rosalind Franklin in the production from 5 September to 21 November 2015, directed by Michael Grandage. The play focuses on Franklin's role in the discovery of the structure of DNA. Kidman and the play earned 'glowing reviews.' Her return to the West End has been hailed a success, especially after having won an acting award for her role in Photograph 51.
Singing.
Her collaboration with Ewan McGregor on "Come What May" peaked at No. 27 in the UK Singles Chart. Later she collaborated with Robbie Williams on "Somethin' Stupid", a cover version for Williams' swing covers album "Swing When You're Winning"; the song peaked at No. 8 in the Australian ARIAnet Singles Chart and at No. 1, for three weeks, in the UK.
In 2006, while voicing a role in the animated movie "Happy Feet", she provided vocals for Norma Jean's "heartsong", a slightly altered version of "Kiss" by Prince. Kidman sang in Rob Marshall's movie musical "Nine" (2009).
Personal life.
Relationships and children.
Kidman has been married twice: previously to actor Tom Cruise, and currently to country singer Keith Urban. She has an adopted son and daughter with Cruise as well as two biological daughters with Urban. Kidman met Cruise in November 1989, while filming "Days of Thunder", they were married on Christmas Eve 1990 in Telluride, Colorado. The couple adopted a daughter, Isabella Jane (born 1992), and a son, Connor Anthony (born 1995). On 5 February 2001, the couple's spokesperson announced their separation. Cruise filed for divorce two days later, and the marriage was dissolved in August of that year, with Cruise citing irreconcilable differences. In her 2007 interview with "Marie Claire", Kidman noted the incorrect reporting of the ectopic pregnancy early in her marriage. "It was wrongly reported as miscarriage, by everyone who picked up the story." "So it's huge news, and it didn't happen." In the June 2006 issue of "Ladies' Home Journal", she said she still loved Cruise: "He was huge; still is. To me, he was just Tom, but to everybody else, he is huge. But he was lovely to me and I loved him. I still love him." In addition, she has expressed shock about their divorce. In 2015, former Church of Scientology executive Mark Rathbun claimed in a documentary film that he was instructed to "facilitate [Cruise's] breakup with Nicole Kidman". Cruise's auditor further claimed Kidman had been wiretapped on Cruise's suggestion.
Prior to marrying Cruise, Kidman lived with Australian stage actor Marcus Graham in the late 1980s. In the mid-1980s, she dated her "Windrider" co-star Tom Burlinson, whom she lived with on and off for three years, according to biographer Andrew Morton.
Kidman met her second husband, New Zealand-Australian country singer Keith Urban, at , an event honouring Australians, in January 2005. They married on 25 June 2006, at Cardinal Cerretti Memorial Chapel in the grounds of St Patrick's Estate, Manly in Sydney. In an interview in 2015, Kidman said, "We didn't really know each other – we got to know each other during our marriage." They maintain homes in Sydney, Sutton Forest (New South Wales, Australia), Los Angeles, and Nashville (Tennessee, USA). The couple's first daughter, Sunday Rose Kidman Urban, was born in 2008, in Nashville. Kidman's father said the daughter's middle name was after Urban's late grandmother, Rose. In 2010, Kidman and Urban had their second daughter, Faith Margaret Kidman Urban, via surrogacy at Nashville's Centennial Women's Hospital. Faith's middle name is after Kidman's late grandmother.
In an on-stage interview by Tina Brown at the 2015 Women in the World (WITW) conference, Kidman stated that she turned her attention to her career after her divorce from Tom Cruise: "Out of my divorce came work that was applauded so that was an interesting thing for me", and led to her Academy Award in 2002.
Religious and political views.
Kidman is a Roman Catholic. She attended Mary Mackillop Chapel in North Sydney. Following criticism of "The Golden Compass" by Catholic leaders as anti-Catholic, Kidman told "Entertainment Weekly" that the Catholic Church is part of her "essence," and that her religious beliefs would prevent her from taking a role in a film she perceived was anti-Catholic. During her divorce from Tom Cruise, she stated that she did not want their children raised as Scientologists. She has been reluctant to discuss Scientology since her divorce. Kidman has donated to U.S. Democratic party candidates.
In 2014, Kidman said she had been practicing Transcendental Meditation since her early twenties.
Wealth, philanthropy and honours.
In 2002, Kidman first appeared on the Australian rich list published annually in the "Business Review Weekly" with an estimated net worth of A$122 million. In the 2011 published list, Kidman's wealth was estimated at A$304 million, down from A$329 million in 2010. Kidman has raised money for, and drawn attention to, disadvantaged children around the world. In 1994, she was appointed a goodwill ambassador for UNICEF, and in 2004, she was honoured as a "Citizen of the World" by the United Nations. Kidman joined the Little Tee Campaign for breast cancer care to design T-shirts or vests to raise money to fight the disease; motivated by her mother's own battle with breast cancer in 1984.
In the 2006 Australia Day Honours, Kidman was made an Order of Australia (AC) for "service to the performing arts as an acclaimed motion picture performer, to health care through contributions to improve medical treatment for women and children and advocacy for cancer research, to youth as a principal supporter of young performing artists, and to humanitarian causes in Australia and internationally." However, due to film commitments and her wedding to Urban, it wasn't until 13 April 2007 that she was presented with the honour. It was presented by the Governor-General of Australia, Major General Michael Jeffery, in a ceremony at Government House, Canberra. Kidman was appointed goodwill ambassador of the United Nations Development Fund for Women (UNIFEM) in 2006. In this capacity, Kidman has addressed international audiences at UN events, raised awareness through the media and testified before the United States House of Representatives Committee on Foreign Affairs to support the International Violence against Women Act. Kidman visited Kosovo in 2006 to learn about women's experiences of conflict and UNIFEM's support efforts. She is the international spokesperson for UNIFEM's Say NO – UNiTE to End Violence against Women initiative. Kidman and the UNIFEM executive director presented over five million signatures collected during the first phase of this to the UN Secretary-General on 25 November 2008.
In the beginning of 2009, Kidman appeared in a series of postage stamps featuring Australian actors. She, Geoffrey Rush, Russell Crowe and Cate Blanchett each appear twice in the series: once as themselves and once as their Academy Award-winning character. On 8 January 2010, alongside Nancy Pelosi, Joan Chen and Joe Torre, Kidman attended the ceremony to help Family Violence Prevention Fund break ground on a new international center located in the Presidio of San Francisco.
In 2015, Kidman became the brand ambassador for Etihad Airways.
Kidman supports the Sydney Swans in the Australian Football League, and once served as a club ambassador.
Nicole Kidman filmography.
, Kidman's movies have grossed more than US$3 billion, with 17 movies making more than US$100 million.
Discography.
Her discography consists of one spoken word album, one extended play, three singles, three music videos, ten other appearances, a number of unreleased tracks and two tribute songs recorded by various artists.
Kidman, primarily known in the field of acting, entered the music industry in the 2000s after recording a number of tracks for the soundtrack album to Baz Luhrmann's 2001 motion picture "Moulin Rouge!" that she starred. Her duet with Ewan McGregor entitled "Come What May" was released as her debut and the second single of the OST through Interscope on 24 September 2001. The composition became the eighth highest selling single by an Australian artist for that year, being certified Gold by Australian Recording Industry Association, while reaching on the UK Singles Chart at number twenty-seven. In addition to, the song received a nomination at the 59th Golden Globe Awards as the Best Original Song and has been listed as the eighty-fifth within AFI's 100 Years...100 Songs by American Film Institute.
"Somethin' Stupid", a cover version of Frank and Nancy Sinatra followed soon. The track recorded as her common duet with English singer-songwriter Robbie Williams was issued on 14 December 2001 by Chrysalis as the lead of his fourth studio album "Swing When You're Winning". Kidman's second single topped the official music charts in Italy, New Zealand, Portugal and England, as well as scored top ten placings all over Europe, including Australia, Austria, Belgium, Denmark, Germany, Netherlands, Norway and Switzerland. Apart from being certified either Gold or Silver in a number of countries, it was classified as the eleventh best selling single of 2002 in Italy, thirtieth in the UK, the fifty-ninth in Australia, and the ninety-third in France, respectively.
On 5 April 2002, Kidman released through Interscope her third single, a cover of Randy Crawford's "One Day I'll Fly Away". A Tony Philips remix of the track was promoted as the pilot single of a follow-up to the original soundtrack of the same name, "Moulin Rouge! Vol. 2". After that, in 2006, she contributed with her vocal for the OST "Happy Feet" on a rendition of Prince' "Kiss". While in 2009, she was featured on the "Nine" soundtrack ("Unusual Way").
Most recently, her name has been credited on a track called "What's the Procedure", issued on 14 March 2013 on the compilation "I Know Why They Call It Pop: Volume 2" by Rok Lok Records. Among others, Kidman also narrated an audiobook in 2012.
Awards.
In 2003, Kidman received a Star on the Hollywood Walk of Fame. In addition to her 2003 Academy Award for Best Actress, Kidman has received Best Actress awards from the following critics' groups or award-granting organisations: the Hollywood Foreign Press (Golden Globes), the Australian Film Institute, Blockbuster Entertainment Awards, Empire Awards, Golden Satellite Awards, Hollywood Film Festival, London Critics Circle, Russian Guild of Film Critics, and the Southeastern Film Critics Association. In 2003, Kidman was given the American Cinematheque Award. She also received recognition from the National Association of Theatre Owners at the ShoWest Convention in 1992 as the Female Star of Tomorrow and in 2002 for a Distinguished Decade of Achievement in Film.

</doc>
<doc id="21505" url="https://en.wikipedia.org/wiki?curid=21505" title="Nucleotide">
Nucleotide

Nucleotides are organic molecules that serve as the monomers, or subunits, of nucleic acids like DNA (deoxyribonucleic acid) and RNA (ribonucleic acid). The building blocks of nucleic acids, nucleotides are composed of a nitrogenous base, a five-carbon sugar (ribose or deoxyribose), and at least one phosphate group. Thus a nucleoside plus a phosphate group yields a nucleotide.
Nucleotides also function to carry packets of energy within the cell in the form of the nucleoside triphosphates (ATP, GTP, CTP and UTP), playing a central role in metabolism. In addition, nucleotides participate in cell signaling (cGMP and cAMP), and are incorporated into important cofactors of enzymatic reactions (e.g. coenzyme A, FAD, FMN, NAD, and NADP+).
In experimental biochemistry, nucleotides can be radiolabeled with radionuclides to yield radionucleotides.
Structure.
A nucleotide is made of a nucleobase (also termed a nitrogenous base), a five-carbon sugar (either ribose or 2-deoxyribose, depending on if it is RNA or DNA), and one or, depending on the definition, more than one phosphate groups. Authoritative chemistry sources such as the ACS Style Guide and IUPAC Gold Book clearly state that the term nucleotide refers only to a molecule containing one phosphate. However, common usage in molecular biology textbooks often extends this definition to include molecules with two or three phosphate groups. Thus, the term "nucleotide" generally refers to a nucleoside monophosphate, but a nucleoside diphosphate or nucleoside triphosphate could be considered a nucleotide as well.
Without the phosphate group, the nucleobase and sugar compose a nucleoside. The phosphate groups form bonds with either the 2, 3, or 5-carbon of the sugar, with the 5-carbon site most common. Cyclic nucleotides form when the phosphate group is bound to two of the sugar's hydroxyl groups. Nucleotides contain either a purine or a pyrimidine base. Ribonucleotides are nucleotides in which the sugar is ribose. Deoxyribonucleotides are nucleotides in which the sugar is deoxyribose.
Nucleic acids are polymeric macromolecules made from nucleotide monomers. In DNA, the purine bases are adenine and guanine, while the pyrimidines are thymine and cytosine. RNA uses uracil in place of thymine. Adenine always pairs with thymine by 2 hydrogen bonds, while guanine pairs with cytosine through 3 hydrogen bonds, in each case because of the unique structures of the bases.
Synthesis.
Nucleotides can be synthesized by a variety of means both in vitro and in vivo.
In vivo, nucleotides can be synthesized de novo or recycled through salvage pathways. The components used in de novo nucleotide synthesis are derived from biosynthetic precursors of carbohydrate and amino acid metabolism, and from ammonia and carbon dioxide. The liver is the major organ of de novo synthesis of all four nucleotides. De novo synthesis of pyrimidines and purines follows two different pathways. Pyrimidines are synthesized first from aspartate and carbamoyl-phosphate in the cytoplasm to the common precursor ring structure orotic acid, onto which a phosphorylated ribosyl unit is covalently linked. Purines, however, are first synthesized from the sugar template onto which the ring synthesis occurs. For reference, the syntheses of the purine and pyrimidine nucleotides are carried out by several enzymes in the cytoplasm of the cell, not within a specific organelle. Nucleotides undergo breakdown such that useful parts can be reused in synthesis reactions to create new nucleotides.
In vitro, protecting groups may be used during laboratory production of nucleotides. A purified nucleoside is protected to create a phosphoramidite, which can then be used to obtain analogues not found in nature and/or to synthesize an oligonucleotide.
Pyrimidine ribonucleotide synthesis.
The synthesis of the pyrimidines CTP and UTP occurs in the cytoplasm and starts with the formation of carbamoyl phosphate from glutamine and CO2. Next, aspartate carbamoyltransferase catalyzes a condensation reaction between aspartate and carbamoyl phosphate to form carbamoyl aspartic acid, which is cyclized into 4,5-dihydroorotic acid by dihydroorotase. The latter is converted to orotate by dihydroorotate oxidase. The net reaction is:
Orotate is covalently linked with a phosphorylated ribosyl unit. The covalent linkage between the ribose and pyrimidine occurs at position C1 of the ribose unit, which contains a pyrophosphate, and N1 of the pyrimidine ring. Orotate phosphoribosyltransferase (PRPP transferase) catalyzes the net reaction yielding orotidine monophosphate (OMP):
Orotidine 5'-monophosphate is decarboxylated by orotidine-5'-phosphate decarboxylase to form uridine monophosphate (UMP). PRPP transferase catalyzes both the ribosylation and decarboxylation reactions, forming UMP from orotic acid in the presence of PRPP. It is from UMP that other pyrimidine nucleotides are derived. UMP is phosphorylated by two kinases to uridine triphosphate (UTP) via two sequential reactions with ATP. First the diphosphate form UDP is produced, which in turn is phosphorylated to UTP. Both steps are fueled by ATP hydrolysis:
CTP is subsequently formed by amination of UTP by the catalytic activity of CTP synthetase. Glutamine is the NH3 donor and the reaction is fueled by ATP hydrolysis, too:
Cytidine monophosphate (CMP) is derived from cytidine triphosphate (CTP) with subsequent loss of two phosphates.
Purine ribonucleotide synthesis.
The atoms which are used to build the purine nucleotides come from a variety of sources: 
The de novo synthesis of purine nucleotides by which these precursors are incorporated into the purine ring proceeds by a 10-step pathway to the branch-point intermediate IMP, the nucleotide of the base hypoxanthine. AMP and GMP are subsequently synthesized from this intermediate via separate, two-step pathways. Thus, purine moieties are initially formed as part of the ribonucleotides rather than as free bases.
Six enzymes take part in IMP synthesis. Three of them are multifunctional:
The pathway starts with the formation of PRPP. PRPS1 is the enzyme that activates R5P, which is formed primarily by the pentose phosphate pathway, to PRPP by reacting it with ATP. The reaction is unusual in that a pyrophosphoryl group is directly transferred from ATP to C1 of R5P and that the product has the α configuration about C1. This reaction is also shared with the pathways for the synthesis of Trp, His, and the pyrimidine nucleotides. Being on a major metabolic crossroad and requiring much energy, this reaction is highly regulated.
In the first reaction unique to purine nucleotide biosynthesis, PPAT catalyzes the displacement of PRPP's pyrophosphate group (PPi) by an amide nitrogen donated from either glutamine (N), glycine (N&C), aspartate (N), folic acid (C1), or CO2. This is the committed step in purine synthesis. The reaction occurs with the inversion of configuration about ribose C1, thereby forming β-5-phosphorybosylamine (5-PRA) and establishing the anomeric form of the future nucleotide.
Next, a glycine is incorporated fueled by ATP hydrolysis and the carboxyl group forms an amine bond to the NH2 previously introduced. A one-carbon unit from folic acid coenzyme N10-formyl-THF is then added to the amino group of the substituted glycine followed by the closure of the imidazole ring. Next, a second NH2 group is transferred from a glutamine to the first carbon of the glycine unit. A carboxylation of the second carbon of the glycin unit is concomittantly added. This new carbon is modified by the additional of a third NH2 unit, this time transferred from an aspartate residue. Finally, a second one-carbon unit from formyl-THF is added to the nitrogen group and the ring covalently closed to form the common purine precursor inosine monophosphate (IMP).
Inosine monophosphate is converted to adenosine monophosphate in two steps. First, GTP hydrolysis fuels the addition of aspartate to IMP by adenylosuccinate synthase, substituting the carbonyl oxygen for a nitrogen and forming the intermediate adenylosuccinate. Fumarate is then cleaved off forming adenosine monophosphate. This step is catalyzed by adenylosuccinate lyase.
Inosine monophosphate is converted to guanosine monophosphate by the oxidation of IMP forming xanthylate, followed by the insertion of an amino group at C2. NAD+ is the electron acceptor in the oxidation reaction. The amide group transfer from glutamine is fueled by ATP hydrolysis.
Pyrimidine and purine degradation.
In humans, pyrimidine rings (C, T, U) can be degraded completely to CO2 and NH3 (urea excretion). That having been said, purine rings (G, A) cannot. Instead they are degraded to the metabolically inert uric acid which is then excreted from the body. Uric acid is formed when GMP is split into the base guanine and ribose. Guanine is deaminated to xanthine which in turn is oxidized to uric acid. This last reaction is irreversible. Similarly, uric acid can be formed when AMP is deaminated to IMP from which the ribose unit is removed to form hypoxanthine. Hypoxanthine is oxidized to xanthine and finally to uric acid. Instead of uric acid secretion, guanine and IMP can be used for recycling purposes and nucleic acid synthesis in the presence of PRPP and aspartate (NH3 donor).
Unnatural base pair (UBP).
An unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. In 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or "Unnatural Base Pair" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium "E. coli" that successfully replicated the unnatural base pairs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into "E. coli" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate the plasmid containing d5SICS–dNaM.
The successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses.
Length unit.
Nucleotide (abbreviated "nt") is a common unit of length for single-stranded nucleic acids, similar to how base pair is a unit of length for double-stranded nucleic acids.
Abbreviation codes for degenerate bases.
The IUPAC has designated the symbols for nucleotides. Apart from the five (A, G, C, T/U) bases, often degenerate bases are used especially for designing PCR primers. These nucleotide codes are listed here. Some primer sequences may also include the character "I", which codes for the non-standard nucleotide inosine. Inosine occurs in tRNAs, and will pair with adenine, cytosine, or thymine. This character does not appear in the following table however, because it does not represent a degeneracy. While inosine can serve a similar function as the degeneracy "D", it is an actual nucleotide, rather than a representation of a mix of nucleotides that covers each possible pairing needed.

</doc>
<doc id="21506" url="https://en.wikipedia.org/wiki?curid=21506" title="Numerical analysis">
Numerical analysis

Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).
One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of formula_1, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.
Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of formula_1, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.
Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.
Before the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.
General introduction.
The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:
The rest of this section outlines several important themes of numerical analysis.
History.
The field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.
To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.
The mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.
Direct and iterative methods.
Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).
In contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.
Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.
Discretization.
Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called "discretization". For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.
Generation and propagation of errors.
The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.
Round-off.
Round-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).
Truncation and discretization error.
Truncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of formula_3, after 10 or so iterations, we conclude that the root is roughly 1.99 (for example). We therefore have a truncation error of 0.01.
Once an error is generated, it will generally propagate through the calculation. For instance, we have already noted that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type is even more inexact.
What does it mean when we say that the truncation error is created when we approximate a mathematical procedure? We know that to integrate a function exactly requires one to find the sum of infinite trapezoids. But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically we can only choose a finite value of the differential element.
Numerical stability and well-posed problems.
Numerical stability is an important notion in numerical analysis. An algorithm is called "numerically stable" if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is "well-conditioned", meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is "ill-conditioned", then any small error in the data will grow to be a large error.
Both the original problem and the algorithm used to solve that problem can be "well-conditioned" and/or "ill-conditioned", and any combination is possible.
So an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation "x"1 to formula_1, for instance "x"1=1.4, and then computing improved guesses "x"2, "x"3, etc.. One such method is the famous Babylonian method, which is given by "x""k"+1 = "xk"/2 + 1/"xk". Another iteration, which we will call Method X, is given by "x""k" + 1 = ("x""k"2−2)2 + "x""k". We have calculated a few iterations of each scheme in table form below, with initial guesses "x"1 = 1.4 and "x"1 = 1.42.
Observe that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess 1.4 and diverges for initial guess 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.
Areas of study.
The field of numerical analysis includes many sub-disciplines. Some of the major ones are:
Computing values of functions.
One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.
Interpolation, extrapolation, and regression.
Interpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?
Extrapolation is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.
Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The least squares-method is one popular way to achieve this.
Solving equations and systems of equations.
Another fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation formula_9 is linear while formula_10 is not.
Much effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.
Root-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.
Solving eigenvalue or singular value problems.
Several important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.
Optimization.
Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.
The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.
The method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.
Evaluating integrals.
Numerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.
Differential equations.
Numerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.
Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.
Software.
Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free alternative is the GNU Scientific Library.
There are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, LabVIEW, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), IT++ (a C++ library), R (similar to S-PLUS) and certain variants of Python. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.
Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary precision arithmetic which can provide more accurate results.
Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis.
External links.
Journals
Online texts
Online course material

</doc>
<doc id="21508" url="https://en.wikipedia.org/wiki?curid=21508" title="Noosphere">
Noosphere

The noosphere (; sometimes noösphere) is the sphere of human thought. The word derives from the Greek νοῦς (nous "mind") and σφαῖρα (sphaira "sphere"), in lexical analogy to "atmosphere" and "biosphere". It was introduced by Pierre Teilhard de Chardin in 1922 in his "Cosmogenesis". Another possibility is the first use of the term by Édouard Le Roy (1870–1954), who together with Teilhard was listening to lectures of Vladimir Ivanovich Vernadsky at the Sorbonne. In 1936, Vernadsky accepted the idea of the noosphere in a letter to Boris Leonidovich Lichkov (though he states that the concept derives from Le Roy). Citing the work of Teilhard's biographer - Rene Cuenot - Sampson and Pitt stated that although the concept was jointly developed by all three men (Vernadsky, LeRoy, and Teilhard), Teilhard believed that he actually invented the word: "I believe, so far as one can ever tell, that the word 'noosphere' was my invention: but it was he Roy who launched it."
History of concept.
In the theory of Vernadsky, the noosphere is the third in a succession of phases of development of the Earth, after the geosphere (inanimate matter) and the biosphere (biological life). Just as the emergence of life fundamentally transformed the geosphere, the emergence of human cognition fundamentally transforms the biosphere. In contrast to the conceptions of the Gaia theorists, or the promoters of cyberspace, Vernadsky's noosphere emerges at the point where humankind, through the mastery of nuclear processes, begins to create resources through the transmutation of elements. It is also currently being researched as part of the Princeton Global Consciousness Project.
Teilhard perceived a directionality in evolution along an axis of increasing "Complexity/Consciousness". For Teilhard, the noosphere is the sphere of thought encircling the earth that has emerged through evolution as a consequence of this growth in complexity / consciousness. The noosphere is therefore as much part of nature as the barysphere, lithosphere, hydrosphere, atmosphere, and biosphere. As a result, Teilhard sees the "social phenomenon the culmination of and not the attenuation of the biological phenomenon." These social phenomena are part of the noosphere and include, for example, legal, educational, religious, research, industrial and technological systems. In this sense, the noosphere emerges through and is constituted by the interaction of human minds. The noosphere thus grows in step with the organization of the human mass in relation to itself as it populates the earth. Teilhard argued the noosphere evolves towards ever greater personalisation, individuation and unification of its elements. He saw the Christian notion of love as being the principal driver of noogenesis. Evolution would culminate in the Omega Point - an apex of thought/consciousness - which he identified with the eschatological return of Christ.
One of the original aspects of the noosphere concept deals with evolution. Henri Bergson, with his "L'évolution créatrice" (1907), was one of the first to propose evolution is "creative" and cannot necessarily be explained solely by Darwinian natural selection. "L'évolution créatrice" is upheld, according to Bergson, by a constant vital force which animates life and fundamentally connects mind and body, an idea opposing the dualism of René Descartes. In 1923, C. Lloyd Morgan took this work further, elaborating on an "emergent evolution" which could explain increasing complexity (including the evolution of mind). Morgan found many of the most interesting changes in living things have been largely discontinuous with past evolution. Therefore, these living things did not necessarily evolve through a gradual process of natural selection. Rather, he posited, the process of evolution experiences jumps in complexity (such as the emergence of a self-reflective universe, or noosphere). Finally, the complexification of human cultures, particularly language, facilitated a quickening of evolution in which cultural evolution occurs more rapidly than biological evolution. Recent understanding of human ecosystems and of human impact on the biosphere have led to a link between the notion of sustainability with the "co-evolution" and harmonization of cultural and biological evolution.

</doc>
<doc id="21511" url="https://en.wikipedia.org/wiki?curid=21511" title="Niccolò Paganini">
Niccolò Paganini

Niccolò (or Nicolò) Paganini (27 October 178227 May 1840) was an Italian violinist, violist, guitarist, and composer. He was the most celebrated violin virtuoso of his time, and left his mark as one of the pillars of modern violin technique. His 24 Caprices for Solo Violin Op.1 are among the best known of his compositions, and have served as an inspiration for many prominent composers.
Biography.
Childhood.
Niccolò Paganini was born in Genoa, then capital of the Republic of Genoa, the third of the six children of Antonio and Teresa (née Bocciardo) Paganini. Paganini's father was an unsuccessful trader, but he managed to supplement his income through playing music on the mandolin. At the age of five, Paganini started learning the mandolin from his father, and moved to the violin by the age of seven. His musical talents were quickly recognized, earning him numerous scholarships for violin lessons. The young Paganini studied under various local violinists, including Giovanni Servetto and Giacomo Costa, but his progress quickly outpaced their abilities. Paganini and his father then traveled to Parma to seek further guidance from Alessandro Rolla. But upon listening to Paganini's playing, Rolla immediately referred him to his own teacher, Ferdinando Paer and, later, Paer's own teacher, Gasparo Ghiretti. Though Paganini did not stay long with Paer or Ghiretti, the two had considerable influence on his composition style.
Early career.
The French invaded northern Italy in March 1796, and Genoa was not spared. The Paganinis sought refuge in their country property in Romairone, near Bolzaneto. It was in this period that Paganini is thought to have developed his relationship with the guitar. He became rather adept on this instrument, but preferred to play it in exclusively intimate, rather than public concerts. He later described the guitar as his "constant companion" on his concert tours. By 1800, Paganini and his father traveled to Livorno, where Paganini played in concerts and his father resumed his maritime work. In 1801, the 18-year-old Paganini was appointed first violin of the Republic of Lucca, but a substantial portion of his income came from freelancing. His fame as a violinist was matched only by his reputation as a gambler and womanizer.
In 1805, Lucca was annexed by Napoleonic France, and the region was ceded to Napoleon's sister, Elisa Baciocchi. Paganini became a violinist for the Baciocchi court, while giving private lessons to Elisa's husband, Felice. In 1807, Baciocchi became the Grand Duchess of Tuscany and her court was transferred to Florence. Paganini was part of the entourage, but, towards the end of 1809, he left Baciocchi to resume his freelance career.
Travelling virtuoso.
For the next few years, Paganini returned to touring in the areas surrounding Parma and Genoa. Though he was very popular with the local audience, he was still not very well known in the rest of Europe. His first break came from an 1813 concert at La Scala in Milan. The concert was a great success. As a result, Paganini began to attract the attention of other prominent, albeit more conservative, musicians across Europe. His early encounters with Charles Philippe Lafont and Louis Spohr created intense rivalry. His concert activities, however, were still limited to Italy for the next few years.
In 1827, Pope Leo XII honoured Paganini with the Order of the Golden Spur. His fame spread across Europe with a concert tour that started in Vienna in August 1828, stopping in every major European city in Germany, Poland, and Bohemia until February 1831 in Strasbourg. This was followed by tours in Paris and Britain. His technical ability and his willingness to display it received much critical acclaim. In addition to his own compositions, theme and variations being the most popular, Paganini also performed modified versions of works (primarily concertos) written by his early contemporaries, such as Rodolphe Kreutzer and Giovanni Battista Viotti.
His travels also brought him into contact with eminent guitar virtuosi of the day, including Ferdinando Carulli in Paris and Mauro Giuliani in Vienna. This did not inspire him to play public concerts with guitar, and even performances of his own guitar trios and quartets were private to the point of being behind closed doors.
Late career and health decline.
Throughout his life, Paganini was no stranger to chronic illnesses. Although no definite medical proof exists, he was reputed to have been affected by Marfan syndrome or Ehlers–Danlos syndrome. In addition, his frequent concert schedule, as well as his extravagant lifestyle, took their toll on his health. He was diagnosed with syphilis as early as 1822, and his remedy, which included mercury and opium, came with serious physical and psychological side effects. In 1834, while still in Paris, he was treated for tuberculosis. Though his recovery was reasonably quick, after the illness his career was marred by frequent cancellations due to various health problems, from the common cold to depression, which lasted from days to months.
In September 1834, Paganini put an end to his concert career and returned to Genoa. Contrary to popular beliefs involving his wishing to keep his music and techniques secret, Paganini devoted his time to the publication of his compositions and violin methods. He accepted students, of whom two enjoyed moderate success: violinist Camillo Sivori and cellist Gaetano Ciandelli. Neither, however, considered Paganini helpful or inspirational. In 1835, Paganini returned to Parma, this time under the employ of Archduchess Marie Louise of Austria, Napoleon's second wife. He was in charge of reorganizing her court orchestra. However, he eventually conflicted with the players and court, so his visions never saw completion. In Paris, he befriended the 11-year-old Polish virtuoso Apollinaire de Kontski, giving him some lessons and a signed testimonial. It was widely put about, falsely, that Paganini was so impressed with de Kontski's skills that he bequeathed him his violins and manuscripts.
Final years, death, and burial.
In 1836, Paganini returned to Paris to set up a casino. Its immediate failure left him in financial ruin, and he auctioned off his personal effects, including his musical instruments, to recoup his losses. At Christmas of 1838, he left Paris for Marseilles and, after a brief stay, travelled to Nice where his condition worsened. In May 1840, the Bishop of Nice sent Paganini a local parish priest to perform the last rites. Paganini assumed the sacrament was premature, and refused.
A week later, on 27 May 1840, Paganini died from internal hemorrhaging before a priest could be summoned. Because of this, and his widely rumored association with the devil, the Church denied his body a Catholic burial in Genoa. It took four years and an appeal to the Pope before the Church let his body be transported to Genoa, but it was still not buried. His remains were finally laid to rest in 1876, in a cemetery in Parma. In 1893, the Czech violinist František Ondříček persuaded Paganini's grandson, Attila, to allow a viewing of the violinist's body. After this bizarre episode, Paganini's body was finally reinterred in a new cemetery in Parma in 1896.
Personal and professional relationships.
Though having no shortage of romantic conquests, Paganini was seriously involved with a singer named Antonia Bianchi from Como, whom he met in Milan in 1813. The two gave concerts together throughout Italy. They had a son, Achilles Cyrus Alexander, born on 23 July 1825 in Palermo and baptized at San Bartolomeo's. They never legalized their union and it ended around April 1828 in Vienna. Paganini brought Achilles on his European tours, and Achilles later accompanied his father until the latter's death. He was instrumental in dealing with his father's burial, years after his death.
Throughout his career, Paganini also became close friends with composers Gioachino Rossini and Hector Berlioz. Rossini and Paganini met in Bologna in the summer of 1818. In January 1821, on his return from Naples, Paganini met Rossini again in Rome, just in time to become the substitute conductor for Rossini's opera "Matilde di Shabran", upon the sudden death of the original conductor. Paganini's efforts earned gratitude from Rossini.
Paganini met Berlioz in Paris, and was a frequent correspondent as a penfriend. He commissioned a piece from the composer, but was not satisfied with the resultant four-movement piece for orchestra and viola obbligato "Harold en Italie". He never performed it, and instead it was premiered a year later by violist Christian Urhan. He did however write his own "Sonata per Gran Viola" Op. 35 (with orchestra or guitar accompaniment) . Despite his alleged lack of interest in "Harold", Paganini often referred to Berlioz as the resurrection of Beethoven and, towards the end of his life, he gave large sums to the composer. They shared an active interest in the guitar, which they both played and used in compositions. Paganini gave Berlioz a guitar, which they both signed on its sound box.
Instruments.
Paganini was in possession of a number of fine string instruments. More legendary than these were the circumstances under which he obtained (and lost) some of them. While Paganini was still a teenager in Livorno, a wealthy businessman named Livron lent him a violin, made by the master luthier Giuseppe Guarneri, for a concert. Livron was so impressed with Paganini's playing that he refused to take it back. This particular violin came to be known as "Il Cannone Guarnerius". On a later occasion in Parma, he won another valuable violin (also by Guarneri) after a difficult sight-reading challenge from a man named Pasini.
Other instruments associated with Paganini include the "Antonio Amati" 1600, the "Nicolò Amati" 1657, the "Paganini-Desaint" 1680 Stradivari, the Guarneri-filius "Andrea" 1706, the "Le Brun" 1712 Stradivari, the "Vuillaume" c. 1720 Bergonzi, the "Hubay" 1726 Stradivari, and the "Comte Cozio di Salabue" 1727 violins; the "Countess of Flanders" 1582 da Salò-di Bertolotti, and the "Mendelssohn" 1731 Stradivari violas; the "Piatti" 1700 Goffriller, the "Stanlein" 1707 Stradivari, and the "Ladenburg" 1736 Stradivari cellos; and the "Grobert of Mirecourt" 1820 (guitar). Four of these instruments were played by the Tokyo String Quartet.
Of his guitars, there is little evidence remaining of his various choices of instrument. The aforementioned guitar that he gave to Berlioz is a French instrument made by one Grobert of Mirecourt. The luthier made his instrument in the style of René Lacote, a more well-known Paris-based guitar-maker. It is preserved and on display in the Musée de la Musique in Paris.
Of the guitars he owned through his life, there was an instrument by Gennaro Fabricatore that he had refused to sell even in his periods of financial stress, and was among the instruments in his possession at the time of his death. There is an unsubstantiated rumour that he also played Stauffer guitars; he may certainly have come across these in his meetings with Giuliani in Vienna.
Compositions.
Paganini composed his own works to play exclusively in his concerts, all of which profoundly influenced the evolution of violin technique. His 24 Caprices were likely composed in the period between 1805 to 1809, while he was in the service of the Baciocchi court. Also during this period, he composed the majority of the solo pieces, duo-sonatas, trios and quartets for the guitar, either as a solo instrument or with strings. These chamber works may have been inspired by the publication, in Lucca, of the guitar quintets of Boccherini. Many of his variations, including "Le Streghe", "The Carnival of Venice", and "Nel cor più non mi sento", were composed, or at least first performed, before his European concert tour.
Generally speaking, Paganini's compositions were technically imaginative, and the timbre of the instrument was greatly expanded as a result of these works. Sounds of different musical instruments and animals were often imitated. One such composition was titled "Il Fandango Spanolo" (The Spanish Dance), which featured a series of humorous imitations of farm animals. Even more outrageous was a solo piece "Duetto Amoroso", in which the sighs and groans of lovers were intimately depicted on the violin. There survives a manuscript of the "Duetto", which has been recorded. The existence of the "Fandango" is known only through concert posters.
However, his works were criticized for lacking characteristics of true polyphonism, as pointed out by Eugène Ysaÿe. Yehudi Menuhin, on the other hand, suggested that this might have been the result of his reliance on the guitar (in lieu of the piano) as an aid in composition. The orchestral parts for his concertos were often polite, unadventurous, and clearly supportive of the soloist. In this, his style is consistent with that of other Italian composers such as Paisiello, Rossini and Donizetti, who were influenced by the guitar-song milieu of Naples during this period.
Paganini was also the inspiration of many prominent composers. Both "La Campanella" and the A minor Caprice (No. 24) have been an object of interest for a number of composers. Franz Liszt, Robert Schumann, Johannes Brahms, Sergei Rachmaninoff, Boris Blacher, Andrew Lloyd Webber, George Rochberg and Witold Lutosławski, among others, wrote well-known variations on these themes.
Violin technique.
The Israeli violinist Ivry Gitlis once referred to Paganini as a phenomenon rather than a development. Though some of the techniques frequently employed by Paganini were already present, most accomplished violinists of the time focused on intonation and bowing techniques. Arcangelo Corelli (1653–1713) was considered a pioneer in transforming the violin from an ensemble instrument to a solo instrument. In the meantime, the polyphonic capability of the violin was firmly established through the Sonatas and Partitas BWV 1001–1006 of Johann Sebastian Bach (1685–1750). Other notable violinists included Antonio Vivaldi (1678–1741) and Giuseppe Tartini (1692–1770), who, in their compositions, reflected the increasing technical and musical demands on the violinist. Although the role of the violin in music drastically changed through this period, progress in violin technique was steady but slow. Techniques requiring agility of the fingers and the bow were still considered unorthodox and discouraged by the established community of violinists.
Much of Paganini's playing (and his violin composition) was influenced by two violinists, Pietro Locatelli (1693–1746) and August Duranowski (Auguste Frédéric Durand) (1770–1834). During Paganini's study in Parma, he came across the 24 Caprices of Locatelli (entitled "L'arte di nuova modulazione – Capricci enigmatici" or "The art of the new style – the enigmatic caprices"). Published in the 1730s, they were shunned by the musical authorities for their technical innovations, and were forgotten by the musical community at large. Around the same time, Durand, a former student of Giovanni Battista Viotti (1755–1824), became a celebrated violinist. He was renowned for his use of harmonics and the left hand pizzicato in his performance. Paganini was impressed by Durand's innovations and showmanship, which later also became the hallmarks of the young violin virtuoso. Paganini was instrumental in the revival and popularization of these violinistic techniques, which are now incorporated into regular compositions.
Another aspect of Paganini's violin techniques concerned his flexibility. He had exceptionally long fingers and was capable of playing three octaves across four strings in a hand span, an extraordinary feat even by today's standards. His seemingly unnatural ability may have been a result of Marfan syndrome.
Inspired works.
Notable works inspired by compositions of Paganini include:
The "Caprice No. 24 in A minor", Op. 1, ("Tema con variazioni") has been the basis of works by many other composers. Notable examples include Brahms's "Variations on a Theme of Paganini" and Rachmaninoff's "Rhapsody on a Theme of Paganini".
Memorials.
The Paganini Competition ("Premio Paganini") is an international violin competition created in 1954 in his home city of Genoa and named in his honour.
In 1972 the State of Italy purchased a large collection of Niccolò Paganini manuscripts from the W. Heyer Library of Cologne. They are housed at the Biblioteca Casanatense in Rome.
In 1982 the city of Genoa commissioned a thematic catalogue of music by Paganini, edited by Maria Rosa Moretti and Anna Sorrento, hence the abbreviation "MS" assigned to his catalogued works.
A minor planet 2859 Paganini discovered in 1978 by Soviet astronomer Nikolai Chernykh is named after him.
Dramatic portrayals.
Paganini has been portrayed by a number of actors in film and television productions, including Stewart Granger in the 1946 biographical portrait "The Magic Bow", Roxy Roth in "A Song to Remember" (1945), Klaus Kinski in "Kinski Paganini" (1989) and David Garrett in "The Devil's Violinist" (2013).
In the Soviet 1982 miniseries "Niccolo Paganini" the musician is portrayed by the Armenian actor Vladimir Msryan. The series focuses on Paganini's relationship with the Roman Catholic Church. Another Soviet actor, Armen Dzhigarkhanyan, plays Paganini's fictionalized arch-rival, an insidious Jesuit official. The information in the series is generally spurious and it also plays to some of the myths and legends rampant during the musician's lifetime. One memorable scene shows Paganini's adversaries sabotaging his violin before a high-profile performance, causing all strings but one to break during the concert. An undeterred Paganini continues to perform on three, two, and finally on a single string. In actuality, Paganini occasionally broke strings during a performance on purpose so he could further display his virtuosity.
In Don Nigro's satirical comedy "Paganini" (1995), the great violinist seeks vainly for his salvation, claiming that he unknowingly sold his soul to the Devil. "Variation upon variation," he cries at one point, "but which variation leads to salvation and which to damnation? Music is a question for which there is no answer." Paganini is portrayed as having killed three of his lovers and sinking repeatedly into poverty, prison, and drink. Each time he is "rescued" by the Devil who appears in different guises, returning Paganini's violin so he can continue playing. In the end, Paganini's salvation—administered by a god-like Clockmaker—turns out to be imprisonment in a large bottle where he plays his music for the amusement of the public through all eternity. "Do not pity him, my dear," the Clockmaker tells Antonia, one of Paganini's murdered wives. "He is alone with the answer for which there is no question. The saved and the damned are the same."
References.
Notes
Sources
External links.
Images

</doc>
<doc id="21512" url="https://en.wikipedia.org/wiki?curid=21512" title="North Atlantic Current">
North Atlantic Current

The North Atlantic Current (also known as North Atlantic Drift and North Atlantic Sea Movement) is a powerful warm ocean current that continues the Gulf Stream northeast. West of Continental Europe, it splits into two major branches. One branch goes southeast, later to become the Canary Current as it passes northwest Africa and turns southwest. The other major branch continues north along the coast of Northwestern Europe. It is thought to have a considerable warming influence on the climate, but there is a minority view. Other branches include the Irminger Current and the Norwegian Current. Driven by the global thermohaline circulation (THC), the North Atlantic Current is part of the wind-driven Gulf Stream, which goes further east and north from the North American coast across the Atlantic and into the Arctic Ocean.

</doc>
<doc id="21513" url="https://en.wikipedia.org/wiki?curid=21513" title="North Atlantic Deep Water">
North Atlantic Deep Water

North Atlantic Deep Water (NADW) is a deep water mass formed in the North Atlantic Ocean. Thermohaline circulation of the world's oceans involves the flow of warm surface waters from the southern hemisphere into the North Atlantic. Water flowing northward becomes modified through evaporation and mixing with other water masses, leading to increased salinity. When this water reaches the North Atlantic it cools and sinks through convection, due to its decreased temperature and increased salinity resulting in increased density. NADW is the outflow of this thick deep layer, which can be detected by its high salinity, high oxygen content, nutrient minima, and chlorofluorocarbons (CFCs). CFCs are anthropogenic substances that enter the surface of the ocean from gas exchange with the atmosphere. This distinct composition allows its path to be traced as it mixes with Circumpolar Deep Water (CDW), which in turn fills the deep Indian Ocean and part of the South Pacific. NADW and its formation is essential to the Atlantic Meridional Overturning Circulation (AMOC), which is responsible for transporting large amounts of water, heat, salt, carbon, nutrients and other substances around the globe. In the conveyor belt model of thermohaline circulation of the world's oceans, the sinking of NADW pulls the waters of the North Atlantic drift northward; however, this is almost certainly an oversimplification of the actual relationship between NADW formation and the strength of the Gulf Stream/North Atlantic drift.
NADW has a temperature of 2-4 °C with a salinity of 34.9-35.0 psu found at a depth between 1500 and 4000m.
Formation and sources.
The NADW is a complex of several water masses formed by deep convection and also by overflow of dense water across the Greenland-Iceland-Scotland Ridge.
The upper layers are formed by deep open ocean convection during winter. Labrador Sea Water (LSW), formed in the Labrador Sea can reach depths of 2000 m as dense water sinks downward. Classical Labrador Sea Water (CLSW) production is dependent on preconditioning of water in the Labrador Sea from the previous year, and the strength of the North Atlantic Oscillation. During a positive NAO phase, conditions exist for strong winter storms to develop. These storms freshen the surface water, and their winds increase cyclonic flow, which allows denser waters to sink. As a result, the temperature, salinity, and density vary yearly. In some years these conditions do not exist and CLSW is not formed. CLSW has characteristic potential temperature of 3 °C, salinity of 34.88 psu, and density of 34.66. Another component of LSW is the Upper Labrador Sea Water (ULSW). ULSW forms at a density lower than CLSW and has a CFC maximum between 1200 and 1500 m in the subtropical North Atlantic. Eddies of cold less saline ULSW have similar densities of warmer saltier water and flow along the DWBC, but maintain their high CFCs. The ULSW eddies erode rapidly as they mix laterally with this warmer saltier water.
The lower waters mass of NADW form from overflow of the Greenland-Iceland-Scotland Ridge. They are Iceland-Scotland Overflow Water (ISOW) and Denmark Strait Overflow Water (DSOW). The overflows are a combination of dense Arctic Ocean water (18%), modified Atlantic water (32%), and intermediate water from the Nordic seas (20%), that entrain and mix with other water masses (contributing 30%) as they flow over the Greenland-Iceland-Scotland Ridge. The formation of both of these waters involves the conversion of warm salty northward flowing surface waters to cold dense deep waters behind the Greenland-Iceland-Scotland Ridge. Water flow from the North Atlantic current enters the Arctic Ocean through the Norwegian Current which splits into the Fram Strait and Barents Sea Branch. Water from the Fram Strait recirculates, reaching a density of DSOW, sinks, and flows towards the Denmark Strait. Water flowing into the Barent Sea feeds ISOW.
ISOW enters the eastern North Atlantic over the Iceland-Scotland Ridge through the Faeroe Bank Channel at a depth of 850 m, with some water flowing over the shallower Iceland-Faeroe Rise. ISOW has a low CFC concentrations and it has been estimated from these concentrations that ISOW resides behind the ridge for 45 years. As the water flows southward at the bottom of the channel, it entrains surrounding water of the eastern North Atlantic, and flows to the western North Atlantic through the Charlie-Gibbs Fracture Zone, entraining with LSW. This water is less dense than (DSOW) and lays above it as it flows cyclonically in the Irminger Basin.
DSOW is the coldest, densest, and freshest water mass of NADW. DSOW formed behind the ridge flows over the Denmark Strait at a depth of 600m. The most significant water mass contributing to DSOW is Arctic Intermediate Water (AIW). Winter cooling and convection allow AIW to sink and pool behind the Denmark Strait. Upper AIW has a high amount of anthropogenic tracers due its exposure to the atmosphere. AIW's tritium and CFC signature is observed in DSOW at the base of the Greenland continental slope. This also showed that the DSOW flowing 450 km to the south was no older than 2 years. Both the DSOW and ISOW flow around the Irminger Basin and Labrador Sea in a deep boundary current. Leaving the Greenland Sea with 2.5 Sv its flow increases to 10 Sv south of Greenland. It is cold and relatively fresh, flowing below 3500 m in the DWBC and spreading inward the deep Atlantic basins.
Spreading pathways.
The southward spread of NADW along the Deep Western Boundary current (DWBC) can be traced by its high oxygen content, high CFCs, and density.
ULSW is the major source of upper NADW. ULSW advects southward from the Labrador Sea in small eddies that mix into the DWBC. A CFC maxima associated with ULSW has been observed along 24°N in the DWBC at 1500 m. Some of the upper ULSW recirculates into the Gulf Stream, while some remains in the DWBC. High CFCs in the subtropics indicate recirculation in the subtropics. ULSW that remains in the DWBC dilutes as it moves equatorward. Deep convection in the Labrador Sea during the late 1980s and early 1990s resulted in CLSW with a lower CFC concentration due to downward mixing. However, convection allowed the CFCs to penetrate further downward to 2000m. These minimum could be tracked, and were first observed in the subtropics in the early 1990s.
ISOW and DSOW flow around the Irminger Basin and DSOW entering the DWBC. These are the two lower portions of the NADW. Another CFC maximum is seen at 3500 m in the subtropics from the DSOW contribution to NADW. Some of the NADW recirculates with the northern gyre. To the south of the gyre NADW flows under the Gulf Stream where it continues along the DWBC until it reaches another gyre in the subtropics.
Lower North Atlantic Deep Water (LNADW), originating in the Greenland and Norwegian Seas, brings high salinity, oxygen, and freon concentrations towards to the Romanche Trench, an equatorial fracture zone in the Mid-Atlantic Ridge (MAR). Found at depths around , LNADW flow east through the trench over AABW, the trench being the only opening in the MAR where inter-basin exchange is possible for these two water masses.
Variability.
It is believed that North Atlantic Deep Water formation has been dramatically reduced at times during the past (such as during the Younger Dryas or during Heinrich events), and that this might correlate with a decrease in the strength of the Gulf Stream and the North Atlantic drift, in turn cooling the climate of northwestern Europe. There is concern that global warming might cause this to happen again. It is also hypothesized that during the Last Glacial Maximum (LGM), NADW was replaced with an analogous watermass that occupied a shallower depth known as Glacial North Atlantic Intermediate Water (GNAIW).

</doc>
<doc id="21514" url="https://en.wikipedia.org/wiki?curid=21514" title="Nanomedicine">
Nanomedicine

Nanomedicine is the medical application of nanotechnology. Nanomedicine ranges from the medical applications of nanomaterials and biological devices, to nanoelectronic biosensors, and even possible future applications of molecular nanotechnology such as biological machines. Current problems for nanomedicine involve understanding the issues related to toxicity and environmental impact of nanoscale materials (materials whose structure is on the scale of nanometers, i.e. billionths of a meter).
Functionalities can be added to nanomaterials by interfacing them with biological molecules or structures. The size of nanomaterials is similar to that of most biological molecules and structures; therefore, nanomaterials can be useful for both in vivo and in vitro biomedical research and applications.
Thus far, the integration of nanomaterials with biology has led to the development of diagnostic devices, contrast agents, analytical tools, physical therapy applications, and drug delivery vehicles.
Nanomedicine seeks to deliver a valuable set of research tools and clinically useful devices in the near future. The National Nanotechnology Initiative expects new commercial applications in the pharmaceutical industry that may include advanced drug delivery systems, new therapies, and in vivo imaging. Nanomedicine research is receiving funding from the US National Institutes of Health, including the funding in 2005 of a five-year plan to set up four nanomedicine centers.
Nanomedicine is a large industry, with nanomedicine sales reaching $6.8 billion in 2004, and with over 200 companies and 38 products worldwide, a minimum of $3.8 billion in nanotechnology R&D is being invested every year. In April 2006, the journal Nature Materials estimated that 130 nanotech-based drugs and delivery systems were being developed worldwide. As the nanomedicine industry continues to grow, it is expected to have a significant impact on the economy.
Drug delivery.
Nanotechnology has provided the possibility of delivering drugs to specific cells using nanoparticles.
The overall drug consumption and side-effects may be lowered significantly by depositing the active agent in the morbid region only and in no higher dose than needed. Targeted drug delivery is intended to reduce the side effects of drugs with concomitant decreases in consumption and treatment expenses. Drug delivery focuses on maximizing bioavailability both at specific places in the body and over a period of time. This can potentially be achieved by molecular targeting by nanoengineered devices. More than $65 billion are wasted each year due to poor bioavailability. A benefit of using nanoscale for medical technologies is that smaller devices are less invasive and can possibly be implanted inside the body, plus biochemical reaction times are much shorter. These devices are faster and more sensitive than typical drug delivery. The efficacy of drug delivery through nanomedicine is largely based upon: a) efficient encapsulation of the drugs, b) successful delivery of drug to the targeted region of the body, and c) successful release of the drug.
Drug delivery systems, lipid- or polymer-based nanoparticles, can be designed to improve the pharmacokinetics and biodistribution of the drug. However, the pharmacokinetics and pharmacodynamics of nanomedicine is highly variable among different patients. When designed to avoid the body's defence mechanisms, nanoparticles have beneficial properties that can be used to improve drug delivery. Complex drug delivery mechanisms are being developed, including the ability to get drugs through cell membranes and into cell cytoplasm. Triggered response is one way for drug molecules to be used more efficiently. Drugs are placed in the body and only activate on encountering a particular signal. For example, a drug with poor solubility will be replaced by a drug delivery system where both hydrophilic and hydrophobic environments exist, improving the solubility. Drug delivery systems may also be able to prevent tissue damage through regulated drug release; reduce drug clearance rates; or lower the volume of distribution and reduce the effect on non-target tissue. However, the biodistribution of these nanoparticles is still imperfect due to the complex host's reactions to nano- and microsized materials and the difficulty in targeting specific organs in the body. Nevertheless, a lot of work is still ongoing to optimize and better understand the potential and limitations of nanoparticulate systems. While advancement of research proves that targeting and distribution can be augmented by nanoparticles, the dangers of nanotoxicity become an important next step in further understanding of their medical uses.
Nanoparticles can be used in combination therapy for decreasing antibiotic resistance or for their antimicrobial properties. Nanoparticles might also used to circumvent multidrug resistance (MDR) mechanisms.
Types of systems used.
Two forms of nanomedicine that have already been tested in mice and are awaiting human trials that will be using gold nanoshells to help diagnose and treat cancer, and using liposomes as vaccine adjuvants and as vehicles for drug transport. Similarly, drug detoxification is also another application for nanomedicine which has shown promising results in rats. Advances in Lipid nanotechnology was also instrumental in engineering medical nanodevices and novel drug delivery systems as well as in developing sensing applications. Another example can be found in dendrimers and nanoporous materials. Another example is to use block co-polymers, which form micelles for drug encapsulation.
Polymeric nano-particles are a competing technology to lipidic (based mainly on Phospholipids) nano-particles. There is an additional risk of toxicity associated with polymers not widely studied or understood. The major advantages of polymers is stability, lower cost and predictable characterisation. However, in the patient's body this very stability (slow degradation) is a negative factor. Phospholipids on the other hand are membrane lipids (already present in the body and surrounding each cell), have a GRAS (Generally Recognised As Safe) status from FDA and are derived from natural sources without any complex chemistry involved. They are not metabolised but rather absorbed by the body and the degradation products are themselves nutrients (fats or micronutrients).
Protein and peptides exert multiple biological actions in the human body and they have been identified as showing great promise for treatment of various diseases and disorders. These macromolecules are called biopharmaceuticals. Targeted and/or controlled delivery of these biopharmaceuticals using nanomaterials like nanoparticles and Dendrimers is an emerging field called nanobiopharmaceutics, and these products are called nanobiopharmaceuticals.
Another highly efficient system for microRNA delivery for example are nanoparticles formed by the self-assembly of two different microRNAs deregulated in cancer.
Another vision is based on small electromechanical systems; nanoelectromechanical systems are being investigated for the active release of drugs. Some potentially important applications include cancer treatment with iron nanoparticles or gold shells.Nanotechnology is also opening up new opportunities in implantable delivery systems, which are often preferable to the use of injectable drugs, because the latter frequently display first-order kinetics (the blood concentration goes up rapidly, but drops exponentially over time). This rapid rise may cause difficulties with toxicity, and drug efficacy can diminish as the drug concentration falls below the targeted range.
Applications.
Some nanotechnology-based drugs that are commercially available or in human clinical trials include:
Cancer.
Existing and potential drug nanocarriers have been reviewed.
Nanoparticles have high surface area to volume ratio. This allows for many functional groups to be attached to a nanoparticle, which can seek out and bind to certain tumor cells. Additionally, the small size of nanoparticles (10 to 100 nanometers), allows them to preferentially accumulate at tumor sites (because tumors lack an effective lymphatic drainage system). Limitations to conventional cancer chemotherapy include drug resistance, lack of selectivity, and lack of solubility. Nanoparticles have the potential to overcome these problems.
In photodynamic therapy, a particle is placed within the body and is illuminated with light from the outside. The light gets absorbed by the particle and if the particle is metal, energy from the light will heat the particle and surrounding tissue. Light may also be used to produce high energy oxygen molecules which will chemically react with and destroy most organic molecules that are next to them (like tumors). This therapy is appealing for many reasons. It does not leave a "toxic trail" of reactive molecules throughout the body (chemotherapy) because it is directed where only the light is shined and the particles exist. Photodynamic therapy has potential for a noninvasive procedure for dealing with diseases, growth and tumors. Kanzius RF therapy is one example of such therapy (nanoparticle hyperthermia) . Also, gold nanoparticles have the potential to join numerous therapeutic functions into a single platform, by targeting specific tumor cells, tissues and organs.
Visualization.
"In vivo" imaging is another area where tools and devices are being developed. Using nanoparticle contrast agents, images such as ultrasound and MRI have a favorable distribution and improved contrast. This might be accomplished by self assembled biocompatible nanodevices that will detect, evaluate, treat and report to the clinical doctor automatically.
The small size of nanoparticles endows them with properties that can be very useful in oncology, particularly in imaging. Quantum dots (nanoparticles with quantum confinement properties, such as size-tunable light emission), when used in conjunction with MRI (magnetic resonance imaging), can produce exceptional images of tumor sites. Nanoparticles of cadmium selenide (quantum dots) glow when exposed to ultraviolet light. When injected, they seep into cancer tumors. The surgeon can see the glowing tumor, and use it as a guide for more accurate tumor removal.These nanoparticles are much brighter than organic dyes and only need one light source for excitation. This means that the use of fluorescent quantum dots could produce a higher contrast image and at a lower cost than today's organic dyes used as contrast media. The downside, however, is that quantum dots are usually made of quite toxic elements.
Tracking movement can help determine how well drugs are being distributed or how substances are metabolized. It is difficult to track a small group of cells throughout the body, so scientists used to dye the cells. These dyes needed to be excited by light of a certain wavelength in order for them to light up. While different color dyes absorb different frequencies of light, there was a need for as many light sources as cells. A way around this problem is with luminescent tags. These tags are quantum dots attached to proteins that penetrate cell membranes. The dots can be random in size, can be made of bio-inert material, and they demonstrate the nanoscale property that color is size-dependent. As a result, sizes are selected so that the frequency of light used to make a group of quantum dots fluoresce is an even multiple of the frequency required to make another group incandesce. Then both groups can be lit with a single light source. They have also found a way to insert nanoparticles into the affected parts of the body so that those parts of the body will glow showing the tumor growth or shrinkage or also organ trouble.
Sensing.
Nanotechnology-on-a-chip is one more dimension of lab-on-a-chip technology. Magnetic nanoparticles, bound to a suitable antibody, are used to label specific molecules, structures or microorganisms. Gold nanoparticles tagged with short segments of DNA can be used for detection of genetic sequence in a sample. Multicolor optical coding for biological assays has been achieved by embedding different-sized quantum dots into polymeric microbeads. Nanopore technology for analysis of nucleic acids converts strings of nucleotides directly into electronic signatures.
Sensor test chips containing thousands of nanowires, able to detect proteins and other biomarkers left behind by cancer cells, could enable the detection and diagnosis of cancer in the early stages from a few drops of a patient's blood. Nanotechnology is helping to advance the use of arthroscopes, which are pencil-sized devices that are used in surgeries with lights and cameras so surgeons can do the surgeries with smaller incisions. The smaller the incisions the faster the healing time which is better for the patients. It is also helping to find a way to make an arthroscope smaller than a strand of hair.
Blood purification.
Magnetic micro particles are proven research instruments for the separation of cells and proteins from complex media. The technology is available under the name Magnetic-activated cell sorting or Dynabeads among others. More recently it was shown in animal models that magnetic nanoparticles can be used for the removal of various noxious compounds including toxins, pathogens, and proteins from whole blood in an extracorporeal circuit similar to dialysis. In contrast to dialysis, which works on the principle of the size related diffusion of solutes and ultrafiltration of fluid across a semi-permeable membrane, the purification with nanoparticles allows specific targeting of substances. Additionally larger compounds which are commonly not dialyzable can be removed.
The purification process is based on functionalized iron oxide or carbon coated metal nanoparticles with ferromagnetic or superparamagnetic properties. Binding agents such as proteins, antibodies, antibiotics, or synthetic ligands are covalently linked to the particle surface. These binding agents are able to interact with target species forming an agglomerate. Applying an external magnetic field gradient allows exerting a force on the nanoparticles. Hence the particles can be separated from the bulk fluid, thereby cleaning it from the contaminants.
The small size (< 100 nm) and large surface area of functionalized nanomagnets leads to advantageous properties compared to hemoperfusion, which is a clinically used technique for the purification of blood and is based on surface adsorption. These advantages are high loading and accessibility of the binding agents, high selectivity towards the target compound, fast diffusion, small hydrodynamic resistance, and low dosage.
This approach offers new therapeutic possibilities for the treatment of systemic infections such as sepsis by directly removing the pathogen. It can also be used to selectively remove cytokines or endotoxins or for the dialysis of compounds which are not accessible by traditional dialysis methods. However the technology is still in a preclinical phase and first clinical trials are not expected before 2017.
Tissue engineering.
Nanotechnology may be used as part of tissue engineering to help reproduce or repair damaged tissue using suitable nanomaterial-based scaffolds and growth factors. Tissue engineering if successful may replace conventional treatments like organ transplants or artificial implants. Nanoparticles such as graphene, carbon nanotubes, molybdenum disulfide and tungsten disulfide are being used as reinforcing agents to fabricate mechanically strong biodegradable polymeric nanocomposites for bone tissue engineering applications. The addition of these nanoparticles in the polymer matrix at low concentrations (~0.2 weight %) leads to significant improvements in the compressive and flexural mechanical properties of polymeric nanocomposites. Potentially, these nanocomposites may be used as a novel, mechanically strong, light weight composite as bone implants.
For example, a flesh welder was demonstrated to fuse two pieces of chicken meat into a single piece using a suspension of gold-coated nanoshells activated by an infrared laser. This could be used to weld arteries during surgery.
Another example is nanonephrology, the use of nanomedicine on the kidney.
Medical devices.
Neuro-electronic interfacing is a visionary goal dealing with the construction of nanodevices that will permit computers to be joined and linked to the nervous system. This idea requires the building of a molecular structure that will permit control and detection of nerve impulses by an external computer. A refuelable strategy implies energy is refilled continuously or periodically with external sonic, chemical, tethered, magnetic, or biological electrical sources, while a nonrefuelable strategy implies that all power is drawn from internal energy storage which would stop when all energy is drained. A nanoscale enzymatic biofuel cell for self-powered nanodevices have been developed that uses glucose from biofluids including human blood and watermelons. One limitation to this innovation is the fact that electrical interference or leakage or overheating from power consumption is possible. The wiring of the structure is extremely difficult because they must be positioned precisely in the nervous system. The structures that will provide the interface must also be compatible with the body's immune system.
Molecular nanotechnology is a speculative subfield of nanotechnology regarding the possibility of engineering molecular assemblers, machines which could re-order matter at a molecular or atomic scale. Nanomedicine would make use of these nanorobots, introduced into the body, to repair or detect damages and infections. Molecular nanotechnology is highly theoretical, seeking to anticipate what inventions nanotechnology might yield and to propose an agenda for future inquiry. The proposed elements of molecular nanotechnology, such as molecular assemblers and nanorobots are far beyond current capabilities.Future advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular machines, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book "The Singularity Is Near" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030.According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a "medical" use for Feynman's theoretical micromachines (see nanotechnology). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) "swallow the doctor". The idea was incorporated into Feynman's 1959 essay "There's Plenty of Room at the Bottom."

</doc>
<doc id="21518" url="https://en.wikipedia.org/wiki?curid=21518" title="NMR (disambiguation)">
NMR (disambiguation)

NMR may refer to:
Applications of nuclear magnetic resonance
History and culture studies
Entertainment media

</doc>
<doc id="21520" url="https://en.wikipedia.org/wiki?curid=21520" title="Null set">
Null set

In set theory, a null set "N" ⊂ R is a set that can be covered by a countable union of intervals of arbitrarily small total length. The notion of null set in set theory anticipates the development of Lebesgue measure since a null set necessarily has measure zero.
Definition.
Suppose "A" is a subset of the real line R such that 
where the "U"n are intervals and |"U"| is the length of "U", then "A" is a null set. Also known as a set of zero-content. 
In terminology of mathematical analysis, this definition requires that there be a sequence of open covers of "A" for which the limit of the lengths of the covers is zero.
Null sets include all finite sets, all countable sets, and even some uncountable sets such as the Cantor set.
Properties.
The empty set is always a null set. More generally, any countable union of null sets is null. Any measurable subset of a null set is itself a null set. Together, these facts show that the "m"-null sets of "X" form a sigma-ideal on "X". Similarly, the measurable "m"-null sets form a sigma-ideal of the sigma-algebra of measurable sets. Thus, null sets may be interpreted as negligible sets, defining a notion of almost everywhere.
Lebesgue measure.
The Lebesgue measure is the standard way of assigning a length, area or volume to subsets of Euclidean space.
A subset "N" of R has null Lebesgue measure and is considered to be a null set in R if and only if:
This condition can be generalised to R"n", using "n"-cubes instead of intervals. In fact, the idea can be made to make sense on any differentiable manifold, even if there is no Lebesgue measure there.
For instance:
if λ is Lebesgue measure for R and π is Lebesgue measure for R2, then the product measure λ x λ = π . In terms of null sets, the following equivalence has been styled a Fubini's theorem: 
Uses.
Null sets play a key role in the definition of the Lebesgue integral: if functions "f" and "g" are equal except on a null set, then "f" is integrable if and only if "g" is, and their integrals are equal.
A measure in which all subsets of null sets are measurable is "complete". Any non-complete measure can be completed to form a complete measure by asserting that subsets of null sets have measure zero. Lebesgue measure is an example of a complete measure; in some constructions, it's defined as the completion of a non-complete Borel measure.
A subset of the Cantor set which is not Borel measurable.
The Borel measure is not complete. One simple construction is to start with the standard Cantor set "K", which is closed hence Borel measurable, and which has measure zero, and to find a subset "F" of "K" which is not Borel measurable. (Since the Lebesgue measure is complete, this "F" is of course Lebesgue measurable.)
First, we have to know that every set of positive measure contains a nonmeasurable subset. Let "f" be the Cantor function, a continuous function which is locally constant on "Kc", and monotonically increasing on 1, with "f"(0) = 0 and "f"(1) = 1. Obviously, "f"("Kc") is countable, since it contains one point per component of "Kc". Hence "f"("Kc") has measure zero, so "f"("K") has measure one. We need a strictly monotonic function, so consider "g"("x") = "f"("x") + "x". Since "g"("x") is strictly monotonic and continuous, it is a homeomorphism. Furthermore, "g"("K") has measure one. Let "E" ⊂ "g"("K") be non-measurable, and let "F" = "g"−1("E"). Because "g" is injective, we have that "F" ⊂ "K", and so "F" is a null set. However, if it were Borel measurable, then "g"("F") would also be Borel measurable (here we use the fact that the preimage of a Borel set by a continuous function is measurable; "g"("F") = ("g"−1)−1("F") is the preimage of "F" through the continuous function "h" = "g"−1.) Therefore, "F" is a null, but non-Borel measurable set.
Haar null.
In a separable Banach space ("X", +), the group operation moves any subset "A" ⊂ "X" to the translates "A" + "x" for any "x" ∈ "X". When there is a probability measure μ on the σ-algebra of Borel subsets of "X", such that for all "x", μ("A" + "x") = 0, then "A" is a Haar null set.
The term refers to the null invariance of the measures of translates, associating it with the complete invariance found with Haar measure.
Some algebraic properties of topological groups have been related to the size of subsets and Haar null sets.
Haar null sets have been used to in Polish groups to show that when "A" is not a meager set then "A"–1"A" contains the identity element. This property is named for Hugo Steinhaus since it is the conclusion of the Steinhaus theorem.

</doc>
<doc id="21522" url="https://en.wikipedia.org/wiki?curid=21522" title="November 24">
November 24


</doc>
<doc id="21523" url="https://en.wikipedia.org/wiki?curid=21523" title="Artificial neural network">
Artificial neural network

In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.
For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, the output neuron that determines which character was read is activated.
Like other machine learning methods systems that learn from data neural networks have been used to solve a wide variety of tasks, like computer vision and speech recognition, that are hard to solve using ordinary rule-based programming.
Background.
Examinations of humans' central nervous systems inspired the concept of artificial neural networks. In an artificial neural network, simple artificial nodes, known as "neurons", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network.
There is no single formal definition of what an artificial neural network is. However, a class of statistical models may commonly be called "neural" if it possesses the following characteristics:
The adaptive weights can be thought of as connection strengths between neurons, which are activated during training and prediction.
Artificial neural networks are similar to biological neural networks in the performing by its units of functions collectively and in parallel, rather than by a clear delineation of subtasks to which individual units are assigned. The term "neural network" usually refers to models employed in statistics, cognitive psychology and artificial intelligence. Neural network models which command the central nervous system and the rest of the brain are part of theoretical neuroscience and computational neuroscience
In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional, artificial intelligence connectionist models. What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. Historically, the use of neural network models marked a directional shift in the late eighties from high-level (symbolic) artificial intelligence, characterized by expert systems with knowledge embodied in "if-then" rules, to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a dynamical system.
History.
Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
Hebbian learning.
In the late 1940s psychologist Donald Hebb created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.
Farley and Wesley A. Clark (1954) first used computational machines, then called "calculators," to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).
Frank Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer computer learning network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit which could not be processed by neural networks until after the backpropagation algorithm was created by Paul Werbos (1975).
Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second significant issue was that computers didn't have enough processing power to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power.
Backpropagation and Resurgence.
A key advance that came later was the backpropagation algorithm which effectively solved the exclusive-or problem, and more generally the problem of quickly training multi-layer neural networks (Werbos 1975).
In the mid-1980s, parallel distributed processing became popular under the name connectionism. The textbook by David E. Rumelhart and James McClelland (1986) provided a full exposition of the use of connectionism in computers to simulate neural processes.
Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and the biological architecture of the brain is debated; it's not clear to what degree artificial neural networks mirror brain function.
Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. But the advent of deep learning in the late 2000s sparked renewed interest in neural networks.
Improvements since 2006.
Computational devices have been created in CMOS, for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, would create a new class of neural computing because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.
Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short term memory (LSTM) of Alex Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.
Fast GPU-based implementations of this approach by Dan Ciresan and colleagues at IDSIA have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge, and others. Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance
on important benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun at NYU.
Deep, highly nonlinear neural architectures similar to the 1980 neocognitron by Kunihiko Fukushima
and the "standard architecture of vision", inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex, can also be pre-trained by unsupervised methods
of Geoff Hinton's lab at University of Toronto. A team from this lab won a 2012 contest sponsored by Merck to design software to help find molecules that might lead to new drugs.
Models.
Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function formula_1 or a distribution over formula_2 or both formula_2 and formula_4, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase "ANN model" is really the definition of a "class" of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).
Network function.
The word "network" in the term 'artificial neural network' refers to the inter–connections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons, some having increased layers of input neurons and output neurons. The synapses store parameters called "weights" that manipulate the data in the calculations.
An ANN is typically defined by three types of parameters:
Mathematically, a neuron's network function formula_5 is defined as a composition of other functions formula_6, which can further be defined as a composition of other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables. A widely used type of composition is the "nonlinear weighted sum", where formula_7, where formula_8 (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent. It will be convenient for the following to refer to a collection of functions formula_9 as simply a vector formula_10.
This figure depicts such a decomposition of formula_11, with dependencies between variables indicated by arrows. These can be interpreted in two ways.
The first view is the functional view: the input formula_12 is transformed into a 3-dimensional vector formula_13, which is then transformed into a 2-dimensional vector formula_14, which is finally transformed into formula_11. This view is most commonly encountered in the context of optimization.
The second view is the probabilistic view: the random variable formula_16 depends upon the random variable formula_17, which depends upon formula_18, which depends upon the random variable formula_2. This view is most commonly encountered in the context of graphical models.
The two views are largely equivalent. In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of formula_14 are independent of each other given their input formula_13). This naturally enables a degree of parallelism in the implementation.
Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where formula_11 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.
Learning.
What has attracted the most interest in neural networks is the possibility of "learning". Given a specific "task" to solve, and a "class" of functions formula_23, learning means using a set of "observations" to find formula_24 which solves the task in some "optimal" sense.
This entails defining a cost function formula_25 such that, for the optimal solution formula_26, formula_27 formula_28 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).
The cost function formula_29 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.
For applications where the solution is dependent on some data, the cost must necessarily be a "function of the observations", otherwise we would not be modelling anything related to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model formula_11, which minimizes formula_31, for data pairs formula_32 drawn from some distribution formula_33. In practical situations we would only have formula_34 samples from formula_33 and thus, for the above example, we would only minimize formula_36. Thus, the cost is minimized over a sample of the data rather than the entire distribution generating the data.
When formula_37 some form of online machine learning must be used, where the cost is partially minimized as each new example is seen. While online machine learning is often used when formula_33 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.
Choosing a cost function.
While it is possible to define some arbitrary ad hoc cost function, frequently a particular cost will be used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function will depend on the desired task. An overview of the three main categories of learning tasks is provided below:
Learning paradigms.
There are three major learning paradigms, each corresponding to a particular abstract learning task. These are supervised learning, unsupervised learning and reinforcement learning.
Supervised learning.
In supervised learning, we are given a set of example pairs formula_39 and the aim is to find a function formula_1 in the allowed class of functions that matches the examples. In other words, we wish to "infer" the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.
A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, formula_5, and the target value formula_42 over all the example pairs. When one tries to minimize this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), one obtains the common and well-known backpropagation algorithm for training neural networks.
Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
Unsupervised learning.
In unsupervised learning, some data formula_12 is given and the cost function to be minimized, that can be any function of the data formula_12 and the network's output, formula_11.
The cost function is dependent on the task (what we are trying to model) and our "a priori" assumptions (the implicit properties of our model, its parameters and the observed variables).
As a trivial example, consider the model formula_46 where formula_47 is a constant and the cost formula_48. Minimizing this cost will give us a value of formula_47 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_12 and formula_5, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).
Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
Reinforcement learning.
In reinforcement learning, data formula_12 are usually not given, but generated by an agent's interactions with the environment. At each point in time formula_53, the agent performs an action formula_54 and the environment generates an observation formula_55 and an instantaneous cost formula_56, according to some (usually unknown) dynamics. The aim is to discover a "policy" for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.
More formally the environment is modeled as a Markov decision process (MDP) with states formula_57 and actions formula_58 with the following probability distributions: the instantaneous cost distribution formula_59, the observation distribution formula_60 and the transition formula_61, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.
ANNs are frequently used in reinforcement learning as part of the overall algorithm. Dynamic programming has been coupled with ANNs (Neuro dynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.
Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
Learning algorithms.
Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost criterion. There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.
Most of the algorithms used in training artificial neural networks employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. The backpropagation training algorithms are usually classified into three categories: steepest descent (with variable learning rate, with variable learning rate and momentum, resilient backpropagation), quasi-Newton (Broyden-Fletcher-Goldfarb-Shanno, one step secant, Levenberg-Marquardt) and conjugate gradient (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled conjugate gradient).
Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are some commonly used methods for training neural networks.
Employing artificial neural networks.
Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that 'learns' from observed data. However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential.
With the correct implementation, ANNs can be used naturally in online learning and large data set applications. Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware.
Applications.
The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.
Real-life applications.
The tasks artificial neural networks are applied to tend to fall within the following broad categories:
Application areas include the system identification and control (vehicle control, trajectory prediction, process control, natural resources management), quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. automated trading systems), data mining (or knowledge discovery in databases, "KDD"), visualization and e-mail spam filtering.
Artificial neural networks have also been used to diagnose several cancers. An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology. These networks have also been used to diagnose prostate cancer. The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient. The models do not depend on assumptions about correlations of different variables. Colorectal cancer has also been predicted using the neural networks. Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. After training, the networks could predict multiple patient outcomes from unrelated institutions.
Neural networks and neuroscience.
Theoretical and computational neuroscience is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.
The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
Types of models.
Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons (e.g.), models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.
Memory networks.
Integrating external memory components with artificial neural networks has a long history dating back to early research in distributed representations and self-organizing maps. E.g. in sparse distributed memory the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with "neurons" essentially serving as address encoders and decoders.
More recently deep learning was shown to be useful in semantic hashing where a deep graphical model of the word-count vectors is obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document.
Neural Turing Machines developed by Google DeepMind extend the capabilities of deep neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.
Memory Networks is another extension to neural networks incorporating long-term memory which was developed by Facebook research. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response.
Neural network software.
Neural network software is used to simulate, research, develop and apply artificial neural networks, biological neural networks and, in some cases, a wider array of adaptive systems.
Types of artificial neural networks.
Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi–input many directional feedback loops and layers. On the whole, these systems use algorithms in their programming to determine control and organization of their functions. Most systems use "weights" to change the parameters of the throughput and the varying connections to the neurons. Artificial neural networks can be autonomous and learn by input from outside "teachers" or even self-teaching from written-in rules.
Theoretical properties.
<section begin=theory />
Computational power.
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required or the settings of the weights.
Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a Universal Turing Machine using a finite number of neurons and standard linear connections. Further, it has been shown that the use of irrational values for weights results in a machine with super-Turing power.
Capacity.
Artificial neural network models have a property called 'capacity', which roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Convergence.
Nothing can be said in general about convergence since it depends on a number of factors. Firstly, there may exist many local minima. This depends on the cost function and the model. Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. Thirdly, for a very large amount of data or parameters, some methods become impractical. In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application. 
Generalization and statistics.
In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. There are two schools of thought for avoiding this problem: The first is to use cross-validation and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. The second is to use some form of "regularization". This is a concept that emerges naturally in a probabilistic (Bayesian) framework, where the regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:
<section end=theory />
Criticism.
Training issues.
A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation . This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean A. Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turn it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
A. K. Dewdney, a former "Scientific American" columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool."
Hardware issues.
To implement large and effective software neural networks, considerable processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons which must often be matched with incredible amounts of CPU processing power and time.
Jürgen Schmidhuber notes that the resurgence of neural networks in the twenty-first century, and their renewed success at image recognition tasks is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before (but adds that this doesn't overcome algorithmic problems such as vanishing gradients "in a fundamental way"). The use of GPUs instead of ordinary CPUs can bring training times for some networks down from months to mere days.
Computing power continues to grow roughly according to Moore's Law, which may provide sufficient resources to accomplish new tasks. Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips with circuits designed to implement neural nets from the ground up.
Practical counterexamples to criticisms.
Arguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud.
Technology writer Roger Bridgman commented on Dewdney's statements about neural nets:
Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.
Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.
Hybrid approaches.
Some other criticisms come from advocates of hybrid models (combining neural networks and symbolic approaches), who believe that the intermix of these two approaches can better capture the mechanisms of the human mind.

</doc>
<doc id="21525" url="https://en.wikipedia.org/wiki?curid=21525" title="Nutrition">
Nutrition

Nutrition is the science that interprets the interaction of nutrients and other substances in food (e.g. phytonutrients, anthocyanins, tannins, etc.) in relation to maintenance, growth, reproduction, health and disease of an organism. It includes food intake, absorption, assimilation, biosynthesis, catabolism and excretion.
The diet of an organism is what it eats, which is largely determined by the availability, the processing and palatability of foods. A healthy diet includes preparation of food and storage methods that preserve nutrients from oxidation, heat or leaching, and that reduce risk of food-born illnesses.
Registered dietitian nutritionists (RDs or RDNs) are health professionals qualified to provide safe, evidence-based dietary advice which includes a review of what is eaten, a thorough review of nutritional health, and a personalized nutritional treatment plan. They also provide preventive and therapeutic programs at work places, schools and similar institutions. Certified Clinical Nutritionists or CCNs, are trained health professionals who also offer dietary advice on the role of nutrition in chronic disease, including possible prevention or remediation by addressing nutritional deficiencies before resorting to drugs. Government regulation especially in terms of licensing, is currently less universal for the CCN than that of RD or RDN. Another advanced Nutrition Professional is a Certified Nutrition Specialist or CNS. These Board Certified Nutritionists typically specialize in obesity and chronic disease. In order to become board certified, potential CNS candidate must pass an examination, much like Registered Dieticians. This exam covers specific domains within the health sphere including; Clinical Intervention and Human Health.
A poor diet may have an injurious impact on health, causing deficiency diseases such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism; health-threatening conditions like obesity and metabolic syndrome; and such common chronic systemic diseases as cardiovascular disease, diabetes, and osteoporosis. A poor diet can cause the wasting of kwashiorkor in acute cases, and the stunting of marasmus in chronic cases of malnutrition.
History.
Antiquity.
The first recorded dietary advice, carved into a Babylonian stone tablet in about 2500 BC, cautioned those with pain inside to avoid eating onions for three days. Scurvy, later found to be a vitamin C deficiency, was first described in 1500 BC in the Ebers Papyrus.
According to Walter Gratzer, the study of nutrition probably began during the 6th century BC. In China, the concept of "Qi" developed, a spirit or "wind" similar to what Western Europeans later called "pneuma". Food was classified into "hot" (for example, meats, blood, ginger, and hot spices) and "cold" (green vegetables) in China, India, Malaya, and Persia. "Humours" developed perhaps first in China alongside "qi". Ho the Physician concluded that diseases are caused by deficiencies of elements (Wu Xing: fire, water, earth, wood, and metal), and he classified diseases as well as prescribed diets. About the same time in Italy, Alcmaeon of Croton (a Greek) wrote of the importance of equilibrium between what goes in and what goes out, and warned that imbalance would result disease marked by obesity or emaciation.
The first recorded nutritional experiment with human subjects is found in the Bible's Book of Daniel. Daniel and his friends were captured by the king of Babylon during an invasion of Israel. Selected as court servants, they were to share in the king's fine foods and wine. But they objected, preferring vegetables (pulses) and water in accordance with their Jewish dietary restrictions. The king's chief steward reluctantly agreed to a trial. Daniel and his friends received their diet for 10 days and were then compared to the king's men. Appearing healthier, they were allowed to continue with their diet.
Around 475 BC, Anaxagoras stated that food is absorbed by the human body and, therefore, contains "homeomerics" (generative components), suggesting the existence of nutrients. Around 400 BC, Hippocrates, who recognized and was concerned with obesity, which may have been common in southern Europe at the time, said, "Let food be your medicine and medicine be your food." The works that are still attributed to him, "Corpus Hippocraticum", called for moderation and emphasized exercise.
Salt, pepper and other spices were prescribed for various ailments in various preparations for example mixed with vinegar. In the 2nd century BC, Cato the Elder believed that cabbage (or the urine of cabbage-eaters) could cure digestive diseases, ulcers, warts, and intoxication. Living about the turn of the millennium, Aulus Celsus, an ancient Roman doctor, believed in "strong" and "weak" foods (bread for example was strong, as were older animals and vegetables).
Galen to Lind.
One mustn't overlook the doctrines of Galen: In use from his life in the 1st century AD until the 17th century, it was heresy to disagree with him for 1500 years. Galen was physician to gladiators in Pergamon, and in Rome, physician to Marcus Aurelius and the three emperors who succeeded him. Most of Galen's teachings were gathered and enhanced in the late 11th century by Benedictine monks at the School of Salerno in "Regimen sanitatis Salernitanum", which still had users in the 17th century. Galen believed in the bodily "humours" of Hippocrates, and he taught that "pneuma" is the source of life. Four elements (earth, air, fire and water) combine into "complexion", which combines into states (the four temperaments: sanguine, phlegmatic, choleric, and melancholic). The states are made up of pairs of attributes (hot and moist, cold and moist, hot and dry, and cold and dry), which are made of four humours: blood, phlegm, green (or yellow) bile, and black bile (the bodily form of the elements). Galen thought that for a person to have gout, kidney stones, or arthritis was scandalous, which Gratzer likens to Samuel Butler's "Erehwon" (1872) where sickness is a crime.
In the 1500s, Paracelsus was probably the first to criticize Galen publicly. Also in the 16th century, scientist and artist Leonardo da Vinci compared metabolism to a burning candle. Leonardo did not publish his works on this subject, but he was not afraid of thinking for himself and he definitely disagreed with Galen. Ultimately, 16th century works of Andreas Vesalius, sometimes called the father of modern medicine, overturned Galen's ideas. He was followed by piercing thought amalgamated with the era's mysticism and religion sometimes fueled by the mechanics of Newton and Galileo. Jan Baptist van Helmont, who discovered several gases such as carbon dioxide, performed the first quantitative experiment. Robert Boyle advanced chemistry. Sanctorius measured body weight. Physician Herman Boerhaave modeled the digestive process. Physiologist Albrecht von Haller worked out the difference between nerves and muscles.
Sometimes overlooked during his life, James Lind, a physician in the British navy, performed the first scientific nutrition experiment in 1747. Lind discovered that lime juice saved sailors that had been at sea for years from scurvy, a deadly and painful bleeding disorder. Between 1500 and 1800, an estimated two million sailors had died of scurvy. The discovery was ignored for forty years, after which British sailors became known as "limeys." The essential vitamin C within citrus fruits would not be identified by scientists until 1932.
Lavoisier and modern science.
Around 1770, Antoine Lavoisier discovered the details of metabolism, demonstrating that the oxidation of food is the source of body heat. Called the most fundamental chemical discovery of the 18th century, Lavoisier discovered the principle of conservation of mass. His ideas made the phlogiston theory of combustion obsolete.
In 1790, George Fordyce recognized calcium as necessary for the survival of fowl. In the early 19th century, the elements carbon, nitrogen, hydrogen, and oxygen were recognized as the primary components of food, and methods to measure their proportions were developed.
In 1816, François Magendie discovered that dogs fed only carbohydrates (sugar), fat (olive oil), and water died evidently of starvation, but dogs also fed protein survived, identifying protein as an essential dietary component. William Prout in 1827 was the first person to divide foods into carbohydrates, fat, and protein. During the 19th century, Jean-Baptiste Dumas and Justus von Liebig quarrelled over their shared belief that animals get their protein directly from plants (animal and plant protein are the same and that humans do not create organic compounds). With a reputation as the leading organic chemist of his day but with no credentials in animal physiology, Liebig grew rich making food extracts like beef bouillon and infant formula that were later found to be of questionable nutritious value. In the 1860s, Claude Bernard discovered that body fat can be synthesized from carbohydrate and protein, showing that the energy in blood glucose can be stored as fat or as glycogen.
In the early 1880s, Kanehiro Takaki observed that Japanese sailors (whose diets consisted almost entirely of white rice) developed beriberi (or endemic neuritis, a disease causing heart problems and paralysis), but British sailors and Japanese naval officers did not. Adding various types of vegetables and meats to the diets of Japanese sailors prevented the disease, (not because of the increased protein as Takaki supposed but because it introduced a few parts per million of thiamine to the diet, later understood as a cure).
In 1896, Eugen Baumann observed iodine in thyroid glands. In 1897, Christiaan Eijkman worked with natives of Java, who also suffered from beriberi. Eijkman observed that chickens fed the native diet of white rice developed the symptoms of beriberi but remained healthy when fed unprocessed brown rice with the outer bran intact. Eijkman cured the natives by feeding them brown rice, discovering that food can cure disease. Over two decades later, nutritionists learned that the outer rice bran contains vitamin B1, also known as thiamine.
From 1900 to the present.
In the early 20th century, Carl von Voit and Max Rubner independently measured caloric energy expenditure in different species of animals, applying principles of physics in nutrition. In 1906, Edith G. Willcock and Frederick Hopkins showed that the amino acid tryptophan aids the well-being of mice but it did not assure their growth. In the middle of twelve years of attempts to isolate them, Hopkins said in a 1906 lecture that "unsuspected dietetic factors," other than calories, protein, and minerals, are needed to prevent deficiency diseases. In 1907, Stephen M. Babcock and Edwin B. Hart conducted the single-grain experiment, which took nearly four years to complete.
Oxford University closed down its nutrition department after World War II because the subject seemed to have been completed between 1912 and 1944.
In 1912, Casimir Funk coined the term vitamin, a vital factor in the diet, from the words "vital" and "amine," because these unknown substances preventing scurvy, beriberi, and pellagra, were thought then to be derived from ammonia. The vitamins were studied in the first half of the 20th century.
In 1913, Elmer McCollum discovered the first vitamins, fat-soluble vitamin A, and water-soluble vitamin B (in 1915; now known to be a complex of several water-soluble vitamins) and named vitamin C as the then-unknown substance preventing scurvy. Lafayette Mendel and Thomas Osborne also performed pioneering work on vitamins A and B. In 1919, Sir Edward Mellanby incorrectly identified rickets as a vitamin A deficiency because he could cure it in dogs with cod liver oil. In 1922, McCollum destroyed the vitamin A in cod liver oil, but found that it still cured rickets. Also in 1922, H.M. Evans and L.S. Bishop discover vitamin E as essential for rat pregnancy, originally calling it "food factor X" until 1925.
In 1925, Hart discovered that trace amounts of copper are necessary for iron absorption. In 1927, Adolf Otto Reinhold Windaus synthesized vitamin D, for which he won the Nobel Prize in Chemistry in 1928. In 1928, Albert Szent-Györgyi isolated ascorbic acid, and in 1932 proved that it is vitamin C by preventing scurvy. In 1935, he synthesized it, and in 1937, he won a Nobel Prize for his efforts. Szent-Györgyi concurrently elucidated much of the citric acid cycle.
In the 1930s, William Cumming Rose identified essential amino acids, necessary protein components that the body cannot synthesize. In 1935, Underwood and Marston independently discovered the necessity of cobalt. In 1936, Eugene Floyd DuBois showed that work and school performance are related to caloric intake. In 1938, Erhard Fernholz discovered the chemical structure of vitamin E and then he tragically disappeared. It was synthesised the same year by Paul Karrer.
In 1940, rationing in the United Kingdom during and after World War II took place according to nutritional principles drawn up by Elsie Widdowson and others. In 1941, the first Recommended Dietary Allowances (RDAs) were established by the National Research Council.
In 1992, The U.S. Department of Agriculture introduced the Food Guide Pyramid. In 2002, a Natural Justice study showed a relation between nutrition and violent behavior. In 2005, one inconclusive study found that obesity could be caused by adenovirus in addition to bad nutrition.
World leaders are looking at alternatives like genetically modified foods to tackle the problem of world hunger and food shortages.
Nutrients.
The list of nutrients that people are known to require is, in the words of Marion Nestle, "almost certainly incomplete". As of 2014, nutrients are thought to be of two types: macro-nutrients which are needed in relatively large amounts, and micronutrients which are needed in smaller quantities. A type of carbohydrate, dietary fiber, i.e. non-digestible material such as cellulose, is required, for both mechanical and biochemical reasons, although the exact reasons remain unclear. Other micronutrients include antioxidants and phytochemicals, which are said to influence (or protect) some body systems. Their necessity is not as well established as in the case of, for instance, vitamins.
Most foods contain a mix of some or all of the nutrient types, together with other substances, such as toxins of various sorts. Some nutrients can be stored internally (e.g., the fat-soluble vitamins), while others are required more or less continuously. Poor health can be caused by a lack of required nutrients or, in extreme cases, too much of a required nutrient. For example, both salt and water (both absolutely required) will cause illness or even death in excessive amounts.
Macronutrients.
The macronutrients are carbohydrates, fats, protein, and water.
The macronutrients (excluding fiber and water) provide structural material (amino acids from which proteins are built, and lipids from which cell membranes and some signaling molecules are built) and energy. Some of the structural material can be used to generate energy internally, and in either case it is measured in Joules or kilocalories (often called "Calories" and written with a capital "C" to distinguish them from little 'c' calories). Carbohydrates and proteins provide 17 kJ approximately (4 kcal) of energy per gram, while fats provide 37 kJ (9 kcal) per gram, though the net energy from either depends on such factors as absorption and digestive effort, which vary substantially from instance to instance. Vitamins, minerals, fiber, and water do not provide energy, but are required for other reasons.
Molecules of carbohydrates and fats consist of carbon, hydrogen, and oxygen atoms. Carbohydrates range from simple monosaccharides (glucose, fructose, galactose) to complex polysaccharides (starch). Fats are triglycerides, made of assorted fatty acid monomers bound to a glycerol backbone. Some fatty acids, but not all, are essential in the diet: they cannot be synthesized in the body. Protein molecules contain nitrogen atoms in addition to carbon, oxygen, and hydrogen. The fundamental components of protein are nitrogen-containing amino acids, some of which are essential in the sense that humans cannot make them internally. Some of the amino acids are convertible (with the expenditure of energy) to glucose and can be used for energy production, just as ordinary glucose, in a process known as gluconeogenesis. By breaking down existing protein, the carbon skeleton of the various amino acids can be metabolized to intermediates in cellular respiration; the remaining ammonia is discarded primarily as urea in urine. This occurs normally only during prolonged starvation.
Carbohydrates.
Carbohydrates may be classified as monosaccharides, disaccharides, or polysaccharides depending on the number of monomer (sugar) units they contain. They constitute a large part of foods such as rice, noodles, bread, and other grain-based products.
Monosaccharides, disaccharides, and polysaccharides contain one, two, and three or more sugar units, respectively. Polysaccharides are often referred to as "complex" carbohydrates because they are typically long, multiple branched chains of sugar units.
Traditionally, simple carbohydrates are believed to be absorbed quickly, and therefore to raise blood-glucose levels more rapidly than complex carbohydrates. This, however, is not accurate. Some simple carbohydrates (e.g., fructose) follow different metabolic pathways (e.g., fructolysis) that result in only a partial catabolism to glucose, while, in essence, many complex carbohydrates may be digested at the same rate as simple carbohydrates.
Glucose stimulates the production of insulin through food entering the bloodstream, which is grasped by the beta cells in the pancreas.
Fiber.
Dietary fiber is a carbohydrate that is incompletely absorbed in humans and in some animals. Like all carbohydrates, when it is metabolized it can produce four Calories (kilocalories) of energy per gram. However, in most circumstances it accounts for less than that because of its limited absorption and digestibility. Dietary fiber consists mainly of cellulose, a large carbohydrate polymer which is indigestible as humans do not have the required enzymes to disassemble it. There are two subcategories: soluble and insoluble fiber. Whole grains, fruits (especially plums, prunes, and figs), and vegetables are good sources of dietary fiber. There are many health benefits of a high-fiber diet. Dietary fiber helps reduce the chance of gastrointestinal problems such as constipation and diarrhea by increasing the weight and size of stool and softening it. Insoluble fiber, found in whole wheat flour, nuts and vegetables, especially stimulates peristalsis – the rhythmic muscular contractions of the intestines, which move digesta along the digestive tract. Soluble fiber, found in oats, peas, beans, and many fruits, dissolves in water in the intestinal tract to produce a gel that slows the movement of food through the intestines. This may help lower blood glucose levels because it can slow the absorption of sugar. Additionally, fiber, perhaps especially that from whole grains, is thought to possibly help lessen insulin spikes, and therefore reduce the risk of type 2 diabetes. The link between increased fiber consumption and a decreased risk of colorectal cancer is still uncertain.
Fat.
A molecule of dietary fat typically consists of several fatty acids (containing long chains of carbon and hydrogen atoms), bonded to a glycerol. They are typically found as triglycerides (three fatty acids attached to one glycerol backbone). Fats may be classified as saturated or unsaturated depending on the detailed structure of the fatty acids involved. Saturated fats have all of the carbon atoms in their fatty acid chains bonded to hydrogen atoms, whereas unsaturated fats have some of these carbon atoms double-bonded, so their molecules have relatively fewer hydrogen atoms than a saturated fatty acid of the same length. Unsaturated fats may be further classified as monounsaturated (one double-bond) or polyunsaturated (many double-bonds). Furthermore, depending on the location of the double-bond in the fatty acid chain, unsaturated fatty acids are classified as omega-3 or omega-6 fatty acids. Trans fats are a type of unsaturated fat with "trans"-isomer bonds; these are rare in nature and in foods from natural sources; they are typically created in an industrial process called (partial) hydrogenation. There are nine kilocalories in each gram of fat. Fatty acids such as conjugated linoleic acid, catalpic acid, eleostearic acid and punicic acid, in addition to providing energy, represent potent immune modulatory molecules.
Saturated fats (typically from animal sources) have been a staple in many world cultures for millennia. Unsaturated fats (e. g., vegetable oil) are considered healthier, while trans fats are to be avoided. Saturated and some trans fats are typically solid at room temperature (such as butter or lard), while unsaturated fats are typically liquids (such as olive oil or flaxseed oil). Trans fats are very rare in nature, and have been shown to be highly detrimental to human health, but have properties useful in the food processing industry, such as rancidity resistance.
Essential fatty acids.
Most fatty acids are non-essential, meaning the body can produce them as needed, generally from other fatty acids and always by expending energy to do so. However, in humans, at least two fatty acids are essential and must be included in the diet. An appropriate balance of essential fatty acids—omega-3 and omega-6 fatty acids—seems also important for health, although definitive experimental demonstration has been elusive. Both of these "omega" long-chain polyunsaturated fatty acids are substrates for a class of eicosanoids known as prostaglandins, which have roles throughout the human body. They are hormones, in some respects. The omega-3 eicosapentaenoic acid (EPA), which can be made in the human body from the omega-3 essential fatty acid alpha-linolenic acid (ALA), or taken in through marine food sources, serves as a building block for series 3 prostaglandins (e.g., weakly inflammatory PGE3). The omega-6 dihomo-gamma-linolenic acid (DGLA) serves as a building block for series 1 prostaglandins (e.g. anti-inflammatory PGE1), whereas arachidonic acid (AA) serves as a building block for series 2 prostaglandins (e.g. pro-inflammatory PGE 2). Both DGLA and AA can be made from the omega-6 linoleic acid (LA) in the human body, or can be taken in directly through food. An appropriately balanced intake of omega-3 and omega-6 partly determines the relative production of different prostaglandins, which is one reason why a balance between omega-3 and omega-6 is believed important for cardiovascular health. In industrialized societies, people typically consume large amounts of processed vegetable oils, which have reduced amounts of the essential fatty acids along with too much of omega-6 fatty acids relative to omega-3 fatty acids.
The conversion rate of omega-6 DGLA to AA largely determines the production of the prostaglandins PGE1 and PGE2. Omega-3 EPA prevents AA from being released from membranes, thereby skewing prostaglandin balance away from pro-inflammatory PGE2 (made from AA) toward anti-inflammatory PGE1 (made from DGLA). Moreover, the conversion (desaturation) of DGLA to AA is controlled by the enzyme delta-5-desaturase, which in turn is controlled by hormones such as insulin (up-regulation) and glucagon (down-regulation). The amount and type of carbohydrates consumed, along with some types of amino acid, can influence processes involving insulin, glucagon, and other hormones; therefore, the ratio of omega-3 versus omega-6 has wide effects on general health, and specific effects on immune function and inflammation, and mitosis (i.e., cell division).
Protein.
Proteins are structural materials in much of the animal body (e.g. muscles, skin, and hair). They also form the enzymes that control chemical reactions throughout the body. Each protein molecule is composed of amino acids, which are characterized by inclusion of nitrogen and sometimes sulphur (these components are responsible for the distinctive smell of burning protein, such as the keratin in hair). The body requires amino acids to produce new proteins (protein retention) and to replace damaged proteins (maintenance). As there is no protein or amino acid storage provision, amino acids must be present in the diet. Excess amino acids are discarded, typically in the urine. For all animals, some amino acids are "essential" (an animal cannot produce them internally) and some are "non-essential" (the animal can produce them from other nitrogen-containing compounds). About twenty amino acids are found in the human body, and about ten of these are essential and, therefore, must be included in the diet. A diet that contains adequate amounts of amino acids (especially those that are essential) is particularly important in some situations: during early development and maturation, pregnancy, lactation, or injury (a burn, for instance). A "complete" protein source contains all the essential amino acids; an "incomplete" protein source lacks one or more of the essential amino acids.
It is possible with protein combinations of two incomplete protein sources (e.g., rice and beans) to make a complete protein source, and characteristic combinations are the basis of distinct cultural cooking traditions. However, complementary sources of protein do not need to be eaten at the same meal to be used together by the body. Excess amino acids from protein can be converted into glucose and used for fuel through a process called gluconeogenesis. The amino acids remaining after such conversion are discarded.
Water.
Water is excreted from the body in multiple forms; including urine and feces, sweating, and by water vapour in the exhaled breath. Therefore, it is necessary to adequately rehydrate to replace lost fluids.
Early recommendations for the quantity of water required for maintenance of good health suggested that 6–8 glasses of water daily is the minimum to maintain proper hydration. However the notion that a person should consume eight glasses of water per day cannot be traced to a credible scientific source. The original water intake recommendation in 1945 by the Food and Nutrition Board of the National Research Council read: "An ordinary standard for diverse persons is 1 milliliter for each calorie of food. Most of this quantity is contained in prepared foods." More recent comparisons of well-known recommendations on fluid intake have revealed large discrepancies in the volumes of water we need to consume for good health. Therefore, to help standardize guidelines, recommendations for water consumption are included in two recent European Food Safety Authority (EFSA) documents (2010): (i) Food-based dietary guidelines and (ii) Dietary reference values for water or adequate daily intakes (ADI). These specifications were provided by calculating adequate intakes from measured intakes in populations of individuals with “desirable osmolarity values of urine and desirable water volumes per energy unit consumed.” For healthful hydration, the current EFSA guidelines recommend total water intakes of 2.0 L/day for adult females and 2.5 L/day for adult males. These reference values include water from drinking water, other beverages, and from food. About 80% of our daily water requirement comes from the beverages we drink, with the remaining 20% coming from food. Water content varies depending on the type of food consumed, with fruit and vegetables containing more than cereals, for example. These values are estimated using country-specific food balance sheets published by the Food and Agriculture Organisation of the United Nations. Other guidelines for nutrition also have implications for the beverages we consume for healthy hydration- for example, the World Health Organization (WHO) recommend that added sugars should represent no more than 10% of total energy intake.
The EFSA panel also determined intakes for different populations. Recommended intake volumes in the elderly are the same as for adults as despite lower energy consumption, the water requirement of this group is increased due to a reduction in renal concentrating capacity. Pregnant and breastfeeding women require additional fluids to stay hydrated. The EFSA panel proposes that pregnant women should consume the same volume of water as non-pregnant women, plus an increase in proportion to the higher energy requirement, equal to 300 mL/day. To compensate for additional fluid output, breastfeeding women require an additional 700 mL/day above the recommended intake values for non-lactating women.
For those who have healthy kidneys, it is somewhat difficult to drink too much water, but (especially in warm humid weather and while exercising) it is dangerous to drink too little. While overhydration is much less common than dehydration, it is also possible to drink far more water than necessary, which can result in water intoxication, a serious and potentially fatal condition. In particular, large amounts of de-ionized water are dangerous.
Micronutrients.
The micronutrients are minerals, vitamins, and others.
Minerals.
Dietary minerals are inorganic chemical elements required by living organisms, other than the four elements carbon, hydrogen, nitrogen, and oxygen that are present in nearly all organic molecules. The term "mineral" is archaic, since the intent is to describe simply the less common elements in the diet. Some are heavier than the four just mentioned, including several metals, which often occur as ions in the body. Some dietitians recommend that these be supplied from foods in which they occur naturally, or at least as complex compounds, or sometimes even from natural inorganic sources (such as calcium carbonate from ground oyster shells). Some minerals are absorbed much more readily in the ionic forms found in such sources. On the other hand, minerals are often artificially added to the diet as supplements; the most famous is likely iodine in iodized salt which prevents goiter.
Macrominerals.
Many elements are essential in relative quantity; they are usually called "bulk minerals". Some are structural, but many play a role as electrolytes. Elements with recommended dietary allowance (RDA) greater than 150  mg/day are, in alphabetical order (with informal or folk-medicine perspectives in parentheses):
Trace minerals.
Many elements are required in trace amounts, usually because they play a catalytic role in enzymes. Some trace mineral elements (RDA < 200 mg/day) are, in alphabetical order:
Vitamins.
As with the minerals discussed above, some vitamins are recognized as organic essential nutrients, necessary in the diet for good health. (Vitamin D is the exception: it can be synthesized in the skin, in the presence of UVB radiation.) Certain vitamin-like compounds that are recommended in the diet, such as carnitine, are thought useful for survival and health, but these are not "essential" dietary nutrients because the human body has some capacity to produce them from other compounds. Moreover, thousands of different phytochemicals have recently been discovered in food (particularly in fresh vegetables), which may have desirable properties including antioxidant activity (see below); however, experimental demonstration has been suggestive but inconclusive. Other essential nutrients that are not classified as vitamins include essential amino acids (see above), choline, essential fatty acids (see above), and the minerals discussed in the preceding section.
Vitamin deficiencies may result in disease conditions, including goitre, scurvy, osteoporosis, impaired immune system, disorders of cell metabolism, certain forms of cancer, symptoms of premature aging, and poor psychological health (including eating disorders), among many others. Excess levels of some vitamins are also dangerous to health (notably vitamin A).
Deficient or excess levels of minerals can also have serious health consequences.
Other nutrients.
In general, other micronutrients are more recent discoveries that have not yet been recognized as vitamins or as required. Phytochemicals may act as antioxidants, but not all phytochemicals are antioxidants.
Phytochemicals.
Phytochemicals are chemical compounds that occur naturally in plants (phyto means "plant" in Greek). In general, the term is used to refer to those chemicals that may have biological significance, for example antioxidants.
There is research interest in the health effects of phytochemicals, but to date there is no conclusive evidence. While many fruits and vegetables that happen to contain phytochemicals are thought to be components of a healthy diet, by comparison dietary supplements based on them have no proven health benefit.
Antioxidants.
As cellular metabolism/energy production requires oxygen, potentially damaging (e.g., mutation causing) compounds known as free radicals can form. Most of these are oxidizers (i.e., acceptors of electrons) and some react very strongly. For the continued normal cellular maintenance, growth, and division, these free radicals must be sufficiently neutralized by antioxidant compounds. Recently, some researchers suggested an interesting theory of evolution of dietary antioxidants. Some are produced by the human body with adequate precursors (glutathione, Vitamin C), and those the body cannot produce may only be obtained in the diet via direct sources (Vitamin C in humans, Vitamin A, Vitamin K) or produced by the body from other compounds (Beta-carotene converted to Vitamin A by the body, Vitamin D synthesized from cholesterol by sunlight). Phytochemicals ("Section Below") and their subgroup, polyphenols, make up the majority of antioxidants; about 4,000 are known. Different antioxidants are now known to function in a cooperative network. For example, Vitamin C can reactivate free radical-containing glutathione or Vitamin E by accepting the free radical itself. Some antioxidants are more effective than others at neutralizing different free radicals. Some cannot neutralize certain free radicals. Some cannot be present in certain areas of free radical development (Vitamin A is fat-soluble and protects fat areas, Vitamin C is water-soluble and protects those areas). When interacting with a free radical, some antioxidants produce a different free radical compound that is less dangerous or more dangerous than the previous compound. Having a variety of antioxidants allows any byproducts to be safely dealt with by more efficient antioxidants in neutralizing a free radical's butterfly effect.
Although initial studies suggested that antioxidant supplements might promote health, later large clinical trials did not detect any benefit and suggested instead that excess supplementation may be harmful.
Intestinal bacterial flora.
Animal intestines contain a large population of gut flora. In humans, the four dominant phyla are Firmicutes, Bacteroidetes, Actinobacteria, and Proteobacteria. They are essential to digestion and are also affected by food that is consumed. Bacteria in the gut perform many important functions for humans, including breaking down and aiding in the absorption of otherwise indigestible food; stimulating cell growth; repressing the growth of harmful bacteria, training the immune system to respond only to pathogens; producing vitamin B12; and defending against some infectious diseases.
Healthy diets.
Whole plant food diet.
Heart disease, cancer, obesity, and diabetes are commonly called "Western" diseases because these maladies were once rarely seen in developing countries. An international study in China found some regions had virtually no cancer or heart disease, while in other areas they reflected "up to a 100-fold increase" coincident with shifts from diets that were found to be entirely plant-based to heavily animal-based, respectively. In contrast, diseases of affluence like cancer and heart disease are common throughout the developed world, including the United States. Adjusted for age and exercise, large regional clusters of people in China rarely suffered from these "Western" diseases possibly because their diets are rich in vegetables, fruits, and whole grains, and have little dairy and meat products. Some studies show these to be, in high quantities, possible causes of some cancers. There are arguments for and against this controversial issue.
The United Healthcare/Pacificare nutrition guideline recommends a whole plant food diet, and recommends using protein only as a condiment with meals. A "National Geographic" cover article from November 2005, entitled "The Secrets of Living Longer", also recommends a whole plant food diet. The article is a lifestyle survey of three populations, Sardinians, Okinawans, and Adventists, who generally display longevity and "suffer a fraction of the diseases that commonly kill people in other parts of the developed world, and enjoy more healthy years of life." In sum, they offer three sets of 'best practices' to emulate. The rest is up to you. In common with all three groups is to "Eat fruits, vegetables, and whole grains."
The "National Geographic" article noted that an NIH funded study of 34,000 Seventh-day Adventists between 1976 and 1988 "...found that the Adventists' habit of consuming beans, soy milk, tomatoes, and other fruits lowered their risk of developing certain cancers. It also suggested that eating whole grain bread, drinking five glasses of water a day, and, most surprisingly, consuming four servings of nuts a week reduced their risk of heart disease."
The French "paradox".
The French paradox is the observation that the French suffer a relatively low incidence of coronary heart disease, despite having a diet relatively rich in saturated fats. A number of explanations have been suggested:
However, statistics collected by the World Health Organization from 1990–2000 show that the incidence of heart disease in France may have been underestimated and, in fact, may be similar to that of neighboring countries.
Animal nutrition.
Nutritional science investigates the metabolic and physiological responses of the body to diet. With advances in the fields of molecular biology, biochemistry, nutritional immunology, molecular medicine and genetics, the study of nutrition is increasingly concerned
with metabolism and metabolic pathways: the sequences of biochemical steps through which substances in living things change from one form to another.
Carnivore and herbivore diets are contrasting, with basic nitrogen and carbon proportions vary for their particular foods. "The nitrogen content of plant tissues averages about 2%, while in fungi, animals, and bacteria it averages about 5% to 10%." Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. All animals' diets must provide sufficient amounts of the basic building blocks they need, up to the point where their particular biology can synthesize the rest.
Animal tissue contains chemical compounds, such as water, carbohydrates (sugar, starch, and fiber), amino acids (in proteins), fatty acids (in lipids), and nucleic acids (DNA and RNA). These compounds in turn consist of elements such as carbon, hydrogen, oxygen, nitrogen, phosphorus, calcium, iron, zinc, magnesium, manganese, and so on. All of these chemical compounds and elements occur in various forms and combinations (e.g. hormones, vitamins, phospholipids, hydroxyapatite).
Animal tissue consists of elements and compounds ingested, digested, absorbed, and circulated through the bloodstream to feed the cells of the body. Except in the unborn fetus, the digestive system is the first system involved . Digestive juices break chemical bonds in ingested molecules, and modify their conformations and energy states. Though some molecules are absorbed into the bloodstream unchanged, digestive processes release them from the matrix of foods. Unabsorbed matter, along with some waste products of metabolism, is eliminated from the body in the feces.
Studies of nutritional status must take into account the state of the body before and after experiments, as well as the chemical composition of the whole diet and of all material excreted and eliminated from the body (in urine and feces). Comparing the food to the waste can help determine the specific compounds and elements absorbed and metabolized in the body. The effects of nutrients may only be discernible over an extended period, during which all food and waste must be analyzed. The number of variables involved in such experiments is high, making nutritional studies time-consuming and expensive, which explains why the science of animal nutrition is still slowly evolving.
In particular, the consumption of whole-plant foods slows digestion and allows better absorption, and a more favorable balance of essential nutrients per Calorie, resulting in better management of cell growth, maintenance, and mitosis (cell division), as well as better regulation of appetite and blood sugar . Regularly scheduled meals (every few hours) have also proven more wholesome than infrequent or haphazard ones.
Plant nutrition.
Plant nutrition is the study of the chemical elements that are necessary for plant growth. There are several principles that apply to plant nutrition. Some elements are directly involved in plant metabolism. However, this principle does not account for the so-called beneficial elements, whose presence, while not required, has clear positive effects on plant growth.
A nutrient that is able to limit plant growth according to Liebig's law of the minimum is considered an essential plant nutrient if the plant cannot complete its full life cycle without it. There are 16 essential plant soil nutrients, besides the three major elemental nutrients carbon and oxygen that are obtained by photosynthetic plants from carbon dioxide in air, and hydrogen, which is obtained from water.
Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Green plants obtain their carbohydrate supply from the carbon dioxide in the air by the process of photosynthesis. Carbon and oxygen are absorbed from the air, while other nutrients are absorbed from the soil. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen. The carbon dioxide molecules are used as the carbon source in photosynthesis.
Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.
Plant nutrition is a difficult subject to understand completely, partially because of the variation between different plants and even between different species or individuals of a given clone. Elements present at low levels may cause deficiency symptoms, and toxicity is possible at levels that are too high. Furthermore, deficiency of one element may present as symptoms of toxicity from another element, and vice versa.
Environmental Nutrition.
Research in the field of nutrition has greatly contributed in finding out the essential facts about how environmental depletion can lead to crucial nutrition-related health problems like contamination, spread of contagious diseases, malnutrition, etc. Moreover, environmental contamination due to discharge of agricultural as well as industrial chemicals like organocholrines, heavy metal, and radionucleotides may adversely affect the human and the ecosystem as a whole. As far as safety of the human health is concerned, then these environmental contaminants can reduce people's nutritional status and health. This could directly or indirectly cause drastic changes in their diet habits. Hence, food-based remedial as well as preventive strategies are essential to address global issues like hunger and malnutrition and to enable the susceptible people to adapt themselves to all these environmental as well as socio-economic alterations.
Advice and guidance.
U.S. Government policies.
In the US, dietitians are registered (RD) or licensed (LD) with the Commission for Dietetic Registration and the American Dietetic Association, and are only able to use the title "dietitian," as described by the business and professions codes of each respective state, when they have met specific educational and experiential prerequisites and passed a national registration or licensure examination, respectively. In California, registered dietitians must abide by the Anyone may call themselves a nutritionist, including unqualified dietitians, as this term is unregulated. Some states, such as the State of Florida, have begun to include the title "nutritionist" in state licensure requirements. Most governments provide guidance on nutrition, and some also impose mandatory disclosure/labeling requirements for processed food manufacturers and restaurants to assist consumers in complying with such guidance.
In the US, nutritional standards and recommendations are established jointly by the US Department of Agriculture and US Department of Health and Human Services. Dietary and physical activity guidelines from the USDA are presented in the concept of MyPlate, which superseded the food pyramid, which replaced the Four Food Groups. The Senate committee currently responsible for oversight of the USDA is the "Agriculture, Nutrition and Forestry Committee". Committee hearings are often televised on C-SPAN.
The U.S. Department of Health and Human Services provides a sample week-long menu that fulfills the nutritional recommendations of the government. Canada's Food Guide is another governmental recommendation.
Government programs.
Federal and state governmental organizations have been working on nutrition literacy interventions in non-primary health care settings to address the nutrition information problem in the U.S. Some programs include:
The Family Nutrition Program (FNP) is a free nutrition education program serving low-income adults around the U.S. This program is funded by the Food Nutrition Service’s (FNS) branch of the United States Department of Agriculture (USDA) usually through a local state academic institution that runs the program. The FNP has developed a series of tools to help families participating in the Food Stamp Program stretch their food dollar and form healthful eating habits including nutrition education.
Expanded Food and Nutrition Education Program (ENFEP) is a unique program that currently operates in all 50 states and in American Samoa, Guam, Micronesia, Northern Marianas, Puerto Rico, and the Virgin Islands. It is designed to assist limited-resource audiences in acquiring the knowledge, skills, attitudes, and changed behavior necessary for nutritionally sound diets, and to contribute to their personal development and the improvement of the total family diet and nutritional well-being.
An example of a state initiative to promote nutrition literacy is Smart Bodies, a public-private partnership between the state’s largest university system and largest health insurer, Louisiana State Agricultural Center and Blue Cross and Blue Shield of Louisiana Foundation. Launched in 2005, this program promotes lifelong healthful eating patterns and physically active lifestyles for children and their families. It is an interactive educational program designed to help prevent childhood obesity through classroom activities that teach children healthful eating habits and physical exercise.
Education.
Nutrition is taught in schools in many countries. In England and Wales, the Personal and Social Education and Food Technology curricula include nutrition, stressing the importance of a balanced diet and teaching how to read nutrition labels on packaging. In many schools, a Nutrition class will fall within the Family and Consumer Science or Health departments. In some American schools, students are required to take a certain number of FCS or Health related classes. Nutrition is offered at many schools, and, if it is not a class of its own, nutrition is included in other FCS or Health classes such as: Life Skills, Independent Living, Single Survival, Freshmen Connection, Health etc. In many Nutrition classes, students learn about the food groups, the food pyramid, Daily Recommended Allowances, calories, vitamins, minerals, malnutrition, physical activity, healthful food choices, portion sizes, and how to live a healthy life.
A 1985, US National Research Council report entitled "Nutrition Education in US Medical Schools" concluded that nutrition education in medical schools was inadequate. Only 20% of the schools surveyed taught nutrition as a separate, required course. A 2006 survey found that this number had risen to 30%.
Nutrition literacy.
At the time of this entry, we were not able to identify any specific nutrition literacy studies in the U.S. at a national level. However, the findings of the 2003 National Assessment of Adult Literacy (NAAL) provide a basis upon which to frame the nutrition literacy problem in the U.S. NAAL introduced the first ever measure of "the degree to which individuals have the capacity to obtain, process and understand basic health information and services needed to make appropriate health decisions" – an objective of Healthy People 2010 and of which nutrition literacy might be considered an important subset. On a scale of below basic, basic, intermediate and proficient, NAAL found 13 percent of adult Americans have proficient health literacy, 44% have intermediate literacy, 29 percent have basic literacy and 14 percent have below basic health literacy. The study found that health literacy increases with education and people living below the level of poverty have lower health literacy than those above it.
Another study examining the health and nutrition literacy status of residents of the lower Mississippi Delta found that 52 percent of participants had a high likelihood of limited literacy skills. While a precise comparison between the NAAL and Delta studies is difficult, primarily because of methodological differences, Zoellner et al. suggest that health literacy rates in the Mississippi Delta region are different from the U.S. general population and that they help establish the scope of the problem of health literacy among adults in the Delta region. For example, only 12 percent of study participants identified the My Pyramid graphic two years after it had been launched by the USDA. The study also found significant relationships between nutrition literacy and income level and nutrition literacy and educational attainment further delineating priorities for the region.
These statistics point to the complexities surrounding the lack of health/nutrition literacy and reveal the degree to which they are embedded in the social structure and interconnected with other problems. Among these problems are the lack of information about food choices, a lack of understanding of nutritional information and its application to individual circumstances, limited or difficult access to healthful foods, and a range of cultural influences and socioeconomic constraints such as low levels of education and high levels of poverty that decrease opportunities for healthful eating and living.
The links between low health literacy and poor health outcomes has been widely documented and there is evidence that some interventions to improve health literacy have produced successful results in the primary care setting. More must be done to further our understanding of nutrition literacy specific interventions in non-primary care settings in order to achieve better health outcomes.
Malnutrition.
Malnutrition refers to insufficient, excessive, or imbalanced consumption of nutrients by an organism. In developed countries, the diseases of malnutrition are most often associated with nutritional imbalances or excessive consumption. In developing countries, malnutrition is more likely to be caused by poor access to a range of nutritious foods or inadequate knowledge. In Mali the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Aga Khan Foundation, trained women's groups to make "equinut", a healthy and nutritional version of the traditional recipe "di-dèguè" (comprising peanut paste, honey and millet or rice flour). The aim was to boost nutrition and livelihoods by producing a product that women could make and sell, and which would be accepted by the local community because of its local heritage.
Although there are more organisms in the world who are malnourished due to insufficient consumption, increasingly more organisms suffer from excessive over-nutrition; a problem caused by an over abundance of sustenance coupled with the instinctual desire (by animals in particular) to consume all that it can.
Nutritionism is the view that excessive reliance on food science and the study of nutrition can lead to poor nutrition and to ill health. It was originally credited to Gyorgy Scrinis, and was popularized by Michael Pollan. Since nutrients are invisible, policy makers rely on nutrition experts to advise on food choices. Because science has an incomplete understanding of how food affects the human body, Pollan argues, nutritionism can be blamed for many of the health problems relating to diet in the Western World today.
Insufficient.
In general, "under-consumption" refers to the long-term consumption of insufficient sustenance in relation to the energy that an organism expends or expels, leading to poor health.
Excessive.
In general, "over-consumption" refers to the long-term consumption of excess sustenance in relation to the energy that an organism expends or expels, leading to poor health and, in animals, obesity. It can cause excessive hair loss, brittle nails, and irregular premenstrual cycles for females.
Unbalanced.
When too much of one or more nutrients is present in the diet to the exclusion of the proper amount of other nutrients, the diet is said to be unbalanced.
Mental agility.
Research indicates that improving the awareness of nutritious meal choices and establishing long-term habits of healthy eating have a positive effect on cognitive and spatial memory capacity, with potential to increase a student's ability to process and retain academic information.
Some organizations have begun working with teachers, policymakers, and managed foodservice contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university level institutions. Health and nutrition have been proven to have close links with overall educational success. Currently, less than 10% of American college students report that they eat the recommended five servings of fruit and vegetables daily. Better nutrition has been shown to have an impact on both cognitive and spatial memory performance; a study showed those with higher blood sugar levels performed better on certain memory tests. In another study, those who consumed yogurt performed better on thinking tasks when compared to those that consumed caffeine-free diet soda or confections. Nutritional deficiencies have been shown to have a negative effect on learning behavior in mice as far back as 1951.
There is limited research available that directly links a student's Grade Point Average (G.P.A.) to their overall nutritional health. Additional substantive data is needed to prove that overall intellectual health is closely linked to a person's diet, rather than just another correlation fallacy.
Mental disorders.
Nutritional supplement treatment may be appropriate for major depression, bipolar disorder, schizophrenia, and obsessive compulsive disorder, the four most common mental disorders in developed countries. Supplements that have been studied most for mood elevation and stabilization include eicosapentaenoic acid and docosahexaenoic acid (each of which an omega-3 fatty acid contained in fish oil but not in flaxseed oil), vitamin B12, folic acid, and inositol.
Cancer.
Cancer is now common in developing countries. According to a study by the International Agency for Research on Cancer, "In the developing world, cancers of the liver, stomach and esophagus were more common, often linked to consumption of carcinogenic preserved foods, such as smoked or salted food, and parasitic infections that attack organs." Lung cancer rates are rising rapidly in poorer nations because of increased use of tobacco. Developed countries "tended to have cancers linked to affluence or a 'Western lifestyle' — cancers of the colon, rectum, breast and prostate — that can be caused by obesity, lack of exercise, diet and age."
Metabolic syndrome.
Several lines of evidence indicate lifestyle-induced hyperinsulinemia and reduced insulin function (i.e., insulin resistance) as a decisive factor in many disease states. For example, hyperinsulinemia and insulin resistance are strongly linked to chronic inflammation, which in turn is strongly linked to a variety of adverse developments such as arterial microinjuries and clot formation (i.e., heart disease) and exaggerated cell division (i.e., cancer). Hyperinsulinemia and insulin resistance (the so-called metabolic syndrome) are characterized by a combination of abdominal obesity, elevated blood sugar, elevated blood pressure, elevated blood triglycerides, and reduced HDL cholesterol. The negative impact of hyperinsulinemia on prostaglandin PGE1/PGE2 balance may be significant.
The state of obesity clearly contributes to insulin resistance, which in turn can cause type 2 diabetes. Virtually all obese and most type 2 diabetic individuals have marked insulin resistance. Although the association between overweight and insulin resistance is clear, the exact (likely multifarious) causes of insulin resistance remain less clear. It is important to note that it has been demonstrated that appropriate exercise, more regular food intake, and reducing glycemic load (see below) all can reverse insulin resistance in overweight individuals (and thereby lower blood sugar levels in those with type 2 diabetes).
Obesity can unfavourably alter hormonal and metabolic status via resistance to the hormone leptin, and a vicious cycle may occur in which insulin/leptin resistance and obesity aggravate one another. The vicious cycle is putatively fuelled by continuously high insulin/leptin stimulation and fat storage, as a result of high intake of strongly insulin/leptin stimulating foods and energy. Both insulin and leptin normally function as satiety signals to the hypothalamus in the brain; however, insulin/leptin resistance may reduce this signal and therefore allow continued overfeeding despite large body fat stores. In addition, reduced leptin signalling to the brain may reduce leptin's normal effect to maintain an appropriately high metabolic rate.
There is a debate about how and to what extent different dietary factors— such as intake of processed carbohydrates, total protein, fat, and carbohydrate intake, intake of saturated and trans fatty acids, and low intake of vitamins/minerals—contribute to the development of insulin and leptin resistance. In any case, analogous to the way modern man-made pollution may possess the potential to overwhelm the environment's ability to maintain homeostasis, the recent explosive introduction of high glycemic index and processed foods into the human diet may possess the potential to overwhelm the body's ability to maintain homeostasis and health (as evidenced by the metabolic syndrome epidemic).
Hyponatremia.
Excess water intake, without replenishment of sodium and potassium salts, leads to hyponatremia, which can further lead to water intoxication at more dangerous levels. A well-publicized case occurred in 2007, when Jennifer Strange died while participating in a water-drinking contest. More usually, the condition occurs in long-distance endurance events (such as marathon or triathlon competition and training) and causes gradual mental dulling, headache, drowsiness, weakness, and confusion; extreme cases may result in coma, convulsions, and death. The primary damage comes from swelling of the brain, caused by increased osmosis as blood salinity decreases.
Effective fluid replacement techniques include water aid stations during running/cycling races, trainers providing water during team games, such as soccer, and devices such as Camel Baks, which can provide water for a person without making it too hard to drink the water.
Antinutrient.
Antinutrients are natural or synthetic compounds that interfere with the absorption of nutrients. Nutrition studies focus on antinutrients commonly found in food sources and beverages.
Sugar consumption in the United States
The relatively recent increased consumption of sugar has been linked to the rise of some afflictions such as diabetes, obesity, and more recently heart disease. Increased consumption of sugar has been tied to these three, among others. Obesity levels have more than doubled in the last 30 years among adults, going from 15% to 35% in the United States. Obesity and diet also happen to be high risk factors for diabetes. In the same time span that obesity doubled, diabetes numbers quadrupled in America. Increased weight, especially in the form of belly fat, and high sugar intake are also high risk factors for heart disease. Both sugar intake and fatty tissue increase the probability of elevated LDL cholesterol in the bloodstream. Elevated amounts of Low-density lipoprotein (LDL) cholesterol, is the primary factor in heart disease. In order to avoid all the dangers of sugar, moderate consumption is paramount.
Processed foods.
Since the Industrial Revolution some two hundred years ago, the food processing industry has invented many technologies that both help keep foods fresh longer and alter the fresh state of food as they appear in nature. Cooling is the primary technology used to maintain freshness, whereas many more technologies have been invented to allow foods to last longer without becoming spoiled. These latter technologies include pasteurisation, autoclavation, drying, salting, and separation of various components, all of which appearing to alter the original nutritional contents of food. Pasteurisation and autoclavation (heating techniques) have no doubt improved the safety of many common foods, preventing epidemics of bacterial infection. But some of the (new) food processing technologies have downfalls as well.
Modern separation techniques such as milling, centrifugation, and pressing have enabled concentration of particular components of food, yielding flour, oils, juices, and so on, and even separate fatty acids, amino acids, vitamins, and minerals. Inevitably, such large-scale concentration changes the nutritional content of food, saving certain nutrients while removing others. Heating techniques may also reduce food's content of many heat-labile nutrients such as certain vitamins and phytochemicals, and possibly other yet-to-be-discovered substances. Because of reduced nutritional value, processed foods are often 'enriched' or 'fortified' with some of the most critical nutrients (usually certain vitamins) that were lost during processing. Nonetheless, processed foods tend to have an inferior nutritional profile compared to whole, fresh foods, regarding content of both sugar and high GI starches, potassium/sodium, vitamins, fiber, and of intact, unoxidized (essential) fatty acids. In addition,
processed foods often contain potentially harmful substances such as oxidized fats and trans fatty acids.
A dramatic example of the effect of food processing on a population's health is the history of epidemics of beri-beri in people subsisting on polished rice. Removing the outer layer of rice by polishing it removes with it the essential vitamin thiamine, causing beri-beri. Another example is the development of scurvy among infants in the late 19th century in the United States. It turned out that the vast majority of sufferers were being fed milk that had been heat-treated (as suggested by Pasteur) to control bacterial disease. Pasteurisation was effective against bacteria, but it destroyed the vitamin C.
As mentioned, lifestyle- and obesity-related diseases are becoming increasingly prevalent all around the world. There is little doubt that the increasingly widespread application of some modern food processing technologies has contributed to this development. The food processing industry is a major part of modern economy, and as such it is influential in political decisions (e.g., nutritional recommendations, agricultural subsidising). In any known profit-driven economy, health considerations are hardly a priority; effective production of cheap foods with a long shelf-life is more the trend. In general, whole, fresh foods have a relatively short shelf-life and are less profitable to produce and sell than are more processed foods. Thus, the consumer is left with the choice between more expensive, but nutritionally superior, whole, fresh foods, and cheap, usually nutritionally inferior, processed foods. Because processed foods are often cheaper, more convenient (in both purchasing, storage, and preparation), and more available, the consumption of nutritionally inferior foods has been increasing throughout the world along with many nutrition-related health complications.

</doc>
<doc id="21526" url="https://en.wikipedia.org/wiki?curid=21526" title="November 22">
November 22

In the ancient astronomy, it is the cusp day between Scorpio and Sagittarius. In some years it is Sagittarius, but others Scorpio. It depends on solar term Xiaoxue (Minor snow).

</doc>
<doc id="21527" url="https://en.wikipedia.org/wiki?curid=21527" title="Number theory">
Number theory

Number theory or arithmetic is a branch of pure mathematics devoted primarily to the study of the integers. It is sometimes called "The Queen of Mathematics" because of its foundational place in the discipline. Number theorists study prime numbers as well as the properties of objects made out of integers (e.g., rational numbers) or defined as generalizations of the integers (e.g., algebraic integers).
Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (e.g., the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, e.g., as approximated by the latter (Diophantine approximation).
The older term for number theory is "arithmetic". By the early twentieth century, it had been superseded by "number theory". (The word "arithmetic" is used by the general public to mean "elementary calculations"; it has also acquired other meanings in mathematical logic, as in "Peano arithmetic", and computer science, as in "floating point arithmetic".) The use of the term "arithmetic" for "number theory" regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, "arithmetical" is preferred as an adjective to "number-theoretic".
History.
Origins.
Dawn of arithmetic.
The first historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BCE) contains a list of "Pythagorean triples", i.e., integers formula_1 such that formula_2.
The triples are too many and too large to have been obtained by brute force. The heading over the first column reads: "The "takiltum" of the diagonal which has been subtracted such that the width..."
The table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity
formula_3,
which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by formula_4, presumably for actual use as a "table", i.e., with a view to applications.
It is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly flowered only later. It has been suggested instead that the table was a source of numerical examples for school problems.
While Babylonian number theory—or what survives of Babylonian mathematics that can be called thus—consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of "algebra") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.
Euclid IX 21—34 is very probably Pythagorean; it is very simple material ("odd times even is even", "if an odd number measures [= divides] an even number, then it also measures [= divides] half of it"), but it is all that is needed to prove that formula_5
is irrational. Pythagorean mystics gave great importance to the odd and the even.
The discovery that formula_5 is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between "numbers" (integers and the rationals—the subjects of arithmetic), on the one hand, and "lengths" and "proportions" (which we would identify with real numbers, whether rational or not), on the other hand.
The Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums
of triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).
We know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in both. The Chinese remainder theorem appears as an exercise in Sun Zi's "Suan Ching", also known as "The Mathematical Classic of Sun Zi" (3rd, 4th or 5th century CE.) (There is one important step glossed over in Sun Zi's solution: it is the problem that was later solved by Āryabhaṭa's Kuṭṭaka – see below.)
There is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have
led nowhere. Like the Pythagoreans' perfect numbers, magic squares have passed from superstition into recreation.
Classical Greece and the early Hellenistic period.
Aside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, "Plato" and "Euclid", respectively.
Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By "arithmetic" he meant, in part, theorising on number, rather than what "arithmetic" or "number theory" have come to mean.) It is through one of Plato's dialogues—namely, "Theaetetus"—that we know that Theodorus had proven that formula_7 are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)
Euclid devoted part of his "Elements" to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; "Elements", Prop. VII.2) and the first known proof of the infinitude of primes ("Elements", Prop. IX.20).
In 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as
Archimedes' cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.
Diophantus.
Very little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's "Arithmetica" survive in the original Greek; four more books survive in an Arabic translation. The "Arithmetica" is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form formula_8 or formula_9. Thus, nowadays, we speak of "Diophantine equations" when we speak of polynomial equations to which rational or integer solutions must be found.
One may say that Diophantus was studying rational points — i.e., points whose coordinates are rational — on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)
formula_10, his aim was to find (in essence) three rational functions formula_11 such that, for all values of formula_12 and formula_13, setting
formula_14 for formula_15 gives a solution to formula_16
Diophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry
While Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).
Āryabhaṭa, Brahmagupta, Bhāskara.
While Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid's Elements reached India before the 18th century.
Āryabhaṭa (476–550 CE) showed that pairs of simultaneous congruences formula_17, formula_18 could be solved by a method he called "kuṭṭaka", or "pulveriser"; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. Āryabhaṭa seems to have had in mind applications to astronomical calculations.
Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations—in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or "cyclic method") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II's Bīja-gaṇita (twelfth century).
Indian mathematics remained largely unknown in Europe until the late eighteenth century; Brahmagupta and Bhāskara's work was translated into English in 1817 by Henry Colebrooke.
Arithmetic in the Islamic golden age.
In the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the "Sindhind",
which may or may not be Brahmagupta's Brāhmasphuţasiddhānta).
Diophantus's main work, the "Arithmetica", was translated into Arabic by Qusta ibn Luqa (820–912).
Part of the treatise "al-Fakhri" (by al-Karajī, 953 – ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporary Ibn al-Haytham knew what would later be called Wilson's theorem.
Western Europe in the Middle Ages.
Other than a treatise on squares in arithmetic progression by Fibonacci — who lived and studied in north Africa and Constantinople during his formative years, ca. 1175–1200 — no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus's "Arithmetica" (Bachet, 1621, following a first attempt by Xylander, 1575).
Early modern number theory.
Fermat.
Pierre de Fermat (1601–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. He wrote down nearly no proofs in number theory; he had no models in the area. He did make repeated use of mathematical induction, introducing the method of infinite descent.
One of Fermat's first interests was perfect numbers (which appear in Euclid, "Elements" IX) and amicable numbers; this led him to work on integer divisors, which were from the beginning among the subjects of the
correspondence (1636 onwards) that put him in touch with the mathematical community of the day. He had already studied Bachet's edition of Diophantus carefully; by 1643, his interests had shifted largely to Diophantine problems and sums of squares (also treated by Diophantus).
Fermat's achievements in arithmetic include:
Fermat's claim ("Fermat's last theorem") to have shown there are no solutions to
formula_25 for all formula_26 (the only known proof of which is beyond his methods) appears only in his annotations on the margin of his copy of Diophantus; he never claimed this to others and thus would have had no need to retract it if he found any mistake in his supposed proof.
Euler.
The interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat's work on the subject. This has been called the "rebirth" of modern number theory, after Fermat's relative lack of success in getting his contemporaries' attention for the subject. Euler's work on number theory includes the following:
Lagrange, Legendre and Gauss.
Joseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations - for instance, the four-square theorem and the basic theory of the misnamed "Pell's equation" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to formula_31) — defining their equivalence relation, showing how to put them in reduced form, etc.
Adrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also
conjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation formula_32 and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove "Fermat's last theorem" for formula_33 (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).
In his "Disquisitiones Arithmeticae" (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the "Disquisitiones" established a link between roots of unity and number theory:
The theory of the division of the circle...which is treated in sec. 7 does not belong
by itself to arithmetic, but its principles can only be drawn from higher arithmetic.
In this way, Gauss arguably made a first foray towards both Évariste Galois's work and algebraic number theory.
Maturity and division into subfields.
Starting early in the nineteenth century, the following developments gradually took place:
Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837), whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually
goes back to Euler (1730s), who used formal power series and non-rigorous (or implicit) limiting arguments. The use of "complex" analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).
The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.
Main subdivisions.
Elementary tools.
The term "elementary" generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (e.g. Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an "elementary" proof may be longer and more difficult for most readers than a non-elementary one.
Number theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.
Analytic number theory.
"Analytic number theory" may be defined
Some subjects generally considered to be part of analytic number theory, e.g., sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.
The following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann Hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.
One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.
Algebraic number theory.
An "algebraic number" is any complex number that is a solution to some polynomial equation formula_34 with rational coefficients; for example, every solution formula_35 of formula_36 (say) is an algebraic number. Fields of algebraic numbers are also called "algebraic number fields", or shortly "number fields". Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.
It could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in "Disquisitiones arithmeticae" can be restated in terms of ideals and
norms in quadratic fields. (A "quadratic field" consists of all
numbers of the form formula_37, where
formula_38 and formula_39 are rational numbers and formula_40
is a fixed rational number whose square root is not rational.)
For that matter, the 11th-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.
The grounds of the subject as we know it were set in the late nineteenth century, when "ideal numbers", the "theory of ideals" and "valuation theory" were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals
and formula_41, the number formula_42 can be factorised both as formula_43 and
formula_44; all of formula_45, formula_46, formula_47 and
formula_48
are irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws,i.e., generalisations of quadratic reciprocity.
Number fields are often studied as extensions of smaller number fields: a field "L" is said to be an "extension" of a field "K" if "L" contains "K".
Classifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions "L" of "K" such that the Galois group Gal("L"/"K") of "L" over "K" is an abelian group—are relatively well understood.
Their classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900—1950.
An example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.
Diophantine geometry.
The central problem of "Diophantine geometry" is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.
For example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in "n"-dimensional space. In Diophantine geometry, one asks whether there are any "rational points" (points all of whose coordinates are rationals) or
"integral points" (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is: are there finitely
or infinitely many rational points on a given curve (or surface)? What about integer points?
An example here may be helpful. Consider the Pythagorean equation formula_49;
we would like to study its rational solutions, i.e., its solutions
formula_50 such that
"x" and "y" are both rational. This is the same as asking for all integer solutions
to formula_51; any solution to the latter equation gives
us a solution formula_52, formula_53 to the former. It is also the
same as asking for all points with rational coordinates on the curve
described by formula_54. (This curve happens to be a circle of radius 1 around the origin.)
[[Image:ECClines-3.svg|right|thumb|300px|Two examples of an elliptic curve, i.e., a curve
of genus 1 having at least one rational point. (Either graph can be seen as a slice of a torus in four-dimensional space.)]]
The rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve—that is, rational or integer solutions to an equation formula_55, where formula_56 is a polynomial in two variables—turns out to depend crucially on the "genus" of the curve. The "genus" can be defined as follows: allow the variables in formula_55 to be complex numbers; then formula_55 defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, i.e., four dimensions). Count
the number of (doughnut) holes in the surface; call this number the "genus" of formula_55. Other geometrical notions turn out to be just as crucial.
There is also the closely linked area of Diophantine approximations: given a number formula_35, how well can it be approximated by rationals? (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call formula_61 (with formula_62) a good approximation to formula_35 if formula_64, where formula_65 is large.) This question is of special interest if formula_35 is an algebraic number. If formula_35 cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be crucial both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that Pi and e have been shown to be transcendental.
Diophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. "Arithmetic geometry", on the other hand, is a contemporary term
for much the same domain as that covered by the term "Diophantine geometry". The term "arithmetic geometry" is arguably used
most often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings' theorem) rather than to techniques in Diophantine approximations.
Recent approaches and subfields.
The areas below date as such from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.
Probabilistic number theory.
Take a number at random between one and a million. How likely is it to be prime? This is just another way of asking how many primes there are between one and a million. Further: how many prime divisors will it have, on average? How many divisors will it have altogether, and with what likelihood? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?
Much of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.
It is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than formula_68 must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.
At times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér's conjecture.
Arithmetic combinatorics.
Let "A" be a set of "N" integers. Consider the set "A" + "A" = { "m" + "n" | "m", "n" ∈ "A" } consisting of all sums of two elements of "A". Is "A + A" much larger than "A"? Barely larger? If "A + A" is barely larger than "A", must "A" have plenty of arithmetic structure, for example, does "A" resemble an arithmetic progression?
If we begin from a fairly "thick" infinite set formula_69, does it contain many elements in arithmetic progression: formula_38,
formula_71, formula_72, formula_73, formula_74 , formula_75, say? Should it be possible to write large integers as sums of elements of formula_69?
These questions are characteristic of "arithmetic combinatorics". This is a presently coalescing field; it subsumes "additive number theory" (which concerns itself with certain very specific sets formula_69 of arithmetic significance, such as the primes or the squares) and, arguably, some of the "geometry of numbers",
together with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term "additive combinatorics" is also used; however, the sets formula_69 being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of formula_79 and formula_69·formula_69 may be
compared.
Computations in number theory.
While the word "algorithm" goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.
An interesting early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in "Elements", together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation formula_82,
or, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (5th–6th century CE) as an algorithm called
"kuṭṭaka" ("pulveriser"), without a proof of correctness.
There are two main questions: "can we compute this?" and "can we compute it rapidly?". Anybody can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.
The difficulty of a computation can be useful: modern protocols for encrypting messages (e.g., RSA) depend on functions that are known to all, but whose inverses (a) are known only to a chosen few, and (b) would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.
On a different note — some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove, of course, that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)
Applications.
The number-theorist Leonard Dickson (1874-1954) said "Thank God that number theory is unsullied by any application". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said "...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations".
Elementary number theory is taught in discrete mathematics courses for computer scientists; and, on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.
Literature.
Two of the most popular introductions to the subject are:
Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods.
Vinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:
Popular choices for a second textbook include:
Prizes.
The American Mathematical Society awards the "Cole Prize in Number Theory". Moreover number theory is one of the three mathematical subdisciplines rewarded by the "Fermat Prize".

</doc>
<doc id="21530" url="https://en.wikipedia.org/wiki?curid=21530" title="Nitroglycerin">
Nitroglycerin

Nitroglycerin (NG), also known as nitroglycerine, trinitroglycerin (TNG), trinitroglycerine, nitro, glyceryl trinitrate (GTN), or 1,2,3-trinitroxypropane, is a heavy, colorless, oily, explosive liquid most commonly produced by nitrating glycerol with white fuming nitric acid under conditions appropriate to the formation of the nitric acid ester. Chemically, the substance is an organic nitrate compound rather than a nitro compound, yet the traditional name is often retained. Invented in 1847, nitroglycerin has been used as an active ingredient in the manufacture of explosives, mostly dynamite, and as such it is employed in the construction, demolition, and mining industries. Since the 1880s, it has been used by the military as an active ingredient, and a gelatinizer for nitrocellulose, in some solid propellants, such as cordite and ballistite.
Nitroglycerin is also a major component in double-based smokeless gunpowders used by reloaders. Combined with nitrocellulose, there are hundreds of (powder) combinations used by rifle, pistol, and shotgun reloaders.
For over 130 years, nitroglycerin has been used medically as a potent vasodilator to treat heart conditions, such as angina pectoris and chronic heart failure. Though it was previously known that these beneficial effects are due to nitroglycerin being converted to nitric oxide, a potent vasodilator, it was not until 2002 that the enzyme for this conversion was discovered to be mitochondrial aldehyde dehydrogenase. Nitroglycerin is available in sublingual tablets, sprays, and patches. Other potential suggested uses include adjunct therapy in prostate cancer.
History.
Nitroglycerin was the first practical explosive produced that was stronger than black powder. It was first synthesized by the Italian chemist Ascanio Sobrero in 1847, working under Théophile-Jules Pelouze at the University of Turin. Sobrero initially called his discovery "pyroglycerine" and warned vigorously against its use as an explosive.
Nitroglycerin was later adopted as a commercially useful explosive by Alfred Nobel, who experimented with safer ways to handle the dangerous compound after his younger brother, Emil Oskar Nobel, and several factory workers were killed in an explosion at the Nobels' armaments factory in 1864 in Heleneborg, Sweden.
One year later, Nobel founded Alfred Nobel & Company in Germany and built an isolated factory in the Krümmel hills of Geesthacht near Hamburg. This business exported a liquid combination of nitroglycerin and gunpowder called "Blasting Oil", but this was extremely unstable and difficult to handle, as evidenced in numerous catastrophes. The buildings of the Krümmel factory were destroyed twice.
In April 1866, three crates of nitroglycerin were shipped to California for the Central Pacific Railroad, which planned to experiment with it as a blasting explosive to expedite the construction of the -long Summit Tunnel through the Sierra Nevada Mountains. One of the crates exploded, destroying a Wells Fargo company office in San Francisco and killing 15 people. This led to a complete ban on the transportation of liquid nitroglycerin in California. The on-site manufacture of nitroglycerin was thus required for the remaining hard-rock drilling and blasting required for the completion of the First Transcontinental Railroad in North America.
Liquid nitroglycerin was widely banned elsewhere as well, and these legal restrictions led to Alfred Nobel and his company's developing dynamite in 1867. This was made by mixing nitroglycerin with diatomaceous earth (""kieselgur"" in German) found in the Krümmel hills. Similar mixtures, such as "dualine" (1867), "lithofracteur" (1869), and "gelignite" (1875), were formed by mixing nitroglycerin with other inert absorbents, and many combinations were tried by other companies in attempts to get around Nobel's tightly held patents for dynamite.
Dynamite mixtures containing nitrocellulose, which increases the viscosity of the mix, are commonly known as "gelatins".
Following the discovery that amyl nitrite helped alleviate chest pain, Dr. William Murrell experimented with the use of nitroglycerin to alleviate angina pectoris and to reduce the blood pressure. He began treating his patients with small diluted doses of nitroglycerin in 1878, and this treatment was soon adopted into widespread use after Murrell published his results in the journal "The Lancet" in 1879. A few months before his death in 1896, Alfred Nobel was prescribed nitroglycerine for this heart condition, writing to a friend: "Isn't it the irony of fate that I have been prescribed nitro-glycerin, to be taken internally! They call it Trinitrin, so as not to scare the chemist and the public." The medical establishment also used the name "glyceryl trinitrate" for the same reason.
Wartime production rates.
Large quantities of nitroglycerin were manufactured during World War I and World War II for use as military propellants and in military engineering work. During World War I, HM Factory, Gretna, the largest propellant factory in Britain, produced about 800 tonne of cordite RDB per week. This amount took at least 336 tons of nitroglycerin per week (assuming no losses in production). The Royal Navy had its own factory at the Royal Navy Cordite Factory, Holton Heath in Dorset, England. A large cordite factory was also built in Canada during World War I. The Canadian Explosives Limited cordite factory at Nobel, Ontario, was designed to produce of cordite per month, requiring about 286 tonnes of nitroglycerin per month.
Instability and desensitization.
In its pure form, nitroglycerin is a contact explosive, with physical shock causing it to explode, and it degrades over time to even more unstable forms. This makes nitroglycerin highly dangerous to transport or use. In its undiluted form, it is one of the world's most powerful explosives, comparable to the more recently developed RDX and PETN.
Early in its history, it was discovered that liquid nitroglycerin can be "desensitized" by cooling it to about . At this temperature nitroglycerin freezes, contracting upon solidification. Thawing it out can be extremely sensitizing, especially if impurities are present or the warming is too rapid. It is possible to chemically "desensitize" nitroglycerin to a point where it can be considered approximately as "safe" as modern high explosives, such as by the addition of approximately 10% to 30% ethanol, acetone, or dinitrotoluene. (The percentage varies with the desensitizing agent used.) Desensitization requires extra effort to reconstitute the "pure" product. Failing this, it must be assumed that desensitized nitroglycerin is substantially more difficult to detonate, possibly rendering it useless as an explosive for practical application.
A serious problem in the use of nitroglycerin results from its high freezing point . Solid nitroglycerin is much less sensitive to shock than the liquid, a feature that is common in explosives. In the past, nitroglycerin was often shipped in the frozen state, but this resulted in a high number of accidents during the thawing process just before its use. This disadvantage is overcome by using mixtures of nitroglycerin with other polynitrates. For example, a mixture of nitroglycerin and ethylene glycol dinitrate freezes at .
Detonation.
Nitroglycerin and any diluents can certainly deflagrate, i.e., burn. The explosive power of nitroglycerin derives from detonation: energy from the initial decomposition causes a strong pressure wave that detonates the surrounding fuel. This is a self-sustained shock wave that propagates through the explosive medium at 30 times the speed of sound as a near-instantaneous pressure-induced decomposition of the fuel into a white-hot gas. Detonation of nitroglycerin generates gases that would occupy more than 1,200 times the original volume at ordinary room temperature and pressure. The heat liberated raises the temperature to about . This is entirely different from deflagration, which depends solely upon available fuel regardless of pressure or shock. The decomposition results in much higher ratio of energy to gas moles released compared to other explosives, making it one of the hottest detonating high explosives.
Manufacturing.
Nitroglycerin can be produced by acid catalyzed nitration of glycerol (glycerine).
The industrial manufacturing process often reacts glycerol with a nearly 1:1 mixture of concentrated sulfuric acid and concentrated nitric acid. This can be produced by mixing white fuming nitric acid—a quite expensive pure nitric acid in which the oxides of nitrogen have been removed, as opposed to red fuming nitric acid, which contains nitrogen oxides—and concentrated sulfuric acid. More often, this mixture is attained by the cheaper method of mixing fuming sulfuric acid, also known as oleum—sulfuric acid containing excess sulfur trioxide—and azeotropic nitric acid (consisting of about 70 percent nitric acid, with the rest being water).
The sulfuric acid produces protonated nitric acid species, which are attacked by glycerol's nucleophilic oxygen atoms. The nitro group is thus added as an ester C-O-NO2 and water is produced. This is different from an electrophilic aromatic substitution reaction in which nitronium ions are the electrophile.
The addition of glycerol results in an exothermic reaction (i.e., heat is produced), as usual for mixed-acid nitrations. However, if the mixture becomes too hot, it results in "runaway", a state of accelerated nitration accompanied by the destructive oxidation of organic materials by the hot nitric acid and the release of poisonous nitrogen dioxide gas at high risk of an explosion. Thus, the glycerin mixture is added slowly to the reaction vessel containing the mixed acid (not acid to glycerin). The nitrator is cooled with cold water or some other coolant mixture and maintained throughout the glycerin addition at about , much below which the esterification occurs too slowly to be useful. The nitrator vessel, often constructed of iron or lead and generally stirred with compressed air, has an emergency trap door at its base, which hangs over a large pool of very cold water and into which the whole reaction mixture (called the charge) can be dumped to prevent an explosion, a process referred to as drowning. If the temperature of the charge exceeds about (actual value varying by country) or brown fumes are seen in the nitrator's vent, then it is immediately drowned.
Use as an explosive and a propellant.
The main use of nitroglycerin, by tonnage, is in explosives such as dynamite and in propellants.
Nitroglycerin is an oily liquid that may explode when subjected to heat, shock or flame. It is dangerously sensitive and dropping or bumping a container may cause it to explode.
Alfred Nobel developed the use of nitroglycerin as a blasting explosive by mixing the nitroglycerin with inert absorbents, particularly ""kieselguhr,"" or diatomaceous earth. He named this explosive dynamite and patented it in 1867. It was supplied ready for use in the form of sticks, individually wrapped in greased waterproof paper. Dynamite and similar explosives were widely adopted for civil engineering tasks, such as in drilling highway and railroad tunnels, for mining, for clearing farmland of stumps, in quarrying, and in demolition work. Likewise, military engineers have used dynamite for construction and demolition work.
Nitroglycerin was also used as an ingredient in military propellants for use in firearms.
Nitroglycerin has been used in conjunction with hydraulic fracturing, a process used to recover oil and gas from shale formations. The technique involves displacing and detonating nitroglycerin in natural or hydraulically induced fracture systems, or displacing and detonating nitroglycerin in hydraulically induced fractures followed by wellbore shots using pelletized TNT.
Nitroglycerin has an advantage over some other high explosives, that on detonation it produces practically no visible smoke. Therefore, it is useful as an ingredient in the formulation of various kinds of "smokeless powder".
Its sensitivity has limited the usefulness of nitroglycerin as a military explosive, and less sensitive explosives such as TNT, RDX, and HMX have largely replaced it in munitions. It remains important in military engineering, and combat engineers still use dynamite.
Alfred Nobel then developed ballistite, by combining nitroglycerin and guncotton. He patented it in 1887. Ballistite was adopted by a number of European governments, as a military propellant. Italy was the first to adopt it. The British Government and the Commonwealth governments adopted cordite instead, which had been developed by Sir Frederick Abel and Sir James Dewar of the United Kingdom in 1889. The original Cordite Mk I consisted of 58% nitroglycerin, 37% guncotton, and 5.0% petroleum jelly. Ballistite and cordite were both manufactured in the forms of "cords".
Smokeless powders were originally developed using nitrocellulose as the sole explosive ingredient. Therefore, they were known as "single-base" propellants. A range of smokeless powders that contain both nitrocellulose and nitroglycerin, known as "double-base" propellants, were also developed. Smokeless powders were originally supplied only for military use, but they were also soon developed for civilian use and were quickly adopted for sports. Some are known as sporting powders. "Triple-base" propellants contain nitrocellulose, nitroglycerin, and nitroguanidine, but are reserved mainly for extremely high-caliber ammunition rounds such as those used in tank cannons and naval artillery.
Blasting gelatin, also known as "gelignite", was invented by Nobel in 1875, using nitroglycerin, wood pulp, and sodium or potassium nitrates. This was an early low-cost, flexible explosive.
Medical use.
Nitroglycerin belongs to a group of drugs called nitrates, which includes many other nitrates like isosorbide dinitrate (Isordil) and isosorbide mononitrate (Imdur, Ismo, Monoket). These agents all exert their effect by being converted to nitric oxide in the body by mitochondrial aldehyde dehydrogenase, and nitric oxide is a potent natural vasodilator.
In medicine, nitroglycerin is used as a medicine for angina pectoris, a painful symptom of ischemic heart disease caused by inadequate flow of blood and oxygen to the heart. Nitroglycerin corrects the imbalance between the flow of oxygen and blood to the heart. The principal action of nitroglycerin is vasodilation (widening of the blood vessels). At low doses, nitroglycerin will dilate veins more than arteries, thereby reducing preload; this is thought to be its primary mechanism of action. But at higher doses, it also dilates arteries, thereby reducing afterload. It is also a potent antihypertensive agent. In cardiac treatment, the lowering of pressure in the arteries reduces the pressure against which the heart must pump, thereby decreasing afterload. Dilating the veins decreases cardiac preload and lowers the oxygen requirement of the heart whilst at the same time reducing ventricular transmural pressure thereby improving coronary blood flow. Improved myocardial oxygen demand vs oxygen delivery ratio leads to the following therapeutic effects during episodes of angina pectoris: subsiding of chest pain, decrease of blood pressure, increase of heart rate, and orthostatic hypotension. Patients experiencing angina when doing certain physical activities can often prevent symptoms by taking nitroglycerin 5 to 10 minutes before the activity.
Nitroglycerin is available in tablets, ointment, solution for intravenous use, transdermal patches, or sprays administered sublingually. Some forms of nitroglycerin last much longer in the body than others. It has been shown that continuous exposure to nitrates can cause the body to stop responding normally to this medicine. Experts recommend that the patches be removed at night, allowing the body a few hours to restore its responsiveness to nitrates. Shorter-acting preparations can be used several times a day with less risk of the body's getting used to this drug. Nitroglycerin was first used by William Murrell to treat angina attacks in 1878, with the discovery published that same year.
Industrial exposure.
Infrequent exposure to high doses of nitroglycerin can cause severe headaches known as "NG head" or "bang head". These headaches can be severe enough to incapacitate some people; however, humans develop a tolerance to and dependence on nitroglycerin after long-term exposure. Withdrawal can (rarely) be fatal; withdrawal symptoms include chest pain and heart problems and if unacceptable may be treated with re-exposure to nitroglycerin or other suitable organic nitrates.
For workers in nitroglycerin (NTG) manufacturing facilities, the effects of withdrawal sometimes include "Sunday Heart Attacks" in those experiencing regular nitroglycerin exposure in the workplace, leading to the development of tolerance for the vasodilating effects. Over the weekend, the workers lose the tolerance and, when they are re-exposed on Monday, the drastic vasodilation produces a fast heart rate, dizziness, and a headache, this is referred to as "Monday Disease."
People can be exposed to nitroglycerin in the workplace by breathing it in, skin absorption, swallowing it, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for nitroglycerin exposure in the workplace as 0.2 ppm (2 mg/m3) skin exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m2 skin exposure over an 8-hour workday. At levels of 75 mg/m3, nitroglycerin is immediately dangerous to life and health.

</doc>
<doc id="21533" url="https://en.wikipedia.org/wiki?curid=21533" title="Navy">
Navy

A navy or maritime force is a fleet of waterborne military vessels (watercraft) and its associated naval aviation, both sea-based and land-based. It is the branch of a nation's armed forces principally designated for naval and amphibious warfare; namely, lake-borne, riverine, littoral, or ocean-borne combat operations and related functions. It includes anything conducted by surface ships, amphibious ships, submarines, and seaborne aviation, as well as ancillary support, communications, training, and other fields; recent developments have included space-related operations. The strategic offensive role of a navy is projection of force into areas beyond a country's shores (for example, to protect sea-lanes, ferry troops, or attack other navies, ports, or shore installations). The strategic defensive purpose of a navy is to frustrate seaborne projection-of-force by enemies. The strategic task of the navy also may incorporate nuclear deterrence by use of Submarine-launched ballistic missiles. Naval operations can be broadly divided between riverine and littoral applications (brown-water navy), open-ocean applications (blue-water navy), and something in between (green-water navy), although these distinctions are more about strategic scope than tactical or operational division.
In most nations, the term "naval", as opposed to "navy", is interpreted as encompassing all maritime military forces, e.g., navy, marine / marine corps, and coast guard forces.
Etymology and meanings.
First attested in English in the early 14th century, the word "navy" came via Old French "navie", "fleet of ships", from the Latin "navigium", "a vessel, a ship, bark, boat", from "navis", "ship". The word "naval" came from Latin "navalis", "pertaining to ship"; cf. Greek ("naus"), "ship", ("nautes"), "seaman, sailor". The earliest attested form of the word is in the Mycenaean Greek compound word , "na-u-do-mo" (*), "shipbuilders", written in Linear B syllabic script.
The word formerly denoted fleets of both commercial and military nature. In modern usage "navy" used alone always denotes a military fleet, although the term "merchant navy" for a commercial fleet still incorporates the non-military word sense. This overlap in word senses between commercial and military fleets grew out of the inherently dual-use nature of fleets; centuries ago, nationality was a trait that unified a fleet across both civilian and military uses. Although nationality of commercial vessels has little importance in peacetime trade other than for tax avoidance, it can have greater meaning during wartime, when supply chains become matters of patriotic attack and defense, and when in some cases private vessels are even temporarily converted to military vessels. The latter was especially important, and common, before 20th-century military technology existed, when merely adding artillery and naval infantry to any sailing vessel could render it fully as martial as any military-owned vessel. Such privateering has been rendered obsolete in blue-water strategy since modern missile and aircraft systems grew to leapfrog over artillery and infantry in many respects; but privateering nevertheless remains potentially relevant in littoral warfare of a limited and asymmetric nature.
History.
Naval warfare developed when humans first fought from water-borne vessels. Prior to the introduction of the cannon and ships with sufficient capacity to carry the large guns, navy warfare primarily involved ramming and boarding actions. In the time of ancient Greece and the Roman Empire, naval warfare centered on long, narrow vessels powered by banks of oarsmen (such as triremes and quinqueremes) designed to ram and sink enemy vessels or come alongside the enemy vessel so its occupants could be attacked hand-to-hand. Naval warfare continued in this vein through the Middle Ages until the cannon became commonplace and capable of being reloaded quickly enough to be reused in the same battle. The Chola Dynasty of medieval India was known as one of the greatest naval powers of its time from 300 BC to 1279 AD. The Chola Navy, Chola kadarpadai comprised the naval forces of the Chola Empire along with several other Naval-arms of the country. The Chola navy played a vital role in the expansion of the Chola Tamil kingdom, including the conquest of the Sri Lanka islands, Kadaaram (Present day Burma), Sri Vijaya (present day Southeast Asia), the spread of Hinduism, Tamil architecture and Tamil culture to Southeast Asia and in curbing the piracy in Southeast Asia in 900 CE. In ancient China, large naval battles were known since the Qin Dynasty ("also see" Battle of Red Cliffs, 208), employing the war junk during the Han Dynasty. However, China's first official standing navy was not established until the Southern Song dynasty in the 12th century, a time when gunpowder was a revolutionary new application to warfare.
The mass and deck space required to carry a large number of cannon made oar-based propulsion impossible, and ships came to rely primarily on sails. Warships were designed to carry increasing numbers of cannon and naval tactics evolved to bring a ship's firepower to bear in a broadside, with ships-of-the-line arranged in a line of battle.
The development of large capacity, sail-powered ships carrying cannon led to a rapid expansion of European navies, especially the Spanish and Portuguese navies which dominated in the 16th and early 17th centuries, and helped propel the age of exploration and colonialism. The repulsion of the Spanish Armada (1588) by the English fleet revolutionized naval warfare by the success of a guns-only strategy and caused a major overhaul of the Spanish Navy, partly along English lines, which resulted in even greater dominance by the Spanish. From the beginning of the 17th century the Dutch cannibalized the Portuguese Empire in the East and, with the immense wealth gained, challenged Spanish hegemony at sea. From the 1620s, Dutch raiders seriously troubled Spanish shipping and, after a number of battles which went both ways, the Dutch Navy finally broke the long dominance of the Spanish Navy in the Battle of the Downs (1639).
England emerged as a major naval power in the mid-17th century in the first Anglo-Dutch war with a technical victory. Successive decisive Dutch victories in the second and third Anglo-Dutch Wars confirmed the Dutch mastery of the seas during the Dutch Golden Age, financed by the expansion of the Dutch Empire. The French Navy won some important victories near the end of the 17th century but a focus upon land forces led to the French Navy's relative neglect, which allowed the Royal Navy to emerge with an ever-growing advantage in size and quality, especially in tactics and experience, from 1695. Throughout the 18th century the Royal Navy gradually gained ascendancy over the French Navy, with victories in the War of Spanish Succession (1701–1714), inconclusive battles in the War of Austrian Succession (1740–1748), victories in the Seven Years' War (1754–1763), a partial reversal during the American War of Independence (1775–1783), and consolidation into uncontested supremacy during the 19th century from the Battle of Trafalgar in 1805. These conflicts saw the development and refinement of tactics which came to be called the line of battle.
The next stage in the evolution of naval warfare was the introduction of metal plating along the hull sides. The increased mass required steam-powered engines, resulting in an arms race between armor and weapon thickness and firepower. The first armored vessels, the French "Gloire" and British HMS "Warrior", made wooden vessels obsolete. Another significant improvement came with the invention of the rotating turrets, which allowed the guns to be aimed independently of ship movement. The battle between the CSS "Virginia" and the USS "Monitor" during the American Civil War (1861–1865) is often cited as the beginning of this age of maritime conflict. The Russian Navy was considered the third strongest in the world on the eve of the Russo-Japanese War, which turned to be a catastrophe for the Russian military in general and the Russian Navy in particular. Although neither party lacked courage, the Russians were defeated by the Japanese in the Battle of Port Arthur, which was the first time in warfare that mines were used for offensive purposes. The warships of the Baltic Fleet sent to the Far East were lost in the Battle of Tsushima. A further step change in naval firepower occurred when the United Kingdom launched HMS "Dreadnought" (1906), but naval tactics still emphasized the line of battle.
The first practical military submarines were developed in the late 19th century and by the end of World War I had proven to be a powerful arm of naval warfare. During World War II, Nazi Germany's submarine fleet of U-boats almost starved the United Kingdom into submission and inflicted tremendous losses on U.S. coastal shipping. The , a sister ship of the , was almost put out of action by miniature submarines known as X-Craft. The X-Craft severely damaged her and kept her in port for some months.
A major paradigm shift in naval warfare occurred with the introduction of the aircraft carrier. First at Taranto in 1940 and then at Pearl Harbor in 1941, the carrier demonstrated its ability to strike decisively at enemy ships out of sight and range of surface vessels. The Battle of Leyte Gulf (1944) was arguably the largest naval battle in history; it was also the last battle in which battleships played a significant role. By the end of World War II, the carrier had become the dominant force of naval warfare.
World War II also saw the United States become by far the largest Naval power in the world. In the late 20th and early 21st centuries, the United States Navy possessed over 70% of the world's total numbers and total tonnage of naval vessels of 1,000 tons or greater. Throughout the rest of the 20th century, the United States Navy would maintain a tonnage greater than that of the next 17 largest navies combined. During the Cold War, the Soviet Navy became a significant armed force, with large numbers of large, heavily armed ballistic missile submarines and extensive use of heavy, long-ranged antisurface missiles to counter the numerous United States carrier battle groups. Only 3 nations (United States, France, and Brazil) presently operate CATOBAR carriers of any size, while Russia, China and India operate sizeable STOBAR carriers (although all three are originally of Russian design). The UK is also currently constructing two Queen Elizabeth class carriers, which will be the largest STOVL vessels in service, and India is currently building two Vikrant-class aircraft carriers (the second one with CATOBAR technology) and considering another. France is also looking at a new carrier, probably using a CATOBAR system and possibly based on the British Queen Elizabeth design.
Operations.
A navy typically operates from one or more naval bases. The base is a port that is specialized in naval operations, and often includes housing, a munitions depot, docks for the vessels, and various repair facilities. During times of war temporary bases may be constructed in closer proximity to strategic locations, as it is advantageous in terms of patrols and station-keeping. Nations with historically strong naval forces have found it advantageous to obtain basing rights in other countries in areas of strategic interest.
Navy ships can operate independently or with a group, which may be a small squadron of comparable ships, or a larger naval fleet of various specialized ships. The commander of a fleet travels in the flagship, which is usually the most powerful vessel in the group. Prior to the invention of radio, commands from the flagship were communicated by means of flags. At night signal lamps could be used for a similar purpose. Later these were replaced by the radio transmitter, or the flashing light when radio silence was needed.
A "blue water navy" is designed to operate far from the coastal waters of its home nation. These are ships capable of maintaining station for long periods of time in deep ocean, and will have a long logistical tail for their support. Many are also nuclear powered to save having to refuel. By contrast a "brown water navy" operates in the coastal periphery and along inland waterways, where larger ocean-going naval vessels can not readily enter. Regional powers may maintain a "green water navy" as a means of localized force projection. Blue water fleets may require specialized vessels, such as mine sweepers, when operating in the littoral regions along the coast.
Traditions.
A basic tradition is that all ships commissioned in a navy are referred to as ships rather than vessels, with the exception of submarines, which are known as boats. The prefix on a ship's name indicates that it is a commissioned ship.
An important tradition on board naval vessels of some nations has been the ship's bell. This was historically used to mark the passage of time, as warning devices in heavy fog, and for alarms and ceremonies.
The ship's captain, and more senior officers are "piped" aboard the ship using a Boatswain's call.
In the United States, the First Navy Jack is a flag that has the words, "Don't Tread on Me" on the flag.
By English tradition, ships have been referred to as a "she". However, it was long considered bad luck to permit women to sail on board naval vessels. To do so would invite a terrible storm that would wreck the ship. The only women that were welcomed on board were figureheads mounted on the prow of the ship.
Firing a cannon salute partially disarms the ship, so firing a cannon for no combat reason showed respect and trust. As the tradition evolved, the number of cannon fired became an indication of the rank of the official being saluted.
Naval organization.
Ships.
Historically, navy ships were primarily intended for warfare. They were designed to withstand damage and to inflict the same, but only carried munitions and supplies for the voyage (rather than merchant cargo). Often, other ships which were not built specifically for warfare, such as the galleon or the armed merchant ships in World War II, did carry armaments. In more recent times, navy ships have become more specialized and have included supply ships, troop transports, repair ships, oil tankers and other logistics support ships as well as combat ships. So long as they are commissioned, however, they are all "ships"...
Modern navy combat ships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines, and amphibious assault ships. There are also support and auxiliary ships, including the oiler, minesweeper, patrol boat, hydrographic and oceanographic survey ship and tender. During the age of sail, the ship categories were divided into the ship of the line, frigate, and sloop-of-war.
Naval ship names are typically prefixed by an abbreviation indicating the national navy in which they serve. For a list of the prefixes used with ship names (HMS, USS, LÉ, etc.) see ship prefix.
Today ships are significantly faster than in former times, thanks to much improved propulsion systems. Also, the efficiency of the engines has improved, in terms of fuel, and of how many sailors it takes to operate them. In World War II, ships needed to refuel very often. However, today ships can go on very long journeys without refueling. Also, in World War II, the engine room needed about a dozen sailors to work the many engines, however, today, only about 4–5 are needed (depending on the class of the ship). Today, naval strike groups on longer missions are always followed by a range of support and replenishment ships supplying them with anything from fuel and munitions, to medical treatment and postal services. This allows strike groups and combat ships to remain at sea for several months at a time.
Boats.
The term "boat" refers to small craft limited in their use by size and usually not capable of making lengthy independent voyages at sea. The old navy adage to differentiate between ships and boats is that boats are capable of being carried by ships. (Submarines by this rule are ships rather than boats, but are customarily referred to as boats reflecting their previous smaller size.)
Navies use many types of boat, ranging from dinghies to landing craft. They are powered by either diesels, out-board gasoline engines, or waterjets. Most boats are built of aluminum, fiberglass, or steel. Rigid-hulled inflatable boats are also used.
Patrol boats are used for patrols of coastal areas, lakes and large rivers.
Landing craft are designed to carry troops, vehicles, or cargo from ship to shore under combat conditions, to unload, to withdraw from the beach, and to return to the ship. They are rugged, with powerful engines, and usually armed. There are many types in today's navies including hovercraft. They will typically have a power-operated bow ramp, a cargo well and after structures that house engine rooms, pilot houses, and stowage compartments. These boats are sometimes carried by larger ships.
Special operations craft are high-speed craft used for insertion and extraction of special forces personnel and some may be transportable (and deployed) by air.
Boats used in non-combat roles include lifeboats, mail boats, line handling boats, buoy boats, aircraft rescue boats, torpedo retrievers, explosive ordnance disposal craft, utility boats, dive boats, targets, and work boats. Boats are also used for survey work, tending divers, and minesweeping operations. Boats for carrying cargo and personnel are sometimes known as launches, gigs, barges or shore party boats.
Units.
Naval forces are typically arranged into units based on the number of ships included, a single ship being the smallest operational unit. Ships may be combined into squadrons or flotillas, which may be formed into fleets. The largest unit size may be the whole Navy or Admiralty.
A task force can be assembled using ships from different fleets for an operational task.
Personnel.
Despite their acceptance in many areas of naval service, women sailors were not permitted to serve on board U.S. submarines until the U.S. Navy lifted the ban in April 2010. The major reasons historically cited by the U.S. Navy were the extended duty tours and close conditions which afford almost no privacy. The United Kingdom's Royal Navy has had similar restrictions. Australia, Canada, Norway, and Spain previously opened submarine service to women sailors.
Ranks.
A navy will typically have two sets of ranks, one for enlisted personnel and one for officers.
Typical ranks for commissioned officers include the following, in ascending order (Commonwealth ranks are listed first on each line; USA ranks are listed second in those instances where they differ from Commonwealth ranks):
"Flag officers" include any rank that includes the word "admiral" (or commodore in services other than the US Navy), and are generally in command of a battle group, strike group or similar flotilla of ships, rather than a single ship or aspect of a ship. However, commodores can also be temporary or honorary positions. For example, during World War II, a Navy captain was assigned duty as a convoy commodore, which meant that he was still a captain, but in charge of all the merchant vessels in the convoy.
The most senior rank employed by a navy will tend to vary depending on the size of the navy and whether it is wartime or peacetime, for example, few people have ever held the rank of Fleet Admiral in the U.S. Navy, the chief of the Royal Australian Navy holds the rank of Vice Admiral, and the chief of the Irish Naval Service holds the rank of Commodore.
Naval infantry.
Naval infantry, commonly known as marines, are a category of infantry that form part of a state’s naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.
During the era of the Roman empire, naval forces included marine legionaries for maritime boarding actions. These were troops primarily trained in land warfare, and did not need to be skilled at handling a ship. Much later during the age of sail, a component of marines served a similar role, being ship-borne soldiers who were used either during boarding actions, as sharp-shooters, or in raids along shorelines.
The Spanish "Infantería de Marina" was formed in 1537, making it the oldest, current marine force in the world. The British Royal Marines combine being both a ship-based force and also being specially trained in commando-style operations and tactics, operating in some cases separately from the rest of the Royal Navy. The Royal Marines also have their own special forces unit.
In the majority of countries, the marine force is part of the navy. The United States Marine Corps is a separate armed service within the United States Department of the Navy, with its own leadership structure.
Naval aviation.
Naval aviation is the application of military air power by navies, whether from warships that embark aircraft, or land bases.
In World War I several navies used floatplanes and flying boats - mainly for scouting. By World War II, aircraft carriers could carry bomber aircraft capable of attacking naval and land targets, as well as fighter aircraft for defence. Since World War II helicopters have been embarked on smaller ships in roles such as anti-submarine warfare and transport. Some navies have also operated land-based aircraft in roles such as maritime patrol and training.
Naval aviation forces primarily perform naval roles at sea. However, they are also used in a variety of other roles.
Notes and references.
http://linguistlist.org/issues/26/26-1302.html

</doc>
<doc id="21538" url="https://en.wikipedia.org/wiki?curid=21538" title="Normed vector space">
Normed vector space

In mathematics, with 2- or 3-dimensional vectors with real-valued entries, the idea of the "length" of a vector is intuitive and can easily be extended to any real vector space R"n". The following properties of "vector length" are crucial.
The generalization of these three properties to more abstract vector spaces leads to the notion of norm. A vector space on which a norm is defined is then called a normed space or normed vector space.
Normed vector spaces are central to the study of linear algebra and functional analysis.
Definition.
A normed vector space is a pair ("V", ‖·‖ ) where "V" is a vector space and ‖·‖ a norm on "V".
A seminormed vector space is a pair ("V","p") where "V" is a vector space and "p" a seminorm on "V".
We often omit "p" or ‖·‖ and just write "V" for a space if it is clear from the context what (semi) norm we are using.
In a more general sense, a vector norm can be taken to be any real-valued function that satisfies these three properties. The properties 1. and 2. together imply that
A useful variation of the triangle inequality is
This also shows that a vector norm is a continuous function.
Note that property 2 depends on a choice of norm formula_9 on the field of scalars. When the scalar field is formula_10 (or more generally a subset of formula_11), this is usually taken to be the ordinary absolute value, but other choices are possible. For example, for a vector space over formula_12 one could take formula_9 to be the "p"-adic norm, which gives rise to a different class of normed vector spaces.
Topological structure.
If ("V", ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric (a notion of "distance") and therefore a topology on "V". This metric is defined in the natural way: the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of "V" in the following sense:
Similarly, for any semi-normed vector space we can define the distance between two vectors u and v as ‖u−v‖. This turns the seminormed space into a pseudometric space (notice this is weaker than a metric) and allows the definition of notions such as continuity and convergence.
To put it more abstractly every semi-normed vector space is a topological vector space and thus carries a topological structure which is induced by the semi-norm.
Of special interest are complete normed spaces called Banach spaces. Every normed vector space "V" sits as a dense subspace inside a Banach space; this Banach space is essentially uniquely defined by "V" and is called the "completion" of "V".
All norms on a finite-dimensional vector space are equivalent from a topological viewpoint as they induce the same topology (although the resulting metric spaces need not be the same). And since any Euclidean space is complete, we can thus conclude that all finite-dimensional normed vector spaces are Banach spaces. A normed vector space "V" is locally compact if and only if the unit ball "B" = {"x" : ‖"x"‖ ≤ 1} is compact, which is the case if and only if "V" is finite-dimensional; this is a consequence of Riesz's lemma. (In fact, a more general result is true: a topological vector space is locally compact if and only if it is finite-dimensional.
The point here is that we don't assume the topology comes from a norm.)
The topology of a seminormed vector space has many nice properties. Given a neighbourhood system formula_14 around 0 we can construct all other neighbourhood systems as
with
Moreover there exists a neighbourhood basis for 0 consisting of absorbing and convex sets. As this property is very useful in functional analysis, generalizations of normed vector spaces with this property are studied under the name locally convex spaces.
Linear maps and dual spaces.
The most important maps between two normed vector spaces are the continuous linear maps. Together with these maps, normed vector spaces form a category.
The norm is a continuous function on its vector space. All linear maps between finite dimensional vector spaces are also continuous.
An "isometry" between two normed vector spaces is a linear map "f" which preserves the norm (meaning ‖"f"(v)‖ = ‖v‖ for all vectors v). Isometries are always continuous and injective. A surjective isometry between the normed vector spaces "V" and "W" is called an "isometric isomorphism", and "V" and "W" are called "isometrically isomorphic". Isometrically isomorphic normed vector spaces are identical for all practical purposes.
When speaking of normed vector spaces, we augment the notion of dual space to take the norm into account. The dual "V" ' of a normed vector space "V" is the space of all "continuous" linear maps from "V" to the base field (the complexes or the reals) — such linear maps are called "functionals". The norm of a functional φ is defined as the supremum of |φ(v)| where v ranges over all unit vectors (i.e. vectors of norm 1) in "V". This turns "V" ' into a normed vector space. An important theorem about continuous linear functionals on normed vector spaces is the Hahn–Banach theorem.
Normed spaces as quotient spaces of seminormed spaces.
The definition of many normed spaces (in particular, Banach spaces) involves a seminorm defined on a vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero. For instance, with the L"p" spaces, the function defined by
is a seminorm on the vector space of all functions on which the Lebesgue integral on the right hand side is defined and finite. However, the seminorm is equal to zero for any function supported on a set of Lebesgue measure zero. These functions form a subspace which we "quotient out", making them equivalent to the zero function.
Finite product spaces.
Given "n" seminormed spaces "X""i" with seminorms "q""i" we can define the product space as
with vector addition defined as
and scalar multiplication defined as
We define a new function "q"
for example as
which is a seminorm on "X". The function "q" is a norm if and only if all "q""i" are norms.
More generally, for each real "p"≥1 we have the seminorm:
For each p this defines the same topological space.
A straightforward argument involving elementary linear algebra shows that the only finite-dimensional seminormed spaces are those arising as the product space of a normed space and a space with trivial seminorm. Consequently, many of the more interesting examples and applications of seminormed spaces occur for infinite-dimensional vector spaces.

</doc>
<doc id="21541" url="https://en.wikipedia.org/wiki?curid=21541" title="Nicene Creed">
Nicene Creed

The Nicene Creed (Greek: or , Latin: "") is a profession of faith widely used in Christian liturgy.
It is called Nicene because it was originally adopted in the city of Nicaea (present day Iznik, Turkey) by the First Council of Nicaea in 325. In 381, it was amended at the First Council of Constantinople, and the amended form is referred to as the Nicene or the Niceno-Constantinopolitan Creed.
The churches of Oriental Orthodoxy and the Assyrian churches use this profession of faith with the verbs in the original plural ("we believe") form. The Eastern Orthodox Church and the Roman Catholic Church use it with the verbs of believing changed to the singular ("I believe") form. The Anglican Communion and many Protestant denominations also use it, sometimes with the verbs of believing in the plural form but generally in the singular.
The Apostles' Creed is also used in the Latin West, but not in the Eastern liturgies. On Sundays and some other days, one or other of these two creeds is recited in the Roman Rite Mass after the homily. The Nicene Creed is also part of the profession of faith required of those undertaking important functions within the Catholic Church.
In the Byzantine Rite, the Nicene Creed is sung or recited at the Divine Liturgy, immediately preceding the Anaphora (Eucharistic Prayer), and is also recited daily at compline.
Nomenclature.
There are several designations for the two forms of the Nicene creed, some with overlapping meanings:
In musical settings, particularly when sung in Latin, this Creed is usually referred to by its first word, "Credo".
History.
The purpose of a creed is to provide a doctrinal statement of correct belief, or orthodoxy. The creeds of Christianity have been drawn up at times of conflict about doctrine: acceptance or rejection of a creed served to distinguish believers and deniers of a particular doctrine or set of doctrines. For that reason a creed was called in Greek a σύμβολον (Eng. "symbolon"), a word that meant half of a broken object which, when placed together with the other half, verified the bearer's identity. The Greek word passed through Latin "symbolum" into English "symbol", which only later took on the meaning of an outward sign of something.
The Nicene Creed was adopted in the face of the Arian controversy, whose leader, Arius, was a member of the clergy of Alexandria. "Arius objected to Alexander's (the bishop of the time) apparent carelessness in blurring the distinction of nature between the Father and the Son by his emphasis on eternal generation". Alexander accused Arius of denying the divinity of the Son and also of being too "Jewish" and "Greek" in his thought. Both Arius and Alexander rejected Gnosticism, Manichaeism and Sabellian formulae. The Nicene Creed was created as a result of the extensive adoption of the doctrine of Arius far outside Alexandria, in order to clarify the key tenets of the Christian faith.
The Nicene Creed of 325 explicitly affirms the co-essential divinity of the Son, applying to him the term "consubstantial". The 381 version speaks of the Holy Spirit as worshipped and glorified with the Father and the Son. The Athanasian Creed (not used in Eastern Christianity) describes in much greater detail the relationship between Father, Son and Holy Spirit. The Apostles' Creed makes no explicit statements about the divinity of the Son and the Holy Spirit, but, in the view of many who use it, the doctrine is implicit in it.
Original Nicene Creed of 325.
The original Nicene Creed was first adopted in 325 at the First Council of Nicaea. At that time, the text ended after the words "We believe in the Holy Spirit", after which an anathema was added. (For other differences, see Comparison between Creed of 325 and Creed of 381, below.)
The Coptic Church has the tradition that the original creed was authored by Pope Athanasius I of Alexandria. F.J.A. Hort and Adolf Harnack argued that the Nicene creed was the local creed of Caesarea (an important center of Early Christianity) brought to the council by Eusebius of Caesarea. J.N.D. Kelly sees as its basis a baptismal creed of the Syro-Phoenician family, related to (but not dependent on) the creed cited by Cyril of Jerusalem and to the creed of Eusebius.
Soon after the Council of Nicaea, new formulae of faith were composed, most of them variations of the Nicene Symbol, to counter new phases of Arianism. The "Catholic Encyclopedia" identifies at least four before the Council of Sardica (341), where a new form was presented and inserted in the Acts of the Council, though it was not agreed on.
Niceno–Constantinopolitan Creed.
What is known as the "Niceno-Constantinopolitan Creed" or the "Nicene-Constantinopolitan Creed" received this name because of a belief that it was adopted at the Second Ecumenical Council held in Constantinople in 381 as a modification of the original Nicene Creed of 325. In that light, it also came to be very commonly known simply as the "Nicene Creed". It is the only authoritative "ecumenical" statement of the Christian faith accepted by the Roman Catholic, Eastern Orthodox, Oriental Orthodox, Anglican, and the major Protestant denominations. (The Apostles' and Athanasian creeds are not as widely accepted.)
It differs in a number of respects, both by addition and omission, from the creed adopted at the First Council of Nicaea. The most notable difference is the additional section "And believe in the Holy Ghost, the Lord and Giver-of-Life, who proceedeth from the Father, who with the Father and the Son together is worshipped and glorified, who spake by the prophets. And believe in one, holy, Catholic and Apostolic Church. We acknowledge one Baptism for the remission of sins, we look for the resurrection of the dead and the life of the world to come. Amen."
Since the end of the 19th century, scholars have questioned the traditional explanation of the origin of this creed, which has been passed down in the name of the council, whose official acts have been lost over time. A local council of Constantinople in 382 and the third ecumenical council (Ephesus, 431) made no mention of it, with the latter affirming the 325 creed of Nicaea as a valid statement of the faith and using it to denounce Nestorianism. Though some scholarship claims that hints of the later creed's existence are discernible in some writings, no extant document gives its text or makes explicit mention of it earlier than the fourth ecumenical council at Chalcedon in 451. Many of the bishops of the 451 council themselves had never heard of it and initially greeted it skeptically, but it was then produced from the episcopal archives of Constantinople, and the council accepted it "not as supplying any omission but as an authentic interpretation of the faith of Nicaea". In spite of the questions raised, it is considered most likely that this creed was in fact adopted at the 381 second ecumenical council.
On the basis of evidence both internal and external to the text, it has been argued that this creed originated not as an editing of the original Creed proposed at Nicaea in 325, but as an independent creed (probably an older baptismal creed) modified to make it more like the Nicene Creed. Some scholars have argued that the creed may have been presented at Chalcedon as "a precedent for drawing up new creeds and definitions to supplement the Creed of Nicaea, as a way of getting round the ban on new creeds in Canon 7 of Ephesus". It is generally agreed that the Niceno-Constantinopolitan Creed is not simply an expansion of the Creed of Nicaea, and was probably based on another traditional creed independent of the one from Nicaea.
The third Ecumenical Council (Council of Ephesus of 431) reaffirmed the original 325 version of the Nicene Creed and declared that "it is unlawful for any man to bring forward, or to write, or to compose a different ( – more accurately translated as used by the Council to mean “different,” “contradictory,” and not “another”) faith as a rival to that established by the holy Fathers assembled with the Holy Ghost in Nicaea" (i.e. the 325 creed) This statement has been interpreted as a prohibition against changing this creed or composing others, but not all accept this interpretation. This question is connected with the controversy whether a creed proclaimed by an Ecumenical Council is definitive in excluding not only excisions from its text but also additions to it.
In one respect, the Eastern Orthodox Church's received text of the Niceno-Constantinopolitan Creed differs from the earliest text, which is included in the acts of the Council of Chalcedon of 451: The Eastern Orthodox Church uses the singular forms of verbs such as "I believe", in place of the plural form ("we believe") used by the council. Byzantine Rite Eastern Catholic Churches use exactly the same form of the Creed, since the Catholic Church teaches that it is wrong to add "and the Son" to the Greek verb "ἐκπορευόμενον", though correct to add it to the Latin "qui procedit", which does not have precisely the same meaning. The form generally used in Western churches does add "and the Son" and also the phrase "God from God", which is found in the original 325 Creed.
Comparison between Creed of 325 and Creed of 381.
The following table, which indicates by brackets the portions of the 325 text that were omitted or moved in 381, and uses "italics" to indicate what phrases, absent in the 325 text, were added in 381, juxtaposes the earlier (325 AD) and later (381 AD) forms of this Creed in the English translation given in Schaff's work, "Creeds of Christendom".
The texts in Greek, as given on the Web site Symbolum Nicaeno-Constantinopolitanum – Greek, can be presented in a similar way, as follows:
Filioque controversy.
In the late 6th century, some Latin-speaking churches added the words "and from the Son" ("Filioque") to the description of the procession of the Holy Spirit, in what many Eastern Orthodox Christians have at a later stage argued is a violation of Canon VII of the Third Ecumenical Council, since the words were not included in the text by either the Council of Nicaea or that of Constantinople.
The Vatican stated in 1995 that, while the words καὶ τοῦ Υἱοῦ ("and the Son") would indeed be heretical if used with the Greek verb ἐκπορεύομαι — which is one of the terms used by St. Gregory of Nazianzus and the one adopted by the Council of Constantinople — the word "Filioque" is not heretical when associated with the Latin verb "procedo" and the related word "processio." Whereas the verb ἐκπορεύομαι (from ἐκ, "out of" and πορεύομαι "to come or go") in Gregory and other Fathers necessarily means "to originate from a cause or principle," the Latin term "procedo" (from "pro", "forward;" and "cedo", "to go") has no such connotation and simply denotes the communication of the Divine Essence or Substance. In this sense, "processio" is similar in meaning to the Greek term προϊέναι, used by the Fathers from Alexandria (especially Cyril of Alexandria) as well as others. Partly due to the influence of the Latin translations of the New Testament (especially of John 15:26), the term ἐκπορευόμενον (the present participle of ἐκπορεύομαι) in the creed was translated into Latin as "procedentem". In time, the Latin version of the Creed came to be interpreted in the West in the light of the Western concept of "processio", which required the affirmation of the "Filioque" to avoid the heresy of Arianism.
Views on the importance of this creed.
The view that the Nicene Creed can serve as a touchstone of true Christian faith is reflected in the name "symbol of faith", which was given to it in Greek and Latin, when in those languages the word "symbol" meant a "token for identification (by comparison with a counterpart)", and which continues in use even in languages in which "symbol" no longer has that meaning.
In the Roman Rite Mass, the Latin text of the Niceno-Constantinopolitan Creed, with "Deum de Deo" (God from God) and "Filioque" (and from the Son), phrases absent in the original text, was previously the only form used for the "profession of faith". The Roman Missal now refers to it jointly with the Apostles' Creed as "the Symbol or Profession of Faith or Creed", describing the second as "the baptismal Symbol of the Roman Church, known as the Apostles' Creed".
The liturgies of the ancient Churches of Eastern Christianity (Eastern Orthodox Church, Oriental Orthodoxy, Assyrian Church of the East and the Eastern Catholic Churches), use the Niceno-Constantinopolitan Creed, never the Western Apostles' Creed.
While in certain places where the Byzantine Rite is used, the choir or congregation sings the Creed at the Divine Liturgy, in many places the Creed is typically recited by the cantor, who in this capacity represents the whole congregation although many, and sometimes all, members of the congregation may join in rhythmic recitation. Where the latter is the practice, it is customary to invite, as a token of honor, any prominent lay member of the congregation who happens to be present, "e.g.", royalty, a visiting dignitary, the Mayor, etc., to recite the Creed in lieu of the cantor. This practice stems from the tradition that the prerogative to recite the Creed belonged to the Emperor, speaking for his populace.
Some evangelical and other Christians consider the Nicene Creed helpful and to a certain extent authoritative, but not infallibly so in view of their belief that only Scripture is truly authoritative. Other groups, such as the Church of the New Jerusalem, The Church of Jesus Christ of Latter-day Saints, and the Jehovah's Witnesses explicitly reject some of the statements in the Creed.
Ancient liturgical versions.
This section is not meant to collect the texts of all liturgical versions of the Nicene Creed, and provides only three, the Greek, the Latin, and the Armenian, of special interest. Others are mentioned separately, but without the texts. All ancient liturgical versions, even the Greek, differ at least to some small extent from the text adopted by the First Councils of Nicaea and Constantinople. The Creed was originally written in Greek, owing to the location of the two councils.
But though the councils' texts have "Πιστεύομεν ... ὁμολογοῦμεν ... προσδοκοῦμεν" ("we" believe ... confess ... await), the Creed that the Churches of Byzantine tradition use in their liturgy has "Πιστεύω ... ὁμολογῶ ... προσδοκῶ" ("I" believe ... confess ... await), accentuating the personal nature of recitation of the Creed. The Latin text, as well as using the singular, has two additions: "Deum de Deo" (God from God) and "Filioque" (and from the Son). The Armenian text has many more additions, and is included as showing how that ancient church has chosen to recite the Creed with these numerous elaborations of its contents.
An English translation of the Armenian text is added; English translations of the Greek and Latin liturgical texts are given at English versions of the Nicene Creed in current use.
Latin liturgical version.
The Latin text adds "Deum de Deo" and "Filioque" to the Greek. On the latter see The Filioque Controversy above. Inevitably also, the overtones of the terms used, such as "" (pantokratora) and "omnipotentem" differ ("pantokratora" meaning Ruler of all; "omnipotentem" meaning omnipotent, Almighty). The implications of this for the interpretation of "" and "qui ... procedit" was the object of the study "The Greek and the Latin Traditions regarding the Procession of the Holy Spirit" published by the Pontifical Council for Promoting Christian Unity in 1996.
Again, the terms "" and "consubstantialem", translated as "of one being" or "consubstantial", have different overtones, being based respectively on Greek (stable being, immutable reality, substance, essence, true nature),[http://www.perseus.tufts.edu/cgi-bin/ptext?doc=Perseus%3Atext%3A1999.04.0057%3Aentry%3D%2376030] and Latin "substantia" (that of which a thing consists, the being, essence, contents, material, substance).
"Credo", which in classical Latin is used with the accusative case of the thing held to be true (and with the dative of the person to whom credence is given), is here used three times with the preposition "in", a literal translation of the Greek "" (in unum Deum ..., in unum Dominum ..., in Spiritum Sanctum ...), and once in the classical preposition-less construction (unam, sanctam, catholicam et apostolicam Ecclesiam).
Armenian liturgical text.
English translation of the Armenian version
Other ancient liturgical versions.
The version in the Church Slavonic language, used by several Eastern Orthodox Churches is practically identical with the Greek liturgical version.
This version is used also by some Byzantine Rite Eastern Catholic Churches. Although the Union of Brest excluded addition of the "Filioque", this was sometimes added by Ruthenian Catholics, whose liturgical books now give the phrase in brackets, and by Ukrainian Catholics. Writing in 1971, the Ruthenian Scholar Fr. Casimir Kucharek noted, "In Eastern Catholic Churches, the "Filioque" may be omitted except when scandal would ensue. Most of the Eastern Catholic Rites use it." However, in the decades that followed 1971 it has come to be used more rarely.
The versions used by Oriental Orthodoxy differ from the Greek liturgical version in having "We believe", as in the original text, instead of "I believe".
The Church of the East, which is in communion neither with the Eastern Orthodox Church nor with Oriental Orthodoxy also uses "We believe".
English translations.
The version found in the 1662 "Book of Common Prayer" is still commonly used by some English speakers, but more modern translations are now more common.
International Consultation on English Texts.
The International Consultation on English Texts published an English translation of the Nicene Creed, first in 1970 and then in successive revisions in 1971 and 1975. These texts were adopted by several churches.
The Roman Catholic Church in the United States, which adopted the 1971 version in 1973, and the Catholic Church in other English-speaking countries, which in 1975 adopted the version published in that year, continued to use them until 2011 upon the introduction of the "Roman Missal third edition".
The 1975 version was included in the 1979 Episcopal Church (United States) "Book of Common Prayer", though with one variation: in the line "For us men and for our salvation", it omitted the word "men".
Other English translations.
For the text of the Nicene Creed published in 1988 by the English Language Liturgical Consultation, the successor body of the International Consultation on English Texts, see their website. For the text as recited in the Roman Rite of the Catholic Church, see the website of the Australian National Catholic Education Commission or Youcat, section 29.

</doc>
<doc id="21544" url="https://en.wikipedia.org/wiki?curid=21544" title="Nuclear fusion">
Nuclear fusion

In nuclear physics, nuclear fusion is a nuclear reaction in which two or more atomic nuclei come very close and then collide at a very high speed and join to form a new nucleus. The mass of this new nucleus is slightly less than the sum of its constituents, by an amount known as the "binding energy". This is because the compound nucleus is a lower-energy system (a "more favourable" system energetically) than the two parent nuclei. The binding energy is released as photons (energy). This is the energy that is given off by a fusion process. Fusion is the process that powers active or "main sequence" stars, or other high magnitude stars.
The fusion of two nuclei with lower masses than iron-56 (which, along with nickel-62, has the largest binding energy per nucleon) generally releases energy, while the fusion of nuclei heavier than iron requires energy input. The opposite is true for the reverse process, nuclear fission. This means that only lighter elements are fusable, such as hydrogen and helium, and likewise, that only heavier elements are fissionable, such as uranium and plutonium. There are, however, extreme astrophysical events that can lead to short periods of fusion with heavier nuclei. This is the process that gives rise to the creation of the heavy elements during events such as a supernova.
Following the discovery of quantum tunneling by physicist Friedrich Hund, in 1929 Robert Atkinson and Fritz Houtermans used the measured masses of light elements to predict that large amounts of energy could be released by fusing small nuclei. Building upon the nuclear transmutation experiments by Ernest Rutherford, carried out several years earlier, the laboratory fusion of hydrogen isotopes was first accomplished by Mark Oliphant in 1932. During the remainder of that decade the steps of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.
Research into developing controlled thermonuclear fusion for civil purposes also began in earnest in the 1950s, and it continues to this day. 
Process.
[[Image:Deuterium-tritium fusion.svg|thumb|Fusion of deuterium with tritium creating helium-4, freeing a neutron, and releasing 17.59 MeV as kinetic energy of the products while a corresponding amount of mass disappears, in agreement with "kinetic E" = |Δ"mc"2|, where "Δ"m is the decrease in the total rest mass of particles.]]
The origin of the energy released in fusion of light elements is due to interplay of two opposing forces, the nuclear force which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. The protons are positively charged and repel each other but they nonetheless stick together, demonstrating the existence of another force referred to as nuclear attraction. This force, called the strong nuclear force, overcomes electric repulsion in a very close range. The effect of this force is not observed outside the nucleus, hence the force is called a short-range force. The same force also pulls the nucleons (neutrons and protons) together. Light nuclei (or nuclei smaller than iron and nickel), are sufficiently small and proton-poor for the nuclear force to overcome the repulsive Coulomb force. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up these nuclei from lighter nuclei by fusion thus releases the extra energy from the net attraction of these particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across still larger atomic nuclei. Thus, energy is no longer released when such nuclei are made by fusion; instead, energy is required as input to such processes.
Fusion reactions of light elements power the stars and produce virtually all elements in a process called nucleosynthesis. The fusion of lighter elements in stars releases energy (and the mass that always accompanies it). For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away from the system in the form of kinetic energy or other forms of energy (such as electromagnetic radiation).
Research into controlled fusion, with the aim of producing fusion power for the production of electricity, has been conducted for over 60 years. It has been accompanied by extreme scientific and technological difficulties, but has resulted in progress. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion reactions. Workable designs for a reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat up plasma to required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.
It takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. This is because all nuclei have a positive charge due to their protons, and as like charges repel, nuclei strongly resist being put close together. Accelerated to high speeds, they can overcome this electrostatic repulsion and be forced close enough for the attractive nuclear force to be sufficiently strong to achieve fusion. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions. The US National Ignition Facility, which uses laser-driven inertial confinement fusion, is thought to be capable of break-even fusion.
The first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.
Energy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is far greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the diagram to the right (one gram of matter would release of energy). Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though "individual" fission reactions are generally much more energetic than "individual" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.
Nuclear fusion in stars.
The most important fusion process in nature is the one that powers stars. In the 20th century, it was realized that the energy released from nuclear fusion reactions accounted for the longevity of the Sun and other stars as a source of heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of that fusion process. The prime energy producer in the Sun is the fusion of hydrogen to form helium, which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons, two neutrinos (which changes two of the protons into neutrons), and energy. Different reaction chains are involved, depending on the mass of the star. For stars the size of the sun or smaller, the proton-proton chain dominates. In heavier stars, the CNO cycle is more important.
As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements, as part of stellar nucleosynthesis. However the heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.
Requirements.
Details and supporting references on the material in this section can be found in textbooks on nuclear physics or nuclear fusion.
A substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through columb forces.
When a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.
The electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from "all" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei get larger.
The net result of these opposing forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.
An exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single particle in nuclear physics, namely, the alpha particle.
The situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough can the strong nuclear force take over (by way of tunneling). Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.
The Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.
Using deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable 5He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining 4He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.
The reaction cross section σ is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:
If a species of nuclei is reacting with itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.
formula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.
The significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.
Methods for achieving fusion.
Thermonuclear fusion.
If the matter is sufficiently heated (hence being plasma), the fusion reaction may occur due to collisions with extreme thermal kinetic energies of the particles. In the form of thermonuclear weapons, thermonuclear fusion is the only fusion technique so far to yield undeniably large amounts of useful fusion energy. Usable amounts of thermonuclear fusion energy released in a controlled manner have yet to be achieved. In nature, this is what produces energy in stars through stellar nucleosynthesis.
Inertial confinement fusion.
Inertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.
Inertial electrostatic confinement.
Inertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.
Beam-beam or beam-target fusion.
If the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called "beam-target" fusion; if both nuclei are accelerated, it is "beam-beam" fusion.
Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—all it takes is a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions end up expending their energy on bremsstrahlung and ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of these nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.
Muon-catalyzed fusion.
Muon-catalyzed fusion is a well-established and reproducible fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction cannot occur because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.
Other principles.
Some other confinement principles have been investigated.
Antimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.
Pyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.
Hybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.
Project PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.
Important reactions.
Astrophysical reaction chains.
At the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature ("T" ≈ 15 MK) and density (160 g/cm3), the energy release rate is only 276 μW/cm3—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates strongly depend on temperature (exp(−"E"/"kT")), achieving reasonable power levels in terrestrial fusion reactors requires 10–100 times higher temperatures (compared to stellar interiors): "T" ≈ 0.1–1.0 GK.
Criteria and candidates for terrestrial reactions.
In artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as "aneutronic".
To be a useful energy source, a fusion reaction must satisfy several criteria. It must:
Few reactions meet these criteria. The following are those with the largest cross sections:
For reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.
Some reaction candidates can be eliminated at once. The D-6Li reaction has no advantage compared to p+- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p+- reaction, but the cross section is far too low, except possibly when "T"i > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p+- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.
In addition to the fusion reactions, the following reactions with neutrons are important in order to "breed" tritium in "dry" fusion bombs and some proposed fusion reactors:
The latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo "Shrimp" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. Li-7 also undergoes a chain reaction due to its release of a neutron after fissioning. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused casualties from the fallout generated.
To evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T2 is a maximum. This is also the temperature at which the value of the triple product "nT"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T2 (see Lawson criterion). (A plasma is "ignited" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T2 at that temperature is given for a few of these reactions in the following table.
Note that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are "right". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.
Neutronicity, confinement requirement, and power density.
Any of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products "E"fus, the energy of the charged fusion products "E"ch, and the atomic number "Z" of the non-hydrogenic reactant.
Specification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low, so it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction. Thus the total reaction would be the sum of (2i), (2ii), and (1):
We count the - fusion energy "per D-D reaction" (not per pair of deuterium atoms) as "E"fus = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as "E"ch = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only approximate.)
Another unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.
With this choice, we tabulate parameters for four of the most important reactions
The last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as ("E"fus-"E"ch)/"E"fus. For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.
Of course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/("Z"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T2. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.
Thus there is a "penalty" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a "hot ion mode", the "penalty" would not apply.) There is at the same time a "bonus" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.
We can now compare these reactions in the following table.
The maximum value of <σv>/T2 is taken from a previous table. The "penalty/bonus" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column "reactivity" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column "Lawson criterion" weights these results with "E"ch and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The last column is labeled "power density" and weights the practical reactivity with "E"fus. It indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.
Bremsstrahlung losses in quasineutral, isotropic plasmas.
The ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.
The huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.
The ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions.
The actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves "must" remain in the plasma until they have given up their energy, and "will" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.
The temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p+- and p+- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in "Fundamental limitations on plasma fusion systems not in thermodynamic equilibrium" by Todd Rider. This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.

</doc>
<doc id="21550" url="https://en.wikipedia.org/wiki?curid=21550" title="National Geographic Society">
National Geographic Society

The National Geographic Society (NGS), headquartered in Washington, D.C., United States, is one of the largest nonprofit scientific and educational institutions in the world. Its interests include geography, archaeology and natural science, the promotion of environmental and historical conservation, and the study of world culture and history. The National Geographic Society’s logo is a yellow portrait frame – rectangular in shape – which appears on the margins surrounding the front covers of its magazines and as its television channel logo. It also operates a website that features extra content and worldwide events.
A significant change was announced by National Geographic Society and 21st Century Fox on September 9, 2015. The two organizations revealed an "expanded joint venture" that would re-organize the Society's media properties and publications into a new company known as National Geographic Partners. The agreement, to be finalized in mid-November 2015, will make National Geographic magazines and the Society's cable networks a for-profit venture to be 73% owned by 21st Century Fox, a conventional for-profit corporation. National Geographic Society will continue as a non-profit, however, with an enhanced endowment.
Overview.
The National Geographic Society was founded in 1888 "to increase and diffuse geographic knowledge." The Society believes in the power of science, exploration and storytelling to change the world, and its purpose is to inspire, illuminate and teach. National Geographic is governed by a board of trustees, whose 21 members include distinguished educators, business executives, former government officials and conservationists.
The organization sponsors and funds scientific research and exploration. The Society publishes a journal, "National Geographic" (purchased by Rupert Murdoch in 2015) in English and nearly 40 local-language editions. It also publishes other magazines, books, school products, maps, and Web and film products in numerous languages and countries. Its Education Foundation gives grants to education organizations and individuals to improve geography education. Its Committee for Research and Exploration has awarded more than 11,000 grants for scientific research and exploration.
National Geographic's various media properties reach more than 700 million people monthly. National Geographic maintains a museum for the public in its Washington, D.C., headquarters. It has helped to sponsor popular traveling exhibits, such as an early 2010s "King Tut" exhibit featuring magnificent artifacts from the tomb of the young Egyptian Pharaoh; "The Cultural Treasures of Afghanistan" which opened in May 2008 and traveled to other cities for 18 months; and an exhibition of China's Terracotta Warriors in its Washington headquarters in 2009–10.
National Geographic has retail stores in Washington, D.C., London, Sydney, and Panama. The locations outside of the United States are operated by Worldwide Retail Store S.L., a Spanish holding company.
History.
The National Geographic Society began as a club for an elite group of academics and wealthy patrons interested in travel. On January 13, 1888, 33 explorers and scientists gathered at the Cosmos Club, a private club then located on Lafayette Square in Washington, D.C., to organize "a society for the increase and diffusion of geographical knowledge." After preparing a constitution and a plan of organization, the National Geographic Society was incorporated two weeks later on January 27. Gardiner Greene Hubbard became its first president and his son-in-law, Alexander Graham Bell, succeeded him in 1897. In 1899, Bell's son-in-law Gilbert Hovey Grosvenor was named the first full-time editor of National Geographic magazine and served the organization for fifty-five years (until 1954), and members of the Grosvenor family have played important roles in the organization since. Bell and Gilbert Hovey Grosvenor devised the successful marketing notion of Society membership and the first major use of photographs to tell stories in magazines. 
The current National Geographic Society president and CEO is Gary E. Knell. The chairman of the board of trustees is John Fahey. The editor-in-chief of National Geographic magazine is Susan Goldberg. Gilbert Melville Grosvenor, a former chairman of the Society board of trustees received the Presidential Medal of Freedom in 2005 for his leadership in geography education. 
In 2004, the National Geographic Society headquarters in Washington, D.C., was one of the first buildings to receive a "Green" certification from Global Green USA. The National Geographic received the prestigious Prince of Asturias Award for Communication and Humanities in October 2006 in Oviedo, Spain.
National Geographic Partners Joint Venture.
On September 9, 2015, the Society announced that it would re-organize its media properties and publications into a new company known as National Geographic Partners, which will be majority-owned by 21st Century Fox with a 73% stake. This new, for-profit corporation, will own "National Geographic" and other magazines, as well as its affiliated television networks—most of which were already owned in joint ventures with Fox. At the time of the deal’s announcement, James Murdoch, chief executive of 21st Century Fox, said in remarks to National Geographic that the pact created "an expanded canvas for the National Geographic brand to grow and reach customers in new ways, and to reach new customers."
On November 2, 2015, roughly two weeks before the closing of the expanded joint venture deal, National Geographic and 21st Century Fox announced that 9 percent of National Geographic's 2,000 employees, approximately 180 people, will be laid off, constituting the biggest staff reduction in the Society's history.
As reported by The Guardian, a spokesman for National Geographic in a November 2, 2015 e-mail statement, briefly discussed the rationale for the staff reductions as part of the "... process of reorganizing in order to move forward strategically following the closing the National Geographic Partners deal, which is expected to occur in mid-November."
Additional specifics were provided to Photo District News by M.J. Jacobsen, National Geographic’s SVP of communications, similar to the contents of a formal announcement by the two companies: "The National Geographic Society and the National Geographic Channels are in the process of reorganizing in order to move forward strategically following the closing of the NG Partners deal (with Fox), which is expected to occur in mid-November," Jaobsen wrote. "Involuntary separations will represent about 9 percent of the overall workforce reduction, many in shared services and a voluntary separation offer has also been made to eligible employees," he added.
Specifics as to which departments would be affected were not immediately available but the Washington Post reported that the staff reduction appears to affect almost every department including the magazine and the National Geographic Channel.
Founders.
There were 33 original founders in 1888. Although Alexander Graham Bell is sometimes discussed as a founder, he was actually the second president, elected on January 7, 1898 and serving until 1903.
Publications.
"National Geographic".
"The National Geographic Magazine", later shortened to "National Geographic", published its first issue in October 1888, nine months after the Society was founded, as the Society's official journal, a benefit for joining the tax-exempt National Geographic Society. Starting with the February 1910 (Vol XXI., No. 2) issue, the magazine began using its now famous trademarked yellow border around the edge of its covers.
There are 12 monthly issues of "National Geographic" per year. The magazine contains articles about geography, popular science, world history, culture, current events and photography of places and things all over the world and universe. "National Geographic" magazine is currently published in 40 local-language editions in many countries around the world. Combined English and other language circulation is around 6.8 million monthly, with some 60 million readers.
Other publications.
In addition to its flagship magazine, the Society publishes several other periodicals:
The Society also runs an online news outlet called "National Geographic Daily News".
Additionally, the Society publishes atlases, books, and maps. It previously published and co-published other magazines, including "National Geographic Adventure", "National Geographic Research" (a scientific journal), and others, and continues to publish special issues of various magazines.
Films and television.
National Geographic Films.
National Geographic Films is a wholly owned taxable subsidiary of the National Geographic Society. Films it has produced include:
Television.
Programs by the National Geographic Society are also broadcast on television. National Geographic television specials and series have been aired on PBS and other networks in the United States and globally for many years. The "Geographic" series in the U.S. started on CBS in 1964, moved to ABC in 1973, shifted to PBS (produced by WQED, Pittsburgh) in 1975, shifted to NBC in 1995, and returned to PBS in 2000.
National Geographic Channel, launched in January 2001, is a joint venture of National Geographic and Fox Cable Networks. It has featured stories on numerous scientific figures such as Jacques Cousteau, Jane Goodall, and Louis Leakey that not only featured their work but as well helped make them world-famous and accessible to millions. Most of the specials were narrated by various actors, including Glenn Close, Linda Hunt, Stacy Keach, Richard Kiley, Burgess Meredith, Susan Sarandon, Alexander Scourby, Martin Sheen, and Peter Strauss. The specials' theme music, by Elmer Bernstein, was also adopted by the National Geographic Channel. The National Geographic Channel has begun to launch a number of sub-branded channels in international markets, such as Nat Geo Adventure, Nat Geo Music, and Nat Geo Wild.
The National Geographic Partners expanded joint venture with 21st Century Fox (to be controlled by the latter) will be concluded in mid-November 2015 with the new for-profit organization to own a 73% stake of the NG affiliated television networks (National Geographic-branded cable TV channels), most of which were already co-owned (with the percentage information not readily available), as per earlier joint ventures with Fox.
National Geographic Museum.
The Society operates the National Geographic Museum, located at 1145 17th Street, NW (17th and M), in Washington, D.C. The museum features changing photography exhibitions featuring the work of National Geographic explorers, photographers, and scientists. There are also changing exhibits related to natural history, culture, history or society.
Support for research and projects.
The Society has helped sponsor many expeditions and research projects over the years, including:
The Society supports many socially based projects including AINA, a Kabul-based organization dedicated to developing an independent Afghan media, which was founded by one of the Society's most famous photographers, Reza.
The Society also organizes the National Geographic Bee, an annual geographic contest for U.S. fourth- through eighth-graders. About 4 million students a year begin the geography competition locally, which culminates in a national competition of the winners of each state each May in Washington, D.C. Journalist Soledad O'Brien is the moderator of the Bee. She succeeded Alex Trebek, host of "Jeopardy!", who moderated the final round of the competition for 25 years, from its inception in 1989 to 2013. Every two years, the Society conducts an international geography competition of competing teams from all over the world. The most recent was held in St. Petersburg, Russia, in July 2013, and had representatives from 19 national teams. The team from the United States emerged as the winner, with teams from Canada and India in second and third place.
Awards.
Hubbard Medal.
The Hubbard Medal is awarded by the National Geographic Society for distinction in exploration, discovery, and research. The medal is named for Gardiner Greene Hubbard, the first National Geographic Society president. The Hubbard Medal has been presented 35 times as of 2010, the most recent award going to Don Walsh.
Alexander Graham Bell Medal.
The National Geographic Society also awards, rarely, the Alexander Graham Bell Medal, for exceptional contributions to geographic research. The award is named after Alexander Graham Bell, scientist, inventor and the second president of the NGS. Up to mid-2011, the medal has been twice presented:

</doc>
<doc id="21556" url="https://en.wikipedia.org/wiki?curid=21556" title="Norns">
Norns

The Norns (Old Norse: "norn", plural: "nornir") in Norse mythology are female beings who rule the destiny of gods and men. They roughly correspond to other controllers of humans' destiny, the Fates, elsewhere in European mythology.
According to Snorri Sturluson's interpretation of the "Völuspá", the three most important norns, Urðr (Wyrd), Verðandi and Skuld, come out from a hall standing at the Well of Urðr (well of fate). They draw water from the well and take sand that lies around it, which they pour over Yggdrasill so that its branches will not rot. These three norns are described as powerful maiden giantesses (Jotuns) whose arrival from Jötunheimr ended the golden age of the gods. They may be the same as the maidens of Mögþrasir who are described in "Vafþrúðnismál" (see below).
Beside these three, there are many other norns who arrive when a person is born in order to determine his or her future. In the pre-Christian Norse societies, norns were thought to have visited newborn children. There were both malevolent and benevolent norns: the former caused all the malevolent and tragic events in the world while the latter were kind and protective goddesses. Recent research has discussed the relation between the myths associated with norns and valkyries and traveling Völvas ("seiðr"-workers).
Within skaldic references, norns are often seen as negative beings that are mostly associated with transitional situations such as violent death and battle. In Egil's Saga, Kveldulf composes a poem lamenting the loss of his eldest son Thorolf. Here, what is stressed is the personal tragedy felt by Kveldulf and the sense that what happened was out of his control or in the hands of fate. It is presumed that Óðinn has chosen Thorolf to be among his einherjar so Bek-Pedersen suggests that since Óðinn has caused the death then the norn has caused the emotional turmoil. Another negative aspect associated with the norns is that they are associated with death (see Skaldic Poetry). Not all aspects of the norns were negative, however, as they were associated with life and birth as well (see "Helgakviða Hundingsbana I" and "Gylfaginning").
Etymology.
The origin of the name "norn" is uncertain, it may derive from a word meaning "to twine" and which would refer to their twining the thread of fate. Bek-Pedersen suggests that the word "norn" has relation to the Swedish dialect word "norna (nyrna)", a verb that means "secretly communicate". This relates to the perception of norns as shadowy, background figures who only really ever reveal their fateful secrets to men as their fates come to pass.
The name "Urðr" (Old English Wyrd, Weird) means "fate". It should be noted that "wyrd" and "urðr" are etymological cognates, which does not guarantee that "wyrd" and "urðr" share the same semantic quality of "fate" over time. Both "Urðr" and "Verðandi" are derived from the Old Norse verb "verða", "to be". While "Urðr" derives from the past tense ("that which became or happened"), "Verðandi" derives from the present tense of "verða" ("that which is happening"). "Skuld" is derived from the Old Norse verb "skulla", "need/ought to be/shall be"; its meaning is "that which should become, or that needs to occur".
Relation to other Germanic female deities.
There is no clear distinction between norns, fylgjas, hamingjas and valkyries, nor with the generic term dísir. Moreover, artistic license permitted such terms to be used for mortal women in Old Norse poetry. To quote Snorri Sturluson's "Skáldskaparmál" on the various names used for women:
These unclear distinctions among norns and other Germanic female deities are discussed in Bek-Pedersen's book "Norns in old Norse Mythology" and in Lionarons article "Disir, Valkyries, Volur, and Norns: The Weise Frauen of the Deutsche Mythologie".
Attestations.
There are a number of surviving Old Norse sources that relate to the norns. The most important sources are the Prose Edda and the Poetic Edda. The latter contains pagan poetry where the norns are frequently referred to, while the former contains, in addition to pagan poetry, retellings, descriptions and commentaries by the 12th and 13th century Icelandic chieftain and scholar Snorri Sturluson.
Skaldic poetry.
A skaldic reference to the norns appears in Hvini's poem in "Ynglingatal" 24 found in "Ynglingasaga" 47, where King Halfdan is put to rest by his men at Borró. This reference brings in the phrase ""norna dómr"" which means "judgment of the nornir". In most cases, when the norns pass judgment, it means death to those who have been judged - in this case, Halfdan. Along with being associated with being bringers of death, Bek-Pedersen suggests that this phrase brings in a quasi-legal aspect to the nature of the norns. This legal association is employed quite frequently within skaldic and eddic sources. This phrase can also be seen as a threat, as death is the final and inevitable decision that the norns can make with regard to human life.
Poetic Edda.
The Poetic Edda is valuable in representing older material in poetry from which Snorri tapped information in the "Prose Edda". Like "Gylfaginning", the "Poetic Edda" mentions the existence of many lesser norns beside the three main norns. Moreover, it also agrees with "Gylfaginning" by telling that they were of several races and that the dwarven norns were the daughters of Dvalin. It also suggests that the three main norns were giantesses (female Jotuns).
"Fáfnismál" contains a discussion between the hero Sigurd and the dragon Fafnir who is dying from a mortal wound from Sigurd. The hero asks Fafnir of many things, among them the nature of the norns. Fafnir explains that they are many and from several races:
It appears from "Völuspá" and "Vafþrúðnismál" that the three main norns were not originally goddesses but giantesses (Jotuns), and that their arrival ended the early days of bliss for the gods, but that they come for the good of mankind.
"Völuspá" relates that three giantesses of huge might are reported to have arrived to the gods from Jotunheim:
"Vafþrúðnismál" probably refers to the norns when it talks of maiden giantesses who arrive to protect the people of earth as protective spirits (hamingjas):
The "Völuspá" contains the names of the three main Norns referring to them as maidens like "Vafþrúðnismál" probably does:
"Helgakviða Hundingsbana I".
The norns visited each newly born child to allot his or her future, and in "Helgakviða Hundingsbana I", the hero Helgi Hundingsbane has just been born and norns arrive at the homestead:
"Helgakviða Hundingsbana II".
In "Helgakviða Hundingsbana II", Helgi Hundingsbane blames the norns for the fact that he had to kill Sigrún's father Högni and brother Bragi in order to wed her:
"Reginsmál".
Like Snorri Sturluson stated in "Gylfaginning", people's fate depended on the benevolence or the malevolence of particular norns. In "Reginsmál", the water dwelling dwarf Andvari blames his plight on an evil norn, presumably one of the daughters of Dvalin:
"Sigurðarkviða hin skamma".
Another instance of Norns being blamed for an undesirable situation appears in "Sigurðarkviða hin skamma", where the valkyrie Brynhild blames malevolent norns for her long yearning for the embrace of Sigurd:
"Guðrúnarkviða II".
Brynhild's solution was to have Gunnarr and his brothers, the lords of the Burgundians, kill Sigurd and afterwards to commit suicide in order to join Sigurd in the afterlife. Her brother Atli (Attila the Hun) avenged her death by killing the lords of the Burgundians, but since he was married to their sister Guðrún, Atli would soon be killed by her. In "Guðrúnarkviða II", the Norns actively enter the series of events by informing Atli in a dream that his wife would kill him. The description of the dream begins with this stanza:
"Guðrúnarhvöt".
After having killed both her husband Atli and their sons, Guðrún blames the Norns for her misfortunes, as in "Guðrúnarhvöt", where Guðrún talks of trying to escaping the wrath of the norns by trying to kill herself:
"Hamðismál".
"Guðrúnarhvöt" deals with how Guðrún incited her sons to avenge the cruel death of their sister Svanhild. In "Hamðismál", her sons' expedition to the Gothic king Ermanaric to exact vengeance is fateful. Knowing that he is about to die at the hands of the Goths, her son Sörli talks of the cruelty of the norns:
"Sigrdrífumál".
Since the norns were beings of ultimate power who were working in the dark, it should be no surprise that they could be referred to in charms, as they are by Sigrdrífa in "Sigrdrífumál":
"Prose Edda".
In the part of Snorri Sturluson's "Prose Edda" which is called "Gylfaginning", Gylfi, the king of Sweden, has arrived at Valhalla calling himself Gangleri. There, he receives an education in Norse mythology from what is Odin in the shape of three men. They explain to Gylfi that there are three main norns, but also many others of various races, æsir, elves and dwarves:
The three main norns take water out of the well of Urd and water Yggdrasil:
Snorri furthermore informs the reader that the youngest norn, Skuld, is in effect also a valkyrie, taking part in the selection of warriors from the slain:
Legendary sagas.
Some of the legendary sagas also contain references to the norns. The "Hervarar saga" contains a poem named "Hlöðskviða", where the Gothic king Angantýr defeats a Hunnish invasion led by his Hunnish half-brother Hlöðr. Knowing that his sister, the shieldmaiden Hervör, is one of the casualties, Angantýr looks at his dead brother and laments the cruelty of the norns:
In younger legendary sagas, such as "Norna-Gests þáttr" and "Hrólfs saga kraka", the norns appear to have been synonymous with völvas (witches, female shamans). In "Norna-Gests þáttr", where they arrive at the birth of the hero to shape his destiny, the norns are not described as weaving the web of fate, instead "Norna" appears to be interchangeable and possibly a synonym of "vala" (völva).
One of the last legendary sagas to be written down, the "Hrólfs saga kraka" talks of the norns simply as evil witches. When the evil half-elven princess Skuld assembles her army to attack Hrólfr Kraki, it contains in addition to undead warriors, elves and norns.
Runic inscription N 351 M.
The belief in the norns as bringers of both gain and loss would last beyond Christianization, as testifies the runic inscription N 351 M from the Borgund stave church:
Franks Casket.
Three women carved on the right panel of Franks Casket, an Anglo-Saxon whalebone chest from the eighth century, have been identified by some scholars as being three norns.
Theories.
A number of theories have been proposed regarding the norns.
Matres and Matrones.
The Germanic Matres and Matrones, female deities venerated in North-West Europe from the 1st to the 5th century AD depicted on votive objects and altars almost entirely in groups of three from the first to the fifth century AD have been proposed as connected with the later Germanic dísir, valkyries, and norns, potentially stemming from them.
Three norns.
Theories have been proposed that there is no foundation in Norse mythology for the notion that the three main norns should each be associated exclusively with the past, the present, and the future; rather, all three represent "destiny" as it is twined with the flow of time. Moreoever, theories have been proposed that the idea that there are three main norns may be due to a late influence from Greek and Roman mythology, where there are also spinning fate goddesses (Moirai and Parcae).
Appearances in media and popular culture.
Music.
Viking death metal band Amon Amarth has an album titled "Fate of Norns". The band itself has many songs involving Norse mythology.
Opera.
Norns feature in the prologue of Richard Wagner's opera Götterdämmerung.
Television.
The 1990s Disney TV series "Gargoyles" features three sisters, referred to by the cast as the "Weird Sisters", that are inferred to be Norns.
A norn was featured in the fourth season of . After his arrival in the Norselands, the norn tasks Hercules with averting Ragnarok. Since the Fates had already been featured in the series, only a single norn appears, who paints history in a book.
In the 2010 series "Lost Girl", there was a Norn who could be petitioned to change fate, for a price. Her price was always the one thing her petitioner values most, whether they realize it or not.
Comics.
The Norns appear in Marvel Comics, usually in stories featuring the Norse inspired superhero Thor.
Anime and manga.
The main love interest of Oh My Goddess! is the Norn Verðandi, rendered as Belldandy. Her elder sister Urðr (rendered as Urd) and younger sister Skuld also show up, living with the protagonist Keichii Morisato and their sister Belldandy. Aside from sticking loosely to the theme of Belldandy representing the present, Urd the past and Skuld the future, they are only loosely related to their mythic namesakes in this media.
The terminals that Yggdrasil from Digimon created for the New Digital World experiments consisting 3 layers are named Ulud, Versandi, and Skuld which are representing for past, present, and future. Ulud Urðr is a past plain which is a volcanic wasteland, inhabited by Dinosaur type and draconic Digimon. Versandi Verðandi is the "present" region which is a world of lush greenery and is home to beast, bird, plant and other nature Digimon. Skuld is the "future" region, a high-tech city where machine and insect Digimon inhabit.
The three Norns also appear as antagonists in Mythical Detective Loki Ragnarok, along with various other figures from Norse mythology, including Thor, Heimdallr, Freyr, Freyja, Fenrir, Jormungandr, and the eponymous Loki.
In the Calibur arc of Sword Art Online, Urðr appears to Kirito's party and gives them a quest to retrieve the sword Excalibur from Thrymheim before the last beast-type Evil God is killed, restoring Jötunheimr to its former glory. Upon successful completion of this quest, Urðr reappears to Kirito's party, along with her sisters Verdandi and Skuld. They thank them for completing the quest, and allow them to keep Excalibur. Other figures and elements from Norse mythology also appear in this arc, including Thrym, Freyja, Thor, and Mjölnir.

</doc>
<doc id="21557" url="https://en.wikipedia.org/wiki?curid=21557" title="Niflheim">
Niflheim

Niflheim (or Niflheimr) ("Mist Home", the "Abode of Mist" or "Mist World" , or probably "world of the darkness" according to the Oxford English Dictionary) is one of the Nine Worlds and is a location in Norse mythology which sometimes overlaps with the notions of Niflhel and Hel. The name "Niflheimr" only appears in two extant sources: "Gylfaginning" and the much-debated "Hrafnagaldr Óðins".
Niflheim was primarily a realm of primordial ice and cold, with the frozen river of Elivágar and the well of Hvergelmir, from which come all the rivers. According to "Gylfaginning", Niflheim was one of the two primordial realms, the other one being Muspelheim, the realm of fire. Between these two realms of cold and heat, creation began when its waters mixed with the heat of Muspelheim to form a "creating steam". Later, it became the abode of Hel, a goddess daughter of Loki, and the afterlife for her subjects, those who did not die a heroic or notable death.
Etymology.
"Nifl" (whence the Icelandic "nifl") being cognate with the Anglo-Saxon "Nifol" ("dark"), Dutch "nevel" and German "nebel" ("fog").
"Gylfaginning".
In "Gylfaginning" by Snorri Sturluson, Gylfi, the king of ancient Scandinavia, receives an education in Norse mythology from Odin in the guise of three men. Gylfi learns from Odin (as "Jafnhárr") that Niflheimr was the first world to be created after Muspelheim:
Odin (as "Þriði") further tells Gylfi that it was when the ice from Niflheimr met the flames from Muspelheimr that creation began and Ymir was formed:
In relation to the world tree Yggdrasill, "Jafnhárr" (Odin) tells Gylfi that Jötunheimr is located under the second root, where Ginnungagap ("Yawning Void") once was:
Gylfi is furthermore informed that when Loki had engendered Hel, she was cast into Niflheimr by Odin:
Hel thus became the mistress of the world of those dead in disease and old age. This is the only instance in which Niflheim and Hel are equated (the Poetic Edda mentions Hel but doesn't say anything about Niflheim).
However, there is some confusion in the different versions of the manuscript, with some of them saying Niflheim where others say Niflhel (the lowest level of Hel). Thus in the passage about the last destination of the "jötunn" who was killed by Thor after he had built Asgard:
"Hrafnagaldr Óðins".
In "Hrafnagaldr Óðins", there is a brief mention of Niflheimr as a location in the North, towards which the sun (Alfr's illuminator) chased the night as it rose:

</doc>
<doc id="21558" url="https://en.wikipedia.org/wiki?curid=21558" title="Nanna">
Nanna

Nanna may refer to:

</doc>
<doc id="21559" url="https://en.wikipedia.org/wiki?curid=21559" title="NASDAQ">
NASDAQ

The NASDAQ Stock Market (), commonly known as the NASDAQ (currently stylized as Nasdaq), is an American stock exchange. It is the second-largest exchange in the world by market capitalization, behind only the New York Stock Exchange. The exchange platform is owned by The NASDAQ OMX Group, which also owns the OMX stock market network and several other US stock and options exchanges.
History.
In 1972, "NASDAQ" stood for National Association of Securities Dealers Automated Quotations. NASDAQ was founded in 1971 by the National Association of Securities Dealers (NASD), which divested itself of NASDAQ in a series of sales in 2000 and 2001. NASDAQ is owned and operated by The NASDAQ OMX Group, the stocks of which were listed on its own stock exchange beginning July 2, 2002, under the ticker symbol .
When the NASDAQ began trading on February 8, 1971, it was the world's first electronic stock market. At first, it was merely a "quotation system" and did not provide a way to perform electronic trades. The NASDAQ helped lower the spread (the difference between the bid price and the ask price of the stock) but was unpopular among brokerages which made much of their money on the spread.
NASDAQ eventually assumed the majority of major trades formerly executed by the over-the-counter (OTC) system of trading, although there are still numerous securities traded in this fashion. As late as 1987, the NASDAQ exchange was still commonly referred to as "OTC" in media and also in the monthly Stock Guides issued by Standard & Poor's Corporation.
Over the years, NASDAQ became more of a stock market by adding trade and volume reporting and automated trading systems. NASDAQ was also the first stock market in the United States to start trading online, highlighting NASDAQ-traded companies (usually in technology) and closing with the declaration that NASDAQ is "the stock market for the next hundred years." Its main index is the NASDAQ Composite, which has been published since its inception. However, its exchange-traded fund tracks the large-cap NASDAQ-100 index, which was introduced in 1985 alongside the NASDAQ 100 Financial Index.
Until 1987, most trading occurred via the telephone. During the October 1987 stock market crash, however, market makers often did not answer their phones. To remedy this, the Small Order Execution System (SOES) was established. SOES provides an electronic method for dealers to enter their trades. NASDAQ requires market makers to honor trades executed using SOES.
In 1992, NASDAQ joined with the London Stock Exchange to form the first intercontinental linkage of securities markets. The National Association of Securities Dealers spun off NASDAQ in 2000 to form a public company, the NASDAQ Stock Market, Inc.
In 2006, the status of NASDAQ was changed from a stock market to a licensed national securities exchange.
To qualify for listing on the exchange, a company must be registered with the United States Securities and Exchange Commission (SEC), must have at least three market makers (financial firms that act as brokers or dealers for specific securities) and must meet minimum requirements for assets, capital, public shares, and shareholders.
In February 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Börse, speculation developed that NASDAQ OMX and Intercontinental Exchange (ICE) could mount a counter-bid of their own for NYSE. NASDAQ OMX could be looking to acquire the American exchange's cash equities business, ICE the derivatives business. As of the time of the speculation, "NYSE Euronext’s market value was $9.75 billion. Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion." Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Merc to join in what would probably have to be, if it proceeded, an $11–12 billion counterbid.
The European Association of Securities Dealers Automatic Quotation System (EASDAQ) was founded originally as a European equivalent to NASDAQ. It was purchased by NASDAQ in 2001 and became NASDAQ Europe. Operations were shut down, however, as a result of the burst of the dot-com bubble. In 2007, NASDAQ Europe was revived as Equiduct, and is currently operating under Börse Berlin.
On June 18, 2012, NASDAQ became a founding member of the United Nations Sustainable Stock Exchanges initiative on the eve of the United Nations Conference on Sustainable Development (Rio+20).
In 2013, NASDAQ was approached by private equity firm Carlyle Group about taking the exchange operator private, but the talks fell apart over a disagreement on price.
Quote availability.
NASDAQ quotes are available at three levels:
Trading schedule.
NASDAQ has a pre-market session from 4:00 AM to 9:30 AM Eastern, a normal trading session from 9:30 AM to 4:00 PM, and a post-market session from 4:00 PM to 8:00 PM.
Market tiers.
NASDAQ has three different market tiers:
Average annualized growth rate.
As of June 2015, the NASDAQ has had an average annualized growth rate of 9.24% since its opening in February 1971. Since the end of the recession in June 2009 however, it has increased by 18.29% per year.

</doc>
<doc id="21560" url="https://en.wikipedia.org/wiki?curid=21560" title="New York Stock Exchange">
New York Stock Exchange

The New York Stock Exchange (NYSE), sometimes known as the "Big Board", is an American stock exchange located at 11 Wall Street, Lower Manhattan, New York City, New York. It is by far the world's largest stock exchange by market capitalization of its listed companies at US$19.69 trillion as of May 2015. The average daily trading value was approximately 169 billion in 2013.
The NYSE trading floor is located at 11 Wall Street and is composed of 21 rooms used for the facilitation of trading. A fifth trading room, located at 30 Broad Street, was closed in February 2007. The main building and the 11 Wall Street building were designated National Historic Landmarks in 1978.
The NYSE is owned by Intercontinental Exchange, an American holding company it also lists (). Previously, it was part of NYSE Euronext (NYX), which was formed by the NYSE's 2007 merger with the fully electronic stock exchange Euronext. NYSE and Euronext now operate as divisions of Intercontinental Exchange.
The NYSE has been the subject of several lawsuits regarding fraud or breach of duty and in 2004 was sued by its former CEO for breach of contract and defamation.
History.
The earliest recorded organization of securities trading in New York among brokers directly dealing with each other can be traced to the Buttonwood Agreement. Previously securities exchange had been intermediated by the auctioneers who also conducted more mundane auctions of commodities such as wheat and tobacco. On May 17, 1792 twenty four brokers signed the Buttonwood Agreement which set a floor commission rate charged to clients and bound the signers to give preference to the other signers in securities sales. The earliest securities traded were mostly governmental securities such as War Bonds from the Revolutionary War and First Bank of the United States stock, although Bank of New York stock was a non-governmental security traded in the early days.
In 1817 the stockbrokers of New York operating under the Buttonwood Agreement instituted new reforms and reorganized. After sending a delegation to Philadelphia to observe the organization of their board of brokers, restrictions on manipulative trading were adopted as well as formal organs of governance. After re-forming as the New York Stock and Exchange Board the broker organization began renting out space exclusively for securities trading, which previously had been taking place at the Tontine Coffee House. Several locations were used between 1817 and 1865, when the present location was adopted.
The invention of the electrical telegraph consolidated markets, and New York's market rose to dominance over Philadelphia after weathering some market panics better than other alternatives. The Civil War greatly stimulated speculative securities trading in New York. By 1869 membership had to be capped, and has been sporadically increased since. The latter half of the nineteenth century saw rapid growth in securities trading.
Securities trade in the latter nineteenth and early twentieth centuries was prone to panics and crashes. Government regulation of securities trading was eventually seen as necessary, with arguably the most dramatic changes occurring in the 1930s after a major stock market crash precipitated an economic depression.
The Stock Exchange Luncheon Club was situated on the seventh floor from 1898 until its closure in 2006.
The main building, located at 18 Broad Street, between the corners of Wall Street and Exchange Place, was designated a National Historic Landmark in 1978, as was the 11 Wall Street building.
The NYSE announced its plans to merge with Archipelago on April 21, 2005, in a deal intended to reorganize the NYSE as a publicly traded company. NYSE's governing board voted to merge with rival Archipelago on December 6, 2005, and become a for-profit, public company. It began trading under the name NYSE Group on March 8, 2006. A little over one year later, on April 4, 2007, the NYSE Group completed its merger with Euronext, the European combined stock market, thus forming the NYSE Euronext, the first transatlantic stock exchange.
Wall Street is the leading US money center for international financial activities and the foremost US location for the conduct of wholesale financial services. "It comprises a matrix of wholesale financial sectors, financial markets, financial institutions, and financial industry firms" (Robert, 2002). The principal sectors are securities industry, commercial banking, asset management, and insurance.
Prior to the acquisition of NYSE Euronext by the ICE in 2013, Marsh Carter was the Chairman of the NYSE and the CEO was Duncan Niederauer. Presently, the Chairman is Jeffrey Sprecher.
Notable events.
The exchange was closed shortly after the beginning of World War I (July 31, 1914), but it partially re-opened on November 28 of that year in order to help the war effort by trading bonds, and completely reopened for stock trading in mid-December.
On September 16, 1920, a bomb exploded on Wall Street outside the NYSE building, killing 33 people and injuring more than 400. The perpetrators were never found. The NYSE building and some buildings nearby, such as the JP Morgan building, still have marks on their façades caused by the bombing.
The Black Thursday crash of the Exchange on October 24, 1929, and the sell-off panic which started on Black Tuesday, October 29, are often blamed for precipitating the Great Depression. In an effort to try to restore investor confidence, the Exchange unveiled a fifteen-point program aimed to upgrade protection for the investing public on October 31, 1938.
On October 1, 1934, the exchange was registered as a national securities exchange with the U.S. Securities and Exchange Commission, with a president and a thirty-three-member board. On February 18, 1971, the non-profit corporation was formed, and the number of board members was reduced to twenty-five.
One of Abbie Hoffman's well-known publicity stunts took place in 1967, when he led members of the Yippie movement to the Exchange's gallery. The provocateurs hurled fistfuls of dollars toward the trading floor below. Some traders booed, and some laughed and waved. Three months later the stock exchange enclosed the gallery with bulletproof glass. Hoffman wrote a decade later, "We didn't call the press; at that time we really had no notion of anything called a media event".
On October 19, 1987, the Dow Jones Industrial Average (DJIA) dropped 508 points, a 22.6% loss in a single day, the second-biggest one-day drop the exchange had experienced. Black Monday was followed by Terrible Tuesday, a day in which the Exchange's systems did not perform well and some people had difficulty completing their trades.
Subsequently, there was another major drop for the Dow on October 13, 1989—the Mini-Crash of 1989. The crash was apparently caused by a reaction to a news story of a $6.75 billion leveraged buyout deal for UAL Corporation, the parent company of United Airlines, which broke down. When the UAL deal fell through, it helped trigger the collapse of the junk bond market causing the Dow to fall 190.58 points, or 6.91 percent.
Similarly, there was a panic in the financial world during the year of 1997; the Asian Financial Crisis. Like the fall of many foreign markets, the Dow suffered a 7.18% drop in value (554.26 points) on October 27, 1997, in what later became known as the 1997 Mini-Crash but from which the DJIA recovered quickly. This was the first time that the "circuit breaker" rule had operated.
On January 26, 2000, an altercation during filming of the music video for "Sleep Now in the Fire", which was directed by Michael Moore, caused the doors of the exchange to be closed and the band Rage Against the Machine to be escorted from the site by security after band members attempted to gain entry into the exchange.
In the aftermath of the September 11, 2001 terrorist attacks, the NYSE was closed for 4 trading sessions, resuming on Monday, September 17, one of the rare times the NYSE was closed for more than one session and only the third time since March 1933.
On May 6, 2010, the Dow Jones Industrial Average posted its largest intraday percentage drop since the October 19, 1987, crash, with a 998-point loss later being called the 2010 Flash Crash (as the drop occurred in minutes before rebounding). The SEC and CFTC published a report on the event, although it did not come to a conclusion as to the cause. The regulators found no evidence that the fall was caused by erroneous ("fat finger") orders.
On October 29, 2012, the stock exchange was shut down for 2 days due to Hurricane Sandy.
The last time the stock exchange was closed due to weather for a full two days was on March 12 and 13 in 1888.
On May 1, 2014, the stock exchange was fined $4.5 million by the Securities and Exchange Commission to settle charges it violated market rules.
On August 14, 2014, Berkshire Hathaway's A Class shares, the highest priced shares on the NYSE, hit $200,000 a share for the first time.
On July 8, 2015, technical issues affected the stock exchange, halting trading at 11:32 am ET. The NYSE reassured stock traders that the outage was "not a result of a cyber breach", and the Department of Homeland Security confirmed that there was "no sign of malicious activity". Trading eventually resumed at 3:10 pm ET the same day.
Trading.
The New York Stock Exchange (sometimes referred to as "the Big Board") provides a means for buyers and sellers to trade shares of stock in companies registered for public trading. The NYSE is open for trading Monday through Friday from 9:30 am – 4:00 pm ET, with the exception of holidays declared by the Exchange in advance.
The NYSE trades in a continuous auction format, where traders can execute stock transactions on behalf of investors. They will gather around the appropriate post where a specialist broker, who is employed by a NYSE member firm (that is, he/she is not an employee of the New York Stock Exchange), acts as an auctioneer in an open outcry auction market environment to bring buyers and sellers together and to manage the actual auction. They do on occasion (approximately 10% of the time) facilitate the trades by committing their own capital and as a matter of course disseminate information to the crowd that helps to bring buyers and sellers together. The auction process moved toward automation in 1995 through the use of wireless hand held computers (HHC). The system enabled traders to receive and execute orders electronically via wireless transmission. On September 25, 1995, NYSE member Michael Einersen, who designed and developed this system, executed 1000 shares of IBM through this HHC ending a 203-year process of paper transactions and ushering in an era of automated trading.
As of January 24, 2007, all NYSE stocks can be traded via its electronic hybrid market (except for a small group of very high-priced stocks). Customers can now send orders for immediate electronic execution, or route orders to the floor for trade in the auction market. In the first three months of 2007, in excess of 82% of all order volume was delivered to the floor electronically. NYSE works with US regulators like the SEC and CFTC to coordinate risk management measures in the electronic trading environment through the implementation of mechanisms like circuit breakers and liquidity replenishment points.
Until 2005, the right to directly trade shares on the exchange was conferred upon owners of the 1366 "seats". The term comes from the fact that up until the 1870s NYSE members sat in chairs to trade. In 1868, the number of seats was fixed at 533, and this number was increased several times over the years. In 1953, the number of seats was set at 1,366. These seats were a sought-after commodity as they conferred the ability to directly trade stock on the NYSE, and seat holders were commonly referred to as members of the NYSE. The Barnes family is the only known lineage to have five generations of NYSE members: Winthrop H. Barnes (admitted 1894), Richard W.P. Barnes (admitted 1926), Richard S. Barnes (admitted 1951), Robert H. Barnes (admitted 1972), Derek J. Barnes (admitted 2003). Seat prices varied widely over the years, generally falling during recessions and rising during economic expansions. The most expensive inflation-adjusted seat was sold in 1929 for $625,000, which, today, would be over six million dollars. In recent times, seats have sold for as high as $4 million in the late 1990s and as low as $1 million in 2001. In 2005, seat prices shot up to $3.25 million as the exchange entered into an agreement to merge with Archipelago and become a for-profit, publicly traded company. Seat owners received $500,000 in cash per seat and 77,000 shares of the newly formed corporation. The NYSE now sells one-year licenses to trade directly on the exchange. Licenses for floor trading are available for $40,000 and a license for bond trading is available for as little as $1,000 as of 2010. Neither are resell-able, but may be transferable in during the change of ownership of a cooperation holding a trading license.
Following the Black Monday market crash in 1987, NYSE imposed trading curbs to reduce market volatility and massive panic sell-offs. Following the 2011 rule change, at the start of each trading day, the NYSE sets three circuit breaker levels at levels of 7% (Level 1), 13% (Level 2), and 20% (Level 3) of the average closing price of the S&P 500 for the preceding trading day. Level 1 and Level 2 declines result in a 15-minute trading halt unless they occur after 3:25pm, when no trading halts apply. A Level 3 decline results in trading being suspended for the remainder of the day. (The biggest one-day decline in the S&P 500 since 1987 was the 9.0% drop on October 15, 2008.)
NYSE Composite Index.
In the mid-1960s, the NYSE Composite Index () was created, with a base value of 50 points equal to the 1965 yearly close. This was done to reflect the value of all stocks trading at the exchange instead of just the 30 stocks included in the Dow Jones Industrial Average. To raise the profile of the composite index, in 2003 the NYSE set its new base value of 5,000 points equal to the 2002 yearly close. Its close at the end of 2013 was 10,400.32.
Merger, acquisition, and control.
On February 15, 2011 NYSE and Deutsche Börse announced their merger to form a new company, as yet unnamed, wherein Deutsche Börse shareholders will have 60% ownership of the new entity, and NYSE Euronext shareholders will have 40%.
On February 1, 2012, the European Commission blocked the merger of NYSE with Deutsche Börse, after commissioner Joaquin Almunia stated that the merger "would have led to a near-monopoly in European financial derivatives worldwide". Instead, Deutsche Börse and NYSE will have to sell either their Eurex derivatives or LIFFE shares in order to not create a monopoly. On February 2, 2012, NYSE Euronext and Deutsche Börse agreed to scrap the merger.
In April 2011, Intercontinental Exchange (ICE), an American futures exchange, and NASDAQ OMX Group had together made an unsolicited proposal to buy NYSE Euronext for approximately , a deal in which NASDAQ would have taken control of the stock exchanges. NYSE Euronext rejected this offer twice, but it was finally terminated after the United States Department of Justice indicated their intention to block the deal due to antitrust concerns.
In December 2012, ICE had proposed to buy NYSE Euronext in a stock swap with a valuation of $8 billion. NYSE Euronext shareholders would receive either $33.12 in cash, or $11.27 in cash and approximately a sixth of a share of ICE. The Chairman and CEO of ICE, Jeffrey Sprecher, will retain those positions, but four members of the NYSE Board of Directors will be added to the ICE board.
Opening and closing bells.
The NYSE's opening and closing bells mark the beginning and the end of each trading day. The 'opening bell' is rung at 9:30 AM ET to mark the start of the day's trading session. At 4 PM ET the 'closing bell' is rung and trading for the day stops. There are bells located in each of the four main sections of the NYSE that all ring at the same time once a button is pressed. There are three buttons that control the bells, located on the control panel behind the podium which overlooks the trading floor. The main bell, which is rung at the beginning and end of the trading day, is controlled by a green button. The second button, colored orange, activates a single-stroke bell that is used to signal a moment of silence. A third, red button controls a backup bell which is used in case the main bell fails to ring.
History.
The signal to start and stop trading wasn't always a bell. The original signal was a gavel (which is still in use today along with the bell), but during the late 1800s, the NYSE decided to switch the gavel for a gong to signal the day's beginning and end. After the NYSE changed to its present location at 18 Broad Street in 1903, the gong was switched to the bell format that is currently being used.
A common sight today is the highly publicized events in which a celebrity or executive from a corporation stands behind the NYSE podium and pushes the button that signals the bells to ring. Many consider the act of ringing the bells to be quite an honor and a symbol of a lifetime of achievement. Furthermore, due to the amount of coverage that the opening/closing bells receive, many companies coordinate new product launches and other marketing-related events to start on the same day as when the company's representatives ring the bell. This daily tradition wasn't always this highly publicized either. In fact, it was only in 1995 that the NYSE began having special guests ring the bells on a regular basis. Prior to that, ringing the bells was usually the responsibility of the exchange's floor managers.
Notable bell-ringers.
Many of the people who ring the bell are business executives whose companies trade on the exchange. However, there have also been many famous people from outside the world of business that have rung the bell. Athletes such as Joe DiMaggio of the New York Yankees and Olympic swimming champion Michael Phelps, entertainers such as rapper Snoop Dogg and members of the band Kiss, and politicians such as Mayor of New York City Rudy Giuliani and President of South Africa Nelson Mandela have all had the honor of ringing the bell. Two United Nations Secretaries General have also rung the bell. On April 27, 2006, Secretary-General Kofi Annan rang the opening bell to launch the United Nations Principles for Responsible Investment. On July 24, 2013, Secretary-General Ban Ki-moon rang the closing bell to celebrate the NYSE joining the United Nations Sustainable Stock Exchanges initiative.
In addition there have been many bell-ringers who are famous for heroic deeds, such as members of the New York police and fire departments following the events of 9/11, members of the United States Armed Forces serving overseas, and participants in various charitable organizations.
There have also been several fictional characters that have rung the bell, including Mickey Mouse, the Pink Panther, Mr. Potato Head, the Aflac Duck, and Darth Vader.
References.
Notes
Bibliography

</doc>
<doc id="21561" url="https://en.wikipedia.org/wiki?curid=21561" title="Nanoengineering">
Nanoengineering

Nanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre, a unit of measurement equalling one billionth of a meter.
Nanoengineering is largely a synonym for nanotechnology, but emphasizes the engineering rather than the pure science aspects of the field.
Degree programs.
The first nanoengineering program in the world was started at the University of Toronto within the Engineering Science program as one of the Options of study in the final years. In 2003, the Lund Institute of Technology started a program in Nanoengineering. In 2004, the College of Nanoscale Science and Engineering at SUNY Polytechnic Institute was established on the campus of the University at Albany. In 2005, the University of Waterloo established a unique program which offers a full degree in Nanotechnology Engineering. Louisiana Tech University started the first program in the U.S. in 2005. In 2006 the University of Duisburg-Essen started a Bachelor and a Master program NanoEngineering. The University of California, San Diego followed shortly thereafter in 2007 with its own department of Nanoengineering.
In 2009, the University of Toronto began offering all Options of study in Engineering Science as degrees, bringing the second nanoengineering degree to Canada.
DTU Nanotech - the Department of Micro- and Nanotechnology - is a department at the Technical University of Denmark established in 1990.

</doc>
