<doc id="19637" url="https://en.wikipedia.org/wiki?curid=19637" title="Molecular nanotechnology">
Molecular nanotechnology

Molecular nanotechnology (MNT) is a technology based on the ability to build structures to complex, atomic specifications by means of mechanosynthesis. This is distinct from nanoscale materials. Based on Richard Feynman's vision of miniature factories using nanomachines to build complex products (including additional nanomachines), this advanced form of nanotechnology (or "molecular manufacturing") would make use of positionally-controlled mechanosynthesis guided by molecular machine systems. MNT would involve combining physical principles demonstrated by biophysics, chemistry, other nanotechnologies, and the molecular machinery of life with the systems engineering principles found in modern macroscale factories.
Introduction.
While conventional chemistry uses inexact processes obtaining inexact results, and biology exploits inexact processes to obtain definitive results, molecular nanotechnology would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.
A roadmap for the development of MNT is an objective of a broadly based technology project led by Battelle (the manager of several U.S. National Laboratories) and the Foresight Institute. The roadmap was originally scheduled for completion by late 2006, but was released in January 2008. The Nanofactory Collaboration is a more focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development. In August 2005, a task force consisting of 50+ international experts from various fields was organized by the Center for Responsible Nanotechnology to study the societal implications of molecular nanotechnology.
Projected applications and capabilities.
Smart materials and nanosensors.
One proposed application of MNT is so-called smart materials. This term refers to any sort of material designed and engineered at the nanometer scale for a specific task. It encompasses a wide variety of possible commercial applications. One example would be materials designed to respond differently to various molecules; such a capability could lead, for example, to artificial drugs which would recognize and render inert specific viruses. Another is the idea of self-healing structures, which would repair small tears in a surface naturally in the same way as self-sealing tires or human skin.
A MNT nanosensor would resemble a smart material, involving a small component within a larger machine that would react to its environment and change in some fundamental, intentional way. A very simple example: a photosensor might passively measure the incident light and discharge its absorbed energy as electricity when the light passes above or below a specified threshold, sending a signal to a larger machine. Such a sensor would supposedly cost less and use less power than a conventional sensor, and yet function usefully in all the same applications — for example, turning on parking lot lights when it gets dark.
While smart materials and nanosensors both exemplify useful applications of MNT, they pale in comparison with the complexity of the technology most popularly associated with the term: the replicating nanorobot.
Replicating nanorobots.
MNT nanofacturing is popularly linked with the idea of swarms of coordinated nanoscale robots working together, a popularization of an early proposal by K. Eric Drexler in his 1986 discussions of MNT, but superseded in 1992. In this early proposal, sufficiently capable nanorobots would construct more nanorobots in an artificial environment containing special molecular building blocks.
Critics have doubted both the feasibility of self-replicating nanorobots and the feasibility of control if self-replicating nanorobots could be achieved: they cite the possibility of mutations removing any control and favoring reproduction of mutant pathogenic variations. Advocates address the first doubt by pointing out that the first macroscale autonomous machine replicator, made of Lego blocks, was built and operated experimentally in 2002. While there are sensory advantages present at the macroscale compared to the limited sensorium available at the nanoscale, proposals for positionally controlled nanoscale mechanosynthetic fabrication systems employ dead reckoning of tooltips combined with reliable reaction sequence design to ensure reliable results, hence a limited sensorium is no handicap; similar considerations apply to the positional assembly of small nanoparts. Advocates address the second doubt by arguing that bacteria are (of necessity) evolved to evolve, while nanorobot mutation could be actively prevented by common error-correcting techniques. Similar ideas are advocated in the Foresight Guidelines on Molecular Nanotechnology, and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous proposed methods by which replicators could, in principle, be safely controlled by good design.
However, the concept of suppressing mutation raises the question: How can design evolution occur at the nanoscale without a process of random mutation and deterministic selection? Critics argue that MNT advocates have not provided a substitute for such a process of evolution in this nanoscale arena where conventional sensory-based selection processes are lacking. The limits of the sensorium available at the nanoscale could make it difficult or impossible to winnow successes from failures. Advocates argue that design evolution should occur deterministically and strictly under human control, using the conventional engineering paradigm of modeling, design, prototyping, testing, analysis, and redesign.
In any event, since 1992 technical proposals for MNT do not include self-replicating nanorobots, and recent ethical guidelines put forth by MNT advocates prohibit unconstrained self-replication.
Medical nanorobots.
One of the most important applications of MNT would be medical nanorobotics or nanomedicine, an area pioneered by Robert Freitas in numerous books and papers. The ability to design, build, and deploy large numbers of medical nanorobots would, at a minimum, make possible the rapid elimination of disease and the reliable and relatively painless recovery from physical trauma. Medical nanorobots might also make possible the convenient correction of genetic defects, and help to ensure a greatly expanded lifespan. More controversially, medical nanorobots might be used to augment natural human capabilities.
Utility fog.
Another proposed application of molecular nanotechnology is "utility fog" — in which a cloud of networked microscopic robots (simpler than assemblers) would change its shape and properties to form macroscopic objects and tools in accordance with software commands. Rather than modify the current practices of consuming material goods in different forms, utility fog would simply replace many physical objects.
Phased-array optics.
Yet another proposed application of MNT would be phased-array optics (PAO). However, this appears to be a problem addressable by ordinary nanoscale technology. PAO would use the principle of phased-array millimeter technology but at optical wavelengths. This would permit the duplication of any sort of optical effect but virtually. Users could request holograms, sunrises and sunsets, or floating lasers as the mood strikes. PAO systems were described in BC Crandall's "Nanotechnology: Molecular Speculations on Global Abundance" in the Brian Wowk article "Phased-Array Optics."
Potential social impacts.
Benefits.
Nanotechnology (or molecular nanotechnology to refer more specifically to the goals discussed here) will let us continue the historical trends in manufacturing right up to the fundamental limits imposed by physical law. It will let us make remarkably powerful molecular computers. It will let us make materials over fifty times lighter than steel or aluminium alloy but with the same strength. We'll be able to make jets, rockets, cars or even chairs that, by today's standards, would be remarkably light, strong, and inexpensive. Molecular surgical tools, guided by molecular computers and injected into the blood stream could find and destroy cancer cells or invading bacteria, unclog arteries, or provide oxygen when the circulation is impaired.
Nanotechnology will replace our entire manufacturing base with a new, radically more precise, radically less expensive, and radically more flexible way of making products. The aim is not simply to replace today's computer chip making plants, but also to replace the assembly lines for cars, televisions, telephones, books, surgical tools, missiles, bookcases, airplanes, tractors, and all the rest. The objective is a pervasive change in manufacturing, a change that will leave virtually no product untouched. Economic progress and military readiness in the 21st Century will depend fundamentally on maintaining a competitive position in nanotechnology.
Despite the current early developmental status of nanotechnology and molecular nanotechnology, much concern surrounds MNT's anticipated impact on economics and on law. Whatever the exact effects, MNT, if achieved, would tend to reduce the scarcity of manufactured goods and make many more goods (such as food and health aids) manufacturable.
It is generally considered that future citizens of a molecular-nanotechnological society would still need money, in the form of unforgeable digital cash or physical specie (in special circumstances). They might use such money to buy goods and services that are unique, or limited within the solar system. These might include: matter, energy, information, real estate, design services, entertainment services, legal services, fame, political power, or the attention of other people to one's political/religious/philosophical message. Furthermore, futurists must consider war, even between prosperous states, and non-economic goals.
If MNT were realized, some resources would remain limited, because unique physical objects are limited (a plot of land in the real Jerusalem, mining rights to the larger near-earth asteroids) or because they depend on the goodwill of a particular person (the love of a famous person, a live audience in a musical concert). Demand will always exceed supply for some things, and a political economy may continue to exist in any case. Whether the interest in these limited resources would diminish with the advent of virtual reality, where they could be easily substituted, is yet unclear. One reason why it might not is a hypothetical preference for "the real thing", although such an opinion could easily be mollified if virtual reality were to develop to a certain level of quality.
MNT should make possible nanomedical capabilities able to cure any medical condition not already cured by advances in other areas. Good health would be common, and poor health of any form would be as rare as smallpox and scurvy are today. Even cryonics would be feasible, as cryopreserved tissue could be fully repaired.
Risks.
Molecular nanotechnology is one of the technologies that some analysts believe could lead to a Technological Singularity.
Some feel that molecular nanotechnology would have daunting risks. It conceivably could enable cheaper and more destructive conventional weapons. Also, molecular nanotechnology might permit weapons of mass destruction that could self-replicate, as viruses and cancer cells do when attacking the human body. Commentators generally agree that, in the event molecular nanotechnology were developed, its self-replication should be permitted only under very controlled or "inherently safe" conditions.
A fear exists that nanomechanical robots, if achieved, and if designed to self-replicate using naturally occurring materials (a difficult task), could consume the entire planet in their hunger for raw materials, or simply crowd out natural life, out-competing it for energy (as happened historically when blue-green algae appeared and outcompeted earlier life forms). Some commentators have referred to this situation as the "grey goo" or "ecophagy" scenario. K. Eric Drexler considers an accidental "grey goo" scenario extremely unlikely and says so in later editions of "Engines of Creation".
In light of this perception of potential danger, the Foresight Institute (founded by K. Eric Drexler to prepare for the arrival of future technologies) has drafted a set of guidelines for the ethical development of nanotechnology. These include the banning of free-foraging self-replicating pseudo-organisms on the Earth's surface, at least, and possibly in other places.
Technical issues and criticism.
The feasibility of the basic technologies analyzed in "Nanosystems" has been the subject of a formal scientific review by U.S. National Academy of Sciences, and has also been the focus of extensive debate on the internet and in the popular press.
Study and recommendations by the U.S. National Academy of Sciences.
In 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" The study committee reviewed the technical content of "Nanosystems", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:
Assemblers versus nanofactories.
A section heading in Drexler's "Engines of Creation" reads "Universal Assemblers", and the following text speaks of multiple types of assemblers which, collectively, could hypothetically "build almost anything that the laws of nature allow to exist." Drexler's colleague Ralph Merkle has noted that, contrary to widespread legend, Drexler never claimed that assembler systems could build absolutely any molecular structure. The endnotes in Drexler's book explain the qualification "almost": "For example, a delicate structure might be designed that, like a stone arch, would self-destruct unless all its pieces were already in place. If there were no room in the design for the placement and removal of a scaffolding, then the structure might be impossible to build. Few structures of practical interest seem likely to exhibit such a problem, however."
In 1992, Drexler published "Nanosystems: Molecular Machinery, Manufacturing, and Computation", a detailed proposal for synthesizing stiff covalent structures using a table-top factory. Diamondoid structures and other stiff covalent structures, if achieved, would have a wide range of possible applications, going far beyond current MEMS technology. An outline of a path was put forward in 1992 for building a table-top factory in the absence of an assembler. Other researchers have begun advancing tentative, alternative proposed paths for this in the years since Nanosystems was published.
Hard versus soft nanotechnology.
In 2004 Richard Jones wrote Soft Machines (nanotechnology and life), a book for lay audiences published by Oxford University. In this book he describes radical nanotechnology (as advocated by Drexler) as a deterministic/mechanistic idea of nano engineered machines that does not take into account the nanoscale challenges such as wetness, stickness, Brownian motion, and high viscosity. He also explains what is soft nanotechnology or more appropriatelly biomimetic nanotechnology which is the way forward, if not the best way, to design functional nanodevices that can cope with all the problems at a nanoscale. One can think of soft nanotechnology as the development of nanomachines that uses the lessons learned from biology on how things work, chemistry to precisely engineer such devices and stochastic physics to model the system and its natural processes in detail.
The Smalley-Drexler debate.
Several researchers, including Nobel Prize winner Dr. Richard Smalley (1943–2005), attacked the notion of universal assemblers, leading to a rebuttal from Drexler and colleagues, and eventually to an exchange of letters. Smalley argued that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Drexler and colleagues, however, noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in "Nanosystems". Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible. However, Drexler addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. It is noteworthy that, contrary to Smalley's opinion that enzymes require water, "Not only do enzymes work vigorously in anhydrous organic media, but in this unnatural milieu they acquire remarkable properties such as greatly enhanced stability, radically altered substrate and enantiomeric specificities, molecular memory, and the ability to catalyse unusual reactions.""
Design issues.
For the future, some means have to be found for MNT design evolution at the nanoscale which mimics the process of biological evolution at the molecular scale. Biological evolution proceeds by random variation in ensemble averages of organisms combined with culling of the less-successful variants and reproduction of the more-successful variants, and macroscale engineering design also proceeds by a process of design evolution from simplicity to complexity as set forth somewhat satirically by John Gall: "A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works." A breakthrough in MNT is needed which proceeds from the simple atomic ensembles which can be built with, e.g., an STM to complex MNT systems via a process of design evolution. A handicap in this process is the difficulty of seeing and manipulation at the nanoscale compared to the macroscale which makes deterministic selection of successful trials difficult; in contrast biological evolution proceeds via action of what Richard Dawkins has called the "blind watchmaker"
comprising random molecular variation and deterministic reproduction/extinction.
At present in 2007 the practice of nanotechnology embraces both stochastic approaches (in which, for example, supramolecular chemistry creates waterproof pants) and deterministic approaches wherein single molecules (created by stochastic chemistry) are manipulated on substrate surfaces (created by stochastic deposition methods) by deterministic methods comprising nudging them with STM or AFM probes and causing simple binding or cleavage reactions to occur. The dream of a complex, deterministic molecular nanotechnology remains elusive. Since the mid-1990s, thousands of surface scientists and thin film technocrats have latched on to the nanotechnology bandwagon and redefined their disciplines as nanotechnology. This has caused much confusion in the field and has spawned thousands of "nano"-papers on the peer reviewed literature. Most of these reports are extensions of the more ordinary research done in the parent fields.
The feasibility of the proposals in "Nanosystems".
The feasibility of Drexler's proposals largely depends, therefore, on whether designs like those in "Nanosystems" could be built in the absence of a universal assembler to build them and would work as described. Supporters of molecular nanotechnology frequently claim that no significant errors have been discovered in "Nanosystems" since 1992. Even some critics concede that "Drexler has carefully considered a number of physical principles underlying the 'high level' aspects of the nanosystems he proposes and, indeed, has thought in some detail" about some issues.
Other critics claim, however, that "Nanosystems" omits important chemical details about the low-level 'machine language' of molecular nanotechnology. They also claim that much of the other low-level chemistry in "Nanosystems" requires extensive further work, and that Drexler's higher-level designs therefore rest on speculative foundations. Recent such further work by Freitas and Merkle is aimed at strengthening these foundations by filling the existing gaps in the low-level chemistry.
Drexler argues that we may need to wait until our conventional nanotechnology improves before solving these issues: "Molecular manufacturing will result from a series of advances in molecular machine systems, much as the first Moon landing resulted from a series of advances in liquid-fuel rocket systems. We are now in a position like that of the British Interplanetary Society of the 1930s which described how multistage liquid-fueled rockets could reach the Moon and pointed to early rockets as illustrations of the basic principle." However, Freitas and Merkle argue that a focused effort to achieve diamond mechanosynthesis (DMS) can begin now, using existing technology, and might achieve success in less than a decade if their "direct-to-DMS approach is pursued rather than a more circuitous development approach that seeks to implement less efficacious nondiamondoid molecular manufacturing technologies before progressing to diamondoid".
To summarize the arguments against feasibility: First, critics argue that a primary barrier to achieving molecular nanotechnology is the lack of an efficient way to create machines on a molecular/atomic scale, especially in the absence of a well-defined path toward a self-replicating assembler or diamondoid nanofactory. Advocates respond that a preliminary research path leading to a diamondoid nanofactory is being developed.
A second difficulty in reaching molecular nanotechnology is design. Hand design of a gear or bearing at the level of atoms might take a few to several weeks. While Drexler, Merkle and others have created designs of simple parts, no comprehensive design effort for anything approaching the complexity of a Model T Ford has been attempted. Advocates respond that it is difficult to undertake a comprehensive design effort in the absence of significant funding for such efforts, and that despite this handicap much useful design-ahead has nevertheless been accomplished with new software tools that have been developed, e.g., at Nanorex.
In the latest report "A Matter of Size: Triennial Review of the National Nanotechnology Initiative" put out by the National Academies Press in December 2006 (roughly twenty years after Engines of Creation was published), no clear way forward toward molecular nanotechnology could yet be seen, as per the conclusion on page 108 of that report: "Although theoretical calculations can be made today, the eventually attainable
range of chemical reaction cycles, error rates, speed of operation, and thermodynamic
efficiencies of such bottom-up manufacturing systems cannot be reliably
predicted at this time. Thus, the eventually attainable perfection and complexity of
manufactured products, while they can be calculated in theory, cannot be predicted
with confidence. Finally, the optimum research paths that might lead to systems
which greatly exceed the thermodynamic efficiencies and other capabilities of
biological systems cannot be reliably predicted at this time. Research funding that
is based on the ability of investigators to produce experimental demonstrations
that link to abstract models and guide long-term vision is most appropriate to
achieve this goal." This call for research leading to demonstrations is welcomed by groups such as the Nanofactory Collaboration who are specifically seeking experimental successes in diamond mechanosynthesis. The "Technology Roadmap for Productive Nanosystems" aims to offer additional constructive insights.
It is perhaps interesting to ask whether or not most structures consistent with physical law can in fact be manufactured. Advocates assert that to achieve most of the vision of molecular manufacturing it is not necessary to be able to build "any structure that is compatible with natural law." Rather, it is necessary to be able to build only a sufficient (possibly modest) subset of such structures—as is true, in fact, of any practical manufacturing process used in the world today, and is true even in biology. In any event, as Richard Feynman once said, "It is scientific only to say what's more likely or less likely, and not to be proving all the time what's possible or impossible."
Existing work on diamond mechanosynthesis.
There is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as mechanosynthesis). This work is slowly permeating the broader nanoscience community and is being critiqued. For instance, Peng et al. (2006) (in the continuing research effort by Freitas, Merkle and their collaborators) reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C2 carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. Over 100,000 CPU hours were invested in this latest study. The DCB6 tooltip motif, initially described by Merkle and Freitas at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface.
The tooltips modeled in this work are intended to be used only in carefully controlled environments (e. g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in Peng et al. (2006) -- tooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Peng et al. (2006) reports that increasing the handle thickness from 4 support planes of C atoms above the tooltip to 5 planes decreases the resonance frequency of the entire structure from 2.0 THz to 1.8 THz. More importantly, the vibrational footprints of a DCB6Ge tooltip mounted on a 384-atom handle and of the same tooltip mounted on a similarly constrained but much larger 636-atom "crossbar" handle are virtually identical in the non-crossbar directions. Additional computational studies modeling still bigger handle structures are welcome, but the ability to precisely position SPM tips to the requisite atomic accuracy has been repeatedly demonstrated experimentally at low temperature, or even at room temperature constituting a basic existence proof for this capability.
Further research to consider additional tooltips will require time-consuming computational chemistry and difficult laboratory work.
A working nanofactory would require a variety of well-designed tips for different reactions, and detailed analyses of placing atoms on more complicated surfaces. Although this appears a challenging problem given current resources, many tools will be available to help future researchers: Moore's Law predicts further increases in computer power, semiconductor fabrication techniques continue to approach the nanoscale, and researchers grow ever more skilled at using proteins, ribosomes and DNA to perform novel chemistry.

</doc>
<doc id="19638" url="https://en.wikipedia.org/wiki?curid=19638" title="Microelectromechanical systems">
Microelectromechanical systems

Microelectromechanical systems (MEMS, also written as "micro-electro-mechanical", "MicroElectroMechanical" or "microelectronic and microelectromechanical systems" and the related "micromechatronics") is the technology of very small devices; it merges at the nano-scale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan, or "micro systems technology" ("MST") in Europe.
MEMS are separate and distinct from the hypothetical vision of molecular nanotechnology or molecular electronics. MEMS are made up of components between 1 to 100 micrometres in size (i.e. 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e. 0.02 to 1.0 mm). They usually consist of a central unit that processes data (the microprocessor) and several components that interact with the surroundings such as microsensors. At these size scales, the standard constructs of classical physics are not always useful. Because of the large surface area to volume ratio of MEMS, surface effects such as electrostatics and wetting dominate over volume effects such as inertia or thermal mass. 
The potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electro discharge machining (EDM), and other technologies capable of manufacturing small devices. An early example of a MEMS device is the resonistor – an electromechanical monolithic resonator.
Materials for MEMS manufacturing.
The fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.
Silicon.
Silicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of inexpensive high-quality materials, and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications.
Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking.
Polymers.
Even though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to produce. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.
Metals.
Metals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.
Ceramics.
The nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus allowing to realize electrostatic MEMS actuation schemes with ultrathin membranes. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments and in biosensors.
MEMS basic processes.
Deposition processes.
One of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between a few nanometres to about 100 micrometres. There are two types of deposition processes, as follows.
Physical deposition.
Physical vapor deposition ("PVD") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and evaporation, in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.
Chemical deposition.
Chemical deposition techniques include chemical vapor deposition ("CVD"), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example, LPCVD (Low Pressure chemical vapor deposition) and PECVD (Plasma Enhanced chemical vapor deposition).
Oxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.
Patterning.
Patterning in MEMS is the transfer of a pattern into a material.
Lithography.
Lithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.
This exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching.
Electron beam lithography.
Electron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), ("exposing" the resist) and of selectively removing either exposed or non-exposed regions of the resist ("developing"). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.
The primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer region. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.
The key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.
Ion beam lithography.
It is known that focused-ion beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.
Ion track technology.
Ion track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable to generate holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 104 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.
X-ray lithography.
X-ray lithography is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply "resist," on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.
Diamond patterning.
A simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.
Diamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.
Etching processes.
There are two basic categories of etching processes: wet etching and dry etching.
In the former, the material is dissolved when immersed in a chemical solution.
In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant.
Wet etching.
Wet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.
Isotropic etching.
Etching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.
Anisotropic etching.
Some single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.
HF etching.
Hydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.
Hydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore, the damage is not felt until it is too late.
Electrochemical etching.
Electrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant ("etch-stop") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.
Dry etching.
Vapor etching.
Xenon difluoride.
Xenon difluoride () is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, , silicon nitride, and various metals for masking. Its reaction to silicon is "plasmaless", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.
Plasma etching.
Modern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.
Plasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.
The source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride (CCl4) etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize ("ash") photoresist and facilitate its removal.
Ion milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10–3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.
Reactive ion etching (RIE).
In reactive-ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical.
Deep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called "Bosch process", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the original Bosch process) while the second variation only consists of two steps.
In the first variation, the etch cycle is as follows:
(i) isotropic etch;
(ii) passivation;
(iii) anisoptropic etch for floor cleaning.
In the 2nd variation, steps (i) and (iii) are combined.
Both variations operate similarly.
The creates a polymer on the surface of the substrate, and the second gas composition ( and ) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.
Die preparation.
After preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.
MEMS manufacturing technologies.
Bulk micromachining.
Bulk micromachining is the oldest paradigm of silicon based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.
Surface micromachining.
Surface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.
High aspect ratio (HAR) silicon micromachining.
Both bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.
Applications.
Some common commercial applications of MEMS include:
Industry structure.
The global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Developpement and is forecasted to reach $72 billion by 2011.
Companies with strong MEMS programs come in many sizes. Larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. Smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. Both large and small companies typically invest in R&D to explore new MEMS technology.
The market for materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.

</doc>
<doc id="19639" url="https://en.wikipedia.org/wiki?curid=19639" title="Marvin Minsky">
Marvin Minsky

Marvin Lee Minsky (August 9, 1927 – January 24, 2016) was an American cognitive scientist in the field of artificial intelligence (AI), co-founder of the Massachusetts Institute of Technology's AI laboratory, and author of several texts on AI and philosophy.
Biography.
Marvin Lee Minsky was born to an eye surgeon father, Henry, and to a Jewish mother, Fannie, who was an activist in Zionist affairs, in New York City, where he attended The Fieldstone School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He then served in the US Navy from 1944 to 1945. He held a BA in mathematics from Harvard (1950) and a PhD in mathematics from Princeton (1954). He was on the MIT faculty from 1958 to his death. In 1959 he and John McCarthy founded what is now known as the MIT Computer Science and Artificial Intelligence Laboratory. He was the Toshiba Professor of Media Arts and Sciences, and professor of electrical engineering and computer science.
Minsky's inventions include the first head-mounted graphical display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). He developed, with Seymour Papert, the first Logo "turtle". Minsky also built, in 1951, the first randomly wired neural network learning machine, SNARC.
Minsky wrote the book "Perceptrons" (with Seymour Papert), which became the foundational work in the analysis of artificial neural networks. This book is the center of a controversy in the history of AI, as some claim it to have had great importance in driving research away from neural networks in the 1970s, and contributing to the so-called AI winter. He also founded several other famous AI models. His book "A framework for representing knowledge" created a new paradigm in programming. While his "Perceptrons" is now more a historical than practical book, the theory of frames is in wide use. Minsky has also written on the possibility that extraterrestrial life may think like humans, permitting communication. He was an adviser on Stanley Kubrick's film ; one of the film's characters, Victor Kaminski, was named in Minsky's honor and Minsky himself is mentioned in the movie and in Arthur C. Clarke's tie-in novel:
In the early 1970s, at the MIT Artificial Intelligence Lab, Minsky and Papert started developing what came to be called the Society of Mind theory. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks. In 1986, Minsky published "The Society of Mind", a comprehensive book on the theory which, unlike most of his previously published work, was written for a general audience.
In November 2006, Minsky published "The Emotion Machine", a book that critiques many popular theories of how human minds work and suggests alternative theories, often replacing simple ideas with more complex ones. Recent drafts of the book are freely available from his webpage.
Personal life and death.
In 1952, Minsky married pediatrician Dr. Gloria Rudisch; together they had three children. Minsky was a talented improvisational pianist who published musings on the connections between music and psychology.
Views.
Minsky was an atheist and a signatory to the Scientists' Open Letter on Cryonics. He was a critic of the Loebner Prize.
Minsky believed that there is no fundamental difference between humans and machines, and that humans are machines whose "intelligence" emerges from the interplay of the many unintelligent but semi-autonomous agents that make up the brain. He has stated that "somewhere down the line, some computers will become more intelligent than most people," but that it's very hard to predict how fast progress will be. He has cautioned that an artificial superintelligence designed to solve an innocuous mathematical problem might decide to take over all of Earth's resources to build supercomputers to help achieve its goal, but believed that such negative scenarios are "hard to take seriously" because he was confident AI would go through "a lot of testing" before being deployed.
Death.
Minsky died of a cerebral hemorrhage at the age of 88. Ray Kurzweil says he was contacted by the cryonics organization Alcor Life Extension Foundation seeking Minsky's body. Kurzweil believes that Minsky was cryonically preserved by Alcor and will be revived by 2045. Minsky was a member of Alcor's Scientific Advisory Board. In keeping with their policy of protecting privacy, Alcor will neither confirm nor deny that Alcor has cryonically preserved Minsky.
Awards and affiliations.
Minsky won the Turing Award (the highest distinction in computer science) in 1969, the Japan Prize in 1990, the IJCAI Award for Research Excellence in 1991, and the Benjamin Franklin Medal from the Franklin Institute in 2001. In 2006, he was inducted as a Fellow of the Computer History Museum "for co-founding the field of artificial intelligence, creating early neural networks and robots, and developing theories of human and machine cognition." In 2011, Minsky was inducted into IEEE Intelligent Systems' AI's Hall of Fame for the "significant contributions to the field of AI and intelligent systems". In 2014, Minsky won the Dan David Prize in the field of "Artificial Intelligence, the Digital Mind". He was also awarded with the 2013 BBVA Foundation Frontiers of Knowledge Award in the Information and Communication Technologies category.
Minsky was affiliated with the following organizations:

</doc>
<doc id="19640" url="https://en.wikipedia.org/wiki?curid=19640" title="Milton Friedman">
Milton Friedman

Milton Friedman (July 31, 1912 – November 16, 2006) was an American economist who received the 1976 Nobel Memorial Prize in Economic Sciences for his research on consumption analysis, monetary history and theory and the complexity of stabilization policy. With George Stigler and others, Friedman was among the intellectual leaders of the second generation of Chicago price theory, a methodological movement at the University of Chicago's Department of Economics, Law School, and Graduate School of Business from the 1940s onward. Several students and young professors that were recruited or mentored by Friedman at Chicago went on to become leading economists; they include Gary Becker, Robert Fogel, and Robert Lucas, Jr.
Friedman's challenges to what he later called "naive Keynesian" theory began with his 1950s reinterpretation of the consumption function. In the 1960s, he became the main advocate opposing Keynesian government policies, and described his approach (along with mainstream economics) as using "Keynesian language and apparatus" yet rejecting its "initial" conclusions. He theorized that there existed a "natural" rate of unemployment, and argued that employment above this rate would cause inflation to accelerate. He argued that the Phillips curve was, in the long run, vertical at the "natural rate" and predicted what would come to be known as stagflation. Friedman promoted an alternative macroeconomic viewpoint known as "monetarism", and argued that a steady, small expansion of the money supply was the preferred policy. His ideas concerning monetary policy, taxation, privatization and deregulation influenced government policies, especially during the 1980s. His monetary theory influenced the Federal Reserve's response to the global financial crisis of 2007–08.
Friedman was an advisor to Republican U.S. President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher. His political philosophy extolled the virtues of a free market economic system with minimal intervention. He once stated that his role in eliminating U.S. conscription was his proudest accomplishment. In his 1962 book "Capitalism and Freedom", Friedman advocated policies such as a volunteer military, freely floating exchange rates, abolition of medical licenses, a negative income tax, and school vouchers. His support for school choice led him to found the Friedman Foundation for Educational Choice.
Milton Friedman's works include many monographs, books, scholarly articles, papers, magazine columns, television programs, and lectures, and cover a broad range of economic topics and public policy issues. His books and essays have had an international influence, including in former communist states. A survey of economists ranked Friedman as the second most popular economist of the twentieth century after John Maynard Keynes, and "The Economist" described him as "the most influential economist of the second half of the 20th century ... possibly of all of it."
Early life.
Friedman was born in Brooklyn, New York on July 31, 1912. His parents, Sára Ethel (née Landau) and Jenő Saul Friedman, were recent Jewish immigrants from Beregszász in Carpathian Ruthenia, Kingdom of Hungary (now Berehove in Ukraine). They both worked as dry goods merchants. Shortly after Milton's birth, the family relocated to Rahway, New Jersey. In his early teens, Friedman was injured in a car accident, which scarred his upper lip. A talented student, Friedman graduated from Rahway High School in 1928, just before his 16th birthday.
In 1932, Friedman graduated from Rutgers University, where he specialized in Mathematics and Economics and initially intended to become an actuary. During his time at Rutgers, Friedman became influenced by two economics professors, Arthur F. Burns and Homer Jones, who convinced him that modern economics could help end the Great Depression.
After graduating from Rutgers, Friedman was offered two scholarships to do graduate work — one in mathematics at Brown University and the other in economics at the University of Chicago. Friedman chose the latter, thus earning a Master of Arts degree in 1933. He was strongly influenced by Jacob Viner, Frank Knight, and Henry Simons. It was at Chicago that Friedman met his future wife, economist Rose Director. During the 1933–1934 academic year he had a fellowship at Columbia University, where he studied statistics with renowned statistician and economist Harold Hotelling. He was back in Chicago for the 1934–1935 academic year, working as a research assistant for Henry Schultz, who was then working on "Theory and Measurement of Demand". That year, Friedman formed what would prove to be lifelong friendships with George Stigler and W. Allen Wallis.
Public service.
Friedman was initially unable to find academic employment, so in 1935 he followed his friend W. Allen Wallis to Washington, where Franklin D. Roosevelt's New Deal was "a lifesaver" for many young economists. At this stage, Friedman said that he and his wife "regarded the job-creation programs such as the WPA, CCC, and PWA appropriate responses to the critical situation," but not "the price- and wage-fixing measures of the National Recovery Administration and the Agricultural Adjustment Administration." Foreshadowing his later ideas, he believed price controls interfered with an essential signaling mechanism to help resources be used where they were most valued. Indeed, Friedman later concluded that all government intervention associated with the New Deal was "the wrong cure for the wrong disease," arguing that the money supply should simply have been expanded, instead of contracted. Later, Friedman and his colleague Anna Schwartz wrote "A Monetary History of the United States, 1867–1960", which argued that the Great Depression was caused by a severe monetary contraction due to banking crises and poor policy on the part of the Federal Reserve.
During 1935, he began work for the National Resources Committee, which was then working on a large consumer budget survey. Ideas from this project later became a part of his "Theory of the Consumption Function". Friedman began employment with the National Bureau of Economic Research during autumn 1937 to assist Simon Kuznets in his work on professional income. This work resulted in their jointly authored publication "Incomes from Independent Professional Practice", which introduced the concepts of permanent and transitory income, a major component of the Permanent Income Hypothesis that Friedman worked out in greater detail in the 1950s. The book hypothesizes that professional licensing artificially restricts the supply of services and raises prices.
During 1940, Friedman was appointed an assistant professor teaching Economics at the University of Wisconsin–Madison, but encountered antisemitism in the Economics department and decided to return to government service. From 1941 to 1943 Friedman worked on wartime tax policy for the Federal Government, as an advisor to senior officials of the United States Department of the Treasury. As a Treasury spokesman during 1942 he advocated a Keynesian policy of taxation. He helped to invent the payroll withholding tax system, since the federal government badly needed money in order to fight the war. He later said, "I have no apologies for it, but I really wish we hadn't found it necessary and I wish there were some way of abolishing withholding now."
Academic career.
Early years.
In 1940, Friedman accepted a position at the University of Wisconsin–Madison, but left because of differences with faculty regarding United States involvement in World War II. Friedman believed the United States should enter the war. In 1943, Friedman joined the Division of War Research at Columbia University (headed by W. Allen Wallis and Harold Hotelling), where he spent the rest of World War II working as a mathematical statistician, focusing on problems of weapons design, military tactics, and metallurgical experiments.
In 1945, Friedman submitted "Incomes from Independent Professional Practice" (co-authored with Kuznets and completed during 1940) to Columbia as his doctoral dissertation. The university awarded him a PhD in 1946. Friedman spent the 1945–1946 academic year teaching at the University of Minnesota (where his friend George Stigler was employed). On February 12, 1945, his son, David D. Friedman was born.
University of Chicago.
In 1946, Friedman accepted an offer to teach economic theory at the University of Chicago (a position opened by departure of his former professor Jacob Viner to Princeton University). Friedman would work for the University of Chicago for the next 30 years. There he contributed to the establishment of an intellectual community that produced a number of Nobel Prize winners, known collectively as the Chicago school of economics.
At that time, Arthur F. Burns, who was then the head of the National Bureau of Economic Research, asked Friedman to rejoin the Bureau's staff. He accepted the invitation, and assumed responsibility for the Bureau's inquiry into the role of money in the business cycle. As a result, he initiated the "Workshop in Money and Banking" (the "Chicago Workshop"), which promoted a revival of monetary studies. During the latter half of the 1940s, Friedman began a collaboration with Anna Schwartz, an economic historian at the Bureau, that would ultimately result in the 1963 publication of a book co-authored by Friedman and Schwartz, "A Monetary History of the United States, 1867–1960".
Friedman spent the 1954–1955 academic year as a Fulbright Visiting Fellow at Gonville and Caius College, Cambridge. At the time, the Cambridge economics faculty was divided into a Keynesian majority (including Joan Robinson and Richard Kahn) and an anti-Keynesian minority (headed by Dennis Robertson). Friedman speculated that he was invited to the fellowship, because his views were unacceptable to both of the Cambridge factions. Later his weekly columns for "Newsweek" magazine (1966–84) were well read and increasingly influential among political and business people. From 1968 to 1978, he and Paul Samuelson participated in the Economics Cassette Series, a biweekly subscription series where the economist would discuss the days' issues for about a half-hour at a time.
Friedman was an economic adviser to Republican presidential candidate Barry Goldwater during 1964.
Personal Life.
Retirement.
In 1977, at the age of 65, Friedman retired from the University of Chicago after teaching there for 30 years. He and his wife moved to San Francisco where he became a visiting scholar at the Federal Reserve Bank of San Francisco. From 1977 on, he was affiliated with the Hoover Institution at Stanford University. During the same year, Friedman was approached by the Free To Choose Network and asked to create a television program presenting his economic and social philosophy.
The Friedmans worked on this project for the next three years, and during 1980, the ten-part series, titled "Free to Choose", was broadcast by the Public Broadcasting Service (PBS). The companion book to the series (co-authored by Milton and his wife, Rose Friedman), also titled "Free To Choose", was the bestselling nonfiction book of 1980 and has since been translated into 14 foreign languages.
Friedman served as an unofficial adviser to Ronald Reagan during his 1980 presidential campaign, and then served on the President's Economic Policy Advisory Board for the rest of the Reagan Administration. Ebenstein says Friedman was "the 'guru' of the Reagan administration." In 1988 he received the National Medal of Science and Reagan honored him with the Presidential Medal of Freedom.
Milton Friedman is known now as one of the most influential economists of the 20th century. Throughout the 1980s and 1990s, Friedman continued to write editorials and appear on television. He made several visits to Eastern Europe and to China, where he also advised governments. He was also for many years a Trustee of the Philadelphia Society.
Later Life.
According to a 2007 article in "Commentary" magazine, his "parents were moderately observant but Friedman, after an intense burst of childhood piety, rejected religion altogether." He described himself as an agnostic.Friedman wrote extensively of his life and experiences, especially in 1998 in his memoirs with his wife Rose, titled "Two Lucky People".
Death.
Friedman died of heart failure at the age of 94 years in San Francisco on November 16, 2006. He was still a working economist performing original economic research; his last column was published in the "The Wall Street Journal" the day after his death. He was survived by his wife (who died on August 18, 2009) and their two children, David, known for the anarcho-capitalist book "The Machinery of Freedom", and Janet. David's son, Patri Friedman, was the executive director of the The Seasteading Institute from 2008–2011.
Scholarly contributions.
Economics.
Friedman was best known for reviving interest in the money supply as a determinant of the nominal value of output, that is, the quantity theory of money. Monetarism is the set of views associated with modern quantity theory. Its origins can be traced back to the 16th-century School of Salamanca or even further; however, Friedman's contribution is largely responsible for its modern popularization. He co-authored, with Anna Schwartz, "A Monetary History of the United States, 1867–1960" (1963), which was an examination of the role of the money supply and economic activity in the U.S. history. A striking conclusion of their research regarded the way in which money supply fluctuations contribute to economic fluctuations. Several regression studies with David Meiselman during the 1960s suggested the primacy of the money supply over investment and government spending in determining consumption and output. These challenged a prevailing, but largely untested, view on their relative importance. Friedman's empirical research and some theory supported the conclusion that the short-run effect of a change of the money supply was primarily on output but that the longer-run effect was primarily on the price level.
Friedman was the main proponent of the monetarist school of economics. He maintained that there is a close and stable association between inflation and the money supply, mainly that inflation could be avoided with proper regulation of the monetary base's growth rate. He famously used the analogy of "dropping money out of a helicopter.", in order to avoid dealing with money injection mechanisms and other factors that would overcomplicate his models.
Friedman's arguments were designed to counter the popular concept of Cost-push inflation, that the increased General Price Level at the time was the result of increases in the price of oil, or increases in wages; as he wrote, 
Friedman rejected the use of fiscal policy as a tool of demand management; and he held that the government's role in the guidance of the economy should be restricted severely. Friedman wrote extensively on the Great Depression, which he termed the Great Contraction, arguing that it had been caused by an ordinary financial shock whose duration and seriousness were greatly increased by the subsequent contraction of the money supply caused by the misguided policies of the directors of the Federal Reserve.
Friedman also argued for the cessation of government intervention in currency markets, thereby spawning an enormous literature on the subject, as well as promoting the practice of freely floating exchange rates. His close friend George Stigler explained, "As is customary in science, he did not win a full victory, in part because research was directed along different lines by the theory of rational expectations, a newer approach developed by Robert Lucas, also at the University of Chicago." The relationship between Friedman and Lucas, or new classical macroeconomics as a whole, was highly complex. The Friedmanian Phillips curve was an interesting starting point for Lucas, but he soon realized that the solution provided by Friedman was not quite satisfactory. Lucas elaborated a new approach in which rational expectations were presumed instead of the Friedmanian adaptive expectations. Due to this reformulation, the story in which the theory of the new classical Phillips curve was embedded radically changed. This modification, however, had a significant effect on Friedman’s own approach, so, as a result, the theory of the Friedmanian Phillips curve also changed. Moreover, new classical Neil Wallace, who was a graduate student at the University of Chicago between 1960 and 1963, regarded Friedman’s theoretical courses as a mess. This evaluation clearly indicates the broken relationship between Friedmanian monetarism and new classical macroeconomics.
Friedman was also known for his work on the consumption function, the permanent income hypothesis (1957), which Friedman himself referred to as his best scientific work. This work contended that rational consumers would spend a proportional amount of what they perceived to be their permanent income. Windfall gains would mostly be saved. Tax reductions likewise, as rational consumers would predict that taxes would have to increase later to balance public finances. Other important contributions include his critique of the Phillips curve and the concept of the natural rate of unemployment (1968). This critique associated his name, together with that of Edmund Phelps, with the insight that a government that brings about greater inflation cannot permanently reduce unemployment by doing so. Unemployment may be temporarily lower, if the inflation is a surprise, but in the long run unemployment will be determined by the frictions and imperfections of the labor market.
Friedman's essay "The Methodology of Positive Economics" (1953) provided the epistemological pattern for his own subsequent research and to a degree that of the Chicago School. There he argued that economics as "science" should be free of value judgments for it to be objective. Moreover, a useful economic theory should be judged not by its descriptive realism but by its simplicity and fruitfulness as an engine of prediction. That is, students should measure the accuracy of its predictions, rather than the 'soundness of its assumptions'. His argument was part of an ongoing debate among such statisticians as Jerzy Neyman, Leonard Savage, and Ronald Fisher.
Statistics.
One of his most famous contributions to statistics is sequential sampling. Friedman did statistical work at the Division of War Research at Columbia, where he and his colleagues came up with the technique. It later became, in the words of "The New Palgrave Dictionary of Economics", "the standard analysis of quality control inspection". The dictionary adds, "Like many of Friedman’s contributions, in retrospect it seems remarkably simple and obvious to apply basic economic ideas to quality control; that however is a measure of his genius."
Public policy positions.
Federal Reserve.
Due to its poor performance, Friedman believed that the Federal Reserve Board should be abolished. He further believed that if the money supply was to be centrally controlled (as by the Federal Reserve System) that the preferable way to do it would be with a mechanical system that would keep the quantity of money increasing at a steady rate.
Exchange rates.
Friedman was a strong advocate for floating exchange rates throughout the entire Bretton-Woods period. He argued that a flexible exchange rate would make external adjustment possible and allow countries to avoid Balance of Payments crisis. He saw fixed exchange rates as an undesirable form of government intervention. The case was articulated in an influential 1953 paper, "The Case for Flexible Exchange Rates", at a time, when most commentators regarded the possibility of floating exchange rates as a fantasy.
School choice.
In his 1955 article "The Role of Government in Education" Friedman proposed supplementing publicly operated schools with privately run but publicly funded schools through a system of school vouchers. Reforms similar to those proposed in the article were implemented in, for example, Chile in 1981 and Sweden in 1992. In 1996, Friedman, together with his wife, founded The Foundation for Educational Choice to advocate school choice and vouchers.
Conscription.
While Walter Oi is credited with establishing the economic basis for a volunteer military, Milton Friedman was a proponent, stating that the draft was "inconsistent with a free society."
In "Capitalism and Freedom", he argued that conscription is inequitable and arbitrary, preventing young men from shaping their lives as they see fit. During the Nixon administration he headed the committee to research a conversion to paid/volunteer armed force. He would later state that his role in eliminating the conscription in the United States was his proudest accomplishment. Friedman did, however, believe a nation could compel military "training" as a reserve in case of war time.
Foreign policy.
Biographer Lanny Ebenstein noted a drift over time in Friedman's views from an interventionist to a more cautious foreign policy. He supported US involvement in the Second World War and initially supported a hard line against Communism, but moderated over time. He opposed the Gulf War and the Iraq War. In a spring 2006 interview, Friedman said that the USA's stature in the world had been eroded by the Iraq War, but that it might be improved if Iraq were to become a peaceful independent country.
Libertarianism and the Republican Party.
He served as a member of President Reagan's Economic Policy Advisory Board starting at 1981. In 1988, he received the Presidential Medal of Freedom and the National Medal of Science. He said that he was a libertarian philosophically, but a member of the U.S. Republican Party for the sake of "expediency" ("I am a libertarian with a small 'l' and a Republican with a capital 'R.' And I am a Republican with a capital 'R' on grounds of expediency, not on principle.") But, he said, "I think the term classical liberal is also equally applicable. I don't really care very much what I'm called. I'm much more interested in having people thinking about the ideas, rather than the person."
Public goods and monopoly.
Friedman was supportive of the state provision of some public goods that private businesses are not considered as being able to provide. However, he argued that many of the services performed by government could be performed better by the private sector. Above all, if some public goods are provided by the state, he believed that they should not be a legal monopoly where private competition is prohibited; for example, he wrote:
Social security, welfare programs, and negative income tax.
After 1960 Friedman attacked Social Security from a free market view stating that it had created welfare dependency.
Friedman proposed that if there had to be a welfare system of any kind, he would replace the existing U.S. welfare system with a negative income tax, a progressive tax system in which the poor receive a basic living income from the government. According to the "New York Times", Friedman's views in this regard were grounded in a belief that while "market forces ... accomplish wonderful things", they "cannot ensure a distribution of income that enables all citizens to meet basic economic needs".
Drug policy.
Friedman also supported libertarian policies such as legalization of drugs and prostitution. During 2005, Friedman and more than 500 other economists advocated discussions regarding the economic benefits of the legalization of marijuana.
Gay rights.
Friedman was also a supporter of gay rights. He never specifically supported same-sex marriage, instead saying "I do not believe there should be any discrimination against gays."
Economic freedom.
Michael Walker of the Fraser Institute and Friedman hosted a series of conferences from 1986 to 1994. The goal was to create a clear definition of economic freedom and a method for measuring it. Eventually this resulted in the first report on worldwide economic freedom, "Economic Freedom in the World". This annual report has since provided data for numerous peer-reviewed studies and has influenced policy in several nations.
Along with sixteen other distinguished economists he opposed the Copyright Term Extension Act and filed an amicus brief in "Eldred v. Ashcroft". He supported the inclusion of the word "no-brainer" in the brief.
Friedman argued for stronger basic legal (constitutional) protection of economic rights and freedoms to further promote industrial-commercial growth and prosperity and buttress democracy and freedom and the rule of law generally in society.
Honors, recognition, and influence.
George H. Nash, a leading historian of American conservatism, says that by, "the end of the 1960s he was probably the most highly regarded and influential conservative scholar in the country, and one of the few with an international reputation." Friedman allowed the libertarian Cato Institute to use his name for its biannual Milton Friedman Prize for Advancing Liberty beginning in 2001. A Friedman Prize was given to the late British economist Peter Bauer in 2002, Peruvian economist Hernando de Soto in 2004, Mart Laar, former Estonian Prime Minister in 2006 and a young Venezuelan student Yon Goicoechea in 2008. His wife Rose, sister of Aaron Director, with whom he initiated the Friedman Foundation for Educational Choice, served on the international selection committee. Friedman was also a recipient of the Nobel Prize in Economics.
Upon Friedman's death, Harvard President Lawrence Summers called him "The Great Liberator" saying "... any honest Democrat will admit that we are now all Friedmanites." He said Friedman's great popular contribution was "in convincing people of the importance of allowing free markets to operate."
In 2013 Stephen Moore, a member of the editorial forward of the "Wall Street Journal" said, "Quoting the most-revered champion of free-market economics since Adam Smith has become a little like quoting the Bible." He adds, "There are sometimes multiple and conflicting interpretations."
Nobel Memorial Prize in Economic Sciences.
Friedman won the Nobel Memorial Prize in Economic Sciences, the sole recipient for 1976, "for his achievements in the fields of consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization policy."
Hong Kong.
Friedman once said, "If you want to see capitalism in action, go to Hong Kong." He wrote in 1990 that the Hong Kong economy was perhaps the best example of a free market economy.
One month before his death, he wrote the article "Hong Kong Wrong – What would Cowperthwaite say?" in the "Wall Street Journal", criticizing Donald Tsang, the Chief Executive of Hong Kong, for abandoning "positive noninterventionism."
Tsang later said he was merely changing the slogan to "big market, small government," where small government is defined as less than 20% of GDP. In a debate between Tsang and his rival, Alan Leong, before the 2007 Chief Executive election, Leong introduced the topic and jokingly accused Tsang of angering Friedman to death.
Chile.
During 1975, two years after the military coup that brought military dictator President Augusto Pinochet to power and ended the government of Salvador Allende, the economy of Chile experienced a severe crisis. Friedman and Arnold Harberger accepted an invitation of a private Chilean foundation to visit Chile and speak on principles of economic freedom. He spent seven days in Chile giving a series of lectures at the Universidad Católica de Chile and the (National) University of Chile. One of the lectures was entitled "The Fragility of Freedom" and according to Friedman, "dealt with precisely the threat to freedom from a centralized military government."
In an April 21, 1975, letter to Pinochet, Friedman considered the "key economic problems of Chile are clearly ... inflation and the promotion of a healthy social market economy". He stated that "There is only one way to end inflation: by drastically reducing the rate of increase of the quantity of money ..." and that "... cutting government spending is by far and away the most desirable way to reduce the fiscal deficit, because it ... strengthens the private sector thereby laying the foundations for "healthy" economic growth". As to how rapidly inflation should be ended, Friedman felt that "for Chile where inflation is raging at 10–20% a month ... gradualism is not feasible. It would involve so painful an "operation" over so long a period that the "patient" would not survive." Choosing "a brief period of higher unemployment..." was the lesser evil.. and that "the experience of Germany, ... of Brazil ..., of the post-war adjustment in the U.S. ... all argue for "shock treatment"". In the letter Friedman recommended to deliver what the "shock approach" with "... a package to eliminate the surprise and to relieve acute distress" and "... for definiteness let me sketch the contents of a package proposal ... to be taken as illustrative" although his knowledge of Chile was "too limited to enable to be precise or comprehensive". He listed a "sample proposal" of 8 monetary and fiscal measures including "the removal of as many as obstacles as possible that now hinder the private market. For example, suspend ... the present law against discharging employees". He closed, stating "Such a "shock program" could end inflation in months". His letter suggested that cutting spending to reduce the fiscal deficit would result in less transitional unemployment than raising taxes.
Sergio de Castro, a Chilean Chicago School graduate, became the nation's Minister of Finance in 1975. During his six-year tenure, foreign investment increased, restrictions were placed on striking and labor unions, and GDP rose yearly. A foreign exchange program was created between the Catholic University of Chile and the University of Chicago. Many other Chicago School alumni were appointed government posts during and after the Pinochet years; others taught its economic doctrine at Chilean universities. They became known as the Chicago Boys.
Friedman did not criticize Pinochet's dictatorship at the time, nor the assassinations, illegal imprisonments, torture, or other atrocities that were well known by then.
In 1976 Friedman defended his unofficial adviser position with: "I do not consider it as evil for an economist to render technical economic advice to the Chilean Government, any more than I would regard it as evil for a physician to give technical medical advice to the Chilean Government to help end a medical plague."
Friedman defended his activity in Chile on the grounds that, in his opinion, the adoption of free market policies not only improved the economic situation of Chile but also contributed to the amelioration of Pinochet's rule and to the eventual transition to a democratic government during 1990. That idea is included in "Capitalism and Freedom", in which he declared that economic freedom is not only desirable in itself but is also a necessary condition for political freedom. In his 1980 documentary "Free to Choose", he said the following: "Chile is not a politically free system, and I do not condone the system. But the people there are freer than the people in Communist societies because government plays a smaller role. ... The conditions of the people in the past few years has been getting better and not worse. They would be still better to get rid of the junta and to be able to have a free democratic system." In 1984, Friedman stated that he has "never refrained from criticizing the political system in Chile." In 1991 he said: "I have nothing good to say about the political regime that Pinochet imposed. It was a terrible political regime. The real miracle of Chile is not how well it has done economically; the real miracle of Chile is that a military junta was willing to go against its principles and support a free market regime designed by principled believers in a free market. [...] In Chile, the drive for political freedom, that was generated by economic freedom and the resulting economic success, ultimately resulted in a referendum that introduced political democracy. Now, at long last, Chile has all three things: political freedom, human freedom and economic freedom. Chile will continue to be an interesting experiment to watch to see whether it can keep all three or whether, now that it has political freedom,that political freedom will tend to be used to destroy or reduce economic freedom." He stressed that the lectures he gave in Chile were the same lectures he later gave in China and other socialist states.
During the 2000 PBS documentary "The Commanding Heights" (based on ), Friedman continued to argue that "free markets would undermine [Pinochet's] political centralization and political control.", and that criticism over his role in Chile missed his main contention that freer markets resulted in freer people, and that Chile's unfree economy had caused the military government. Friedman advocated for free markets which undermined "political centralization and political control".
Iceland.
Friedman visited Iceland during the autumn of 1984, met with important Icelanders and gave a lecture at the University of Iceland on the "tyranny of the "status quo"." He participated in a lively television debate on August 31, 1984 with socialist intellectuals, including Ólafur Ragnar Grímsson, who later became the president of Iceland. When they complained that a fee was charged for attending his lecture at the University and that, hitherto, lectures by visiting scholars had been free-of-charge, Friedman replied that previous lectures had not been free-of-charge in a meaningful sense: lectures always have related costs. What mattered was whether attendees or non-attendees covered those costs. Friedman thought that it was fairer that only those who attended paid. In this discussion Friedman also stated that he did not receive any money for delivering that lecture.
Estonia.
Although Friedman never visited Estonia, his book "Free to Choose" exercised a great influence on that nation's then 32-year-old prime minister, Mart Laar, who has claimed that it was the only book on economics he had read before taking office. Laar's reforms are often credited with responsibility for transforming Estonia from an impoverished Soviet Republic to the "Baltic Tiger." A prime element of Laar's program was introduction of the flat tax. Laar won the 2006 Milton Friedman Prize for Advancing Liberty, awarded by the Cato Institute.
United Kingdom.
After 1950 Friedman was frequently invited to lecture in Britain, and by the 1970s his ideas had gained widespread attention in conservative circles. For example, he was a regular speaker at the Institute of Economic Affairs (IEA), a libertarian think tank. Conservative politician Margaret Thatcher closely followed IEA programs and ideas, and met Friedman there in 1978. He also strongly influenced Keith Joseph, who became Thatcher's senior advisor on economic affairs, as well as Alan Walters and Patrick Minford, two other key advisers. Major newspapers, including the "Daily Telegraph," "The Times," and "The Financial Times" all promulgated Friedman's monetarist ideas to British decision-makers. Friedman's ideas strongly influenced Thatcher and her allies when she became Prime Minister in 1979.
Critiques.
Famous Critiques of Overall Work.
Econometrician David Hendry criticized part of Friedman's and Anna Schwartz's 1982 "Monetary Trends". When asked about it during an interview with Icelandic TV in 1984, Friedman said that the critique applied to a different problem than that which he and Schwartz had tackled, and was thus not relevant, and also pointed to the (as of 1984) lack of consequential peer review amongst econometricians on Hendry's work. In 2006, Hendry stated that Friedman was guilty of "serious errors" of misunderstanding that meant "the t-ratios he reported for UK money demand were overstated by nearly 100 per cent", and said that, in a paper published in 1991 with Neil Ericsson, he had refuted "almost every empirical claim […] made about UK money demand" by Friedman and Schwartz. A 2004 paper updated and confirmed the validity of the Hendry–Ericsson findings through 2000.
After Friedman's death in 2006, Keynesian Nobel laureate Paul Krugman praised Friedman as a "great economist and a great man," and acknowledged his many, widely accepted contributions to empirical economics. Nonetheless, Krugman criticized Friedman, writing that "he slipped all too easily into claiming both that markets always work and that only markets work. It's extremely hard to find cases in which Friedman acknowledged the possibility that markets could go wrong, or that government intervention could serve a useful purpose."
In her book "The Shock Doctrine", author and social activist Naomi Klein criticized Friedman's economic liberalism, identifying it with the principles that guided the economic restructuring that followed the military coups in countries such as Chile and Indonesia. Based on their assessments of the extent to which what she describes as neoliberal policies contributed to income disparities and inequality, both Klein and Noam Chomsky have suggested that the primary role of what they describe as neoliberalism was as an ideological cover for capital accumulation by multinational corporations.
Because of his involvement with the Pinochet government, there were international protests when Friedman was awarded the Nobel Prize in 1976 . Friedman was accused of supporting the military dictatorship in Chile because of the relation of economists of the University of Chicago to Pinochet, and a controversial six-day trip he took to Chile during March 1975 (less than two years after the coup that deposed President Salvador Allende). Friedman answered that he never was an adviser to the dictatorship, but only gave some lectures and seminars on inflation, and met with officials, including Augusto Pinochet, while in Chile.
Chilean economist Orlando Letelier asserted that Pinochet's dictatorship resorted to oppression because of popular opposition to Chicago School policies in Chile. After a 1991 speech on drug legalisation, Friedman answered a question on his involvement with the Pinochet regime, saying that he was never an advisor to Pinochet (also mentioned in his 1984 Iceland interview), but that a group of his students at the University of Chicago were involved in Chile's economic reforms. Friedman credited these reforms with high levels of economic growth and with the establishment of democracy that has subsequently occurred in Chile.
Critique of Capitalism and Freedom.
Capitalism and Freedom is a seminal work by Friedman. In the book, Friedman talks about the need to move to a classically liberal society, that free markets would help nations and individuals in the long-run and fix the efficiency problems currently faced by the United States and other major countries of the 1950's and 1960's. He goes through the chapters specifying a specific issue in each respective chapter from the role of government and money supply to social welfare programs to a special chapter on occupational licensure. Friedman concludes Capitalism and Freedom with his “classical liberal” stance, that government should stay out of matters that do not need and should only involve itself when absolutely necessary for the survival of its people and the country. He recounts how the best of a country’s abilities come from its free markets while its failures come from government intervention.

</doc>
<doc id="19641" url="https://en.wikipedia.org/wiki?curid=19641" title="Mass media">
Mass media

The mass media is a diversified collection of media technologies that reach a large audience via mass communication. The technologies through which this communication takes place include a variety of outlets.
Broadcast media transmit information electronically, via such media as film, radio, recorded music, or television. Digital media comprises both Internet and mobile mass communication. Internet media comprise such services as email, social media sites, websites, and Internet-based radio and television. Many other mass media outlets have an additional presence on the web, by such means as linking to or running TV ads online, or distributing QR Codes in outdoor or print media to direct mobile users to a website. In this way, they can utilise the easy accessibility and outreach capabilities the Internet affords, as thereby easily broadcast information throughout many different regions of the world simultaneously and cost-efficiently. Outdoor media transmit information via such media as AR advertising; billboards; blimps; flying billboards (signs in tow of airplanes); placards or kiosks placed inside and outside of buses, commercial buildings, shops, sports stadiums, subway cars, or trains; signs; or skywriting. Print media transmit information via physical objects, such as books, comics, magazines, newspapers, or pamphlets. Event organizing and public speaking can also be considered forms of mass media.
The organizations that control these technologies, such as movie studios, publishing companies, and radio and television stations, are also known as the mass media.
Issues with definition.
In the late 20th century, mass media could be classified into eight mass media industries: books, the Internet, magazines, movies, newspapers, radio, recordings, and television. The explosion of digital communication technology in the late 20th and early 21st centuries made prominent the question: what forms of media should be classified as "mass media"? For example, it is controversial whether to include cell phones, computer games (such as MMORPGs), and video games in the definition. In the 2000s, a classification called the "seven mass media" became popular. In order of introduction, they are:
Each mass medium has its own content types, creative artists, technicians, and business models. For example, the Internet includes blogs, podcasts, web sites, and various other technologies built atop the general distribution network. The sixth and seventh media, Internet and mobile phones, are often referred to collectively as digital media; and the fourth and fifth, radio and TV, as broadcast media. Some argue that video games have developed into a distinct mass form of media.
While a telephone is a two-way communication device, mass media communicates to a large group. In addition, the telephone has transformed into a cell phone which is equipped with Internet access. A question arises whether this makes cell phones a mass medium or simply a device used to access a mass medium (the Internet). There is currently a system by which marketers and advertisers are able to tap into satellites, and broadcast commercials and advertisements directly to cell phones, unsolicited by the phone's user. This transmission of mass advertising to millions of people is another form of mass communication.
Video games may also be evolving into a mass medium. Video games (for example massively multiplayer online role-playing games (MMORPGs, such as "RuneScape") provide a common gaming experience to millions of users across the globe and convey the same messages and ideologies to all their users. Users sometimes share the experience with one another by playing online. Excluding the Internet however, it is questionable whether players of video games are sharing a common experience when they play the game individually. It is possible to discuss in great detail the events of a video game with a friend one has never played with, because the experience is identical to each. The question, then, is whether this is a form of mass communication.
Characteristics.
Five characteristics of mass communication have been identified by sociologist John Thompson of Cambridge University:
Mass vs. mainstream and alternative.
The term "mass media" is sometimes erroneously used as a synonym for "mainstream media". Mainstream media are distinguished from alternative media by their content and point of view. Alternative media are also "mass media" outlets in the sense that they use technology capable of reaching many people, even if the audience is often smaller than the mainstream.
In common usage, the term "mass" denotes not that a given number of individuals receives the products, but rather that the products are available in principle to a plurality of recipients.
Mass vs. local and speciality.
Mass media are distinguished from local media by the notion that whilst mass media aims to reach a very large market, such as the entire population of a country, local media broadcasts to a much smaller population and area, and generally focuses on regional news rather than global events. A third type of media, speciality media, provide for specific demographics, such as specialty channels on TV (sports channels, porn channels, etc.). These definitions are not set in stone, and it is possible for a media outlet to be promoted in status from a local media outlet to a global media outlet. Some local media, which take an interest in state or provincial news, can rise to prominence because of their investigative journalism, and to the local region's preference of updates in national politics rather than regional news. "The Guardian", formerly known as the "Manchester Guardian", is an example of one such media outlet; once a regional daily newspaper, "The Guardian" is currently a nationally respected paper.
Forms of mass media.
Broadcast.
The sequencing of content in a broadcast is called a schedule. With all technological endeavours a number of technical terms and slang have developed. Please see the list of broadcasting terms for a glossary of terms used.
Radio and television programs are distributed over frequency bands that in the United States are highly regulated. Such regulation includes determination of the width of the bands, range, licensing, types of receivers and transmitters used, and acceptable content.
Cable television programs are often broadcast simultaneously with radio and television programs, but have a more limited audience. By coding signals and requiring a cable converter box at individual recipients' locations, cable also enables subscription-based channels and pay-per-view services.
A broadcasting organisation may broadcast several programs simultaneously, through several channels (frequencies), for example BBC One and Two. On the other hand, two or more organisations may share a channel and each use it during a fixed part of the day, such as the Cartoon Network/Adult Swim. Digital radio and digital television may also transmit multiplexed programming, with several channels compressed into one ensemble.
When broadcasting is done via the Internet the term webcasting is often used. In 2004, a new phenomenon occurred when a number of technologies combined to produce podcasting. Podcasting is an asynchronous broadcast/narrowcast medium. Adam Curry and his associates, the "Podshow", are principal proponents of podcasting.
Film.
The term 'film' encompasses motion pictures as individual projects, as well as the field in general. The name comes from the photographic film (also called filmstock), historically the primary medium for recording and displaying motion pictures. Many other terms exist: "motion pictures" (or just "pictures" and "picture"), "the silver screen", "photoplays", "the cinema", "picture shows", "flicks", and commonly "movies".
Films are produced by recording people and objects with cameras, or by creating them using animation techniques and/or special effects. Films comprise a series of individual frames, but when these images are shown in rapid succession, an illusion of motion is created. Flickering between frames is not seen because of an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion: a psychological effect identified as beta movement.
Film is considered by many to be an important art form; films entertain, educate, enlighten, and inspire audiences. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the film message. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.
Video games.
A video game is a computer-controlled game in which a video display, such as a monitor or television, is the primary feedback device. The term "computer game" also includes games which display only text (and which can, therefore, theoretically be played on a teletypewriter) or which use other methods, such as sound or vibration, as their primary feedback device, but there are very few new games in these categories. There always must also be some sort of input device, usually in the form of button/joystick combinations (on arcade games), a keyboard and mouse/trackball combination (computer games), a controller (console games), or a combination of any of the above. Also, more esoteric devices have been used for input, e.g., the player's motion. Usually there are rules and goals, but in more open-ended games the player may be free to do whatever they like within the confines of the virtual universe.
In common usage, an "arcade game" refers to a game designed to be played in an establishment in which patrons pay to play on a per-use basis. A "computer game" or "PC game" refers to a game that is played on a personal computer. A "Console game" refers to one that is played on a device specifically designed for the use of such, while interfacing with a standard television set. A "video game" (or "videogame") has evolved into a catchall phrase that encompasses the aforementioned along with any game made for any other device, including, but not limited to, advanced calculators, mobile phones, PDAs, etc.
Audio recording and reproduction.
Sound recording and reproduction is the electrical or mechanical re-creation and/or amplification of sound, often as music. This involves the use of audio equipment such as microphones, recording devices, and loudspeakers. From early beginnings with the invention of the phonograph using purely mechanical techniques, the field has advanced with the invention of electrical recording, the mass production of the 78 record, the magnetic wire recorder followed by the tape recorder, the vinyl LP record. The invention of the compact cassette in the 1960s, followed by Sony's Walkman, gave a major boost to the mass distribution of music recordings, and the invention of digital recording and the compact disc in 1983 brought massive improvements in ruggedness and quality. The most recent developments have been in digital audio players.
An album is a collection of related audio recordings, released together to the public, usually commercially.
The term record album originated from the fact that 78 RPM Phonograph disc records were kept together in a book resembling a photo album. The first collection of records to be called an "album" was Tchaikovsky's "Nutcracker Suite", release in April 1909 as a four-disc set by Odeon records. It retailed for 16 shillings—about £15 in modern currency.
A music video (also promo) is a short film or video that accompanies a complete piece of music, most commonly a song. Modern music videos were primarily made and used as a marketing device intended to promote the sale of music recordings. Although the origins of music videos go back much further, they came into their own in the 1980s, when Music Television's format was based on them. In the 1980s, the term "rock video" was often used to describe this form of entertainment, although the term has fallen into disuse.
Music videos can accommodate all styles of filmmaking, including animation, live action films, documentaries, and non-narrative, abstract film.
Internet.
The Internet (also known simply as "the Net" or less precisely as "the Web") is a more interactive medium of mass media, and can be briefly described as "a network of networks". Specifically, it is the worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It consists of millions of smaller domestic, academic, business, and governmental networks, which together carry various information and services, such as email, online chat, file transfer, and the interlinked web pages and other documents of the World Wide Web.
Contrary to some common usage, the Internet and the World Wide Web are not synonymous: the Internet is the system of interconnected "computer networks", linked by copper wires, fiber-optic cables, wireless connections etc.; the Web is the contents, or the interconnected "documents", linked by hyperlinks and URLs. The World Wide Web is accessible through the Internet, along with many other services including e-mail, file sharing and others described below.
Toward the end of the 20th century, the advent of the World Wide Web marked the first era in which most individuals could have a means of exposure on a scale comparable to that of mass media. Anyone with a web site has the potential to address a global audience, although serving to high levels of web traffic is still relatively expensive. It is possible that the rise of peer-to-peer technologies may have begun the process of making the cost of bandwidth manageable. Although a vast amount of information, imagery, and commentary (i.e. "content") has been made available, it is often difficult to determine the authenticity and reliability of information contained in web pages (in many cases, self-published). The invention of the Internet has also allowed breaking news stories to reach around the globe within minutes. This rapid growth of instantaneous, decentralized communication is often deemed likely to change mass media and its relationship to society.
"Cross-media" means the idea of distributing the same message through different media channels. A similar idea is expressed in the news industry as "convergence". Many authors understand cross-media publishing to be the ability to publish in both print and on the web without manual conversion effort. An increasing number of wireless devices with mutually incompatible data and screen formats make it even more difficult to achieve the objective “create once, publish many”.
The Internet is quickly becoming the center of mass media. Everything is becoming accessible via the internet. Rather than picking up a newspaper, or watching the 10 o'clock news, people can log onto the internet to get the news they want, when they want it. For example, many workers listen to the radio through the Internet while sitting at their desk.
Even the education system relies on the Internet. Teachers can contact the entire class by sending one e-mail. They may have web pages on which students can get another copy of the class outline or assignments. Some classes have class blogs in which students are required to post weekly, with students graded on their contributions.
Blogs (web logs).
Blogging, too, has become a pervasive form of media. A blog is a website, usually maintained by an individual, with regular entries of commentary, descriptions of events, or interactive media such as images or video. Entries are commonly displayed in reverse chronological order, with most recent posts shown on top. Many blogs provide commentary or news on a particular subject; others function as more personal online diaries. A typical blog combines text, images and other graphics, and links to other blogs, web pages, and related media. The ability for readers to leave comments in an interactive format is an important part of many blogs. Most blogs are primarily textual, although some focus on art (artlog), photographs (photoblog), sketchblog, videos (vlog), music (MP3 blog), audio (podcasting) are part of a wider network of social media. Microblogging is another type of blogging which consists of blogs with very short posts.
RSS feeds.
RSS is a format for syndicating news and the content of news-like sites, including major news sites like Wired, news-oriented community sites like Slashdot, and personal blogs. It is a family of Web feed formats used to publish frequently updated content such as blog entries, news headlines, and podcasts. An RSS document (which is called a "feed" or "web feed" or "channel") contains either a summary of content from an associated web site or the full text. RSS makes it possible for people to keep up with web sites in an automated manner that can be piped into special programs or filtered displays.
Podcast.
A podcast is a series of digital-media files which are distributed over the Internet using syndication feeds for playback on portable media players and computers. The term podcast, like broadcast, can refer either to the series of content itself or to the method by which it is syndicated; the latter is also called podcasting. The host or author of a podcast is often called a podcaster.
Mobile.
Mobile phones were introduced in Japan in 1979 but became a mass media only in 1998 when the first downloadable ringing tones were introduced in Finland. Soon most forms of media content were introduced on mobile phones, tablets and other portable devices, and today the total value of media consumed on mobile vastly exceeds that of internet content, and was worth over 31 billion dollars in 2007 (source Informa). The mobile media content includes over 8 billion dollars worth of mobile music (ringing tones, ringback tones, truetones, MP3 files, karaoke, music videos, music streaming services etc.); over 5 billion dollars worth of mobile gaming; and various news, entertainment and advertising services. In Japan mobile phone books are so popular that five of the ten best-selling printed books were originally released as mobile phone books.
Similar to the internet, mobile is also an interactive media, but has far wider reach, with 3.3 billion mobile phone users at the end of 2007 to 1.3 billion internet users (source ITU). Like email on the internet, the top application on mobile is also a personal messaging service, but SMS text messaging is used by over 2.4 billion people. Practically all internet services and applications exist or have similar cousins on mobile, from search to multiplayer games to virtual worlds to blogs. Mobile has several unique benefits which many mobile media pundits claim make mobile a more powerful media than either TV or the internet, starting with mobile being permanently carried and always connected. Mobile has the best audience accuracy and is the only mass media with a built-in payment channel available to every user without any credit cards or PayPal accounts or even an age limit. Mobile is often called the 7th Mass Medium and either the fourth screen (if counting cinema, TV and PC screens) or the third screen (counting only TV and PC).
Print media.
Magazine.
A magazine is a periodical publication containing a variety of articles, generally financed by advertising and/or purchase by readers.
Magazines are typically published weekly, biweekly, monthly, bimonthly or quarterly, with a date on the cover that is in advance of the date it is actually published. They are often printed in color on coated paper, and are bound with a soft cover.
Magazines fall into two broad categories: consumer magazines and business magazines. In practice, magazines are a subset of periodicals, distinct from those periodicals produced by scientific, artistic, academic or special interest publishers which are subscription-only, more expensive, narrowly limited in circulation, and often have little or no advertising.
Magazines can be classified as:
Newspaper.
A newspaper is a publication containing news and information and advertising, usually printed on low-cost paper called newsprint. It may be general or special interest, most often published daily or weekly. The first printed newspaper was published in 1605, and the form has thrived even in the face of competition from technologies such as radio and television. Recent developments on the Internet are posing major threats to its business model, however. Paid circulation is declining in most countries, and advertising revenue, which makes up the bulk of a newspaper's income, is shifting from print to online; some commentators, nevertheless, point out that historically new media such as radio and television did not entirely supplant existing.
Outdoor media.
Outdoor media is a form of mass media which comprises billboards, signs, placards placed inside and outside of commercial buildings/objects like shops/buses, flying billboards (signs in tow of airplanes), blimps, skywriting, AR Advertising. Many commercial advertisers use this form of mass media when advertising in sports stadiums. Tobacco and alcohol manufacturers used billboards and other outdoor media extensively. However, in 1998, the Master Settlement Agreement between the US and the tobacco industries prohibited the billboard advertising of cigarettes. In a 1994 Chicago-based study, Diana Hackbarth and her colleagues revealed how tobacco- and alcohol-based billboards were concentrated in poor neighbourhoods. In other urban centers, alcohol and tobacco billboards were much more concentrated in African-American neighborhoods than in white neighborhoods.
Purposes.
Mass media encompasses much more than just news, although it is sometimes misunderstood in this way. It can be used for various purposes:
Professions involving mass media.
Journalism.
Journalism is the discipline of collecting, analyzing, verifying and presenting information regarding current events, trends, issues and people. Those who practice journalism are known as journalists.
News-oriented journalism is sometimes described as the "first rough draft of history" (attributed to Phil Graham), because journalists often record important events, producing news articles on short deadlines. While under pressure to be first with their stories, news media organizations usually edit and proofread their reports prior to publication, adhering to each organization's standards of accuracy, quality and style. Many news organizations claim proud traditions of holding government officials and institutions accountable to the public, while media critics have raised questions about holding the press itself accountable to the standards of professional journalism.
Public relations.
Public relations is the art and science of managing communication between an organization and its key publics to build, manage and sustain its positive image. Examples include:
Publishing.
Publishing is the industry concerned with the production of literature or information – the activity of making information available for public view. In some cases, authors may be their own publishers.
Traditionally, the term refers to the distribution of printed works such as books and newspapers. With the advent of digital information systems and the Internet, the scope of publishing has expanded to include websites, blogs, and the like.
As a business, publishing includes the development, marketing, production, and distribution of newspapers, magazines, books, literary works, musical works, software, other works dealing with information.
Publication is also important as a legal concept; (1) as the process of giving formal notice to the world of a significant intention, for example, to marry or enter bankruptcy, and; (2) as the essential precondition of being able to claim defamation; that is, the alleged libel must have been published.
Software publishing.
A software publisher is a publishing company in the software industry between the developer and the distributor. In some companies, two or all three of these roles may be combined (and indeed, may reside in a single person, especially in the case of shareware).
Software publishers often license software from developers with specific limitations, such as a time limit or geographical region. The terms of licensing vary enormously, and are typically secret.
Developers may use publishers to reach larger or foreign markets, or to avoid focussing on marketing. Or publishers may use developers to create software to meet a market need that the publisher has identified.
History.
The history of mass media can be traced back to the days when dramas were performed in various ancient cultures. This was the first time when a form of media was "broadcast" to a wider audience. The first dated printed book known is the "Diamond Sutra", printed in China in 868 AD, although it is clear that books were printed earlier. Movable clay type was invented in 1041 in China. However, due to the slow spread of literacy to the masses in China, and the relatively high cost of paper there, the earliest printed mass-medium was probably European popular prints from about 1400. Although these were produced in huge numbers, very few early examples survive, and even most known to be printed before about 1600 have not survived. The term "mass media" was coined with the creation of print media, which is notable for being the first example of mass media, as we use the term today. This form of media started in Europe in the Middle Ages.
Johannes Gutenberg's invention of the printing press allowed the mass production of books to sweep the nation. He printed the first book, a Latin Bible, on a printing press with movable type in 1453. The invention of the printing press gave rise to some of the first forms of mass communication, by enabling the publication of books and newspapers on a scale much larger than was previously possible. The invention also transformed the way the world received printed materials, although books remained too expensive really to be called a mass-medium for at least a century after that. Newspapers developed from about 1612, with the first example in English in 1620; but they took until the 19th century to reach a mass-audience directly. The first high-circulation newspapers arose in London in the early 1800s, such as The Times, and were made possible by the invention of high-speed rotary steam printing presses, and railroads which allowed large-scale distribution over wide geographical areas. The increase in circulation, however, led to a decline in feedback and interactivity from the readership, making newspapers a more one-way medium.
The phrase "the media" began to be used in the 1920s. The notion of "mass media" was generally restricted to print media up until the post-Second World War, when radio, television and video were introduced. The audio-visual facilities became very popular, because they provided both information and entertainment, because the colour and sound engaged the viewers/listeners and because it was easier for the general public to passively watch TV or listen to the radio than to actively read. In recent times, the Internet become the latest and most popular mass medium. Information has become readily available through websites, and easily accessible through search engines. One can do many activities at the same time, such as playing games, listening to music, and social networking, irrespective of location. Whilst other forms of mass media are restricted in the type of information they can offer, the internet comprises a large percentage of the sum of human knowledge through such things as Google Books. Modern day mass media consists of the internet, mobile phones, blogs, podcasts and RSS feeds.
During the 20th century, the growth of mass media was driven by technology, including that which allowed much duplication of material. Physical duplication technologies such as printing, record pressing and film duplication allowed the duplication of books, newspapers and movies at low prices to huge audiences. Radio and television allowed the electronic duplication of information for the first time. Mass media had the economics of linear replication: a single work could make money. An example of Riel and Neil's theory. proportional to the number of copies sold, and as volumes went up, unit costs went down, increasing profit margins further. Vast fortunes were to be made in mass media. In a democratic society, the media can serve the electorate about issues regarding government and corporate entities (see Media influence). Some consider the concentration of media ownership to be a threat to democracy.
Influence and sociology.
Limited-effects theory, originally tested in the 1940s and 1950s, considers that because people usually choose what media to interact with based on what they already believe, media exerts a negligible influence. Class-dominant theory argues that the media reflects and projects the view of a minority elite, which controls it. Culturalist theory, which was developed in the 1980s and 1990s, combines the other two theories and claims that people interact with media to create their own meanings out of the images and messages they receive. This theory states that audience members play an active, rather than passive role in relation to mass media.
In an article entitled "Mass Media Influence on Society", rayuso argues that the media "in the US" is dominated by five major companies (Time Warner, VIACOM, Vivendi Universal, Walt Disney and News Corp) which own 95% of all mass media including theme parks, movie studios, television and radio broadcast networks and programing, video news, sports entertainment, telecommunications, wireless phones, video games software, electronic media and music companies. Whilst historically, there was more diversity in companies, they have recently merged to form an elite which have the power to shape the opinion and beliefs of people. People buy after seeing thousands of advertisements by various companies in TV, newspapers or magazines, which are able to affect their purchasing decisions. The definition of what is acceptable by society is dictated by the media. This power can be used for good, for example encouraging children to play sport. However, it can also be used for bad, for example children being influenced by cigars smoked by film stars, their exposure to sex images, their exposure to images of violence and their exposure to junk food ads. The documentary Supersize Me describes how companies like McDonalds have been sued in the past, the plaintiffs claiming that it was the fault of their liminal and subliminal advertising that "forced" them to perchance the product. The Barbie and Ken dolls of the 1950s are sometimes cited as the main cause for the obsession in modern-day society for women to be skinny and men to be buff. After the attacks of 9/11, the media gave extensive coverage of the event and exposed Osama Bin Laden's guilt for the attack, information they were told by the authorities. This shaped the public opinion to support the war on terrorism, and later, the war on Iraq. A main concern is that due to this immense power of the mass media (being able to drive the public opinion), media receiving inaccurate information could cause the public opinion to support the wrong cause.
In his book The Commercialization of American Culture, Matthew P. McAllister says that "a well-developed media system, informing and teaching its citizens, helps democracy move toward its ideal state."
In 1997, J. R. Finnegan Jr. and K. Viswanath identified 3 main effects or functions of mass media:
Since the 1950s, when cinema, radio and TV began to be the primary or the only source of information for a larger and larger percentage of the population, these media began to be considered as central instruments of mass control. Up to the point that it emerged the idea that when a country has reached a high level of industrialization, the country itself "belongs to the person who controls communications."
Mass media play a significant role in shaping public perceptions on a variety of important issues, both through the information that is dispensed through them, and through the interpretations they place upon this information. They also play a large role in shaping modern culture, by selecting and portraying a particular set of beliefs, values, and traditions (an entire way of life), as reality. That is, by portraying a certain interpretation of reality, they shape reality to be more in line with that interpretation. Mass media also play a crucial role in the spread of civil unrest activities such as anti-government demonstrations, riots, and general strikes. That is, the use of radio and television receivers has made the unrest influence among cities not only by the geographic location of cities, but also by proximity within the mass media distribution networks.
Racism and stereotyping.
Mass media sources, through theories like framing and agenda-setting, can affect the scope of a story as particular facts and information are highlighted (Media influence). This can directly correlate with how individuals may perceive certain groups of people, as the only media coverage a person receives can be very limited and may not reflect the whole story or situation; stories are often covered to reflect a particular perspective to target a specific demographic.
Example.
According to Stephen Balkaran, an Instructor of Political Science and African American Studies at Central Connecticut State University, mass media has played a large role in the way white Americans perceive African-Americans. The media focus on African-American in the contexts of crime, drug use, gang violence, and other forms of anti-social behavior has resulted in a distorted and harmful public perception of African-Americans. African-Americans have been subjected to oppression and discrimination for the past few hundred years. According to Stephen Balkaran in his article Mass Media and Racism, "The media has played a key role in perpetuating the effects of this historical oppression and in contributing to African-Americans' continuing status as second-class citizens". This has resulted in an uncertainty among white Americans as to what the genuine nature of African-Americans really is. Despite the resulting racial divide, the fact that these people are undeniably American has "raised doubts about the white man's value system". This means that there is a somewhat "troubling suspicion" among some Americans that their white America is tainted by the black influence. Mass media as well as propaganda tend to reinforce or introduce stereotypes to the general public.
Ethical issues and criticism.
Lack of local or specific topical focus is a common criticism of mass media. A mass news media outlet is often forced to cover national and international news due to it having to cater for and be relevant for a wide demographic. As such, it has to skip over many interesting or important local stories because they simply do not interest the large majority of their viewers. An example given by the website WiseGeek is that "the residents of a community might view their fight against development as critical, but the story would only attract the attention of the mass media if the fight became controversial or if precedents of some form were set".
The term "mass" suggests that the recipients of media products constitute a vast sea of passive, undifferentiated individuals. This is an image associated with some earlier critiques of "mass culture" and mass society which generally assumed that the development of mass communication has had a largely negative impact on modern social life, creating a kind of bland and homogeneous culture which entertains individuals without challenging them. However, interactive digital media have also been seen to challenge the read-only paradigm of earlier broadcast media.
Whilst some refer to the mass media as "opiate of the masses", others argue that is a vital aspect of human societies. By understanding mass media, one is then able to analyse and find a deeper understanding of one's population and culture. This valuable and powerful ability is one reason why the field of media studies is popular. As WiseGeek says, "watching, reading, and interacting with a nation's mass media can provide clues into how people think, especially if a diverse assortment of mass media sources are perused".
Since the 1950s, in the countries that have reached a high level of industrialization, the mass media of cinema, radio and TV have a key role in political power.
Contemporary research demonstrates an increasing level of concentration of media ownership, with many media industries already highly concentrated and dominated by a very small number of firms.
Future.
In 2002, Arnold Kling wrote that "the newspaper business is going to die within the next twenty years. Newspaper publishing will continue, but only as a philanthropic venture." Jim Pinkerton said in 2006 of the future of mass media, "Every country with ambitions on the international stage will soon have its own state-supported media."
Leo Laporte, founder of the TWiT network of podcasts, says that "there will always be a need for storytellers, people who dig up facts and explain them".

</doc>
<doc id="19643" url="https://en.wikipedia.org/wiki?curid=19643" title="Mahabharata">
Mahabharata

The Mahabharata or Mahābhārata (US ; UK ; , "", ) is one of the two major Sanskrit epics of ancient India, the other being the "Ramayana".
The "Mahabharata" is an epic narrative of the Kurukshetra War and the fates of the Kaurava and the Pandava princes. It also contains philosophical and devotional material, such as a discussion of the four "goals of life" or "purusharthas" (12.161). Among the principal works and stories in the "Mahabharata" are the "Bhagavad Gita", the story of Damayanti, an abbreviated version of the Ramayana, and the Rishyasringa, often considered as works in their own right.
Traditionally, the authorship of the "Mahabharata" is attributed to Vyasa. There have been many attempts to unravel its historical growth and compositional layers. The oldest preserved parts of the text are thought to be not much older than around 400 BCE, though the origins of the epic probably fall between the 8th and 9th centuries BCE. The text probably reached its final form by the early Gupta period (c. 4th century CE). The title may be translated as "the great tale of the Bhārata dynasty". According to the "Mahabharata" itself, the tale is extended from a shorter version of 24,000 verses called simply "Bhārata".
The "Mahabharata" is the longest known epic poem and has been described as "the longest poem ever written". Its longest version consists of over 100,000 "shloka" or over 200,000 individual verse lines (each shloka is a couplet), and long prose passages. About 1.8 million words in total, the "Mahabharata" is roughly ten times the length of the "Iliad" and the "Odyssey" combined, or about four times the length of the Ramayana. W. J. Johnson has compared the importance of the "Mahabharata" in the context of world civilization to that of the Bible, the works of Shakespeare, the works of Homer, Greek drama, or the Qur'an.
Textual history and structure.
The epic is traditionally ascribed to the sage Vyasa, who is also a major character in the epic. Vyasa described it as being "itihāsa" (history). He also describes the Guru-shishya parampara, which traces all great teachers and their students of the Vedic times.
The first section of the "Mahabharata" states that it was Ganesha who wrote down the text to Vyasa's dictation. Ganesha is said to have agreed to write it only if Vyasa never paused in his recitation. Vyasa agrees on condition that Ganesha takes the time to understand what was said before writing it down.
The epic employs the story within a story structure, otherwise known as frametales, popular in many Indian religious and non-religious works. It is recited by the sage Vaisampayana, a disciple of Vyasa, to the King Janamejaya who is the great-grandson of the Pandava prince Arjuna. The story is then recited again by a professional storyteller named Ugrasrava Sauti, many years later, to an assemblage of sages performing the 12-year sacrifice for the king Saunaka Kulapati in the Naimisha Forest.
The text has been described by some early 20th-century western Indologists as unstructured and chaotic. Hermann Oldenberg supposed that the original poem must once have carried an immense "tragic force" but dismissed the full text as a "horrible chaos." Moritz Winternitz ("Geschichte der indischen Literatur" 1909) considered that "only unpoetical theologists and clumsy scribes" could have lumped the parts of disparate origin into an unordered whole.
Accretion and redaction.
Research on the "Mahabharata" has put an enormous effort into recognizing and dating layers within the text. Some elements of the present "Mahabharata" can be traced back to Vedic times. The background to the "Mahabharata" suggests the origin of the epic occurs "after the very early Vedic period" and before "the first Indian 'empire' was to rise in the third century B.C." That this is "a date not too far removed from the 8th or 9th century B.C." is likely. It is generally agreed that "Unlike the Vedas, which have to be preserved letter-perfect, the epic was a popular work whose reciters would inevitably conform to changes in language and style," so the earliest 'surviving' components of this dynamic text are believed to be no older than the earliest 'external' references we have to the epic, which may include an allusion in Panini's 4th century BCE grammar "Ashtādhyāyī" 4:2:56. It is estimated that the Sanskrit text probably reached something of a "final form" by the early Gupta period (about the 4th century CE). Vishnu Sukthankar, editor of the first great critical edition of the "Mahabharata", commented: "It is useless to think of reconstructing a fluid text in a literally original shape, on the basis of an archetype and a "stemma codicum". What then is possible? Our objective can only be to reconstruct "the oldest form of the text which it is possible to reach" on the basis of the manuscript material available." That manuscript evidence is somewhat late, given its material composition and the climate of India, but it is very extensive.
The "Mahabharata" itself (1.1.61) distinguishes a core portion of 24,000 verses: the "Bharata" proper, as opposed to additional secondary material, while the "Ashvalayana Grhyasutra" (3.4.4) makes a similar distinction. At least three redactions of the text are commonly recognized: "Jaya" (Victory) with 8,800 verses attributed to Vyasa, "Bharata" with 24,000 verses as recited by Vaisampayana, and finally the "Mahabharata" as recited by Ugrasrava Sauti with over 100,000 verses. However, some scholars, such as John Brockington, argue that "Jaya" and "Bharata" refer to the same text, and ascribe the theory of "Jaya" with 8,800 verses to a misreading of a verse in "Adiparvan" (1.1.81).
The redaction of this large body of text was carried out after formal principles, emphasizing the numbers 18 and 12. The addition of the latest parts may be dated by the absence of the "Anushasana-parva" and the "Virata parva" from the "Spitzer manuscript". The oldest surviving Sanskrit text dates to the Kushan Period (200 CE).
According to what one character says at Mbh. 1.1.50, there were three versions of the epic, beginning with "Manu" (1.1.27), "Astika" (1.3, sub-parva 5) or "Vasu" (1.57), respectively. These versions would correspond to the addition of one and then another 'frame' settings of dialogues. The "Vasu" version would omit the frame settings and begin with the account of the birth of Vyasa. The "astika" version would add the "sarpasattra" and "ashvamedha" material from Brahmanical literature, introduce the name "Mahabharata", and identify Vyasa as the work's author. The redactors of these additions were probably Pancharatrin scholars who according to Oberlies (1998) likely retained control over the text until its final redaction. Mention of the Huna in the "Bhishma-parva" however appears to imply that this parva may have been edited around the 4th century.
The Adi-parva includes the snake sacrifice ("sarpasattra") of Janamejaya, explaining its motivation, detailing why all snakes in existence were intended to be destroyed, and why in spite of this, there are still snakes in existence. This "sarpasattra" material was often considered an independent tale added to a version of the "Mahabharata" by "thematic attraction" (Minkowski 1991), and considered to have a particularly close connection to Vedic (Brahmana) literature. The Panchavimsha Brahmana (at 25.15.3) enumerates the officiant priests of a "sarpasattra" among whom the names Dhrtarashtra and Janamejaya, two main characters of the "Mahabharata"'s "sarpasattra", as well as Takshaka, the name of a snake in the "Mahabharata", occur.
Historical references.
The earliest known references to the "Mahabharata" and its core "Bharata" date to the "Ashtadhyayi" (sutra 6.2.38) of Pāṇini ("fl." 4th century BCE) and in the "Ashvalayana Grhyasutra" (3.4.4). This may mean the core 24,000 verses, known as the "Bharata", as well as an early version of the extended "Mahabharata", were composed by the 4th century BCE.
A report by the Greek writer Dio Chrysostom (c. 40 - c. 120 CE) about Homer's poetry being sung even in India seems to imply that the "Iliad" had been translated into Sanskrit. However, scholars have, in general, taken this as evidence for the existence of a "Mahabharata" at this date, whose episodes Dio or his sources identify with the story of the "Iliad".
Several stories within the "Mahabharata" took on separate identities of their own in Classical Sanskrit literature. For instance, Abhijñānashākuntala by the renowned Sanskrit poet Kālidāsa (c. 400 CE), believed to have lived in the era of the Gupta dynasty, is based on a story that is the precursor to the "Mahabharata". Urubhanga, a Sanskrit play written by Bhāsa who is believed to have lived before Kālidāsa, is based on the slaying of Duryodhana by the splitting of his thighs by Bhima.
The copper-plate inscription of the Maharaja Sharvanatha (533–534 CE) from Khoh (Satna District, Madhya Pradesh) describes the "Mahabharata" as a "collection of 100,000 verses" ("shatasahasri samhita").
The 18 parvas or books.
The division into 18 parvas is as follows:
Historical context.
The historicity of the Kurukshetra War is unclear. Many historians estimate the date of the Kurukshetra war to Iron Age India of the 10th century BCE. The setting of the epic has a historical precedent in Iron Age (Vedic) India, where the Kuru kingdom was the center of political power during roughly 1200 to 800 BCE. A dynastic conflict of the period could have been the inspiration for the "Jaya", the foundation on which the Mahabharata corpus was built, with a climactic battle eventually coming to be viewed as an epochal event.
Puranic literature presents genealogical lists associated with the "Mahabharata" narrative. The evidence of the Puranas is of two kinds. Of the first kind, there is the direct statement that there were 1015 (or 1050) years between the birth of Parikshit (Arjuna's grandson) and the accession of Mahapadma Nanda (400-329 BCE), which would yield an estimate of about 1400 BCE for the Bharata battle. However, this would imply improbably long reigns on average for the kings listed in the genealogies.
Of the second kind are analyses of parallel genealogies in the Puranas between the times of Adhisimakrishna (Parikshit's great-grandson) and Mahapadma Nanda. Pargiter accordingly estimated 26 generations by averaging 10 different dynastic lists and, assuming 18 years for the average duration of a reign, arrived at an estimate of 850 BCE for Adhisimakrishna, and thus approximately 950 BCE for the Bharata battle.
B. B. Lal used the same approach with a more conservative assumption of the average reign to estimate a date of 836 BCE, and correlated this with archaeological evidence from Painted Grey Ware sites, the association being strong between PGW artifacts and places mentioned in the epic.
Attempts to date the events using methods of archaeoastronomy have produced, depending on which passages are chosen and how they are interpreted, estimates ranging from the late 4th to the mid-2nd millennium BCE.
The late 4th millennium date has a precedent in the calculation of the Kaliyuga epoch, based on planetary conjunctions, by Aryabhata (6th century). Aryabhatta's date of February 18 3102 BCE for Mahabharata war has become widespread in Indian tradition. Some sources mark this as the disappearance of Krishna from earth. The Aihole inscription of Pulikeshi II, dated to Saka 556 = 634 CE, claims that 3735 years have elapsed since the Bharata battle, putting the date of Mahabharata war at 3137 BCE.
Another traditional school of astronomers and historians, represented by Vriddha-Garga, Varahamihira (author of the "Brhatsamhita") and Kalhana (author of the "Rajatarangini"), place the Bharata war 653 years after the Kaliyuga epoch, corresponding to 2449 BCE.
Synopsis.
The core story of the work is that of a dynastic struggle for the throne of Hastinapura, the kingdom ruled by the Kuru clan. The two collateral branches of the family that participate in the struggle are the Kaurava and the Pandava. Although the Kaurava is the senior branch of the family, Duryodhana, the eldest Kaurava, is younger than Yudhishthira, the eldest Pandava. Both Duryodhana and Yudhishthira claim to be first in line to inherit the throne.
The struggle culminates in the great battle of Kurukshetra, in which the Pandavas are ultimately victorious. The battle produces complex conflicts of kinship and friendship, instances of family loyalty and duty taking precedence over what is right, as well as the converse.
The "Mahabharata" itself ends with the death of Krishna, and the subsequent end of his dynasty and ascent of the Pandava brothers to heaven. It also marks the beginning of the Hindu age of Kali Yuga, the fourth and final age of humankind, in which great values and noble ideas have crumbled, and people are heading towards the complete dissolution of right action, morality and virtue.
The older generations.
King Janamejaya's ancestor Shantanu, the king of Hastinapura, has a short-lived marriage with the goddess Ganga and has a son, Devavrata (later to be called Bhishma, a great warrior), who becomes the heir apparent. Many years later, when King Shantanu goes hunting, he sees Satyavati, the daughter of the chief of fisherman, and asks her father for her hand. Her father refuses to consent to the marriage unless Shantanu promises to make any future son of Satyavati the king upon his death. To resolve his father's dilemma, Devavrata agrees to relinquish his right to the throne. As the fisherman is not sure about the prince's children honouring the promise, Devavrata also takes a vow of lifelong celibacy to guarantee his father's promise.
Shantanu has two sons by Satyavati, Chitrāngada and Vichitravirya. Upon Shantanu's death, Chitrangada becomes king. He lives a very short uneventful life and dies. Vichitravirya, the younger son, rules Hastinapura. Meanwhile, the King of Kāśī arranges a swayamvara for his three daughters, neglecting to invite the royal family of Hastinapur. In order to arrange the marriage of young Vichitravirya, Bhishma attends the swayamvara of the three princesses Amba, Ambika and Ambalika, uninvited, and proceeds to abduct them. Ambika and Ambalika consent to be married to Vichitravirya.
The oldest princess Amba, however, informs Bhishma that she wishes to marry king of Shalva whom Bhishma defeated at their swayamvara. Bhishma lets her leave to marry king of Shalva, but Shalva refuses to marry her, still smarting at his humiliation at the hands of Bhishma. Amba then returns to marry Bhishma but he refuses due to his vow of celibacy. Amba becomes enraged and becomes Bhishma's bitter enemy, holding him responsible for her plight. Later she is reborn to King Drupada as Shikhandi (or Shikhandini) and causes Bhishma's fall, with the help of Arjuna, in the battle of Kurukshetra.
The Pandava and Kaurava princes.
When Vichitravirya dies young without any heirs, Satyavati asks her first son Vyasa to father children with the widows. The eldest, Ambika, shuts her eyes when she sees him, and so her son Dhritarashtra is born blind. Ambalika turns pale and bloodless upon seeing him, and thus her son Pandu is born pale and unhealthy (the term Pandu may also mean 'jaundiced'). Due to the physical challenges of the first two children, Satyavati asks Vyasa to try once again. However, Ambika and Ambalika send their maid instead, to Vyasa's room. Vyasa fathers a third son, Vidura, by the maid. He is born healthy and grows up to be one of the wisest characters in the "Mahabharata". He serves as Prime Minister (Mahamantri or Mahatma) to King Pandu and King Dhritarashtra.
When the princes grow up, Dhritarashtra is about to be crowned king by Bhishma when Vidura intervenes and uses his knowledge of politics to assert that a blind person cannot be king. This is because a blind man cannot control and protect his subjects. The throne is then given to Pandu because of Dhritarashtra's blindness. Pandu marries twice, to Kunti and Madri. Dhritarashtra marries Gandhari, a princess from Gandhara, who blindfolds herself so that she may feel the pain that her husband feels. Her brother Shakuni is enraged by this and vows to take revenge on the Kuru family. One day, when Pandu is relaxing in the forest, he hears the sound of a wild animal. He shoots an arrow in the direction of the sound. However the arrow hits the sage Kindama, who curses him that if he engages in a sexual act, he will die. Pandu then retires to the forest along with his two wives, and his brother Dhritarashtra rules thereafter, despite his blindness.
Pandu's older queen Kunti, however, had been given a boon by Sage Durvasa that she could invoke any god using a special mantra. Kunti uses this boon to ask Dharma the god of justice, Vayu the god of the wind, and Indra the lord of the heavens for sons. She gives birth to three sons, Yudhishthira, Bhima, and Arjuna, through these gods. Kunti shares her mantra with the younger queen Madri, who bears the twins Nakula and Sahadeva through the Ashwini twins. However, Pandu and Madri indulge in sex, and Pandu dies. Madri dies on his funeral pyre out of remorse. Kunti raises the five brothers, who are from then on usually referred to as the Pandava brothers.
Dhritarashtra has a hundred sons through Gandhari, all born after the birth of Yudhishthira. These are the Kaurava brothers, the eldest being Duryodhana, and the second Dushasana. Other Kaurava brothers were Vikarna and Sukarna. The rivalry and enmity between them and the Pandava brothers, from their youth and into manhood, leads to the Kurukshetra war.
Lakshagraha (the house of lac).
After the deaths of their mother (Madri) and father (Pandu), the Pandavas and their mother Kunti return to the palace of Hastinapur. Yudhishthira is made Crown Prince by Dhritarashtra, under considerable pressure from his kingdom. Dhritarashtra wanted his own son Duryodhana to become king and lets his ambition get in the way of preserving justice.
Shakuni, Duryodhana and Dusasana plot to get rid of the Pandavas. Shakuni calls the architect Purochana to build a palace out of flammable materials like lac and ghee. He then arranges for the Pandavas and the Queen Mother Kunti to stay there, with the intention of setting it alight. However, the Pandavas are warned by their wise uncle, Vidura, who sends them a miner to dig a tunnel. They are able to escape to safety and go into hiding. Back at Hastinapur, the Pandavas and Kunti are presumed dead.
Marriage to Draupadi.
Whilst they were in hiding the Pandavas learn of a swayamvara which is taking place for the hand of the Pāñcāla princess Draupadī. The Pandavas enter the competition in disguise as Brahmins. The task is to string a mighty steel bow and shoot a target on the ceiling, which is the eye of a moving artificial fish, while looking at its reflection in oil below. Most of the princes fail, many being unable to lift the bow. Arjuna succeeds however. The Pandavas return home and inform their mother that Arjuna has won a competition and to look at what they have brought back. Without looking, Kunti asks them to share whatever it is Arjuna has won among themselves. On explaining the previous life of Draupadi, she ends up being the wife of all five brothers.
Indraprastha.
After the wedding, the Pandava brothers are invited back to Hastinapura. The Kuru family elders and relatives negotiate and broker a split of the kingdom, with the Pandavas obtaining a new territory. Yudhishthira has a new capital built for this territory at Indraprastha. Neither the Pandava nor Kaurava sides are happy with the arrangement however.
Shortly after this, Arjuna elopes with and then marries Krishna's sister, Subhadra. Yudhisthra wishes to establish his position as king; he seeks Krishna's advice. Krishna advises him, and after due preparation and the elimination of some opposition, Yudhishthira carries out the "rājasūya yagna" ceremony; he is thus recognised as pre-eminent among kings.
The Pandavas have a new palace built for them, by Maya the Danava. They invite their Kaurava cousins to Indraprastha. Duryodhana walks round the palace, and mistakes a glossy floor for water, and will not step in. After being told of his error, he then sees a pond, and assumes it is not water and falls in. Draupadi laughs at him and ridicules him by saying that this is because of his blind father Dhritrashtra. He then decides to avenge his humiliation.
The dice game.
Shakuni, Duryodhana's uncle, now arranges a dice game, playing against Yudhishthira with loaded dice. Yudhishthira loses all his wealth, then his kingdom. He then even gambles his brothers, himself, and finally his wife into servitude. The jubilant Kauravas insult the Pandavas in their helpless state and even try to disrobe Draupadi in front of the entire court, but her honour is saved by Krishna who miraculously creates lengths of cloth to replace the ones being removed.
Dhritarashtra, Bhishma, and the other elders are aghast at the situation, but Duryodhana is adamant that there is no place for two crown princes in Hastinapura. Against his wishes Dhritarashtra orders for another dice game. The Pandavas are required to go into exile for 12 years, and in the 13th year must remain hidden. If discovered by the Kauravas, they will be forced into exile for another 12 years.
Exile and return.
The Pandavas spend thirteen years in exile; many adventures occur during this time. They also prepare alliances for a possible future conflict. They spend their final year in disguise in the court of Virata, and are discovered just after the end of the year.
At the end of their exile, they try to negotiate a return to Indraprastha. However, this fails, as Duryodhana objects that they were discovered while in hiding, and that no return of their kingdom was agreed. War becomes inevitable.
The battle at Kurukshetra.
The two sides summon vast armies to their help and line up at Kurukshetra for a war. The kingdoms of Panchala, Dwaraka, Kasi, Kekaya, Magadha, Matsya, Chedi, Pandyas, Telinga, and the Yadus of Mathura and some other clans like the Parama Kambojas were allied with the Pandavas. The allies of the Kauravas included the kings of Pragjyotisha, Anga, Kekaya, Sindhudesa (including Sindhus, Sauviras and Sivis), Mahishmati, Avanti in Madhyadesa, Madra, Gandhara, Bahlika people, Kambojas and many others. Before war being declared, Balarama had expressed his unhappiness at the developing conflict and left to go on pilgrimage; thus he does not take part in the battle itself. Krishna takes part in a non-combatant role, as charioteer for Arjuna.
Before the battle, Arjuna, seeing the opposing army includes many relatives and loved ones, including his great grandfather Bhishma and his teacher Drona, has doubts about the battle and he fails to lift his Gāndeeva bow. Krishna wakes him up to his call of duty in the famous Bhagavad Gita section of the epic.
Though initially sticking to chivalrous notions of warfare, both sides soon adopt dishonourable tactics. At the end of the 18-day battle, only the Pandavas, Satyaki, Kripa, Ashwatthama, Kritavarma, Yuyutsu and Krishna survive.
The end of the Pandavas.
After "seeing" the carnage, Gandhari, who had lost all her sons, curses Krishna to be a witness to a similar annihilation of his family, for though divine and capable of stopping the war, he had not done so. Krishna accepts the curse, which bears fruit 36 years later.
The Pandavas, who had ruled their kingdom meanwhile, decide to renounce everything. Clad in skins and rags they retire to the Himalaya and climb towards heaven in their bodily form. A stray dog travels with them. One by one the brothers and Draupadi fall on their way. As each one stumbles, Yudhishthira gives the rest the reason for their fall (Draupadi was partial to Arjuna, Nakula and Sahadeva were vain and proud of their looks, and Bhima and Arjuna were proud of their strength and archery skills, respectively). Only the virtuous Yudhishthira, who had tried everything to prevent the carnage, and the dog remain. The dog reveals himself to be the god Yama (also known as Yama Dharmaraja), and then takes him to the underworld where he sees his siblings and wife. After explaining the nature of the test, Yama takes Yudhishthira back to heaven and explains that it was necessary to expose him to the underworld because (Rajyante narakam dhruvam) any ruler has to visit the underworld at least once. Yama then assures him that his siblings and wife would join him in heaven after they had been exposed to the underworld for measures of time according to their vices.
Arjuna's grandson Parikshit rules after them and dies bitten by a snake. His furious son, Janamejaya, decides to perform a snake sacrifice ("sarpasattra") in order to destroy the snakes. It is at this sacrifice that the tale of his ancestors is narrated to him.
The reunion.
The "Mahabharata" mentions that Karna, the Pandavas, and Dhritarashtra's sons eventually ascended to svarga and "attained the state of the gods" and banded together — "serene and free from anger."
Themes.
Just war.
The "Mahabharata" offers one of the first instances of theorizing about "just war", illustrating many of the standards that would be debated later across the world. In the story, one of five brothers asks if the suffering caused by war can ever be justified. A long discussion ensues between the siblings, establishing criteria like "proportionality" (chariots cannot attack cavalry, only other chariots; no attacking people in distress), "just means" (no poisoned or barbed arrows), "just cause" (no attacking out of rage), and fair treatment of captives and the wounded.
Versions, translations, and derivative works.
"Critical Edition".
Between 1919 and 1966, scholars at the Bhandarkar Oriental Research Institute, Pune, compared the various manuscripts of the epic from India and abroad and produced the "Critical Edition" of the "Mahabharata", on 13,000 pages in 19 volumes, followed by the "Harivamsha" in another two volumes and six index volumes. This is the text that is usually used in current "Mahabharata" studies for reference. This work is sometimes called the "Pune" or "Poona" edition of the "Mahabharata".
Regional versions.
Many regional versions of the work developed over time, mostly differing only in minor details, or with verses or subsidiary stories being added. These include the Tamil street theatre, terukkuttu and kattaikkuttu, the plays of which use themes from the Tamil language versions of "Mahabharata", focusing on Draupadi.
Outside the Indian subcontinent, in Indonesia, a version was developed in ancient Java as Kakawin Bhāratayuddha in the 11th century under the patronage of King Dharmawangsa (990–1016) and later it spread to the neighboring island of Bali, which remains a Hindu majority island today. It has become the fertile source for Javanese literature, dance drama (wayang wong), and wayang shadow puppet performances. This Javanese version of the Mahabharata differs slightly from the original Indian version. For example, Draupadi is only wed to Yudhishthira, not to all the Pandava brothers; this might demonstrate ancient Javanese opposition to polyandry. The author later added some female characters to be wed to the Pandavas, for example, Arjuna is described as having many wives and consorts next to Subhadra. Another difference is that Shikhandini does not change her sex and remains a woman, to be wed to Arjuna, and takes the role of a warrior princess during the war. Another twist is that Gandhari is described as antagonistic character who hates the Pandavas: her hate is out of jealousy because during Gandhari's swayamvara, she was in love with Pandu but was later wed to his blind elder brother instead, whom she did not love, so she blindfolded herself as protest. Another notable difference is the inclusion of the Punakawans, the clown servants of the main characters in the storyline. These characters include Semar, Petruk, Gareng and Bagong, who are much-loved by Indonesian audiences. There are also some spin-off episodes developed in ancient Java, such as Arjunawiwaha composed in 11th century.
A Kawi version of the "Mahabharata", of which eight of the eighteen "parvas" survive, is found on the Indonesian island of Bali. It has been translated into English by Dr. I. Gusti Putu Phalgunadi.
Translations.
A Persian translation of "Mahabharta", titled "Razmnameh", was produced at Akbar's orders, by Faizi and `Abd al-Qadir Bada'uni in the 18th century.
The first complete English translation was the Victorian prose version by Kisari Mohan Ganguli, published between 1883 and 1896 (Munshiram Manoharlal Publishers) and by M. N. Dutt (Motilal Banarsidass Publishers). Most critics consider the translation by Ganguli to be faithful to the original text. The complete text of Ganguli's translation is in the public domain and is available online.
Another English prose translation of the full epic, based on the "Critical Edition", is in progress, published by University Of Chicago Press. It was initiated by Indologist J. A. B. van Buitenen (books 1–5) and, following a 20-year hiatus caused by the death of van Buitenen, is being continued by D. Gitomer of DePaul University (book 6), J. L. Fitzgerald of Brown University (books 11–13) and Wendy Doniger of the University of Chicago (books 14–18).
An early poetry translation by Romesh Chunder Dutt and published in 1898 condenses the main themes of the "Mahabharata" into English verse. A later poetic "transcreation" (author's own description) of the full epic into English, done by the poet P. Lal, is complete, and in 2005 began being published by Writers Workshop, Calcutta. The P. Lal translation is a non-rhyming verse-by-verse rendering, and is the only edition in any language to include all slokas in all recensions of the work (not just those in the "Critical Edition"). The completion of the publishing project is scheduled for 2010. Sixteen of the eighteen volumes are now available.
A project to translate the full epic into English prose, translated by various hands, began to appear in 2005 from the Clay Sanskrit Library, published by New York University Press. The translation is based not on the "Critical Edition" but on the version known to the commentator Nīlakaṇṭha. Currently available are 15 volumes of the projected 32-volume edition.
Indian economist Bibek Debroy has also begun an unabridged English translation in ten volumes. Volume 1: Adi Parva was published in March 2010.
Many condensed versions, abridgements and novelistic prose retellings of the complete epic have been published in English, including works by Ramesh Menon, William Buck, R. K. Narayan, C. Rajagopalachari, K. M. Munshi, Krishna Dharma, Romesh C. Dutt, Bharadvaja Sarma, John D. Smith and Sharon Maas.
Derivative literature.
Bhasa, the 2nd- or 3rd-century CE Sanskrit playwright, wrote two plays on episodes in the Marabharata, "Urubhanga" (Broken Thigh), about the fight between Duryodhana and Bhima, while "Madhyamavyayoga" (The Middle One) set around Bhima and his son, Ghatotkacha. The first important play of 20th century was "Andha Yug" ("The Blind Epoch"), by Dharamvir Bharati, which came in 1955, found in "Mahabharat", both an ideal source and expression of modern predicaments and discontent. Starting with Ebrahim Alkazi it was staged by numerous directors. V. S. Khandekar's Marathi novel, "Yayati" (1960) and Girish Karnad's debut play "Yayati" (1961) are based on the story of King Yayati found in the "Mahabharat". Bengali writer and playwright, Buddhadeva Bose wrote three plays set in Mahabharat, "Anamni Angana", "Pratham Partha" and "Kalsandhya".
Chitra Banerjee Divakaruni wrote a version from the perspective of Draupadi entitled "", which was published in 2008.
Amar Chitra Katha published a 1,260 page comic book version of the "Mahabharata".
In film and television.
In Indian cinema, several film versions of the epic have been made, dating back to 1920. In Telugu film Daana Veera Soora Karna (1977) directed by and starring N. T. Rama Rao depicts Karna as the lead character. The "Mahabharata" was also reinterpreted by Shyam Benegal in Kalyug. Prakash Jha directed 2010 film Raajneeti was partially inspired by the "Mahabharata". A 2013 animated adaptation holds the record for India's most expensive animated film.
In the late 1980s, the "Mahabharat" TV series, directed by Ravi Chopra, was televised on India's national television (Doordarshan). In the Western world, a well-known presentation of the epic is Peter Brook's nine-hour play, which premiered in Avignon in 1985, and its five-hour movie version "The Mahabharata" (1989).
Uncompleted projects on the "Mahabharata" include a ones by Rajkumar Santoshi, and a theaterical adaptation planned by Satyajit Ray.
Jain version.
Jain versions of "Mahabharata" can be found in the various Jain texts like "Harivamsapurana" (the story of Harivamsa) "Trisastisalakapurusa Caritra" (Hagiography of 63 Illustrious persons), "Pandavacaritra" (lives of Pandavas) and "Pandavapurana" (stories of Pandavas). From the earlier canonical literature, "Antakrddaaśāh" (8th cannon) and "Vrisnidasa" ("upangagama" or secondary canon) contain the stories of Neminatha (22nd Tirthankara), Krishna and Balarama. Prof. Padmanabh Jaini notes that, unlike in the Hindu Puranas, the names Baladeva and Vasudeva are not restricted to Balarama and Krishna in Jain puranas. Instead they serve as names of two distinct class of mighty brothers, who appear nine times in each half of time cycles of the Jain cosmology and rule the half the earth as half-chakravartins. Jaini traces the origin of this list of brothers to the Jinacharitra by Bhadrabahu swami (4th–3rd century BCE). According to Jain cosmology Balarama, Krishna and Jarasandha are the ninth and the last set of Baladeva, Vasudeva, and Partivasudeva. The main battle is not the Mahabharata, but the fight between Krishna and Jarasandha (who is killed by Krishna). Ultimately, the Pandavas and Balarama take renunciation as Jain monks and are reborn in heavens, while on the other hand Krishna and Jarasandha are reborn in hell. In keeping with the law of karma, Krishna is reborn in hell for his exploits (sexual and violent) while Jarasandha for his evil ways. Prof. Jaini admits a possibility that perhaps because of his popularity, the Jain authors were keen to rehabilitate Krishna. The Jain texts predict that after his karmic term in hell is over sometime during the next half time-cycle, Krishna will be reborn as a Jain Tirthankara and attain liberation. Krishna and Balrama are shown as contemporaries and cousins of 22nd Tirthankara, Neminatha. According to this story, Krishna arranged young Neminath’s marriage with Rajamati, the daughter of Ugrasena, but Neminatha, empathizing with the animals which were to be slaughtered for the marriage feast, left the procession suddenly and renounced the world.
Kuru family tree.
This shows the line of royal and family succession, not necessarily the parentage. See the notes below for detail.
Key to Symbols
Notes
The birth order of siblings is correctly shown in the family tree (from left to right), except for Vyasa and Bhishma whose birth order is not described, and Vichitravirya and Chitrangada who were born after them. The fact that Ambika and Ambalika are sisters is not shown in the family tree. The birth of Duryodhana took place after the birth of Karna, Yudhishthira and Bhima, but before the birth of the remaining Pandava brothers.
Some siblings of the characters shown here have been left out for clarity; these include Chitrāngada, the eldest brother of Vichitravirya. Vidura, half-brother to Dhritarashtra and Pandu.
Cultural influence.
In the "Bhagavad Gita", Krishna explains to Arjuna his duties as a warrior and prince and elaborates on different Yogic and Vedantic philosophies, with examples and analogies. This has led to the Gita often being described as a concise guide to Hindu philosophy and a practical, self-contained guide to life. In more modern times, Swami Vivekananda, Bal Gangadhar Tilak, Mahatma Gandhi and many others used the text to help inspire the Indian independence movement.
Use in German anti-Semitism.
According to Vishwa and Joydeep, German indologists arbitrarily identified "layers" in the Mahabharata and Bhagavad Gita with the objective of fueling European anti-Semitism via the Indo-Aryan migration theory. This required equating Brahmins with Jews, resulting in anti-Brahminism.

</doc>
<doc id="19644" url="https://en.wikipedia.org/wiki?curid=19644" title="Mein Kampf">
Mein Kampf

Mein Kampf (, "My Struggle") is an autobiography by the National Socialist leader Adolf Hitler, in which he outlines his political ideology and future plans for Germany. Volume 1 of "Mein Kampf" was published in 1925 and Volume 2 in 1926. The book was edited by Hitler's deputy Rudolf Hess.
Hitler began dictating the book to Hess while imprisoned for what he considered to be "political crimes" following his failed Putsch in Munich in November 1923. Although Hitler received many visitors initially, he soon devoted himself entirely to the book. As he continued, Hitler realized that it would have to be a two-volume work, with the first volume scheduled for release in early 1925. The governor of Landsberg noted at the time that "he hopes the book will run into many editions, thus enabling him to fulfill his financial obligations and to defray the expenses incurred at the time of his trial." In 2016, following the expiry of the copyright held by the Bavarian state government, "Mein Kampf" was republished in Germany for the first time since 1945.
Title.
Hitler originally wanted to call his forthcoming book "Viereinhalb Jahre (des Kampfes) gegen Lüge, Dummheit und Feigheit", or "Four and a Half Years (of Struggle) Against Lies, Stupidity and Cowardice". Max Amann, head of the Franz Eher Verlag and Hitler's publisher, is said to have suggested the much shorter ""Mein Kampf"" or ""My Struggle"".
Contents.
The arrangement of chapters is as follows: 
Analysis.
In "Mein Kampf", Hitler used the main thesis of "the Jewish peril", which posits a Jewish conspiracy to gain world leadership. The narrative describes the process by which he became increasingly antisemitic and militaristic, especially during his years in Vienna. He speaks of not having met a Jew until he arrived in Vienna, and that at first his attitude was liberal and tolerant. When he first encountered the anti-semitic press, he says, he dismissed it as unworthy of serious consideration. Later he accepted the same anti-semitic views, which became crucial in his program of national reconstruction of Germany.
"Mein Kampf" has also been studied as a work on political theory. For example, Hitler announces his hatred of what he believed to be the world's two evils: Communism and Judaism.
During his work, Hitler blamed Germany's chief woes on the parliament of the Weimar Republic, the Jews, and Social Democrats, as well as Marxists, though he believed that Marxists, Social Democrats, and the parliament were all working for Jewish interests. He announced that he wanted to completely destroy the parliamentary system, believing it to be corrupt in principle, as those who reach power are inherent opportunists.
Antisemitism.
While historians diverge on the exact date Hitler decided to forcibly emigrate the Jewish people to Madagascar, few place the decision before the mid-1930s. First published in 1925, "Mein Kampf" shows the ideas that crafted Hitler's personal grievances and ambitions for creating a New Order.
Historian Ian Kershaw points out that several passages in "Mein Kampf" are undeniably of a genocidal nature. Hitler wrote "the nationalization of our masses will succeed only when, aside from all the positive struggle for the soul of our people, their international poisoners are exterminated" and in another passage he suggested that "If at the beginning of the war and during the war twelve or fifteen thousand of these Hebrew corrupters of the nation had been subjected to poison gas, such as had to be endured in the field by hundreds of thousands of our very best German workers of all classes and professions, then the sacrifice of millions at the front would not have been in vain."
The racial laws to which Hitler referred resonate directly with his ideas in "Mein Kampf". In his first edition of "Mein Kampf", Hitler stated that the destruction of the weak and sick is far more humane than their protection. Apart from his allusion to humane treatment, Hitler saw a purpose in destroying "the weak" in order to provide the proper space and purity for the "strong".
Lebensraum.
In the chapter "Eastern Orientation or Eastern Policy", Hitler argued that the Germans needed Lebensraum in the East, a "historic destiny" that would properly nurture the German people. Hitler believed that "the organization of a Russian state formation was not the result of the political abilities of the Slavs in Russia, but only a wonderful example of the state-forming efficacity of the German element in an inferior race."
In "Mein Kampf" Hitler openly stated the future German expansion in the East:
Hitler's later invasions of Czechoslovakia and Poland, and his attack against the Soviet Union directly resonated from his desire for Lebensraum as spelled out in "Mein Kampf".
Popularity.
Although Hitler originally wrote this book mostly for the followers of National Socialism, it grew in popularity. He accumulated a tax debt of 405,500 Reichsmark (very roughly in 2015 GBP£1.1 million, €1.4 million, US$1.5 million) from the sale of about 240,000 copies before he became chancellor in 1933 (at which time his debt was waived).
After Hitler rose to power, the book gained a large amount of popularity. (Two other books written by party members, Gottfried Feder's "Breaking The Interest Slavery" and Alfred Rosenberg's "The Myth of the Twentieth Century," have since lapsed into comparative literary obscurity, and no translation of Feder's book from the original German is known.) The book was in high demand in libraries and often reviewed and quoted in other publications. Hitler had made about 1.2 million Reichsmarks from the income of his book in 1933, when the average annual income of a teacher was about 4,800 Marks. During Hitler's years in power, the book was given free to every newlywed couple and every soldier fighting at the front. By 1939 the book had sold 5.2 million copies in eleven languages. By the end of the war, about 10 million copies of the book had been sold or distributed in Germany.
After becoming chancellor of Germany in 1933, Hitler began to distance himself from the book and dismissed it as "fantasies behind bars" that were little more than a series of articles for the "Völkischer Beobachter" and later told Hans Frank that "If I had had any idea in 1924 that I would have become Reich chancellor, I never would have written the book."
There are currently six e-book versions of "Mein Kampf" available for sale. In 2014, two of these versions reached the 12th and 15th spots on the iTunes Politics and Current Events section. The same year a digital version of the book reached number one on the Amazon Propaganda and Political Psychology chart.
Contemporary observations.
"Mein Kampf", in essence, lays out the ideological program Hitler established for the German revolution, by identifying the Jews and "Bolsheviks" as racially and ideologically inferior and threatening, and "Aryans" and National Socialists as racially superior and politically progressive. Hitler's revolutionary goals included expulsion of the Jews from Greater Germany and the unification of German peoples into one Greater Germany. Hitler desired to restore German lands to their greatest historical extent, real or imagined.
Due to its racist content and the historical effect of Nazism upon Europe during World War II and the Holocaust, it is considered a highly controversial book. Criticism has not come solely from opponents of Nazism. Italian Fascist dictator and Nazi ally Benito Mussolini was also critical of the book, saying that it was "a boring tome that I have never been able to read" and remarked that Hitler's beliefs, as expressed in the book, were "little more than commonplace clichés".
The German journalist Konrad Heiden, an early critic of the Nazi Party, observed that the content of "Mein Kampf" is essentially a political argument with other members of the Nazi Party who had appeared to be Hitler's friends, but whom he was actually denouncing in the book's content – sometimes by not even including references to them.
The American literary theorist and philosopher Kenneth Burke wrote a 1939 rhetorical analysis of the work, "The Rhetoric of Hitler's "Battle"", which revealed its underlying message of aggressive intent.
In March 1940, British writer George Orwell reviewed a then-recently published uncensored translation of "Mein Kampf" for the "New English Weekly". Orwell suggested that the force of Hitler's personality shone through the often "clumsy" writing, capturing the magnetic allure of Hitler for many Germans. In essence, Orwell notes, Hitler offers only visions of endless struggle and conflict in the creation of "a horrible brainless empire" that "stretch to Afghanistan or thereabouts". He wrote, "Whereas Socialism, and even capitalism in a more grudging way, have said to people 'I offer you a good time,' Hitler has said to them, 'I offer you struggle, danger, and death,' and as a result a whole nation flings itself at his feet." Orwell's review was written in the aftermath of the 1939 Molotov–Ribbentrop Pact, when Hitler made peace with Russia after more than a decade of vitriolic rhetoric and threats between the two nations; with the pact in place, Orwell believed, England was now facing a risk of Nazi attack and the UK must not underestimate the appeal of Hitler's ideas.
In his 1943 book "The Menace of the Herd", Austrian scholar Erik von Kuehnelt-Leddihn described Hitler's ideas in "Mein Kampf" and elsewhere as "a veritable "reductio ad absurdum" of 'progressive' thought" and betraying "a curious lack of original thought" that shows Hitler offered no innovative or original ideas but was merely "a "virtuoso" of commonplaces which he may or may not repeat in the guise of a 'new discovery.'" Hitler's stated aim, Kuehnelt-Leddihn writes, is to quash individualism in furtherance of political goals:
In his "The Second World War", published in several volumes in the late 1940s and early 1950s, Winston Churchill wrote that he felt that after Hitler's ascension to power, no other book than "Mein Kampf" deserved more intensive scrutiny.
German publication history.
While Hitler was in power (1933–1945), "Mein Kampf" came to be available in three common editions. The first, the "Volksausgabe" or People's Edition, featured the original cover on the dust jacket and was navy blue underneath with a gold swastika eagle embossed on the cover. The "Hochzeitsausgabe", or Wedding Edition, in a slipcase with the seal of the province embossed in gold onto a parchment-like cover was given free to marrying couples. In 1940, the "Tornister-Ausgabe" was released. This edition was a compact, but unabridged, version in a red cover and was released by the post office, available to be sent to loved ones fighting at the front. These three editions combined both volumes into the same book.
A special edition was published in 1939 in honour of Hitler's 50th birthday. This edition was known as the "Jubiläumsausgabe", or Anniversary Issue. It came in both dark blue and bright red boards with a gold sword on the cover. This work contained both volumes one and two. It was considered a deluxe version, relative to the smaller and more common "Volksausgabe".
The book could also be purchased as a two-volume set during Hitler's rule, and was available in soft cover and hardcover. The soft cover edition contained the original cover (as pictured at the top of this article). The hardcover edition had a leather spine with cloth-covered boards. The cover and spine contained an image of three brown oak leaves.
English translations.
Dugdale abridgement.
The first English translation was an abridgement by Edgar Dugdale who started work on it in 1931, at the prompting of his wife, Blanche. When he learned that the London publishing firm of Hurst & Blackett had secured the rights to publish an abridgement in the United Kingdom, he offered it for free in April 1933. However, a local Nazi Party representative insisted that the translation be further abridged before publication, so it was held back until 13 October 1933, although excerpts were allowed to run in "The Times" in late July. It was published by Hurst & Blackett as part of "The Paternoster Library".
In America, Houghton Mifflin secured the rights to the Dugdale abridgement on 29 July 1933. The only differences between the American and British versions are that the title was translated "My Struggle" in the UK and "My Battle" in America; and that Dugdale is credited as translator in the US edition, while the British version withheld his name. Both Dugdale and his wife were active in the Zionist movement; Blanche was the niece of Lord Balfour, and they wished to avoid publicity.
Reynal and Hitchcock translation.
Houghton and Mifflin licensed Reynal & Hitchcock the rights to publish a full unexpurgated translation in 1938. The book was translated from the two volumes of the first German edition (1925 and 1927), with notations appended noting any changes made in later editions, which were deemed "not as extensive as popularly supposed." The translation, made by a committee from the New School for Social Research headed by Dr. Alvin Johnson, was said to have been made with a view to readability rather than in an effort to rigidly reproduce Hitler's sometimes idiosyncratic German form.
The text was heavily annotated for an American audience with biographical and historical details derived largely from German sources. As the translators deemed the book "a propagandistic essay of a violent partisan", which "often warps historical truth and sometimes ignores it completely," the tone of many of these annotations reflected a conscious attempt to provide "factual information that constitutes an extensive critique of the original." The book appeared for sale on 28 February 1939.
Murphy translation.
One of the earlier complete English translations of "Mein Kampf" was by James Murphy in 1939. It was the only English translation approved by Nazi Germany. The version published by Hutchison & Co. in association with Hurst & Blackett, Ltd (London) in 1939 of the combined volumes I and II is profusely illustrated with many full page drawings and photographs. The opening line, "It has turned out fortunate for me to-day that destiny appointed Braunau-on-the-Inn to be my birthplace," is characteristic of Hitler's sense of destiny that began to develop in the early 1920s. Hurst & Blackett ceased publishing the Murphy translation in 1942 when the original plates were destroyed by German bombing, but it is still published and available in facsimile editions and also on the Internet. An audio reading of volume one is also available online.
Stackpole translation and controversy.
The small Pennsylvania firm of Stackpole and Sons released its own unexpurgated translation by William Soskin on the same day as Houghton Mifflin, amid much legal wrangling. The Second Circuit Court of Appeals ruled in Houghton Mifflin's favour that June and ordered Stackpole to stop selling their version, but litigation followed for a few more years until the case was finally resolved in September 1941.
Among other things, Stackpole argued that Hitler could not have legally transferred his right to a copyright in the United States to Eher Verlag in 1925, because he was not a citizen of any country. "Houghton Mifflin v. Stackpole" was a minor landmark in American copyright law, definitively establishing that stateless persons have the same copyright status in the United States that any other foreigner would. In the three months that Stackpole's version was available it sold 12,000 copies.
Cranston translation and controversy.
Houghton Mifflin's abridged English translation left out some of Hitler's more anti-Semitic and militaristic statements. This motivated Alan Cranston, an American reporter for United Press International in Germany (and later a U.S. Senator from California), to publish his own abridged and annotated translation. Cranston believed this version more accurately reflected the contents of the book and Hitler's intentions. In 1939, Cranston was sued by Hitler's publisher for copyright infringement, and a Connecticut judge ruled in Hitler's favour. By the time the publication of Cranston's version was stopped, 500,000 copies had already been sold. Today, the profits and proceeds are given to various charities.
Manheim translation.
Houghton Mifflin published a translation by Ralph Manheim in 1943. They did this to avoid having to share their profits with Reynal & Hitchcock, and to increase sales by offering a more readable translation. The Manheim translation was first published in the United Kingdom by Hurst & Blackett in 1969 amid some controversy.
Excerpts.
In addition to the above translations and abridgments, the following collections of excerpts were available in English before the start of the war:
Official Nazi translation.
A previously unknown English translation was released in 2008, which was prepared by the official Nazi printing office, the Franz Eher Verlag. In 1939, the Nazi propaganda ministry hired James Murphy to create an English version of "Mein Kampf", which they hoped to use to promote Nazi goals in English-speaking countries. While Murphy was in Germany, he became less enchanted with Nazi ideology and made some statements that the Propaganda Ministry disliked. As a result, they asked him to leave Germany immediately. He was not able to take any of his notes but later sent his wife back to obtain his partial translation. These notes were later used to create the Murphy translation.
The Nazi government did not abandon their English translation efforts. They used their own staff to create a new and official translation and it was published in very small numbers in Germany. At least one copy found its way to a British/American POW camp. It is the only official English translation produced by the Nazi government and printed on Nazi printing presses. This copy is known as the "Stalag" edition.
Sales and royalties.
Sales of Dugdale abridgment in the United Kingdom.
Sales of the Houghton Mifflin Dugdale translation in the United States.
The first printing of the U.S. Dugdale edition, the October 1933 with 7,603 copies, of which 290 were given away as complimentary gifts.
The royalty on the first printing in the U.S. was 15% or $3,206.45 total. Curtis Brown, literary agent, took 20%, or $641.20 total, and the IRS took $384.75, leaving Eher Verlag $2,180.37 or RM 5668.
The January 1937 second printing was c. 4,000 copies.
There were three separate printings from August 1938 to March 1939, totaling 14,000; sales totals by 31 March 1939 were 10,345.
The Murphy and Houghton Mifflin translations were the only ones published by the authorised publishers while Hitler was still alive, and not at war with the U.K. and the U.S.
There was some resistance from Eher Verlag to Hurst and Blackett's Murphy translation, as they had not been granted the rights to a full translation. However, they allowed it "de facto" permission by not lodging a formal protest, and on 5 May 1939, even inquired about royalties. The British publishers responded on the 12th that the information they requested was "not yet available" and the point would be moot within a few months, on 3 September 1939, when all royalties were halted due to the state of war existing between Britain and Germany.
Royalties were likewise held up in the United States due to the litigation between Houghton Mifflin and Stackpole. Because the matter was only settled in September 1941, only a few months before a state of war existed between Germany and the U.S., all Eher Verlag ever got was a $2,500 advance from Reynal and Hitchcock. It got none from the unauthorised Stackpole edition or the 1943 Manheim edition.
Current availability.
At the time of his suicide, Hitler's official place of residence was in Munich, which led to his entire estate, including all rights to "Mein Kampf", changing to the ownership of the state of Bavaria. The government of Bavaria, in agreement with the federal government of Germany, refused to allow any copying or printing of the book in Germany, and opposed it also in other countries but with less success. As per German copyright law, the entire text entered the public domain on 1 January 2016, 70 years after the author's death.
Owning and buying the book in Germany is not an offence. Trading in old copies is lawful as well, unless it is done in such a fashion as to "promote hatred or war." In particular, the unmodified edition is not covered by §86 StGB that forbids dissemination of means of propaganda of unconstitutional organisations, since it is a "pre-constitutional work" and as such cannot be opposed to the free and democratic basic order, according to a 1979 decision of the Federal Court of Justice of Germany. Most German libraries carry heavily commented and excerpted versions of "Mein Kampf." In 2008, Stephan Kramer, secretary-general of the Central Council of Jews in Germany, not only recommended lifting the ban, but volunteered the help of his organization in editing and annotating the text, saying that it is time for the book to be made available to all online.
A variety of restrictions or special circumstances apply in other countries.
India.
"Mein Kampf" has been a popular book in India since its first publication there in 1928. It has gone through hundreds of editions and sold over a hundred thousand copies.
Russia.
In the Russian Federation, "Mein Kampf" has been published at least three times since 1992; the Russian text is also available on websites. In 2006 the Public Chamber of Russia proposed banning the book. In 2009 St. Petersburg's branch of the Russian Ministry of Internal Affairs requested to remove an annotated and hyper-linked Russian translation of the book from a historiography web site. On 13 April 2010, it was announced that "Mein Kampf" is outlawed on grounds of extremism promotion.
Sweden.
"Mein Kampf" has been reprinted several times since 1945; in 1970, 1992, 2002 and 2010. In 1992 the Government of Bavaria tried to stop the publication of the book, and the case went to the Supreme Court of Sweden which ruled in favour of the publisher, stating that the book is protected by copyright, but that the copyright holder is unidentified (and not the State of Bavaria) and that the original Swedish publisher from 1934 had gone out of business. It therefore refused the Government of Bavaria's claim.
The only translation changes came in the 1970 edition, but they were only linguistic, based on a new Swedish standard.
Turkey.
"Mein Kampf" was widely available and growing in popularity in Turkey, even to the point where it became a bestseller, selling up to 100,000 copies in just two months in 2005. Analysts and commentators believe the popularity of the book to be related to a rise in nationalism, anti-U.S. and antisemitic sentiment "because of what is happening in the Middle East, the Israeli-Palestinian problem and the war in Iraq". Doğu Ergil, a political scientist at Ankara University, said both left-wingers, the far-right and Islamists, had found common ground—"not on a common agenda for the future, but on their anxieties, fears and hate".
United States.
In the United States, "Mein Kampf" can be found at many community libraries and can be bought, sold and traded in bookshops. The U.S. government seized the copyright in September 1942 during the Second World War under the Trading with the Enemy Act and in 1979, Houghton Mifflin, the U.S. publisher of the book, bought the rights from the government pursuant to . More than 15,000 copies are sold a year.
Online availability.
In 1999, the Simon Wiesenthal Center documented that major Internet booksellers such as Amazon.com and Barnesandnoble.com sell "Mein Kampf" to Germany. After a public outcry, both companies agreed to stop those sales to addresses in Germany. The book is currently available through both companies online. It is also available in various languages, including German, at the Internet Archive. Since the January 2016 republication of the book in Germany, the book can be ordered at Amazon's German website.
2016 republication in Germany.
On 3 February 2010, the Institute of Contemporary History (IfZ) in Munich announced plans to republish an annotated version of the text, for educational purposes in schools and universities, in 2015, when the copyright currently held by the Bavarian state government expires (2016). The book had last been published in Germany in 1945. A group of German historians argued that a republication was necessary to get an authoritative annotated edition by the time the copyright runs out, which might open the way for neo-Nazi groups to publish their own versions. "Once Bavaria's copyright expires, there is the danger of charlatans and neo-Nazis appropriating this infamous book for themselves," Wolfgang Heubisch said. The Bavarian government opposed the plan, citing respect for victims of the Holocaust. Its Finance Ministry said that permits for reprints would not be issued, at home or abroad. This would also apply to a new annotated edition. The republished book might be banned as Nazi propaganda. Even after expiration of the copyright, the Bavarian government emphasised that "the dissemination of Nazi ideologies will remain prohibited in Germany and is punishable under the penal code".
On 12 December 2013 the Bavarian government cancelled its financial support for an annotated edition. The Institute of Contemporary History (IfZ) in Munich, which is preparing the translation, announced that it intended to proceed with publication after the copyright expired. The IfZ scheduled an edition of "Mein Kampf" for release in 2016.
Richard Verber, vice-president of the Board of Deputies of British Jews, stated in 2015 that the board trusted the academic and educational value of republishing. “We would, of course, be very wary of any attempt to glorify Hitler or to belittle the Holocaust in any way,” Verber declared to "The Observer". “But this is not that. I do understand how some Jewish groups could be upset and nervous, but it seems it is being done from a historical point of view and to put it in context.”
An annotated edition of "Mein Kampf" was published in Germany in January 2016 and sold out within hours on Amazon's German site. The book's publication led to public debate in Germany, and divided reactions from Jewish groups, with some supporting, and others opposing, the decision to publish. German officials had previously said they would limit public access to the text amid fears that its republication could stir neo-Nazi sentiment. Some bookstores stated that they would not stock the book. Dussmann, a Berlin bookstore, stated that one copy was available on the shelves in the history section, but that it would not be advertised and more copies would be available only on order.
Sequel.
After the party's poor showing in the 1928 elections, Hitler believed that the reason for his loss was the public's misunderstanding of his ideas. He then retired to Munich to dictate a sequel to "Mein Kampf" to expand on its ideas, with more focus on foreign policy.
Only two copies of the 200-page manuscript were originally made, and only one of these was ever made public. The document was neither edited nor published during the Nazi era and remains known as "Zweites Buch", or "Second Book". To keep the document strictly secret, in 1935 Hitler ordered that it be placed in a safe in an air raid shelter. It remained there until being discovered by an American officer in 1945.
The authenticity of the document found in 1945 has been verified by Josef Berg (former employee of the Nazi publishing house Eher Verlag) and Telford Taylor (former Brigadier General U.S.A.R. and Chief Counsel at the Nuremberg war-crimes trials).
In 1958, the "Zweites Buch" was found in the archives of the United States by American historian Gerhard Weinberg. Unable to find an American publisher, Weinberg turned to his mentor – Hans Rothfels at the Institute of Contemporary History in Munich, and his associate Martin Broszat – who published "Zweites Buch" in 1961. A pirated edition was published in English in New York in 1962. The first authoritative English edition was not published until 2003 ("Hitler's Second Book: The Unpublished Sequel to Mein Kampf," ISBN 1-929631-16-2).

</doc>
<doc id="19645" url="https://en.wikipedia.org/wiki?curid=19645" title="Morpheus">
Morpheus

Morpheus may refer to:

</doc>
<doc id="19648" url="https://en.wikipedia.org/wiki?curid=19648" title="May 26">
May 26


</doc>
<doc id="19649" url="https://en.wikipedia.org/wiki?curid=19649" title="MVS">
MVS

Multiple Virtual Storage, more commonly called MVS, was the most commonly used operating system on the System/370 and System/390 IBM mainframe computers. It was developed by IBM, but is unrelated to IBM's other mainframe operating systems, e.g., VSE, VM, TPF.
First released in 1974, MVS was extended by program products with new names multiple times, first to MVS/SE (MVS/System Extension), next to MVS/SP (MVS/System Product) Version 1, next to MVS/XA (MVS/eXtended Architecture), next to MVS/ESA (MVS/Enterprise Systems Architecture), next to OS/390 and finally to z/OS (when 64-bit support was added with the zSeries models). IBM added Unix support (originally called OPEN EDITION) in MVS/SP V4.3 and has obtained POSIX and UNIX™ certifications at several different levels from IEEE, X/Open and The Open Group. The MVS core remains fundamentally the same operating system. By design, programs written for MVS run on z/OS without modification.
At first IBM described MVS as simply a new release of OS/VS2, but it was, in fact a major rewrite. OS/VS2 release 1 was an upgrade of OS/360 MVT that retained most of the original code and, like MVT, was mainly written in assembly language. The MVS core was almost entirely written in Assembler XF, although a few modules were written in PL/S, but not the performance-sensitive ones, in particular not the Input/Output Supervisor (IOS). IBM's use of "OS/VS2" emphasized upwards compatibility: application programs that ran under MVT did not even need recompiling to run under MVS. The same Job Control Language files could be used unchanged; utilities and other non-core facilities like TSO ran unchanged. IBM and users almost unanimously called the new system MVS from the start, and IBM continued to use the term "MVS" in the naming of later "major" versions such as MVS/XA.
Evolution of MVS.
OS/360 MFT (Multitasking with a Fixed number of Tasks) provided multitasking: several memory partitions, each of a fixed size, were set up when the operating system was installed and when the operator redefined them. For example, there could be a small partition, two medium partitions, and a large partition. If there were two large programs ready to run, one would have to wait until the other finished and vacated the large partition.
OS/360 MVT (Multitasking with a Variable number of Tasks) was an enhancement that further refined memory use. Instead of using fixed-size memory partitions, MVT allocated memory to regions for job steps as needed, provided enough "contiguous" physical memory was available. This was a significant advance over MFT's memory management, but had some weaknesses: if a job allocated memory dynamically (as most sort programs and database management systems do), the programmers had to estimate the job's maximum memory requirement and pre-define it for MVT. A job step that contained a mix of small and large programs wasted memory while the small programs ran. Most seriously, memory could become fragmented, i.e., the memory not used by current jobs could be divided into uselessly small chunks between the areas used by current jobs, and the only remedy was to wait some current jobs finished before starting any new ones.
In the early 1970s IBM sought to mitigate these difficulties by introducing virtual memory (which IBM called "virtual storage"), which allowed programs to request address spaces larger than physical memory. The original implementations had a single virtual address space, shared by all jobs. OS/VS1 was OS/360 MFT within a single virtual address space; OS/VS2 SVS was OS/360 MVT within a single virtual address space. So OS/VS1 and SVS in principle had the same disadvantages as MFT and MVT, but the impacts were less severe because jobs could request much larger address spaces and the requests came out of a 16 MB pool even if physical storage was smaller.
In the mid-1970s IBM introduced MVS, which not only supported virtual storage that was larger than the available real storage, as did SVS, but also allowed an indefinite number of applications to run in different address spaces. Two concurrent programs might try to access the same virtual memory address, but the virtual memory system redirected these requests to different areas of physical memory. Each of these address spaces consisted of three areas: an operating system (one instance shared by all jobs), an application area unique for each application, and a shared virtual area used for various purposes, including inter-job communication. IBM promised that application areas would always be at least 8 MB. This made MVS the perfect solution for business problems that resulted from the need to run more applications.
MVS maximized processing potential by providing multiprogramming and multiprocessing capabilities. Like its MVT and OS/VS2 SVS predecessors, MVS supported multiprogramming; program instructions and associated data are scheduled by a control program and given processing cycles. Unlike a single-programming operating system, these systems maximize the use of the processing potential by dividing processing cycles among the instructions associated with several different concurrently running programs. This way, the control program does not have to wait for the I/O operation to complete before proceeding. By executing the instructions for multiple programs, the computer is able to switch back and forth between active and inactive programs.
Early editions of MVS (mid-1970s) were among the first of the IBM OS series to support multiprocessor configurations, though the M65MP variant of OS/360 running on 360 Models 65 and 67 had provided limited multiprocessor support. The 360 Model 67 had also hosted the multiprocessor capable TSS/360, MTS and CP-67 operating systems. Because multiprocessing systems can execute instructions simultaneously, they offer greater processing power than single-processing system. As a result, MVS was able to address the business problems brought on by the need to process large amounts of data.
Multiprocessing systems are either loosely coupled, which means that each computer has access to a common workload, or tightly coupled, which means that the computers share the same real storage and are controlled by a single copy of the operating system. MVS retained both the loosely coupled multiprocessing of Attached Support Processor (ASP) and the tightly coupled multiprocessing of OS/360 Model 65 Multiprocessing. In tightly coupled systems, two CPUs shared concurrent access to the same memory (and copy of the operating system) and peripherals, providing greater processing power and a degree of graceful degradation if one CPU failed. In loosely coupled configurations each of a group of processors (single and / or tightly coupled) had its own memory and operating system but shared peripherals and the operating system component JES3 allowed managing the whole group from one console. This provided greater resilience and let operators decide which processor should run which jobs from a central job queue. MVS JES3 gave users the opportunity to network together two or more data processing systems via shared disks and Channel-to-Channedl Adapters (CTCA's). This capability eventually became available to JES2 users as Multi-Access SPOOL (MAS).
MVS originally supported 24-bit addressing (i.e., up to 16 MB). As the underlying hardware progressed, it supported 31-bit (XA and ESA; up to 2048 MB) and now (as z/OS) 64-bit addressing. The most significant motives for the rapid upgrade to 31-bit addressing were the growth of large transaction-processing networks, mostly controlled by CICS, which ran in a single address space—and the DB2 relational database management system needed more than 8 MB of application address space to run efficiently. (Early versions were configured into two address spaces that communicated via the shared virtual area, but this imposed a significant overhead since all such communications had transmit via the operating system.)
The main user interfaces to MVS are: Job Control Language (JCL), which was originally designed for batch processing but from the 1970s onwards was also used to start and allocate resources to long-running interactive jobs such CICS; and TSO (Time Sharing Option), the interactive time-sharing interface, which was mainly used to run development tools and a few end-user information systems. ISPF is a TSO application for users on 3270-family terminals (and later, on VM as well), which allows the user to accomplish the same tasks as TSO's command line but in a menu and form oriented manner, and with a full screen editor and file browser. TSO's basic interface is command line, although facilities were added later for form-driven interfaces).
MVS took a major step forward in fault-tolerance, built on the earlier STAE facility, that IBM called "software recovery". IBM decided to do this after years of practical real-world experience with MVT in the business world. System failures were now having major impacts on customer businesses, and IBM decided to take a major design jump, to assume that despite the very best software development and testing techniques, that 'problems WILL occur.' This profound assumption was pivotal in adding great percentages of fault-tolerance code to the system and likely contributed to the system's success in tolerating software and hardware failures. Statistical information is hard to come by to prove the value of these design features (how can you measure 'prevented' or 'recovered' problems?), but IBM has, in many dimensions, enhanced these fault-tolerant software recovery and rapid problem resolution features, over time.
This design specified a hierarchy of error-handling programs, in system (kernel/'privileged') mode, called Functional Recovery Routines, and in user ('task' or 'problem program') mode, called "ESTAE" (Extended Specified Task Abnormal Exit routines) that were invoked in case the system detected an error (actually, hardware processor or storage error, or software error). Each recovery routine made the 'mainline' function reinvokable, captured error diagnostic data sufficient to debug the causing problem, and either 'retried' (reinvoke the mainline) or 'percolated' (escalated error processing to the next recovery routine in the hierarchy).
Thus, with each error the system captured diagnostic data, and attempted to perform a repair and keep the system up. The worst thing possible was to take down a user address space (a 'job') in the case of unrepaired errors. Though it was an initial design point, it was not until the most recent MVS version (z/OS), that recovery program was not only guaranteed its own recovery routine, but each recovery routine now has its own recovery routine. This recovery structure was embedded in the basic MVS control program, and programming facilities are available and used by application program developers and 3rd party developers.
Practically, the MVS software recovery made problem debugging both easier and more difficult. Software recovery requires that programs leave 'tracks' of where they are and what they are doing, thus facilitating debugging—but the fact that processing progresses despite an error can overwrite the tracks. Early date capture at the time of the error maximizes debugging, and facilities exist for the recovery routines (task and system mode, both) to do this.
IBM included additional criteria for a major software problem that required IBM service. If a mainline component failed to initiate software recovery, that was considered a valid reportable failure. Also, if a recovery routine failed to collect significant diagnostic data such that the original problem was solvable by data collected by that recovery routine, IBM standards dictated that this fault was reportable and required repair. Thus, IBM standards, when rigorously applied, encouraged continuous improvement.
IBM introduced an on-demand hypervisor, a major serviceability tool, called Dynamic Support System (DSS), in the first release of MVS. This facility could be invoked to initiate a session to create diagnostic procedures, or invoke already-stored procedures. The procedures 'trapped' special events, such as the loading of a program, device I/O, system procedure calls, and then triggered the activation of the previously defined procedures. These procedures, which could be invoked recursively, allowed for reading and writing of data, and alteration of instruction flow. Program Event Recording hardware was used. Due to the overhead of this tool, it was removed from customer-available MVS systems. Program-Event Recording (PER) exploitation was performed by the enhancement of the diagnostic "SLIP" command with the introduction of the PER support (SLIP/Per) in SU 64/65 (1978).
Multiple copies of MVS (or other IBM operating systems) could share the
same machine if that machine was controlled by VM/370. In this case VM/370 was the real operating system, and regarded the "guest" operating systems as applications with unusually high privileges. As a result of later hardware enhancements one instance of an operating system (either MVS, or VM with guests, or other) could also occupy a Logical Partition (LPAR) instead of an entire physical system.
Multiple MVS instances can be organized and collectively administered in a structure called a "systems complex" or "sysplex", introduced in September, 1990. Instances interoperate through a software component called a Cross-system Coupling Facility (XCF) and a hardware component called a "Hardware Coupling Facility" (CF or Integrated Coupling Facility, ICF, if co-located on the same mainframe hardware). Multiple sysplexes can be joined via standard network protocols such as IBM's proprietary Systems Network Architecture (SNA) or, more recently, via TCP/IP. The z/OS operating system (MVS' most recent descendant) also has native support to execute POSIX and Single UNIX Specification applications. The support began with MVS/SP V4R3, and IBM has obtained UNIX 95 certification for z/OS V1R2 and later.
The system is typically used in business and banking, and applications are often written in COBOL. COBOL programs were traditionally used with transaction processing systems like IMS and CICS. For a program running in CICS, special EXEC CICS statements are inserted in the COBOL source code. A preprocessor (translator) replaces those EXEC CICS statements with the appropriate COBOL code to call CICS before the program is compiled — not altogether unlike SQL used to call DB2. Applications can also be written in other languages such as C, C++, Java, assembly language, FORTRAN, BASIC, RPG, and REXX. Language support is packaged as a common component called "Language Environment" or "LE" to allow uniform debugging, tracing, profiling, and other language independent functions.
MVS systems are traditionally accessed by 3270 terminals or by PCs running 3270 emulators. However, many mainframe applications these days have custom web or GUI interfaces. The z/OS operating system has built-in support for TCP/IP. System management, done in the past with a 3270 terminal, is now done through the Hardware Management Console (HMC) and, increasingly, Web interfaces. Operator consoles are provided through 2074 emulators, so you are unlikely to see any S/390 or zSeries processor with a real 3270 connected to it.
The native character encoding scheme of MVS and its peripherals is EBCDIC, but the TR instruction made it easy to translate to other 7- and 8-bit codes. Over time IBM added hardware-accelerated services to perform translation to and between larger codes, hardware-specific service for Unicode transforms and software support of, e.g., ASCII, ISO/IEC 8859, UTF-8, UTF-16, and UTF-32. The software translation services take source and destination code pages as inputs.
MVS filesystem.
Files are properly called data sets in MVS. Names of those files are organized in "catalogs" that are VSAM files themselves.
Data set names (DSNs, mainframe term for filenames) are organized in a hierarchy whose levels are separated with dots, e.g. "DEPT01.SYSTEM01.FILE01". Each level in the hierarchy can be up to eight characters long. The total filename length is a maximum of 44 characters including dots. By convention, the components separated by the dots are used to organize files similarly to directories in other operating systems. For example, there were utility programs that performed similar functions to those of Windows Explorer (but without the GUI and usually in batch processing mode) - adding, renaming or deleting new elements and reporting all the contents of a specified element. However, unlike in many other systems, these levels are not usually actual directories but just a naming convention (like the original Macintosh File System, where folder hierarchy was an illusion maintained by the Finder). TSO supports a default prefix for files (similar to a "current directory" concept), and RACF supports setting up access controls based on filename patterns, analogous to access controls on directories on other platforms.
As with other members of the OS family, MVS' data sets were record-oriented. MVS inherited three main types from its predecessors:
Sequential and ISAM datasets could store either fixed-length or variable length records, and all types could occupy more than one disk volume.
All of these are based on the VTOC disk structure.
Early IBM database management systems used various combinations of ISAM and BDAM datasets - usually BDAM for the actual data storage and ISAM for indexes.
In the early 1970s IBM's virtual memory operating systems introduced a new file management component, VSAM, which provided similar facilities:
These VSAM formats became the basis of IBM's database management systems, IMS/VS and DB2 - usually ESDS for the actual data storage and KSDS for indexes.
VSAM also included a catalog component used for MVS' master catalog.
Partitioned datasets (PDS) were sequential datasets subdivided into "members" that could be processed as sequential files in their own right. The most important use of PDS was for program libraries - system administrators used the main PDS as a way to allocate disk space to a project and the project team then created and edited the members.
Generation Data Groups (GDGs) were originally designed to support grandfather-father-son backup procedures - if a file was modified, the changed version became the new "son", the previous "son" became the "father", the previous "father" became the "grandfather" and the previous "grandfather" was deleted. But one could set up GDGs with a lot more than 3 generations and some applications used GDGs to collect data from several sources and feed the information to one program - each collecting program created a new generation of the file and the final program read the whole group as a single sequential file (by not specifying a generation in the JCL).
Modern versions of MVS (e.g., z/OS) also support POSIX-compatible "slash" filesystems along with facilities for integrating the two filesystems. That is, the OS can make an MVS dataset appear as a file to a POSIX program or subsystem. These newer filesystems include Hierarchical File System (HFS) (not to be confused with Apple's Hierarchical File System) and zFS (not to be confused with Sun's ZFS).
Programs running on network-connected computers (such as the AS/400) can use local data management interfaces to transparently create, manage, and access VSAM record-oriented files by using client-server products implemented according to Distributed Data Management Architecture (DDM). DDM is also the base architecture for the MVS DB2 server that implements Distributed Relational Database Architecture (DRDA).
History and modernity.
MVS is now a part of z/OS, older MVS releases are no longer supported by IBM and since 2007 only 64-bit z/OS releases are supported. z/OS supports running older 24-bit and 31-bit MVS applications alongside 64-bit applications.
MVS releases up to 3.8j (24-bit, released in 1981) were freely available and it is now possible to run the MVS 3.8j release in mainframe emulators for free.
MVS/370.
MVS/370 is a generic term for all versions of the MVS operating system prior to MVS/XA. The System/370 architecture, at the time MVS was released, supported only 24-bit virtual addresses, so the MVS/370 operating system architecture is based on a 24-bit address. Because of this 24-bit address length, programs running under MVS/370 are each given 16 megabytes of contiguous virtual storage.
MVS/XA.
MVS/XA, or Multiple Virtual Storage/Extended Architecture, was a version of MVS that supported the 370-XA architecture, which expanded addresses from 24 bits to 31 bits, providing a 2 gigabyte addressable memory area. It also supported a 24-bit legacy addressing mode for older 24-bit applications (i.e. those that stored a 24-bit address in the lower 24 bits of a 32-bit word and utilized the upper 8 bits of that word for other purposes).
MVS/ESA.
MVS/ESA: MVS Enterprise System Architecture. Version of MVS, first introduced as MVS/SP Version 3 in February 1988. Replaced by/renamed as OS/390 late 1995 and subsequently as z/OS.
MVS/ESA OpenEdition: upgrade to Version 4 Release 3 of MVS/ESA announced February 1993 with support for POSIX and other standards. While the initial release only had National Institute of Standards and Technology (NIST) certification for Federal Information Processing Standard (FIPS) 151 compliance, subsequent releases were certified at higher levels and by other organizations, e.g. X/Open and its successor, The Open Group. It included about 1 million new lines of code, which provide an API shell, utilities, and an extended user interface. Works with a hierarchical file system provided by DFSMS (Data Facility System Managed Storage). The shell and utilities are based on Mortice Kerns' InterOpen products. Independent specialists reckon it was over 80% open systems-compliant—more than most Unix systems. DCE2 support announced February 1994, and many application development tools in March 1995. Mid 1995 IBM started to stop referring to OpenEdition as a separate entity, as all the open features became a standard part of vanilla MVS/ESA SP Version 5 Release 1. Under OS/390, it became UNIX System Services, and has kept that name under z/OS.
Closely related operating systems.
Japanese mainframe manufacturers Fujitsu and Hitachi both repeatedly and illegally obtained IBM's MVS source code and internal documentation in one of the 20th century's most famous cases of industrial espionage. Fujitsu relied heavily on IBM's code in its MSP mainframe operating system, and likewise Hitachi did the same for its VOS3 operating system. MSP and VOS3 were heavily marketed in Japan, where they still hold a substantial share of the mainframe installed base, but also to some degree in other countries, notably Australia. Even IBM's bugs and documentation misspellings were faithfully copied. IBM cooperated with the U.S. Federal Bureau of Investigation in a sting operation, reluctantly supplying Fujitsu and Hitachi with proprietary MVS and mainframe hardware technologies during the course of multi-year investigations culminating in the early 1980s—investigations which implicated senior company managers and even some Japanese government officials. Amdahl, however, was not involved in Fujitsu's theft of IBM's intellectual property. Any communications from Amdahl to Fujitsu were through "Amdahl Only Specifications" which were scrupulously cleansed of any IBM IP or any references to IBM's IP.
Subsequent to the investigations, IBM reached multimillion-dollar settlements with both Fujitsu and Hitachi, collecting substantial fractions of both companies' profits for many years. Reliable reports indicate that the settlements exceeded US$500,000,000. The three companies have long since amicably agreed to many joint business ventures. For example, in 2002 IBM and Hitachi collaborated on developing the IBM z800 mainframe model.
Because of this historical copying, MSP and VOS3 are properly classified as "forks" of MVS, and many third party software vendors with MVS-compatible products were able to produce MSP- and VOS3-compatible versions with little or no modification.
When IBM introduced its 64-bit z/Architecture mainframes in the year 2000, IBM also introduced the 64-bit z/OS operating system, the direct successor to OS/390 and MVS. Fujitsu and Hitachi opted not to license IBM's z/Architecture for their quasi-MVS operating systems and hardware systems, and so MSP and VOS3, while still nominally supported by their vendors, maintain most of MVS's 1980s architectural limitations to the present day. Since z/OS still supports MVS-era applications and technologies—indeed, z/OS still contains most of MVS's code, albeit greatly enhanced and improved over decades of evolution—applications (and operational procedures) running on MSP and VOS3 can move to z/OS much more easily than to other operating systems.

</doc>
<doc id="19652" url="https://en.wikipedia.org/wiki?curid=19652" title="Monoid">
Monoid

In abstract algebra, a branch of mathematics, a monoid is an algebraic structure with a single associative binary operation and an identity element. Monoids are studied in semigroup theory as they are semigroups with identity. Monoids occur in several branches of mathematics; for instance, they can be regarded as categories with a single object. Thus, they capture the idea of function composition within a set. Monoids are also commonly used in computer science, both in its foundational aspects and in practical programming. The set of strings built from a given set of characters is a free monoid. The transition monoid and syntactic monoid are used in describing finite state machines, whereas trace monoids and history monoids provide a foundation for process calculi and concurrent computing. Some of the more important results in the study of monoids are the Krohn–Rhodes theorem and the star height problem. The history of monoids, as well as a discussion of additional general properties, are found in the article on semigroups.
Definition.
Suppose that "S" is a set and • is some binary operation , then "S" with • is a monoid if it satisfies the following two axioms:
In other words, a monoid is a semigroup with an identity element. It can also be thought of as a magma with associativity and identity. The identity element of a monoid is unique. A monoid in which each element has an inverse is a group.
Depending on the context, the symbol for the binary operation may be omitted, so that the operation is denoted by juxtaposition; for example, the monoid axioms may be written formula_1 and formula_2. This notation does not imply that it is numbers being multiplied.
Monoid structures.
Submonoids.
A submonoid of a monoid is a subset "N" of "M" that is closed under the monoid operation and contains the identity element "e" of "M". Symbolically, "N" is a submonoid of "M" if , whenever , and . "N" is thus a monoid under the binary operation inherited from "M".
Generators.
A subset "S" of "M" is said to be a generator of "M" if "M" is the smallest set containing "S" that is closed under the monoid operation, or equivalently "M" is the result of applying the finitary closure operator to "S". If there is a generator of "M" that has finite cardinality, then "M" is said to be finitely generated. Not every set "S" will generate a monoid, as the generated structure may lack an identity element.
Commutative monoid.
A monoid whose operation is commutative is called a commutative monoid (or, less commonly, an abelian monoid). Commutative monoids are often written additively. Any commutative monoid is endowed with its algebraic preordering ≤, defined by if there exists "z" such that . An order-unit of a commutative monoid "M" is an element "u" of "M" such that for any element "x" of "M", there exists a positive integer "n" such that . This is often used in case "M" is the positive cone of a partially ordered abelian group "G", in which case we say that "u" is an order-unit of "G".
Partially commutative monoid.
A monoid for which the operation is commutative for some, but not all elements is a trace monoid; trace monoids commonly occur in the theory of concurrent computation.
Examples.
Moreover, "f" can be considered as a function on the points formula_7 given by
or, equivalently
Multiplication of elements in formula_3 is then given by function composition.
Note also that when formula_11 then the function "f" is a permutation of formula_7
and gives the unique cyclic group of order "n".
Properties.
In a monoid, one can define positive integer powers of an element "x" : "x"1 = "x", and "x"n = "x" • ... • "x" ("n" times) for "n" > 1 . The rule of powers "x""n" + "p" = "x""n" • "x""p" is obvious.
From the definition of a monoid, one can show that the identity element "e" is unique. Then, for any "x", one can set "x"0 = "e" and the rule of powers is still true with nonnegative exponents.
It is possible to define invertible elements: an element "x" is called invertible if there exists an element "y" such that and . The element "y" is called the inverse of "x". If "y" and "z" are inverses of "x", then by associativity . Thus inverses, if they exist, are unique.
If "y" is the inverse of "x", one can define negative powers of "x" by setting and ("n" times) for . And the rule of exponents is still verified for all "n", "p" rational integers. This is why the inverse of "x" is usually written "x"−1. The set of all invertible elements in a monoid "M", together with the operation •, forms a group. In that sense, every monoid contains a group (possibly only the trivial group consisting of only the identity).
However, not every monoid sits inside a group. For instance, it is perfectly possible to have a monoid in which two elements "a" and "b" exist such that holds even though "b" is not the identity element. Such a monoid cannot be embedded in a group, because in the group we could multiply both sides with the inverse of "a" and would get that , which isn't true. A monoid has the cancellation property (or is cancellative) if for all "a", "b" and "c" in "M", always implies and always implies . A commutative monoid with the cancellation property can always be embedded in a group via the Grothendieck construction. That is how the additive group of the integers (a group with operation +) is constructed from the additive monoid of natural numbers (a commutative monoid with operation + and cancellation property). However, a non-commutative cancellative monoid need not be embeddable in a group.
If a monoid has the cancellation property and is "finite", then it is in fact a group. Proof: Fix an element "x" in the monoid. Since the monoid is finite, for some . But then, by cancellation we have that where "e" is the identity. Therefore, , so "x" has an inverse.
The right- and left-cancellative elements of a monoid each in turn form a submonoid (i.e. obviously include the identity and not so obviously are closed under the operation). This means that the cancellative elements of any commutative monoid can be extended to a group.
It turns out that requiring the cancellative property in a monoid is not required to perform the Grothendieck construction – commutativity is sufficient. However, if the original monoid has an absorbing element then its Grothendieck group is the trivial group. Hence the homomorphism is, in general, not injective.
An inverse monoid is a monoid where for every "a" in "M", there exists a unique "a"−1 in "M" such that and . If an inverse monoid is cancellative, then it is a group.
In the opposite direction, a zerosumfree monoid is an additively written monoid in which implies that and : equivalently, that no element other than zero has an additive inverse.
Acts and operator monoids.
Let "M" be a monoid, with the binary operation denoted by • and the identity element denoted by "e". Then a (left) "M"-act (or left act over "M") is a set "X" together with an operation which is compatible with the monoid structure as follows:
This is the analogue in monoid theory of a (left) group action. Right "M"-acts are defined in a similar way. A monoid with an act is also known as an operator monoid. Important examples include transition systems of semiautomata. A transformation semigroup can be made into an operator monoid by adjoining the identity transformation.
Monoid homomorphisms.
A homomorphism between two monoids and is a function such that
where "e""M" and "e""N" are the identities on "M" and "N" respectively. Monoid homomorphisms are sometimes simply called monoid morphisms.
Not every semigroup homomorphism is a monoid homomorphism, since it may not map the identity to the identity of the target monoid, even though the element it maps the identity to will be an identity of the image of the mapping. In contrast, a semigroup homomorphisms between groups is always a group homomorphism, as it necessarily preserves the identity. Since for monoids this isn't always true, it is necessary to state this as a separate requirement.
A bijective monoid homomorphism is called a monoid isomorphism. Two monoids are said to be isomorphic if there is a monoid isomorphism between them.
Equational presentation.
Monoids may be given a presentation, much in the same way that groups can be specified by means of a group presentation. One does this by specifying a set of generators Σ, and a set of relations on the free monoid Σ∗. One does this by extending (finite) binary relations on Σ∗ to monoid congruences, and then constructing the quotient monoid, as above.
Given a binary relation , one defines its symmetric closure as . This can be extended to a symmetric relation by defining if and only if and for some strings with . Finally, one takes the reflexive and transitive closure of "E", which is then a monoid congruence.
In the typical situation, the relation "R" is simply given as a set of equations, so that formula_13. Thus, for example,
is the equational presentation for the bicyclic monoid, and
is the plactic monoid of degree 2 (it has infinite order). Elements of this plactic monoid may be written as formula_16 for integers "i", "j", "k", as the relations show that "ba" commutes with both "a" and "b".
Relation to category theory.
Monoids can be viewed as a special class of categories. Indeed, the axioms required of a monoid operation are exactly those required of morphism composition when restricted to the set of all morphisms whose source and target is a given object. That is,
More precisely, given a monoid , one can construct a small category with only one object and whose morphisms are the elements of "M". The composition of morphisms is given by the monoid operation •.
Likewise, monoid homomorphisms are just functors between single object categories. So this construction gives an equivalence between the category of (small) monoids Mon and a full subcategory of the category of (small) categories Cat. Similarly, the category of groups is equivalent to another full subcategory of Cat.
In this sense, category theory can be thought of as an extension of the concept of a monoid. Many definitions and theorems about monoids can be generalised to small categories with more than one object. For example, a quotient of a category with one object is just a quotient monoid.
Monoids, just like other algebraic structures, also form their own category, Mon, whose objects are monoids and whose morphisms are monoid homomorphisms.
There is also a notion of monoid object which is an abstract definition of what is a monoid in a category. A monoid object in Set is just a monoid.
Monoids in computer science.
In computer science, many abstract data types can be endowed with a monoid structure. In a common pattern, a sequence of elements of a monoid is "folded" or "accumulated" to produce a final value. For instance, many iterative algorithms need to update some kind of "running total" at each iteration; this pattern may be elegantly expressed by a monoid operation. Alternatively, the associativity of monoid operations ensures that the operation can be parallelized by employing a prefix sum or similar algorithm, in order to utilize multiple cores or processors efficiently.
Given a sequence of values of type "M" with identity element formula_17 and associative operation formula_18, the "fold" operation is defined as follows:
In addition, any data structure can be 'folded' in a similar way, given a serialization of its elements. For instance, the result of "folding" a binary tree might differ depending on pre-order vs. post-order tree traversal.
Complete monoids.
A complete monoid is a commutative monoid equipped with an infinitary sum operation formula_20 for any index set "I" such that:
formula_21
and
formula_22
A continuous monoid is an ordered commutative monoid in which every directed set has a least upper bound compatible with the monoid operation:
formula_23
These two concepts are closely related: a continuous monoid is a complete monoid in which the infinitary sum may be defined as
where the supremum on the right runs over all finite subsets "E" of "I" and each sum on the right is a finite sum in the monoid.

</doc>
<doc id="19653" url="https://en.wikipedia.org/wiki?curid=19653" title="May 31">
May 31


</doc>
<doc id="19654" url="https://en.wikipedia.org/wiki?curid=19654" title="May 30">
May 30


</doc>
<doc id="19655" url="https://en.wikipedia.org/wiki?curid=19655" title="May 23">
May 23


</doc>
<doc id="19659" url="https://en.wikipedia.org/wiki?curid=19659" title="May 16">
May 16


</doc>
<doc id="19660" url="https://en.wikipedia.org/wiki?curid=19660" title="May 22">
May 22


</doc>
<doc id="19662" url="https://en.wikipedia.org/wiki?curid=19662" title="Mean value theorem">
Mean value theorem

In mathematics, the mean value theorem states, roughly: that given a planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints.
The theorem is used to prove global statements about a function on an interval starting from local hypotheses about derivatives at points of the interval.
More precisely, if a function formula_1 is continuous on the closed interval formula_2 , where formula_3 , then there exists a point formula_4 in formula_5 such that:
A special case of this theorem was first described by Parameshvara (1370–1460), from the Kerala school of astronomy and mathematics in India, in his commentaries on Govindasvāmi and Bhaskara II. The mean value theorem in its modern form was later stated by Augustin Louis Cauchy (1789–1857). It is one of the most important results in differential calculus, as well as one of the most important theorems in mathematical analysis, and is useful in proving the fundamental theorem of calculus. The mean value theorem follows from the more specific statement of Rolle's theorem, and can be used to prove the more general statement of Taylor's theorem (with Lagrange form of the remainder term).
Formal statement.
Let formula_7 , and differentiable on the open interval formula_5, where <math>a . Then there exists some formula_4 in formula_5 such that
The mean value theorem is a generalization of Rolle's theorem, which assumes formula_12, so that the right-hand side above is zero.
The mean value theorem is still valid in a slightly more general setting. One only needs to assume that formula_13 , and that for every formula_14 in formula_5 the limit
exists as a finite number or equals formula_17 or formula_18 . If finite, that limit equals formula_19 . An example where this version of the theorem applies is given by the real-valued cube root function mapping formula_20 , whose derivative tends to infinity at the origin.
Note that the theorem, as stated, is false if a differentiable function is complex-valued instead of real-valued. For example, define formula_21 for all real formula_14 . Then
while formula_24 for any real formula_14 .
Proof.
The expression formula_26 gives the slope of the line joining the points formula_27 and formula_28 , which is a chord of the graph of formula_1 , while formula_19 gives the slope of the tangent to the curve at the point formula_31 . Thus the Mean value theorem says that given any chord of a smooth curve, we can find a point lying between the end-points of the chord such that the tangent at that point is parallel to the chord. The following proof illustrates this idea.
Define formula_32 , where formula_33 is a constant. Since formula_1 is continuous on formula_2 and differentiable on formula_5 , the same is true for formula_37 . We now want to choose formula_33 so that formula_37 satisfies the conditions of Rolle's theorem. Namely
By Rolle's theorem, since formula_37 is differentiable and formula_42 , there is some formula_4 in formula_5 for which formula_45 , and it follows from the equality formula_32 that,
as required.
A simple application.
Assume that "f" is a continuous, real-valued function, defined on an arbitrary interval "I" of the real line. If the derivative of "f" at every interior point of the interval "I" exists and is zero, then "f" is constant in the interior.
Proof: Assume the derivative of "f" at every interior point of the interval "I" exists and is zero. Let ("a", "b") be an arbitrary open interval in "I". By the mean value theorem, there exists a point "c" in ("a","b") such that
This implies that "f"("a") = "f"("b"). Thus, "f" is constant on the interior of "I" and thus is constant on "I" by continuity. (See below for a multivariable version of this result.)
Remarks: 
Cauchy's mean value theorem.
Cauchy's mean value theorem, also known as the extended mean value theorem, is a generalization of the mean value theorem. It states: If functions "f" and "g" are both continuous on the closed interval ["a", "b"], and differentiable on the open interval ("a", "b"), then there exists some "c" ∈ ("a", "b"), such that
Of course, if and if , this is equivalent to:
Geometrically, this means that there is some tangent to the graph of the curve
which is parallel to the line defined by the points ("f"("a"), "g"("a")) and ("f"("b"), "g"("b")). However Cauchy's theorem does not claim the existence of such a tangent in all cases where ("f"("a"), "g"("a")) and ("f"("b"), "g"("b")) are distinct points, since it might be satisfied only for some value "c" with , in other words a value for which the mentioned curve is stationary; in such points no tangent to the curve is likely to be defined at all. An example of this situation is the curve given by
which on the interval [−1, 1] goes from the point (−1, 0) to (1, 0), yet never has a horizontal tangent; however it has a stationary point (in fact a cusp) at .
Cauchy's mean value theorem can be used to prove l'Hôpital's rule. The mean value theorem is the special case of Cauchy's mean value theorem when .
Proof of Cauchy's mean value theorem.
The proof of Cauchy's mean value theorem is based on the same idea as the proof of the mean value theorem.
Generalization for determinants.
Assume that formula_56 and formula_57 are differentiable functions on formula_5 that are continuous on formula_2. Define
There exists formula_61 such that formula_62.
Notice that
and if we place formula_64, we get Cauchy's mean value theorem. If we place formula_64 and formula_66 we get Lagrange's mean value theorem.
The proof of the generalization is quite simple: each of formula_67 and formula_68 are determinants with two identical rows, hence formula_69. The Rolle's theorem implies that there exists formula_70 such that formula_71.
Mean value theorem in several variables.
The mean value theorem generalizes to real functions of multiple variables. The trick is to use parametrization to create a real function of one variable, and then apply the one-variable theorem.
Let formula_72 be an open connected subset of formula_73 , and let formula_74 be a differentiable function. Fix points formula_75 such that the interval formula_76 lies in formula_72 , and define formula_78 . Since formula_37 is a differentiable function in one variable, the mean value theorem gives:
for some formula_4 between 0 and 1. But since formula_82 and formula_83 , computing formula_84 explicitly we have:
where formula_86 denotes a gradient and formula_87 a dot product. Note that this is an exact analog of the theorem in one variable (in the case formula_88 this "is" the theorem in one variable). By the Cauchy-Schwarz inequality, the equation gives the estimate:
In particular, when the partial derivatives of formula_1 are bounded, formula_1 is Lipschitz continuous (and therefore uniformly continuous). Note that formula_1 is not assumed to be continuously differentiable nor continuous on the closure of formula_72 . However, in the above, we used the chain rule so the existence of formula_94 would not be sufficient.
As an application of the above, we prove that formula_1 is constant if formula_72 is open and connected and every partial derivative of formula_1 is 0. Pick some point formula_98 , and let formula_99 . We want to show formula_100 for every formula_101 . For that, let formula_102 . Then "E" is closed and nonempty. It is open too: for every formula_103 ,
for every formula_105 in some neighborhood of formula_14 . (Here, it is crucial that formula_14 and formula_105 are sufficiently close to each other.) Since formula_72 is connected, we conclude formula_110 .
Remark that all arguments in the above are made in a coordinate-free manner; hence, they actually generalize to the case when formula_72 is a subset of a Banach space.
Mean value theorem for vector-valued functions.
There is no exact analog of the mean value theorem for vector-valued functions.
Jean Dieudonné in his classic treatise "Foundations of Modern Analysis" discards the mean value theorem and replaces it by mean inequality as the proof is not constructive and one cannot find the mean value and in applications one only needs mean inequality. Serge Lang in "Analysis I "uses the mean value theorem, in integral form, as an instant reflex but this use requires the continuity of the derivative. If one uses the Henstock-Kurzweil integral one can have the mean value theorem in integral form without the additional assumption that derivative should be continuous as every derivative is Henstock-Kurzweil integrable. The problem is roughly speaking the following: If "f" : "U" → R"m" is a differentiable function (where "U" ⊂ R"n" is open) and if "x" + "th", "x, h" ∈ R"n", "t" ∈ 1 is the line segment in question (lying inside "U"), then one can apply the above parametrization procedure to each of the component functions "fi" ("i" = 1, ..., "m") of "f" (in the above notation set "y" = "x" + "h"). In doing so one finds points "x" + "tih" on the line segment satisfying
But generally there will not be a "single" point "x" + "t*h" on the line segment satisfying
for all "i" "simultaneously". For example define:
Then "f"(2π) − f(0) = 0 ∈ R2, but formula_115 and formula_116 are never simultaneously zero as "x" ranges over 2π.)
However a certain type of generalization of the mean value theorem to vector-valued functions is obtained as follows: Let "f" be a continuously differentiable real-valued function defined on an open interval "I", and let "x" as well as "x" + "h" be points of "I". The mean value theorem in one variable tells us that there exists some "t*" between 0 and 1 such that
On the other hand, we have, by the fundamental theorem of calculus followed by a change of variables,
Thus, the value "f′"("x" + "t*h") at the particular point "t*" has been replaced by the mean value
This last version can be generalized to vector valued functions:
Proof. Let "f"1, ..., "fm" denote the components of "f" and define:
Then we have
The claim follows since "Df" is the matrix consisting of the components formula_123
Proof. Let "u" in R"m" denote the value of the integral
Now we have (using the Cauchy–Schwarz inequality):
Now cancelling the norm of "u" from both ends gives us the desired inequality.
Proof. From Lemma 1 and 2 it follows that
Mean Value Theorems for Definite Integrals.
First Mean Value Theorem for Definite Integrals.
Let "f" : ["a", "b"] → R be a continuous function. Then there exists "c" in ("a", "b") such that
Since the mean value of "f" on ["a", "b"] is defined as
we can interpret the conclusion as "f" achieves its mean value at some "c" in ("a", "b").
In general, if "f" : ["a", "b"] → R is continuous and "g" is an integrable function that does not change sign on ["a", "b"], then there exists "c" in ("a", "b") such that
Proof of the First Mean Value Theorem for Definite Integrals.
Suppose "f" : ["a", "b"] → R is continuous and "g" is a nonnegative integrable function on ["a", "b"]. By the extreme value theorem, there exists "m" and "M" such that for each "x" in ["a", "b"], formula_132 and formula_133. Since "g" is nonnegative,
Now let 
If formula_136, we're done since 
means
so for any "c" in ("a", "b"),
If "I" ≠ 0, then
By the intermediate value theorem, "f" attains every value of the interval ["m", "M"], so for some "c" in ["a", "b"]
that is,
Finally, if "g" is negative on ["a", "b"], then
and we still get the same result as above.
QED
Second Mean Value Theorem for Definite Integrals.
There are various slightly different theorems called the second mean value theorem for definite integrals. A commonly found version is as follows:
Here formula_145 stands for formula_146, the existence of which follows from the conditions. Note that it is essential that the interval ("a", "b"] contains "b". A variant not having this requirement is:
Mean value theorem for integration fails for vector-valued functions.
If the function formula_72 returns a multi-dimensional vector, then the MVT for integration is not true, even if the domain of formula_72 is also multi-dimensional.
For example, consider the following 2-dimensional function defined on an formula_150-dimensional cube:
Then, by symmetry it is easy to see that the mean value of formula_72 over its domain is (0,0):
However, there is no point in which formula_154, because formula_155 everywhere.
A probabilistic analogue of the mean value theorem.
Let "X" and "Y" be non-negative random variables such that E["X"] < E["Y"] < ∞ and formula_156 (i.e. "X" is smaller than "Y" in the usual stochastic order). Then there exists an absolutely continuous non-negative random variable "Z" having probability density function
Let "g" be a measurable and differentiable function such that E["g"("X")], E["g"("Y")] < ∞, and let its derivative "g′" be measurable and Riemann-integrable on the interval ["x", "y"] for all "y" ≥ "x" ≥ 0. Then, E["g′"("Z")] is finite and
Generalization in complex analysis.
As noted above, the theorem does not hold for differentiable complex-valued functions. Instead, a generalization of the theorem is stated such:
Let "f" : Ω → C be a holomorphic function on the open convex set Ω, and let "a" and "b" be distinct points in Ω. Then there exist points "u", "v" on "Lab" (the line segment from "a" to "b") such that
Where Re() is the Real part and Im() is the Imaginary part of a complex-valued function.

</doc>
<doc id="19664" url="https://en.wikipedia.org/wiki?curid=19664" title="Mallow">
Mallow

Mallow or Mallows may refer to:

</doc>
<doc id="19665" url="https://en.wikipedia.org/wiki?curid=19665" title="Marc Bloch">
Marc Bloch

Marc Léopold Benjamin Bloch ( ; 6 July 1886 – 16 June 1944) was a French historian who cofounded the highly influential Annales School of French social history. Bloch was a quintessential modernist. An assimilated Alsatian Jew from an academic family in Paris, he was deeply affected in his youth by the Dreyfus Affair. He studied at the elite École Normale Supérieure; in 1908–9 he studied at Berlin and Leipzig. He fought in the trenches of the Western Front for four years. In 1919 he became Lecturer in Medieval history at Strasbourg University, after the German professors were all expelled; he was called to the University of Paris in 1936 as professor of economic history. He is best known for his pioneering studies "French Rural History" and "Feudal Society" and his posthumously-published unfinished meditation on the writing of history, "The Historian's Craft." A French soldier in both World Wars, he was captured and shot by the Gestapo during the German occupation of France for his work in the French Resistance.
Youth and First World War.
Born in Lyon to a Jewish family, the son of the professor of ancient history Gustave Bloch, Marc studied at the École Normale Supérieure and Fondation Thiers in Paris, then at Berlin and Leipzig. He was an officer of infantry in World War I, rising to the rank of captain and being awarded the Légion d'honneur.
After the war, he went to the university at Strasbourg, then in 1936 succeeded Henri Hauser as professor of economic history at the Sorbonne.
Career.
In 1924 he published one of his most famous works "Les rois thaumaturges: étude sur le caractère surnaturel attribué à la puissance royale particulièrement en France et en Angleterre" (translated in English as "The magic-working kings" or "The royal touch: sacred monarchy and scrofula in England and France") in which he collected, described and studied the documents pertaining to the ancient tradition that the kings of the Middle Ages were able to cure the disease of scrofula simply by touching people suffering from it. This tradition has its roots in the magical role of kings in ancient societies. This work by Bloch had a great impact not only on the social history of the Middle Ages but also on cultural anthropology.
Bloch's most important work centered on the study of feudalism. In 1939 he published a large work, available in a two-volume English translation as "Feudal Society." In some ways, his most innovative work is his monograph 1931 "French Rural History."
Annales.
With colleague Lucien Febvre he founded the Annales School in 1929, by starting the new scholarly journal, "Annales d'Histoire Economique et Sociale" ("Annals of economic and social history"), which broke radically with traditional historiography by insisting on the importance of taking all levels of society into consideration and emphasized the collective nature of mentalities.
Bloch has had lasting influence in the field of historiography through his unfinished manuscript "The Historian's Craft", which he was working on at his death. Bloch's book is often considered one of the most important historiographical works of the 20th century.
Historiography.
Bloch was highly interdisciplinary, influenced by the geography of Paul Vidal de la Blache (1845–1918) and the sociology of Émile Durkheim (1858–1917). In "Méthodologie Historique" (written in 1906 but not published until 1988), Bloch rejected the histoire événementielle (event history) of his mentors Charles-Victor Langlois and Charles Seignobos to argue for greater analysis of the role of structural and social phenomena in determining the outcome of historical events. Bloch was trying to reinvent history as a social science, but he departed significantly from Durkheim in his refusal to exclude psychology from history; Bloch maintained that the individual actor should be considered along with social forces. Bloch's methodology was also greatly influenced by his father, Gustave Bloch, a historian of the ancient world, and by 19th-century scholars such as Gabriel Monod, Ernest Renan, and Numa Denis Fustel de Coulanges.
Bloch vigorously supported the idea of international scholarly cooperation and tried unsuccessfully to set up an international journal with American support. Bloch wrote some 500 reviews of German books and articles, While promoting the importance of German historiography and admiring its scholarly rigor, he repeatedly criticized its nationalism and methodological limitations.
Miracles and mentalities.
In "Les Rois Thaumaturges" (1924) Bloch looked at the long-standing folk belief that the king could cure scrofula by touch. The kings of France and England indeed regularly practised the ritual. Bloch was not concerned with the effectiveness of the royal touch—he acted like an anthropologist in asking why people believed it and how it shaped relations between king and commoner. The book was highly influential in introducing comparative studies (in this case France and England), as well as long-duration studies spanning a thousand years (with specific events used as illustrations). By investigating the impact of rituals, the efficacy of myths, and all the possible sources of collective behavior, he became the "father of historical anthropology." Bloch's revolutionary charting of mentalities resonated with scholars who were reading Freud and Proust. Stirling (2007) examines this essentially stylistic trait alongside Bloch's peculiarly quixotic idealism, which tempered and sometimes compromised his work through his hope for a truly cooperative model of historical inquiry. While humanizing and questioning him, Stirling gives credit to Bloch for helping to break through the monotonous methodological alternance between positivism and narrative history, creating a new, synthetic version of the historical practice that has since become so ingrained in the discipline that it is typically overlooked.
Rural history.
Bloch's own ideas on rural history were best expressed in his masterworks, "French Rural History" ("Les caractères originaux de l'histoire rurale française," 1931) and "Feudal Society" (1939).
In "L'Individualisme Agraire du XVIIIe Siècle" (1978), Bloch characterized the agrarian reforms of 18th-century France as a "failed revolution," citing the persistence of regional traditions as evidence for their failure. A typical example of the Annales School's "total history," Bloch's argument weaves the connections between politics, culture, and economics against a backdrop of class conflict to illustrate how "the conscious actions of men have overcome the rhythms of the materialist causality of history." He argued that the anti-feudal sentiment of French peasants expressed in the 1789 cahier de doléances (list of grievances) was linked to the "seigneurial reaction" of the late 18th century in which lords significantly increased feudal dues. Bloch argued that it was this intensified exploitation that provoked peasant revolt, leading to the Revolution.
History of technology.
The November 1935 issue of the "Annales" contains Febvre's introduction that defines three essential approaches to a history of technology: to investigate technology, to understand the progress of technology, and to understand the relationship of technology to other human activities. Bloch's article, "The Advent and Triumph of the Watermill in Medieval Europe," incorporates these approaches by investigating the connections between technology and broader social issues.
Second World War.
In 1939 France declared war on Germany after its invasion and occupation of Poland. As France mobilized its troops, Marc Bloch left his position at the Sorbonne and took up his reserve status as a captain in the French Army at the age of 52. He was encouraged at the time by colleagues both in France and abroad to leave the country. He said it was his personal obligation to stand for the moral imperative.
His memoir of the first days of World War II, "Strange Defeat," written in 1940 but not published until 1946, blamed the French military establishment, along with her social and political culture, for the sudden total military defeat and helped after the war to neutralize the traumatic memory of France's failure and to build a new French identity.
Bloch joined the French Resistance in late 1942, driven by ardent patriotism, identification with his Jewish roots and a conception of France as the champion of liberty. His code name was "Narbonne". He was eventually captured in Lyon by Vichy police in March 1944 and turned over to the Gestapo. He was then imprisoned in Montluc prison, and was tortured by the Gestapo at their headquarters. He was interrogated by Klaus Barbie who was in charge of interrogations at the prison. Under such treatment Bloch remained "calm and stoic" throughout, according to his biographer Carole Fink, reportedly providing no other information to his captors than his real name.
Execution.
At around 8pm on the night of 16 June 1944, ten days after D-Day, Marc Bloch was among twenty-eight Resistance prisoners taken by German troops in a camionette (an open truck) along the River Saone to a place called La Rousille just before the village of Saint-Didier-de-Formans in the Ain department. Here, shortly after 10.00 pm in a meadow surrounded by high bushes, Bloch was executed by firing squad, one of the first group of four of the twenty-eight, handcuffed in pairs, to face the machine guns, and one of twenty-six men to be murdered that night in a period of twenty minutes. The victims were stripped of all means of identification and left by the Germans in the field. The following day they were found by Marcel Pouveret, a schoolmaster, who informed the mayor of St Didier, to whom he was assistant, and the mayor called in the gendarmerie of Trevoux to bury the bodies. There is today a memorial to those killed in the meadow near where they were shot.
Marc Bloch, one of the greatest historians of the twentieth century and a hero of the Resistance, was murdered less than a month before his fifty-eighth birthday.
As Bloch had spent his final days in prison, he left unfinished one of his most intimate works and a classic of historiography, "The Historian's Craft" ("Apologie pour l'histoire ou Métier d'historien"), which was edited and published posthumously, by which time Marc Bloch had become a national martyr following the Allied liberation.

</doc>
<doc id="19667" url="https://en.wikipedia.org/wiki?curid=19667" title="Michael Ventris">
Michael Ventris

Michael George Francis Ventris, OBE (; 12 July 1922 – 6 September 1956) was an English linguist and architect who, along with John Chadwick and Alice Kober, deciphered Linear B, a previously unknown ancient script discovered at Knossos by Arthur Evans. A prodigy in languages, Ventris had pursued the decipherment as a vocation since his adolescence. After creating a new field of study, Ventris died in an automobile accident a few weeks before the publication of his definitive work, "Documents in Mycenaean Greek".
Biography.
Early life.
Ventris was born into a traditional army family. His father, Edward Francis Vereker Ventris, reached the rank of lieutenant colonel in the Indian Army; however, his career was suddenly brought to an end, as he contracted tuberculosis and retired. His grandfather, Francis Ventris, was a major-general who ended his career as Commander of British Forces in China. During his time in England, Edward Ventris met and married Anna Dorothea Janasz (Dora), the daughter of a wealthy immigrant landholder from Poland. Michael Ventris was their only child.
Health became an issue early on in Ventris's life, as he developed at a young age chronic bronchial asthma. The family then moved to Switzerland for eight years, seeking a clean and healthy environment for their child. A number of health clinics and spas catered to the physical well-being of Ventris, constantly observing his well-being. Ventris started school in Gstaad, where classes were taught in French and German. He soon was fluent in both languages and showing proficiency for Swiss German. He was capable of learning a language within a matter of weeks, which allowed him to acquire fluency in a dozen languages. His mother, of Polish descent, often spoke to him in her own tongue, in which he was fluent by the age of eight. At this time, he was reading Adolf Erman's "Die Hieroglyphen" in German.
In 1931, the Ventris family returned home. Michael's father's physical condition was worsening as he got older. From 1931 to 1935 Ventris was sent to Bickley Hill School in Stowe. His parents, unable to continue living together, divorced in 1935, when he was 13. At this time, he secured a scholarship to Stowe School for Boys Stowe School, quartered in an 18th-century stately home. At Stowe he learned some Latin and classical Greek. He did not do outstanding work there - by then he was spending most of his spare time learning as much as he could about Linear B, some of his study time being spent under the covers at night with a flashlight. When he was not away at school, Ventris lived with his mother, before 1935 in coastal hotels, after 1935 (when they were built) in the avant garde Berthold Lubetkin's Highpoint modernist apartments in Highgate. His mother's acquaintances, who frequented the house, included many sculptors, painters, and writers of the day. The money for her sophisticated lifestyle came from the Polish estates.
Young adult.
Ventris's father died in 1938 when Ventris was 16 years old. Dora became administrator of the estate. Hard times, however, lay ahead. After the German invasion of Poland in 1939 the family holdings in that country were gone, and all income from there ceased. In 1940 Dora's father died. The family became destitute. Ventris lost his mother to clinical depression and an overdose of barbiturates. He never spoke of her, assuming instead an ebullient and energetic manner in whatever he decided to do, a trait which won him numerous friends. At the same time they noted that he had a dark and mysterious side as well, associated with feelings that he was a fraud, and not a true genius. A friend of the family, Russian sculptor Naum Gabo, took Ventris under his wing. Ventris later said that Gabo was the most family he had ever had. It may have been at Gabo's house that he began the study of Russian. He had resolved on architecture for a career. He enrolled at the Architectural Association School of Architecture. There he met and married Lois, who preferred to be called Betty. Her social background was similar to what Ventris's had been: her family was well-to-do, she had travelled in Europe, and she was interested in architecture, in addition to which she was popular and was considered very beautiful.
Ventris did not complete his architecture studies, being conscripted in 1942. He chose the Royal Air Force (RAF). His preference was for navigator rather than pilot, and he completed the extensive training in the UK and Canada, to qualify early in 1944 and be commissioned. While training, he studied Russian intensively for several weeks, the purpose of which, if any, is not clear. He took part in the bombing of Germany, as aircrew on the Handley Page Halifax with No. 76 Squadron RAF, initially at RAF Breighton and then at RAF Holme-on-Spalding Moor. After the conclusion of the war he served out the rest of his term on the ground in Germany, for which he was chosen because of his knowledge of Russian. His duties are unclear. His friends all assumed he was completing intelligence assignments, interpreting his denials as part of a legal gag. No such assignments have turned up, however, even after these many decades since his service. There is also no evidence that he was ever part of any code-breaking unit, as was Chadwick, even though the public readily believed this explanation of his genius and success with Linear B.
Architect and palaeographer.
After the war he worked briefly in Sweden, learning enough Swedish to communicate with scholars in it. Then he came home to complete his architectural education with honors in 1948 and settled down with Lois working as an architect. He designed schools for the Ministry of Education. Then he and his wife designed a home for themselves and their family. He had two children, a son, Nikki (1942–1984) and a daughter, Tessa (born 1946). Concurrently he stepped up his effort on Linear B, discovering finally that it was Greek, a revelation to an academic public that had more or less given up on the mysterious script. No one, not even Ventris, suspected that it is the earliest known form of Greek. Ventris was awarded an OBE in 1955 for "services to Mycenaean paleography." A few years after deciphering Linear B in 1951–1953, Ventris, who lived in Hampstead, died instantly in a late-night collision with a parked truck while driving home, aged 34. It is believed that his death, like his mother's, was suicide.
An English Heritage blue plaque commemorates Ventris at his home in Hampstead.
Decipherment.
At the beginning of the 20th century, archaeologist Arthur Evans began excavating Knōssos, an ancient city on the island of Crete. In doing so he uncovered a great many clay tablets inscribed with an unknown script. Some were older and were named Linear A. The bulk were of more recent vintage, and were dubbed Linear B. Evans spent the next several decades trying to decipher both, to no avail.
In 1936, Evans hosted an exhibition of Cretan archaeology at Burlington House in London, home of the Royal Academy. It was the jubilee anniversary (50 years) of the British School of Archaeology in Athens, contemporaneous owners and managers of the Knossos site. Evans had given the site to them some years previously. Villa Ariadne, Evans's home there, was now part of the school. Boys from Stowe school were in attendance at one lecture and tour conducted by Evans himself at age 85. Ventris, 14 years old, was present and remembered Evans walking with a stick. The stick was undoubtedly the cane named Prodger which Evans carried all his life to assist him with his short-sightedness and night blindness. Evans held up tablets of the unknown scripts for the audience to see. During the interview period following the lecture, Ventris immediately confirmed that Linear B was as yet undeciphered, and determined to decipher it.
Ventris's initial theory was that Etruscan and Linear B were related and that this might provide a key to decipherment. Although this proved incorrect, it was a link he continued to explore until the early 1950s.
Shortly after Evans died, Alice Kober noted that certain words in Linear B inscriptions had changing word endings — perhaps declensions in the manner of Latin or Greek. Using this clue, Ventris constructed a series of grids associating the symbols on the tablets with consonants and vowels. While "which" consonants and vowels these were remained mysterious, Ventris learned enough about the structure of the underlying language to begin guessing.
Some Linear B tablets had been discovered on the Greek mainland, and there was reason to believe that some of the chains of symbols he had encountered on the Cretan tablets were names. Noting that certain names appeared only in the Cretan texts, Ventris made the inspired guess that those names applied to cities on the island. This proved to be correct. Armed with the symbols he could decipher from this, Ventris soon unlocked much text and determined that the underlying language of Linear B was in fact Greek. This overturned Evans's theories of Minoan history by establishing that Cretan civilization, at least in the later periods associated with the Linear B tablets, had been part of Mycenean Greece.

</doc>
<doc id="19668" url="https://en.wikipedia.org/wiki?curid=19668" title="Maniac Mansion">
Maniac Mansion

Maniac Mansion is a 1987 graphic adventure video game developed and published by Lucasfilm Games. It follows teenage protagonist Dave Miller as he attempts to rescue his girlfriend from a mad scientist, whose mind has been enslaved by a sentient meteor. The player uses a point-and-click interface to guide Dave and two of his six playable friends through the scientist's mansion while solving puzzles and avoiding dangers. Gameplay is nonlinear, and the game must be completed in different ways based on the player's choice of characters. Initially released for the Commodore 64 and Apple II, "Maniac Mansion" was Lucasfilm Games' first self-published product.
The game was conceived in 1985 by Ron Gilbert and Gary Winnick, who sought to tell a comedic story based on horror film and B movie clichés. They mapped out the project as a paper-and-pencil game before coding commenced. While earlier adventure titles had relied on command lines, Gilbert disliked such systems, and he developed "Maniac Mansion"s simpler point-and-click interface as a replacement. To speed up production, he created a game engine called SCUMM, which was used in many later LucasArts titles. After its release, "Maniac Mansion" was ported to several platforms. A port for the Nintendo Entertainment System had to be reworked heavily, in response to complaints by Nintendo of America that the game was inappropriate for children.
"Maniac Mansion" was critically acclaimed: reviewers lauded its graphics, cutscenes, animation and humor. Writer Orson Scott Card praised it as a step toward "computer games a valid storytelling art." It influenced numerous graphic adventure titles, and its point-and-click interface became a standard feature in the genre. The game's success solidified Lucasfilm as a serious rival to adventure game studios such as Sierra On-Line. In 1990, "Maniac Mansion" was adapted into a three-season television series of the same name, written by Eugene Levy and starring Joe Flaherty. A sequel to the game, entitled "Day of the Tentacle", was released in 1993.
Overview.
"Maniac Mansion" is a graphic adventure game in which the player uses a point-and-click interface to guide characters through a two-dimensional game world and to solve puzzles. Fifteen action commands, such as "Walk To" and "Unlock", may be selected by the player from a menu on the screen's lower half. The player starts the game by choosing two out of six characters to accompany protagonist Dave Miller. Each character possesses unique abilities: for example, Syd and Razor can play musical instruments, while Bernard can repair appliances. The game may be completed with any combination of characters; but, since many puzzles are solvable only by certain characters, different paths must be taken based on the group's composition. "Maniac Mansion" features cutscenes, a word coined by Ron Gilbert, that interrupt gameplay to advance the story and inform the player about offscreen events.
The game takes place in the mansion of the fictional Edison family: Dr. Fred, a mad scientist; Nurse Edna, his wife; and their son Weird Ed. Living with the Edisons are two large, disembodied tentacles, one purple and the other green. The intro sequence shows that a sentient meteor crashed near the mansion twenty years earlier; it brainwashed the Edisons and directed Dr. Fred to obtain human brains for use in experiments. The game begins as Dave Miller prepares to enter the mansion to rescue his girlfriend, Sandy Pantz, who was kidnapped by Dr. Fred. With the exception of the green tentacle, the mansion's inhabitants are hostile, and will throw the player characters into the dungeon—or, in some situations, kill them—if they see them. When a character dies, the player must choose a replacement from the unselected characters; and the game ends if all characters are killed. "Maniac Mansion" has five possible endings, based on which characters are chosen, which survive, and what the characters accomplish.
Development.
Conception.
"Maniac Mansion" was conceived in 1985, when Lucasfilm Games employees Ron Gilbert and Gary Winnick were assigned to create an original game. Gilbert had been hired the previous year as a programmer for the game "Koronis Rift". He befriended Winnick over their similar tastes in humor, film and television. Company management provided little oversight in the creation of "Maniac Mansion", a trend to which Gilbert credited the success of several of his games for Lucasfilm.
Gilbert and Winnick co-wrote and co-designed the project, but they worked separately as well: Gilbert on programming and Winnick on visuals. As both of them enjoyed B horror films, they decided to make a comedy-horror game set in a haunted house. They drew inspiration from a film whose name Winnick could not recall. He described it as "a ridiculous teen horror movie", in which teenagers inside a building were killed one by one without any thought of leaving. This film, combined with clichés from popular horror movies such as "Friday the 13th" and "A Nightmare on Elm Street", became the basis for the game's setting. Early work on the game progressed organically: according to Gilbert, "Very little was written down. Gary and I just talked and laughed a lot, and out it came." Lucasfilm Games relocated to the Stable House at Skywalker Ranch during "Maniac Mansion"s conception period, and the ranch's Main House was used as a model for the mansion. Several rooms from the Main House received exact reproductions in the game, such as a library with a spiral staircase and a media room with a large-screen TV and grand piano.
Story and characters were a primary concern for Gilbert and Winnick. The pair based the game's cast on friends, family members, acquaintances and stereotypes. For example, Winnick's girlfriend Ray was the inspiration for Razor, while Dave and Wendy were based, respectively, on Gilbert and a fellow Lucasfilm employee named Wendy. According to Winnick, the Edison family was shaped after characters from EC Comics and Warren Publishing magazines. The sentient meteor that brainwashes Dr. Fred was inspired by a segment from the 1982 anthology film "Creepshow". A man-eating plant, similar to that of "Little Shop of Horrors", was included as well. The developers sought to strike a balance between tension and humor with the game's story.
Initially, Gilbert and Winnick struggled to choose a gameplay genre for "Maniac Mansion". While visiting relatives over Christmas, Gilbert saw his cousin play "King's Quest: Quest for the Crown", an adventure game by Sierra On-Line. Although he was a fan of text adventures, this was Gilbert's first experience with a graphic adventure, and he used the holiday to play the game and familiarize himself with the format. As a result, he decided to develop his and Winnick's ideas into a graphic adventure game.
"Maniac Mansion"s story and structure were designed before coding commenced. The project's earliest incarnation was a simple paper-and-pencil board game, in which the mansion's floor plan was used as a game board, and cards represented events and characters. Lines connected the rooms to illustrate pathways by which characters could travel. Strips of cellulose acetate were used to map out the game's puzzles by tracking which items worked together when used by certain characters. Impressed by the map's complexity, Winnick included it in the final game as a poster hung on a wall. Because each character contributes different skills and resources, the pair spent months working on the event combinations that could occur. This extended the game's production time beyond that of previous Lucasfilm Games projects, which almost led to Gilbert's firing. The game's dialogue, written by David Fox, was not created until after programming had begun.
Production and SCUMM.
Gilbert started programming "Maniac Mansion" in 6502 assembly language, but he quickly decided that the project was too large and complex for this method. He decided that a new game engine would have to be created. Its coding language was initially planned to be LISP-inspired, but Gilbert opted for one similar to C. Lucasfilm employee Chip Morningstar contributed the base code for the engine, which Gilbert then built on. Gilbert hoped to create a "system that could be used on many adventure games, cutting down the time it took to make them". "Maniac Mansion"s first six-to-nine months of production were dedicated largely to engine development. The game was developed around the Commodore 64 home computer, an 8-bit system with only 64 KB of memory. The team wanted to include scrolling screens, but as it was normally impossible to scroll bitmap graphics on the Commodore 64, they had to utilize lower-detail tile graphics. Winnick gave each character a large head made of three stacked sprites to make them recognizable.
Although Gilbert wrote much of the foundational code for "Maniac Mansion", the majority of the game's events were programmed by Lucasfilm employee David Fox. Fox was between projects and planned to work on the game only for a month, but he remained with the team for six months. With Gilbert, he wrote the characters' dialog and choreographed the action. Winnick's concept art inspired him to add new elements to the game: for example, Fox allowed the player to place a hamster inside the kitchen's microwave.
The team wanted to avoid punishing the player for applying everyday logic in "Maniac Mansion". Fox noted that one Sierra game features a scene in which the player, without prior warning, may encounter a game over screen simply by picking up a shard of glass. He characterized such game design as "sadistic", and he commented, "I know that in the real world I can successfully pick up a broken piece of mirror without dying". Because of the project's nonlinear puzzle design, the team struggled to prevent no-win scenarios, in which the player unexpectedly became unable to complete the game. As a result of this problem, Gilbert later explained, "We were constantly fighting against the desire just to rip out all the endings and just go with three characters, or even sometimes just one character". Lucasfilm Games had only one playtester, and many dead-ends went undetected as a result. Further playtesting was provided by Gilbert's uncle, to whom Gilbert mailed a floppy disk of the game's latest version each week.
The "Maniac Mansion" team wanted to retain the structure of a text-based adventure game, but without the standard command-line interface. Gilbert and Winnick were frustrated by the genre's text parsers and frequent game over screens. While in college, Gilbert had enjoyed "Colossal Cave Adventure" and the games of Infocom, but he disliked their lack of visuals. He found the inclusion of graphics in Sierra On-Line games, such as "King's Quest", to be a step in the right direction. However, these games still require the player to type, and to guess which commands must be input. In response, Gilbert programmed a point-and-click graphical user interface that displays every possible command. Fox had made a similar attempt to streamline Lucasfilm's earlier "", and he conceived the entirety of "Maniac Mansion"s interface, according to Gilbert. Forty input commands were planned at first, but the number was gradually reduced to 12. Gilbert finished the "Maniac Mansion" engine—which he later named "Script Creation Utility for Maniac Mansion" (SCUMM)—after roughly one year of work. Although the game was designed for the Commodore 64, the SCUMM engine allowed it to be ported easily to other platforms.
After 18 to 24 months of development, "Maniac Mansion" debuted at the 1987 Consumer Electronics Show in Chicago. The game was released for the Commodore 64 and Apple II in October 1987. While previous Lucasfilm Games products had been published by outside companies, "Maniac Mansion" was self-published. This became a trend at Lucasfilm. The company hired Ken Macklin, an acquaintance of Winnick's, to design the game's packaging artwork. Gilbert and Winnick collaborated with the marketing department to design the back cover. The two also created an insert that includes hints, a backstory and jokes. An MS-DOS port was released in early 1988, developed in part by Lucasfilm employees Aric Wilmunder and Brad Taylor. Ports for the Amiga, Atari ST and Nintendo Entertainment System (NES) followed, with the Amiga and Atari ST ports in 1989 and the NES port in 1990. The 16-bit versions of Maniac Mansion featured a copy protection system requiring the user to enter graphical symbols out of a code book include with the game. This was not present in the Commodore 64 and Apple versions due to lack of disk space, so those instead used an on-disk copy protection.
Nintendo Entertainment System version.
There were two separate versions of the game developed for the NES. The first port was handled and published by Jaleco only in Japan. Released in 1989 it featured characters redrawn in a cute art style and generally shrunken rooms. No scrolling is present, leading to rooms larger than a single screen to be displayed via flip-screens. Many of the background details are missing, and instead of a save feature an over 100 character long password is required to save progress.
In September 1990 Jaleco released an American version of "Maniac Mansion" as the first NES title developed by Lucasfilm Games in cooperation with Realtime Associates. Generally, this port is regarded as being far closer to the original game than the Japanese effort. Company management was occupied with other projects, and so the port received little attention until employee Douglas Crockford volunteered to direct it. The team used a modified version of the SCUMM engine called "NES SCUMM" for the port. According to Crockford, " of the main differences between the NES and PCs is that the NES can do certain things much faster". The graphics had to be entirely redrawn to match the NES's display resolution. Tim Schafer, who later designed "Maniac Mansion"s sequel "Day of the Tentacle", received his first professional credit as a playtester for the NES version of "Maniac Mansion".
During "Maniac Mansion"s development for the Commodore 64, Lucasfilm had censored profanity in the script: for instance, the early line of dialogue "Don't be a shit head" became "Don't be a tuna head". However, additional content was removed from the NES version to make it suitable for a younger audience, and to conform with Nintendo's policies. Jaleco USA president Howie Rubin warned Crockford about content to which Nintendo might object, such as the word "kill". After reading the NES Game Standards Policy for himself, Crockford suspected that further elements of "Maniac Mansion" could be problematic, and he sent a list of questionable content to Jaleco. When the company replied that the content was reasonable, Lucasfilm Games submitted "Maniac Mansion" for approval.
One month later, Nintendo of America contacted Lucasfilm Games to request the removal of offensive text and nude graphics. Crockford censored this content but attempted to leave the game's essence intact. For example, Nintendo wanted graffiti in one room—which provided an important hint to players—removed from the game. Unable to comply without simultaneously removing the hint, the team simply shortened it. Sexually suggestive and otherwise "graphic" dialogue was edited, including a remark from Dr. Fred about "pretty brains sucked out". The nudity described by Nintendo encompassed a swimsuit calendar, a classical sculpture and a poster of a mummy in a Playmate pose. After a brief fight to keep the sculpture, the team ultimately removed all three. The phrase "NES SCUMM" in the credits sequence was censored as well.
Lucasfilm Games re-submitted the edited version of "Maniac Mansion" to Nintendo, which then manufactured 250,000 cartridges. Each cartridge was fitted with a battery-powered back-up to save data. Nintendo announced the port through its official magazine in early 1990, and it provided further coverage later that year. The ability to microwave a hamster remained in the game, which Crockford cited as an example of the censors' contradictory criteria. However, Nintendo later noticed it, and after the first batch of cartridges was sold, Jaleco was forced to remove the content from future shipments.
Late in development, Jaleco commissioned Realtime Associates to provide background music, which no previous version of "Maniac Mansion" had featured. Realtime Associates' founder and president David Warhol noted that "video games at that time had to have 'wall to wall' music". He brought in George "The Fat Man" Sanger and his band, along with David Hayes, to compose the score. Their goal was to create songs that suited each character, such as a punk rock theme for Razor, an electronic rock theme for Bernard and a version of Thin Lizzy's "The Boys Are Back in Town" for Dave Miller. Warhol translated their work into NES chiptune music.
Reception.
Keith Farrell of "Compute!'s Gazette" was struck by "Maniac Mansion"s similarity to film, particularly in its use of cutscenes to impart "information or urgency". He lauded the game's graphics, animation and high level of detail. "Commodore User"s Bill Scolding and three reviewers from "Zzap!64" compared the game to "The Rocky Horror Picture Show". Further comparisons were drawn to "Psycho", "Friday the 13th", "The Texas Chain Saw Massacre", "The Addams Family" and "Scooby-Doo". Russ Ceccola of "Commodore Magazine" found the cutscenes to be creative and well made, and he commented that the "characters are distinctively Lucasfilm's, bringing facial expressions and personality to each individual character". In "Compute!", Orson Scott Card praised the game's humor, cinematic storytelling and lack of violence. He called it "compellingly good" and evidence of Lucasfilm's push "to make computer games a valid storytelling art".
German magazine "Happy-Computer" commended the point-and-click interface and likened it to that of "Uninvited" by ICOM Simulations. The publication highlighted "Maniac Mansion"s graphics, originality and overall enjoyability: one of the writers called it the best adventure title yet released. "Happy-Computer" later reported that "Maniac Mansion" was the highest-selling video game in West Germany for three consecutive months. The game's humor received praise from "Zzap!64", whose reviewers called the point-and-click controls "tremendous" and the total package "innovative and polished". Shay Addams of "Questbusters: The Adventurer's Newsletter" preferred "Maniac Mansion"s interface to that of "Labyrinth: The Computer Game". He considered the game to be Lucasfilm's best, and he recommended it to Commodore 64 and Apple II users unable to run titles with better visuals, such as those from Sierra On-Line. A writer for "ACE" enjoyed the game's animation and depth, but he noted that fans of text-based adventures would dislike the game's simplicity.
Ports.
Reviewing the MS-DOS and Atari ST ports, a critic from "The Games Machine" called "Maniac Mansion" "an enjoyable romp" that was structurally superior to later LucasArts adventure games. However, the writer noticed poor pathfinding and disliked the limited audio. Reviewers for "The Deseret News" lauded the audiovisuals and considered the product "wonderful fun". "Computer Gaming World"s Charles Ardai praised the game for attaining "the necessary and precarious balance between laughs and suspense that so many comic horror films and novels lack". Although he faulted the control system's limited options, he hailed it as "one of the most comfortable ever devised". Writing for "VideoGames & Computer Entertainment", Bill Kunkel and Joyce Worley stated that the game's plot and premise were typical of the horror genre; but they praised the interface and execution.
Reviewing "Maniac Mansion"s Amiga version four years after its release, Simon Byron of "The One Amiga" praised the game for retaining "charm and humour", but suggested that its art direction had become "tacky" compared to more recent titles. Stephen Bradly of "Amiga Format" found the game derivative, but he encountered "loads of visual humour" in it; and he added, "Strangely, it's quite compelling after a while." Michael Labiner of Germany's "Amiga Joker" considered "Maniac Mansion" to be one of the best adventure games for the system. He noted minor graphical flaws, such as a limited color palette, but he argued that the gameplay made up for such shortcomings. Writing for "Datormagazin" in Sweden, Ingela Palmér commented that the Amiga and Commodore 64 versions of "Maniac Mansion" were nearly identical. She criticized the graphics and gameplay of both releases, but felt the game to be highly enjoyable regardless.
Reviewing the NES release, British magazine "Mean Machines" commended the game's presentation, playability and replay value. However, the publication noted undetailed graphics and "ear-bashing tunes". The magazine's Julian Rignall compared "Maniac Mansion" to the title "Shadowgate", but he preferred the former's controls and lack of "death-without-warning situations". Writers for Germany's "Video Games" referred to the NES version as a "classic". Co-reviewer Heinrich Lenhardt stated that "Maniac Mansion" was unlike any other NES adventure game, and that it was no less enjoyable than its home computer releases. Co-reviewer Winnie Forster found it to be "one of the most original representatives of the game genre". In retrospective features, "Edge" magazine called the NES version "somewhat neutered" and "GamesTM" referred to it as "infamous" and "heavily censored".
TV adaptation and game sequel.
Lucasfilm conceived the idea for a television adaptation of "Maniac Mansion", the rights to which were purchased by The Family Channel in 1990. The two companies collaborated with Atlantis Films to produce a sitcom named after the game, which debuted in September of that year. It aired on YTV in Canada and The Family Channel in the United States. Based in part on the video game, the series focuses on the Edison family's life and stars Joe Flaherty as Dr. Fred. Its writing staff was led by Eugene Levy. Gilbert later said that the premise of the series changed during production, until it differed heavily from the game's original plot. Upon its debut, the adaptation received positive reviews from "Variety", "Entertainment Weekly" and the "Los Angeles Times". "Time" named it one of the year's best new series. However, Ken Tucker of "Entertainment Weekly" questioned the decision to air the series on The Family Channel, given Flaherty's subversive humor. Discussing the series in retrospect, Richard Cobbett of "PC Gamer" criticized its generic storylines and lack of relevance to the game. The series lasted for three seasons; sixty-six episodes were filmed.
In the early 1990s, LucasArts tasked Dave Grossman and Tim Schafer, both of whom had worked on the "Monkey Island" series, with designing a sequel to "Maniac Mansion". Gilbert and Winnick initially assisted with the project's writing. The team included voice acting and more detailed graphics, which Gilbert had originally envisioned for "Maniac Mansion". The first game's nonlinear design was discarded, and the team implemented a Chuck Jones-inspired visual style, alongside numerous puzzles based on time travel. Bernard and the Edison family were retained. The sequel, entitled "Day of the Tentacle", was released in 1993.
Impact and legacy.
In 2010, the staff of "GamesTM" dubbed "Maniac Mansion" a "seminal" title that overhauled the gameplay of the graphic adventure genre. Removing the need to guess syntax allowed players to concentrate on the story and puzzles, which created a smoother and more enjoyable experience, according to the magazine. Eurogamer's Kristan Reed agreed: he believed that the design was "infinitely more elegant and intuitive" than its predecessors, and that it freed players from "guessing-game frustration". Designer Dave Grossman, who worked on Lucasfilm Games' later "Day of the Tentacle" and "The Secret of Monkey Island", felt that "Maniac Mansion" had revolutionized the adventure game genre. Although 1985's "Uninvited" had featured a point-and-click interface, it was not influential. "Maniac Mansion"s implementation of the concept was widely imitated in other adventure titles. Writing in the game studies journal "Kinephanos", Jonathan Lessard argued that "Maniac Mansion" led a "Casual Revolution" in the late 1980s, which opened the adventure genre to a wider audience. Similarly, Christopher Buecheler of GameSpy called the game a contributor to its genre's subsequent critical adoration and commercial success.
Reed highlighted the "wonderfully ambitious" design of "Maniac Mansion", in reference to its writing, interface and cast of characters. Game designer Sheri Graner Ray believed the game to challenge "damsel in distress" stereotypes through its inclusion of female protagonists. Conversely, writer Mark Dery argued that the goal of rescuing a kidnapped cheerleader reinforced negative gender roles. The Lucasfilm team built on their experiences from "Maniac Mansion" and became increasingly ambitious in subsequent titles. Gilbert admitted to making mistakes—such as the inclusion of no-win situations—in "Maniac Mansion", and he applied these lessons to future projects. For example, the game relies on timers rather than events to trigger cutscenes, which occasionally results in awkward transitions: Gilbert worked to avoid this flaw with the "Monkey Island" series. Because of "Maniac Mansion"s imperfections, however, Gilbert considers it his favorite of his games.
According to writers Mike and Sandie Morrison, Lucasfilm Games became "serious competition" in the adventure genre after the release of "Maniac Mansion". The game's success solidified Lucasfilm as one of the leading producers of adventure games: authors Rusel DeMaria and Johnny Wilson described it as a "landmark title" for the company. In their view, "Maniac Mansion"—along with "Space Quest: The Sarien Encounter" and "Leisure Suit Larry in the Land of the Lounge Lizards"—inaugurated a "new era of humor-based adventure games". This belief was shared by Reed, who wrote that "Maniac Mansion" "set in motion a captivating chapter in the history of gaming" that encompassed wit, invention and style. The SCUMM engine was reused by Lucasfilm in eleven later titles; improvements were made to its code with each game. Over time, rival adventure game developers adopted this paradigm in their own software. "GamesTM" attributed the change to a desire to streamline production and create enjoyable games. Following his 1992 departure from LucasArts—a conglomeration of Lucasfilm Games, ILM and Skywalker Sound formed in 1990—Gilbert used SCUMM to create adventure games and "Backyard Sports" titles for Humongous Entertainment.
In 2011, Richard Cobbett summarized "Maniac Mansion" as "one of the most intricate and important adventure games ever made". "Retro Gamer" ranked it as one of the ten best Commodore 64 games in 2006, and IGN later named it one of the ten best LucasArts adventure game. Seven years after the NES version's debut, "Nintendo Power" named it the 61st best game ever. The publication dubbed it the 16th best NES title in 2008. The game's uniqueness and clever writing were praised by "Nintendo Power": in 2010, the magazine's Chris Hoffman stated that the game is "unlike anything else out there — a point-and-click adventure with an awesome sense of humor and multiple solutions to almost every puzzle." In its retrospective coverage, "Nintendo Power" several times noted the ability to microwave a hamster, which the staff considered to be an iconic scene. In March 2012, "Retro Gamer" listed the hamster incident as one of the "100 Classic Gaming Moments".
"Maniac Mansion" enthusiasts have drawn fan art of its characters, participated in tentacle-themed cosplay and produced a trailer for a fictitious film adaptation of the game. German fan Sascha Borisow created a fangame remake, entitled "Maniac Mansion Deluxe", with enhanced audio and visuals. He used the Adventure Game Studio engine to develop the project, which he distributed free of charge on the Internet. By the end of 2004, the remake had over 200,000 downloads. A remake with three-dimensional graphics called "Meteor Mess" was created by the German developer Vampyr Games, and, as of 2011, another group in Germany is producing one with art direction similar to that of "Day of the Tentacle". Fans have created an episodic series of games based on "Maniac Mansion" as well. Gilbert has said that he would like to see an official remake, similar in its graphics and gameplay to "The Secret of Monkey Island: Special Edition" and "Monkey Island 2 Special Edition: LeChuck's Revenge". However, he expressed doubts about its potential quality, in light of George Lucas' enhanced remakes of the original "Star Wars" trilogy.

</doc>
<doc id="19669" url="https://en.wikipedia.org/wiki?curid=19669" title="Marx Brothers">
Marx Brothers

The Marx Brothers were a family comedy act that was successful in vaudeville, on Broadway, and in motion pictures from 1905 to 1949. Five of the Marx Brothers' thirteen feature films were selected by the American Film Institute (AFI) as among the top 100 comedy films, with two of them ("Duck Soup" and "A Night at the Opera") in the top twelve. The brothers were included in AFI's 100 Years...100 Stars list of the 25 greatest male stars of Classic Hollywood cinema, the only performers to be inducted collectively.
The group are almost universally known today by their stage names: Chico, Harpo, Groucho, Gummo, and Zeppo Marx. The core of the act was the three elder brothers: Chico, Harpo, and Groucho. Each developed a highly distinctive stage persona.
Harpo and Chico "more or less retired" after 1949, while Groucho went on to begin a second career in television. The two younger brothers Gummo and Zeppo did not develop their stage characters to the same extent. The two eventually left the act to pursue business careers at which they were successful, as well as a large theatrical agency for a time, through which they represented their brothers and others. Gummo was not in any of the movies; Zeppo appeared in the first five films in relatively straight (non-comedic) roles. The performing lives of the brothers were brought about by their mother Minnie Marx, who also acted as their manager.
Brothers' names, family background, and lifetimes.
The Marx Brothers were five brothers born to U.S. immigrants Miene "Minnie" Schoenberg (professionally known as Minnie Palmer, who acted as their manager) and Samuel (born "Simon", nicknamed "Frenchy") Marx. The brothers are best known by their stage names:
A sixth brother Manfred ("Mannie") was actually the first son of Sam and Minnie, who was born in 1886 and died in infancy, though an online family tree states that he was born in 1885: 
Minnie Marx came from a family of performers. Her mother was a yodeling harpist and her father a ventriloquist; both were funfair entertainers. Around 1880, the family emigrated to New York City, where Minnie married Sam in 1884. During the early 20th century, Minnie helped her younger brother Abraham Elieser Adolf (stage name Al Shean) to enter show business; he became highly successful on vaudeville and Broadway as half of the musical comedy double act Gallagher and Shean, and this gave the brothers an entree to musical comedy, vaudeville,and Broadway at Minnie's instigation. Minnie also acted as the brothers' manager, using the name Minnie Palmer so that agents would not realize that she was also their mother. All the brothers confirmed that Minnie Marx had been the head of the family and the driving force in getting the troupe launched, the only person who could keep them in order; she was said to be a hard bargainer with theatre management.
Of the five brothers, the three that were most commonly associated with the stage title "the Marx Brothers"—Harpo, Chico, and Groucho—remained lifelong performers. Harpo and Chico "more or less retired" after 1949, while Groucho began a second career and became a well-known television host. Gummo and Zeppo both left performing to run a large theatrical agency together, through which they represented their brothers as well as others at times. Both also became well-respected businessmen. Gummo gained success through his agency activities and a raincoat business, and Zeppo became a multi-millionaire through his engineering business.
Early life.
The Marx Brothers were born in New York City, the sons of Jewish immigrants from Germany and France. Their mother Minnie Schönberg was from Dornum in East Frisia, and their father Simon Marx was a native of Alsace and worked as a tailor. (His name was changed to Samuel Marx, and he was nicknamed "Frenchy".) The family lived in the poor Yorkville section of New York City's Upper East Side, centered in the Irish, German, and Italian quarters.
Stage beginnings.
The brothers were from a family of artists, and their musical talent was encouraged from an early age. Harpo was particularly talented, learning to play an estimated six different instruments throughout his career. He became a dedicated harpist, which gave him his nickname. Chico was an excellent pianist, Groucho a guitarist and singer, and Zeppo a vocalist.
They got their start in vaudeville, where their uncle Albert Schönberg performed as Al Shean of Gallagher and Shean. Groucho's debut was in 1905, mainly as a singer. By 1907, he and Gummo were singing together as "The Three Nightingales" with Mabel O'Donnell. The next year, Harpo became the fourth Nightingale and by 1910, the group briefly expanded to include their mother Minnie and their Aunt Hannah. The troupe was renamed "The Six Mascots".
Comedy.
One evening in 1912, a performance at the Opera House in Nacogdoches, Texas was interrupted by shouts from outside about a runaway mule. The audience hurried out to see what was happening. Groucho was angered by the interruption and, when the audience returned, he made snide comments at their expense, including "Nacogdoches is full of roaches" and "the jackass is the flower of Tex-ass". Instead of becoming angry, the audience laughed. The family then realized that it had potential as a comic troupe. (However, in his autobiography "Harpo Speaks", Harpo Marx states that the runaway mule incident occurred in Ada, Oklahoma. A 1930 article in the "San Antonio Express" newspaper states that the incident took place in Marshall, Texas.)
The act slowly evolved from singing with comedy to comedy with music. The brothers' sketch "Fun in Hi Skule" featured Groucho as a German-accented teacher presiding over a classroom that included students Harpo, Gummo, and Chico. The last version of the school act was titled "Home Again" and was written by their uncle Al Shean. The "Home Again" tour reached Flint, Michigan in 1915, where 14-year-old Zeppo joined his four brothers for what is believed to be the only time that all five Marx Brothers appeared together on stage. Then Gummo left to serve in World War I, reasoning that "anything is better than being an actor!" Zeppo replaced him in their final vaudeville years and in the jump to Broadway, and then to Paramount films.
During World War I, anti-German sentiments were common, and the family tried to conceal its German origin. Mother Minnie learned that farmers were excluded from the draft rolls, so she purchased a poultry farm near Countryside, Illinois — but the brothers soon found that chicken ranching was not in their blood. During this time, Groucho discontinued his "German" stage personality.
By this time, "The Four Marx Brothers" had begun to incorporate their unique style of comedy into their act and to develop their characters. Both Groucho's and Harpo's memoirs say that their now-famous on-stage personae were created by Al Shean. Groucho began to wear his trademark greasepaint mustache and to use a stooped walk. Harpo stopped speaking onstage and began to wear a red fright wig and carry a taxi-cab horn. Chico spoke with a fake Italian accent, developed off-stage to deal with neighborhood toughs, while Zeppo adopted the role of the romantic (and "peerlessly cheesy", according to James Agee) straight man.
The on-stage personalities of Groucho, Chico, and Harpo were said to have been based on their actual traits. Zeppo, on the other hand, was considered the funniest brother offstage, despite his straight stage roles. He was the youngest and had grown up watching his brothers, so he could fill in for and imitate any of the others when illness kept them from performing. "He was so good as Captain Spaulding "Animal Crackers" that I would have let him play the part indefinitely, if they had allowed me to smoke in the audience", Groucho recalled. (Zeppo did impersonate Groucho in the film version of "Animal Crackers". Groucho was unavailable to film the scene in which the Beaugard painting is stolen, so the script was contrived to include a power failure, which allowed Zeppo to play the Spaulding part in near-darkness.) In December 1917 the Marx brothers were noted in a advertisement playing in a musical comedy act "Home Again".
By the 1920s, the Marx Brothers had become one of America's favorite theatrical acts, with their sharp and bizarre sense of humor. They satirized high society and human hypocrisy, and they became famous for their improvisational comedy in free-form scenarios. A famous early instance was when Harpo arranged to chase a fleeing chorus girl across the stage during the middle of a Groucho monologue to see if Groucho would be thrown off. However, to the audience's delight, Groucho merely reacted by commenting, "First time I ever saw a taxi hail a passenger". When Harpo chased the girl back in the other direction, Groucho calmly checked his watch and ad-libbed, "The 9:20's right on time. You can set your watch by the Lehigh Valley."
The brothers' vaudeville act had made them stars on Broadway under Chico's management and with Groucho's creative direction — first with the musical revue "I'll Say She Is" (1924–1925) and then with two musical comedies: "The Cocoanuts" (1925–1926) and "Animal Crackers" (1928–1929). Playwright George S. Kaufman worked on the last two and helped sharpen the brothers' characterizations.
Out of their distinctive costumes, the brothers looked alike, even down to their receding hairlines. Zeppo could pass for a younger Groucho, and played the role of his son in "Horse Feathers". A scene in "Duck Soup" finds Groucho, Harpo, and Chico all appearing in the famous greasepaint eyebrows, mustache, and round glasses while wearing nightcaps. The three are indistinguishable, enabling them to carry off the "mirror scene" perfectly.
Origin of the stage names.
The stage names of the brothers (except Zeppo) were coined by monologist Art Fisher during a poker game in Galesburg, Illinois, based both on the brothers' personalities and Gus Mager's "Sherlocko the Monk", a popular comic strip of the day which included a supporting character named "Groucho". As Fisher dealt each brother a card, he addressed him, for the very first time, by the names they would keep for the rest of their lives.
The reasons behind Chico's and Harpo's stage names are undisputed, and Gummo's is fairly well established. Groucho's and Zeppo's are far less clear. Arthur was named Harpo because he played the harp, and Leonard became Chico (pronounced "Chick-o") because he was, in the slang of the period, a "chicken chaser". ("Chickens"—later "chicks"—was period slang for women. "In England now," said Groucho, "they were called 'birds'.")
In his autobiography, Harpo explains that Milton became Gummo because he crept about the theater like a gumshoe detective. Other sources report that Gummo was the family's hypochondriac, having been the sickliest of the brothers in childhood, and therefore wore rubber overshoes, called gumshoes, in all kinds of weather. Still others report that Milton was the troupe's best dancer, and dance shoes tended to have rubber soles. Groucho stated that the source of the name was Gummo wearing galoshes. Whatever the details, the name relates to rubber-soled shoes.
The reason that Julius was named Groucho is perhaps the most disputed. There are three explanations:
I kept my money in a 'grouch bag'. This was a small chamois bag that actors used to wear around their neck to keep other hungry actors from pinching their dough. Naturally, you're going to think that's where I got my name from. But that's not so. Grouch bags were worn on manly chests long before there was a Groucho.
Herbert was not nicknamed by Art Fisher, since he did not join the act until Gummo had departed. As with Groucho, three explanations exist for Herbert's name "Zeppo":
Maxine Marx reported in "The Unknown Marx Brothers" that the brothers listed their "real" names (Julius, Leonard, Adolph, Milton, and Herbert) on playbills and in programs, and only used the nicknames behind the scenes, until Alexander Woollcott overheard them calling one another by the nicknames. He asked them why they used their real names publicly when they had such wonderful nicknames, and they replied, "That wouldn't be dignified." Woollcott answered with a belly laugh. Woollcott did not meet the Marx Brothers until the premiere of "I'll Say She Is," which was their first Broadway show, so this would mean that they used their real names throughout their vaudeville days, and that the name "Gummo" never appeared in print during his time in the act. Other sources report that the Marx Brothers did go by their nicknames during their vaudeville era, but briefly listed themselves by their given names when "I'll Say She Is" opened because they were worried that a Broadway audience would reject a vaudeville act if they were perceived as low class.
Motion pictures.
Paramount.
The Marx Brothers' stage shows became popular just as motion pictures were evolving to "talkies". They signed a contract with Paramount Pictures and embarked on their film career at Paramount's Astoria, New York, studios. Their first two released films (after an unreleased short silent film titled "Humor Risk") were adaptations of the Broadway shows "The Cocoanuts" (1929) and "Animal Crackers" (1930). Both were written by George S. Kaufman and Morrie Ryskind. Production then shifted to Hollywood, beginning with a short film that was included in Paramount's twentieth anniversary documentary, "The House That Shadows Built" (1931), in which they adapted a scene from "I'll Say She Is". Their third feature-length film, "Monkey Business" (1931), was their first movie not based on a stage production.
"Horse Feathers" (1932), in which the brothers satirized the American college system and Prohibition, was their most popular film yet, and won them the cover of "Time". It included a running gag from their stage work, in which Harpo produces a ludicrous array of props from his coat, including a wooden mallet, a fish, a coiled rope, a tie, a poster of a woman in her underwear, a cup of hot coffee, a sword; and, just after Groucho warns him that he "can't burn the candle at both ends," a candle burning at both ends.
During this period Chico and Groucho starred in a radio comedy series, "Flywheel, Shyster and Flywheel". Though the series was short lived, much of the material developed for it was used in subsequent films. The show's scripts and recordings were believed lost until copies of the scripts were found in the Library of Congress in the 1980s. After publication in a book they were performed with Marx Brothers impersonators for BBC Radio.
Their last Paramount film, "Duck Soup" (1933), directed by the highly regarded Leo McCarey, is the highest rated of the five Marx Brothers films on the American Film Institute's "100 years ... 100 Movies" list. It did not do as well financially as "Horse Feathers", but was the sixth-highest grosser of 1933. The film sparked a dispute between the Marxes and the village of Fredonia, New York. "Freedonia" was the name of a fictional country in the script, and the city fathers wrote to Paramount and asked the studio to remove all references to Freedonia because "it is hurting our town's image". Groucho fired back a sarcastic retort asking them to change the name of their town, because "it's hurting our picture."
MGM, RKO, and United Artists.
After expiration of the Paramount contract Zeppo left the act to become an agent. He and brother Gummo went on to build one of the biggest talent agencies in Hollywood, helping the likes of Jack Benny and Lana Turner get their starts. Groucho and Chico did radio, and there was talk of returning to Broadway. At a bridge game with Chico, Irving Thalberg began discussing the possibility of the Marxes joining Metro-Goldwyn-Mayer. They signed, now billed as "Groucho, Chico, Harpo, Marx Bros."
Unlike the free-for-all scripts at Paramount, Thalberg insisted on a strong story structure that made the brothers more sympathetic characters, interweaving their comedy with romantic plots and non-comic musical numbers, and targeting their mischief-making at obvious villains. Thalberg was adamant that scripts include a "low point", where all seems lost for both the Marxes and the romantic leads. He instituted the innovation of testing the film's script before live audiences before filming began, to perfect the comic timing, and to retain jokes that earned laughs and replace those that did not. Thalberg restored Harpo's harp solos and Chico's piano solos, which had been omitted from "Duck Soup".
The first Marx Brothers/Thalberg film was "A Night at the Opera" (1935), a satire on the world of opera, where the brothers help two young singers in love by throwing a production of "Il Trovatore" into chaos. The film—including its famous scene where an absurd number of people crowd into a tiny stateroom on a ship—was a great success, and was followed two years later by an even bigger hit, "A Day at the Races" (1937), in which the brothers cause mayhem in a sanitarium and at a horse race. The film features Groucho and Chico's famous "Tootsie Frootsie Ice Cream" sketch. In a 1969 interview with Dick Cavett, Groucho said that the two movies made with Thalberg were the best that they ever produced. Despite the Thalberg films' success, the brothers left MGM in 1937; Thalberg had died suddenly on September 14, 1936, two weeks after filming began on "A Day at the Races", leaving the Marxes without an advocate at the studio.
After a short experience at RKO ("Room Service", 1938), the Marx Brothers returned to MGM and made three more films: "At the Circus" (1939), "Go West" (1940) and "The Big Store" (1941). Prior to the release of "The Big Store" the team announced they were retiring from the screen. Four years later, however, Chico persuaded his brothers to make two additional films, "A Night in Casablanca" (1946) and "Love Happy" (1949), to alleviate his severe gambling debts. Both pictures were released by United Artists.
Later years.
From the 1940s onward Chico and Harpo appeared separately and together in nightclubs and casinos. Chico fronted a big band, the Chico Marx Orchestra (with 17-year-old Mel Tormé as a vocalist). Groucho made several radio appearances during the 1940s and starred in "You Bet Your Life", which ran from 1947 to 1961 on NBC radio and television. He authored several books, including "Groucho and Me" (1959), "Memoirs of a Mangy Lover" (1964) and "The Groucho Letters" (1967).
Groucho and Chico briefly appeared together in a 1957 short film promoting the "Saturday Evening Post" entitled "Showdown at Ulcer Gulch," directed by animator Shamus Culhane, Chico's son-in-law. Groucho, Chico, and Harpo worked together (in separate scenes) in "The Story of Mankind" (1957). In 1959, the three began production of "Deputy Seraph," a TV series starring Harpo and Chico as blundering angels, and Groucho (in every third episode) as their boss, the "Deputy Seraph." The project was abandoned when Chico was found to be uninsurable (and incapable of memorizing his lines) due to severe arteriosclerosis. On March 8 of that year, Chico and Harpo starred as bumbling thieves in "The Incredible Jewel Robbery", a half-hour pantomimed episode of the "General Electric Theater" on CBS. Groucho made a cameo appearance—uncredited, because of constraints in his NBC contract—in the last scene, and delivered the only line of dialogue ("We won't talk until we see our lawyer!").
According to a September 1947 article in "Newsweek", Groucho, Harpo, Chico and Zeppo all signed to appear as themselves in a biopic entitled "The Life and Times of the Marx Brothers". In addition to being a non-fiction biography of the Marxes, the film would have featured the brothers reenacting much of their previously unfilmed material from both their vaudeville and Broadway eras. The film, had it been made, would have been the first performance by the Brothers as a quartet since 1933.
The five brothers made only one television appearance together, in 1957, on an early incarnation of "The Tonight Show" called "Tonight! America After Dark", hosted by Jack Lescoulie. Five years later (October 1, 1962) after Jack Paar's tenure, Groucho made a guest appearance to introduce the "Tonight Show's" new host, Johnny Carson.
Around 1960, the acclaimed director Billy Wilder considered writing and directing a new Marx Brothers film. Tentatively titled "A Day at the U.N.", it was to be a comedy of international intrigue set around the United Nations building in New York. Wilder had discussions with Groucho and Gummo, but the project was put on hold because of Harpo's ill-health and abandoned when Chico died in 1961. He was 74. Three years later, on September 28, 1964, Harpo died at the age of 75 of a heart attack one day after heart surgery.
In 1966 Filmation produced a pilot for a Marx Brothers cartoon. Groucho's voice was supplied by Pat Harrington Jr. and other voices were done by Ted Knight and Joe Besser.
In 1970, the four Marx Brothers had a brief reunion of sorts in the animated ABC television special "The Mad, Mad, Mad Comedians", produced by Rankin-Bass animation (of "Rudolph the Red-Nosed Reindeer" fame). The special featured animated reworkings of various famous comedians' acts, including W. C. Fields, Jack Benny, George Burns, Henny Youngman, the Smothers Brothers, Flip Wilson, Phyllis Diller, Jack E. Leonard, George Jessel and the Marx Brothers. Most of the comedians provided their own voices for their animated counterparts, except for Fields and Chico Marx (both had died), and Zeppo Marx (who had left show business in 1933). Voice actor Paul Frees filled in for all three (no voice was needed for Harpo). The Marx Brothers' segment was a reworking of a scene from their Broadway play "I'll Say She Is", a parody of Napoleon which Groucho considered among the brothers' funniest routines. The sketch featured animated representations, if not the voices, of all four brothers. Romeo Muller is credited as having written special material for the show, but the script for the classic "Napoleon Scene" was probably supplied by Groucho.
Impact on modern entertainment.
On January 16, 1977, the Marx Brothers were inducted into the Motion Picture Hall of Fame. With the deaths of Gummo in April 1977, Groucho in August 1977, and Zeppo in November 1979, the brothers were gone. But their impact on the entertainment community continues well into the 21st century.
Many television shows and movies have used Marx Brothers references. "Animaniacs" and "Tiny Toons", for example, have featured Marx Brothers jokes and skits. Hawkeye Pierce (Alan Alda) on "M*A*S*H" occasionally put on a fake nose and glasses, and, holding a cigar, did a Groucho impersonation to amuse patients recovering from surgery. Early episodes also featured a singing and off-scene character named Captain Spaulding as a tribute.
Bugs Bunny impersonated Groucho Marx in the 1947 cartoon "Slick Hare" and in a later cartoon he again impersonated Groucho hosting a TV show called "You Beat Your Wife," asking Elmer Fudd if he had stopped beating his wife. Tex Avery's cartoon "Hollywood Steps Out" (1941) featured appearances by Harpo and Groucho. They appeared, sometimes with Chico and Zeppo caricatured, in cartoons starring Mickey Mouse, Flip the Frog and others. In the "Airwolf" episode 'Condemned', four anti-virus formulae for a deadly plague were named after the four Marx Brothers.
In "All In The Family", Rob Reiner often did imitations of Groucho, and Sally Struthers dressed as Harpo in one episode in which she (as Gloria Stivic) and Rob (as Mike Stivic) were going to a Marx Brothers film festival, with Reiner dressing as Groucho. Gabe Kaplan did many Groucho imitations on his sit-com "Welcome Back, Kotter" and Robert Hegyes sometimes imitated both Chico and Harpo on the show. In Woody Allen's film "Hannah and Her Sisters" (1986), Woody's character, after an unsuccessful suicide attempt, is inspired to go on living after seeing a revival showing of "Duck Soup". In "Manhattan" (1979), he names the Marx Brothers as something that makes life worth living.
In "Everyone Says I Love You" (1996), he and Goldie Hawn dress as Groucho for a Marx Brothers celebration in France, and the song "Hooray for Captain Spaulding", from "Animal Crackers", is performed, with various actors dressed as the brothers, striking poses famous to Marx fans. (The film itself is named after a song from "Horse Feathers", a version of which plays over the opening credits.)
Harpo Marx appeared as himself in a sketch on "I Love Lucy" in which he and Lucille Ball reprised the mirror routine from "Duck Soup", with Lucy dressed up as Harpo. Lucy had worked with the Marxes when she appeared in a supporting role in an earlier Marx Brothers film, "Room Service". Chico once appeared on "I've Got a Secret" dressed up as Harpo; his secret was shown in a caption reading, "I'm pretending to be Harpo Marx (I'm Chico)". The Marx Brothers were spoofed in the second act of the Broadway Review "A Day in Hollywood/A Night in the Ukraine".
Filmography.
Films with the four Marx Brothers:
Films with the three Marx Brothers (post-Zeppo):
Solo endeavors:
Legacy.
Awards and honors.
The Marx Brothers were collectively named #20 on AFI's list of the Top 25 American male screen legends of Classic Hollywood. They are the only group to be so honored.
The "Sweathogs" of the ABC-TV series ""Welcome Back Kotter"" (John Travolta, Robert Hegyes, Lawrence Hilton-Jacobs, and Ron Palillo) patterned much of their on-camera banter in that series after the Marx Brothers. Series star Gabe Kaplan was reputedly a big Marx Brothers fan.

</doc>
<doc id="19672" url="https://en.wikipedia.org/wiki?curid=19672" title="May 28">
May 28


</doc>
<doc id="19673" url="https://en.wikipedia.org/wiki?curid=19673" title="MP3">
MP3

MPEG-1 or MPEG-2 Audio Layer III, more commonly referred to as MP3, is an audio coding format for digital audio which uses a form of lossy data compression. It is a common audio format for consumer audio streaming or storage, as well as a de facto standard of digital audio compression for the transfer and playback of music on most digital audio players.
The use of lossy compression is designed to greatly reduce the amount of data required to represent the audio recording and still sound like a faithful reproduction of the original uncompressed audio for most listeners. An MP3 file that is created using the setting of 128 kbit/s will result in a file that is about 1/11 the size of a CD-quality file. An MP3 file can also be constructed at higher or lower bit rates, with higher or lower resulting quality.
The compression works by reducing the accuracy of certain parts of a sound that are considered to be beyond the auditory resolution ability of most people. This method is commonly referred to as perceptual coding. It uses psychoacoustic models to discard or reduce precision of components less audible to human hearing, and then records the remaining information in an efficient manner.
MP3 was designed by the Moving Picture Experts Group (MPEG) as part of its MPEG-1 standard and later extended in the MPEG-2 standard. The first subgroup for audio was formed by several teams of engineers at Fraunhofer IIS, University of Hanover, AT&T-Bell Labs, Thomson-Brandt, CCETT, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III was approved as a committee draft of ISO/IEC standard in 1991, finalised in 1992 and published in 1993 (ISO/IEC 11172-3:1993). Backwards compatible MPEG-2 Audio (MPEG-2 Part 3) with additional bit rates and sample rates was published in 1995 (ISO/IEC 13818-3:1995).
History.
Development.
The MP3 lossy audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described a complete set of auditory curves regarding this phenomenon. Ernst Terhardt "et al." created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.
The psychoacoustic masking codec was first proposed in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, NJ, and M. A. Krasner both in the United States. Krasner was the first to publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the linear predictive coding (LPC) gain present in speech. Both Krasner and Schroeder built upon the work performed by Eberhard F. Zwicker in the areas of tuning and masking of critical bands, that in turn built on the fundamental research in the area from Bell Labs of Harvey Fletcher and his collaborators. A wide variety of (mostly perceptual) audio compression algorithms were reported in IEEE's refereed Journal on Selected Areas in Communications. That journal reported in February 1988 on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations.
The immediate predecessors of MP3 were "Optimum Coding in the Frequency Domain" (OCF), and Perceptual Transform Coding (PXFM). These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.
As a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music. He completed his doctoral work in 1989. MP3 is directly descended from OCF and PXFM, representing the outcome of the collaboration of Brandenburg—working as a postdoc at AT&T-Bell Labs with James D. Johnston ("JJ") of AT&T-Bell Labs—with the Fraunhofer Institute for Integrated Circuits, Erlangen, with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders. In 1990, Brandenburg became an assistant professor at Erlangen-Nuremberg. While there, he continued to work on music compression with scientists at the Fraunhofer Society (in 1993 he joined the staff of the Fraunhofer Institute).
The song "Tom's Diner" by Suzanne Vega was the first song used by Karlheinz Brandenburg to develop the MP3. Brandenburg adopted the song for testing purposes, listening to it again and again each time refining the scheme, making sure it did not adversely affect the subtlety of Vega's voice.
Standardization.
In 1991, there were two available proposals that were assessed for an MPEG audio standard: Musicam (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual Entropy Coding). As proposed by the Dutch corporation Philips, the French research institute CCETT, and the German standards organization Institute for Broadcast Technology, the Musicam technique was chosen due to its simplicity and error robustness, as well as for its high level of computational efficiency. The Musicam format, based on sub-band coding, became the basis for the MPEG Audio compression format, incorporating, for example, its frame structure, header format, sample rates, etc.
While much of Musicam's technology and ideas were incorporated into the definition of MPEG Audio Layer I and Layer II, only the filter bank alone would remain in the Layer III (MP3) format, as part of the computationally inefficient hybrid filter bank. Under the chairmanship of Professor Musmann of the University of Hanover, the editing of the standard was delegated to Dutchman Leon van de Kerkhof and to German Gerhard Stoll, who worked on Layer I and Layer II respectively.
ASPEC was the joint proposal of AT&T Bell Laboratories, Thomson Consumer Electronics, Fraunhofer Society and CNET. It provided the highest coding efficiency.
A working group consisting of van de Kerkhof, Stoll, Italian Leonardo Chiariglione, Frenchman Yves-François Dehery, German Karlheinz Brandenburg, and American James D. Johnston (USA) took ideas from ASPEC, integrated the filter bank from Layer II, added some of their own ideas and created the MP3 format, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s.
The algorithms for MPEG-1 Audio Layer I, II and III were approved in 1991 and finalized in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3 (a.k.a. "MPEG-1 Audio" or "MPEG-1 Part 3"), published in 1993.
Further work on MPEG audio was finalized in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3 (a.k.a. "MPEG-2 Part 3" or backwards compatible "MPEG-2 Audio" or "MPEG-2 Audio BC"), originally published in 1995. MPEG-2 Part 3 (ISO/IEC 13818-3) defined additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III. The new sampling rates are exactly half that of those originally defined in MPEG-1 Audio. This reduction in sampling rate serves to cut the available frequency fidelity in half while likewise cutting the bitrate by 50%.
MPEG-2 Part 3 also enhanced MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel.
An additional extension to "MPEG-2" is named "MPEG-2.5" audio, as MPEG-3 already had a different meaning. This extension was developed at Fraunhofer IIS, the registered MP3 patent holders. Like MPEG-2, MPEG-2.5 adds new sampling rates exactly half of that previously possible with MPEG-2. It thus widens the scope of MP3 to include human speech and other applications requiring only 25% of the frequency reproduction possible with MPEG-1. While not an ISO recognized standard, MPEG-2.5 is widely supported by both inexpensive and brand name digital audio players as well as computer software based MP3 encoders and decoders.
A sample rate comparison between MPEG-1, 2 and 2.5 is given later in the article. MPEG-2.5 was not developed by MPEG and was never approved as an international standard. MPEG-2.5 is thus an unofficial or proprietary extension to the MP3 format.
The ISO standard ISO/IEC 11172-3 (a.k.a. MPEG-1 Audio) defined three formats: the MPEG-1 Audio Layer I, Layer II and Layer III. The ISO standard ISO/IEC 13818-3 (a.k.a. MPEG-2 Audio) defined extended version of the MPEG-1 Audio: MPEG-2 Audio Layer I, Layer II and Layer III. MPEG-2 Audio (MPEG-2 Part 3) should not be confused with MPEG-2 AAC (MPEG-2 Part 7 – ISO/IEC 13818-7).
Compression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampling rate of the input signal. Nevertheless, compression ratios are often published. They may use the Compact Disc (CD) parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit). Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term "compression ratio" for lossy encoders.
Karlheinz Brandenburg used a CD recording of Suzanne Vega's song "Tom's Diner" to assess and refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as "The mother of MP3". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of noise artifacts unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats.
Going public.
A reference simulation software implementation, written in the C language and later known as "ISO 11172-5", was developed (in 1991–1996) by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, Layer 3). It was approved as a committee draft of ISO/IEC technical report in March 1994 and printed as document CD 11172-5 in April 1994. It was approved as a draft technical report (DTR/DIS) in November 1994, finalized in 1996 and published as international standard ISO/IEC TR 11172-5:1998 in 1998.
Internet distribution.
In the second half of '90s, MP3 files began to spread on the Internet. The popularity of MP3s began to rise rapidly with the advent of Nullsoft's audio player Winamp, released in 1997. In 1998, the first portable solid state digital audio player MPMan, developed by SaeHan Information Systems which is headquartered in Seoul, South Korea, was released and the Rio PMP300 was sold afterwards in 1998, despite legal suppression efforts by the RIAA.
In November 1997, the website mp3.com was offering thousands of MP3s created by independent artists for free. The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from CDs, which would have previously been nearly impossible. The first large peer-to-peer filesharing network, Napster, was launched in 1999.
The ease of creating and sharing MP3s resulted in widespread copyright infringement. Major record companies argued that this free sharing of music reduced sales, and called it "music piracy". They reacted by pursuing lawsuits against Napster (which was eventually shut down and later sold) and against individual users who engaged in file sharing.
Unauthorized MP3 file sharing continues on next-generation peer-to-peer networks. Some authorized services, such as Beatport, Bleep, Juno Records, eMusic, Zune Marketplace, Walmart.com, Rhapsody, the recording industry approved re-incarnation of Napster, and Amazon.com sell unrestricted music in the MP3 format.
Design.
File structure.
An MP3 file is made up of MP3 frames, which consist of a header and a data block. This sequence of frames is called an elementary stream. Due to the "byte reservoir", frames are not independent items and cannot usually be extracted on arbitrary frame boundaries. The MP3 Data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. The diagram shows that the MP3 Header consists of a sync word, which is used to identify the beginning of a valid frame. This is followed by a bit indicating that this is the MPEG standard and two bits that indicate that layer 3 is used; hence MPEG-1 Audio Layer 3 or MP3. After this, the values will differ, depending on the MP3 file. "ISO/IEC 11172-3" defines the range of values for each section of the header along with the specification of the header. Most MP3 files today contain ID3 metadata, which precedes or follows the MP3 frames, as noted in the diagram.
The data stream can contain an optional checksum, but the checksum only protects the header data, not the audio data.
Joint stereo is done only on a frame-to-frame basis.
Encoding and decoding.
The MPEG-1 standard does not include a precise specification for an MP3 encoder, but does provide example psychoacoustic models, rate loop, and the like in the non-normative part of the original standard. At present, these suggested implementations are quite dated. Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information from the audio input. As a result, there are many different MP3 encoders available, each producing files of differing quality. Comparisons are widely available, so it is easy for a prospective user of an encoder to research the best choice. An encoder that is proficient at encoding at higher bit rates (such as LAME) is not necessarily as good at lower bit rates.
During encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples. If there is a transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of quantization noise accompanying the transient. (See psychoacoustics.)
Frequency resolution is limited by the small long block window size, which decreases coding efficiency.
Time resolution can be too low for highly transient signals and may cause smearing of percussive sounds.
Due to the tree structure of the filter bank, pre-echo problems are made worse, as the combined impulse response of the two filter banks does not, and cannot, provide an optimum solution in time/frequency resolution. Additionally, the combining of the two filter banks' outputs creates aliasing problems that must be handled partially by the "aliasing compensation" stage; however, that creates excess energy to be coded in the frequency domain, thereby decreasing coding efficiency.
Decoding, on the other hand, is carefully defined in the standard. Most decoders are "bitstream compliant", which means that the decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process).
Encoder / decoder overall delay is not defined, which means there is no official provision for gapless playback. However, some encoders such as LAME can attach additional metadata that will allow players that can handle it to deliver seamless playback.
Quality.
When performing lossy audio encoding, such as creating an MP3 file, there is a trade-off between the amount of space used and the sound quality of the result. Typically, the creator is allowed to set a bit rate, which specifies how many kilobits the file may use per second of audio. The higher the bit rate, the larger the compressed file will be, and, generally, the closer it will sound to the original file.
With too low a bit rate, compression artifacts (i.e., sounds that were not present in the original recording) may be audible in the reproduction. Some audio is hard to compress because of its randomness and sharp attacks. When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard. A sample of applause compressed with a relatively low bit rate provides a good example of compression artifacts.
Besides the bit rate of an encoded piece of audio, the quality of MP3 files also depends on the quality of the encoder itself, and the difficulty of the signal being encoded. As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders may feature quite different quality, even with identical bit rates. As an example, in a public listening test featuring two different MP3 encoders at about 128 kbit/s, one scored 3.66 on a 1–5 scale, while the other scored only 2.22.
Quality is dependent on the choice of encoder and encoding parameters.
The simplest type of MP3 file uses one bit rate for the entire file: this is known as Constant Bit Rate (CBR) encoding. Using a constant bit rate makes encoding simpler and faster. However, it is also possible to create files where the bit rate changes throughout the file. These are known as Variable Bit Rate (VBR) files. The idea behind this is that, in any piece of audio, some parts will be much easier to compress, such as silence or music containing only a few instruments, while others will be more difficult to compress. So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts. With some encoders, it is possible to specify a given quality, and the encoder will vary the bit rate accordingly. Users who know a particular "quality setting" that is transparent to their ears can use this value when encoding all of their music, and generally speaking not need to worry about performing personal listening tests on each piece of music to determine the correct bit rate.
Perceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones).
A test given to new students by Stanford University Music Professor Jonathan Berger showed that student preference for MP3-quality music has risen each year. Berger said the students seem to prefer the 'sizzle' sounds that MP3s bring to music.
An in-depth study of MP3 audio quality, sound artist and composer Ryan Maguire's project "The Ghost in the MP3" isolates the sounds lost during MP3 compression. In 2015, he released the track "moDernisT" (an anagram of "Tom's Diner"), composed exclusively from the sounds deleted during MP3 compression of the song "Tom's Diner", the track originally used in the formulation of the MP3 standard. A detailed account of the techniques used to isolate the sounds deleted during MP3 compression, along with the conceptual motivation for the project, was published in the 2014 Proceedings of the International Computer Music Conference.
Bit rate.
Several bit rates are specified in the MPEG-1 Audio Layer III standard: 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256 and 320 kbit/s, with available sampling frequencies of 32, 44.1 and 48 kHz. MPEG-2 Audio Layer III allows bit rates of 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160 kbit/s with sampling frequencies of 16, 22.05 and 24 kHz. MPEG-2.5 Audio Layer III is restricted to bit rates of 8, 16, 24, 32, 40, 48, 56 and 64 kbit/s with sampling frequencies of 8, 11.025, and 12 kHz. Because of the Nyquist–Shannon sampling theorem, frequency reproduction is always strictly less than half of the sampling frequency, and imperfect filters requires a larger margin for error (noise level versus sharpness of filter), so 8 kHz sampling rate limits the maximum frequency to 4 kHz, while 48 kHz maximum sampling rate limits an MP3 to 24 kHz sound reproduction.
A sample rate of 44.1 kHz is almost always used, because this is also used for CD audio, the main source used for creating MP3 files. A greater variety of bit rates are used on the Internet. The rate of 128 kbit/s is commonly used, at a compression ratio of 11:1, offering adequate audio quality in a relatively small space. As Internet bandwidth availability and hard drive sizes have increased, higher bit rates up to 320 kbit/s are widespread.
Uncompressed audio as stored on an audio-CD has a bit rate of 1,411.2 kbit/s, (16 bit/sample × 44100 samples/second × 2 channels / 1000 bits/kilobit), so the bitrates 128, 160 and 192 kbit/s represent compression ratios of approximately 11:1, 9:1 and 7:1 respectively.
Non-standard bit rates up to 640 kbit/s can be achieved with the LAME encoder and the freeformat option, although few MP3 players can play those files. According to the ISO standard, decoders are only required to be able to decode streams up to 320 kbit/s.
Early MPEG Layer III encoders used what is now called Constant Bit Rate (CBR). The software was only able to use a uniform bitrate on all frames in an MP3 file. Later more sophisticated MP3 encoders were able to use the bit reservoir to target an average bit rate selecting the encoding rate for each frame based on the complexity of the sound in that portion of the recording.
A more sophisticated MP3 encoder can produce variable bitrate audio. MPEG audio may use bitrate switching on a per-frame basis, but only layer III decoders must support it. VBR is used when the goal is to achieve a fixed level of quality. The final file size of a VBR encoding is less predictable than with constant bitrate. Average bitrate is VBR implemented as a compromise between the two: the bitrate is allowed to vary for more consistent quality, but is controlled to remain near an average value chosen by the user, for predictable file sizes. Although an MP3 decoder must support VBR to be standards compliant, historically some decoders have bugs with VBR decoding, particularly before VBR encoders became widespread.
Layer III audio can also use a "bit reservoir", a partially full frame's ability to hold part of the next frame's audio data, allowing temporary changes in effective bitrate, even in a constant bitrate stream. Internal handling of the bit reservoir increases encoding delay.
There is no scale factor band 21 (sfb21) for frequencies above approx 16 kHz, forcing the encoder to choose between less accurate representation in band 21 or less efficient storage in all bands below band 21, the latter resulting in wasted bitrate in VBR encoding.
Ancillary Data.
The ancillary data field can be used to store user defined data. The ancillary data is optional and the number of bits available is not explicitly given. The ancillary data is located after the Huffman code bits and ranges to where the next frame’s main_data_begin points to. mp3PRO uses ancillary data to encode their bits to improve audio quality.
Metadata.
A "tag" in an audio file is a section of the file that contains metadata such as the title, artist, album, track number or other information about the file's contents. The MP3 standards do not define tag formats for MP3 files, nor is there a standard container format that would support metadata and obviate the need for tags.
However, several "de facto" standards for tag formats exist. As of 2010, the most widespread are ID3v1 and ID3v2, and the more recently introduced APEv2. These tags are normally embedded at the beginning or end of MP3 files, separate from the actual MP3 frame data. MP3 decoders either extract information from the tags, or just treat them as ignorable, non-MP3 junk data.
Playing & editing software often contains tag editing functionality, but there are also tag editor applications dedicated to the purpose.
Aside from metadata pertaining to the audio content, tags may also be used for DRM.
ReplayGain is a standard for measuring and storing the loudness of an MP3 file (audio normalization) in its metadata tag, enabling a ReplayGain-compliant player to automatically adjust the overall playback volume for each file. MP3Gain may be used to reversibly modify files based on ReplayGain measurements so that adjusted playback can be achieved on players without ReplayGain capability.
Licensing, ownership and legislation.
The basic MP3 decoding and encoding technology is patent-free in the European Union, all patents having expired there. In the United States, the technology will be substantially patent-free on 31 December 2017 (see below). The majority of MP3 patents expired in the US between 2007 and 2015.
In the past, many organizations have claimed ownership of patents related to MP3 decoding or encoding. These claims led to a number of legal threats and actions from a variety of sources. As a result, uncertainty about which patents must be licensed in order to create MP3 products without committing patent infringement in countries that allow software patents was a common feature of the early stages of adoption of the technology.
The initial near-complete MPEG-1 standard (parts 1, 2 and 3) was publicly available on 6 December 1991 as ISO CD 11172. In most countries, patents cannot be filed after prior art has been made public, and patents expire 20 years after the initial filing date, which can be up to 12 months later for filings in other countries. As a result, patents required to implement MP3 expired in most countries by December 2012, 21 years after the publication of ISO CD 11172.
An exception is the United States, where patents filed prior to 8 June 1995 expire 17 years after the publication date of the patent, but application extensions make it possible for a patent to issue much later than normally expected (see submarine patents). The various MP3-related patents expire on dates ranging from 2007 to 2017 in the U.S. Patents filed for anything disclosed in ISO CD 11172 a year or more after its publication are questionable. If only the known MP3 patents filed by December 1992 are considered, then MP3 decoding has been patent-free in the US since 22 September 2015 when expired which had a PCT filing in October 1992. If the longest-running patent mentioned in the aforementioned references is taken as a measure, then the MP3 technology will be patent-free in the United States on 30 December 2017 when , held by the Fraunhofer-Gesellschaft and administered by Technicolor, expires.
Technicolor (formerly called Thomson Consumer Electronics) claims to control MP3 licensing of the Layer 3 patents in many countries, including the United States, Japan, Canada and EU countries. Technicolor has been actively enforcing these patents.
MP3 license revenues generated about €100 million for the Fraunhofer Society in 2005.
In September 1998, the Fraunhofer Institute sent a letter to several developers of MP3 software stating that a license was required to "distribute and/or sell decoders and/or encoders". The letter claimed that unlicensed products "infringe the patent rights of Fraunhofer and Thomson. To make, sell and/or distribute products using the Layer-3 standard and thus our patents, you need to obtain a license under these patents from us."
Sisvel S.p.A. and its U.S. subsidiary Audio MPEG, Inc. previously sued Thomson for patent infringement on MP3 technology, but those disputes were resolved in November 2005 with Sisvel granting Thomson a license to their patents. Motorola followed soon after, and signed with Sisvel to license MP3-related patents in December 2005. Except for three patents, the US patents administered by Sisvel had all expired in 2015, however (the exceptions are: , expires February 2017, , expires February 2017 and , expires 9. April 2017.
In September 2006, German officials seized MP3 players from SanDisk's booth at the IFA show in Berlin after an Italian patents firm won an injunction on behalf of Sisvel against SanDisk in a dispute over licensing rights. The injunction was later reversed by a Berlin judge, but that reversal was in turn blocked the same day by another judge from the same court, "bringing the Patent Wild West to Germany" in the words of one commentator.
In February 2007, Texas MP3 Technologies sued Apple, Samsung Electronics and Sandisk in eastern Texas federal court, claiming infringement of a portable MP3 player patent that Texas MP3 said it had been assigned. Apple, Samsung, and Sandisk all settled the claims against them in January 2009.
Alcatel-Lucent has asserted several MP3 coding and compression patents, allegedly inherited from AT&T-Bell Labs, in litigation of its own. In November 2006, before the companies' merger, Alcatel sued Microsoft for allegedly infringing seven patents. On 23 February 2007, a San Diego jury awarded Alcatel-Lucent US $1.52 billion in damages for infringement of two of them. The court subsequently tossed the award, however, finding that one patent had not been infringed and that the other was not even owned by Alcatel-Lucent; it was co-owned by AT&T and Fraunhofer, who had licensed it to Microsoft, the judge ruled. That defense judgment was upheld on appeal in 2008. See Alcatel-Lucent v. Microsoft for more information.
Alternative technologies.
Other lossy formats exist. Among these, mp3PRO, AAC, and MP2 are all members of the same technological family as MP3 and depend on roughly similar psychoacoustic models. The Fraunhofer Gesellschaft owns many of the basic patents underlying these formats as well, with others held by Alcatel-Lucent, and Thomson Consumer Electronics.
There are also open compression formats like Opus and Vorbis that are available free of charge and without any known patent restrictions. Some of the newer audio compression formats, such as AAC, WMA Pro and Vorbis, are free of some limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder.
Besides lossy compression methods, lossless formats are a significant alternative to MP3 because they provide unaltered audio content, though with an increased file size compared to lossy compression. Lossless formats include FLAC (Free Lossless Audio Codec), Apple Lossless and many others.

</doc>
<doc id="19674" url="https://en.wikipedia.org/wiki?curid=19674" title="May 15">
May 15


</doc>
<doc id="19675" url="https://en.wikipedia.org/wiki?curid=19675" title="May 13">
May 13


</doc>
<doc id="19676" url="https://en.wikipedia.org/wiki?curid=19676" title="May 14">
May 14


</doc>
<doc id="19677" url="https://en.wikipedia.org/wiki?curid=19677" title="May 20">
May 20


</doc>
<doc id="19679" url="https://en.wikipedia.org/wiki?curid=19679" title="Mary Rose">
Mary Rose

The Mary Rose was a carrack-type warship of the English Tudor navy of King Henry VIII. After serving for 33 years in several wars against France, Scotland, and Brittany and after being substantially rebuilt in 1536, she saw her last action on 1545. While leading the attack on the galleys of a French invasion fleet, she sank in the Solent, the straits north of the Isle of Wight. 
The wreck of the "Mary Rose" was rediscovered in 1971. It was salvaged in 1982 by the Mary Rose Trust, in one of the most complex and expensive projects in the history of maritime archaeology. The surviving section of the ship and thousands of recovered artefacts are of immeasurable value as a Tudor-era time capsule. The excavation and salvage of the "Mary Rose" was a milestone in the field of maritime archaeology, comparable in complexity and cost only to the raising of the Swedish 17th-century warship "Vasa" in 1961. 
The finds include weapons, sailing equipment, naval supplies and a wide array of objects used by the crew. Many of the artefacts are unique to the "Mary Rose" and have provided insights into topics ranging from naval warfare to the history of musical instruments. Since the mid-1980s, while undergoing conservation, the remains of the hull have been on display at the Portsmouth Historic Dockyard. An extensive collection of well-preserved artefacts is on display at the nearby Mary Rose Museum, built to display the reconstructed ship and its artefacts.
The "Mary Rose" was one of the largest ships in the English navy through more than three decades of intermittent war and was one of the earliest examples of a purpose-built sailing warship. She was armed with new types of heavy guns that could fire through the recently invented gun-ports. After being substantially rebuilt in 1536, she was also one of the earliest ships that could fire a broadside, although the line of battle tactics that employed it had not yet been developed. Several theories have sought to explain the demise of the "Mary Rose", based on historical records, knowledge of 16th-century shipbuilding, and modern experiments. The precise cause of her sinking is still unclear, because of conflicting testimonies and a lack of conclusive physical evidence.
Historical context.
By the late 15th century, England was a relatively insignificant state on the periphery of Europe. The great victories against France in the Hundred Years' War were in the past; only the small enclave of Calais in northern France remained as a remnant of the vast continental holdings of the English kings. The War of the Roses—the civil war between the houses of York and Lancaster—had ended with Henry VII's establishment of the House of Tudor, the new ruling dynasty of England. The ambitious naval policies of Henry V were not continued by his successors, and from 1422 to 1509 only six ships were built for the crown. The marriage alliance between Anne of Brittany and Charles VIII of France in 1491, and his successor Louis XII in 1499, left England with a weakened strategic position on its southern flank. Despite this, Henry VII managed to maintain a comparatively long period of peace and a small but powerful core of a navy.
At the onset of the early modern period, the great European powers were France, the Holy Roman Empire and Spain. All three became involved in the War of the League of Cambrai in 1508. The conflict was initially aimed at the Republic of Venice but eventually turned against France. Through the Spanish possessions in the Low Countries, England had close economic ties with the Spanish Habsburgs, and it was the young Henry VIII's ambition to repeat the glorious martial endeavours of his predecessors. In 1509, six weeks into his reign, Henry married the Spanish princess Catherine of Aragon and joined the League, intent on certifying his historical claim as king of both England and France. By 1511 Henry was part of an anti-French alliance that included Ferdinand II of Aragon, Pope Julius II and Holy Roman emperor Maximilian.
The small navy that Henry VIII inherited from his father had only two sizeable ships, the carracks "Regent" and "Sovereign". Just months after his accession, two large ships were ordered: the "Mary Rose" and the "Peter Pomegranate" (later known as the "Peter" after being rebuilt in 1536) of about 500 and 450 tons respectively. Which king ordered the building of the "Mary Rose" is unclear; although construction began during Henry VIII's reign, the plans for naval expansion could have been in the making earlier. Henry VIII oversaw the project and he ordered additional large ships to be built, most notably the "Henry Grace à Dieu" ("Henry Thanks to God"), or "Great Harry" at more than 1000 tons burthen. By the 1520s the English state had established a "de facto" permanent "Navy Royal", the organizational ancestor of the modern Royal Navy.
Construction.
The construction of the "Mary Rose" began in 1510 in Portsmouth and she was launched in July 1511. She was then towed to London and fitted with rigging and decking, and supplied with armaments. Other than the structural details needed to sail, stock and arm the "Mary Rose", she was also equipped with flags, banners and streamers (extremely elongated flags that were flown from the top of the masts) that were either painted or gilded.
Constructing a warship of the size of the "Mary Rose" was a major undertaking, requiring vast quantities of high-quality material. In the case of building a state-of-the-art warship, these materials were primarily oak. The total amount of timber needed for the construction can only be roughly calculated since only about one third of the ship still exists. One estimate for the number of trees is around 600 mostly large oaks, representing about 16 hectares (40 acres) of woodland. The huge trees that had been common in Europe and the British Isles in previous centuries were by the 16th century quite rare, which meant that timbers were brought in from all over southern England. The largest timbers used in the construction were of roughly the same size as those used in the roofs of the largest cathedrals in the high Middle Ages. An unworked hull plank would have weighed over 300 kg (660 lb), and one of the main deck beams would have weighed close to three-quarters of a tonne.
Naming.
The common explanation for the ship's name was that it was inspired by Henry VIII's favourite sister, Mary Tudor, and the rose as the emblem of the Tudors. According to historians David Childs, David Loades and Peter Marsden, no direct evidence of naming the ship after the King's sister exists. It was far more common at the time to give ships pious Christian names, a long-standing tradition in Western Europe, or to associate them with their royal patrons. Names like "Grace Dieu" (Thank God) and "Holighost" (Holy Spirit) had been common since the 15th century and other Tudor navy ships had names like the "Regent" and "Three Ostrich Feathers" (referring to the crest of the Prince of Wales). The Virgin Mary is a more likely candidate for a namesake, and she was also associated with the mystic rose. The name of the sister ship of the "Mary Rose", the "Peter Pomegranate", is believed to have been named in honour of Saint Peter, and the badge of the Queen Catharine of Aragon, a pomegranate. According to Childs, Loades and Marsden, the two ships, which were built around the same time, were named in honour of the king and queen, respectively.
Design.
The "Mary Rose" was substantially rebuilt in 1536. The 1536 rebuilding turned a ship of 500 tons into one of 700 tons, and added an entire extra tier of broadside guns to the old carrack-style structure. By consequence, modern research is based mostly on interpretations of the concrete physical evidence of this version of the "Mary Rose". The construction of the original design from 1509 is less known.
The "Mary Rose" was built according to the carrack-style with high "castles" in the bow and stern with a low waist of open decking in the middle. The shape of the hull has a so-called tumblehome form and reflected the use of the ship as a platform for heavy guns. Above the waterline, the hull gradually narrows to compensate for the weight of the guns and to make boarding more difficult. Since only part of the hull has survived, it is not possible to determine many of the basic dimensions with any great accuracy. The moulded breadth, the widest point of the ship roughly above the waterline, was about 12 metres (39 ft) and the keel about 32 metres (105 ft), although the ship's overall length is uncertain.
The hull had four levels separated by three decks. The terminology for these in the 16th century was still not standardised so the terms used here are those that were applied by the Mary Rose Trust. The "hold" lay furthest down in the ship, right above the bottom planking below the waterline. This is where the kitchen, or galley, was situated and the food was cooked. Directly aft of the galley was the mast step, a rebate in the centre-most timber of the keelson, right above the keel, which supported the main mast, and next to it the main bilge pump. To increase the stability of the ship, the hold was where the ballast was placed and much of the supplies were kept. Right above the hold was the "orlop", the lowest deck. Like the hold it was partitioned and was also used as a storage area for everything from food to spare sails.
Above the orlop lay the "main deck" which housed the heaviest guns. The side of the hull on the main deck level had seven gunports on each side fitted with heavy lids that would have been watertight when closed. This was also the highest deck that was caulked and waterproof. Along the sides of the main deck there were cabins under the forecastle and sterncastle which have been identified as belonging to the carpenter, barber-surgeon, pilot and possibly also the master gunner and some of the officers. The top deck in the hull structure was the "upper deck" (or weather deck) which was exposed to the elements in the waist. It was a dedicated fighting deck without any known partitions and a mix of heavy and light guns. Over the open waist the upper deck was entirely covered with a coarse netting as a defence measure against boarding. Though very little of the upper deck has survived, it has been suggested that it housed the main living quarters of the crew underneath the sterncastle. A drainage located in this area has been identified as a possible "piss-dale", a general urinal to complement the regular toilets that would probably have been located in the bow.
The castles of the "Mary Rose" had additional decks, but since virtually nothing of them survives, their design has had to be reconstructed from historical records. Contemporary ships of equal size were consistently listed as having three decks in both castles. Although speculative, this layout is supported by the illustration in the Anthony Roll and the gun inventories.
During the early stages of excavation of the wreck, it was believed that the ship had originally been built with clinker (or clench) planking, a technique where the hull consisted of overlapping planks that bore the structural strength of the ship. Cutting gunports into a clinker-built hull would have meant weakening the ship's structural integrity, and it was assumed that she was later rebuilt to accommodate a hull with carvel edge-to-edge planking with a skeletal structure to support a hull perforated with gunports. Later examination indicates that the clinker planking is not present throughout the ship; only the outer structure of the sterncastle is built with overlapping planking, though not with a true clinker technique.
Sails and rigging.
Although only the lower fittings of the rigging survives, a 1514 inventory and the only known contemporary depiction of the ship from the Anthony Roll have been used to determine how the propulsion system of the "Mary Rose" was designed. Nine, or possibly ten, sails were flown from four masts and a bowsprit: the foremast and mainmast had two and three square sails respectively; the mizzen mast had a lateen sail and a small square sail and the bonaventure mizzen had at least one lateen sail, and possibly also a square sail, and the bowsprit flew a small square spritsail. According to the Anthony Roll illustration (see top of this section), the yards (the spars from which the sails were set) on the foremast and mainmast were also equipped with sheerhooks, twin curved blades sharpened on the inside, that were intended to cut an enemy ship's rigging during boarding actions.
The sailing capabilities of the "Mary Rose" were commented on by her contemporaries and were once even put to the test. In March 1513 a contest was arranged off The Downs, west of Kent, in which she raced against nine other ships. She won the contest, and Admiral Edward Howard described her enthusiastically as "the noblest ship of sayle any gret ship, at this howr, that I trow be in Cristendom". Several years later, while sailing between Dover and The Downs, Vice-Admiral William Fitzwilliam noted that both the "Henry Grace à Dieu" and the "Mary Rose" performed very well, riding steadily in rough seas and that it would have been a "hard chose" between the two. Modern experts have been more sceptical to her sailing qualities, believing that ships at this time were almost incapable of sailing close against the wind, and describing the handling of the "Mary Rose" as being like "a wet haystack".
Armament.
The "Mary Rose" represented a transitional ship design in naval warfare. Since ancient times, war at sea had been fought much like that on land: with melee weapons and bows and arrows, but on floating wooden platforms rather than battlefields. Though the introduction of guns was a significant change, it only slowly changed the dynamics of ship-to-ship combat. As guns became heavier and able to take more powerful gunpowder charges, they needed to be placed lower in the ship, closer to the water line. Gunports cut in the hull of ships had been introduced as early as 1501, only about a decade before the "Mary Rose" was built. This made broadsides, coordinated volleys from all the guns on one side of a ship, possible for the first time in history, at least in theory. Naval tactics throughout the 16th century and well into the 17th century focused on countering the oar-powered galleys that were armed with heavy guns in the bow, facing forwards, which were aimed by turning the entire ship against its target. Combined with inefficient gunpowder and the difficulties inherent in firing accurately from moving platforms, this meant that boarding remained the primary tactic for decisive victory throughout the 16th century.
Bronze and iron guns.
As the "Mary Rose" was built and served during a period of rapid development of heavy artillery, her armament was a mix of old designs and innovations. The heavy armament was a mix of older-type wrought iron and cast bronze guns, which differed considerably in size, range and design. The large iron guns were made up of staves or bars welded into cylinders and then reinforced by shrinking iron hoops and breech loaded, from the back, and equipped with simpler gun-carriages made from hollowed-out elm logs with only one pair of wheels, or without wheels entirely. The bronze guns were cast in one piece and rested on four-wheel carriages which were essentially the same as those used until the 19th century. The breech-loaders were cheaper to produce and both easier and faster to reload, but could take less powerful charges than cast bronze guns. Generally, the bronze guns used cast iron shot and were more suited to penetrate hull sides while the iron guns used stone shot that would shatter on impact and leave large, jagged holes, but both could also fire a variety of ammunition intended to destroy rigging and light structure or injure enemy personnel.
The majority of the guns were small iron guns with short range that could be aimed and fired by a single person. The two most common are the "bases", breech-loading swivel guns, most likely placed in the castles, and "hailshot pieces", small muzzle-loaders with rectangular bores and fin-like protrusions that were used to support the guns against the railing and allow the ship structure to take the force of the recoil. Though the design is unknown, there were two "top pieces" in a 1546 inventory (finished after the sinking) which was probably similar to a base, but placed in one or more of the fighting tops.
The ship went through several changes in her armament throughout her career, most significantly accompanying her "rebuilding" in 1536 (see below), when the number of anti-personnel guns was reduced and a second tier of carriage-mounted long guns fitted. There are three inventories that list her guns, dating to 1514, 1540 and 1546. Together with records from the armoury at the Tower of London, these show how the configuration of guns changed as gun-making technology evolved and new classifications were invented. In 1514, the armament consisted mostly of anti-personnel guns like the larger breech-loading iron "murderers" and the small "serpentines", "demi-slings" and stone guns. Only a handful of guns in the first inventory were powerful enough to hole enemy ships, and most would have been supported by the ship's structure rather than resting on carriages. The inventories of both the "Mary Rose" and the Tower had changed radically by 1540. There were now the new cast bronze "cannons", "demi-cannons", "culverins" and "sakers" and the wrought iron "port pieces" (a name that indicated they fired through ports), all of which required carriages, had longer range and were capable of doing serious damage to other ships. The analysis of the 1514 inventory combined with hints of structural changes in the ship both indicate that the gunports on the main deck were indeed a later addition.
Various types of ammunition could be used for different purposes: plain spherical shot of stone or iron smashed hulls, spiked bar shot and shot linked with chains would tear sails or damage rigging, and canister shot packed with sharp flints produced a devastating shotgun effect. Trials made with replicas of culverins and port pieces showed that they could penetrate wood the same thickness of the "Mary Rose's" hull planking, indicating a stand-off range of at least 90 m (295 ft). The port pieces proved particularly efficient at smashing large holes in wood when firing stone shot and were a devastating anti-personnel weapon when loaded with flakes or pebbles.
Hand-held weapons.
To defend against being boarded, "Mary Rose" carried large stocks of melee weapons, including pikes and bills; 150 of each kind were stocked on the ship according to the Anthony Roll, a figure confirmed roughly by the excavations. Swords and daggers were personal possessions and not listed in the inventories, but the remains of both have been found in great quantities, including the earliest dated example of a British basket-hilted sword.
A total of 250 longbows were carried on board, and 172 of these have so far been found, as well as almost 4000 arrows, bracers (arm guards) and other archery-related equipment. Longbow archery in Tudor England was mandatory for all able adult men, and despite the introduction of field artillery and handguns, they were used alongside new missile weapons in great quantities. On the "Mary Rose", the longbows could only have been drawn and shot properly from behind protective panels in the open waist or from the top of the castles as the lower decks lacked sufficient headroom. There were several types of bows of various size and range. Lighter bows would have been used as "sniper" bows, while the heavier design could possibly have been used to shoot fire arrows.
The inventories of both 1514 and 1546 also list several hundred heavy darts and lime pots that were designed to be thrown onto the deck of enemy ships from the fighting tops, although no physical evidence of either of these weapon types has been identified. Of the 50 handguns listed in the Anthony Roll, the complete stocks of five matchlock muskets and fragments of another eleven have been found. They had been manufactured mainly in Italy, with some originating from Germany. Found in storage were several "gunshields", a rare type of firearm consisting of a wooden shield with a small gun fixed in the middle.
Crew.
Throughout her 33-year career, the crew of the "Mary Rose" changed several times and varied considerably in size. It would have a minimal skeleton crew of 17 men or fewer in peace time and when she was "laid up in ordinary" (in reserve). The average wartime manning would have been about 185 soldiers, 200 sailors, 20–30 gunners and an assortment of other specialists such as surgeons, trumpeters and members of the admiral's staff, for a total of 400–450 men. When taking part in land invasions or raids, such as in the summer of 1512, the number of soldiers could have swelled to just over 400 for a combined total of more than 700. Even with the normal crew size of around 400, the ship was quite crowded, and with additional soldiers would have been extremely cramped.
Little is known of the identities of the men who served on the "Mary Rose", even when it comes to the names of the officers, who would have belonged to the gentry. Two admirals and four captains (including Edward and Thomas Howard, who served both positions) are known through records, as well as a few ship masters, pursers, master gunners and other specialists. Forensic science has been used by artists to create reconstructions of faces of eight crew members, and the results were publicized in May 2013. In addition, researchers have extracted DNA from remains in the hopes of identifying origins of crew, and potentially living descendants.
Of the vast majority of the crewmen, soldiers, sailors and gunners alike, nothing has been recorded. The only source of information for these men has been through osteological analysis of the human bones found at the wrecksite. An approximate composition of some of the crew has been conjectured based on contemporary records. The "Mary Rose" would have carried a captain, a master responsible for navigation, and deck crew. There would also have been a purser responsible for handling payments, a boatswain, the captain's second in command, at least one carpenter, a pilot in charge of navigation, and a cook, all of whom had one or more assistants (mates). The ship was also staffed by a barber-surgeon who tended to the sick and wounded, along with an apprentice or mate and possibly also a junior surgeon. The only positively identified person who went down with the ship was Vice-Admiral George Carew. McKee, Stirland and several other authors have also named Roger Grenville, father of Richard Grenville of the Elizabethan-era "Revenge", captain during the final battle, although the accuracy of the sourcing for this has been disputed by maritime archaeologist Peter Marsden.
The bones of a total of 179 people were found during the excavations of the "Mary Rose", including 92 "fairly complete skeletons", more or less complete collections of bones associated with specific individuals. Analysis of these has shown that crew members were all male, most of them young adults. Some were no more than 11–13 years old, and the majority (81%) under 30. They were mainly of English origin and, according to archaeologist Julie Gardiner, they most likely came from the West Country; many following their aristocratic masters into maritime service. There were also a few people from continental Europe. An eyewitness testimony right after the sinking refers to a survivor who was a Fleming, and the pilot may very well have been French. Analysis of oxygen isotopes in teeth indicates that some were also of southern European origin. In general they were strong, well-fed men, but many of the bones also reveal tell-tale signs of childhood diseases and a life of grinding toil. The bones also showed traces of numerous healed fractures, probably the result of on-board accidents.
There are no extant written records of the make-up of the broader categories of soldiers and sailors, but since the "Mary Rose" carried some 300 longbows and several thousand arrows there had to be a considerable proportion of longbow archers. Examination of the skeletal remains has found that there was a disproportionate number of men with a condition known as "os acromiale", affecting their shoulder blades. This condition is known among modern elite archery athletes and is caused by placing considerable stress on the arm and shoulder muscles, particularly of the left arm that is used to hold the bow to brace against the pull on the bowstring. Among the men who died on the ship it was likely that some had practised using the longbow since childhood, and served on board as specialist archers.
A group of six skeletons were found grouped close to one of the 2-tonne bronze culverins on the main deck near the bow. All but one of these crewmen (possibly a "powder monkey" not involved in heavy work) were strong, well-muscled men. They had all engaged in heavy pulling and pushing, indicated by fusing of parts of the spine and ossification, the growth of new bone, on several vertebrae. These have been tentatively classified as members of a complete gun crew, and all died at their battle station.
Military career.
First French war.
The "Mary Rose" first saw battle in 1512, in a joint naval operation with the Spanish against the French. The English were to meet the French and Breton fleets in the English Channel while the Spanish attacked them in the Bay of Biscay and then attack Gascony. The 35-year-old Sir Edward Howard was appointed Lord High Admiral in April and chose the "Mary Rose" as his flagship. His first mission was to clear the seas of French naval forces between England to the northern coast of Spain to allow for the landing of supporting troops near the French border at Fuenterrabia. The fleet consisted of 18 ships, among them the large ships the "Regent" and the "Peter Pomegranate", carrying over 5,000 men. Howard's expedition led to the capture of twelve Breton ships and a four-day raiding tour of Brittany where English forces successfully fought against local forces and burned numerous settlements.
The fleet returned to Southampton in June where it was visited by King Henry. In August the fleet sailed for Brest where it encountered a joint, but ill-coordinated, French-Breton fleet at the battle of St. Mathieu. The English with one of the great ships in the lead (according to Marsden the "Mary Rose") battered the French ships with heavy gunfire and forced them to retreat. The Breton flagship "Cordelière" put up a fight and was boarded by the 1,000-ton "Regent". By accident or through the unwillingness of the Breton crew to surrender, the powder magazine of the "Cordelière" caught fire and blew up in a violent explosion, setting fire to the "Regent" and eventually sinking her. About 180 English crew members saved themselves by throwing themselves into the sea and only a handful of Bretons survived, only to be captured. The captain of the "Regent", 600 soldiers and sailors, the High Admiral of France and the steward of the town of Morlaix were killed in the incident, making it the focal point of several contemporary chronicles and reports. On , the English burnt 27 French ships, captured another five and landed forces near Brest to raid and take prisoners, but storms forced the fleet back to Dartmouth in Devon and then to Southampton for repairs.
In the spring of 1513, the "Mary Rose" was once more chosen by Howard as the flagship for an expedition against the French. Before seeing action, she took part in a race against other ships where she was deemed to be one of the most nimble and the fastest of the great ships in the fleet (see details under "Sails and rigging"). On , Howard's force arrived off Brest only to see a small enemy force join with the larger force in the safety of Brest harbour and its fortifications. The French had recently been reinforced by a force of galleys from the Mediterranean, which sank one English ship and seriously damaged another. Howard landed forces near Brest, but made no headway against the town and was by now getting low on supplies. Attempting to force a victory, he took a small force of small oared vessels on a daring frontal attack on the French galleys on . Howard himself managed to reach the ship of French admiral, Prégent de Bidoux, and led a small party to board it. The French fought back fiercely and cut the cables that attached the two ships, separating Howard from his men. It left him at the mercy of the soldiers aboard the galley, who instantly killed him.
Demoralised by the loss of its admiral and seriously short of food, the fleet returned to Plymouth. Thomas Howard, elder brother of Edward, was assigned the new Lord Admiral, and was set to the task of arranging another attack on Brittany. The fleet was not able to mount the planned attack because of adverse winds and great difficulties in supplying the ships adequately and the "Mary Rose " took up winter quarters in Southampton. In August the Scots joined France in war against England, but were dealt a crushing defeat at the Battle of Flodden on 1513. A follow-up attack in early 1514 was supported by a naval force that included the "Mary Rose", but without any known engagements. The French and English mounted raids on each other throughout that summer, but achieved little, and both sides were by then exhausted. By autumn the war was over and a peace treaty was sealed by the marriage of Henry's sister, Mary, to French king Louis XII.
After the peace "Mary Rose" was placed in the reserves, "in ordinary". She was laid up for maintenance along with her sister ship the "Peter Pomegranate" in July 1514. In 1518 she received a routine repair and caulking, waterproofing with tar and oakum (old rope fibres) and was then assigned a small skeleton crew who lived on board the ship until 1522. She served briefly on a mission with other warships to "scour the seas" in preparation for Henry VIII's journey across the Channel to the summit with the French king Francis I at the Field of the Cloth of Gold in June 1520.
Second French war.
In 1522, England was once again at war with France because of a treaty with the Holy Roman Emperor Charles V. The plan was for an attack on two fronts with an English thrust in northern France. The "Mary Rose" participated in the escort transport of troops in June 1522, and by the Breton port of Morlaix was captured. The fleet sailed home and the "Mary Rose" berthed for the winter in Dartmouth. The war raged on until 1525 and saw the Scots join the French side. Though Charles Brandon came close to capturing Paris in 1523, there was little gained either against France or Scotland throughout the war. With the defeat of the French army and capture of Francis I by Charles V's forces at the Battle of Pavia on 1525, the war was effectively over without any major gains or major victories for the English side.
Maintenance and "in ordinary".
The "Mary Rose" was kept in reserve from 1522 to 1545. She was once more caulked and repaired in 1527 in a newly dug dock at Portsmouth and her longboat was repaired and trimmed. Little documentation about the "Mary Rose" between 1528 and 1539 exists. A document written by Thomas Cromwell in 1536 specifies that the "Mary Rose" and six other ships were "made new" during his service under the king, though it is unclear which years he was referring to and what "made new" actually meant. A later document from January 1536 by an anonymous author states that the "Mary Rose" and other ships were "new made", and dating of timbers from the ship confirms some type of repair being done in 1535 or 1536. This would have coincided with the controversial dissolution of the monasteries that resulted in a major influx of funds into the royal treasury. The nature and extent of this repair is unknown. Many experts, including Margaret Rule, the project leader for the raising of the "Mary Rose", have assumed that it meant a complete rebuilding from clinker planking to carvel planking, and that it was only after 1536 that the ship took on the form that it had when it sank and that was eventually recovered in the 20th century. Marsden has speculated that it could even mean that the "Mary Rose" was originally built in a style that was closer to 15th-century ships, with a rounded, rather than square, stern and without the main deck gunports.
Third French war.
Henry's complicated marital situation and his high-handed dissolution of the monasteries angered the Pope and Catholic rulers throughout Europe, which increased England's diplomatic isolation. In 1544 Henry had agreed to attack France together with Emperor Charles V, and English forces captured Boulogne at great cost in September, but soon England was left in the lurch after Charles had achieved his objectives and brokered a separate peace.
In May 1545, the French had assembled a large fleet in the estuary of the Seine with the intent to land troops on English soil. The estimates of the size of the fleet varied considerably; between 123 and 300 vessels according to French sources; and up to 226 sailing ships and galleys according to the chronicler Edward Hall. In addition to the massive fleet, 50,000 troops were assembled at Havre de Grâce (modern-day Le Havre). An English force of 160 ships and 12,000 troops under Viscount Lisle was ready at Portsmouth by early June, before the French were ready to set sail, and an ineffective pre-emptive strike was made in the middle of the month. In early July the huge French force under the command of Admiral Claude d'Annebault set sail for England and entered the Solent unopposed with 128 ships on . The English had around 80 ships with which to oppose the French, including the flagship "Mary Rose". But since they had virtually no heavy galleys, the vessels that were at their best in sheltered waters like the Solent, the English fleet promptly retreated into Portsmouth harbour.
Battle of the Solent.
The English were becalmed in port and unable to manoeuvre. On 1545, the French galleys advanced on the immobilised English fleet, and initially threatened to destroy a force of 13 small galleys, or "rowbarges", the only ships that were able to move against them without a wind. The wind picked up and the sailing ships were able to go on the offensive before the oared vessels were overwhelmed. Two of the largest ships, the "Henry Grace à Dieu" and the "Mary Rose", led the attack on the French galleys in the Solent.
Early in the battle something went wrong. While engaging the French galleys the "Mary Rose" suddenly heeled (leaned) heavily over to her starboard (right) side and water rushed in through the open gunports. The crew was powerless to correct the sudden imbalance, and could only scramble for the safety of the upper deck as the ship began to sink rapidly. As she leaned over, equipment, ammunition, supplies and storage containers shifted and came loose, adding to the general chaos. The massive port side brick oven in the galley collapsed completely and the huge 360-litre (90 gallon) copper cauldron was thrown onto the orlop deck above. Heavy guns came free and slammed into the opposite side, impeding escape or crushing men beneath them.
For those who were not injured or killed outright by moving objects, there was little time to reach safety, especially for the men who were manning the guns on the main deck or fetching ammunition and supplies in the hold. The companionways that connected the decks with one another would have become bottlenecks for fleeing men, something indicated by the positioning of many of the skeletons recovered from the wreck. What turned the sinking into a major tragedy in terms of lives lost was the anti-boarding netting that covered the upper decks in the waist (the midsection of the ship) and the sterncastle. With the exception of the men who were stationed in the tops in the masts, most of those who managed to get up from below deck were trapped under the netting; they would have been in view of the surface, and their colleagues above, but with little or no chance to break through, and were dragged down with the ship. Out of a crew of at least 400, fewer than 35 escaped, a catastrophic casualty rate of over 90%.
Causes of sinking.
Contemporary accounts.
Several accounts of the sinking have been preserved that describe the incident but the only confirmed eyewitness account is the testimony of a surviving Flemish crewman written down by the Holy Roman Emperor's ambassador François van der Delft in a letter dated . According to the unnamed Fleming, the ship had fired all of its guns of one side and was turning to present the guns on the other side to the enemy ship, when she was caught in a strong gust of wind, heeled and took in water through the open gunports. In a letter to William Paget dated former Lord High Admiral John Russel claimed that the ship had been lost because of "rechenes and great negligence". Three years after the sinking, the Hall's Chronicle gave the reason for the sinking as being caused by "to much foly ... for she was laden with much ordinaunce, and the portes left open, which were low, & the great ordinaunce unbreached, so that when the ship should turne, the water entered, and sodainly she sanke."
Later accounts repeat the explanation that the ship heeled over while going about and that the ship was brought down because of the open gunports. A biography of Peter Carew, brother of George Carew, written by John Hooker sometime after 1575, gives the same reason for the sinking, but adds that insubordination among the crew was to blame. The biography claims that George Carew noted that the "Mary Rose" showed signs of instability as soon as her sails were raised. George's uncle Gawen Carew had passed by with his own ship the "Matthew Gonson" during the battle to inquire about the situation of his nephew's ship. In reply he was told "that he had a sorte of knaves whom he could not rule". Contrary to all other accounts, Martin du Bellay, a French cavalry officer who was present at the battle, stated that the "Mary Rose" had been sunk by French guns.
Modern theories.
The most common explanation for the sinking among modern historians is that the ship was unstable for a number of reasons. When a strong gust of wind hit the sails at a critical moment, the open gunports proved fatal, the ship flooded and quickly foundered. Coates offered a variant of this hypothesis, which explains why a ship which served for several decades without sinking, and which even fought in actions in the rough seas off Brittany, unexpectedly foundered: the ship had accumulated additional weight over the years in service and finally become unseaworthy. That the ship was turning after firing all the cannons on one side has been questioned by Marsden after examination of guns recovered in both the 19th and 20th centuries; guns from both sides were found still loaded. This has been interpreted to mean that something else could have gone wrong since it is assumed that an experienced crew would not have failed to secure the gunports before making a potentially risky turn.
The most recent surveys of the ship indicate that the ship was modified late in her career and has lent support to the idea that the "Mary Rose" was altered too much to be properly seaworthy. Marsden has suggested that the weight of additional heavy guns would have increased her draught so much that the waterline was less than one metre (c. 3 feet) from the gunports on the main deck.
Peter Carew's claim of insubordination has been given support by James Watt, former Medical Director-General of the Royal Navy, based on records of an epidemic of dysentery in Portsmouth which could have rendered the crew incapable of handling the ship properly, while historian Richard Barker has suggested that the crew actually knew that the ship was an accident waiting to happen, at which they balked and refused to follow orders. Marsden has noted that the Carew biography is in some details inconsistent with the sequence of events reported by both French and English eyewitnesses. It also reports that there were 700 men on board, an unusually high number. The distance in time to the event it describes may mean that it was embellished to add a dramatic touch. The report of French galleys sinking the "Mary Rose" as stated by Martin du Bellay has been described as "the account of a courtesan" by naval historian Maurice de Brossard. Du Bellay and his two brothers were close to king Francis I and du Bellay had much to gain from portraying the sinking as a French victory. English sources, even if biased, would have nothing to gain from portraying the sinking as the result of crew incompetence rather than conceding to a victory to the much-feared gun galleys.
Dominic Fontana, a geographer at the University of Portsmouth, has voiced support for du Bellay's version of the sinking based on the battle as it is depicted in the Cowdray Engraving, and modern GIS analysis of the modern scene of the battle. By plotting the fleets and calculating the conjectured final manoeuvres of the "Mary Rose", Fontana reached the conclusion that the ship had been hit low in the hull by the galleys and was destabilised after taking in water. He has interpreted the final heading of the ship straight due north as a failed attempt to reach the shallows at Spitbank only a few hundred metres away. This theory has been given partial support by Alexzandra Hildred, one of the experts who has worked with the "Mary Rose", though she has suggested that the close proximity to Spitbank could also indicate that the sinking occurred while trying to make a hard turn to avoid running aground.
Experiments.
In 2000, the Channel 4 television programme "What Sank the Mary Rose" attempted to investigate the causes suggested for her sinking by means of experiments with scale models of the ship and metal weights to simulate the presence of troops on the upper decks. Initial tests showed that the ship was able to make the turn described by eyewitnesses without capsizing. In later tests, a fan was used to create a breeze similar to the one reported to have suddenly sprung up on the day of the sinking as the real "Mary Rose" went to make the turn. As the model made the turn, the breeze in the upper works forced it to heel more than at calm, forcing the main deck gun ports below the waterline and foundering the model within a few seconds. The sequence of events closely followed what eyewitnesses had reported, particularly the suddenness with which the ship sank.
History as a shipwreck.
A salvage attempt was ordered by Secretary of State William Paget only days after the sinking, and Charles Brandon, the king's brother-in-law, took charge of practical details. The operation followed the standard procedure for raising ships in shallow waters: strong cables were attached to the sunken ship and fastened to two empty ships, or hulks. At low tide, the ropes were pulled taut with capstans. When the high tide came in, the hulks rose and with them the wreck. It would then be towed into shallower water and the procedure repeated until the whole ship could be raised completely.
A list of necessary equipment was compiled by 1 August and included, among other things, massive cables, capstans, pulleys, and 40 pounds of tallow for lubrication. The proposed salvage team comprised 30 Venetian mariners and a Venetian carpenter with 60 English sailors to serve them. The two ships to be used as hulks were "Jesus of Lübeck" and "Samson", each of 700 tons burthen and similar in size to the "Mary Rose". Brandon was so confident of success that he reassured the king that it would only be a matter of days before they could raise the "Mary Rose". The optimism proved unfounded. Since the ship had settled at a 60-degree angle to starboard much of it was stuck deep into the clay of the seabed. This made it virtually impossible to pass cables under the hull and required far more lifting power than if the ship had settled on a hard seabed. An attempt to secure cables to the main mast appears only to have resulted in its being snapped off.
The project was only successful in raising rigging, some guns and other items. At least two other salvage teams in 1547 and 1549 received payment for raising more guns from the wreck. Despite the failure of the first salvage operation, there was still lingering belief in the possibility of retrieving the "Mary Rose" at least until 1546, when she was presented as part of the illustrated list of English warships called the Anthony Roll. When all hope of raising the complete ship was finally abandoned is not known. It could have been after Henry VIII's death in January 1547 or even as late as 1549, when the last guns were brought up. The "Mary Rose" was remembered well into the reign of Elizabeth I, and according to one of the queen's admirals, William Monson (1569–1643), the wreck was visible from the surface at low tide in the late 16th century.
Deterioration.
After the sinking, the partially buried wreck created a barrier at a right angle against the currents of the Solent. Two scour pits, large underwater ditches, formed on either side of the wreck while silt and seaweed was deposited inside the ship. A deep but narrow pit formed on the upward tilting port side, while a shallower, broader pit formed on the starboard side, which had mostly been buried by the force of the impact. The abrasive actions of sand and silt carried by the currents and the activity of fungi, bacteria and wood-boring crustaceans and molluscs, such as the "teredo" "shipworm", began to break down the structure of the ship. Eventually the exposed wooden structure was weakened and gradually collapsed. The timbers and contents of the port side were deposited in the scour pits and the remaining ship structure, or carried off by the currents. Following the collapse of the exposed parts of the ship the site was levelled with the seabed and was gradually covered by layers of sediment, concealing most of the remaining structure. During the 16th century a hard layer of compacted clay and crushed shells formed over the ship, stabilising the site and sealing the Tudor-era deposits. Further layers of soft silt covered the site during the 18th and 19th centuries, but frequent changes in the tidal patterns and currents in the Solent occasionally exposed some of the timbers, leading to its accidental rediscovery in 1836 and aided in locating the wreck in 1971. After the ship had been salvaged it was determined that about 40% of the original structure had survived.
Rediscovery in 19th century.
In the summer of 1836, a group of five fishermen caught their nets on timbers protruding from the bottom of the Solent. They contacted a diver to help them remove the hindrance, and on , Henry Abbinett became the first person to see the "Mary Rose" in almost 300 years. Later, two other professional divers, John Deane and William Edwards, were employed. Using a recently invented rubber suit and metal diving helmet, Deane and Edwards began to examine the wreck and salvage items from it. Along with an assortment of timbers and wooden objects, including several longbows, they brought up several bronze and iron guns, which were sold to the Board of Ordnance for over £220. Initially, this caused a dispute between Deane (who had also brought in his brother Charles into the project), Abbinett and the fishermen who had hired them. The matter was eventually settled by allowing the fishermen a share of the proceeds from the sale of the first salvaged guns, while Deane received exclusive salvage rights at the expense of Abbinett. The wreck was soon identified as the "Mary Rose" from the inscriptions of one of the bronze guns manufactured in 1537.
The identification of the ship led to significant public interest in the salvage operation, and caused a great demand for the objects which were brought up. Though many of the objects could not be properly conserved at the time and subsequently deteriorated, many were documented with pencil sketches and watercolour drawings which survive to this day. John Deane ceased working on the wreck in 1836, but returned in 1840 with new, more destructive methods. With the help of condemned bomb shells filled with gunpowder acquired from the Ordnance Board he blasted his way into parts of the wreck. Fragments of bombs and traces of blasting craters were found during the modern excavations, but there was no evidence that Deane managed to penetrate the hard layer that had sealed off the Tudor levels. Deane reported retrieving a bilge pump and the lower part of the main mast, both of which would have been located inside the ship. The recovery of small wooden objects like longbows, suggest that Deane did manage to penetrate the Tudor levels at some point, though this has been disputed by the excavation project leader Margaret Rule. Newspaper reports on Deane's diving operations in October 1840 report that the ship was clinker built, but since the sterncastle is the only part of the ship with this feature, an alternative explanation has been suggested: Deane did not penetrate the hard shelly layer that covered most of the ship, but only managed to get into remains of the sterncastle that today no longer exist. Despite the rough handling by Deane the "Mary Rose" escaped the wholesale destruction by giant rakes and explosives that was the fate of other wrecks in the Solent.
Modern rediscovery.
The modern search for the "Mary Rose" was initiated by the Southsea branch of the British Sub-Aqua Club in 1965 as part of a project to locate shipwrecks in the Solent. The project was under the leadership of historian, journalist and amateur diver Alexander McKee. Another group led by Lieutenant-Commander Alan Bax of the Royal Navy, sponsored by the Committee for Nautical Archaeology in London, also formed a search team. Initially the two teams had differing views on where to find the wreck, but eventually joined forces. In February 1966 a chart from 1841 was found that marked the positions of the "Mary Rose" and several other wrecks. The charted position coincided with a trench (one of the scour pits) that had already been located by McKee's team, and a definite location was finally established at a position 3 km (1.9 mi) south of the entrance to Portsmouth Harbour () in water with a depth of (36 feet) at low tide. Diving on the site began in 1966 and a sonar scan by Harold Edgerton in 1967–68 revealed some type of buried feature. In 1970 a loose timber was located and on 1971, the first structural details of the buried hull were identified after they were partially uncovered by winter storms.
A major problem for the team from the start was that wrecksites in the UK lacked any legal protection from plunderers and treasure hunters. Sunken ships, once being moving objects, were legally treated as chattel and were awarded to those who could first raise them. The Merchant Shipping Act of 1894 also stipulated that any objects raised from a wreck should be auctioned off to finance the salvage operations, and there was nothing preventing anyone from "stealing" the wreck and making a profit. The problem was handled by forming an organisation, the Mary Rose Committee, aiming "to find, excavate, raise and preserve for all time such remains of the ship "Mary Rose" as may be of historical or archaeological interest".
To keep intruders at bay, the Committee arranged a lease of the seabed where the wreck lay from the Portsmouth authorities, thereby discouraging anyone from trespassing on the underwater property. In hindsight this was only a legalistic charade which had little chance of holding up in a court of law. In combination with secrecy as to the exact location of the wreck, it saved the project from interference. It was not until the passing of the Protection of Wrecks Act on 1973 that the "Mary Rose" was declared to be of national historic interest that enjoyed full legal protection from any disturbance by commercial salvage teams. Despite this, years after the passing of the 1973 act and the excavation of the ship, lingering conflicts with salvage legislation remained a threat to the "Mary Rose" project as "personal" finds such as chests, clothing and cooking utensils risked being confiscated and auctioned off.
Survey and excavation.
Following the discovery of the wreck in 1971, the project became known to the general public and received increasing media attention. This helped bring in more donations and equipment, primarily from private sources. By 1974 the Committee had representatives from the National Maritime Museum, the Royal Navy, the BBC and local organisations. In 1974 the project received royal patronage from Prince Charles, who participated in dives on the site. This attracted yet more publicity, and also more funding and assistance. The initial aims of the Mary Rose Committee were now more officially and definitely confirmed. The Committee had become a registered charity in 1974, which made it easier to raise funds, and the application for excavation and salvage had been officially approved by the UK government.
By 1978 the initial excavation work had uncovered a complete and coherent site with an intact ship structure and the orientation of the hull had been positively identified as being on an almost straight northerly heading with a 60-degree heel to starboard and a slight downward tilt towards the bow. As no records of English shipbuilding techniques used in vessels like the "Mary Rose" survive, excavation of the ship would allow for a detailed survey of her design and shed new light on the construction of ships of the era. A full excavation also meant removing the protective layers of silt that prevented the remaining ship structure from being destroyed through biological decay and the scouring of the currents; the operation had to be completed within a predetermined timespan of a few years or it risked irreversible damage. It was also considered desirable to recover and preserve the remains of the hull if possible. For the first time, the project was faced with the practical difficulties of actually raising, conserving and preparing the hull for public display.
To handle this new, considerably more complex and expensive task, it was decided that a new organisation was needed. The Mary Rose Trust, a limited charitable trust, with representatives from many organisations would handle the need for a larger operation and a large infusion of funds. In 1979 a new diving vessel was purchased to replace the previous 12 m (40 ft) catamaran "Roger Greenville" which had been used from 1971. The choice fell on the salvage vessel "Sleipner", the same craft that had been used as a platform for diving operations on the "Vasa". The project went from a team of only twelve volunteers working four months a year to over 50 individuals working almost around the clock nine months a year. In addition there were over 500 volunteer divers and a laboratory staff of about 70 that ran the shore base and conservation facilities. During the four diving seasons from 1979 to 1982 over 22,000 diving hours was spent on the site, an effort that amounted to 11.8-man years.
Salvage.
Raising the "Mary Rose" meant overcoming a number of delicate problems that had never been encountered before. The salvage of the Swedish warship "Vasa" 1959–61 was the only comparable precedent, but it had been a relatively straightforward operation since the hull was completely intact and rested upright on the seabed. It had been raised with basically the same methods as were in use in Tudor England: cables were slung under the hull and attached to two pontoons on either side of the ship which was then gradually raised and towed into shallower waters. Only one third of the "Mary Rose" was intact and she lay deeply embedded in mud. If the hull were raised in the traditional way, there was no guarantee that it would have enough structural strength to hold together out of water. Many suggestions for salvage were discarded, including the construction of a cofferdam around the wreck site, filling the ship with small buoyant objects (such as ping pong balls) or even pumping brine into the seabed and freezing it so that it would float and take the hull with it. After lengthy discussions it was decided in February 1980 that the hull would first be emptied of all its contents and strengthened with steel braces and frames. It would then be lifted to the surface with floating sheerlegs attached to nylon strops passing under the hull and transferred to a cradle. It was also decided that the ship would be recovered before the end of the diving season in 1982. If the wreck stayed uncovered any longer it risked irreversible damage from biological decay and tidal scouring.
During the last year of the operation, the massive scope of full excavation and salvage was beginning to take its toll on those closely involved in the project. In May 1981 Alexander McKee voiced concerns about the method chosen for the salvage and openly questioned Margaret Rule's position as excavation leader. McKee felt ignored in what he viewed as a project where he had always played a central role, both as the initiator of the search for the "Mary Rose" and other ships in the Solent, and as an active member throughout the diving operations. He had several supporters who all pointed to the risk of the project's turning into an embarrassing failure if the ship were damaged during salvage. To address these concerns it was suggested that the hull should be placed on top of a supporting steel cradle underwater. This would avoid the inherent risks of damaging the wooden structure if it were lifted out of the water without appropriate support. The idea of using nylon strops was also discarded in favour of drilling holes through the hull at 170 points and passing iron bolts through them to allow the attachment of wires connected to a lifting frame.
In the spring of 1982, after three intense seasons of archaeological underwater work, preparations began for the salvage. The operation soon ran into problems: early on there were difficulties with the custom-made lifting equipment; divers on the project belonging to the Royal Engineers had to be pulled because of the outbreak of the Falklands War; and the method of lifting the hull had to be considerably altered as late as June. After the frame was properly attached to the hull it was slowly jacked up on four legs straddling the wreck site to pull the ship off the seabed. The massive crane of the barge "Tog Mor" was then used to lift the frame and hull on to the specially designed cradle which was padded with water-filled bags. On the morning of 1982, the final lift of the entire package of cradle, hull and lifting frame began. At 9:03 the first timbers of the "Mary Rose" broke the surface in the presence of the salvaging team, Prince Charles and curious spectators on boats circling the site. A second set of bags under the hull was inflated with air to cushion the waterlogged wood and finally the whole package was transferred to the barge that would take the hull ashore. Though eventually successful, the salvage operation was close to floundering on two occasions; first when one of the supporting legs of the lifting frame was bent and had to be removed and later when a corner of the frame, with "an unforgettable crunch", slipped more than a metre (3 feet) and came close to crushing part of the hull.
Archaeology.
As one of the most ambitious and expensive projects in the history of maritime archaeology, the "Mary Rose" project broke new ground within this field in the UK. Besides becoming one of the first wrecks to be protected under the new Protection of Wrecks Act in 1973 it also created several new precedents. It was the first time that a British privately funded project was able to apply modern scientific standards fully and without having to auction off part of the findings to finance its activities; where previous projects often had to settle for just a partial recovery of finds, everything found in connection with the "Mary Rose" was recovered and recorded. The salvage made it possible to establish the first historic shipwreck museum in the UK to receive government accreditation and funding. The excavation of the "Mary Rose" wrecksite proved that it was possible to achieve a level of exactness in underwater excavations comparable to those on dry land.
Throughout the 1970s, the "Mary Rose" was meticulously surveyed, excavated and recorded with the latest methods within the field of maritime archaeology. Working in an underwater environment meant that principles of land-based archaeology did not always apply. Mechanical excavators, airlifts and suction dredges were used in the process of locating the wreck, but as soon as it began to be uncovered in earnest, more delicate techniques were employed. Many objects from the "Mary Rose" had been well preserved in form and shape, but many were quite delicate, requiring careful handling. Artefacts of all sizes were supported with soft packing material, such as old plastic ice cream containers, and some of the arrows that were "soft like cream cheese" had to be brought up in special styrofoam containers. The airlifts that sucked up clay, sand and dirt off-site or to the surface were still used, but with much greater precision since they could potentially disrupt the site. The many layers of sediment that had accumulated on the site could be used to date artefacts in which they were found, and had to be recorded properly. The various types of accretions and remnants of chemicals with artefacts were essential clues to objects that had long since broken down and disappeared, and needed to be treated with considerable care.
The excavation and salvage in the 1970s and early '80s meant that diving operations ceased, even though modern scaffolding and part of the bow were left on the seabed. The pressure on conservators to treat tens of thousands of artefacts and the high costs of conserving, storing and displaying the finds and the ship meant that there were no funds available for diving. In 2002, the UK Ministry of Defence announced plans to build two new aircraft carriers. Because of the massive size of the new vessels, the outlet from Portsmouth needed to be surveyed to make sure that they could sail no matter the tide. The planned route for the underwater channel ran close to the "Mary Rose" wrecksite, which meant that funding was supplied to survey and excavate the site once more. Even though the planned carriers were down-sized enough to not require alteration of Portsmouth outlet, the excavations had already exposed timbers and were completed in 2005. Among the most important finds was the ten-metre (32 feet) stem, the forward continuation of the keel, which provided more exact details about the original profile of the ship.
Finds.
Over 26,000 artefacts and pieces of timber were salvaged along with remains of about half the crew members, The faces of some crew members have been reconstructed. Analysis of the crew skeletons shows many had suffered malnutrition, and had evidence of rickets, scurvy, and other deficiency diseases was found. Crew members also developed arthritis through the stresses on their joints from heavy lifting and maritime life generally, and suffered bone fractures.
As the ship was intended to function as a floating, self-contained community, it was stocked with victuals (food and drink) that could sustain its inhabitants for extended periods of time. The casks used for storage on the "Mary Rose" have been compared with those from a wreck of a trade vessel from the 1560s and have revealed that they were of better quality, more robust and reliable, an indication that supplies for the Tudor navy were given high priority, and their requirements set a high standard for cask manufacturing at the time. As a miniature society at sea, the wreck of the "Mary Rose" held personal objects belonging to individual crew members. This included clothing, games, various items for spiritual or recreation use, or objects related to mundane everyday tasks such as personal hygiene, fishing and sewing. The master carpenter's chest, for example, contained a backgammon set, a book, three plates, a sundial, and a tankard, goods suggesting he was relatively wealthy.
The ship carried several skilled craftsmen and was equipped for handling both routine maintenance and repairing extensive battle damage. In and around one of the cabins on the main deck under the sterncastle, archaeologists found a "collection of woodworking tools ... unprecedented in its range and size", consisting of eight chests of carpentry tools. Along with loose mallets and tar pots used for caulking, this variety of tools belonged to one or several of the carpenters employed on the "Mary Rose".
Many of the cannons and other weapons from the "Mary Rose" have provided invaluable physical evidence about 16th-century weapon technology. The surviving gunshields are almost all from the "Mary Rose", and the four small cast iron hailshot pieces are the only known examples of this type of weapon.
Animal remains have been found in the wreck of the "Mary Rose". These include the skeletons of a rat, a frog and a dog. The dog, a mongrel between eighteen months and two years in age, was found near the hatch to the ship's carpenter's cabin and is thought to have been brought aboard as a ratter. Nine barrels have been found to contain bones of cattle, indicating that they contained pieces of beef butchered and stored as ship's rations. In addition, the bones of pigs and fish, stored in baskets, have also been found.
Musical instruments.
Two fiddles, a bow, a still shawm or "doucaine", three three-hole pipes, and a tabor drum with a drumstick were found throughout the wreck. These would have been used for the personal enjoyment of the crew and to provide a rhythm to work on the rigging and turning the capstans on the upper decks. The tabor drum is the earliest known example of its kind and the drumstick of a previously unknown design. The tabor pipes are considerably longer than any known examples from the period. Their discovery proved that contemporary illustrations, previously viewed with some suspicion, were in fact accurate depictions of the instruments. Before the discovery of the "Mary Rose" shawm, an early predecessor to the oboe, instrument historians had been puzzled by reference to "still shawms", or "soft" shawms, that were said to have a sound that was less shrill than earlier shawms. The still shawm disappeared from the musical scene some time in the 16th century, and the instrument found on the "Mary Rose" is the only surviving example. A reproduction has been made and played. Combined with a pipe and tabor, it provides a "very effective bass part" that would have produced "rich and full sound, which would have provided excellent music for dancing on board ship". Only a few other fiddle-type instruments from the 16th century exist, but none of them of the type found on the "Mary Rose". Reproductions of both fiddles have been made, though less is known of their design than the shawm since the neck and strings were missing.
Navigation tools.
In the remains of a small cabin in the bow of the ship and in a few other locations around the wreck was found the earliest dated set of navigation instruments in Europe found so far: compasses, divider calipers, a stick used for charting, protractors, sounding leads, tide calculators and a logreel, an instrument for calculating speed. Several of these objects are not only unique in having such an early, definite dating, but also because they pre-date written records of their use; protractors would have reasonably been used to measure distance on maps, but sea charts are not known to have been used by English navigators during the first half of the 16th century, compasses were not depicted on English ships until the 1560s, and the first mention of a logreel is from 1574.
Barber-surgeon's cabin.
The cabin located on the main deck underneath the sterncastle is thought to have belonged to the barber-surgeon. He was a trained professional who saw to the health and welfare of the crew and acted as the medical expert on board. The most important of these finds were found in an intact wooden chest which contained over 60 objects relating to the barber-surgeon's medical practice: the wooden handles of a complete set of surgical tools and several shaving razors (although none of the steel blades had survived), a copper syringe for wound irrigation and treatment of gonorrhoea, and even a skilfully crafted feeding bottle for feeding incapacitated patients. More objects were found around the cabin, such as earscoops, shaving bowls and combs. With this wide selection of tools and medicaments the barber-surgeon, along with one or more assistants, could set bone fractures, perform amputations and deal with other acute injuries, treat a number of diseases and provide crew members with a minimal standard of personal hygiene.
Conservation.
Preservation of the "Mary Rose" and her contents was an essential part of the project from the start. Though many artefacts, especially those that were buried in silt, had been preserved, the long exposure to an underwater environment had rendered most of them sensitive to exposure to air after recovery. Archaeologists and conservators had to work in tandem from the start to prevent deterioration of the artefacts. After recovery, finds were placed in so-called passive storage, which would prevent any immediate deterioration before the active conservation which would allow them to be stored in an open-air environment. Passive storage depended on the type of material that the object was made of, and could vary considerably. Smaller objects from the most common material, wood, were sealed in polyethylene bags to preserve moisture. Timbers and other objects that were too large to be wrapped were stored in unsealed water tanks. Growth of fungi and microbes that could degrade wood were controlled by various techniques, including low-temperature storage, chemicals, and in the case of large objects, common pond snails that consumed wood-degrading organisms but not the wood itself.
Other organic materials such as leather, skin and textiles were treated similarly, by keeping them moist in tanks or sealed plastic containers. Bone and ivory was desalinated to prevent damage from salt crystallisation, as was glass, ceramic and stone. Iron, copper and copper alloy objects were kept moist in a sodium sesquicarbonate solution to prevent oxidisation and reaction with the chlorides that had penetrated the surface. Alloys of lead and pewter are inherently stable in the atmosphere and generally require no special treatment. Silver and gold were the only materials that required no special passive storage.
Conserving the hull of the "Mary Rose" was the most complicated and expensive task for the project. In 2002 a donation of from the Heritage Lottery Fund and equivalent monetary support from the Portsmouth City and Hampshire County Councils was needed to keep the work with conservation on schedule. During passive conservation, the ship structure could for practical reasons not be completely sealed, so instead it was regularly sprayed with filtered, recycled water that was kept at a temperature of 2 to 5 °C (35 to 41 °F) to keep it from drying out. Drying waterlogged wood that has been submerged for several centuries without appropriate conservation causes considerable shrinkage (20–50%) and leads to severe warping and cracking as water evaporates from the cellular structure of the wood. The substance polyethylene glycol (PEG) had been used before on archaeological wood, and was during the 1980s being used to conserve the "Vasa". After almost ten years of small-scale trials on timbers, an active three-phase conservation programme of the hull of the "Mary Rose" began in 1994. During the first phase, which lasted from 1994 to 2003, the wood was sprayed with low-molecular-weight PEG to replace the water in the cellular structure of the wood. From 2003 to 2010, a higher-molecular-weight PEG was used to strengthen the mechanical properties of the outer surface layers. The third phase will consist of a controlled air drying that will last three to five years, giving a final date of complete conservation of the "Mary Rose" no later than 2015.
Display.
After the decision to salvage the "Mary Rose," discussions ensued as to where she would eventually go on permanent display. The east end of Portsea Island at Eastney emerged as an early alternative, but was rejected because of parking problems and the distance from the dockyard where she was originally built. Placing the ship next to the famous flagship of Horatio Nelson, HMS "Victory", at Portsmouth Historic Dockyard was proposed in July 1981. A group called the Maritime Preservation Society even suggested Southsea Castle, where Henry VIII had witnessed the sinking, as a final resting place and there was widespread scepticism to the dockyard location. At one point a county councillor even threatened to withdraw promised funds if the dockyard site became more than an interim solution. As costs for the salvage project mounted, there was a debate in the Council chamber and in the local paper "The News" as to whether the money could be spent more appropriately. Although author David Childs writes that in the early 1980s "the debate was as a fiery one", the project was never seriously threatened because of the great symbolic importance of the "Mary Rose" to the naval history of both Portsmouth and England.
Since the mid-1980s, the hull of the "Mary Rose" has been kept in a covered dry dock while undergoing conservation. Although the hull has been open to the public for viewing, the need for keeping the ship saturated first with water and later a polyethylene glycol (PEG) solution has meant that visitors have been separated from the hull by a glass barrier. The specially built ship hall had been visited by over seven million visitors as of 2007, since it first opened on 1983, just under a year after it was successfully salvaged.
A separate Mary Rose Museum was housed in a structure called No. 5 Boathouse near the ship hall and was opened to the public on 1984. containing displays explaining the history of the ship and a small number of conserved artefacts, from entire bronze cannons to household items. In September 2009 the temporary "Mary Rose" display hall was closed to visitors to facilitate construction of the new museum building, which opened to the public on 31 May 2013.
The new Mary Rose Museum was designed by architects Wilkinson Eyre, Perkins+Will and built by construction firm Warings. The construction has been challenging because the museum has been built over the ship in the dry dock which is a listed monument. During construction of the museum, conservation of the hull continued inside a sealed "hotbox". In April 2013 the polyethylene glycol sprays were turned off and the process of controlled airdrying began. By 2016 the "hotbox" will be removed and for the first time since 1545, the ship will be revealed dry. This new museum displays most of the artefacts recovered from within the ship in context with the conserved hull. Since opening it has been visited by over 500,000 people.

</doc>
<doc id="19680" url="https://en.wikipedia.org/wiki?curid=19680" title="Mario Kart">
Mario Kart

There have been five "Mario Kart" games released for home consoles, three portable games, and three Namco co-developed arcade games, for a total of eleven. The latest title in the series, "Mario Kart 8", was released on Wii U in May 2014. The series has sold over 100 million copies worldwide to date.
History.
The first title in the "Mario Kart" series is "Super Mario Kart" and was released for the Super Nintendo Entertainment System in 1992. The development of the first game was overseen by Shigeru Miyamoto, the Japanese video game designer who helped create the original "Super Mario Bros.", as well as many other successful games for Nintendo. Darran Jones of NowGamer suggests that the original success of "Super Mario Kart" was the result of including characters previously seen in "Mario Bros." games, while also being a new type of racing game.
Gameplay.
In the "Mario Kart" series, players compete in go-kart races, controlling one of a selection of characters from the "Mario" franchise. Up to twelve characters can compete in each race.
One of the features of the series is the use of various power-up items obtained by driving into item boxes laid out on the course. These power-ups include mushrooms to give players a speed boost, Koopa Shells to be thrown at opponents, and banana peels that can be laid on the track as hazards. The type of weapon received from an item box is often random, though sometimes influenced by the player's current position in the race. For example, players lagging far behind may receive more powerful items while the leader will only receive small defensive items. Called rubber banding, this gameplay mechanism allows other players or computers a realistic chance to catch up to the leading player.
As the series has progressed, each new installment has introduced new elements in order to keep the gameplay fresh, such as new courses, box items and playable characters. These changes include:
Courses.
Many course themes recur throughout the series. Most are based on an existing area in the "Mario" series (Bowser's Castle being among the most prominent), but there are a number of courses that have not appeared elsewhere, but still belong in the Mushroom Kingdom, such as Rainbow Road. Each game in the series includes at least 16 original courses and up to 6 original battle arenas. Each game's tracks are divided into four "cups", or groups in which the player has to have the highest overall placing to win. Most courses can be done in three laps. The first game to feature courses from previous games was "Mario Kart: Super Circuit", which contained all of the tracks from the original Super NES game. Starting with "Mario Kart DS", each entry in the series has featured 16 original courses and 16 "retro" tracks drawn from previous titles, spread across four cups each. In "Mario Kart 8", new tracks are available in two downloadable packages, eight for each package downloaded. This downloadable content includes courses based on other Nintendo titles, such as "The Legend of Zelda" and "Animal Crossing", making them the first "Mario Kart" courses to be set outside the "Mario" universe.
Modes of play.
Each installment features a variety of different modes. The following four modes recur most often in the series:
List of "Mario Kart" games.
Arcade games.
At one point, there was also a game in the series planned for the Virtual Boy in 1995. Entitled "Super Mario Kart: Virtual Cup," it was likely to be the first sequel to "Super Mario Kart." The game was cancelled due to the Virtual Boy's failure, but was revealed in a 2000 issue of German gaming magazine "The Big N."
Other appearances.
Several "Mario Kart"-related items appear in the "Super Smash Bros." series, with "Super Smash Bros. Brawl" in particular featuring a Mario Circuit stage based on Figure-8 Circuit from "Mario Kart DS", "Super Smash Bros. for Nintendo 3DS" featuring a Rainbow Road stage based on its appearance in "Mario Kart 7", and "Super Smash Bros. for Wii U" featuring a Mario Circuit stage based on its appearance in "Mario Kart 8". 
Certain courses from the series have also appeared in "F-Zero X", "Fortune Street", and the "Mario & Sonic" series. Various items from the series can also be seen in games such as "Nintendogs" and "Animal Crossing".
Merchandise.
The "Mario Kart" series has had a range of merchandise released.
Among them are a slot car racer series based on "Mario Kart DS", which comes with Mario and Donkey Kong figures, while Wario and Luigi are available separately. A line of radio-controlled karts have also been marketed, with are controlled by Game Boy Advance-shaped controllers, and feature Mario, Donkey Kong, and Yoshi. There are additional, larger karts that depict the same trio and are radio-controlled by a GameCube-shape controller.
Japanese figurines of Mario, Luigi, Peach, Toad, Yoshi, Wario, Donkey Kong, and Bowser are also available for purchase as well as for "Mario Kart 64", figures of Mario, Luigi, Wario, Bowser, Donkey Kong, and Yoshi were made by Toybiz. There are also Sound Drops inspired by "Mario Kart Wii" with eight sounds from the game. A land-line telephone featuring Mario holding a lightning bolt while seated in his kart, has also been marketed.
K'Nex released "Mario Kart Wii" sets, with Mario, Luigi, Yoshi, Donkey Kong, and Bowser in karts and bikes, as well as tracks from the game. "Mario Kart 7" and "Mario Kart 8" K'Nex sets have also been released.
Nintendo's own customer rewards program Club Nintendo released merchandise from the series as well. These include a "Mario Kart 8" original soundtrack, a "Mario Kart Wii"-themed stopwatch, and gold trophies modeled after those in "Mario Kart 7". Before Club Nintendo, a "Mario Kart 64" soundtrack was offered by mail.
In 2014, McDonald's released "Mario Kart 8" toys with Happy Meals. They featured 8 of the characters in karts that were customizable with stickers.
Reception.
"Nintendo Power" listed the "Mario Kart" series as being one of the greatest multi-player experiences, citing the diversity in game modes as well as the entertainment value found.
"Guinness World Records" listed six records set by the "Mario Kart" series, including "First Console Kart Racing Game", "Best Selling Racing Game" and "Longest Running Kart Racing Franchise." "Guinness World Records" ranked the original "Super Mario Kart" number 1 on the list of top 50 console games of all time based on initial impact and lasting legacy.
The series has sold over 100 million copies. "Mario Kart Wii" is the best selling installment in the series, selling 35.53 million as of March 2014.

</doc>
<doc id="19683" url="https://en.wikipedia.org/wiki?curid=19683" title="Module (disambiguation)">
Module (disambiguation)

Module or modular may refer to the concept of modularity. It may also refer to:

</doc>
<doc id="19684" url="https://en.wikipedia.org/wiki?curid=19684" title="May 21">
May 21


</doc>
<doc id="19685" url="https://en.wikipedia.org/wiki?curid=19685" title="Mythology">
Mythology

Mythology can refer to the collected myths of a group of peopletheir collection of stories they tell to explain nature, history, and customsor to the study of such myths.
As a collection of such explanatory stories, mythology is a vital feature of every culture. Many sources for myths have been proposed, ranging from personification of nature or personification of natural phenomena, to truthful or hyperbolic accounts of historical events to explanations of existing rituals. Although the term is complicated by its implicit condescension, mythologizing is not just an ancient or primitive practice, as shown by contemporary mythopoeia such as urban legends and the expansive fictional mythoi created by fantasy novels and comics. A culture's collective mythology helps convey belonging, shared and religious experiences, behavioral models, and moral and practical lessons.
The study of myth dates back to ancient history. Rival classes of the Greek myths by Euhemerus, Plato, and Sallustius were developed by the Neoplatonists and later revived by Renaissance mythographers. The nineteenth-century comparative mythology reinterpreted myth as a primitive and failed counterpart of science (E. B. Tylor), a "disease of language" (Max Müller), or a misinterpretation of magical ritual (James Frazer).
Recent approaches have rejected conflict between the value of myth and rational thought, often viewing myths as expressions for understanding general psychological, cultural, or societal truths, rather than being merely inaccurate historical accounts.
Etymology.
The English term "mythology" predates the word "myth" by centuries. It first appeared in the fifteenth-century, borrowed from the Middle French term "mythologie". The word "mythology", ("exposition of myths"), comes from Middle French "mythologie", from Late Latin "mythologia", from Greek μυθολογία "mythologia" ("legendary lore, a telling of mythic legends; a legend, story, tale") from μῦθος "mythos" ("myth") and -λογία "-logia" ("study"). Both terms translated the subject of Latin author Fulgentius' fifth-century "Mythologiæ", which was concerned with the explication of Greek and Roman stories about their gods, commonly referred to as classical mythology. Although Fulgentius' conflation with the contemporary African Saint Fulgentius is now questioned, the "Mythologiæ" explicitly treated its subject matter as allegories requiring interpretation and not as true events. (The word "mythología" [] appears in Plato, but was used as a general term for "fiction" or "story-telling" of any kind, combining "mỹthos" [, "narrative, fiction"] and "-logía" [, "discourse, able to speak about"].) From Lydgate until the seventeenth or eighteenth-century, "mythology" was similarly used to mean a moral, fable, allegory, or a parable. From its earliest use in reference to a collection of traditional stories or beliefs, mythology implied the falsehood of the stories being described. Although, with its remaining association with sacred tales of the Greeks and Romans, it came to be applied by analogy with similar bodies of traditional stories among other polytheistic cultures around the world. The Greek loanword "mythos" (pl. "mythoi") and Latinate "mythus" (pl. "mythi") both appeared in English before the first attestation of "myth" in 1830.
Terminology.
In present use, "mythology" usually refers to the collected myths of a group of people, but may also mean the study of such myths. For example, Greek mythology, Roman mythology, and Hittite mythology all describe the body of myths retold among those cultures. However, landscape mythology describes the study of landscape used across various peoples with ties to totemism. Alan Dundes defined myth as a sacred narrative, which explains how the world and humanity evolved into their present for.,Dundes classified a sacred narrative as "a story that serves to define the fundamental worldview of a culture by explaining aspects of the natural world and delineating the psychological and social practices and ideals of a societ.y; Bruce Lincoln defined "myth" as "ideology in narrative form." Many scholars in other fields use the term "myth" in somewhat different ways; in a very broad sense, the word can refer to any traditional story, popular misconception, or imaginary entity. Due to this pejorative sense, some scholars opt to return to the earlier term "mythos." Its use was similarly pejorative and now more commonly refers to its Aristotelian sense as a "plot point" or to a collective mythology, as in the world building of H.P. Lovecraft.
Mythology is now often sharply distinguished from didactic literature such as fables, but its relationship with other traditional stories, such as legends and folktales, is much more nebulous. Main characters in myths are usually gods, demigods, or supernatural humans, while legends generally feature humans as their main characters. However, many exceptions or combinations exist, as in the "Iliad", "Odyssey", and "Aeneid". Myths are often endorsed by rulers and priests and are closely linked to religion or spirituality. In fact, many societies group their myths, legends, and history together, considering myths to be true accounts of their remote past. Creation myths particularly, take place in a primordial age when the world had not yet achieved its current form. Other myths explain how a society's customs, institutions, and taboos were established and sanctified. A separate space is created for folktales, which are not considered true by the people who tell them. As stories spread to other cultures or as faiths change, myths can come to be considered folktales, sometimes even to the point of being reinterpreted as one. Its divine characters are recast as either as humans or demihumans such as giants, elves, and faeries.
Origins.
Euhemerism.
One theory claims myths are distorted accounts of real historical events. According to this theory, storytellers repeatedly elaborate upon historical accounts until the figures in those accounts gain the status of gods. For example, one might argue that the myth of the wind-god Aeolus evolved from a historical account of a king who taught his people to use sails and interpret the winds. Herodotus (fifth-century BC) and Prodicus also made claims of this kind. This theory is named "euhemerism" after mythologist Euhemerus (c.320 BC), who suggested that Greek gods developed from legends about human beings.
Allegory.
Some theories propose that myths began as allegories. According to one such theory, myths began as allegories for natural phenomena: Apollo represents the sun, Poseidon represents water, and so on. According to another theory, myths began as allegories for philosophical or spiritual concepts: Athena represents wise judgment, Aphrodite represents desire, and so on. The nineteenth-century Sanskritist Max Müller supported an allegorical theory of myth. He believed myths began as allegorical descriptions of nature, but gradually came to be interpreted literally. For example, a poetic description of the sea as "raging" was eventually taken literally, and the sea was then thought of as a raging god.
Personification.
Some thinkers believe myths resulted from the personification of inanimate objects and forces. According to these thinkers, the ancients worshiped natural phenomena, such as fire and air, gradually coming to describe them as gods. For example, according to the theory of mythopoeic thought, the ancients tended to view things as persons, not as mere objects; thus, they described natural events as acts of personal gods, giving rise to myths.
Myth-ritual theory.
According to the myth-ritual theory, the existence of myth is tied to ritual. In its most extreme form, this theory claims myths arose to explain rituals. This claim was first put forward by the biblical scholar William Robertson Smith. According to Smith, people begin performing rituals for a reason that is not related to myth. Forgetting the original reason for a ritual, they try to account for it by inventing a myth and claiming the ritual commemorates the events described in that myth. James Frazer, an anthropologist, has a similar theory. He believes primitive humans start out with a belief in magical laws. Later, when they begin to lose faith in magic, they invent myths about gods, claiming that their formerly considered magical rituals are actually religious rituals intended to appease the gods.
Functions of myth.
Mircea Eliade, a historian, argued that one of the foremost functions of myth is to establish models for behavior and that myths may also provide a religious experience. By telling or reenacting myths, members of traditional societies detach themselves from the present, returning to the mythical age, thereby bringing themselves closer to the divine.
Lauri Honko asserte that, in some cases, a society will reenact a myth in an attempt to reproduce the conditions of the mythical age. For example, it will reenact the healing performed by a god at the beginning of time in order to heal someone in the present. Similarly, Roland Barthes argued that modern culture explores religious experience. Since it is not the job of science to define human morality, a religious experience is an attempt to connect with a perceived moral past, which is in contrast with the technological present.
Joseph Campbell writes:
"In the long view of the history of mankind, four essential functions of mythology can be discerned. The first and most distinctive – vitalizing all – is that of eliciting and supporting a sense of awe before the mystery of being." 
"The second function of mythology is to render a cosmology, an image of the universe that will support and be supported by this sense of awe before the mystery of the presence and the presence of a mystery." 
"A third function of mythology is to support the current social order, to integrate the individual organically with his group;"
"The fourth function of mythology is to initiate the individual into the order of realities of his own psyche, guiding him toward his own spiritual enrichment and realization."
In a later work Campbell explains the relationship of myth to civilization:
Yet the history of civilisation is not one of harmony.
Campbell gives his answer to the question, ""what is the function of myth today"?" in the second episode of Bill Moyers's "The Power of Myth" series.
Study of mythology.
Historically, the important approaches to the study of mythology have been those of Vico, Schelling, Schiller, Jung, Freud, Lévy-Bruhl, Lévi-Strauss, Frye, the Soviet school, and the Myth and Ritual School.
Pre-modern theories.
The critical interpretation of myth goes back as far as the Presocratics. Euhemerus was one of the most important pre-modern mythologists. He interpreted myths as accounts of actual historical events - distorted over many retellings. Sallustius, divided myths into five categories – theological, physical (or concerning natural laws), animistic (or concerning soul), material, and mixed. Mixed concerns myths which show the interaction between two or more of the previous categories and are particularly used in initiations.
To those who are trying to change content of the myth according to probability would find criticism in Plato's "Phaedrus" (229d), in which Socrates says that it is the province of one who is "vehemently curious and laborious, and not entirely happy . . ."
Plato famously condemned poetic myth, when discussing the education of the young, in the "Republic", primarily on the grounds there was a danger the young and uneducated might take the stories of gods and heroes literally. Nevertheless, he constantly referred to myths of all kinds throughout his writings. As Platonism developed in the phases commonly called Middle Platonism and neoplatonism, such writers as Plutarch, Porphyry, Proclus, Olympiodorus, and Damascius wrote explicitly about the symbolic interpretation of traditional and Orphic myths. Interest in polytheistic mythology revived during the Renaissance, with early works on mythography appearing in the sixteenth-century, such as the "Theologia mythologica" (1532). While myths are not the same as fables, legends, folktales, fairy tales, anecdotes, or fiction, the concepts may overlap. Notably, during the nineteenth century period of Romanticism, folktales and fairy tales were perceived as eroded fragments of earlier mythology (famously by the Brothers Grimm and Elias Lönnrot). Mythological themes are often consciously employed in literature, beginning with Homer. The resulting work may expressly refer to a mythological background without itself being part of a body of myths (Cupid and Psyche). Medieval romance in particular plays with this process of turning myth into literature. "Euhemerism, as stated earlier," refers to the rationalization of myths, putting themes formerly imbued with mythological qualities into pragmatic contexts. An example of this would be following a cultural or religious paradigm shift (notably the re-interpretation of pagan mythology following Christianization).
Conversely, historical and literary material may acquire mythological qualities over time. For example, the Matter of Britain (the legendary history of Great Britain, especially those focused on King Arthur and the knights of the Round Table) and the Matter of France, based on historical events of the fifth and eighth-centuries respectively, were first made into epic poetry and became partly mythological over the following centuries. "Conscious generation" of mythology has been termed "mythopoeia" by J. R. R. Tolkien, and was notoriously also suggested, very separately, by Nazi ideologist Alfred Rosenberg.
Nineteenth-century theories.
The first scholarly theories of myth appeared during the second half of the nineteenth-century. In general, these nineteenth-century theories framed myth as a failed or obsolete mode of thought, often by interpreting myth as the primitive counterpart of modern science.
For example, E. B. Tylor interpreted myth as an attempt at a literal explanation for natural phenomena. Unable to conceive impersonal natural laws, early humans tried to explain natural phenomena by attributing souls to inanimate objects, giving rise to animism. According to Tylor, human thought evolves through various stages, starting with mythological ideas and gradually progressing to scientific ideas. Not all scholars, not even all nineteenth-century scholars, have agreed with his view. For example, Lucien Lévy-Bruhl claimed "the primitive mentality is a condition of the human mind, and not a stage in its historical development."
Max Müller called myth a "disease of language". He speculated that myths arose due to the lack of abstract nouns and neuter gender in ancient languages. Anthropomorphic figures of speech, necessary in such languages, were eventually taken literally, leading to the idea that natural phenomena were in actuality conscious beings or gods.
James Frazer saw myths as a misinterpretation of magical rituals, which were themselves based on a mistaken idea of natural law. According to Frazer, humans begin with an unfounded belief in impersonal magical laws. When they realize applications of these laws do not work, they give up their belief in natural law in favor of a belief in personal gods controlling nature, thus giving rise to religious myths. Meanwhile, humans continue practicing formerly magical rituals through force of habit, reinterpreting them as reenactments of mythical events. Finally humans come to realize nature follows natural laws, and they discover their true nature through science. Here again, science makes myth obsolete as humans progress "from magic through religion to science."
Robert Segal asserts that by pitting mythical thought against modern scientific thought, such theories imply modern humans must abandon myth.
Twentieth-century theories.
Many twentieth-century theories of myth rejected the nineteenth-century theories' opposition of myth and science. In general, "twentieth-century theories have tended to see myth as almost anything but an outdated counterpart to science […]. Consequently, modern individuals are not obliged to abandon myth for science."
Swiss psychologist Carl Jung tried to understand the psychology behind world myths. Jung asserted that all humans share certain innate unconscious psychological forces, which he called "archetypes". He believed similarities between the myths from different cultures reveals the existence of these universal archetypes.
Joseph Campbell believed there were two different orders of mythology: myths that "are metaphorical of spiritual potentiality in the human being," and myths "that have to do with specific societies." His major work is "The Masks of God I-IV". In the first volume, "Primitive Mythology", he clearly outlines his intention:
In his fourth volume Campbell coined the phrase, "creative mythology", which he explains as:
Claude Lévi-Strauss believed myths reflect patterns in the mind and interpreted those patterns more as fixed mental structures, specifically pairs of opposites (good/evil, compassionate/callous), rather than unconscious feelings or urges.
In his appendix to "Myths, Dreams and Mysteries", and in "The Myth of the Eternal Return", Mircea Eliade attributed modern humans’ anxieties to their rejection of myths and the sense of the sacred.
In the 1950s, Roland Barthes published a series of essays examining modern myths and the process of their creation in his book "Mythologies".
Following the Structuralist Era (roughly the 1960s to 1980s), the predominant anthropological and sociological approaches to myth increasingly treat myth as a form of narrative that can be studied, interpreted, and analyzed like ideology, history, and culture. In other words, myth is a form of understanding and telling stories that is connected to power, political structures, and political and economic interests. These approaches are very much in contrast to approaches such as those of Campbell and Eliade that hold that myth has some type of essential connection to ultimate sacred meanings that transcend cultural specifics. In particular, there is a long-standing exploration of myth in relation to history from diverse social sciences. Most of these studies share the assumption that there is no necessary difference between history and myth in the sense that history is factual, real, accurate, and truth, while myth is the opposite. Myth, like ideology, is a word used to disparage the histories (or ways of understanding) of other sociopolitical groups.
Comparative mythology.
Comparative mythology is the systematic comparison of myths from different cultures. It seeks to discover underlying themes that are common to the myths of multiple cultures. In some cases, comparative mythologists use the similarities between different mythologies to argue that those mythologies have a common source. This common source may be a common source of inspiration (e.g. a certain natural phenomenon that inspired similar myths in different cultures) or a common "protomythology" that diverged into the various mythologies we see today.
Nineteenth-century interpretations of myth were often highly comparative, seeking a common origin for all myths. However, modern-day scholars tend to be more suspicious of comparative approaches, avoiding overly general or universal statements about mythology. One exception to this modern trend is Joseph Campbell's book "The Hero with a Thousand Faces" (1949), which claims that all hero myths follow the same underlying pattern. This theory of a monomyth is out of favor with the mainstream study of mythology.
Modern mythology.
In modern society, myth is often regarded as historical or obsolete. Many scholars in the field of cultural studies are now beginning to research the idea that myth has worked itself into modern discourses. Modern formats of communication allow for widespread communication across the globe, thus enabling mythological discourse and exchange among greater audiences than ever before. Various elements of myth can now be found in television, cinema, and video games.
Although myth was traditionally transmitted through the oral tradition on a small scale, the technology of the film industry has enabled filmmakers to transmit myths to large audiences via film dissemination. In the psychology of Carl Jung, myths are the expression of a culture or society’s goals, fears, ambitions, and dreams. Film is ultimately an expression of the society in which it was credited, and reflects the norms and ideals of the time and location in which it is created. In this sense, film is simply the evolution of myth. The technological aspect of film changes the way myth is distributed, but the core idea of the myth is the same.
The basis of modern storytelling in both cinema and television lies deeply rooted in the mythological tradition. Many contemporary and technologically advanced movies often rely on ancient myths to construct narratives. The Disney Corporation is well-known among cultural study scholars for "reinventing" traditional childhood myths. While many films are not as obvious as Disney fairy tales in respect to the employment of myth, the plots of many films are largely based on the rough structure of the myth. Mythological archetypes, such as the cautionary tale regarding the abuse of technology, battles between gods, and creation stories, are often the subject of major film productions. These films are often created under the guise of cyberpunk action movies, fantasy, dramas, and apocalyptic tales. Although the range of narratives, as well as the medium in which it is being told, is constantly increasing, it is clear myth continues to be a pervasive and essential component of the collective imagination.
Recent films such as "Clash of the Titans", "Immortals", and "Thor" continue the trend of mining traditional mythology in order to create a plot for modern consumption.
With the invention of modern myths such as urban legends, the mythological tradition will carry on to the increasing variety of mediums available in the twenty-first-century and beyond. The crucial idea is that myth is not simply a collection of stories permanently fixed to a particular time and place in history, but an ongoing social practice within every society. Nowadays, many authors use mythology as a basis for their books, such as Rick Riordan, whose Percy Jackson and the Olympians series is situated in a modern-day world where the Greek deities are manifest, as well as his Kane Chronicles with the Egyptian pantheon.

</doc>
<doc id="19688" url="https://en.wikipedia.org/wiki?curid=19688" title="Mind map">
Mind map

A mind map is a diagram used to visually organize information. A mind map is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.
Mind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available.
Mind maps are considered to be a type of spider diagram. A similar concept in the 1970s was "idea sun bursting".
Origins.
Although the term "mind map" was first popularized by British popular psychology author and television personality Tony Buzan, the use of diagrams that visually "map" information using branching and radial maps traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, brainstorming, memory, visual thinking, and problem solving by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by Porphyry of Tyros, a noted thinker of the 3rd century, as he graphically visualized the concept categories of Aristotle. Philosopher Ramon Llull (1235–1315) also used such techniques.
The semantic network was developed in the late 1950s as a theory to understand human learning and developed further by Allan M. Collins and M. Ross Quillian during the early 1960s. Mind maps are similar in radial structure to concept maps, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.
Popularisation of the term "mind map".
Buzan's specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called "Use Your Head". In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.
Buzan says the idea was inspired by Alfred Korzybski's general semantics as popularized in science fiction novels, such as those of Robert A. Heinlein and A. E. van Vogt. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of cerebral hemispheres in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.
Mind map guidelines.
Buzan suggests the following guidelines for creating mind maps:
Uses.
As with other diagramming tools, mind maps can be used to generate, visualize, structure, and classify ideas, and as an aid to studying and organizing information, solving problems, making decisions, and writing.
Mind maps have many applications in personal, family, educational, and business situations, including notetaking, brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a mnemonic technique, or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.
In addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance expert search systems, search engines and search and tag query recommender. To do so, mind maps can be analysed with classic methods of information retrieval to classify a mind map's author or documents that are linked from within the mind map.
Research.
Effectiveness - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science". Other studies also report positive effects through the use of mind maps. Farrand, Hussain, and Hennessy (2002) found that spider diagrams (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about concept mapping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions". The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.
Features of Mind Maps - Beel & Langer (2011) conducted a comprehensive analysis of the content of mind maps. They analysed 19,379 mind maps from 11,179 users of the mind mapping applications SciPlore MindMapping (now Docear) and MindMeister. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps.
Automatic Creating of Mind Maps - There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. Rothenberger et al. extracted the main story of a text and presented it as mind map. And there is a patent about automatically creating sub-topics in mind maps.
Pen and Paper vs Computer - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.
Tools.
Mind-mapping software can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images. It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional note-taking.

</doc>
<doc id="19690" url="https://en.wikipedia.org/wiki?curid=19690" title="Machine gun">
Machine gun

A machine gun is a fully automatic mounted or portable firearm, designed to fire bullets in quick succession from an ammunition belt or magazine, typically at a rate of 300 to 1800 rounds per minute. Fully automatic firearms are generally categorized as submachine guns, assault rifles, battle rifles, automatic shotguns, machine guns, or autocannons. Machine guns with multiple rotating barrels are referred to as "rotary machine guns."
As a class of military firearms, true machine guns are fully automatic weapons designed to be used as support weapons and generally used when attached to a mount or fired from the ground on a bipod or tripod. Light machine guns are small enough to be fired hand-held, but are more effective when fired from a prone position. The difference between machine guns and other categories of weapons is based on caliber, with autocannons using calibers of 20 mm or larger, and whether the gun fires conventional bullets, shells, shotgun cartridges, or explosive rounds. Fully automatic guns firing shotgun cartridges are usually called automatic shotguns, and those firing large-caliber explosive rounds are generally considered either autocannons or automatic grenade launchers ("grenade machine guns"). Submachine guns are hand-held automatic weapons for personal defense or short-range combat firing pistol-caliber rounds. In contrast to submachine guns and autocannons, machine guns (like rifles) tend to have a very high ratio of barrel length to caliber (a long barrel for a small caliber); indeed, a true machine gun is essentially a fully automatic rifle, and often the primary criterion for a machine gun as opposed to a battle rifle is the presence of a quick-change barrel, heavyweight barrel, or other cooling system. Battle rifles and assault rifles may be capable of fully automatic fire, but are not designed for sustained fire. Many (though by no means all) machine guns also use belt feeding and open bolt operation, features not normally found on rifles.
In United States gun law, "machine gun" is a legal term for any weapon able to fire more than one shot per trigger pull regardless of caliber, the receiver of any such weapon, any weapon convertible to such a state using normal tools, or any component or part that will modify an existing firearm such that it functions as a "machine gun" such as a drop-in auto sear. Civilian possession of such weapons is not prohibited by any Federal law and not illegal in many states, but they must be registered as Title II weapons under the National Firearms Act and have a tax stamp paid. The Hughes Amendment to the Firearm Owners Protection Act of 1986 banned new production of firearms classified as machine guns for most civilian applications, however, so only "grandfathered" weapons produced before this date are legally transferable.
Overview of modern machine guns.
Unlike semi-automatic firearms, which require one trigger pull per round fired (usually due to the sear only resetting when the trigger is released), a machine gun is designed to fire for as long as the trigger is held down. Nowadays the term is restricted to relatively heavy weapons, able to provide continuous or frequent bursts of automatic fire for as long as ammunition lasts. Machine guns are normally used against personnel, aircraft and light vehicles, or to provide suppressive fire, either directly or indirectly. They are commonly mounted on fast attack vehicles such as technicals to provide heavy mobile firepower, armored vehicles such as tanks for engaging targets too small to justify use of the primary weaponry or too fast to effectively engage with it, and on aircraft as defensive armament or for strafing ground targets, though on fighter aircraft true machine guns have mostly been supplanted by large-caliber rotary guns.
Some machine guns have in practice sustained fire almost continuously for hours; other automatic weapons overheat after less than a minute of use. Because they become very hot, practically all machine guns fire from an open bolt, to permit air cooling from the breech between bursts. They also usually have either a barrel cooling system, slow-heating heavyweight barrel, or removable barrels which allow a hot barrel to be replaced.
Although subdivided into "light", "medium", "heavy" or "general-purpose", even the lightest machine guns tend to be substantially larger and heavier than standard infantry arms. Medium and heavy machine guns are either mounted on a tripod or on a vehicle; when carried on foot, the machine gun and associated equipment (tripod, ammunition, spare barrels) require additional crew members.
Light machine guns are designed to provide mobile fire support to a squad and are typically air-cooled weapons fitted with a box magazine or drum and a bipod; they may use full-size rifle rounds, but modern examples often use intermediate rounds. Medium machine guns use full-sized rifle rounds and are designed to be used from fixed positions mounted on a tripod. Heavy machine gun is a term originating in World War 1 to describe heavyweight medium machine guns and persisted into World War 2 with Japanese Hotchkiss M1914 clones; today, however, it is used to refer to automatic weapons with a caliber of at least .50in (12.7mm) but less than 20mm. A general-purpose machine gun is usually a lightweight medium machine gun which can either be used with a bipod and drum in the light machine gun role or a tripod and belt feed in the medium machine gun role.
US Army doctrine also includes the role of "squad automatic weapon" (SAW), a weapon which is used by a single crewman and is regarded as an "automatic rifle" rather than a machine gun, though the weapon itself may actually be a machine gun in functional terms. FM 3-22.68 "Crew-Served Machine Guns", describes how the M249 can be used either as a machine gun or as an automatic rifle: "Both the M249 automatic rifle and the M249 machine gun are identical, but its employment is different. The M249 automatic rifle is operated by an automatic rifleman, but its ammunition may be carried by other Soldiers within the squad or unit. The M249 machine gun is a crew-served weapon."
Machine guns usually have simple iron sights, though the use of optics is becoming more common. A common aiming system for direct fire is to alternate solid ("ball") rounds and tracer ammunition rounds (usually one tracer round for every four ball rounds), so shooters can see the trajectory and "walk" the fire into the target, and direct the fire of other soldiers.
Many heavy machine guns, such as the Browning M2 .50 caliber machine gun, are accurate enough to engage targets at great distances. During the Vietnam War, Carlos Hathcock set the record for a long-distance shot at 7382 ft (2250 m) with a .50 caliber heavy machine gun he had equipped with a telescopic sight. This led to the introduction of .50 caliber anti-materiel sniper rifles, such as the Barrett M82.
Other automatic weapons are subdivided into several categories based on the size of the bullet used, and whether the cartridge is fired from a positively locked closed bolt, or a non-positively locked open bolt. Fully automatic firearms using pistol-caliber ammunition are called machine pistols or submachine guns largely on the basis of size; those using shotgun cartridges are almost always referred to as automatic shotguns. The term personal defense weapon (PDW) is sometimes applied to weapons firing dedicated armor-piercing rounds which would otherwise be regarded as machine pistols or SMGs, but it is not particularly strongly defined and has historically been used to describe a range of weapons from ordinary SMGs to compact assault rifles. Selective fire rifles firing a full-power rifle cartridge from a closed bolt are called automatic rifles or battle rifles, while rifles that fire an intermediate cartridge are called assault rifles.
Assault rifles are a compromise between the size and weight of a pistol-caliber submachinegun and a full size battle rifle, firing intermediate cartridges and allowing semi-automatic and burst or full-automatic fire options (selective fire), sometimes with both of the latter present.
In certain states, like California, certain weapons that cosmetically resemble true assault rifles, but are only semi-automatic (autoloading), are categorized as assault weapons and possession by civilians is generally illegal. Supporters of gun rights generally consider this application of the phrase "assault weapon" to be a misnomer and this term is in fact seldom used outside of the United States for these civilian firearms.
Operation.
All machine guns follow a cycle:
Cycle is repeated as long as the trigger is activated by operator.
Releasing the trigger resets the trigger mechanism by engaging a sear so the weapon stops firing with bolt carrier fully at the rear.
The operation is basically the same for all semi automatic or automatic weapons, regardless of the means of activating these mechanisms. Some examples:
Firing a machine gun for prolonged periods produces large amounts of heat. In a worst-case scenario this may cause a cartridge to overheat and detonate even when the trigger is not pulled, potentially leading to damage or causing the gun to cycle its action and keep firing until it has exhausted its ammunition supply or jammed (this is known as "cook-off", distinct from "runaway fire" where the sear fails to disengage when the trigger is released). To prevent this, some kind of cooling system is required. Early machine guns were often water-cooled; while very effective, the water also added considerable weight to an already bulky design. Air-cooled machine guns often feature quick-change barrels, often carried by a crew member, passive cooling fins, or in some designs systems of forced-air cooling such as that employed by the Lewis Gun. The higher the rate of fire, the more often barrels must be changed and allowed to cool. To minimize this, most air-cooled guns are fired only in short bursts or at a reduced rate of fire. Some designs - such as the many variants of the MG42 - are capable of rates of fire in excess of 1,200 rounds per minute.
In weapons where the round seats and fires at the same time, mechanical timing is essential for operator safety, to prevent the round from firing before it is seated properly. Machine guns are controlled by one or more mechanical sears. When a sear is in place, it effectively stops the bolt at some point in its range of motion. Some sears stop the bolt when it is locked to the rear. Other sears stop the firing pin from going forward after the round is locked into the chamber. Almost all machine guns have a "safety" sear, which simply keeps the trigger from engaging.
History.
It would not be until the mid-19th century that successful machine-gun designs came into existence. The key characteristic of modern machine guns, their relatively high rate of fire and more importantly mechanical loading, came with the Model 1862 Gatling gun, which was adopted by the United States Navy. These weapons were still powered by hand; however, this changed with Hiram Maxim's idea of harnessing recoil energy to power reloading in his Maxim machine gun. Dr. Gatling also experimented with electric-motor-powered models; this externally powered machine reloading has seen use in modern weapons as well.
While technical use of the term "machine gun" has varied, the modern definition used by the Sporting Arms and Ammunition Manufacturers' Institute of America is "A fully automatic firearm that loads, fires and ejects continuously when the trigger is held to the rear until the ammunition is exhausted or pressure on the trigger is released." This definition excludes most early manually operated repeating arms such as volley guns and the Gatling gun.
Early rapid-firing weapons.
The first known ancestors of multi-shot weapons were medieval organ guns, while the first to have the ability to fire multiple shots from a single barrel without a full manual reload were revolvers made in Europe in the late 1500s. One is a shoulder-gun-length weapon made in Nuremberg, Germany, circa 1580. Another is a revolving arquebus, produced by Hans Stopler of Nuremberg in 1597.
True repeating long arms were difficult to manufacture prior to the development of the unitary firearm cartridge; nevertheless, lever-action repeating rifles such as the Kalthoff repeater and Cookson repeater were made in small quantities in the 17th century.
Another early revolving gun was created by James Puckle, a London lawyer, who patented what he called "The Puckle Gun" on May 15, 1718. It was a design for a manually operated 1.25 in. (32 mm) caliber, flintlock cannon with a revolver cylinder able to fire 6-11 rounds before reloading by swapping out the cylinder, intended for use on ships. It was one of the earliest weapons to be referred to as a machine gun, being called such in 1722, though its operation does not match the modern usage of the term. According to Puckle, it was able to fire round bullets at Christians and square bullets at Turks. However, it was a commercial failure and was not adopted or produced in any meaningful quantity.
In 1777, Philadelphia gunsmith Joseph Belton offered the Continental Congress a "new improved gun", which was capable of firing up to twenty shots in five seconds; unlike older repeaters using complex lever-action mechanisms, it used a simpler system of superposed loads, and was loaded with a single large paper cartridge. Congress requested that Belton modify 100 flintlock muskets to fire eight shots in this manner, but rescinded the order when Belton's price proved too high.
In the early and mid-19th century, a number of rapid-firing weapons appeared which offered multi-shot fire, mostly volley guns. Volley guns (such as the Mitrailleuse) and double barreled pistols relied on duplicating all parts of the gun, though the Nock gun used the otherwise-undesirable "chain fire" phenomenon (where multiple chambers are ignited at once) to propagate a spark from a single flintlock mechanism to multiple barrels. Pepperbox pistols also did away with needing multiple hammers but used multiple manually operated barrels. Revolvers further reduced this to only needing a pre-prepared cylinder and linked advancing the cylinder to cocking the hammer. However, these were still manually operated.
The Agar Gun, otherwise known as a "coffee-mill gun" because of its resemblance to a coffee mill, was invented by Wilson Agar at the beginning of the US Civil War. The weapon featured mechanized loading using a hand crank linked to a hopper above the weapon. The weapon featured a single barrel and fired through the turning of the same crank; it operated using paper cartridges fitted with percussion caps and inserted into metal tubes which acted as chambers; it was therefore functionally similar to a revolver. The weapon was demonstrated to President Lincoln in 1861. He was so impressed with the weapon that he purchased 10 on the spot for $1,500 apiece. The Union Army eventually purchased a total of 54 of the weapons. However, due to antiquated views of the Ordnance Department the weapons, like its more famous counterpart the Gatling Gun, saw only limited use.
The Gatling gun, patented in 1861 by Richard Jordan Gatling, was the first to offer controlled, sequential fire with mechanical loading. The design's key features were machine loading of prepared cartridges and a hand-operated crank for sequential high-speed firing. It first saw very limited action in the American Civil War; it was subsequently improved and used in the Franco-Prussian war and North-West Rebellion. Many were sold to other armies in the late 19th century and continued to be used into the early 20th century, until they were gradually supplanted by Maxim guns. Early multi-barrel guns were approximately the size and weight of contemporary artillery pieces, and were often perceived as a replacement for cannon firing grapeshot or canister shot. The large wheels required to move these guns around required a high firing position which increased the vulnerability of their crews. Sustained firing of gunpowder cartridges generated a cloud of smoke making concealment impossible until smokeless powder became available in the late 19th century. Gatling guns were targeted by artillery they could not reach and their crews were targeted by snipers they could not see. The Gatling gun was used most successfully to expand European colonial empires by killing warriors of non-industrialized societies.
In 1870 a Lt. D. H. Friberg of the Swedish army patented a fully automatic recoil-operated firearm action and may have produced firing prototypes of a derived design around 1882: this was the forerunner to the 1907 Kjellman machine gun, though due to rapid residue buildup from the use of black powder Friberg's design was not a practical weapon.
Maxim and World War 1.
The first practical self-powered machine gun was invented in 1884 by Sir Hiram Maxim. The "Maxim gun" used the recoil power of the previously fired bullet to reload rather than being hand-powered, enabling a much higher rate of fire than was possible using earlier designs such as the Nordenfelt and Gatling weapons. Maxim also introduced the use of water cooling, via a water jacket around the barrel, to reduce overheating. Maxim's gun was widely adopted and derivative designs were used on all sides during the First World War. The design required fewer crew and was lighter and more usable than the Nordenfelt and Gatling guns. First World War combat experience greatly increased the importance of the machine gun. The United States Army issued four machine guns per regiment in 1912, but that allowance increased to 336 machine guns per regiment by 1919.
Heavy guns based on the Maxim such as the Vickers machine gun were joined by many other machine weapons, which mostly had their start in the early 20th century such as the Hotchkiss machine gun. Submachine guns (e.g., the German MP18) as well as lighter machine guns (the Lewis gun, for example) saw their first major use in World War I, along with heavy use of large-caliber machine guns. The biggest single cause of casualties in World War I was actually artillery, but combined with wire entanglements, machine guns earned a fearsome reputation. The automatic mechanisms of machine guns were applied to handguns, giving rise to automatic pistols (and eventually machine pistols) such as the Borchardt (1890s) and later submachine guns (such as the Beretta 1918). Machine guns were mounted in aircraft for the first time in World War I. Firing through a moving propeller was solved in a variety of ways, including the interrupter gear, metal reinforcement of the propeller, or simply avoiding the problem with wing-mounted guns or having a pusher propeller.
Interwar era and World War II.
As better materials became available following the First World War, light machine guns became more readily portable; designs such as the Bren light machine gun replaced bulky predecessors like the Lewis gun in the squad support weapon role, while the modern division between medium machine guns like the M1919 Browning machine gun and heavy machine guns like the Browning M2 became clearer. Water jacket cooling systems were largely abandoned as unnecessary.
The interwar years also produced the first widely used and successful general-purpose machine gun, the German MG34. While this machine gun was equally able in the light and medium roles, it proved difficult to manufacture in quantity, and experts on industrial metalworking were called in to redesign the weapon for modern tooling, creating the MG42. This weapon was simpler, cheaper to produce, fired faster, and replaced the MG34 in every application except vehicle mounts, since the MG42's barrel changing system could not be operated when it was mounted.
Submachine guns evolved during the war, going from complex and finely made weapons like the Thompson submachine gun to weapons designed for mass-production and easy replacement like the Sten gun. On the Eastern Front, entire Soviet divisions were equipped with nothing but the PPSh-41 submachine gun. Experience in close-range city combat led to the German military desiring a weapon representing a compromise between the high fire volume of the SMG and the accuracy of a full-size rifle; after a false start with the FG 42, this led to the development of the MP44 select-fire assault rifle, the first weapon to be called such.
Cold War.
Experience with the MG42 led to the US issuing a requirement to replace the aging Browning Automatic Rifle with a similar weapon, which would also replace the M1919; simply using the MG42 itself was not possible, as the design brief required a weapon which could be fired from the hip or shoulder like the BAR. The resulting design, the M60 machine gun, was issued to troops during the Vietnam War.
As it became clear that a high-volume-of-fire weapon would be needed for fast-moving jet aircraft to reliably hit their opponents, Gatling's work with electrically powered weapons was recalled and the 20mm M61 Vulcan designed; a miniaturized 7.62mm version initially known as the "mini-Vulcan" and soon shorted to "minigun" was soon in production for use on helicopters, where the volume of fire could compensate for the instability of the helicopter as a firing platform.
Future.
Conventional machine gun development has been slowed by the fact that existing machine gun designs are adequate for most purposes, although significant developments are taking place with regard to caseless ammunition, antiarmor and antimissile weapons.
Human interface.
The most common interface on machine guns is a pistol grip and trigger. On earlier manual machine guns, the most common type was a hand crank. On externally powered machine guns, such as miniguns, an electronic button or trigger on a joystick is commonly used. Light machine guns often have a butt stock attached, while vehicle and tripod mounted machine guns usually have spade grips. In the late 20th century, scopes and other complex optics became more common as opposed to the more basic iron sights.
Loading systems in early manual machine guns were often from a hopper of loose (un-linked) cartridges. Manual-operated volley guns usually had to be reloaded manually all at once (each barrel reloaded by hand, or with a set of cartridges affixed to a plate that was inserted into the weapon). With hoppers, the rounds could often be added while the weapon was firing. This gradually changed to belt-fed types. Belts were either held in the open by the person, or in a bag or box. Some modern vehicle machine guns used linkless feed systems however.
Modern machine guns are commonly mounted in one of four ways. The first is a bipod – often these are integrated with the weapon. This is common on light machine guns and some medium machine guns. Another is a tripod, where the person holding it does not form a 'leg' of support. Medium and heavy machine guns usually use tripods. On ships, vehicles and aircraft machine guns are usually mounted on a pintle mount – basically a steel post that is connected to the frame or body. Tripod and pintle mounts are usually used with spade grips. The last major mounting type is one that is disconnected from humans, as part of an armament system, such as a tank coaxial or part of aircraft's armament. These are usually electrically fired and have complex sighting systems, for example the US Helicopter Armament Subsystems.

</doc>
