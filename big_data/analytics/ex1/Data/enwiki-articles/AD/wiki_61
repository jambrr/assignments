<doc id="22153" url="https://en.wikipedia.org/wiki?curid=22153" title="Nuclear power">
Nuclear power

Nuclear power is the use of nuclear reactions that release nuclear energy to generate heat, which most frequently is then used in steam turbines to produce electricity in a nuclear power plant. The term includes nuclear fission, nuclear decay and nuclear fusion. Presently, the nuclear fission of elements in the actinide series of the periodic table produce the vast majority of nuclear energy in the direct service of humankind, with nuclear decay processes, primarily in the form of geothermal energy, and radioisotope thermoelectric generators, in niche uses making up the rest.
In 2011 nuclear power provided 10% of the world's electricity In 2007, the IAEA reported there were 439 nuclear power reactors in operation in the world, operating in 31 countries. In Japan, all nuclear reactors were gradually shut down in the wake of the Fukushima Daiichi nuclear disaster to be assessed for safety. 25 reactors have applied for restart, and on August 11, 2015 Kyushu Electric Power restarted the number one reactor at its Sendai plant. In 2011 worldwide nuclear output fell by 4.3%, the largest decline on record, on the back of sharp declines in Japan (-44.3%) and Germany (-23.2%).
Nuclear fission power is a low carbon power generation method of producing electricity, and gives similar greenhouse gas emissions per unit of energy generated to renewable energy. As all electricity supplying technologies use cement etc., during construction, emissions are yet to be brought to zero. A 2014 analysis of the carbon footprint literature by the Intergovernmental Panel on Climate Change (IPCC) reported that fission electricities embodied total life-cycle emission intensity value of 12 g eq/kWh is the lowest out of all commercial baseload energy sources, and second lowest out of all commercial electricity technologies known, after wind power which is an Intermittent energy source with embodied greenhouse gas emissions, per unit of energy generated of 11 g eq/kWh. Each result is contrasted with coal & fossil gas at 820 and 490 g eq/kWh. With this translating into, from the beginning of Fission-electric power station commercialization in the 1970s, having prevented the emission of about 64 billion tonnes of carbon dioxide equivalent, greenhouse gases that would have otherwise resulted from the burning of fossil fuels in thermal power stations.
There is a social debate about nuclear power. Proponents, such as the World Nuclear Association and Environmentalists for Nuclear Energy, contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. Opponents, such as Greenpeace International and NIRS, contend that nuclear power poses many threats to people and the environment.
Far-reaching fission power reactor accidents, or accidents that resulted in medium to long-lived fission product contamination of inhabited areas, have occurred in Generation I & II reactor designs, blueprinted between 1950 and 1980. These include the Chernobyl disaster which occurred in 1986, the Fukushima Daiichi nuclear disaster (2011), and the more contained Three Mile Island accident (1979). There have also been some nuclear submarine accidents. In terms of lives lost per unit of energy generated, analysis has determined that fission-electric reactors have caused fewer fatalities per unit of energy generated than the other major sources of energy generation. Energy production from coal, petroleum, natural gas and hydroelectricity has caused a greater number of fatalities per unit of energy generated due to air pollution and energy accident effects. Four years after the Fukushima-Daiichi accident, there have been no fatalities due to exposure to radiation, and no discernible increased incidence of radiation-related health effects are expected among exposed members of the public and their descendants. The Japan Times estimated 1,600 deaths were the result of evacuation, due to physical and mental stress stemming from long stays at shelters, a lack of initial care as a result of hospitals being disabled by the disaster, and suicides.
History.
Origins.
In 1932 physicist Ernest Rutherford discovered that when lithium atoms were "split" by protons from a proton accelerator, immense amounts of energy were released in accordance with the principle of mass–energy equivalence. However, he and other nuclear physics pioneers Niels Bohr and Albert Einstein believed harnessing the power of the atom for practical purposes anytime in the near future was unlikely, with Rutherford labeling such expectations "moonshine."
The same year, his doctoral student James Chadwick discovered the neutron, which was immediately recognized as a potential tool for nuclear experimentation because of its lack of an electric charge. Experimentation with bombardment of materials with neutrons led Frédéric and Irène Joliot-Curie to discover induced radioactivity in 1934, which allowed the creation of radium-like elements at much less the price of natural radium. Further work by Enrico Fermi in the 1930s focused on using slow neutrons to increase the effectiveness of induced radioactivity. Experiments bombarding uranium with neutrons led Fermi to believe he had created a new, transuranic element, which was dubbed hesperium.
But in 1938, German chemists Otto Hahn and Fritz Strassmann, along with Austrian physicist Lise Meitner and Meitner's nephew, Otto Robert Frisch, conducted experiments with the products of neutron-bombarded uranium, as a means of further investigating Fermi's claims. They determined that the relatively tiny neutron split the nucleus of the massive uranium atoms into two roughly equal pieces, contradicting Fermi. This was an extremely surprising result: all other forms of nuclear decay involved only small changes to the mass of the nucleus, whereas this process—dubbed "fission" as a reference to biology—involved a complete rupture of the nucleus. Numerous scientists, including Leó Szilárd, who was one of the first, recognized that if fission reactions released additional neutrons, a self-sustaining nuclear chain reaction could result. Once this was experimentally confirmed and announced by Frédéric Joliot-Curie in 1939, scientists in many countries (including the United States, the United Kingdom, France, Germany, and the Soviet Union) petitioned their governments for support of nuclear fission research, just on the cusp of World War II, for the development of a nuclear weapon.
In the United States, where Fermi and Szilárd had both emigrated, this led to the creation of the first man-made reactor, known as Chicago Pile-1, which achieved criticality on December 2, 1942. This work became part of the Manhattan Project, which made enriched uranium and built large reactors to breed plutonium for use in the first nuclear weapons, which were used on the cities of Hiroshima and Nagasaki.
In 1945, the pocketbook "The Atomic Age" heralded the untapped atomic power in everyday objects and depicted a future where fossil fuels would go unused. One science writer, David Dietz, wrote that instead of filling the gas tank of your car two or three times a week, you will travel for a year on a pellet of atomic energy the size of a vitamin pill. Glenn Seaborg, who chaired the Atomic Energy Commission, wrote "there will be nuclear powered earth-to-moon shuttles, nuclear powered artificial hearts, plutonium heated swimming pools for SCUBA divers, and much more". These overly optimistic predications remain unfulfilled.
United Kingdom, Canada, and USSR proceeded over the course of the late 1940s and early 1950s. Electricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW. Work was also strongly researched in the US on nuclear marine propulsion, with a test reactor being developed by 1953 (eventually, the USS Nautilus, the first nuclear-powered submarine, would launch in 1955). In 1953, US President Dwight Eisenhower gave his "Atoms for Peace" speech at the United Nations, emphasizing the need to develop "peaceful" uses of nuclear power quickly. This was followed by the 1954 Amendments to the Atomic Energy Act which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector.
Early years.
On June 27, 1954, the USSR's Obninsk Nuclear Power Plant became the world's first nuclear power plant to generate electricity for a power grid, and produced around 5 megawatts of electric power.
Later in 1954, Lewis Strauss, then chairman of the United States Atomic Energy Commission (U.S. AEC, forerunner of the U.S. Nuclear Regulatory Commission and the United States Department of Energy) spoke of electricity in the future being "too cheap to meter". Strauss was very likely referring to hydrogen fusion —which was secretly being developed as part of Project Sherwood at the time—but Strauss's statement was interpreted as a promise of very cheap energy from nuclear fission. The U.S. AEC itself had issued far more realistic testimony regarding nuclear fission to the U.S. Congress only months before, projecting that "costs can be brought down... ... about the same as the cost of electricity from conventional sources..."
In 1955 the United Nations' "First Geneva Conference", then the world's largest gathering of scientists and engineers, met to explore the technology. In 1957 EURATOM was launched alongside the European Economic Community (the latter is now the European Union). The same year also saw the launch of the International Atomic Energy Agency (IAEA).
The world's first commercial nuclear power station, Calder Hall at Windscale, England, was opened in 1956 with an initial capacity of 50 MW (later 200 MW). The first commercial nuclear generator to become operational in the United States was the Shippingport Reactor (Pennsylvania, December 1957).
One of the first organizations to develop nuclear power was the U.S. Navy, for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, , was put to sea in December 1954. As of 2016, the U.S. Navy submarine fleet is made up entirely of nuclear-powered vessels, with 75 submarines in service. Two U.S. nuclear submarines, and , have been lost at sea. The Russian Navy is currently (2016) estimated to have 61 nuclear submarines in service; eight Soviet and Russian nuclear submarines have been lost at sea. This includes the Soviet submarine K-19 reactor accident in 1961 which resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The Soviet submarine K-27 reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. Moreover, Soviet submarine K-429 sank twice, but was raised after each incident. Several serious nuclear and radiation accidents have involved nuclear submarine mishaps.
The U.S. Army also had a nuclear power program, beginning in 1954. The SM-1 Nuclear Power Plant, at Fort Belvoir, Virginia, was the first power reactor in the U.S. to supply electrical energy to a commercial grid (VEPCO), in April 1957, before Shippingport. The SL-1 was a U.S. Army experimental nuclear power reactor at the National Reactor Testing Station in eastern Idaho. It underwent a steam explosion and meltdown in January 1961, which killed its three operators. In Soviet Union in The Mayak Production Association there were a number of accidents including an explosion that released 50-100 tonnes of high-level radioactive waste, contaminating a huge territory in the eastern Urals and causing numerous deaths and injuries. The Soviet regime kept this accident secret for about 30 years. The event was eventually rated at 6 on the seven-level INES scale (third in severity only to the disasters at Chernobyl and Fukushima).
Development.
Installed nuclear capacity initially rose relatively quickly, rising from less than 1 gigawatt (GW) in 1960 to 100 GW in the late 1970s, and 300 GW in the late 1980s. Since the late 1980s worldwide capacity has risen much more slowly, reaching 366 GW in 2005. Between around 1970 and 1990, more than 50 GW of capacity was under construction (peaking at over 150 GW in the late 1970s and early 1980s) — in 2005, around 25 GW of new capacity was planned. More than two-thirds of all nuclear plants ordered after January 1970 were eventually cancelled. A total of 63 nuclear units were canceled in the USA between 1975 and 1980.
During the 1970s and 1980s rising economic costs (related to extended construction times largely due to regulatory changes and pressure-group litigation) and falling fossil fuel prices made nuclear power plants then under construction less attractive. In the 1980s (U.S.) and 1990s (Europe), flat load growth and electricity liberalization also made the addition of large new baseload capacity unattractive.
The 1973 oil crisis had a significant effect on countries, such as France and Japan, which had relied more heavily on oil for electric generation (39% and 73% respectively) to invest in nuclear power.
Some local opposition to nuclear power emerged in the early 1960s, and in the late 1960s some members of the scientific community began to express their concerns. These concerns related to nuclear accidents, nuclear proliferation, high cost of nuclear power plants, nuclear terrorism and radioactive waste disposal. In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975 and anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America. By the mid-1970s anti-nuclear activism had moved beyond local protests and politics to gain a wider appeal and influence, and nuclear power became an issue of major public protest. Although it lacked a single co-ordinating organization, and did not have uniform goals, the movement's efforts gained a great deal of attention. In some countries, the nuclear power conflict "reached an intensity unprecedented in the history of technology controversies". 
In France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn. In May 1979, an estimated 70,000 people, including then governor of California Jerry Brown, attended a march and rally against nuclear power in Washington, D.C. Anti-nuclear power groups emerged in every country that has had a nuclear power programme.
Three mile Island and Chernobyl.
Health and safety concerns, the 1979 accident at Three Mile Island, and the 1986 Chernobyl disaster played a part in stopping new plant construction in many countries, although the public policy organization, the Brookings Institution states that new nuclear units, at the time of publishing in 2006, had not been built in the U.S. because of soft demand for electricity, and cost overruns on nuclear plants due to regulatory issues and construction delays. By the end of the 1970s it became clear that nuclear power would not grow nearly as dramatically as once believed. Eventually, more than 120 reactor orders in the U.S. were ultimately cancelled and the construction of new reactors ground to a halt. A cover story in the February 11, 1985, issue of "Forbes" magazine commented on the overall failure of the U.S. nuclear power program, saying it "ranks as the largest managerial disaster in business history".
Unlike the Three Mile Island accident, the much more serious Chernobyl accident did not increase regulations affecting Western reactors since the Chernobyl reactors were of the problematic RBMK design only used in the Soviet Union, for example lacking "robust" containment buildings. Many of these RBMK reactors are still in use today. However, changes were made in both the reactors themselves (use of a safer enrichment of uranium) and in the control system (prevention of disabling safety systems), amongst other things, to reduce the possibility of a duplicate accident.
An international organization to promote safety awareness and professional development on operators in nuclear facilities was created: WANO; World Association of Nuclear Operators.
Opposition in Ireland and Poland prevented nuclear programs there, while Austria (1978), Sweden (1980) and Italy (1987) (influenced by Chernobyl) voted in referendums to oppose or phase out nuclear power. In July 2009, the Italian Parliament passed a law that cancelled the results of an earlier referendum and allowed the immediate start of the Italian nuclear program. After the Fukushima Daiichi nuclear disaster a one-year moratorium was placed on nuclear power development, followed by a referendum in which over 94% of voters (turnout 57%) rejected plans for new nuclear power.
Nuclear renaissance.
[[File:OL3.jpg|thumb|250px|Olkiluoto 3 under construction in 2009. It is the first EPR design, but problems with workmanship and supervision have created costly delays which led to an inquiry by the Finnish nuclear regulator STUK.
In December 2012, Areva estimated that the full cost of building the reactor will be about €8.5 billion, or almost three times the original delivery price of €3 billion.]]
Since about 2001 the term "nuclear renaissance" has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. In 2012, the World Nuclear Association reported that nuclear electricity generation was at its lowest level since 1999. As of January 2016, however, 65 new nuclear power reactors were under construction. Over 150 were planned, equivalent to nearly half of capacity at that time.
Fukushima Daiichi Nuclear Disaster.
Japan's 2011 Fukushima Daiichi nuclear accident, which occurred in a reactor design from the 1960s, prompted a re-examination of nuclear safety and nuclear energy policy in many countries. Germany plans to close all its reactors by 2022, and Italy has re-affirmed its ban on electric utilities generating, but not importing, fission derived electricity. In 2011 the International Energy Agency halved its prior estimate of new generating capacity to be built by 2035. In 2013 Japan signed a deal worth $22 billion, in which Mitsubishi Heavy Industries would build four modern "Atmea" reactors for Turkey. In August 2015, following 4 years of near zero fission-electricity generation, Japan began restarting its fission fleet, after safety upgrades were completed, beginning with Sendai fission-electric station.
In March 2011 the nuclear emergencies at Japan's Fukushima Daiichi Nuclear Power Plant and shutdowns at other nuclear facilities raised questions among some commentators over the future of the renaissance. China, Germany, Switzerland, Israel, Malaysia, Thailand, United Kingdom, Italy and the Philippines have reviewed their nuclear power programs. Indonesia and Vietnam still plan to build nuclear power plants.
The World Nuclear Association has said that "nuclear power generation suffered its biggest ever one-year fall through 2012 as the bulk of the Japanese fleet remained offline for a full calendar year". Data from the International Atomic Energy Agency showed that nuclear power plants globally produced 2346 TWh of electricity in 2012 – seven per cent less than in 2011. The figures illustrate the effects of a full year of 48 Japanese power reactors producing no power during the year. The permanent closure of eight reactor units in Germany was also a factor. Problems at Crystal River, Fort Calhoun and the two San Onofre units in the USA meant they produced no power for the full year, while in Belgium Doel 3 and Tihange 2 were out of action for six months. Compared to 2010, the nuclear industry produced 11% less electricity in 2012.
Post-Fukushima controversy.
The Fukushima Daiichi nuclear accident sparked controversy about the import of the accident and its effect on nuclear's future. IAEA Director General Yukiya Amano said the Japanese nuclear accident "caused deep public anxiety throughout the world and damaged confidence in nuclear power", and the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035. But by 2015, the Agency's outlook had become more promising. "Nuclear power is a critical element in limiting greenhouse gas emissions," the agency noted, and "the prospects for nuclear energy remain positive in the medium to long term despite a negative impact in some countries in the aftermath of the [Fukushima-Daiichi] accident...it is still the second-largest source worldwide of low-carbon electricity. And the 72 reactors under construction at the start of last year were the most in 25 years." Though Platts reported in 2011 that "the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world", Progress Energy Chairman/CEO Bill Johnson made the observation that "Today there’s an even more compelling case that greater use of nuclear power is a vital part of a balanced energy strategy". In 2011, "The Economist" opined that nuclear power "looks dangerous, unpopular, expensive and risky", and that "it is replaceable with relative ease and could be forgone with no huge structural shifts in the way the world works". Earth Institute Director Jeffrey Sachs disagreed, claiming combating climate change would require an expansion of nuclear power. "We won't meet the carbon targets if nuclear is taken off the table," he said. "We need to understand the scale of the challenge."
Investment banks were critical of nuclear soon after the accident. Many disputed their impartiality, however, due to significant investments in renewable energy, perceived by some as a valid alternative to nuclear. In early April 2011, analysts at Swiss-based investment bank UBS said: "At Fukushima, four reactors have been out of control for weeks, casting doubt on whether even an advanced economy can master nuclear safety...we believe the Fukushima accident was the most serious ever for the credibility of nuclear power". UBS has helped to raise more than $20 billion since 2006 and advised on more than a dozen deals for renewable energy and cleantech companies. Deutsche Bank advised that "the global impact of the Fukushima accident is a fundamental shift in public perception with regard to how a nation prioritizes and values its populations health, safety, security, and natural environment when determining its current and future energy pathways...renewable energy will be a clear long-term winner in most energy systems, a conclusion supported by many voter surveys conducted over the past few weeks. Deutsche Bank has over €1 billion in capital invested in renewables projects in Europe, North & South America, and Asia.
Manufacturers also recognized a profit opportunity in negative public perceptions about nuclear. In September 2011, German engineering giant Siemens announced it will withdraw entirely from the nuclear industry, as a response to the Fukushima nuclear accident in Japan, and said that it would no longer build nuclear power plants anywhere in the world. The company’s chairman, Peter Löscher, said that "Siemens was ending plans to cooperate with Rosatom, the Russian state-controlled nuclear power company, in the construction of dozens of nuclear plants throughout Russia over the coming two decades". Renewable energy is a core component of Siemens's profit base. In February, 2016 the firm proposed a €10 billion renewable energy investment in Egypt.
In February 2012, the United States Nuclear Regulatory Commission approved the construction of two additional reactors at the Vogtle Electric Generating Plant, the first reactors to be approved in over 30 years since the Three Mile Island accident, but NRC Chairman Gregory Jaczko cast a dissenting vote citing safety concerns stemming from Japan's 2011 Fukushima nuclear disaster, and saying "I cannot support issuing this license as if Fukushima never happened". Jaczko resigned in April 2012. One week after Southern received the license to begin major construction on the two new reactors, a dozen environmental and anti-nuclear groups sued to stop the Plant Vogtle expansion project, saying "public safety and environmental problems since Japan's Fukushima Daiichi nuclear reactor accident have not been taken into account". In July 2012, the suit was rejected by the Washington, D.C. Circuit Court of Appeals.
Countries such as Australia, Austria, Denmark, Greece, Ireland, Italy, Latvia, Liechtenstein, Luxembourg, Malta, Portugal, Israel, Malaysia, New Zealand, and Norway have no nuclear power reactors and remain opposed to nuclear power. However, by contrast, some countries remain in favor, and financially support nuclear fusion research, including EU wide funding of the ITER project.
Industry.
Capacity and production.
Nuclear power capacity remained relatively stable between the mid 1980s until the accident at the Fukushima Daiichi reactor in March 2011. In June 2015, Platts reported
global nuclear generation increased by 1% in 2014, the first annual increase since Fukushima.
The United States produces the most nuclear energy, with nuclear power providing 19% of the electricity it consumes, while France produces the highest percentage of its electrical energy from nuclear reactors—80% as of 2006. In the European Union as a whole, nuclear energy provides 30% of the electricity. Nuclear energy policy differs among European Union countries, and some, such as Austria, Estonia, Ireland and Italy, have no active nuclear power stations. In comparison, France has a large number of these plants, with 16 multi-unit stations in current use.
Many military and some civilian (such as some icebreaker) ships use nuclear marine propulsion, a form of nuclear propulsion. A few space vehicles have been launched using full-fledged nuclear reactors: 33 reactors belong to the Soviet RORSAT series and one was the American SNAP-10A.
International research is continuing into safety improvements such as passively safe plants, the use of nuclear fusion, and additional uses of process heat such as hydrogen production (in support of a hydrogen economy), for desalinating sea water, and for use in district heating systems.
Nuclear (fission) power stations, excluding the contribution from naval nuclear fission reactors, provided 11% of the world's electricity in 2012, somewhat less than that generated by hydro-electric stations at 16%. Since electricity accounts for about 25% of humanity's energy usage with the majority of the rest coming from fossil fuel reliant sectors such as transport, manufacture and home heating, nuclear fission's contribution to the global final energy consumption is about 2.5%, a little more than the combined global electricity production from "new renewables"; wind, solar, biofuel and geothermal power, which together provided 2% of global final energy consumption in 2014.
Regional differences in the use of fission energy are large. Fission energy generation, with a 20% share of the U.S. electricity production, is the single largest deployed technology among current low-carbon power sources in the country. In addition, two-thirds of the European Union's twenty-seven nations' low-carbon energy is produced by fission. Some of these nations have banned its generation, such as Italy, which ended the use of fission-electric generation, which started in 1963, in 1990. France is the largest user of nuclear energy, deriving 75% of its electricity from fission.
In 2013, the IAEA reported that there were 437 operational civil fission-electric reactors in 31 countries, although not every reactor was producing electricity. In addition, there were approximately 140 naval vessels using nuclear propulsion in operation, powered by some 180 reactors. As of 2013, attaining a net energy gain from sustained nuclear fusion reactions, excluding natural fusion power sources such as the Sun, remains an ongoing area of international physics and engineering research. With commercial fusion power production remaining unlikely before 2050.
Since commercial nuclear energy began in the mid-1950s, 2008 was the first year that no new nuclear power plant was connected to the grid, although two were connected in 2009.
In 2015, the IAEA reported that worldwide there were 67 civil fission-electric power reactors under construction in 15 countries including Gulf states such as the United Arab Emirates (UAE). Over half of the 67 total being built were in Asia, with 28 in China. Eight new grid connections were completed by China in 2015 and the most recently completed reactor to be connected to the electrical grid, as of January 2016, was at the Kori Nuclear Power Plant in the Republic of Korea. In the US, four new Generation III reactors were under construction at Vogtle and Summer station, while a fifth was nearing completion at Watts Bar station, all five were expected to become operational before 2020. In 2013, four aging uncompetitive U.S reactors were closed. According to the World Nuclear Association, the global trend is for new nuclear power stations coming online to be balanced by the number of old plants being retired.
Economics.
Nuclear power plants typically have high capital costs for building the plant, but low fuel costs. Although nuclear power plants can vary their output the electricity is generally less favorably priced when doing so. Nuclear power plants are therefore typically run as much as possible to keep the cost of the generated electrical energy as low as possible, supplying mostly base-load electricity.
Internationally the price of nuclear plants rose 15% annually in 1970-1990. Yet, nuclear power has total costs in 2012 of about $96 per megawatt hour (MWh), most of which involves capital construction costs, compared with solar power at $130 per MWh, and natural gas at the low end at $64 per MWh.
In 2015, the "Bulletin of the Atomic Scientists" unveiled the Nuclear Fuel Cycle Cost Calculator, an online tool that estimates the full cost of electricity produced by three configurations of the nuclear fuel cycle. Two years in the making, this interactive calculator is the first generally accessible model to provide a nuanced look at the economic costs of nuclear power; it lets users test how sensitive the price of electricity is to a full range of components—more than 60 parameters that can be adjusted for the three configurations of the nuclear fuel cycle considered by this tool (once-through, limited-recycle, full-recycle). Users can select the fuel cycle they would like to examine, change cost estimates for each component of that cycle, and even choose uncertainty ranges for the cost of particular components. This approach allows users around the world to compare the cost of different nuclear power approaches in a sophisticated way, while taking account of prices relevant to their own countries or regions.
In recent years there has been a slowdown of electricity demand growth. In Eastern Europe, a number of long-established projects are struggling to find finance, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where the electricity market is competitive, cheap natural gas is available, and its future supply relatively secure, this also poses a major problem for nuclear projects and existing plants.
Analysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, accident liability and other factors were borne by consumers rather than suppliers. In addition, because the potential liability from a nuclear accident is so great, the full cost of liability insurance is generally limited/capped by the government, which the U.S. Nuclear Regulatory Commission concluded constituted a significant subsidy. Many countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.
Following the 2011 Fukushima Daiichi nuclear disaster, costs are expected to increase for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats.
The economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multibillion-dollar investments ride on the choice of an energy source. Comparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants as well as the future costs of fossil fuels and renewables as well as for energy storage solutions for intermittent power sources. Cost estimates also need to take into account plant decommissioning and nuclear waste storage costs. On the other hand, measures to mitigate global warming, such as a carbon tax or carbon emissions trading, may favor the economics of nuclear power.
Nuclear power organizations.
There are multiple organizations which have taken a position on nuclear power and the nuclear power industry– some are proponents, and some are opponents.
Proponents.
The majority of pro-nuclear energy organizations and associations is either industry-supported or directly formed from industry members as advocacy groups or trade associations.
Future of the industry.
The nuclear power industry in western nations has a history of construction delays, cost overruns, plant cancellations, and nuclear safety issues despite significant government subsidies and support. In December 2013, "Forbes" magazine reported that, in developed countries, "reactors are not a viable source of new power". Even in developed nations where they make economic sense, they are not feasible because nuclear’s "enormous costs, political and popular opposition, and regulatory uncertainty". This view echoes the statement of former Exelon CEO John Rowe, who said in 2012 that new nuclear plants "don’t make any sense right now" and won’t be economically viable in the foreseeable future. John Quiggin, economics professor, also says the main problem with the nuclear option is that it is not economically-viable. Quiggin says that we need more efficient energy use and more renewable energy commercialization. Former NRC member Peter Bradford and Professor Ian Lowe have recently made similar statements. However, some "nuclear cheerleaders" and lobbyists in the West continue to champion reactors, often with proposed new but largely untested designs, as a source of new power.
Much more new build activity is occurring in developing countries like South Korea, India and China. In March 2016, China had 30 reactors in operation, 24 under construction and plans to build more, However, according to a government research unit, China must not build "too many nuclear power reactors too quickly", in order to avoid a shortfall of fuel, equipment and qualified plant workers.
In the US, licenses of almost half its reactors have been extended to 60 years, Two new Generation III reactors are under construction at Vogtle, a dual construction project which marks the end of a 34-year period of stagnation in the US construction of civil nuclear power reactors. The station operator licenses of almost half the present 104 power reactors in the US, as of 2008, have been given extensions to 60 years. As of 2012, U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed. Relevant state legislatures are trying to close Vermont Yankee and Indian Point Nuclear Power Plant.
The U.S. NRC and the U.S. Department of Energy have initiated research into Light water reactor sustainability which is hoped will lead to allowing extensions of reactor licenses beyond 60 years, provided that safety can be maintained, as the loss in non-CO2-emitting generation capacity by retiring reactors "may serve to challenge U.S. energy security, potentially resulting in increased greenhouse gas emissions, and contributing to an imbalance between electric supply and demand."
There is a possible impediment to production of nuclear power plants as only a few companies worldwide have the capacity to forge single-piece reactor pressure vessels, which are necessary in the most common reactor designs. Utilities across the world are submitting orders years in advance of any actual need for these vessels. Other manufacturers are examining various options, including making the component themselves, or finding ways to make a similar item using alternate methods.
According to the World Nuclear Association, globally during the 1980s one new nuclear reactor started up every 17 days on average, and in the year 2015 it was estimated that this rate could in theory eventually increase to one every 5 days, although no plans exist for that. As of 2007, Watts Bar 1 in Tennessee, which came on-line on February 7, 1996, was the last U.S. commercial nuclear reactor to go on-line. This is often quoted as evidence of a successful worldwide campaign for nuclear power phase-out. Electricity shortages, fossil fuel price increases, global warming, and heavy metal emissions from fossil fuel use, new technology such as passively safe plants, and national energy security may renew the demand for nuclear power plants.
Nuclear power plant.
Just as many conventional thermal power stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear power plants convert the energy released from the nucleus of an atom via nuclear fission that takes place in a nuclear reactor. The heat is removed from the reactor core by a cooling system that uses the heat to generate steam, which drives a steam turbine connected to a generator producing electricity.
Life cycle of nuclear fuel.
A nuclear reactor is only part of the life-cycle for nuclear power. The process starts with mining (see "Uranium mining"). Uranium mines are underground, open-pit, or in-situ leach mines. In any case, the uranium ore is extracted, usually converted into a stable and compact form such as yellowcake, and then transported to a processing facility. Here, the yellowcake is converted to uranium hexafluoride, which is then enriched using various techniques. At this point, the enriched uranium, containing more than the natural 0.7% U-235, is used to make rods of the proper composition and geometry for the particular reactor that the fuel is destined for. The fuel rods will spend about 3 operational cycles (typically 6 years total now) inside the reactor, generally until about 3% of their uranium has been fissioned, then they will be moved to a spent fuel pool where the short lived isotopes generated by fission can decay away. After about 5 years in a spent fuel pool the spent fuel is radioactively and thermally cool enough to handle, and it can be moved to dry storage casks or reprocessed.
Conventional fuel resources.
Uranium is a fairly common element in the Earth's crust. Uranium is approximately as common as tin or germanium in the Earth's crust, and is about 40 times more common than silver. Uranium is a constituent of most rocks, dirt, and of the oceans. The fact that uranium is so spread out is a problem because mining uranium is only economically feasible where there is a large concentration. Still, the world's present measured resources of uranium, economically recoverable at a price of 130 USD/kg, are enough to last for between 70 and 100 years.
According to the OECD in 2006, there is an expected 85 years worth of uranium in identified resources, when that uranium is used in present reactor technology, with 670 years of economically recoverable uranium in total conventional resources and phosphate ores, while also using present reactor technology, a resource that is recoverable from between 60-100 US$/kg of Uranium. The OECD have noted that: For example, the OECD have determined that with a pure fast reactor fuel cycle with a burn up of, and recycling of, all the Uranium and actinides, actinides which presently make up the most hazardous substances in nuclear waste, there is 160,000 years worth of Uranium in total conventional resources and phosphate ore. According to the OECD's red book in 2011, due to increased exploration, known uranium resources have grown by 12.5% since 2008, with this increase translating into greater than a century of uranium available if the metals usage rate were to continue at the 2011 level.
Current light water reactors make relatively inefficient use of nuclear fuel, fissioning only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable, and more efficient reactor designs, such as the currently under construction Generation III reactors achieve a higher efficiency burn up of the available resources, than the current vintage generation II reactors, which make up the vast majority of reactors worldwide.
Breeding.
As opposed to current light water reactors which use uranium-235 (0.7% of all natural uranium), fast breeder reactors use uranium-238 (99.3% of all natural uranium). It has been estimated that there is up to five billion years' worth of uranium-238 for use in these power plants.
Breeder technology has been used in several reactors, but the high cost of reprocessing fuel safely, at 2006 technological levels, requires uranium prices of more than 200 USD/kg before becoming justified economically. Breeder reactors are still however being pursued as they have the potential to burn up all of the actinides in the present inventory of nuclear waste while also producing power and creating additional quantities of fuel for more reactors via the breeding process. In 2005, there were two breeder reactors producing power: the Phénix in France, which has since powered down in 2009 after 36 years of operation, and the BN-600 reactor, a reactor constructed in 1980 Beloyarsk, Russia which is still operational as of 2013. The electricity output of BN-600 is 600 MW — Russia plans to expand the nation's use of breeder reactors with the BN-800 reactor, was scheduled to become operational in 2014, but due to delays is not scheduled to produce power until 2017. The technical design of a yet larger breeder, the BN-1200 reactor was originally scheduled to be finalized in 2013, with construction slated for 2015 but has also been delayed. Japan's Monju breeder reactor restarted (having been shut down in 1995) in 2010 for 3 months, but shut down again after equipment fell into the reactor during reactor checkups, it is planned to become re-operational in late 2013. Both China and India are building breeder reactors. With the Indian 500 MWe Prototype Fast Breeder Reactor scheduled to become operational in 2014, with plans to build five more by 2020. The China Experimental Fast Reactor began producing power in 2011.
Another alternative to fast breeders is thermal breeder reactors that use uranium-233 bred from thorium as fission fuel in the thorium fuel cycle. Thorium is about 3.5 times more common than uranium in the Earth's crust, and has different geographic characteristics. This would extend the total practical fissionable resource base by 450%. India's three-stage nuclear power programme features the use of a thorium fuel cycle in the third stage, as it has abundant thorium reserves but little uranium.
Solid waste.
The most important waste stream from nuclear power plants is spent nuclear fuel. It is primarily composed of unconverted uranium as well as significant quantities of transuranic actinides (plutonium and curium, mostly). In addition, about 3% of it is fission products from nuclear reactions. The actinides (uranium, plutonium, and curium) are responsible for the bulk of the long-term radioactivity, whereas the fission products are responsible for the bulk of the short-term radioactivity.
High-level radioactive waste.
High-level radioactive waste management concerns management and disposal of highly radioactive materials created during production of nuclear power. The technical issues in accomplishing this are daunting, due to the extremely long periods radioactive wastes remain deadly to living organisms. Of particular concern are two long-lived fission products, Technetium-99 (half-life 220,000 years) and Iodine-129 (half-life 15.7 million years), which dominate spent nuclear fuel radioactivity after a few thousand years. The most troublesome transuranic elements in spent fuel are Neptunium-237 (half-life two million years) and Plutonium-239 (half-life 24,000 years). Consequently, high-level radioactive waste requires sophisticated treatment and management to successfully isolate it from the biosphere. This usually necessitates treatment, followed by a long-term management strategy involving permanent storage, disposal or transformation of the waste into a non-toxic form.
Governments around the world are considering a range of waste management and disposal options, usually involving deep-geologic placement, although there has been limited progress toward implementing long-term waste management solutions. This is partly because the timeframes in question when dealing with radioactive waste range from 10,000 to millions of years, according to studies based on the effect of estimated radiation doses.
Some proposed nuclear reactor designs however such as the American Integral Fast Reactor and the Molten salt reactor can use the nuclear waste from light water reactors as a fuel, transmutating it to isotopes that would be safe after hundreds, instead of tens of thousands of years. This offers a potentially more attractive alternative to deep geological disposal.
Another possibility is the use of thorium in a reactor especially designed for thorium (rather than mixing in thorium with uranium and plutonium (i.e. in existing reactors). Used thorium fuel remains only a few hundreds of years radioactive, instead of tens of thousands of years.
Since the fraction of a radioisotope's atoms decaying per unit of time is inversely proportional to its half-life, the relative radioactivity of a quantity of buried human radioactive waste would diminish over time compared to natural radioisotopes (such as the decay chains of 120 trillion tons of thorium and 40 trillion tons of uranium which are at relatively trace concentrations of parts per million each over the crust's 3 * 1019 ton mass). For instance, over a timeframe of thousands of years, after the most active short half-life radioisotopes decayed, burying U.S. nuclear waste would increase the radioactivity in the top 2000 feet of rock and soil in the United States (10 million km2) by ≈ 1 part in 10 million over the cumulative amount of natural radioisotopes in such a volume, although the vicinity of the site would have a far higher concentration of artificial radioisotopes underground than such an average.
Low-level radioactive waste.
The nuclear industry also produces a large volume of low-level radioactive waste in the form of contaminated items like clothing, hand tools, water purifier resins, and (upon decommissioning) the materials of which the reactor itself is built. In the US, the Nuclear Regulatory Commission has repeatedly attempted to allow low-level materials to be handled as normal waste: landfilled, recycled into consumer items, etcetera.
Comparing radioactive waste to industrial toxic waste.
In countries with nuclear power, radioactive wastes comprise less than 1% of total industrial toxic wastes, much of which remains hazardous for long periods. Overall, nuclear power produces far less waste material by volume than fossil-fuel based power plants. Coal-burning plants are particularly noted for producing large amounts of toxic and mildly radioactive ash due to concentrating naturally occurring metals and mildly radioactive material from the coal. A 2008 report from Oak Ridge National Laboratory concluded that coal power actually results in more radioactivity being released into the environment than nuclear power operation, and that the population effective dose equivalent, or dose to the public from radiation from coal plants is 100 times as much as from the ideal operation of nuclear plants. Indeed, coal ash is much less radioactive than spent nuclear fuel on a weight per weight basis, but coal ash is produced in much higher quantities per unit of energy generated, and this is released directly into the environment as fly ash, whereas nuclear plants use shielding to protect the environment from radioactive materials, for example, in dry cask storage vessels.
Waste disposal.
Disposal of nuclear waste is often said to be the Achilles' heel of the industry. Presently, waste is mainly stored at individual reactor sites and there are over 430 locations around the world where radioactive material continues to accumulate. Some experts suggest that centralized underground repositories which are well-managed, guarded, and monitored, would be a vast improvement. There is an "international consensus on the advisability of storing nuclear waste in deep geological repositories", with the lack of movement of nuclear waste in the 2 billion year old natural nuclear fission reactors in Oklo, Gabon being cited as "a source of essential information today."
As of 2009 there were no commercial scale purpose built underground repositories in operation. The Waste Isolation Pilot Plant in New Mexico has been taking nuclear waste since 1999 from production reactors, but as the name suggests is a research and development facility.
Reprocessing.
Reprocessing can potentially recover up to 95% of the remaining uranium and plutonium in spent nuclear fuel, putting it into new mixed oxide fuel. This produces a reduction in long term radioactivity within the remaining waste, since this is largely short-lived fission products, and reduces its volume by over 90%. Reprocessing of civilian fuel from power reactors is currently done in Britain, France and (formerly) Russia, soon will be done in China and perhaps India, and is being done on an expanding scale in Japan. The full potential of reprocessing has not been achieved because it requires breeder reactors, which are not commercially available. France is generally cited as the most successful reprocessor, but it presently only recycles 28% (by mass) of the yearly fuel use, 7% within France and another 21% in Russia.
Reprocessing is not allowed in the U.S. The Obama administration has disallowed reprocessing of nuclear waste, citing nuclear proliferation concerns. In the U.S., spent nuclear fuel is currently all treated as waste.
Depleted uranium.
Uranium enrichment produces many tons of depleted uranium (DU) which consists of U-238 with most of the easily fissile U-235 isotope removed. U-238 is a tough metal with several commercial uses—for example, aircraft production, radiation shielding, and armor—as it has a higher density than lead. Depleted uranium is also controversially used in munitions; DU penetrators (bullets or APFSDS tips) "self sharpen", due to uranium's tendency to fracture along shear bands.
Accidents and safety, the human and financial costs.
Some serious nuclear and radiation accidents have occurred. Benjamin K. Sovacool has reported that worldwide there have been 99 accidents at nuclear power plants. Fifty-seven accidents have occurred since the Chernobyl disaster, and 57% (56 out of 99) of all nuclear-related accidents have occurred in the USA.
Nuclear power plant accidents include the Chernobyl accident (1986) with approximately 60 deaths so far attributed to the accident and a predicted, eventual total death toll, of from 4000 to 25,000 latent cancers deaths. The Fukushima Daiichi nuclear disaster (2011), has not caused any radiation related deaths, with a predicted, eventual total death toll, of from 0 to 1000, and the Three Mile Island accident (1979), no causal deaths, cancer or otherwise, have been found in follow up studies of this accident. Nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985). International research is continuing into safety improvements such as passively safe plants, and the possible future use of nuclear fusion.
In terms of lives lost per unit of energy generated, nuclear power has caused fewer accidental deaths per unit of energy generated than all other major sources of energy generation. Energy produced by coal, petroleum, natural gas and hydropower has caused more deaths per unit of energy generated, from air pollution and energy accidents. This is found in the following comparisons, when the immediate nuclear related deaths from accidents are compared to the immediate deaths from these other energy sources, when the latent, or predicted, indirect cancer deaths from nuclear energy accidents are compared to the immediate deaths from the above energy sources, and when the combined immediate and indirect fatalities from nuclear power and all fossil fuels are compared, fatalities resulting from the mining of the necessary natural resources to power generation and to air pollution. With these data, the use of nuclear power has been calculated to have prevented in the region of 1.8 million deaths between 1971 and 2009, by reducing the proportion of energy that would otherwise have been generated by fossil fuels, and is projected to continue to do so.
Although according to Benjamin K. Sovacool, fission energy accidents ranked first in terms of their total economic cost, accounting for 41 percent of all property damage attributed to energy accidents. Analysis presented in the international Journal, "Human and Ecological Risk Assessment" found that coal, oil, Liquid petroleum gas and hydroelectric accidents(primarily due to the Banqiao dam burst) have resulted in greater economic impacts than nuclear power accidents.
Following the 2011 Japanese Fukushima nuclear disaster, authorities shut down the nation's 54 nuclear power plants, but it has been estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life. As of 2013, the Fukushima site remains highly radioactive, with some 160,000 evacuees still living in temporary housing, and some land will be unfarmable for centuries. The difficult Fukushima disaster cleanup will take 40 or more years, and cost tens of billions of dollars.
Forced evacuation from a nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. A comprehensive 2005 study concluded that "the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date". Frank N. von Hippel, a U.S. scientist, commented on the 2011 Fukushima nuclear disaster, saying that "fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas".
Nuclear proliferation.
Many technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that they can be used to make nuclear weapons if a country chooses to do so. When this happens a nuclear power program can become a route leading to a nuclear weapon or a public annex to a "secret" weapons program. The concern over Iran's nuclear activities is a case in point.
A fundamental goal for American and global security is to minimize the nuclear proliferation risks associated with the expansion of nuclear power. If this development is "poorly managed or efforts to contain risks are unsuccessful, the nuclear future will be dangerous". The Global Nuclear Energy Partnership is one such international effort to create a distribution network in which developing countries in need of energy, would receive nuclear fuel at a discounted rate, in exchange for that nation agreeing to forgo their own indigenous develop of a uranium enrichment program. The France-based Eurodif/"European Gaseous Diffusion Uranium Enrichment Consortium" was/is one such program that successfully implemented this concept, with Spain and other countries without enrichment facilities buying a share of the fuel produced at the French controlled enrichment facility, but without a transfer of technology. Iran was an early participant from 1974, and remains a shareholder of Eurodif via Sofidif.
According to Benjamin K. Sovacool, a "number of high-ranking officials, even within the United Nations, have argued that they can do little to stop states using nuclear reactors to produce nuclear weapons". A 2009 United Nations report said that:
the revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.
On the other hand, one factor influencing the support of power reactors is due to the appeal that these reactors have at reducing nuclear weapons arsenals through the Megatons to Megawatts Program, a program which eliminated 425 metric tons of highly enriched uranium(HEU), the equivalent of 17,000 nuclear warheads, by diluting it with natural uranium making it equivalent to low enriched uranium(LEU), and thus suitable as nuclear fuel for commercial fission reactors. This is the single most successful non-proliferation program to date.
The Megatons to Megawatts Program, the brainchild of Thomas Neff of MIT, was hailed as a major success by anti-nuclear weapon advocates as it has largely been the driving force behind the sharp reduction in the quantity of nuclear weapons worldwide since the cold war ended. However without an increase in nuclear reactors and greater demand for fissile fuel, the cost of dismantling and down blending has dissuaded Russia from continuing their disarmament.
Currently, according to Harvard professor Matthew Bunn: "The Russians are not remotely interested in extending the program beyond 2013. We've managed to set it up in a way that costs them more and profits them less than them just making new low-enriched uranium for reactors from scratch. But there are other ways to set it up that would be very profitable for them and would also serve some of their strategic interests in boosting their nuclear exports."
Up to 2005, the Megatons to Megawatts Program had processed $8 billion of HEU/weapons grade uranium into LEU/reactor grade uranium, with that corresponding to the elimination of 10,000 nuclear weapons.
For approximately two decades, this material generated nearly 10 percent of all the electricity consumed in the United States (about half of all US nuclear electricity generated) with a total of around 7 trillion kilowatt-hours of electricity produced. Enough energy to energize the entire United States electric grid for about two years. In total it is estimated to have cost $17 billion, a "bargain for US ratepayers", with Russia profiting $12 billion from the deal. Much needed profit for the Russian nuclear oversight industry, which after the collapse of the Soviet economy, had difficulties paying for the maintenance and security of the Russian Federations highly enriched uranium and warheads.
In April 2012 there were thirty one countries that have civil nuclear power plants, of which nine have nuclear weapons, with the vast majority of these nuclear weapons states having first produced weapons, before commercial fission electricity stations. Moreover, the re-purposing of civilian nuclear industries for military purposes would be a breach of the Non-proliferation treaty, of which 190 countries adhere to.
Environmental issues.
Life cycle analysis (LCA) of carbon dioxide emissions show nuclear power as comparable to renewable energy sources. Emissions from burning fossil fuels are many times higher.
According to the United Nations (UNSCEAR), regular nuclear power plant operation including the nuclear fuel cycle causes radioisotope releases into the environment amounting to 0.0002 millisieverts (mSv) per year of public exposure as a global average. (Such is small compared to variation in natural background radiation, which averages 2.4 mSv/a globally but frequently varies between 1 mSv/a and 13 mSv/a depending on a person's location as determined by UNSCEAR). As of a 2008 report, the remaining legacy of the worst nuclear power plant accident (Chernobyl) is 0.002 mSv/a in global average exposure (a figure which was 0.04 mSv per person averaged over the entire populace of the Northern Hemisphere in the year of the accident in 1986, although far higher among the most affected local populations and recovery workers).
Climate change.
Climate change causing weather extremes such as heat waves, reduced precipitation levels and droughts can have a significant impact on nuclear energy infrastructure. Seawater is corrosive and so nuclear energy supply is likely to be negatively affected by the fresh water shortage. This generic problem may become increasingly significant over time. This can force nuclear reactors to be shut down, as happened in France during the 2003 and 2006 heat waves. Nuclear power supply was severely diminished by low river ﬂow rates and droughts, which meant rivers had reached the maximum temperatures for cooling reactors. During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power and in 2009 a similar situation created a 8GW shortage and forced the French government to import electricity. Other cases have been reported from Germany, where extreme temperatures have reduced nuclear power production 9 times due to high temperatures between 1979 and 2007. In particular:
Similar events have happened elsewhere in Europe during those same hot summers. If global warming continues, this disruption is likely to increase.
Comparison with renewable energy.
As of 2013, the World Nuclear Association has said "There is unprecedented interest in renewable energy, particularly solar and wind energy, which provide electricity without giving rise to any carbon dioxide emission. Harnessing these for electricity depends on the cost and efficiency of the technology, which is constantly improving, thus reducing costs per peak kilowatt".
Renewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being intermittent or variable. The International Energy Agency concluded that deployment of renewable technologies (RETs), when it increases the diversity of electricity sources, contributes to the flexibility of the system. Their report also concluded: "At high levels of grid penetration by RETs the consequences of unmatched demand and supply can pose challenges for grid management. This characteristic may affect how, and the degree to which, RETs can displace fossil fuels and nuclear capacities in power generation."
Renewable electricity supply in the 20-50+% range has already been implemented in several European systems, albeit in the context of an integrated European grid system. In 2012, the share of electricity generated by renewable sources in Germany was 21.9%, compared to 16.0% for nuclear power after Germany shut down 7-8 of its 18 nuclear reactors in 2011. In the United Kingdom, the amount of energy produced from renewable energy is expected to exceed that from nuclear power by 2018, and Scotland plans to obtain all electricity from renewable energy by 2020. The majority of installed renewable energy across the world is in the form of hydro power.
The IPCC has said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.
The cost of nuclear power has followed an increasing trend whereas the cost of electricity is declining for wind power. In about 2011, wind power became as inexpensive as natural gas, and anti-nuclear groups have suggested that in 2010 solar power became cheaper than nuclear power. Data from the EIA in 2011 estimated that in 2016, solar will have a levelized cost of electricity almost twice that of nuclear (21¢/kWh for solar, 11.39¢/kWh for nuclear), and wind somewhat less (9.7¢/kWh). However, the US EIA has also cautioned that levelized costs of intermittent sources such as wind and solar are not directly comparable to costs of "dispatchable" sources (those that can be adjusted to meet demand).
From a safety stand point, nuclear power, in terms of lives lost per unit of electricity delivered, is comparable to and in some cases, lower than many renewable energy sources. There is however no radioactive spent fuel that needs to be stored or reprocessed with conventional renewable energy sources. A nuclear plant needs to be disassembled and removed. Much of the disassembled nuclear plant needs to be stored as low level nuclear waste.
Nuclear decommissioning.
The price of energy inputs and the environmental costs of every nuclear power plant continue long after the facility has finished generating its last useful electricity. Once no longer economically viable, nuclear reactors and uranium enrichment facilities are generally decommissioned, returning the facility and its parts to a safe enough level to be entrusted for other uses, such as greenfield status. After a cooling-off period that may last decades, reactor core materials are dismantled and cut into small pieces to be packed in containers for interim storage or transmutation experiments. The process is expensive, time-consuming, dangerous for workers and potentially hazardous to the natural environment as it presents opportunities for human error, accidents or sabotage.
The total energy required for decommissioning can be as much as 50% more than the energy needed for the original construction. In most cases, the decommissioning process costs between US $300 million to US$5.6 billion. Decommissioning at nuclear sites which have experienced a serious accident are the most expensive and time-consuming. In the U.S. in 2011, there are 13 reactors that had permanently shut down and are in some phase of decommissioning. With Yankee Rowe Nuclear Power Station having completed the process in 2007, after ceasing commercial electricity production in 1992. The majority of the 15 years, was used to allow the station to naturally cool-down on its own, which makes the manual disassembly process both safer and cheaper.
Debate on nuclear power.
The nuclear power debate concerns the controversy which has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes. The debate about nuclear power peaked during the 1970s and 1980s, when it "reached an intensity unprecedented in the history of technology controversies", in some countries.
Proponents of nuclear energy contend that nuclear power is a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on imported energy sources. Proponents claim that nuclear power produces virtually no conventional air pollution, such as greenhouse gases and smog, in contrast to the chief viable alternative of fossil fuel. Nuclear power can produce base-load power unlike many renewables which are intermittent energy sources lacking large-scale and cheap ways of storing energy. M. King Hubbert saw oil as a resource that would run out, and proposed nuclear energy as a replacement energy source. Proponents claim that the risks of storing waste are small and can be further reduced by using the latest technology in newer reactors, and the operational safety record in the Western world is excellent when compared to the other major kinds of power plants.
Opponents believe that nuclear power poses many threats to people and the environment. These threats include the problems of processing, transport and storage of radioactive nuclear waste, the risk of nuclear weapons proliferation and terrorism, as well as health risks and environmental damage from uranium mining. They also contend that reactors themselves are enormously complex machines where many things can and do go wrong; and there have been serious nuclear accidents. Critics do not believe that the risks of using nuclear fission as a power source can be fully offset through the development of new technology. They also argue that when all the energy-intensive stages of the nuclear fuel chain are considered, from uranium mining to nuclear decommissioning, nuclear power is neither a low-carbon nor an economical electricity source.
Arguments of economics and safety are used by both sides of the debate.
Use in space.
Both fission and fusion appear promising for space propulsion applications, generating higher mission velocities with less reaction mass. This is due to the much higher energy density of nuclear reactions: some 7 orders of magnitude (10,000,000 times) more energetic than the chemical reactions which power the current generation of rockets.
Radioactive decay has been used on a relatively small scale (few kW), mostly to power space missions and experiments by using radioisotope thermoelectric generators such as those developed at Idaho National Laboratory.
Research.
Advanced concepts.
Current fission reactors in operation around the world are second or third generation systems, with most of the first-generation systems having been retired some time ago. Research into advanced generation IV reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals, including to improve nuclear safety, improve proliferation resistance, minimize waste, improve natural resource utilization, the ability to consume existing nuclear waste in the production of electricity, and decrease the cost to build and run such plants. Most of these reactors differ significantly from current operating light water reactors, and are generally not expected to be available for commercial construction before 2030.
The nuclear reactors to be built at Vogtle are new AP1000 third generation reactors, which are said to have safety improvements over older power reactors. However, John Ma, a senior structural engineer at the NRC, is concerned that some parts of the AP1000 steel skin are so brittle that the "impact energy" from a plane strike or storm driven projectile could shatter the wall. Edwin Lyman, a senior staff scientist at the Union of Concerned Scientists, is concerned about the strength of the steel containment vessel and the concrete shield building around the AP1000.
The Union of Concerned Scientists has referred to the EPR (nuclear reactor), currently under construction in China, Finland and France, as the only new reactor design under consideration in the United States that "...appears to have the potential to be significantly safer and more secure against attack than today's reactors."
One disadvantage of any new reactor technology is that safety risks may be greater initially as reactor operators have little experience with the new design. Nuclear engineer David Lochbaum has explained that almost all serious nuclear accidents have occurred with what was at the time the most recent technology. He argues that "the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes". As one director of a U.S. research laboratory put it, "fabrication, construction, operation, and maintenance of new reactors will face a steep learning curve: advanced technologies will have a heightened risk of accidents and mistakes. The technology may be proven, but people are not".
Hybrid nuclear fusion-fission.
Hybrid nuclear power is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to delays in the realization of pure fusion. When a sustained nuclear fusion power plant is built, it has the potential to be capable of extracting all the fission energy that remains in spent fission fuel, reducing the volume of nuclear waste by orders of magnitude, and more importantly, eliminating all actinides present in the spent fuel, substances which cause security concerns.
Nuclear fusion.
Nuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission. These reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. Fusion power has been under theoretical and experimental investigation since the 1950s.
Construction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated. A follow on commercial nuclear fusion power station, DEMO, has been proposed. There are also suggestions for a power plant based upon a different fusion approach, that of an inertial fusion power plant.
Fusion powered electricity generation was initially believed to be readily achievable, as fission-electric power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050.

</doc>
<doc id="22156" url="https://en.wikipedia.org/wiki?curid=22156" title="BI Norwegian Business School">
BI Norwegian Business School

BI Norwegian Business School () is the largest business school in Norway and the second largest in all of Europe. BI has in total four campuses with the main one located in Oslo. The university has 812 employees consisting of an academic staff of 757 people and 428 administrative staff. In 2015, BI Norwegian Business School had 18,728 students. BI is the largest supplier of economic and administrative competence and skills in Norway with more than 200 000 graduates since 1983. BI Norwegian Business School is a private foundation and is accredited by NOKUT as a specialised university institution. BI organised its academic activities in nine separate research departments covering all of the disciplines that can be expected at a modern European business school. 
History.
BI Norwegian Business School was founded in 1943 by Finn Øien as "Bedriftøkonomisk Institut" (), hence the abbreviation "BI". 
Current activities.
As of 2013, the school has 19,649 students made up of 52% women and 55.6% full-time students Thereby, the Bachelor of Science programs have 15,570 students and the Master of Science programs have 3,968 students. 1,300 are international students. There are 831 employees, of whom 419 are faculty and 412 are administrative. 
BI offers a full set of programs for bachelor, master, and doctoral degrees, as well as executive education and tailor-made programs for businesses. The teaching languages are English (BBA and graduate programs) and Norwegian (majority of undergraduate programs and custom programs for local businesses). The school currently participates in exchange programs with 170 foreign institutions in 45 countries.
The internationally award winning main campus in Nydalen (Oslo) was designed by Niels Torp, who also designed Gardermoen Airport.
Norsk Kundebarometer.
Norsk Kundebarometer (NKB) () is a research program run by BI, with a focus on relations between customers and businesses. Based on an annual survey of Norwegian households, it collects data that may be used for comparison between businesses, comparisons between various industries, and comparisons over time.
Activities abroad.
BI has educated roughly 1700 students in China through its close relationship with Fudan University in Shanghai, and is also the majority shareholder of the ISM University of Management and Economics (previously known as International School of Management) with around 1800 students located in Vilnius and Kaunas in Lithuania.
Degree programs.
Undergraduate (All taught in Norwegian except Business Administration), 
"Bachelor in":
Graduate (Only available in Oslo; all taught in English except MSc in Professional Accountancy)
The current Executive Masters of Business Administration (EMBA) course at the Norwegian business school is fantastic. The new structure and the combination of the general EMBA courses with other renowned business schools in Europe, Asia, and the United States gives the program an additional edge. The new structure exposes participants to insights on business structure in Europe, Latin America, Asia and North America; hence, providing participants with a global business exposure.
Student organizations.
The school has two student organizations, one for the main campus in Oslo and one for the other campuses. The Oslo student organization is called "Studentforeningen ved Handelshøyskolen BI i Oslo" (SBIO) (). This union was formed in 2005 after the relocation of the three locations in Oslo into one—Nydalen Campus. The three previous unions were called Bedriftøkonomisk Studentersamfund (BS), BISON and MØSS. BS was the oldest union, formed in 1964. The union for the other campuses is "BI Studentsamfunn" (BIS) (). This union was founded on 7 February 1987 and is today the largest student union of a private school in Norway.
The student newspaper is named "INSIDE", and its circulation is 11,000.
The all-male student choir is named UFDA The Choir Boys and was established in 1986.
Quality accreditations.
BI is accredited as a specialised university institution by the Norwegian Agency for Quality Assurance in Education (NOKUT).
BI has also received the following recognitions from private institutions:

</doc>
<doc id="22158" url="https://en.wikipedia.org/wiki?curid=22158" title="Nuclear proliferation">
Nuclear proliferation

[[Image:Nuclear weapon programs worldwide oct2006.png|thumb|300px|World map with nuclear weapons development status represented by color.
]]
Nuclear proliferation is the spread of nuclear weapons, fissionable material, and weapons-applicable nuclear technology and information to nations not recognized as "Nuclear Weapon States" by the "Treaty on the Nonproliferation of Nuclear Weapons", also known as the Nuclear Nonproliferation Treaty or NPT. Leading experts on nuclear proliferation, such as Etel Solingen of the University of California, Irvine, suggest that states' decisions to build nuclear weapons is largely determined by the interests of their governing domestic coalitions.
Proliferation has been opposed by many nations with and without nuclear weapons, the governments of which fear that more countries with nuclear weapons may increase the possibility of nuclear warfare (up to and including the so-called "countervalue" targeting of civilians with nuclear weapons), de-stabilize international or regional relations, or infringe upon the national sovereignty of states.
Four countries besides the five recognized Nuclear Weapons States have acquired, or are presumed to have acquired, nuclear weapons: India, Pakistan, North Korea, and Israel. None of these four is a party to the NPT, although North Korea acceded to the NPT in 1985, then withdrew in 2003 and conducted announced nuclear tests in 2006, 2009, and 2013. One critique of the NPT is that it is discriminatory in recognizing as nuclear weapon states only those countries that tested nuclear weapons before 1968 and requiring all other states joining the treaty to forswear nuclear weapons.
Research into the development of nuclear weapons was undertaken during World War II by the United States (in cooperation with the United Kingdom and Canada), Germany, Japan, and the USSR. The United States was the first and is the only country to have used a nuclear weapon in war, when it used two bombs against Japan in August 1945. With their loss during the war, Germany and Japan ceased to be involved in any nuclear weapon research. In August 1949, the USSR tested a nuclear weapon. The United Kingdom tested a nuclear weapon in October 1952. France developed a nuclear weapon in 1960. The People's Republic of China detonated a nuclear weapon in 1964. India exploded a nuclear device in 1974, and Pakistan tested a weapon in 1998. In 2006, North Korea conducted a nuclear test.
Non-proliferation efforts.
Early efforts to prevent nuclear proliferation involved intense government secrecy, the wartime acquisition of known uranium stores (the Combined Development Trust), and at times even outright sabotage—such as the bombing of a heavy-water facility thought to be used for a German nuclear program. None of these efforts were explicitly public, because the weapon developments themselves were kept secret until the bombing of Hiroshima.
Earnest international efforts to promote nuclear non-proliferation began soon after World War II, when the Truman Administration proposed the Baruch Plan of 1946, named after Bernard Baruch, America's first representative to the United Nations Atomic Energy Commission. The Baruch Plan, which drew heavily from the Acheson–Lilienthal Report of 1946, proposed the verifiable dismantlement and destruction of the U.S. nuclear arsenal (which, at that time, was the only nuclear arsenal in the world) after all governments had cooperated successfully to accomplish two things: (1) the establishment of an "international atomic development authority," which would actually own and control all military-applicable nuclear materials and activities, and (2) the creation of a system of automatic sanctions, which not even the U.N. Security Council could veto, and which would proportionately punish states attempting to acquire the capability to make nuclear weapons or fissile material.
Baruch's plea for the destruction of nuclear weapons invoked basic moral and religious intuitions. In one part of his address to the UN, Baruch said, "Behind the black portent of the new atomic age lies a hope which, seized upon with faith, can work out our salvation. If we fail, then we have damned every man to be the slave of Fear. Let us not deceive ourselves. We must elect World Peace or World Destruction... We must answer the world's longing for peace and security." With this remark, Baruch helped launch the field of nuclear ethics, to which many policy experts and scholars have contributed.
Although the Baruch Plan enjoyed wide international support, it failed to emerge from the UNAEC because the Soviet Union planned to veto it in the Security Council. Still, it remained official American policy until 1953, when President Eisenhower made his "Atoms for Peace" proposal before the U.N. General Assembly. Eisenhower's proposal led eventually to the creation of the International Atomic Energy Agency (IAEA) in 1957. Under the "Atoms for Peace" program thousands of scientists from around the world were educated in nuclear science and then dispatched home, where many later pursued secret weapons programs in their home country.
Efforts to conclude an international agreement to limit the spread of nuclear weapons did not begin until the early 1960s, after four nations (the United States, the Soviet Union, the United Kingdom and France) had acquired nuclear weapons (see List of states with nuclear weapons for more information). Although these efforts stalled in the early 1960s, they renewed once again in 1964, after China detonated a nuclear weapon. In 1968, governments represented at the Eighteen Nation Disarmament Committee (ENDC) finished negotiations on the text of the NPT. In June 1968, the U.N. General Assembly endorsed the NPT with General Assembly Resolution 2373 (XXII), and in July 1968, the NPT opened for signature in Washington, DC, London and Moscow. The NPT entered into force in March 1970.
Since the mid-1970s, the primary focus of non-proliferation efforts has been to maintain, and even increase, international control over the fissile material and specialized technologies necessary to build such devices because these are the most difficult and expensive parts of a nuclear weapons program. The main materials whose generation and distribution is controlled are highly enriched uranium and plutonium. Other than the acquisition of these special materials, the scientific and technical means for weapons construction to develop rudimentary, but working, nuclear explosive devices are considered to be within the reach of industrialized nations.
Since its founding by the United Nations in 1957, the International Atomic Energy Agency (IAEA) has promoted two, sometimes contradictory, missions: on the one hand, the Agency seeks to promote and spread internationally the use of civilian nuclear energy; on the other hand, it seeks to prevent, or at least detect, the diversion of civilian nuclear energy to nuclear weapons, nuclear explosive devices or purposes unknown. The IAEA now operates a safeguards system as specified under Article III of the Nuclear Non-Proliferation Treaty (NPT) of 1968, which aims to ensure that civil stocks of uranium, plutonium, as well as facilities and technologies associated with these nuclear materials, are used only for peaceful purposes and do not contribute in any way to proliferation or nuclear weapons programs. It is often argued that proliferation of nuclear weapons to many other states has been prevented by the extension of assurances and mutual defence treaties to these states by nuclear powers, but other factors, such as national prestige, or specific historical experiences, also play a part in hastening or stopping nuclear proliferation.
Dual use technology.
Dual-use technology refers to the possibility of military use of civilian nuclear power technology. Many technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that several stages of the nuclear fuel cycle allow diversion of nuclear materials for nuclear weapons. When this happens a nuclear power program can become a route leading to the atomic bomb or a public annex to a secret bomb program. The crisis over Iran’s nuclear activities is a case in point.
Many UN and US agencies warn that building more nuclear reactors unavoidably increases nuclear proliferation risks. A fundamental goal for American and global security is to minimize the proliferation risks associated with the
expansion of nuclear power. If this development is "poorly managed or efforts to contain risks are unsuccessful, the nuclear future will be dangerous". For nuclear power programs to be developed and managed safely and securely, it is important that countries have domestic “good governance” characteristics that will encourage proper nuclear operations and management:
These characteristics include low degrees of corruption (to avoid officials selling materials and technology for their own personal gain as occurred with the A.Q. Khan smuggling network in Pakistan), high degrees of political stability (defined by the World Bank as “likelihood that the government will be destabilized or overthrown by unconstitutional or violent means, including motivated violence and terrorism”), high governmental effectiveness scores (a World Bank aggregate measure of “the quality of the civil service and the degree of its independence from political pressures the quality of policy formulation and implementation”), and a strong degree of regulatory competence.
International cooperation.
Nuclear Non-Proliferation Treaty.
At present, 189 countries are States Parties to the "Treaty on the Nonproliferation of Nuclear Weapons", more commonly known as the Nuclear Nonproliferation Treaty or NPT. These include the five Nuclear Weapons States (NWS) recognized by the NPT: the People's Republic of China, France, Russian Federation, the UK, and the United States.
Notable non-signatories to the NPT are Israel, Pakistan, and India (the latter two have since tested nuclear weapons, while Israel is considered by most to be an unacknowledged nuclear weapons state). North Korea was once a signatory but withdrew in January 2003. The legality of North Korea's withdrawal is debatable but as of 9 October 2006, North Korea clearly possesses the capability to make a nuclear explosive device.
International Atomic Energy Agency.
The IAEA was established on 29 July 1957 to help nations develop nuclear energy for peaceful purposes. Allied to this role is the administration of safeguards arrangements to provide assurance to the international community that individual countries are honoring their commitments under the treaty. Though established under its own international treaty, the IAEA reports to both the United Nations General Assembly and the Security Council.
The IAEA regularly inspects civil nuclear facilities to verify the accuracy of documentation supplied to it. The agency checks inventories, and samples and analyzes materials. Safeguards are designed to deter diversion of nuclear material by increasing the risk of early detection. They are complemented by controls on the export of sensitive technology from countries such as UK and United States through voluntary bodies such as the Nuclear Suppliers Group. The main concern of the IAEA is that uranium not be enriched beyond what is necessary for commercial civil plants, and that plutonium which is produced by nuclear reactors not be refined into a form that would be suitable for bomb production.
Scope of safeguards.
Traditional safeguards are arrangements to account for and control the use of nuclear materials. This verification is a key element in the international system which ensures that uranium in particular is used only for peaceful purposes.
Parties to the NPT agree to accept technical safeguard measures applied by the IAEA. These require that operators of nuclear facilities maintain and declare detailed accounting records of all movements and transactions involving nuclear material. Over 550 facilities and several hundred other locations are subject to regular inspection, and their records and the nuclear material being audited. Inspections by the IAEA are complemented by other measures such as surveillance cameras and instrumentation.
The inspections act as an alert system providing a warning of the possible diversion of nuclear material from peaceful activities. The system relies on;
All NPT non-weapons states must accept these full-scope safeguards. In the five weapons states plus the non-NPT states (India, Pakistan and Israel), facility-specific safeguards apply. IAEA inspectors regularly visit these facilities to verify completeness and accuracy of records.
The terms of the NPT cannot be enforced by the IAEA itself, nor can nations be forced to sign the treaty. In reality, as shown in Iraq and North Korea, safeguards can be backed up by diplomatic, political and economic measures.
While traditional safeguards easily verified the correctness of formal declarations by suspect states, in the 1990s attention turned to what might not have been declared. While accepting safeguards at declared facilities, Iraq had set up elaborate equipment elsewhere in an attempt to enrich uranium to weapons grade. North Korea attempted to use research reactors (not commercial electricity-generating reactors) and a reprocessing plant to produce some weapons-grade plutonium.
The weakness of the NPT regime lay in the fact that no obvious diversion of material was involved. The uranium used as fuel probably came from indigenous sources, and the nuclear facilities were built by the countries themselves without being declared or placed under safeguards. Iraq, as an NPT party, was obliged to declare all facilities but did not do so. Nevertheless, the activities were detected and brought under control using international diplomacy. In Iraq, a military defeat assisted this process.
In North Korea, the activities concerned took place before the conclusion of its NPT safeguards agreement. With North Korea, the promised provision of commercial power reactors appeared to resolve the situation for a time, but it later withdrew from the NPT and declared it had nuclear weapons.
Additional Protocol.
In 1993 a program was initiated to strengthen and extend the classical safeguards system, and a model protocol was agreed by the IAEA Board of Governors 1997. The measures boosted the IAEA's ability to detect undeclared nuclear activities, including those with no connection to the civil fuel cycle.
Innovations were of two kinds. Some could be implemented on the basis of IAEA's existing legal authority through safeguards agreements and inspections. Others required further legal authority to be conferred through an Additional Protocol. This must be agreed by each non-weapons state with IAEA, as a supplement to any existing comprehensive safeguards agreement. Weapons states have agreed to accept the principles of the model additional protocol.
Key elements of the model Additional Protocol:
As of 3 July 2015, 146 countries have signed Additional Protocols and 126 have brought them into force. The IAEA is also applying the measures of the Additional Protocol in Taiwan. Under the Joint Comprehensive Plan of Action, Iran has agreed to implement its protocol provisionally. Among the leading countries that have not signed the Additional Protocol are Egypt, which says it will not sign until Israel accepts comprehensive IAEA safeguards, and Brazil, which opposes making the protocol a requirement for international cooperation on enrichment and reprocessing, but has not ruled out signing.
Limitations of safeguards.
The greatest risk from nuclear weapons proliferation comes from countries which have not joined the NPT and which have significant unsafeguarded nuclear activities; India, Pakistan, and Israel fall within this category. While safeguards apply to some of their activities, others remain beyond scrutiny.
A further concern is that countries may develop various sensitive nuclear fuel cycle facilities and research reactors under full safeguards and then subsequently opt out of the NPT. Bilateral agreements, such as insisted upon by Australia and Canada for sale of uranium, address this by including fallback provisions, but many countries are outside the scope of these agreements. If a nuclear-capable country does leave the NPT, it is likely to be reported by the IAEA to the UN Security Council, just as if it were in breach of its safeguards agreement. Trade sanctions would then be likely.
IAEA safeguards can help ensure that uranium supplied as nuclear fuel and other nuclear supplies do not contribute to nuclear weapons proliferation. In fact, the worldwide application of those safeguards and the substantial world trade in uranium for nuclear electricity make the proliferation of nuclear weapons much less likely.
The Additional Protocol, once it is widely in force, will provide credible assurance that there are no undeclared nuclear materials or activities in the states concerned. This will be a major step forward in preventing nuclear proliferation.
Other developments.
The Nuclear Suppliers Group communicated its guidelines, essentially a set of export rules, to the IAEA in 1978. These were to ensure that transfers of nuclear material or equipment would not be diverted to unsafeguarded nuclear fuel cycle or nuclear explosive activities, and formal government assurances to this effect were required from recipients. The Guidelines also recognised the need for physical protection measures in the transfer of sensitive facilities, technology and weapons-usable materials, and strengthened retransfer provisions. The group began with seven membersthe United States, the former USSR, the UK, France, Germany, Canada and Japanbut now includes 46 countries including all five nuclear weapons states.
The International Framework for Nuclear Energy Cooperation is an international project involving 25 partner countries, 28 observer and candidate partner countries, and the International Atomic Energy Agency, the Generation IV International Forum, and the European Commission. Its goal is to "[..] provide competitive, commercially-based services as an alternative to a state’s development of costly, proliferation-sensitive facilities, and address other issues associated with the safe and secure management of used fuel and radioactive waste."
According to Kenneth D. Bergeron's "Tritium on Ice: The Dangerous New Alliance of Nuclear Weapons and Nuclear Power", tritium is not classified as a "special nuclear material" but rather as a by-product. It is seen as an important litmus test on the seriousness of the United States' intention to nuclear disarm. This radioactive super-heavy hydrogen isotope is used to boost the efficiency of fissile materials in nuclear weapons. The United States resumed tritium production in 2003 for the first time in 15 years. This could indicate that there is a potential nuclear arm stockpile replacement since the isotope naturally decays.
In May 1995, NPT parties reaffirmed their commitment to a Fissile Materials Cut-off Treaty to prohibit the production of any further fissile material for weapons. This aims to complement the Comprehensive Test Ban Treaty of 1996 (not entered into force as of 2011) and to codify commitments made by the United States, the UK, France and Russia to cease production of weapons material, as well as putting a similar ban on China. This treaty will also put more pressure on Israel, India and Pakistan to agree to international verification.
On 9 August 2005, Ayatollah Ali Khamenei issued a fatwa forbidding the production, stockpiling and use of nuclear weapons. Khamenei's official statement was made at the meeting of the International Atomic Energy Agency (IAEA) in Vienna. As of February 2006 Iran formally announced that uranium enrichment within their borders has continued. Iran claims it is for peaceful purposes but the United Kingdom, France, Germany, and the United States claim the purpose is for nuclear weapons research and construction.
Unsanctioned nuclear activity.
NPT Non Signatories.
India, Pakistan and Israel have been "threshold" countries in terms of the international non-proliferation regime. They possess or are quickly capable of assembling one or more nuclear weapons. They have remained outside the 1970 NPT. They are thus largely excluded from trade in nuclear plant or materials, except for safety-related devices for a few safeguarded facilities.
In May 1998 India and Pakistan each exploded several nuclear devices underground. This heightened concerns regarding an arms race between them, with Pakistan involving the People's Republic of China, an acknowledged nuclear weapons state. Both countries are opposed to the NPT as it stands, and India has consistently attacked the Treaty since its inception in 1970 labeling it as a lopsided treaty in favor of the nuclear powers.
Relations between the two countries are tense and hostile, and the risks of nuclear conflict between them have long been considered quite high. Kashmir is a prime cause of bilateral tension, its sovereignty being in dispute since 1948. There is persistent low level bilateral military conflict due to alleged backing of insurgency by Pakistan in India, and correspondingly Indian sponsoring of terrorism in Pakistan along with the disputed status of Kashmir.
Both engaged in a conventional arms race in the 1980s, including sophisticated technology and equipment capable of delivering nuclear weapons. In the 1990s the arms race quickened. In 1994 India reversed a four-year trend of reduced allocations for defence, and despite its much smaller economy, Pakistan was expected to push its own expenditures yet higher. Both have lost their patrons: India, the former USSR, and Pakistan, the United States.
But it is the growth and modernization of China's nuclear arsenal and its assistance with Pakistan's nuclear power programme and, reportedly, with missile technology, which exacerbate Indian concerns. In particular, as viewed by Indian strategists, Pakistan is aided by China's People's Liberation Army.
India.
Nuclear power for civil use is well established in India. Its civil nuclear strategy has been directed towards complete independence in the nuclear fuel cycle, necessary because of its outspoken rejection of the NPT. This self-sufficiency extends from uranium exploration and mining through fuel fabrication, heavy water production, reactor design and construction, to reprocessing and waste management. It has a small fast breeder reactor and is planning a much larger one. It is also developing technology to utilise its abundant resources of thorium as a nuclear fuel.
India has 14 small nuclear power reactors in commercial operation, two larger ones under construction, and ten more planned. The 14 operating ones (2548 MWe total) comprise:
The two under construction and two of the planned ones are 450 MWe versions of these 200 MWe domestic products. Construction has been seriously delayed by financial and technical problems. In 2001 a final agreement was signed with Russia for the country's first large nuclear power plant, comprising two VVER-1000 reactors, under a Russian-financed US$3 billion contract. The first unit is due to be commissioned in 2007. A further two Russian units are under consideration for the site. Nuclear power supplied 3.1% of India's electricity in 2000.
Its weapons material appears to come from a Canadian-designed 40MW "research" reactor which started up in 1960, well before the NPT, and a 100MW indigenous unit in operation since 1985. Both use local uranium, as India does not import any nuclear fuel. It is estimated that India may have built up enough weapons-grade plutonium for a hundred nuclear warheads.
It is widely believed that the nuclear programs of India and Pakistan used CANDU reactors to produce fissionable materials for their weapons; however, this is not accurate. Both Canada (by supplying the 40 MW research reactor) and the United States (by supplying 21 tons of heavy water) supplied India with the technology necessary to create a nuclear weapons program, dubbed CIRUS (Canada-India Reactor, United States). Canada sold India the reactor on the condition that the reactor and any by-products would be "employed for peaceful purposes only.". Similarly, the United States sold India heavy water for use in the reactor "only... in connection with research into and the use of atomic energy for peaceful purposes". India, in violation of these agreements, used the Canadian-supplied reactor and American-supplied heavy water to produce plutonium for their first nuclear explosion, Smiling Buddha. The Indian government controversially justified this, however, by claiming that Smiling Buddha was a "peaceful nuclear explosion."
The country has at least three other research reactors including the tiny one which is exploring the use of thorium as a nuclear fuel, by breeding fissile U-233. In addition, an advanced heavy-water thorium cycle is under development.
India exploded a nuclear device in 1974, the so-called Smiling Buddha test, which it has consistently claimed was for peaceful purposes. Others saw it as a response to China's nuclear weapons capability. It was then universally perceived, notwithstanding official denials, to possess, or to be able to quickly assemble, nuclear weapons. In 1999 it deployed its own medium-range missile and has developed an intermediate-range missile capable of reaching targets in China's industrial heartland.
In 1995 the United States quietly intervened to head off a proposed nuclear test. However, in 1998 there were five more tests in Operation Shakti. These were unambiguously military, including one claimed to be of a sophisticated thermonuclear device, and their declared purpose was "to help in the design of nuclear weapons of different yields and different delivery systems".
Indian security policies are driven by:
It perceives nuclear weapons as a cost-effective political counter to China's nuclear and conventional weaponry, and the effects of its nuclear weapons policy in provoking Pakistan is, by some accounts, considered incidental.
India has had an unhappy relationship with China. After an uneasy ceasefire ended the 1962 war, relations between the two nations were frozen until 1998. Since then a degree of high-level contact has been established and a few elementary confidence-building measures put in place. China still occupies some territory which it captured during the aforementioned war, claimed by India, and India still occupies some territory claimed by China. Its nuclear weapon and missile support for Pakistan is a major bone of contention.
American President George W. Bush met with India Prime Minister Manmohan Singh to discuss India's involvement with nuclear weapons. The two countries agreed that the United States would give nuclear power assistance to India.
Pakistan.
Over the several years, the Nuclear power infrastructure has been well established by Pakistan which is dedicated for the industrial and economic development of the country. Its current nuclear policy is directed and aimed to promote the socio-economic development of the people as a "foremost priority"; and to fulfill the energy, economic, and industrial needs from the nuclear sources. Currently, there are three operational mega-commercial nuclear power plants while three larger ones are under construction. The nuclear power supplies 787MW (roughly ~3.6%) of electricity as of 2012, and the country has projected to produce 8800MW electricity by 2030. Infrastructure established by the IAEA and the U.S. in the 1950s–1960s were based on peaceful research and development and economic prosperity of the country.
Although the civil-sector nuclear power was established in the 1950s, the country has an active nuclear weapons program which was started in the 1970s. The bomb program has its roots after East-Pakistan gained its independence as Bangladesh after India's successful intervention led to a decisive victory on Pakistan in 1971. This large-scale but clandestine atomic bomb project was directed towards the development of ingenious development of reactor and military-grade plutonium. In 1974, when India surprised the outer world with its successful detonation of its own bomb, codename "Smiling Buddha", it became "imperative for Pakistan" to pursue the weapons research. According to leading scientist in the program, it became clear once India detonated the bomb, "Newton's third law" came into "operation", from then on it was a classic case of "action and reaction". Earlier efforts were directed towards mastering the plutonium technology from France, but plutonium route was partially slowed down when the plan was failed after the U.S. intervention to cancel the project. Contrary to popular perception, Pakistan did not forego the "plutonium" route and covertly continued its indegenious research under Munir Khan and it succeeded with plutonium route in the early 1980s. Reacting on India's nuclear test (Smiling Buddha), Bhutto and the country's elite political and military science circle sensed this test as final and dangerous anticipation to Pakistan's "moral and physical existence." With Aziz Ahmed on his side, Bhutto launched a serious diplomatic offense and aggressively maintained at the session of the United Nations Security Council:
After 1974, Bhutto's government redoubled its effort, this time equally focused on uranium and plutonium. Pakistan had established science directorates in almost all of her embassies in the important countries of the world, with theoretical physicist S.A. Butt being the director. Abdul Qadeer Khan then established a network through Dubai to smuggle URENCO technology to Engineering Research Laboratories. Earlier, he worked with "Physics Dynamics Research Laboratories" (FDO), a subsidiary of the Dutch firm VMF-Stork based in Amsterdam. Later after joining, the Urenco, he had access through photographs and documents of the technology. Against the popular perception, the technology that A.Q. Khan had brought from Urenco was based on first generation civil rector technology, filled with many serious technical errors, though it was authentic and vital link for centrifuge project of the country. After the British Government stopped the British subsidiary of the American Emerson Electric Co. from shipping the components to Pakistan, he describes his frustration with a supplier from Germany as: "That man from the German team was unethical. When he did not get the order from us, he wrote a letter to a Labour Party member and questions were asked in ." By 1978, his efforts were paid off and made him into a national hero. In 1981, as a tribute, President General Muhammad Zia-ul-Haq, renamed the research institute after his name.
In early 1996, Prime minister Benazir Bhutto made it clear that "if India conducts a nuclear test, Pakistan could be forced to "follow suit". In 1997, her statement was echoed by Prime minister Nawaz Sharif who maintained to the fact that: "Since 1972, akistan had progressed significantly, and we have left that stage (developmental) far behind. Pakistan will not be made a "hostage" to India by signing the CTBT, before (India).!" In May 1998, within weeks of India's nuclear tests, Pakistan announced that it had conducted six underground tests in the Chagai Hills, five on the 28th and one on the 30th of that month. Seismic events consistent with these claims were recorded.
In 2004, the revelation of A.Q. Khan's efforts led the exposure of many defunct European consortium who defied export restrictions in the 1970s, and many of defunct Dutch companies exported thousands of centrifuges to Pakistan as early as 1976. Many centrifuge components were apparently manufactured in Malaysian Scomi Precision Engineering with the assistance of South Asian and German companies, and used a UAE-based computer company as a false front.
It was widely believed to have direct involvement of the government of Pakistan. This claim could not be verified due to the refusal of the government of Pakistan to allow IAEA to interview the alleged head of the nuclear black market, who happened to be no other than A.Q. Khan. Confessing his crimes later a month on national television, he bailed out the government by taking full responsibility. Independent investigation conducted by IISS confirmed that he had control over the import-export deals, and his acquisition activities were largely unsupervised by Pakistan governmental authorities. All of his activities went undetected for several years. He duly confessed of running the atomic proliferation ring from Pakistan to Iran and North Korea. He was immediately given presidential immunity. Exact nature of the involvement at the governmental level is still unclear, but the manner in which the government acted cast doubt on the sincerity of Pakistan.
North Korea.
The Democratic Peoples Republic of Korea (or better known as North Korea), joined the NPT in 1985 and had subsequently signed a safeguards agreement with the IAEA. However, it was believed that North Korea was diverting plutonium extracted from the fuel of its reactor at Yongbyon, for use in nuclear weapons. The subsequent confrontation with IAEA on the issue of inspections and suspected violations, resulted in North Korea threatening to withdraw from the NPT in 1993. This eventually led to negotiations with the United States resulting in the Agreed Framework of 1994, which provided for IAEA safeguards being applied to its reactors and spent fuel rods. These spent fuel rods were sealed in canisters by the United States to prevent North Korea from extracting plutonium from them. North Korea had to therefore freeze its plutonium programme.
During this period, Pakistan-North Korea cooperation in missile technology transfer was being established. A high level delegation of Pakistan military visited North Korea in August–September 1992, reportedly to discuss the supply of missile technology to Pakistan. In 1993, PM Benazir Bhutto repeatedly traveled to China, and the paid state visit to North Korea. The visits are believed to be related to the subsequent acquisition technology to developed its Ghauri system by Pakistan. During the period 1992–1994, A.Q. Khan was reported to have visited North Korea thirteen times. The missile cooperation program with North Korea was under Dr. A. Q. Khan Research Laboratories. At this time China was under U.S. pressure not to supply the M Dongfeng series of missiles to Pakistan. It is believed by experts that possibly with Chinese connivance and facilitation, the latter was forced to approach North Korea for missile transfers. Reports indicate that North Korea was willing to supply missile sub-systems including rocket motors, inertial guidance systems, control and testing equipment for US$50 million.
It is not clear what North Korea got in return. Joseph S. Bermudez Jr. in "Jane's Defence Weekly" (27 November 2002) reports that Western analysts had begun to question what North Korea received in payment for the missiles; many suspected it was the nuclear technology. The KRL was in charge of both uranium program and also of the missile program with North Korea. It is therefore likely during this period that cooperation in nuclear technology between Pakistan and North Korea was initiated. Western intelligence agencies began to notice exchange of personnel, technology and components between KRL and entities of the North Korean 2nd Economic Committee (responsible for weapons production).
A "New York Times" report on 18 October 2002 quoted U.S. intelligence officials having stated that Pakistan was a major supplier of critical equipment to North Korea. The report added that equipment such as gas centrifuges appeared to have been "part of a barter deal" in which North Korea supplied Pakistan with missiles. Separate reports indicate ("The Washington Times", 22 November 2002) that U.S. intelligence had as early as 1999 picked up signs that North Korea was continuing to develop nuclear arms. Other reports also indicate that North Korea had been working covertly to develop an enrichment capability for nuclear weapons for at least five years and had used technology obtained from Pakistan (Washington Times, 18 October 2002).
Israel.
Israel is also thought to possess an arsenal of potentially up to several hundred nuclear warheads based on estimates of the amount of fissile material produced by Israel. This has never been openly confirmed or denied however, due to Israel's policy of deliberate ambiguity.
An Israeli nuclear installation is located about ten kilometers to the south of Dimona, the Negev Nuclear Research Center. Its construction commenced in 1958, with French assistance. The official reason given by the Israeli and French governments was to build a nuclear reactor to power a "desalination plant", in order to "green the Negev". The purpose of the Dimona plant is widely assumed to be the manufacturing of nuclear weapons, and the majority of defense experts have concluded that it does in fact do that. However, the Israeli government refuses to confirm or deny this publicly, a policy it refers to as "ambiguity".
Norway sold 20 tonnes of heavy water needed for the reactor to Israel in 1959 and 1960 in a secret deal. There were no "safeguards" required in this deal to prevent usage of the heavy water for non-peaceful purposes. The British newspaper "Daily Express" accused Israel of working on a bomb in 1960.
When the United States intelligence community discovered the purpose of the Dimona plant in the early 1960s, it demanded that Israel agree to international inspections. Israel agreed, but on a condition that U.S., rather than IAEA, inspectors were used, and that Israel would receive advanced notice of all inspections.
Some claim that because Israel knew the schedule of the inspectors' visits, it was able to hide the alleged purpose of the site from the inspectors by installing temporary false walls and other devices before each inspection. The inspectors eventually informed the U.S. government that their inspections were useless due to Israeli restrictions on what areas of the facility they could inspect. In 1969, the United States terminated the inspections.
In 1986, Mordechai Vanunu, a former technician at the Dimona plant, revealed to the media some evidence of Israel's nuclear program. Israeli agents arrested him from Italy, drugged him and transported him to Israel, and an Israeli court then tried him in secret on charges of treason and espionage, and sentenced him to eighteen years imprisonment. He was freed on 21 April 2004, but was severely limited by the Israeli government. He was arrested again on 11 November 2004, though formal charges were not immediately filed.
Comments on photographs taken by Mordechai Vanunu inside the Negev Nuclear Research Center have been made by prominent scientists. British nuclear weapons scientist Frank Barnaby, who questioned Vanunu over several days, estimated Israel had enough plutonium for about 150 weapons. Ted Taylor, a bomb designer employed by the United States of America has confirmed the several hundred warhead estimate based on Vanunu's photographs.
According to Lieutenant Colonel Warner D. Farr in a report to the USAF Counterproliferation Center while France was previously a leader in nuclear research "Israel and France were at a similar level of expertise after the war, and Israeli scientists could make significant contributions to the French effort." In 1986 Francis Perrin, French high-commissioner for atomic energy from 1951 to 1970 stated that in 1949 Israeli scientists were invited to the Saclay nuclear research facility, this cooperation leading to a joint effort including sharing of knowledge between French and Israeli scientists especially those with knowledge from the Manhattan Project.
Nuclear arms control in South Asia.
The public stance of the two states on non-proliferation differs markedly. Pakistan has initiated a series of regional security proposals. It has repeatedly proposed a nuclear free zone in South Asia and has proclaimed its willingness to engage in nuclear disarmament and to sign the Non-Proliferation Treaty if India would do so. It has endorsed a United States proposal for a regional five power conference to consider non-proliferation in South Asia.
India has taken the view that solutions to regional security issues should be found at the international rather than the regional level, since its chief concern is with China. It therefore rejects Pakistan's proposals.
Instead, the 'Gandhi Plan', put forward in 1988, proposed the revision of the Non-Proliferation Treaty, which it regards as inherently discriminatory in favor of the nuclear-weapon States, and a timetable for complete nuclear weapons disarmament. It endorsed early proposals for a Comprehensive Test Ban Treaty and for an international convention to ban the production of highly enriched uranium and plutonium for weapons purposes, known as the 'cut-off' convention.
The United States for some years, especially under the Clinton administration, pursued a variety of initiatives to persuade India and Pakistan to abandon their nuclear weapons programs and to accept comprehensive international safeguards on all their nuclear activities. To this end, the Clinton administration proposed a conference of the five nuclear-weapon states, Japan, Germany, India and Pakistan.
India refused this and similar previous proposals, and countered with demands that other potential weapons states, such as Iran and North Korea, should be invited, and that regional limitations would only be acceptable if they were accepted equally by China. The United States would not accept the participation of Iran and North Korea and these initiatives have lapsed.
Another, more recent approach, centers on 'capping' the production of fissile material for weapons purposes, which would hopefully be followed by 'roll back'. To this end, India and the United States jointly sponsored a UN General Assembly resolution in 1993 calling for negotiations for a 'cut-off' convention. Should India and Pakistan join such a convention, they would have to agree to halt the production of fissile materials for weapons and to accept international verification on their relevant nuclear facilities (enrichment and reprocessing plants). It appears that India is now prepared to join negotiations regarding such a Cut-off Treaty, under the UN Conference on Disarmament.
Bilateral confidence-building measures between India and Pakistan to reduce the prospects of confrontation have been limited. In 1990 each side ratified a treaty not to attack the other's nuclear installations, and at the end of 1991 they provided one another with a list showing the location of all their nuclear plants, even though the respective lists were regarded as not being wholly accurate. Early in 1994 India proposed a bilateral agreement for a 'no first use' of nuclear weapons and an extension of the 'no attack' treaty to cover civilian and industrial targets as well as nuclear installations.
Having promoted the Comprehensive Test Ban Treaty since 1954, India dropped its support in 1995 and in 1996 attempted to block the Treaty. Following the 1998 tests the question has been reopened and both Pakistan and India have indicated their intention to sign the CTBT. Indian ratification may be conditional upon the five weapons states agreeing to specific reductions in nuclear arsenals. The UN Conference on Disarmament has also called upon both countries "to accede without delay to the Non-Proliferation Treaty", presumably as non-weapons states.
NPT signatories.
Egypt.
In 2004 and 2005, Egypt disclosed past undeclared nuclear activities and material to the IAEA. In 2007 and 2008, high enriched and low enriched uranium particles were found in environmental samples taken in Egypt. In 2008, the IAEA states Egypt's statements were consistent with its own findings. In May 2009, "Reuters" reported that the IAEA was conducting further investigation in Egypt.
Iran.
In 2003, the IAEA reported that Iran had been in breach of its obligations to comply with provisions of its safeguard agreement. In 2005, the IAEA Board of Governors voted in a rare non-consensus decision to find Iran in non-compliance with its NPT Safeguards Agreement and to report that non-compliance to the UN Security Council. In response, the UN Security Council passed a series of resolutions citing concerns about the program. Iran's representative to the UN argues sanctions compel Iran to abandon its rights under the Nuclear Nonproliferation Treaty to peaceful nuclear technology. Iran says its uranium enrichment program is exclusively for peaceful purposes and has enriched uranium to "less than 5 percent," consistent with fuel for a nuclear power plant and significantly below the purity of WEU (around 90%) typically used in a weapons program. The director general of the International Atomic Energy Agency, Yukiya Amano, said in 2009 he had not seen any evidence in IAEA official documents that Iran was developing nuclear weapons.
Iraq.
Up to the late 1980s it was generally assumed that any undeclared nuclear activities would have to be based on the diversion of nuclear material from safeguards. States acknowledged the possibility of nuclear activities entirely separate from those covered by safeguards, but it was assumed they would be detected by national intelligence activities. There was no particular effort by IAEA to attempt to detect them.
Iraq had been making efforts to secure a nuclear potential since the 1960s. In the late 1970s a specialised plant, Osiraq, was constructed near Baghdad. The plant was attacked during the Iran–Iraq War and was destroyed by Israeli bombers in June 1981.
Not until the 1990 NPT Review Conference did some states raise the possibility of making more use of (for example) provisions for "special inspections" in existing NPT Safeguards Agreements. Special inspections can be undertaken at locations other than those where safeguards routinely apply, if there is reason to believe there may be undeclared material or activities.
After inspections in Iraq following the UN Gulf War cease-fire resolution showed the extent of Iraq's clandestine nuclear weapons program, it became clear that the IAEA would have to broaden the scope of its activities. Iraq was an NPT Party, and had thus agreed to place all its nuclear material under IAEA safeguards. But the inspections revealed that it had been pursuing an extensive clandestine uranium enrichment programme, as well as a nuclear weapons design programme.
The main thrust of Iraq's uranium enrichment program was the development of technology for electromagnetic isotope separation (EMIS) of indigenous uranium. This uses the same principles as a mass spectrometer (albeit on a much larger scale). Ions of uranium-238 and uranium-235 are separated because they describe arcs of different radii when they move through a magnetic field. This process was used in the Manhattan Project to make the highly enriched uranium used in the Hiroshima bomb, but was abandoned soon afterwards.
The Iraqis did the basic research work at their nuclear research establishment at Tuwaitha, near Baghdad, and were building two full-scale facilities at Tarmiya and Ash Sharqat, north of Baghdad. However, when the war broke out, only a few separators had been installed at Tarmiya, and none at Ash Sharqat.
The Iraqis were also very interested in centrifuge enrichment, and had been able to acquire some components including some carbon-fibre rotors, which they were at an early stage of testing. In May 1998, Newsweek reported that Abdul Qadeer Khan had sent Iraq centrifuge designs, which were apparently confiscated by the UNMOVIC officials. Iraqi officials said "the documents were authentic but that they had not agreed to work with A. Q. Khan, fearing an ISI sting operation, due to strained relations between two countries. The Government of Pakistan and A. Q. Khan strongly denied this allegation whilst the government declared the evidence to be "fraudulent".
They were clearly in violation of their NPT and safeguards obligations, and the IAEA Board of Governors ruled to that effect. The UN Security Council then ordered the IAEA to remove, destroy or render harmless Iraq's nuclear weapons capability. This was done by mid-1998, but Iraq then ceased all cooperation with the UN, so the IAEA withdrew from this work.
The revelations from Iraq provided the impetus for a very far-reaching reconsideration of what safeguards are intended to achieve.
Libya.
Libya possesses ballistic missiles and previously pursued nuclear weapons under the leadership of Muammar Gaddafi. On 19 December 2003, Gaddafi announced that Libya would voluntarily eliminate all materials, equipment and programs that could lead to internationally proscribed weapons, including weapons of mass destruction and long-range ballistic missiles. Libya signed the Nuclear Non-Proliferation Treaty (NPT) in 1968 and ratified it in 1975, and concluded a safeguards agreement with the International Atomic Energy Agency (IAEA) in 1980. In March 2004, the IAEA Board of Governors welcomed Libya's decision to eliminate its formerly undeclared nuclear program, which it found had violated Libya's safeguards agreement, and approved Libya's Additional Protocol. The United States and the United Kingdom assisted Libya in removing equipment and material from its nuclear weapons program, with independent verification by the IAEA.
Myanmar.
A report in the "Sydney Morning Herald" and "Searchina", a Japanese newspaper, report that two Myanmarese defectors saying that the Myanmar junta was secretly building a nuclear reactor and plutonium extraction facility with North Korea's help, with the aim of acquiring its first nuclear bomb in five years. According to the report, "The secret complex, much of it in caves tunnelled into a mountain at Naung Laing in northern Burma, runs parallel to a civilian reactor being built at another site by Russia that both the Russians and Burmese say will be put under international safeguards." In 2002, Myanmar had notified IAEA of its intention to pursue a civilian nuclear programme. Later, Russia announced that it would build a nuclear reactor in Myanmar. There have also been reports that two Pakistani scientists, from the AQ Khan stable, had been dispatched to Myanmar where they had settled down, to help Myanmar's project. Recently, the David Albright-led Institute for Science and International Security (ISIS) rang alarm bells about Myanmar attempting a nuclear project with North Korean help. If true, the full weight of international pressure will be brought against Myanmar, said officials familiar with developments. But equally, the information that has been peddled by the defectors is also "preliminary" and could be used by the west to turn the screws on Myanmar—on democracy and human rights issues—in the run-up to the elections in the country in 2010. During an ASEAN meeting in Thailand in July 2009, US secretary of state Hillary Clinton highlighted concerns of the North Korean link. "We know there are also growing concerns about military cooperation between North Korea and Burma which we take very seriously," Clinton said. However, in 2012, after contact with the American president, Barack Obama, the Burmese leader, Thein Sein, renounced military ties with DPRK (North Korea).
North Korea.
The Democratic People's Republic of Korea (DPRK) acceded to the NPT in 1985 as a condition for the supply of a nuclear power station by the USSR. However, it delayed concluding its NPT Safeguards Agreement with the IAEA, a process which should take only 18 months, until April 1992.
During that period, it brought into operation a small gas-cooled, graphite-moderated, natural-uranium (metal) fuelled "Experimental Power Reactor" of about 25 MWt (5 MWe), based on the UK Magnox design. While this was a well-suited design to start a wholly indigenous nuclear reactor development, it also exhibited all the features of a small plutonium production reactor for weapons purposes. North Korea also made substantial progress in the construction of two larger reactors designed on the same principles, a prototype of about 200 MWt (50 MWe), and a full-scale version of about 800 MWt (200 MWe). They made only slow progress; construction halted on both in 1994 and has not resumed. Both reactors have degraded considerably since that time and would take significant efforts to refurbish.
In addition it completed and commissioned a reprocessing plant that makes the Magnox spent nuclear fuel safe, recovering uranium and plutonium. That plutonium, if the fuel was only irradiated to a very low burn-up, would have been in a form very suitable for weapons. Although all these facilities at Yongbyon were to be under safeguards, there was always the risk that at some stage, the DPRK would withdraw from the NPT and use the plutonium for weapons.
One of the first steps in applying NPT safeguards is for the IAEA to verify the initial stocks of uranium and plutonium to ensure that all the nuclear materials in the country have been declared for safeguards purposes. While undertaking this work in 1992, IAEA inspectors found discrepancies which indicated that the reprocessing plant had been used more often than the DPRK had declared, which suggested that the DPRK could have weapons-grade plutonium which it had not declared to the IAEA. Information passed to the IAEA by a Member State (as required by the IAEA) supported that suggestion by indicating that the DPRK had two undeclared waste or other storage sites.
In February 1993 the IAEA called on the DPRK to allow special inspections of the two sites so that the initial stocks of nuclear material could be verified. The DPRK refused, and on 12 March announced its intention to withdraw from the NPT (three months' notice is required). In April 1993 the IAEA Board concluded that the DPRK was in non-compliance with its safeguards obligations and reported the matter to the UN Security Council. In June 1993 the DPRK announced that it had "suspended" its withdrawal from the NPT, but subsequently claimed a "special status" with respect to its safeguards obligations. This was rejected by IAEA.
Once the DPRK's non-compliance had been reported to the UN Security Council, the essential part of the IAEA's mission had been completed. Inspections in the DPRK continued, although inspectors were increasingly hampered in what they were permitted to do by the DPRK's claim of a "special status". However, some 8,000 corroding fuel rods associated with the experimental reactor have remained under close surveillance.
Following bilateral negotiations between the United States and the DPRK, and the conclusion of the Agreed Framework in October 1994, the IAEA has been given additional responsibilities. The agreement requires a freeze on the operation and construction of the DPRK's plutonium production reactors and their related facilities, and the IAEA is responsible for monitoring the freeze until the facilities are eventually dismantled. The DPRK remains uncooperative with the IAEA verification work and has yet to comply with its safeguards agreement.
While Iraq was defeated in a war, allowing the UN the opportunity to seek out and destroy its nuclear weapons programme as part of the cease-fire conditions, the DPRK was not defeated, nor was it vulnerable to other measures, such as trade sanctions. It can scarcely afford to import anything, and sanctions on vital commodities, such as oil, would either be ineffective or risk provoking war.
Ultimately, the DPRK was persuaded to stop what appeared to be its nuclear weapons programme in exchange, under the agreed framework, for about US$5 billion in energy-related assistance. This included two 1000 MWe light water nuclear power reactors based on an advanced U.S. System-80 design.
In January 2003 the DPRK withdrew from the NPT. In response, a series of discussions among the DPRK, the United States, and China, a series of six-party talks (the parties being the DPRK, the ROK, China, Japan, the United States and Russia) were held in Beijing; the first beginning in April 2004 concerning North Korea's weapons program.
On 10 January 2005, North Korea declared that it was in the possession of nuclear weapons. On 19 September 2005, the fourth round of the Six-Party Talks ended with a joint statement in which North Korea agreed to end its nuclear programs and return to the NPT in exchange for diplomatic, energy and economic assistance. However, by the end of 2005 the DPRK had halted all six-party talks because the United States froze certain DPRK international financial assets such as those in a bank in Macau.
On 9 October 2006, North Korea announced that it has performed its first-ever nuclear weapon test. On 18 December 2006, the six-party talks finally resumed. On 13 February 2007, the parties announced "Initial Actions" to implement the 2005 joint statement including shutdown and disablement of North Korean nuclear facilities in exchange for energy assistance. Reacting to UN sanctions imposed after missile tests in April 2009, North Korea withdrew from the six-party talks, restarted its nuclear facilities and conducted a second nuclear test on 25 May 2009.
On 12 February 2013, North Korea conducted an underground nuclear explosion with an estimated yield of 6 to 7 kilotonnes. The detonation registered a magnitude 4.9 disturbance in the area around the epicenter.
"See also: North Korea and weapons of mass destruction and Six-party talks"
Russia.
Security of nuclear weapons in Russia remains a matter of concern. According to high-ranking Russian SVR defector Tretyakov, he had a meeting with two Russian businessman representing a state-created "C-W" corporation in 1991. They came up with a project of destroying large quantities of chemical wastes collected from Western countries at the island of Novaya Zemlya (a test place for Soviet nuclear weapons) using an underground nuclear blast. The project was rejected by Canadian representatives, but one of the businessmen told Tretyakov that he keeps his own nuclear bomb at his dacha outside Moscow. Tretyakov thought that man was insane, but the "businessmen" (Vladimir K. Dmitriev) replied: "Do not be so naive. With economic conditions the way they are in Russia today, anyone with enough money can buy a nuclear bomb. It's no big deal really".
South Africa.
In 1991, South Africa acceded to the NPT, concluded a comprehensive safeguards agreement with the IAEA, and submitted a report on its nuclear material subject to safeguards. At the time, the state had a nuclear power programme producing nearly 10% of the country's electricity, whereas Iraq and North Korea only had research reactors.
The IAEA's initial verification task was complicated by South Africa's announcement that between 1979 and 1989 it built and then dismantled a number of nuclear weapons. South Africa asked the IAEA to verify the conclusion of its weapons programme. In 1995 the IAEA declared that it was satisfied all materials were accounted for and the weapons programme had been terminated and dismantled.
South Africa has signed the NPT, and now holds the distinction of being the only known state to have indigenously produced nuclear weapons, and then verifiably dismantled them.
Syria.
On 6 September 2007, Israel bombed an officially unidentified site in Syria which it later asserted was a nuclear reactor under construction ("see Operation Orchard"). The alleged reactor was not asserted to be operational and it was not asserted that nuclear material had been introduced into it. Syria said the site was a military site and was not involved in any nuclear activities. The IAEA requested Syria to provide further access to the site and any other locations where the debris and equipment from the building had been stored. Syria denounced what it called the Western "fabrication and forging of facts" in regards to the incident. IAEA Director General Mohamed ElBaradei criticized the strikes and deplored that information regarding the matter had not been shared with his agency earlier.
Breakout capability.
For a state that does not possess nuclear weapons, the capability to produce one or more weapons quickly and with little warning is called a breakout capability.
Arguments for and against proliferation.
There has been much debate in the academic study of International Security as to the advisability of proliferation. In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles DeGaulle, argued in books like "The Balance of Terror: Strategy for the Nuclear Age" (1961) that mere possession of a nuclear arsenal, what the French called the "force de frappe", was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability.
Some very prominent neo-realist scholars, such as Kenneth Waltz, Emeritus Professor of Political Science at UC Berkeley and Adjunct Senior Research Scholar at Columbia University, and John Mearsheimer, R. Wendell Harrison Distinguished Service Professor of Political Science at the University of Chicago, continue to argue along the lines of Gallois (though these scholars rarely acknowledge their intellectual debt to Gallois and his contemporaries). Specifically, these scholars advocate some forms of nuclear proliferation, arguing that it will decrease the likelihood of war, especially in troubled regions of the world. Aside from the majority opinion which opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favor selective proliferation, and those such as Waltz, who advocate a laissez-faire attitude to programs like North Korea's.
Total proliferation.
In embryo, Waltz argues that the logic of mutually assured destruction (MAD) should work in all security environments, regardless of historical tensions or recent hostility. He sees the Cold War as the ultimate proof of MAD logicthe only occasion when enmity between two Great Powers did not result in military conflict. This was, he argues, because nuclear weapons promote caution in decision-makers. Neither Washington nor Moscow would risk a nuclear apocalypse to advance territorial or power goals, hence a peaceful stalemate ensued (Waltz and Sagan (2003), p. 24). Waltz believes there to be no reason why this effect would not occur in all circumstances.
Selective proliferation.
John Mearsheimer would not support Waltz's optimism in the majority of potential instances; however, he has argued for nuclear proliferation as policy in certain places, such as post–Cold War Europe. In two famous articles, Professor Mearsheimer opines that Europe is bound to return to its pre–Cold War environment of regular conflagration and suspicion at some point in the future. He advocates arming both Germany and Ukraine with nuclear weaponry in order to achieve a balance of power between these states in the east and France/UK in the west. If this does not occur, he is certain that war will eventually break out on the European continent.
Another separate argument against Waltz's open proliferation and in favor of Mearsheimer's selective distribution is the possibility of nuclear terrorism. Some countries included in the aforementioned laissez-faire distribution could predispose the transfer of nuclear materials or a bomb falling into the hands of groups not affiliated with any governments. Such countries would not have the political will or ability to safeguard attempts at devices being transferred to a third party. Not being deterred by self-annihilation, terrorism groups could push forth their own nuclear agendas or be used as shadow fronts to carry out the attack plans by mentioned unstable governments.
Arguments against both positions.
There are numerous arguments presented against both selective and total proliferation, generally targeting the very neorealist assumptions (such as the primacy of military security in state agendas, the weakness of international institutions, and the long-run unimportance of economic integration and globalization to state strategy) its proponents tend to make. With respect to Mearsheimer's specific example of Europe, many economists and neoliberals argue that the economic integration of Europe through the development of the European Union has made war in most of the European continent so disastrous economically so as to serve as an effective deterrent. Constructivists take this one step further, frequently arguing that the development of EU political institutions has led or will lead to the development of a nascent European identity, which most states on the European continent wish to partake in to some degree or another, and which makes all states within or aspiring to be within the EU regard war between them as unthinkable.
As for Waltz, the general opinion is that most states are not in a position to safely guard against nuclear use, that he underestimates the long-standing antipathy in many regions, and that weak states will be unable to prevent – or will actively provide for – the disastrous possibility of nuclear terrorism. Waltz has dealt with all of these objections at some point in his work; though to many, he has not adequately responded (Betts (2000)).
The Learning Channel documentary Doomsday: "On The Brink" illustrated 40 years of U.S. and Soviet nuclear weapons accidents. Even the 1995 Norwegian rocket incident demonstrated a potential scenario in which Russian democratization and military downsizing at the end of the Cold War did not eliminate the danger of accidental nuclear war through command and control errors. After asking: might a future Russian ruler or renegade Russian general be tempted to use nuclear weapons to make foreign policy? The documentary writers revealed a greater danger of Russian security over its nuclear stocks, but especially the ultimate danger of human nature to want the ultimate weapon of mass destruction to exercise political and military power. Future world leaders might not understand how close the Soviets, Russians, and Americans were to doomsday, how easy it all seemed because apocalypse was avoided for a mere 40 years between rivals, politicians not terrorists, who loved their children and did not want to die, against 30,000 years of human prehistory. History and military experts agree that proliferation can be slowed, but never stopped (technology cannot be uninvented).
Proliferation begets proliferation.
Proliferation begets proliferation is a concept described by Scott Sagan in his article, "Why Do States Build Nuclear Weapons?". This concept can be described as a strategic chain reaction. If one state produces a nuclear weapon it creates almost a domino effect within the region. States in the region will seek to acquire nuclear weapons to balance or eliminate the security threat. Sagan describes this reaction best in his article when he states, “Every time one state develops nuclear weapons to balance against its main rival, it also creates a nuclear threat to another region, which then has to initiate its own nuclear weapons program to maintain its national security”. Going back through history we can see how this has taken place. When the United States demonstrated that it had nuclear power capabilities after the bombing of Hiroshima and Nagasaki, the Russians started to develop their program in preparation for the Cold War. With the Russian military buildup, France and the United Kingdom perceived this as a security threat and therefore they pursued nuclear weapons (Sagan, pg 71).
Iran.
Former Iranian President Mahmoud Ahmadinejad has been a frequent critic of the concept of nuclear apartheid as it has been put into practice by several countries, particularly the United States. In an interview with CNN's Christiane Amanpour, Ahmadinejad said that Iran was "against 'nuclear apartheid,' which means some have the right to possess it, use the fuel, and then sell it to another country for 10 times its value. We're against that. We say clean energy is the right of all countries. But also it is the duty and the responsibility of all countries, including ours, to set up frameworks to stop the proliferation of it." Hours after that interview, he spoke passionately in favor of Iran's right to develop nuclear technology, claiming the nation should have the same liberties.
Iran is a signatory of the Nuclear Non-Proliferation Treaty and claims that any work done in regards to nuclear technology is related only to civilian uses, which is acceptable under the treaty. Iran violated its safeguards obligations under the treaty by performing uranium-enrichment in secret, after which the United Nations Security Council ordered Iran to suspend all uranium-enrichment until July 2015.
India.
India has also been discussed in the context of nuclear apartheid. India has consistently attempted to pass measures that would call for full international disarmament, however they have not succeeded due to protests from those states that already have nuclear weapons. In light of this, India viewed nuclear weapons as a necessary right for all nations as long as certain states were still in possession of nuclear weapons. India stated that nuclear issues were directly related to national security.
Years before India's first underground nuclear test in 1998, the Comprehensive Nuclear-Test-Ban Treaty was passed. Some have argued that coercive language was used in an attempt to persuade India to sign the treaty, which was pushed for heavily by neighboring China. India viewed the treaty as a means for countries that already had nuclear weapons, primarily the five nations of the United Nations Security Council, to keep their weapons while ensuring that no other nations could develop them.

</doc>
<doc id="22159" url="https://en.wikipedia.org/wiki?curid=22159" title="NPT">
NPT

NPT may refer to:

</doc>
<doc id="22161" url="https://en.wikipedia.org/wiki?curid=22161" title="Nuclear energy">
Nuclear energy

Nuclear energy may refer to:

</doc>
<doc id="22164" url="https://en.wikipedia.org/wiki?curid=22164" title="Netlist">
Netlist

In electronic design, a netlist is a description of the connectivity of an electronic circuit. A single netlist is effectively a collection of several related lists. In its simplest form, a netlist consists of a list of the terminals ("pins") of the electronic components in a circuit and a list of the electrical conductors that interconnect the terminals. A net is a conductor that interconnects two or more component terminals. 
The structure, complexity and representation of netlists can vary considerably, but the fundamental purpose of every netlist is to convey connectivity information. Netlists usually provide nothing more than instances, nets, and perhaps some attributes. If they express much more than this, they are usually considered to be a hardware description language such as Verilog or VHDL, or one of several languages specifically designed for input to simulators.
Netlists can be "physical" or "logical", "instance-based" or "net-based", and "flat" or "hierarchical". The latter can be either "folded" or "unfolded".
Contents and structure of a netlist.
Most netlists either contain or refer to descriptions of the parts or devices used.
Each time a part is used in a netlist, this is called an "instance".
Thus, each instance has a "master", or "definition".
These definitions will usually list the connections that can be made to that kind of device, and some basic properties of that device.
These connection points are called "ports" or "pins", among several other names.
An "instance" could be anything from a MOSFET transistor or a bipolar transistor, to a resistor, capacitor, or integrated circuit chip.
Instances have "ports". In the case of a vacuum cleaner, these ports would be the three metal prongs in the plug. Each port has a name, and in continuing the vacuum cleaner example, they might be "Neutral", "Live" and "Ground". Usually, each instance will have a unique name, so that if you have two instances of vacuum cleaners, one might be "vac1" and the other "vac2". Besides their names, they might otherwise be identical.
Nets are the "wires" that connect things together in the circuit. There may or may not be any special attributes associated with the nets in a design, depending on the particular language the netlist is written in, and that language's features.
Instance based netlists usually provide a list of the instances used in a design.
Along with each instance, either an ordered list of net names is provided, or a list of pairs provided, of an instance port name, along with the net name to which that port is connected.
In this kind of description, the list of nets can be gathered from the connection lists, and there is no place to associate particular attributes with the nets themselves.
SPICE is an example of instance-based netlists.
Net-based netlists usually describe all the instances and their attributes, then describe each net, and say which port they are connected on each instance.
This allows for attributes to be associated with nets.
EDIF is probably the most famous of the net-based netlists.
Hierarchy.
In large designs, it is a common practice to split the design into pieces, each piece becoming a "definition" which can be used as instances in the design. In the vacuum cleaner analogy, one might have a vacuum cleaner definition with its ports, but now this definition would also include a full description of the machine's internal components and how they connect (motors, switches, etc.), like a wiring diagram does. 
A definition which includes no instances is called a "primitive" (or a "leaf", or other names); whereas a definition which includes instances is "hierarchical".
A "folded" hierarchy allows a single definition to be represented several times by instances. An "unfolded" hierarchy does not allow a definition to be used more than once in the hierarchy. 
Folded hierarchies can be extremely compact. A small netlist of just a few instances can describe designs with a very large number of instances. For example, suppose definition A is a simple primitive, like a memory cell. Then suppose definition B contains 32 instances of A; C contains 32 instances of B; D contains 32 instances of C; and E contains 32 instances of D. The design now contains 5 definitions (A through E) and 128 instances. Yet, E describes a circuit that contains over a million memory cells.
Unfolding.
In a "flat" design, only primitives are instanced. Hierarchical designs can be recursively "exploded" ("flattened") by creating a new copy (with a new name) of each definition each time it is used. If the design is highly folded, expanding it like this will result in a much larger netlist database, but preserves the hierarchy dependencies. Given a hierarchical netlist, the list of instance names in a path from the root definition to a primitive instance specifies the single unique path to that primitive. The paths to every primitive, taken together, comprise a large but flat netlist that is exactly equivalent to the compact hierarchical version.
Backannotation.
Backannotation are data that could be added to a hierarchical netlist. Usually they are kept separate from the netlist, because several such alternate sets of data could be applied to a single netlist. These data may have been extracted from a physical design, and might provide extra information for more accurate simulations. Usually the data are composed of a hierarchical path and a piece of data for that primitive or finding the values of RC delay due to interconnection.
Inheritance.
Another concept often used in netlists is that of inheritance. Suppose a definition of a capacitor has an associated attribute called "Capacitance", corresponding to the physical property of the same name, with a default value of "100 pF" (100 picofarads). Each instance of this capacitor might also have such an attribute, only with a different value of capacitance. And other instances might not associate any capacitance at all. In the case where no capacitance is specified for an instance, the instance will "inherit" the 100 pF value from its definition. A value specified will "override" the value on the definition. If a great number of attributes end up being the same as on the definition, a great amount of information can be "inherited", and not have to be redundantly specified in the netlist, saving space, and making the design easier to read by both machines and people.

</doc>
<doc id="22165" url="https://en.wikipedia.org/wiki?curid=22165" title="Nuclear disarmament">
Nuclear disarmament

Nuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-weapon-free world, in which nuclear weapons are completely eliminated.
Nuclear disarmament groups include the Campaign for Nuclear Disarmament, Peace Action, Greenpeace, International Physicians for the Prevention of Nuclear War, Mayors for Peace, Global Zero, the International Campaign to Abolish Nuclear Weapons, and the Nuclear Age Peace Foundation. There have been many large anti-nuclear demonstrations and protests. On June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history.
In recent years, some U.S. elder statesmen have also advocated nuclear disarmament. Sam Nunn, William Perry, Henry Kissinger, and George Shultz have called upon governments to embrace the vision of a world free of nuclear weapons, and in various op-ed columns have proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Organisations such as Global Zero, an international non-partisan group of 300 world leaders dedicated to achieving nuclear disarmament, have also been established.
Proponents of nuclear disarmament say that it would lessen the probability of nuclear war occurring, especially accidentally. Critics of nuclear disarmament say that it would undermine deterrence.
History.
In 1945 in the New Mexico desert, American scientists conducted "Trinity," the first nuclear weapons test, marking the beginning of the atomic age. Even before the Trinity test, national leaders debated the impact of nuclear weapons on domestic and foreign policy. Also involved in the debate about nuclear weapons policy was the scientific community, through professional associations such as the Federation of Atomic Scientists and the Pugwash Conference on Science and World Affairs.
On August 6, 1945, towards the end of World War II, the Little Boy device was detonated over the Japanese city of Hiroshima. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings (including the headquarters of the 2nd General Army and Fifth Division) and killed approximately 75,000 people, among them 20,000 Japanese soldiers and 20,000 Koreans. Detonation of the Fat Man device exploded over the Japanese city of Nagasaki three days later on 9 August 1945, destroying 60% of the city and killing approximately 35,000 people, among them 23,200-28,200 Japanese civilian munitions workers and 150 Japanese soldiers. Subsequently, the world’s nuclear weapons stockpiles grew.
Operation Crossroads was a series of nuclear weapon tests conducted by the United States at Bikini Atoll in the Pacific Ocean in the summer of 1946. Its purpose was to test the effect of nuclear weapons on naval ships. Pressure to cancel Operation Crossroads came from scientists and diplomats. Manhattan Project scientists argued that further nuclear testing was unnecessary and environmentally dangerous. A Los Alamos study warned "the water near a recent surface explosion will be a witch's brew" of radioactivity. To prepare the atoll for the nuclear tests, Bikini's native residents were evicted from their homes and resettled on smaller, uninhabited islands where they were unable to sustain themselves.
Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when a Hydrogen bomb test in the Pacific contaminated the crew of the Japanese fishing boat "Lucky Dragon". One of the fishermen died in Japan seven months later. The incident caused widespread concern around the world and "provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries". The anti-nuclear weapons movement grew rapidly because for many people the atomic bomb "encapsulated the very worst direction in which society was moving".
Nuclear disarmament movement.
Peace movements emerged in Japan and in 1954 they converged to form a unified "Japanese Council Against Atomic and Hydrogen Bombs". Japanese opposition to the Pacific nuclear weapons tests was widespread, and "an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons". In the United Kingdom, the first Aldermaston March organised by the Direct Action Committee and supported by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. CND organised Aldermaston marches into the late 1960s when tens of thousands of people took part in the four-day events.
On November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.
In 1958, Linus Pauling and his wife presented the United Nations with the petition signed by more than 11,000 scientists calling for an end to nuclear-weapon testing. The "Baby Tooth Survey," headed by Dr Louise Reiss, demonstrated conclusively in 1961 that above-ground nuclear testing posed significant public health risks in the form of radioactive fallout spread primarily via milk from cows that had ingested contaminated grass. Public pressure and the research results subsequently led to a moratorium on above-ground nuclear weapons testing, followed by the Partial Test Ban Treaty, signed in 1963 by John F. Kennedy and Nikita Khrushchev. On the day that the treaty went into force, the Nobel Prize Committee awarded Pauling the Nobel Peace Prize, describing him as "Linus Carl Pauling, who ever since 1946 has campaigned ceaselessly, not only against nuclear weapons tests, not only against the spread of these armaments, not only against their very use, but against all warfare as a means of solving international conflicts." Pauling started the International League of Humanists in 1974. He was president of the scientific advisory board of the World Union for Protection of Life and also one of the signatories of the Dubrovnik-Philadelphia Statement.
In the 1980s, a popular movement for nuclear disarmament again gained strength in the light of the weapons build-up and aggressive rhetoric of US President Ronald Reagan. Reagan had "a world free of nuclear weapons" as his personal mission, and was largely scorned for this in Europe. His officials tried to stop such talks but Reagan was able to start discussions on nuclear disarmament with Soviet Union. He changed the name "SALT" (Strategic Arms Limitation Talks) to "START" (Strategic Arms Reduction Talks).
On June 3, 1981, Thomas launched the White House Peace Vigil in Washington, D.C.. He was later joined on the vigil by anti-nuclear activists Concepcion Picciotto and Ellen Benjamin.
On June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history. International Day of Nuclear Disarmament protests were held on June 20, 1983 at 50 sites across the United States. In 1986, hundreds of people walked from Los Angeles to Washington DC in the Great Peace March for Global Nuclear Disarmament. There were many Nevada Desert Experience protests and peace camps at the Nevada Test Site during the 1980s and 1990s.
On May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades. In 2008, 2009, and 2010, there have been protests about, and campaigns against, several new nuclear reactor proposals in the United States.
There is an annual protest against U.S. nuclear weapons research at Lawrence Livermore National Laboratory in California and in the 2007 protest, 64 people were arrested. There have been a series of protests at the Nevada Test Site and in the April 2007 Nevada Desert Experience protest, 39 people were cited by police. There have been anti-nuclear protests at Naval Base Kitsap for many years, and several in 2008.
World Peace Council.
One of the earliest peace organisations to emerge after the Second World War was the World Peace Council, which was directed by the Communist Party of the Soviet Union through the Soviet Peace Committee. Its origins lay in the Communist Information Bureau's (Cominform) doctrine, put forward 1947, that the world was divided between peace-loving progressive forces led by the Soviet Union and warmongering capitalist countries led by the United States. In 1949, Cominform directed that peace "should now become the pivot of the entire activity of the Communist Parties", and most western Communist parties followed this policy. Lawrence Wittner, a historian of the post-war peace movement, argues that the Soviet Union devoted great efforts to the promotion of the WPC in the early post-war years because it feared an American attack and American superiority of arms at a time when the USA possessed the atom bomb but the Soviet Union had not yet developed it.
In 1950, the WPC launched its Stockholm Appeal calling for the absolute prohibition of nuclear weapons. The campaign won popular support, collecting, it is said, 560 million signatures in Europe, most from socialist countries, including 10 million in France (including that of the young Jacques Chirac), and 155 million signatures in the Soviet Union – the entire adult population. Several non-aligned peace groups who had distanced themselves from the WPC advised their supporters not to sign the Appeal.
The WPC had uneasy relations with the non-aligned peace movement and has been described as being caught in contradictions as "it sought to become a broad world movement while being instrumentalized increasingly to serve foreign policy in the Soviet Union and nominally socialist countries." From the 1950s until the late 1980s it tried to use non-aligned peace organizations to spread the Soviet point of view. At first there was limited co-operation between such groups and the WPC, but western delegates who tried to criticize the Soviet Union or the WPC's silence about Russian armaments were often shouted down at WPC conferences and by the early 1960s they had dissociated themselves from the WPC.
Arms reduction treaties.
After the 1986 Reykjavik Summit between U.S. President Ronald Reagan and the new Soviet General Secretary Mikhail Gorbachev, the United States and the Soviet Union concluded two important nuclear arms reduction treaties: the INF Treaty (1987) and START I (1991). After the end of the Cold War, the United States and the Russian Federation concluded the Strategic Offensive Reductions Treaty (2003) and the New START Treaty (2010).
In the Soviet Union (USSR), voices against Soviet nuclear weapons were few and far between since there was no widespread Freedom of speech and Freedom of the press as political factors. Certain citizens who had become prominent enough to safely criticize the Soviet government, such as Andrei Sakharov, did speak out against nuclear weapons, but that was to little effect. Dissident movements that emerged in the Soviet bloc in the 1980s drew attention to Soviet armaments, some demonstrating at congresses of the World Peace Council, but they were suppressed.
When the extreme danger intrinsic to nuclear war and the possession of nuclear weapons became apparent to all sides during the Cold War, a series of disarmament and nonproliferation treaties were agreed upon between the United States, the Soviet Union, and several other states throughout the world. Many of these treaties involved years of negotiations, and seemed to result in important steps in arms reductions and reducing the risk of nuclear war.
Key treaties
Only one country has been known to ever dismantle their nuclear arsenal completely—the apartheid government of South Africa apparently developed half a dozen crude fission weapons during the 1980s, but they were dismantled in the early 1990s.
United Nations.
In its landmark resolution 1653 of 1961, "Declaration on the prohibition of the use of nuclear and thermo-nuclear weapons," the UN General Assembly stated that use of nuclear weaponry “would exceed even the scope of war and cause indiscriminate suffering and destruction to mankind and civilization and, as such, is contrary to the rules of international law and to the laws of humanity”.
The UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.
Its goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.
Following the retirement of Sergio Duarte in February 2012, Angela Kane was appointed as the new High Representative for Disarmament Affairs.
U.S. nuclear policy.
Despite a general trend toward disarmament in the early 2000s, the George W. Bush administration repeatedly pushed to fund policies that would allegedly make nuclear weapons more usable in the post–Cold War environment. feel that even considering such programs harms the credibility of the United States as a proponent of nonproliferation.
Recent controversial U.S. nuclear policies.
Former U.S. officials Henry Kissinger, George Shultz, Bill Perry, and Sam Nunn (aka 'The Gang of Four' on Nuclear Deterrence)." proposed in January 2007 that the United States rededicate itself to the goal of eliminating nuclear weapons, concluding: "We endorse setting the goal of a world free of nuclear weapons and working energetically on the actions required to achieve that goal." Arguing a year later that "with nuclear weapons more widely available, deterrence is decreasingly effective and increasingly hazardous," the authors concluded that although "it is tempting and easy to say we can't get there from here, . . . we must chart a course” toward that goal." During his Presidential campaign, U.S. President Elect Barack Obama pledged to "set a goal of a world without nuclear weapons, and pursue it."
U.S. policy options for nuclear terrorism.
The United States has taken the lead in ensuring that nuclear materials globally are properly safeguarded. A popular program that has received bipartisan domestic support for over a decade is the Cooperative Threat Reduction Program (CTR). While this program has been deemed a success, many believe that its funding levels need to be increased so as to ensure that all dangerous nuclear materials are secured in the most expeditious manner possible. The CTR program has led to several other innovative and important nonproliferation programs that need to continue to be a budget priority in order to ensure that nuclear weapons do not spread to actors hostile to the United States.
Key programs:
Other states.
While the vast majority of states have adhered to the stipulations of the Nuclear Nonproliferation Treaty, a few states have either refused to sign the treaty or have pursued nuclear weapons programs while not being members of the treaty. Many view the pursuit of nuclear weapons by these states as a threat to nonproliferation and world peace, and therefore seek policies to discourage the spread of nuclear weapons to these states, a few of which are often described by the US as "rogue states".
Recent developments.
Eliminating nuclear weapons has long been an aim of the pacifist left. But now many mainstream politicians, academic analysts, and retired military leaders also advocate nuclear disarmament. Sam Nunn, William Perry, Henry Kissinger, and George Shultz have called upon governments to embrace the vision of a world free of nuclear weapons, and in three "Wall Street Journal" opeds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Nunn reinforced that agenda during a speech at the Harvard Kennedy School on October 21, 2008, saying, "I’m much more concerned about a terrorist without a return address that cannot be deterred than I am about deliberate war between nuclear powers. You can’t deter a group who is willing to commit suicide. We are in a different era. You have to understand the world has changed." In 2010, the four were featured in a documentary film entitled "Nuclear Tipping Point". The film is a visual and historical depiction of the ideas laid forth in the Wall Street Journal op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.
Global Zero is an international non-partisan group of 300 world leaders dedicated to achieving nuclear disarmament. The initiative, launched in December 2008, promotes a phased withdrawal and verification for the destruction of all devices held by official and unofficial members of the nuclear club. The Global Zero campaign works toward building an international consensus and a sustained global movement of leaders and citizens for the elimination of nuclear weapons. Goals include the initiation of United States-Russia bilateral negotiations for reductions to 1,000 total warheads each and commitments from the other key nuclear weapons countries to participate in multilateral negotiations for phased reductions of nuclear arsenals. Global Zero works to expand the diplomatic dialogue with key governments and continue to develop policy proposals on the critical issues related to the elimination of nuclear weapons.
The International Conference on Nuclear Disarmament took place in Oslo in February, 2008, and was organized by The Government of Norway, the Nuclear Threat Initiative and the Hoover Institute. The Conference was entitled "Achieving the Vision of a World Free of Nuclear Weapons" and had the purpose of building consensus between nuclear weapon states and non-nuclear weapon states in relation to the Nuclear Non-proliferation Treaty.
The Tehran International Conference on Disarmament and Non-Proliferation took place in Tehran in April 2010. The conference was held shortly after the signing of the New START, and resulted in a call of action toward eliminating all nuclear weapons. Representatives from 60 countries were invited to the conference. Non-governmental organizations were also present.
Among the prominent figures who have called for the abolition of nuclear weapons are "the philosopher Bertrand Russell, the entertainer Steve Allen, CNN’s Ted Turner, former Senator Claiborne Pell, Notre Dame president Theodore Hesburgh, South African Bishop Desmond Tutu and the Dalai Lama".
Others have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability–instability paradox, including in south Asia. Kenneth Waltz has argued that nuclear weapons have created a nuclear peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common prior to their invention at the end of World War II. In the July 2012 issue of Foreign Affairs Waltz took issue with the view of most U.S., European, and Israeli, commentators and policymakers that a nuclear-armed Iran would be unacceptable. Instead Waltz argues that it would probably be the best possible outcome, as it would restore stability to the Middle East by balancing Israel's regional monopoly on nuclear weapons. Professor John Mueller of Ohio State University, the author of "Atomic Obsession", has also dismissed the need to interfere with Iran's nuclear program and expressed that arms control measures are counterproductive. During a 2010 lecture at the University of Missouri, which was broadcast by C-SPAN, Dr. Mueller has also argued that the threat from nuclear weapons, especially nuclear terrorism, has been exaggerated, both in the popular media and by officials.
Former Secretary Kissinger says there is a new danger, which cannot be addressed by deterrence: "The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn’t operate in any comparable way". George Shultz has said, "If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable".

</doc>
<doc id="22170" url="https://en.wikipedia.org/wiki?curid=22170" title="Net (mathematics)">
Net (mathematics)

In mathematics, more specifically in general topology and related branches, a net or Moore–Smith sequence is a generalization of the notion of a sequence. In essence, a sequence is a function with domain the natural numbers, and in the context of topology, the codomain of this function is usually any topological space. However, in the context of topology, sequences do not fully encode all information about a function between topological spaces. In particular, the following two conditions are not equivalent in general for a map "f" between topological spaces "X" and "Y":
It is true, however, that condition 1 implies condition 2. The difficulty encountered when attempting to prove that condition 2 implies condition 1 lies in the fact that topological spaces are, in general, not first-countable.
If the first-countability axiom were imposed on the topological spaces in question, the two above conditions would be equivalent. In particular, the two conditions are equivalent for metric spaces.
The purpose of the concept of a net, first introduced by E. H. Moore and H. L. Smith in 1922, is to generalize the notion of a sequence so as to confirm the equivalence of the conditions (with "sequence" being replaced by "net" in condition 2). In particular, rather than being defined on a countable linearly ordered set, a net is defined on an arbitrary directed set. In particular, this allows theorems similar to that asserting the equivalence of condition 1 and condition 2, to hold in the context of topological spaces that do not necessarily have a countable or linearly ordered neighbourhood basis around a point. Therefore, while sequences do not encode sufficient information about functions between topological spaces, nets do because collections of open sets in topological spaces are much like directed sets in behaviour. The term "net" was coined by Kelley.
Nets are one of the many tools used in topology to generalize certain concepts that may only be general enough in the context of metric spaces. A related notion, that of the filter, was developed in 1937 by Henri Cartan.
Definition.
If "X" is a topological space, a "net" in "X" is a function from some directed set "A" to "X".
If "A" is a directed set, we often write a net from "A" to "X" in the form ("x"α), which expresses the fact that the element α in "A" is mapped to the element "x"α in "X".
Examples of nets.
Every non-empty totally ordered set is directed. Therefore every function on such a set is a net. In particular, the natural numbers with the usual order form such a set, and a sequence is a function on the natural numbers, so every sequence is a net.
Another important example is as follows. Given a point "x" in a topological space, let "N""x" denote the set of all neighbourhoods containing "x". Then "N""x" is a directed set, where the direction is given by reverse inclusion, so that "S" ≥ "T" if and only if "S" is contained in "T". For "S" in "N""x", let "x""S" be a point in "S". Then ("x""S") is a net. As "S" increases with respect to ≥, the points "x""S" in the net are constrained to lie in decreasing neighbourhoods of "x", so intuitively speaking, we are led to the idea that "x""S" must tend towards "x" in some sense. We can make this limiting concept precise.
Limits of nets.
If ("x"α) is a net from a directed set "A" into "X", and if "Y" is a subset of "X", then we say that ("x"α) is eventually in "Y" (or residually in "Y") if there exists an α in "A" so that for every β in "A" with β ≥ α, the point "x"β lies in "Y".
If ("x"α) is a net in the topological space "X", and "x" is an element of "X", we say that the net converges towards "x" or has limit "x" and write
if and only if
Intuitively, this means that the values "x"α come and stay as close as we want to "x" for large enough α.
Note that the example net given above on the neighborhood system of a point "x" does indeed converge to "x" according to this definition.
Given a base for the topology, in order to prove convergence of a net it is necessary and sufficient to prove that there exists some point "x", such that ("x"α) is eventually in all members of the base containing this putative limit.
Supplementary definitions.
Let φ be a net on "X" based on the directed set "D" and let "A" be a subset of "X", then φ is said to be frequently in (or cofinally in) "A" if for every α in "D" there exists some β ≥ α, β in "D", so that φ(β) is in "A".
A point "x" in "X" is said to be an accumulation point or cluster point of a net if (and only if) for every neighborhood "U" of "x", the net is frequently in "U".
A net φ on set "X" is called universal, or an ultranet if for every subset "A" of "X", either φ is eventually in "A" or φ is eventually in "X" − "A".
Examples.
Sequence in a topological space:
A sequence ("a"1, "a"2, ...) in a topological space "V" can be considered a net in "V" defined on N.
The net is eventually in a subset "Y" of "V" if there exists an N in N such that for every "n" ≥ "N", the point "a""n" is in "Y".
We have lim"n" "a""n" = "L" if and only if for every neighborhood "Y" of "L", the net is eventually in "Y".
The net is frequently in a subset "Y" of "V" if and only if for every "N" in N there exists some "n" ≥ "N" such that "a""n" is in "Y", that is, if and only if infinitely many elements of the sequence are in "Y". Thus a point "y" in "V" is a cluster point of the net if and only if every neighborhood "Y" of "y" contains infinitely many elements of the sequence.
Function from a metric space to a topological space:
Consider a function from a metric space "M" to a topological space "V", and a point "c" of "M". We direct the set "M"\{"c"} reversely according to distance from "c", that is, the relation is "has at least the same distance to "c" as", so that "large enough" with respect to the relation means "close enough to "c"". The function ƒ is a net in "V" defined on "M"\{"c"}.
The net ƒ is eventually in a subset "Y" of "V" if there exists an "a" in "M"\{"c"} such that for every "x" in "M"\{"c"} with d("x","c") ≤ d("a","c"), the point f("x") is in "Y".
We have lim"x" → "c" ƒ("x") = "L" if and only if for every neighborhood "Y" of "L", ƒ is eventually in "Y".
The net ƒ is frequently in a subset "Y" of "V" if and only if for every "a" in "M"\{"c"} there exists some "x" in "M"\{"c"} with d("x","c") ≤ d("a","c") such that "f(x)" is in "Y".
A point "y" in "V" is a cluster point of the net ƒ if and only if for every neighborhood "Y" of "y", the net is frequently in "Y".
Function from a well-ordered set to a topological space:
Consider a well-ordered set "c" with limit point "c", and a function ƒ from [0, "c") to a topological space "V". This function is a net on [0, "c").
It is eventually in a subset "Y" of "V" if there exists an "a" in [0, "c") such that for every "x" ≥ "a", the point f("x") is in "Y".
We have lim"x" → "c" ƒ("x") = "L" if and only if for every neighborhood "Y" of "L", ƒ is eventually in "Y".
The net ƒ is frequently in a subset "Y" of "V" if and only if for every "a" in [0, "c") there exists some "x" in ["a", "c") such that "f(x)" is in "Y".
A point "y" in "V" is a cluster point of the net ƒ if and only if for every neighborhood "Y" of "y", the net is frequently in "Y".
The first example is a special case of this with "c" = ω.
See also ordinal-indexed sequence.
Properties.
Virtually all concepts of topology can be rephrased in the language of nets and limits. This may be useful to guide the intuition since the notion of limit of a net is very similar to that of limit of a sequence. The following set of theorems and lemmas help cement that similarity:
Cauchy nets.
In a metric space or uniform space, one can speak of "Cauchy nets" in much the same way as Cauchy sequences.
The concept even generalises to Cauchy spaces.
Relation to filters.
A filter is another idea in topology that allows for a general definition for convergence in general topological spaces. The two ideas are equivalent in the sense that they give the same concept of convergence. More specifically, for every filter base an "associated net" can be constructed, and convergence of the filter base implies convergence of the associated net—and the other way around (for every net there is a filter base, and convergence of the net implies convergence of the filter base). For instance, any net formula_2 in formula_3 induces a filter base of tails formula_4 where the filter in formula_3 generated by this filter base is called the net's "eventuality filter." This correspondence allows for any theorem that can be proven with one concept to be proven with the other. For instance, continuity of a function from one topological space to the other can be characterized either by the convergence of a net in the domain implying the convergence of the corresponding net in the codomain, or by the same statement with filter bases.
Robert G. Bartle argues that despite their equivalence, it is useful to have both concepts. He argues that nets are enough like sequences to make natural proofs and definitions in analogy to sequences, especially ones using sequential elements, such as is common in analysis, while filters are most useful in algebraic topology. In any case, he shows how the two can be used in combination to prove various theorems in general topology.
Limit superior.
Limit superior and limit inferior of a net of real numbers can be defined in a similar manner as for sequences. Some authors work even with more general structures than the real line, like complete lattices.
For a net formula_6 we put
Limit superior of a net of real numbers has many properties analogous to the case of sequences, e.g.
where equality holds whenever one of the nets is convergent.

</doc>
<doc id="22171" url="https://en.wikipedia.org/wiki?curid=22171" title="Nuclear winter">
Nuclear winter

Nuclear winter (also known as atomic winter) is a largely hypothetical global climatic effect of city and natural wildfire firestorms. It is most frequently suggested to manifest as a result of the combined combustion pollution from the burning of at least 100 city sized areas at firestorm-intensity. The term was specifically coined to refer to computer model results where this smoke remained for years, or even decades, and caused massive planet-wide temperature drops ("winters") for as long as it remained.
The climate models in the public domain suggest that the ignition of 100 firestorms, comparable in intensity to that observed in Hiroshima in 1945, would produce a "small" nuclear winter. The burning of these firestorms would result in the injection of soot (specifically black carbon) into the Earth's stratosphere, producing an anti-greenhouse effect that lowers the Earth's surface temperature. The models conclude that the cumulative products of 100 of these firestorms would unmistakably cool the global climate by approximately 1 °C, largely eliminating the magnitude of anthropogenic global warming for two to three years. The authors speculate, but do not model, that this would have global agricultural losses as a consequence.
A much larger number of firestorms, however, were the initial focus of the computer modelers that coined the term in the 1980s. These were speculated to be a result of any countervalue city-airburst nuclear weapon during a US-Soviet total war. These larger firestorms were believed to cause nuclear winter conditions for as long as a decade, with summer cooling by about 20 °C in core agricultural regions of the US, Europe, and China, and by as much as 35 °C in Russia. This cooling was produced due to a 99% reduction in the natural solar radiation reaching the surface of the planet in the first few years, gradually clearing over several decades.
As nuclear devices need not be involved in the ignition of a firestorm, the term is a common misnomer. This is due to, in greatest part, the vast majority of published papers stating, without qualitative justification, that nuclear explosions are the cause of the modeled firestorm effects. The only phenomenon that is scrutinized and computer modeled in the nuclear winter papers is the climate forcing agent of firestorm-soot, a product which can be ignited and formed by a myriad of other, more common, means.
On the fundamental level, it is known that firestorms can inject soot smoke/aerosols into the stratosphere, as each natural occurrence of a wildfire firestorm has been found to "surprisingly frequently" produce minor "nuclear winter" effects, with short-lived, almost immeasurable drops in surface temperatures, confined to the global hemisphere that they burned in. This is somewhat analogous to the frequent volcanic eruptions that inject sulfates into the stratosphere and thereby produce minor, even negligible, volcanic winter effects.
A suite of satellite and aircraft-based firestorm-soot-monitoring instruments are at the forefront of attempts to accurately determine the lifespan, quantity, injection height, and optical properties of this smoke. Information regarding all of these properties is necessary to truly ascertain the length and depth of the cooling effect of firestorms, independent of the nuclear winter computer model projections.
Presently, from satellite tracking data, stratospheric smoke aerosols are removed in a time span under approximately two months. The existence of any hint of a tipping point into a new stratospheric condition where the aerosols would not be removed within this time frame remains to be determined.
Mechanism.
The nuclear winter scenario assumes that 100 or more city firestorms are ignited by the nuclear explosions of a nuclear war, and the firestorms lift large enough amounts of sooty smoke into the upper troposphere and lower stratosphere, soot lifted by the movement offered by the pyrocumulonimbus clouds that form during a firestorm. At above the Earth's surface, the absorption of sunlight could further heat the soot in the smoke, lifting some or all of it into the stratosphere, where the smoke could persist for years, if there is no rain to wash it out. This aerosol of particles could heat the stratosphere and block out a portion of the sun's light from reaching the surface, causing surface temperatures to drop drastically, and with that, it is predicted surface air temperatures would be akin to, or colder than, a given region's winter for months to years on end.
The modeled stable inversion layer of hot soot between the troposphere and high stratosphere that produces the anti-greenhouse effect was dubbed the "Smokeosphere" by Stephen Schneider et al. in their 1988 paper.
Although it is common in the climate models for the city firestorms to be ignited by nuclear explosions, they need not be ignited by nuclear devices; more conventional ignition sources can instead be the spark of the firestorms. As prior to the previously mentioned solar heating effect, the soot's injection height is controlled by the rate of energy release from the firestorm's fuel, not the size, or lack thereof, of an initial nuclear explosion. For example, the mushroom cloud from the bomb dropped on Hiroshima reached a height of six kilometers ("middle" troposphere) within a few minutes and then dissipated due to winds, while the individual fires within the city took almost three hours to form into a firestorm and produce a "pyrocumulus" cloud, a cloud that is assumed to have reached "upper" tropospheric heights, as over its multiple hours of burning, the firestorm released an estimated 1000 times the energy of the bomb.
While the firestorm of Dresden and Hiroshima and the mass fires of Tokyo and Nagasaki occurred with mere months separating them in 1945, the more intense and conventionally lit Hamburg firestorm occurred in 1943. Despite this, these five fires potentially placed five percent as much smoke into the stratosphere as the hypothetical 100 nuclear-ignited fires of modern models. While it is believed that the effects of the mass of soot emitted by 100 firestorms (one to five teragrams) would have been detectable with technical instruments in WWII, only five percent of that would not have been possible to observe at that time.
Aerosol removal timescale.
The exact timescale for how long this smoke remains, and thus how severely this smoke affects the climate once it reaches the stratosphere, is dependent on both chemical and physical removal processes.
The most important physical removal mechanism is "rainout", both during the "fire-driven convective column" phase—which produces "black rain" near the fire site—and rainout after the convective plume's dispersal, where the smoke is no longer concentrated and thus "wet removal" is believed to be "very efficient." However these efficient removal mechanisms in the troposphere are avoided in the Robock 2007 study, where solar heating is modeled to quickly "loft" the soot into the stratosphere, "detraining" or separating the darker soot particles from the fire clouds' whiter water condensation.
Once in the stratosphere, the physical removal mechanisms having an impact on the timescale of the soot particles' residence are how quickly the aerosol of soot collides and coagulates with other particles via Brownian motion, and falls out of the atmosphere via gravity-driven dry deposition, and the time it takes for the "phoretic effect" to move coagulated particles to a lower level in the atmosphere. Whether by coagulation or the phoretic effect, once the aerosol of smoke particles are at this lower atmospheric level, cloud seeding can begin, permitting precipitation to wash the smoke aerosol out of the atmosphere by the wet deposition mechanism.
The chemical processes that affect the removal are dependent on the ability of atmospheric chemistry to oxidize the carbonaceous component of the smoke, via reactions with oxidative species such as ozone and nitrogen oxides, both of which are found at all levels of the atmosphere, and which also occur at greater concentrations when air is heated to high temperatures, which will be discussed later.
Historical data on residence times of aerosols, albeit a different mixture of aerosols, in this case stratospheric sulfur aerosols and volcanic ash from megavolcano eruptions, appear to be in the one-to-two-year time scale.
The satellite tracking of wildfire smoke aerosols from the 17 North American pyrocumulonimbus-cloud-injection events in 2002, indicates that the aerosols are removed in a time span under approximately two months, although the exact mechanisms by which they are removed, and the existence of any hint of a tipping point into a new stratospheric condition were the aerosols would not be removed within this timeframe, remains to be experimentally determined.
Aerosol–atmosphere interactions are still poorly understood.
Soot properties.
Sooty aerosols can have a wide range of properties, as well as complex shapes, making it difficult to determine their evolving atmospheric Optical depth value. The conditions present during the creation of the soot are believed to be considerably important as to their final properties, with soot generated on the more efficient spectrum of burning efficiency considered almost "elemental carbon black," while on the more inefficient end of the burning spectrum, greater quantities of partially burnt/oxidized fuel are present. These partially burnt "organics" as they are known, often form "tar balls" and "brown carbon" during common lower-intensity wildfires, and can also coat the purer carbon black particles. However, as the soot of greatest importance is that which is injected to the highest altitudes by the pyroconvection of the firestorm—a fire being fed with storm-force winds of air—it is estimated that the majority of the soot under these conditions is of the more oxidized carbon black nature.
Consequences.
Climatic effects.
A study presented at the annual meeting of the American Geophysical Union in December 2006 found that even a small-scale, regional nuclear war could disrupt the global climate for a decade or more. In a regional nuclear conflict scenario where two opposing nations in the subtropics would each use 50 Hiroshima-sized nuclear weapons (about 15 kiloton each) on major populated centres, the researchers estimated as much as five million tons of soot would be released, which would produce a cooling of several degrees over large areas of North America and Eurasia, including most of the grain-growing regions. The cooling would last for years, and according to the research could be "catastrophic".
Ozone depletion.
A 2008 study by Michael J. Mills and coauthors, published in the Proceedings of the National Academy of Sciences, found that a nuclear weapons exchange between Pakistan and India using their current arsenals could create a near-global ozone hole, triggering human health problems and causing environmental damage for at least a decade. The computer-modeling study looked at a nuclear war between the two countries involving 50 Hiroshima-sized nuclear devices on each side, producing massive urban fires and lofting as much as five million metric tons of soot about into the mesosphere. The soot would absorb enough solar radiation to heat surrounding gases, causing a series of surface chemistry reactions that would break down the stratospheric ozone layer protecting Earth from harmful ultraviolet radiation.
Nuclear summer.
A "nuclear summer" is a hypothesized scenario in which, after a nuclear winter has abated, a greenhouse effect then occurs due to CO2 released by combustion and methane released from the decay of the organic matter that froze during the nuclear winter. It is supported scientifically far less, than nuclear winter, as a risk.
History.
Early work.
In 1952, a few weeks prior to the Ivy Mike (10.4 megaton) test on Elugelab island, there was a concern that the "small particles"/aerosols lifted by the explosion might cool the Earth. Major Norair Lulejian, USAF, and astronomer Natarajan Visvanathan, studied this possibility reporting their findings in
"Effects of Superweapons Upon the Climate of the World". According to a document by the Defense Threat Reduction Agency, this report was the initial study of the "nuclear winter" concept that was popularized by others decades later. It indicated no appreciable chance of explosion-induced climate change.
Following numerous surface bursts of high yield "Hydrogen bomb" explosions on Pacific Proving Ground islands such as those of Ivy Mike in the year 1952 and Castle Bravo (15 megaton) in 1954, "The Effects of Nuclear Weapons" by Samuel Glasstone was published in 1957 which contained a section entitled "Nuclear Bombs and the Weather" (pages 69–71), which states: "The dust raised in severe volcanic eruptions, such as that at Krakatoa in 1883, is known to cause a noticeable reduction in the sunlight reaching the earth … The amount of or other surface debris remaining in the atmosphere after the explosion of even the largest nuclear weapons is probably not more than about 1 percent or so of that raised by the Krakatoa eruption. Further, solar radiation records reveal that none of the nuclear explosions to date has resulted in any detectable change in the direct sunlight recorded on the ground."
In the 1966 RAND corporation memorandum "The Effects of Nuclear War on the Weather and Climate" by E. S. Batten, while primarily analysing potential dust effects from surface bursts, it notes "in addition to the effects of the debris, extensive fires ignited by nuclear detonations might change the surface characteristics of the area and modify local weather patterns...however, a more thorough knowledge of the atmosphere is necessary to determine their exact nature, extent, and magnitude."
In the 1985 "The Effects on the Atmosphere of a Major Nuclear Exchange", it argues that a "plausible" estimate on the amount of stratospheric dust injected following a surface burst of 1 megaton is 0.3 teragrams, of which "8 percent" would be in the submicron/micrometer range. The potential cooling from soil dust was again looked at in 1992, in a US National Academy of Sciences (NAS) report on geoengineering, which estimated that about 1010 kg teragrams of stratospheric injected soil dust with particulate grain dimensions of 0.1 to 1 micrometer would be required to mitigate the warming from a doubling of atmospheric , that is, to produce ~ 2 degree celsius of cooling.
In 1969, Paul Crutzen discovered that NOx (oxides of nitrogen) could be an efficient
catalyst for the destruction of the ozone layer/stratospheric ozone. With studies on the potential effects of NOx generated by engine heat in stratosphere flying Supersonic Transport(SST) airplanes in the 1970s serving as a backdrop, John Hampson in 1974 suggested in the journal "Nature" that due to the nuclear fireballs creation of atmospheric NOx, a full-scale nuclear exchange could result in depletion of the ozone shield, possibly subjecting the earth to ultraviolet radiation for a year or more. Hampson's hypothesis "led directly", in 1975, to the United States National Research Council (NRC) reporting on the models of ozone depletion following nuclear war in the book "Long-Term Worldwide Effects of Multiple Nuclear-Weapons Detonations". In this 1975 book it states that a nuclear war involving 4000Mt (megaton) from "present arsenals" would probably deposit much less dust in the stratosphere than the Krakatoa eruption, judging that the effect of dust and oxides of nitrogen would probably be slight climatic cooling which "would probably lie within normal global climatic variability, but the possibility of climatic changes of a more dramatic nature cannot be ruled out". While on the issue of fireball generated NOx and ozone layer loss therefrom, its model calculations in the early-to-mid 1970s on the effects of a nuclear war with the use of large numbers of multi-megaton yield detonations returned conclusions that this could reduce ozone levels by 50 per cent or more in the northern hemisphere.
More reliably, in 1976 a study on the experimental measurements of an earlier atmospheric nuclear test as it affected the ozone layer found that nuclear detonations are tentatively exonerated in depleting ozone, after initially discouraging model calculations. In total about 500 megatons were atmospherically detonated between 1945 and 1971, with a peak occurring in 1961–62, when 340 megatons were detonated in the atmosphere by the United States and Soviet Union. During this 1-2 year peak, counting only the multi-megaton range detonations in the two nations nuclear test series, a total yield estimated at 300 megatons of energy was released, due to this, 3 x 10^34 additional molecules of nitric oxide(about 5000 tons per megaton) are believed to have entered the stratosphere, and while ozone depletion of 2.2 percent was noted in 1963, the decline had started prior to 1961 and is believed to have been caused by other meteorological effects, thus the 1985 book "The Effects on the Atmosphere of a Major Nuclear Exchange" states: "one can not draw definite conclusions about the effects of nuclear explosions on stratospheric ozone".
In 1982 Australian physicist Brian Martin, who frequently corresponded with John Hampson, penned a short historical synopsis on the history of interest in the effects of the direct NOx generated by nuclear fireballs, and in doing so, also outlined Hampson's other non-mainstream viewpoints, particularly those relating to greater ozone destruction from upper-atmospheric detonations as a result of any widely used anti-ballistic missile(ABM-1 Galosh) system. However, Martin ultimately concludes that it is "unlikely that in the context of a major nuclear war" ozone degradation would be of serious concern. Singling out views about potential ozone loss and therefore increases in Ultraviolet light leading to the widespread destruction of crops, as advocated by journalist Jonathan Schell in his popular 1982 book "The Fate of the Earth", as highly unlikely.
More recent accounts on the specific ozone layer destruction potential of NOx species, are much less than earlier assumed from simplistic calculations, as "about 1.2 million tons" of natural and anthropogenic generated stratospheric NOx is believed to be formed each year according to Robert P.Parson in the 1990s.
Science Fiction.
The first published suggestion that a cooling of climate or winter could be an effect of a nuclear war, appears to have been originally put forth by Poul Anderson and F.N Waldrop in their post war story ""Tomorrow's Children"", which was contained in the March 1947 issue of the "Astounding Science Fiction" magazine, the story which is primarily about a team of scientists hunting down mutants, warns of a "Fimbulwinter" caused by dust that blocked sunlight after the recent fictitious nuclear war and speculates that this may even trigger a new ice age. Anderson went on to publish a novel based partly on this story in 1961 titling it; "Twilight World". Similarly in 1985 it was noted by T.G Parsons that the story "Torch" by C. Anvil, which likewise appeared in "Astounding Science Fiction magazine" but in the April 1957 edition, contains the essence of the "Twilight at Noon"/"nuclear winter" hypothesis. In the story a nuclear warhead ignites an oil field and the soot produced "screens out part of the sun's radiation" which results in Arctic temperatures for much of the population of North America and the Soviet Union.
1980s.
The 1988 Air Force Geophysics Laboratory publication "An assessment of global atmospheric effects of a major nuclear war" by Muench, H. Stuart et al. contains a chronology and review of the major reports on the nuclear winter hypothesis from 1983-86. In general these reports arrive at similar conclusions as they are based on the same "assumptions, the same basic data" with minor model-code differences "to arrive at the same answer". They skip the modeling steps of assessing the possibility of fire and the initial fire plumes and instead start the modeling process with a "spatially uniform" "soot cloud" which has found its way into the atmosphere.
In 1981, William J. Moran began discussions and research in the NRC on the dust effects of a large exchange of nuclear warheads, having seen a possible parallel in the dust effects of a war with that of the asteroid created K-T boundary and its popular analysis a year earlier by Luis Alvarez in 1980. An NRC study panel on the topic met in December 1981 and April 1982 in preparation of the release of "The Effects on the Atmosphere of a Major Nuclear Exchange" in 1985.
As part of a study on the creation of oxidizing species such as NOx and ozone in the troposphere after a nuclear war, launched in 1980 by "Ambio", a journal of the Royal Swedish Academy of Sciences, Paul Crutzen and John Birks began preparing for the 1982 publication of a calculation on the effects of nuclear war on stratospheric ozone, using the latest models for the time. However they found that in part as a result of the trend towards more numerous but less energetic, sub-megaton range nuclear warheads(made possible by the ceaseless march to increase ICBM warhead accuracy/Circular Error Probable) the ozone layer danger was "not very significant".
It was after being confronted with these results that they "chanced" upon the notion, as "an afterthought" of nuclear detonations igniting massive fires everywhere and most crucially, the smoke from these conventional fires, then going on to absorb sunlight and with that surface temperatures plummeting. In early 1982 the two colleagues circulated a draft paper with the first suggestions of alterations in short-term climate from fires, presumed to occur following a nuclear war. Later in the same year, the special issue of "Ambio" devoted to the possible environmental consequences of nuclear war by Crutzen and Birks was titled ""Twilight at Noon"" and largely anticipated the nuclear winter hypothesis. The paper which looked into fires and their climatic effect discussed particulate matter from large fires, nitrogen oxide, ozone depletion and the effect of nuclear twilight on agriculture. Crutzen and Birks' calculations suggested that smoke particulates injected into the atmosphere by fires in cities, forests and petroleum reserves could prevent up to 99% of sunlight from reaching the Earth's surface, with this darkness persisting "for as long as the fires" burned, which was assumed to be many weeks, with climatic consequences: "The normal dynamic and temperature structure of the atmosphere would therefore change considerably over a large fraction of the Northern Hemisphere, which will probably lead to important changes in land surface temperatures and wind systems." A policy implication of their work was that a successful nuclear decapitation strike could have severe climatic consequences for the perpetrator.
Interest in nuclear war environmental effects also arose in the USSR. After becoming aware of the papers by N.P.Bochkov and E.I.Chazov, Russian atmospheric scientist Georgy Golitsyn applied his research on dust-storms to the situation following a large nuclear war. His suggestion that the atmosphere would be heated and that the surface of the planet would cool appeared in "The Herald of the Academy of Sciences" in September 1983.
In 1982, the so-called TTAPS team (Richard P. Turco, Owen Toon, Thomas P. Ackerman, James B. Pollack and Carl Sagan) undertook a 1-dimensional computational modeling study of the atmospheric consequences of nuclear war, publishing their results in "Science" in December 1983. The phrase "nuclear winter" was coined by Turco just prior to publication. In this early paper, TTAPS used assumption based estimates on the total smoke and dust emissions that would result from a major nuclear exchange, and with that, began analyzing the subsequent effects on the atmospheric radiation balance and temperature structure as a result of this quantity of assumed smoke. To compute dust and smoke impacts, they employed a one-dimensional microphysics/radiative-transfer model of the Earth's lower atmosphere (to the mesopause), which defined only the vertical characteristics of the global climate perturbation.
Upon learning of the TTAPS scenarios, Vladimir Alexandrov and G. I. Stenchikov also published a report in 1983 on the climatic consequences of nuclear war based on simulations with a three-dimensional global circulation model. (Two years later Alexandrov disappeared under mysterious circumstances.) Richard Turco and Starley L. Thompson were critical of the Soviet model, Turco calling it "a primitive rendition of an obsolete US model". Both however largely rescinded their "demeaning" and "particularly harsh" quotes some time later, stating that this Soviet model had the same weaknesses as all others and applauded Alexandrov's "pioneering contribution" that "deserved special recognition".
In 1984 the WMO commissioned Georgy Golitsyn and N. A. Phillips to review the state of the science.They found that studies generally assumed a scenario that half of the world's nuclear weapons would be used, ~5000 Mt, destroying approximately 1,000 cities, and creating large quantities of carbonaceous smoke – 1– being most likely, with a range of 0.2– (NAS; TTAPS assumed ). The smoke resulting would be largely opaque to solar radiation but transparent to infra-red, thus cooling by blocking sunlight but not causing warming from enhancing the greenhouse effect. The optical depth of the smoke can be much greater than unity. Forest fires resulting from non-urban targets could increase aerosol production further. Dust from near-surface explosions against hardened targets also contributes; each Mt-equivalent of explosion could release up to 5 million tons of dust, but most would quickly fall out; high altitude dust is estimated at 0.1-1 million tons per Mt-equivalent of explosion. Burning of crude oil could also contribute substantially.
The 1-D radiative-convective models used in these studies produced a range of results, with coolings up to 15–42 °C between 14 and 35 days after the war, with a "baseline" of about 20 °C. Somewhat more sophisticated calculations using 3-D GCMs produced similar results: temperature drops of between 20 and 40 °C, though with regional variations.
All calculations show large heating (up to 80 °C) at the top of the smoke layer at about 10 km; this implies a substantial modification of the circulation there and the possibility of advection of the cloud into low latitudes and the southern hemisphere.
The report made no attempt to compare the likely human impacts of the post-war cooling to the direct deaths from explosions.
1990.
In 1990, in a paper entitled "Climate and Smoke: An Appraisal of Nuclear Winter," TTAPS give a more detailed description of the short- and long-term atmospheric effects of a nuclear war using a three-dimensional model:
First 1 to 3 months:
Following 1 to 3 years:
Kuwait wells in the first Gulf War.
Following Iraq's invasion of Kuwait and Iraqi threats of igniting the country's 800 or so oil wells were made, speculation on the cumulative climatic effect of this, presented at the World Climate Conference in Geneva that November in 1990, ranged from a nuclear winter type scenario, to heavy acid rain and even short term immediate global warming. As threatened, the wells were set ablaze by the retreating Iraqis by March 1991 and the 600 or so successfully set Kuwaiti oil wells were not fully extinguished until November 6, 1991, eight months after the end of the war, and they consumed an estimated six million barrels of oil daily at their peak intensity.
In articles printed in the Wilmington morning star and the Baltimore Sun newspapers of January 1991, prominent authors of nuclear winter papers – Richard P. Turco, John W. Birks, Carl Sagan, Alan Robock and Paul Crutzen together collectively stated that they expected catastrophic nuclear winter like effects with continental sized impacts of "sub-freezing" temperatures as a result of if the Iraqis went through with their threats of igniting 300 to 500 pressurized oil wells and they burned for a few months.
Later when Operation Desert Storm had begun in late January 1991, coinciding with the first few oil fires being lit, Dr. S. Fred Singer and Carl Sagan discussed the possible environmental impacts of the Kuwaiti petroleum fires on the ABC News program "Nightline". Sagan again argued that some of the effects of the smoke could be similar to the effects of a nuclear winter, with smoke lofting into the stratosphere, a region of the atmosphere beginning around above sea level at Kuwait, resulting in global effects and that he believed the net effects would be very similar to the explosion of the Indonesian volcano Tambora in 1815, which resulted in the year 1816 being known as the "Year Without a Summer".
He reported on initial modeling estimates that forecast impacts extending to south Asia, and perhaps to the northern hemisphere as well. Sagan stressed this outcome was so likely that, "It should affect the war plans." Singer, on the other hand, said that his calculations showed that the smoke would go to an altitude of about and then be rained out after about three to five days and thus the lifetime of the smoke would be limited. Both height estimates made by Singer and Sagan turned out to be wrong, albeit with Singers narrative being closer to what transpired, with the comparatively minimal atmospheric effects remaining limited to the Persian Gulf region, with smoke plumes, in general, lofting to about and a few times as high as .
Sagan later conceded in his book "The Demon-Haunted World" that his predictions obviously did not turn out to be correct: "it "was" pitch black at noon and temperatures dropped 4–6 °C over the Persian Gulf, but not much smoke reached stratospheric altitudes and Asia was spared."
Sagan and his colleagues expected that a "self-lofting" of the sooty smoke would occur when it absorbed the sun's heat radiation, with little to no scavenging occurring, whereby the black particles of soot would be heated by the sun and lifted/lofted higher and higher into the air, thereby injecting the soot into the stratosphere, a position where they argued it would take years for the sun blocking effect of this aerosol of soot to fall out of the air, and with that, catastrophic ground level cooling and agricultural impacts in Asia and possibly the Northern Hemisphere as a whole.
The Atmospheric scientist tasked with studying the atmospheric impact of the Kuwaiti fires by the National Science Foundation, Peter Hobbs, stated that "the fires' modest impact suggested that "some numbers to support the Nuclear Winter hypothesis... were probably a little overblown."
Hobbs found that at the peak of the fires, the smoke absorbed 75 to 80% of the sun’s radiation. The particles rose to a maximum of , and when combined with scavenging by clouds the smoke had a short residency time of a maximum of a few days in the atmosphere.
Pre-war claims of wide scale, long-lasting, and significant global environmental impacts were thus not borne out, and found to be significantly exaggerated by the media and speculators, with climate models by those not supporting the nuclear winter hypothesis at the time of the fires predicting only more localized effects such as a daytime temperature drop of ~10 °C within ~200 km of the source.
The idea of oil well and oil reserve smoke pluming to the stratosphere serving as a main contributor to the soot of a nuclear winter was a central tenet of the early climatology papers on the hypothesis; they were considered more of a possible contributor than smoke from cities, as the smoke from oil has a higher ratio of black soot, thus absorbing more sunlight. Hobbs compared the papers' assumed "emission factor" or soot generating efficiency from ignited oil pools and found, upon comparing to measured values from oil pools at Kuwait, which were the greatest soot producers, the emissions of soot assumed in the nuclear winter calculations are still "too high". Following the results of the Kuwaiti oil fires being in disagreement with the core nuclear winter promoting scientists, the 1990s nuclear winter papers generally attempted to distance themselves from suggesting oil well and reserve smoke will reach the stratosphere.
In 2007, a nuclear winter study, which will be discussed later, noted that modern computer models have been applied to the Kuwait oil fires, finding that individual smoke plumes are not able to loft smoke into the stratosphere, but that smoke from fires covering a large area like some forest fires can lift smoke into the stratosphere, and this is supported by recent evidence that it occurs far more often than previously thought. The study also suggested that the burning of the comparably smaller cities, which would be expected to follow a nuclear strike, would also loft significant amounts of smoke into the stratosphere:
However the above simulation notably contained the assumption that no dry and wet deposition/rain would occur.
Recent modeling.
Based on new work published in 2007 and 2008 by some of the authors of the original studies, several new hypotheses have been put forth. However far from being "new", the very same beginning to "significant" nuclear winter effects, was in the mid 1980s models, similarly regarded to have been a threat from a total of 100 or so city firestorms.
A minor nuclear war with each country using 50 Hiroshima-sized atom bombs as airbursts on urban areas could produce climate change unprecedented in recorded human history. A nuclear war between the United States and Russia today could produce nuclear winter, with temperatures plunging below freezing in the summer in major agricultural regions, threatening the food supply for most of the planet. The climatic effects of the smoke from burning cities and industrial areas would last for several years, much longer than previously thought. New climate model simulations, which are said to have the capability of including the entire atmosphere and oceans, show that the smoke would be lofted by solar heating to the upper stratosphere, where it would remain for years.
Compared to climate change for the past millennium, even the smallest exchange modeled would plunge the planet into temperatures colder than the Little Ice Age (the period of history between approximately A.D. 1600 and A.D. 1850). This would take effect instantly, and agriculture would be severely threatened. Larger amounts of smoke would produce larger climate changes, and for the 150 teragrams (Tg) case produce a true nuclear winter (1 Tg is 1012 grams), making agriculture impossible for years. In both cases, new climate model simulations show that the effects would last for more than a decade.
2007 study on global nuclear war.
A study published in the "Journal of Geophysical Research" in July 2007, "Nuclear winter revisited with a modern climate model and current nuclear arsenals: Still catastrophic consequences", used current climate models to look at the consequences of a global nuclear war involving most or all of the world's current nuclear arsenals (which the authors judged to be one the size of the world's arsenals twenty years earlier). The authors used a global circulation model, ModelE from the NASA Goddard Institute for Space Studies, which they noted "has been tested extensively in global warming experiments and to examine the effects of volcanic eruptions on climate." The model was used to investigate the effects of a war involving the entire current global nuclear arsenal, projected to release about 150 Tg of smoke into the atmosphere, as well as a war involving about one third of the current nuclear arsenal, projected to release about 50 Tg of smoke. In the 150 Tg case they found that:
In addition, they found that this cooling caused a weakening of the global hydrological cycle, reducing global precipitation by about 45%. As for the 50 Tg case involving one third of current nuclear arsenals, they said that the simulation "produced climate responses very similar to those for the 150 Tg case, but with about half the amplitude," but that "the time scale of response is about the same." They did not discuss the implications for agriculture in depth, but noted that a 1986 study which assumed no food production for a year projected that "most of the people on the planet would run out of food and starve to death by then" and commented that their own results show that, "This period of no food production needs to be extended by many years, making the impacts of nuclear winter even worse than previously thought."
2014.
In 2014, Michael J. Mills (at the US National Center for Atmospheric Research, NCAR), Owen B. Toon (of the original TTAPS team), Julia Lee-Taylor, and Alan Robock published "Multi-decadal global cooling and unprecedented ozone loss following a regional nuclear conflict" in the journal "Earth's Future". The authors used computational models developed by NCAR to simulate the climatic effects of a regional nuclear war in which 100 "small" (15 kt) weapons are detonated over cities. They concluded, in part, that
global ozone losses of 20-50% over populated areas, levels unprecedented in human history, would accompany the coldest average surface temperatures in the last 1000 years. We calculate summer enhancements in UV indices of 30-80% over Mid-Latitudes, suggesting widespread damage to human health, agriculture, and terrestrial and aquatic ecosystems. Killing frosts would reduce growing seasons by 10-40 days per year for 5 years. Surface temperatures would be reduced for more than 25 years, due to thermal inertia and albedo effects in the ocean and expanded sea ice. The combined cooling and enhanced UV would put significant pressures on global food supplies and could trigger a global nuclear famine.
Criticism and debate.
The TTAPS study was widely reported and criticized in the media. Later model runs in some cases predicted less severe effects, but continued to support the overall conclusion of significant global cooling. Recent studies (2006) substantiate that smoke from urban firestorms in a local nuclear war would lead to long lasting global cooling but in a less dramatic manner than a global nuclear war, while a 2007 study of the effects of global nuclear war supported the conclusion that it would lead to full-scale nuclear winter.
The original work by Sagan and others was criticized as a "myth" and "discredited theory" in the 1987 book "Nuclear War Survival Skills", a civil defense manual by Cresson Kearny for the Oak Ridge National Laboratory. Kearny said the amount of cooling would last only a few days. According to the 1988 publication "An assessment of global atmospheric effects of a major nuclear war", Kearny's criticisms were directed at the excessive amount of soot that the modelers assumed would reach the stratosphere, citing a Soviet study that modern cities would not burn as firestorms, as most flammable city items would be buried under combustible rubble and that the TTAPs study included a massive overestimate on the size and extent of non-urban wildfires that would result from a nuclear war. The TTAPs authors responded that, amongst other things, they did not believe target planners would intentionally blast cities into rubble, but instead argued fires would begin in relatively undamaged suburbs when nearby sites were hit, and partially conceded his point about non-urban wildfires.
While Kearny, who was not a climate scientist himself, based his cooling estimate of a few days entirely on the 1986 paper "Nuclear Winter Reappraised" by Starley Thompson and Stephen Schneider.
However, a 1988 article by Brian Martin in "Science and Public Policy" states that although their paper concluded the effects would be less severe than originally thought, with the authors describing these effects as a "nuclear autumn", other statements by Thompson and Schneider show that they "resisted the interpretation that this means a rejection of the basic points made about nuclear winter". In addition, the authors of the 2007 study above state that "because of the use of the term 'nuclear autumn' by Thompson and Schneider even though the authors made clear that the climatic consequences would be large, in policy circles the theory of nuclear winter is considered by some to have been exaggerated and disproved [e.g., Martin, 1988." And in 2007 Schneider emphasized the danger of serious climate changes from a limited nuclear war(Pakistan and India) of the kind analyzed in the 2006 study above, saying "The sun is much stronger in the tropics than it is in mid-latitudes. Therefore, a much more limited war could have a much larger effect, because you are putting the smoke in the worst possible place."
Russell Seitz, Associate of the Harvard University Center for International Affairs, argues that the models' assumptions give results which the researchers want to achieve and is a case of "worst-case analysis run amok". Seitz's opposition caused the proponents of nuclear winter to issue responses in the media, and while both sides made important points, they were largely incapable of collaborating as the proponents believed it was simply necessary to show only the possibility of climatic catastrophe, often a worst-case scenario, while opponents insisted that to be taken seriously, nuclear winter should be shown as likely under "reasonable" scenarios. One of these areas of contention, as elucidated by Lynn R. Anspaugh, is upon the question of which season should be used as the backdrop for the US-USSR war models, as most models choose the summer in the Northern Hemisphere as the start point to produce the maximum soot lofting and therefore eventual winter effect, whereas it has been pointed out that if the firestorms occurred in the winter months, when there is much less intense sunlight to loft soot into a stable region of the stratosphere, the magnitude of the cooling effect from the same number of firestorms as ignited in the summer models, would be negligible according to a January model run by Covey et al.
John Maddox, editor of the journal "Nature", issued a series of skeptical comments about nuclear winter studies during his tenure, being a long-time critic of environmental doomsdayism, his critical analysis of the hypothesis is regarded to have withstood the test of time. Similarly S. Fred Singer was a long term vocal critic of the hypothesis in the journal and in televised debates with Carl Sagan.
Lynn R. Anspaugh also expressed frustration that although a managed (Chapleau, Ontario) forest fire in Canada on 3 August 1985 is said to have been lit by proponents of nuclear winter, with the fire potentially serving as an opportunity to do some basic measurements of the optical properties of the smoke and smoke-to-fuel ratio, which would have helped refine the estimates of these critical model inputs, the proponents did not indicate that any such measurements were made. Peter V. Hobbs, who would later successfully attain funding to fly into and sample the smoke clouds from the Kuwait oil fires in 1991, also expressed frustration that he was denied funding to sample the Canadian, and other forest fires in this way. Richard Turco(of TTAPs fame) simply wrote a 10-page memorandum with information derived from his notes and some satellite images, that the smoke plume reached 6 km in altitude.
In 1986, atmospheric scientist Joyce Penner from the Lawrence Livermore National Laboratory published an article in "Nature" in which she focused on the specific variables of the smoke's optical properties and the quantity of smoke remaining airborne after the city fires and found that the published estimates of these variables varied so widely that depending on which estimates were chosen the climate effect could be negligible, minor or massive.
The assumed optical properties for black carbon in more recent nuclear winter papers(2006) are still "based on those assumed in earlier nuclear winter simulations".
In 1987 P. M. Kelly of the University of East Anglia Climatic Research Unit stated that "although there are a handful of vociferous critics, the atmospheric community is united in its conclusion that the threat of nuclear winter is genuine".
William R. Cotton Professor of Atmospheric Science at Colorado State University, specialist in cloud physics modeling and co-creator of the highly influential, and previously mentioned RAMS atmosphere model, had in the 1980s modeled and supported the predictions made by earlier nuclear winter papers, but has since reversed this position according to a book co-authored by him in 2007, stating that, amongst other systematically examined assumptions; far more rain out/wet deposition of soot will occur than is assumed in modern papers on the subject and that "We must wait for a new generation of GCMs to be implemented to examine potential consequences quantitatively" and "that nuclear winter was largely politically motivated from the beginning".
The contribution of smoke from the ignition of live non-desert vegetation, living forests and so on near to many missile silos, a source of smoke originally brought up in the initial "Twilight at Noon" paper and also found in the popular TTAPs publication, was found after examination by Bush and Small in 1987, that the burning of live vegetation would contribute only slightly to the estimated total "nonurban smoke production". With the vegetations potential to sustain burning only probable if it is within a radius or two from the surface of the nuclear fireball, which is at a distance that would also experience extreme blast winds that would influence any such fires. This reduction in the estimate of nonurban smoke is supported by the earlier preliminary "Estimating Nuclear Forest Fires" publication of 1984, and by the 1950-60s in-field examination of tropical forests after Operation Castle, and Operation Redwing.
In a paper by the United States Department of Homeland Security finalized in 2010, fire experts stated that due to the nature of modern city design and construction, with the U.S.  serving as an example, a firestorm is unlikely after a nuclear detonation in a modern city. This is not to say that fires won't occur over a large area after a detonation, but that the fires would not coalesce and form the all important stratosphere punching firestorm plume that the nuclear winter papers require as a prerequisite assumption in their climate computer models. The nuclear bombing of Nagasaki for example, did not produce a firestorm. This was similarly noted as early as 1986-88, when the assumed quantity of fuel "mass loading"(the amount of fuel per square meter) in cities underpinning the winter models was found to be too high and intentionally creates heat fluxes that lofts smoke into the lower stratosphere, yet assessments "more characteristic of conditions" to be found in real-world modern cities, had found that the fuel loading, and hence the heat flux the results from burning, would rarely loft smoke much higher than 4 km.
Policy implications.
During the early 1980s, Fidel Castro recommended to the Kremlin a harder line against Washington, even suggesting the possibility of nuclear strikes. The pressure stopped after Soviet officials gave Castro a briefing on the ecological impact on Cuba of nuclear strikes on the United States. In 2010 Alan Robock, a co-author of nuclear winter papers was summoned to Cuba to help Castro promote his new view that nuclear war would bring about Armageddon, Robock's 90 minute lecture was later aired on nationwide television in the country.
However, according to Robock, in so far as getting US government attention and affecting nuclear policy, he has failed. In 2009, together with Owen Toon, he gave a talk to the United States Congress but nothing transpired from it and the then presidential science adviser, John Holdren, did not respond to their requests in 2009 or at the time of writing in 2011.
In an interview in 2000, Mikhail Gorbachev, in response to the comment "In the 1980s, you warned about the unprecedented dangers of nuclear weapons and took very daring steps to reverse the arms race," said "Models made by Russian and American scientists showed that a nuclear war would result in a nuclear winter that would be extremely destructive to all life on Earth; the knowledge of that was a great stimulus to us, to people of honor and morality, to act in that situation."
However a 1984 US "Interagency Intelligence Assessment" expresses a far more skeptical and cautious approach by stating that as the hypothesis is not convincing scientifically, they predicted that Soviet nuclear policy would be to maintain their strategic nuclear posture, such as their fielding of the high throw-weight SS-18 missile and they would merely attempt to exploit the hypothesis for propaganda purposes, such as directing scrutiny on the US portion of the nuclear arms race. Moreover, it goes on to express the belief that if Soviet officials did begin to take nuclear winter seriously, it would probably make them demand exceptionally high standards of scientific proof for the hypothesis, as the implications of it would undermine their military doctrine—a level of scientific proof which perhaps could not be met without field experimentation. The un-redacted portion of the document ends with the suggestion that substantial increases in Soviet Civil defense food stockpiles might be an early indicator that Nuclear Winter was beginning to influence Soviet upper echelon thinking.
In 1985 "Time" magazine noted "the suspicions of some Western scientists that the nuclear winter hypothesis was promoted by Moscow to give anti-nuclear groups in the U.S. and Europe some fresh ammunition against America's arms buildup."
In 1986, the Defense Nuclear Agency document "An update of Soviet research on and exploitation of Nuclear winter 1984–1986" charted the minimal research contribution on, and Soviet propaganda usage of, the nuclear winter phenomenon.
Dr. Vitalii Nikolaevich Tsygichko, a Senior Analyst at the Soviet Academy of Sciences, author of the study, "Mathematical Model of Soviet Strategic Operations on the Continental Theater", and a former member of the General Staff, has said that Soviet military analysts discussed the idea of a "nuclear winter" (although they did not use that exact term) years before U.S. scientists wrote about it in the 1980s. Starley L. Thompson, of the National Center for Atmospheric Research, Boulder, Colorado, says that Soviet research into nuclear winter in 1983 used US computer models that had been developed in the early 1970s. Soviet intelligence officer Sergei Tretyakov, who defected in 1990, maintained that "the KGB was responsible for creating the entire nuclear winter story to stop the Pershing II missiles".
In 1989 Carl Sagan and colleague Richard Turco wrote a policy implications paper that appeared in Ambio that suggests that as nuclear winter is a "well-established prospect", both superpowers should jointly reduce their nuclear arsenals to "Canonical Deterrent Force" levels of 100-300 individual warheads each, such that in "the event of nuclear war would minimize the likelihood of nuclear winter."
As the implications of nuclear winter began to be taken seriously in the late 1980s, military analysts turned to reinforce ""existing trends"" in warhead miniaturization, of higher accuracy and lower yield nuclear warheads. This trend, enabled by GPS navigation etc., was motivated by the desire to still destroy the target but while reducing the severity of fallout collateral damage depositing on neighboring, and potentially friendly, countries. As it relates to the likelihood of nuclear winter, the hazard from thermal radiation ignited fires would also be reduced. While the TTAPS paper had described a 3000 Mt counterforce attack on ICBM sites; Michael Altfeld of Michigan State University and political scientist Stephen Cimbala of Pennsylvania State University argued that smaller, more accurate warheads and lower detonation heights could produce the same counterforce strike with only 3 Mt and produce less climatic effects, even if cities were targeted, as lower fuzing heights, such as surface bursts, would limit the range of the burning thermal rays due to terrain masking and shadows cast by buildings, while also temporarily lofting far more radioactive soil into the atmosphere. This logic is similarly reflected in the 1984 "Interagency Intelligence assessment", which suggests that targeting planners would simply have to consider target combustibility along with yield, height of burst, timing and other factors to reduce the amount of smoke to safeguard against the potentiality of a nuclear winter. Therefore, as a consequence of attempting to limit the target fire hazard by reducing the range of thermal radiation with fuzing for surface and sub-surface bursts, this will result in a scenario where the far more concentrated, and therefore deadlier, "local" fallout that is generated following a surface burst forms, as opposed to the comparatively dilute "global" fallout created when nuclear weapons are fuzed in air burst mode.
Altfeld and Cimbala also argued that belief in the possibility of nuclear winter would actually make nuclear war more likely, contrary to the views of Sagan and others, because it would inspire the development of more accurate, and lower explosive yield, nuclear weapons. As it suggests that the replacement of the then Cold War viewed strategic nuclear weapons in the multi-megaton yield range, with weapons of explosive yields closer to tactical nuclear weapons, such as the Robust Nuclear Earth Penetrator, would safeguard against the nuclear winter potential. Tactical nuclear weapons, on the low end of the scale have yields that overlap with large conventional weapons, and are therefore often viewed "as blurring the distinction between conventional and nuclear weapons", making the prospect of using them "easier" in a conflict.
Mitigation techniques.
A number of solutions have been proposed to mitigate the potential harm of a nuclear winter if one appears inevitable; with the problem being attacked at both ends, from those focusing on preventing the growth of fires and therefore limiting the amount of smoke that reaches the stratosphere in the first place, to food production under dimmed skies with the assumption that the very worst-case analysis results of the nuclear winter models prove accurate and no other mitigation strategies are fielded.
Fire control.
In a report from 1967, techniques included various methods of applying liquid nitrogen, dry ice, and water to nuclear-caused fires. The report considered attempting to stop the spread of fires by creating firebreaks by blasting combustible material out of an area, possibly even with nuclear weapons, along with the use of preventative Hazard reduction burns. According to the report, one of the most promising techniques investigated was initiation of rain from seeding of mass-fire thunderheads and other clouds passing over the developing, and then steady-state, firestorm.
Producing food without sunlight.
David Denkenberger and Joshua Pearce have proposed in Feeding Everyone No Matter What a variety of alternate foods which convert fossil fuels or biomass into food without sunlight to address nuclear winter. The solution using a fossil fuel energy source is natural-gas-digesting bacteria. One example of a biomass alternate food is that mushrooms can grow directly on wood without sunlight. Another example is that cellulosic biofuel production typically already creates sugar as an intermediate product.
Large-scale food stockpiling.
The minimum annual global wheat storage is approximately 2 months. To feed everyone despite nuclear winter, years of food storage prior to the event has been proposed. While the suggested masses of preserved food would likely never get used as a nuclear winter is comparatively unlikely to occur, the stockpiling of food would have the positive result of ameliorating the impact of the far more frequent distruptions to regional food supplies caused by lower-level conflicts and droughts. There is however the danger that if a sudden rush to food stockpiling occurs without the buffering effect offered by Victory gardens etc., it may exacerbate current food security problems by elevating present food prices.
Climate engineering.
Despite the name "nuclear winter", nuclear events are not necessary to produce the modeled climatic effect. In an effort to find a quick and cheap solution to the global warming projection of at least two degrees of surface warming as a result of the doubling in CO2 levels within the atmosphere, through solar radiation management(a form of climate engineering) the underlying nuclear winter effect has been looked at as perhaps holding potential. Besides the more common suggestion to inject sulfur compounds into the stratosphere to approximate the effects of a volcanic winter, the injection of other chemical species such as the release of a particular type of soot particle, to create minor "nuclear winter" conditions, has also been proposed by Paul Crutzen and others. According to the threshold/minor "nuclear winter" computer models, if one to five teragrams of firestorm-generated soot is injected into the low stratosphere, it is modeled, through the anti-greenhouse effect, to heat the stratosphere but cool the lower troposphere and produce 1.25 °C cooling for two to three years; after 10 years, average global temperatures would still be 0.5 °C lower than before the soot injection.
Potential climatic precedence.
Similar climatic effects to "nuclear winter" followed historical supervolcano eruptions, which plumed sulfate aerosols high into the stratosphere, with this being known as a volcanic winter.
Similarly, extinction-level comet and asteroid impacts are also believed to have generated impact winters by the pulverization of massive amounts of fine rock dust. This pulverized rock can also produce "volcanic winter" effects, if sulfate-bearing rock is hit in the impact and lofted high into the air, and "nuclear winter" effects, with the heat of the heavier rock ejecta igniting regional and possibly even global forest firestorms.
This global "impact firestorms" hypothesis, initially supported by Wolbach, Melosh and veteran nuclear winter modeler Owen Toon, suggests that as a result of massive impact events, the small sand-grain-sized ejecta fragments created can meteorically re-enter the atmosphere forming a hot blanket of global debris high in the air, potentially turning the entire sky red-hot for minutes to hours, and with that, burning the complete global inventory of above-ground carbonaceous material, including rain forests. This hypothesis is suggested as a means to explain the severity of the Cretaceous–Paleogene extinction event, as the earth impact of an asteroid about 10 km wide which precipitated the extinction is not regarded as sufficiently energetic to have caused the level of extinction from the initial impact's energy release alone.
The global "impact firestorms"/firestorm winter, however, has been questioned in more recent years (2003–2013) by Claire Belcher, Tamara Goldin and H. Jay Melosh, with this re-evaluation being dubbed the "Cretaceous-Palaeogene firestorm debate" by Belcher. The issues raised by these scientists in the debate are the perceived low quantity of soot in the sediment beside the fine-grained iridium-rich asteroid dust layer, if the quantity of re-entering ejecta was perfectly global in blanketing the atmosphere, and if so, the duration and profile of the re-entry heating, whether it was a high thermal pulse of heat or the more prolonged and therefore more incendiary "oven" heating, and finally, how much the "self-shielding effect" from the first wave of now-cooled meteors in dark flight contributed to diminishing the total heat experienced on the ground from later waves of meteors, in part due to the Cretaceous period being a high-atmospheric-oxygen era, with concentrations above that of the present day. In 2013, Owen Toon et al. were critical of the re-evaluations the hypothesis is undergoing. It will be difficult to successfully tease out the percentage contribution of the soot in this period's geological sediment record from living plants and fossil fuels present at the time, in much the same manner that the fraction of the material ignited by the meteor's heating effects will be difficult to determine, as other ignition sources that were also present at, or soon after, the impact such as mantle lava flows complicate the matter.

</doc>
<doc id="22172" url="https://en.wikipedia.org/wiki?curid=22172" title="Ode">
Ode

Ode (from ) is a type of lyrical stanza. A classic ode is structured in three major parts: the "strophe", the "antistrophe", and the "epode". Different forms such as the "homostrophic ode" and the "irregular ode" also exist. It is an elaborately structured poem praising or glorifying an event or individual, describing nature intellectually as well as emotionally.
Greek odes were originally poetic pieces performed with musical accompaniment. As time passed on, they gradually became known as personal lyrical compositions whether sung (with or without musical instruments) or merely recited (always with accompaniment). The primary instruments used were the aulos and the lyre (the latter was the most revered instrument to the Ancient Greeks). 
There are three typical forms of odes: the Pindaric, Horatian, and irregular. Pindaric odes follow the form and style of Pindar. Horatian odes follow conventions of Horace; the odes of Horace deliberately imitated the Greek lyricists such as Alcaeus and Anacreon. Irregular odes use rhyme, but not the three-part form of the Pindaric ode, nor the two- or four-line stanza of the Horatian ode.
English ode.
An English ode is a lyrical stanza in praise of, or dedicated to someone or something that captures the poet's interest or serves as an inspiration for the ode. The lyrics can be on various themes. The earliest odes in the English language, using the word in its strict form, were the "Epithalamium" and "Prothalamium" of Edmund Spenser. 
In the 17th century, the most important original odes in English are by Abraham Cowley. These were iambic, but had irregular line length patterns and rhyme schemes. Cowley based the principle of his Pindariques on an apparent misunderstanding of Pindar's metrical practice but, nonetheless, others widely imitated his style, with notable success by John Dryden.
With Pindar's metre being better understood in the 18th century, the fashion for Pindaric odes faded, though there are notable actual Pindaric odes by Thomas Gray, "The Progress of Poesy" and "The Bard".
Around 1800, William Wordsworth revived Cowley's Pindarick for one of his finest poems, the "" ode. Others also wrote odes: Samuel Taylor Coleridge, John Keats, and Percy Bysshe Shelley who wrote odes with regular stanza patterns. Shelley's "Ode to the West Wind", written in fourteen line terza rima stanzas, is a major poem in the form. Perhaps the greatest odes of the 19th century, however, were Keats's "Five Great Odes of 1819", which included "Ode to a Nightingale", "Ode on Melancholy", "Ode on a Grecian Urn", "Ode to Psyche", and "To Autumn". After Keats, there have been comparatively few major odes in English. One major exception is the fourth verse of the poem "For the Fallen" by Laurence Binyon, which is often known as "The Ode to the Fallen", or simply as "The Ode".
W.H. Auden also wrote "Ode", one of the most popular poems from his earlier career when he lived in London, in opposition to people's ignorance over the reality of war. In an interview, Auden once stated that he had intended to title the poem "My Silver Age" in mockery of the supposedly imperial Golden age, however chose "Ode" as it seemed to provide a more sensitive exploration of warfare. 
"Ode on a Grecian Urn", while an ekphrasis, also functions as an ode to the artistic beauty the narrator observes. The English ode's most common rhyme scheme is ABABCDECDE.

</doc>
<doc id="22189" url="https://en.wikipedia.org/wiki?curid=22189" title="Temple of Olympian Zeus, Athens">
Temple of Olympian Zeus, Athens

The Temple of Olympian Zeus (, "Naos tou Olympiou Dios"), also known as the Olympieion or Columns of the Olympian Zeus, is a colossal ruined temple in the center of the Greek capital Athens that was dedicated to Zeus, king of the Olympian gods. Construction began in the 6th century BC during the rule of the Athenian tyrants, who envisaged building the greatest temple in the ancient world, but it was not completed until the reign of the Roman Emperor Hadrian in the 2nd century AD some 638 years after the project had begun. During the Roman periods it was renowned as the largest temple in Greece and housed one of the largest cult statues in the ancient world.
The temple's glory was short-lived, as it fell into disuse after being pillaged in a barbarian invasion in the 3rd century AD. It was probably never repaired and was reduced to ruins thereafter. In the centuries after the fall of the Roman Empire, it was extensively quarried for building materials to supply building projects elsewhere in the city. Despite this, a substantial part of the temple remains today, and it continues to be a major tourist attraction.
History.
Classical and Hellenistic periods.
The temple is located approximately 1640 feet south-east of the Acropolis, and about 700 m (2,300 feet) south of the center of Athens, Syntagma Square. Its foundations were laid on the site of an ancient outdoor sanctuary dedicated to Zeus. An earlier temple had stood there, constructed by the tyrant Pisistratus around 550 BC. The building was demolished after the death of Peisistratos and the construction of a colossal new Temple of Olympian Zeus was begun around 520 BC by his sons, Hippias and Hipparchos. 
They sought to surpass two famous contemporary temples, the Heraion of Samos and the Temple of Artemis at Ephesus, which was one of the Seven Wonders of the Ancient World. Designed by the architects Antistates, Callaeschrus, Antimachides and Pornius, the Temple of Olympian Zeus was intended to be built of local limestone in the Corinthian style on a colossal platform measuring 41 m (134.5 feet) by 108 m (353.5 feet). It was to be flanked by a double colonnade of eight columns across the front and back and twenty-one on the flanks, surrounding the cella. 
The work was abandoned when the tyranny was overthrown and Hippias was expelled in 510 BC. Only the platform and some elements of the columns had been completed by this point, and the temple remained in this state for 336 years. The temple was left unfinished during the years of Athenian democracy, apparently because the Greeks thought it was hubris to build on such a scale. In the treatise "Politics", Aristotle cited the temple as an example of how tyrannies engaged the populace in great works for the state (like a white elephant) and left them no time, energy or means to rebel.
It was not until 174 BC that the Seleucid king Antiochus IV Epiphanes, who presented himself as the earthly embodiment of Zeus, revived the project and placed the Roman architect Decimus Cossutius in charge. The design was changed to have three rows of eight columns across the front and back of the temple and a double row of twenty on the flanks, for a total of 104 columns. The columns would stand 17 m (55.5 feet) high and 2 m (6.5 ft) in diameter. The building material was changed to the expensive but high-quality Pentelic marble and the order was changed from Doric to Corinthian, marking the first time that this order had been used on the exterior of a major temple. However, the project ground to a halt again in 164 BC with the death of Antiochus. The temple was still only half-finished by this stage.
Serious damage was inflicted on the partly built temple by Lucius Cornelius Sulla's sack of Athens in 86 BC. While looting the city, Sulla seized some of the incomplete columns and transported them back to Rome, where they were re-used in the Temple of Jupiter on the Capitoline Hill. A half-hearted attempt was made to complete the temple during Augustus' reign as the first Roman emperor, but it was not until the accession of Hadrian in the 2nd century AD that the project was finally completed around 638 years after it had begun. 
In 124-125 AD, when the strongly Philhellene Hadrian visited Athens, a massive building programme was begun that included the completion of the Temple of Olympian Zeus. A walled marble-paved precinct was constructed around the temple, making it a central focus of the ancient city. Cossutius's design was used with few changes and the temple was formally dedicated by Hadrian in 132, who took the title of "Panhellenios" in commemoration of the occasion. The temple and the surrounding precinct were adorned with numerous statues depicting Hadrian, the gods and personifications of the Roman provinces. A colossal statue of Hadrian was raised behind the building by the people of Athens in honour of the emperor's generosity. An equally colossal chryselephantine statue of Zeus occupied the cella of the temple. The statue's form of construction was unusual, as the use of chryselephantine was by this time regarded as archaic. It has been suggested that Hadrian was deliberately imitating Phidias' famous statue of Athena Parthenos in the Parthenon, seeking to draw attention to the temple and himself by doing so.
The Temple of Olympian Zeus was badly damaged during the Herulian sack of Athens in 267. It is unlikely to have been repaired, given the extent of the damage to the rest of the city. Assuming that it was not abandoned it would certainly have been closed down in 425 by the Christian emperor Theodosius II when he prohibited the worship of the old Roman and Greek gods. Material from the (presumably now ruined) building was incorporated into a basilica constructed nearby during the 5th or 6th century.
Medieval and Modern periods.
Over the following centuries, the temple was systematically quarried to provide building materials and material for the houses and churches of medieval Athens. By the end of the Byzantine period, it had been almost totally destroyed; when Ciriaco de' Pizzicolli (Cyriacus of Ancona) visited Athens in 1436 he found only 21 of the original 104 columns still standing. The fate of one of the columns is recorded by a Greek inscription on one of the surviving columns, which states that "on 27 April 1759 he pulled down the column". This refers to the Turkish governor of Athens, Tzisdarakis, who is recorded by a chronicler as having "destroyed one of Hadrian's columns with gunpowder" in order to re-use the marble to make plaster for the mosque that he was building in the Monastiraki district of the city. During the Ottoman period the temple was known to the Greeks as the Palace of Hadrian, while the Turks called it the Palace of Belkis, from a Turkish legend that the temple had been the residence of Solomon's wife.
Fifteen columns remain standing today and a sixteenth column lies on the ground where it fell during a storm in 1852. Nothing remains of the cella or the great statue that it once housed.
The temple was excavated in 1889-1896 by Francis Penrose of the British School in Athens (who also played a leading role in the restoration of the Parthenon), in 1922 by the German archaeologist Gabriel Welter and in the 1960s by Greek archaeologists led by Ioannes Travlos. The temple, along with the surrounding ruins of other ancient structures, is a historical precinct administered by Ephorate of Antiquites of the Greek Interior Ministry.
On 21 January 2007, a group of Hellenic neopagans held a ceremony honoring Zeus on the grounds of the temple. The event was organized by Ellinais, an organization which won a court battle to obtain recognition for Ancient Greek religious practices in the fall of 2006.

</doc>
<doc id="22190" url="https://en.wikipedia.org/wiki?curid=22190" title="Organic electronics">
Organic electronics

Organic electronics is a field of materials science concerning the design, synthesis, characterization, and application of organic small molecules or polymers that show desirable electronic properties such as conductivity. Unlike conventional inorganic conductors and semiconductors, organic electronic materials are constructed from organic (carbon-based) small molecules or polymers using synthetic strategies developed in the context of organic and polymer chemistry. One of the benefits of organic electronics is their low cost compared to traditional inorganic electronics.
History.
Conductive materials are substances that can transmit electrical charges. Traditionally, most known conductive materials have been inorganic. Metals such as copper and aluminum are the most familiar conductive materials, and have high electrical conductivity due to their abundance of delocalized electrons that move freely throughout the inter-atomic spaces. Some metallic conductors are alloys of two or more metal elements, common examples of such alloys include steel, brass, bronze, and pewter.
In the eighteenth and early nineteenth centuries, people began to study the electrical conduction in metals. In his experiments with lightning, Benjamin Franklin proved that an electrical charge travels along a metallic rod. Later, Georg Simon Ohm discovered that the current passing through a substance is directly proportional to the potential difference, known as Ohm's law. This relationship between potential difference and current became a widely used measure of the ability of various materials to conduct electricity. Since the discovery of conductivity, studies have focused primarily on inorganic conductive materials with only a few exceptions.
Henry Letheby discovered the earliest known organic conductive material in 1862. Using anodic oxidation of aniline in sulfuric acid, he produced a partly conductive material, that was later identified as polyaniline. In the 1950s, the phenomenon that polycyclic aromatic compounds formed semi-conducting charge-transfer complex salts with halogens was discovered, showing that some organic compounds could be conductive as well.
More recent work has expanded the range of known organic conductive materials. A high conductivity of 1 S/cm (S = Siemens) was reported in 1963 for a derivative of tetraiodopyrrole. In 1972, researchers found metallic conductivity(conductivity comparable to a metal) in the charge-transfer complex TTF-TCNQ.
In 1977, it was discovered that polyacetylene can be oxidized with halogens to produce conducting materials from either insulating or semiconducting materials. In recent decades, research on conductive polymers has prospered, and the 2000 Nobel Prize in Chemistry was awarded to Alan J. Heeger, Alan G. MacDiarmid, and Hideki Shirakawa jointly for their work on conductive polymers.
Conductive plastics have recently undergone development for applications in industry. In 1987, the first organic diode device of was produced at Eastman Kodak by Ching W. Tang and Steven Van Slyke. spawning the field of organic light-emitting diodes (OLED) research and device production. For his work, Ching W. Tang is widely considered as the father of organic electronics.
Technology for plastic electronics constructed on thin and flexible plastic substrates was developed in the 1990s. In 2000, the company Plastic Logic was founded as a spin-off of Cavendish Laboratory to develop a broad range of products using the plastic electronics technology.
Conductive organic materials.
Attractive properties of polymer conductors include a wide range of electrical conductivity that can be tuned by varying the concentrations of chemical dopants, mechanical flexibility, and high thermal stability. Organic conductive materials can be grouped into two main classes: conductive polymers and conductive small molecules.
Conductive small molecules are usually used in the construction of organic semiconductors, which exhibit degrees of electrical conductivity between those of insulators and metals. Semiconducting small molecules include polycyclic aromatic compounds such as pentacene, anthracene and rubrene.
Conductive polymers are typically intrinsically conductive. Their conductivity can be comparable to metals or semiconductors. Most conductive polymers are not thermoformable, during production. However they can provide very high electrical conductivity without showing similar mechanical properties to other commercially available polymers. Both organic synthesis and advanced dispersion techniques can be used to tune the electrical properties of conductive polymers, unlike typical inorganic conductors. The most well-studied class of conductive polymers is the so-called linear-backbone “polymer blacks” including polyacetylene, polypyrrole, polyaniline, and their copolymers. Poly(p-phenylene vinylene) and its derivatives are used for electroluminescent semiconducting polymers. Poly(3-alkythiophenes) are also a typical material for use in solar cells and transistors.
Organic light-emitting diode.
An OLED (organic light-emitting diode) consists of a thin film of organic material that emits light under stimulation by an electric current. A typical OLED consists of an anode, a cathode, OLED organic material and a conductive layer. 
Discovery of OLED.
André Bernanose was the first person to observe electroluminescence in organic materials, and Ching W. Tang, reported fabrication of an OLED device in 1987. The OLED device incorporated a double-layer structure motif consisting of separate hole transporting and electron-transporting layers, with light emission taking place in between the two layers. Their discovery opened a new era of current OLED research and device design.
Classification and current research.
OLED organic materials can be divided into two major families: small-molecule-based and polymer-based. Small molecule OLEDs (SM-OLEDs) include organometallic chelates(Alq3), fluorescent and phosphorescent dyes, and conjugated dendrimers. Fluorescent dyes can be selected according to the desired range of emission wavelengths; compounds like perylene and rubrene are often used. Very recently, Dr. Kim J. et al. at University of Michigan reported a pure organic light emitting crystal, Br6A, by modifying its halogen bonding, they succeeded in tuning the phosphorescence to different wavelengths including green, blue and red. By modifying the structure of Br6A, scientists are attempting to achieve a next generation organic light emitting diode. Devices based on small molecules are usually fabricated by thermal evaporation under vacuum. While this method enables the formation of well-controlled homogeneous film; is hampered by high cost and limited scalability.
Polymer light-emitting diodes (PLEDs), similar to SM-OLED, emit light under an applied electric current. Polymer-based OLEDs are generally more efficient than SM-OLEDs requiring a comparatively lower amount of energy to produce the same luminescence. Common polymers used in PLEDs include derivatives of poly(p-phenylene vinylene) and polyfluorene. The emitted color can be tuned by substitution of different side chains onto the polymer backbone or modifying the stability of the polymer. In contrast to SM-OLEDs, polymer-based OLEDs cannot be fabricated through vacuum evaporation, and must instead be processed using solution-based techniques. Compared to thermal evaporation, solution based methods are more suited to creating films with large dimensions. Zhenan Bao. et al. at Stanford University reported a novel way to construct large-area organic semiconductor thin films using aligned single crystalline domains.
Organic field-effect transistor.
An Organic field-effect transistor is a field-effect transistor utilizing organic molecules or polymers as the active semiconducting layer. A field-effect transistor (FET) is any semiconductor material that utilizes electric field to control the shape of a channel of one type of charge carrier, thereby changing its conductivity. Two major classes of FET are n-type and p-type semiconductor, classified according to the charge type carried. In the case of organic FETs (OFETs), p-type OFET compounds are generally more stable than n-type due to the susceptibility of the latter to oxidative damage.
Discovery of the OFET.
J.E. Lilienfeld first proposed the field-effect transistor in 1930, but the first OFET was not reported until 1987, when Koezuka et al. constructed one using Polythiophene which shows extremely high conductivity. Other conductive polymers have been shown to act as semiconductors, and newly synthesized and characterized compounds are reported weekly in prominent research journals. Many review articles exist documenting the development of these materials.
Classification of OFETs and current research.
Like OLEDs, OFETs can be classified into small-molecule and polymer-based system. Charge transport in OFETs can be quantified using a measure called carrier mobility; currently, rubrene-based OFETs show the highest carrier mobility of 20–40 cm2/(V·s). Another popular OFET material is Pentacene. Due to its low solubility in most organic solvents, it's difficult to fabricate thin film transistors (TFTs) from pentacene itself using conventional spin-cast or, dip coating methods, but this obstacle can be overcome by using the derivative TIPS-pentacene. Current research focuses more on thin-film transistor (TFT) model, which eliminates the usage of conductive materials. Very recently, two studies conducted by Dr. Bao Z. et al. and Dr. Kim J. et al. demonstrated control over the formation of designed thin-film transistors. By controlling the formation of crystalline TFT, it is possible to create an aligned (as opposed to randomly ordered) charge transport pathway, resulting in enhanced charge mobility.
Organic electronic devices.
Organic solar cells could cut the cost of solar power by making use of inexpensive organic polymers rather than the expensive crystalline silicon used in most solar cells. What's more, the polymers can be processed using low-cost equipment such as ink-jet printers or coating equipment employed to make photographic film, which reduces both capital and operating costs compared with conventional solar-cell manufacturing.
Silicon thin film solar cells on flexible substrates allow a significant cost reduction of large-area photovoltaics for several reasons:
Inexpensive polymeric substrates like polyethylene terephthalate (PET) or polycarbonate (PC) have the potential for further cost reduction in photovoltaics. Protomorphous solar cells prove to be a promising concept for efficient and low-cost photovoltaics on cheap and flexible substrates for large-area production as well as small and mobile applications.
One advantage of printed electronics is that different electrical and electronic components can be printed on top of each other, saving space and increasing reliability and sometimes they are all transparent. One ink must not damage another, and low temperature annealing is vital if low-cost flexible materials such as paper and plastic film are to be used. There is much sophisticated engineering and chemistry involved here, with iTi, Pixdro, Asahi Kasei, Merck & Co.|Merck, BASF, HC Starck, Hitachi Chemical and Frontier Carbon Corporation among the leaders.
Electronic devices based on organic compounds are now widely used, with many new products under development. Sony reported the first full-color, video-rate, flexible, plastic display made purely of organic materials; television screen based on OLED materials; biodegradable electronics based on organic compound and low-cost organic solar cell are also available.
Fabrication methods.
There are important differences between the processing of small molecule organic semiconductors and semiconducting polymers. Small molecule semiconductors are quite often insoluble and typically require deposition via vacuum sublimation. While usually thin films of soluble conjugated polymers. Devices based on conductive polymers can be prepared by solution processing methods. Both solution processing and vacuum based methods produce amorphous and polycrystalline films with variable degree of disorder. “Wet” coating techniques require polymers to be dissolved in a volatile solvent, filtered and deposited onto a substrate. Common examples of solvent-based coating techniques include drop casting, spin-coating, doctor-blading, inkjet printing and screen printing. Spin-coating is a widely used technique for small area thin film production. It may result in a high degree of material loss. The doctor-blade technique results in a minimal material loss and was primarily developed for large area thin film production. Vacuum based thermal deposition of small molecules requires evaporation of molecules from a hot source. The molecules are then transported through vacuum onto a substrate. The process of condensing these molecules on the substrate surface results in thin film formation. Wet coating techniques can in some cases be applied to small molecules depending on their solubility.
Organic solar cells.
Compared to conventional inorganic solar cell, organic solar cells have the advantage of lower fabrication cost. An organic solar cell is a device that uses organic electronics to convert light into electricity. Organic solar cells utilize organic photovoltaic materials, organic semiconductor diodes that convert light into electricity. Figure to the right shows five commonly used organic photovoltaic materials. Electrons in these organic molecules can be delocalized in a delocalized π orbital with a corresponding π* antibonding orbital. The difference in energy between the π orbital, or highest occupied molecular orbital(HOMO), and π* orbital, or lowest unoccupied molecular orbital(LUMO) is called the band gap of organic photovoltaic materials. Typically, the band gap lies in the range of 1-4eV.
The difference in the band gap of organic photovoltaic materials leads to different chemical structures and forms of organic solar cells. Different forms of solar cells includes single-layer organic photovoltaic cells, bilayer organic photovoltaic cells and heterojunction photovoltaic cells. However, all three of these types of solar cells share the approach of sandwiching the organic electronic layer between two metallic conductors, typically indium tin oxide.
Organic field-effect transistors.
An organic field-effect transistor device consists of three major components: the source, the drain and the gate. Generally, a field-effect transistor has two plates, source in contact with drain and the gate respectively, working as conducting channel. The electrons move from source to the drain, and the gate serves to control the electrons’ movement from source to drain. Different types of FETs are designed based on carrier properties. Thin film transistor (TFT), among them, is an easy fabricating one. In a thin film transistor, the source and drain are made by directly depositing a thin layer of semiconductor followed by a thin film of insulator between semiconductor and the metal gate contact. Such a thin film is made by either thermal evaporation, or simply spins coating. In a TFT device, there is no carrier movement between the source and drain. After applying a positive charge, accumulation of electrons on the interface cause bending of the semiconductor and ultimately lowers the conduction band with regards to the Fermi-level of the semiconductor. Finally, a highly conductive channel is formed at the interface.
Features.
Conductive polymers are lighter, more flexible, and less expensive than inorganic conductors. This makes them a desirable alternative in many applications. It also creates the possibility of new applications that would be impossible using copper or silicon.
Organic electronics not only includes organic semiconductors, but also organic dielectrics, conductors and light emitters.
New applications include smart windows and electronic paper. Conductive polymers are expected to play an important role in the emerging science of molecular computers.
In general organic conductive polymers have a higher resistance and therefore conduct electricity poorly and inefficiently, as compared to inorganic conductors. Researchers currently are exploring ways of "doping" organic semiconductors, like melanin, with relatively small amounts of conductive metals to boost conductivity. Another method is to rely on two layers of gate dielectric, thereby allowing the high turn-on voltage caused by the fluorinated polymer to be mitigated by the low-voltage electrical characteristics of the high-k metal-oxide and the low defect count at the interface of the former to be used instead of the leaky defects of the latter. However, for many applications, inorganic conductors will remain the only viable option.

</doc>
<doc id="22194" url="https://en.wikipedia.org/wiki?curid=22194" title="Operating system">
Operating system

An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs. The operating system is a component of the system software in a computer system. Application programs usually require an operating system to function.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers.
Examples of popular desktop operating systems include Apple OS X, Linux and its variants, and Microsoft Windows. So-called mobile operating systems include Android and iOS. Other classes of operating systems, such as real-time (RTOS), also exist.
Types of operating systems.
Single- and multi-tasking.
A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency. This is achieved by time-sharing, dividing the available processor time between multiple processes that are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking. 32-bit versions of both Windows NT and Win9x, used preemptive multi-tasking.
Single- and multi-user.
Single-user operating systems have no facilities to distinguish users, but may allow multiple programs to run in tandem. A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
Distributed.
A distributed operating system manages a group of distinct computers and makes them appear to be a single computer. The development of networked computers that could be linked and communicate with each other gave rise to distributed computing. Distributed computations are carried out on more than one machine. When computers in a group work in cooperation, they form a distributed system.
Templated.
In an OS, distributed and cloud computing context, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines. The technique is used both in virtualization and cloud computing management, and is common in large server warehouses.
Embedded.
Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines like PDAs with less autonomy. They are able to operate with a limited number of resources. They are very compact and extremely efficient by design. Windows CE and Minix 3 are some examples of embedded operating systems.
Real-time.
A real-time operating system is an operating system that guarantees to process events or data within a certain short amount of time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. An event-driven system switches between tasks based on their priorities or external events while time-sharing operating systems switch tasks based on clock interrupts.
Library.
A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries. These libraries are composed with the application and configuration code to construct unikernels which are specialized, single address space, machine images that can be deployed to cloud or embedded environments.
History.
Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plug boards. These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards. After programmable general purpose computers were invented, machine languages (consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).
In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period of time and would arrive at a scheduled time with program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the Universal Turing machine.
Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and generating computer code from human-readable symbolic code. This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England the job queue was at one time a washing line (clothes line) from which tapes were hung with different colored clothes-pegs to indicate job-priority.
An improvement was the Atlas Supervisor introduced with the Manchester Atlas commissioned in 1962, "considered by many to be the first recognisable modern operating system". Brinch Hansen described it as "the most significant breakthrough in the history of operating systems."
Mainframes.
Through the 1950s, many major features were pioneered in the field of operating systems, including batch processing, input/output interrupt, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094.
During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and applications written for OS/360 can still be run on modern machines.
OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during update. When the process is terminated for any reason, all of these resources are re-claimed by the operating system.
The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: COS/360 (Compatibility Operating System), DOS/360 (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).
Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.
In 1961, Burroughs Corporation introduced the B5000 with the MCP, (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no machine language or assembler, and indeed the MCP was the first OS to be written exclusively in a high-level language ESPOL, a dialect of ALGOL. MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. During development of the AS/400, IBM made an approach to Burroughs to licence MCP to run on the AS/400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. MCP is still in use today in the Unisys ClearPath/MCP line of computers.
UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems. Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
General Electric and MIT developed General Electric Comprehensive Operating Supervisor (GECOS), which introduced the concept of ringed security privilege levels. After acquisition by Honeywell it was renamed General Comprehensive Operating System (GCOS).
Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community.
From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/165 and 360/168) were microprogrammed implementations.
The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
Microcomputers.
The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as "monitors". One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS). In the 1980s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative Graphical User Interface (GUI) to the Mac OS.
The introduction of the Intel 80386 CPU chip in October of 1985, with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X.
The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
Examples of operating systems.
Unix and Unix-like operating systems.
Unix was originally written in assembly language. Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).
The "Unix-like" family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name "UNIX" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. "UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
Four operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and Sparc servers, and PCs. Apple's OS X, a replacement for Apple's earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD.
Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
BSD and its descendants.
A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.
In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.
Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995).
OS X.
OS X (formerly "Mac OS X") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. OS X is the successor to the original Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, OS X is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.
The operating system was first released in 1999 as Mac OS X Server 1.0, with a desktop-oriented version (Mac OS X v10.0 "Cheetah") following in March 2001. Since then, six more distinct "client" and "server" editions of OS X have been released, until the two were merged in OS X 10.7 "Lion".
Prior to its merging with OS X, the server edition OS X Server was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. OS X Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as "OS X" (dropping "Mac" from the name). The server tools are now offered as an application.
Linux.
The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.
Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smart-watches. Although estimates suggest that Linux is used on only 1.82% of all "desktop" (or laptop) PCs, it has been widely adopted for use in servers and embedded systems such as cell phones. Linux has superseded Unix on many platforms and is used on most supercomputers including the top 207. Many of the same computers are also on Green500 (but in different order), and Linux runs on the top 10. Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android.
Google Chrome OS.
Chrome OS is an operating system based on the Linux kernel and designed by Google. It is developed out in the open in the Chromium OS open source variant and Google makes a proprietary variant of it (similar to the split for the Chrome and Chromium browser). Since Chromium OS targets computer users who spend most of their time on the Internet, it is mainly a web browser with limited ability to run local applications, though it has a built-in file manager and media player (in later versions, (modified) Android apps have also been supported, since the browser has been made to support them). Instead, it relies on Internet applications (or Web apps) used in the web browser to accomplish tasks such as word processing. Chromium OS differs from Chrome OS in that Chromium is open-source and used primarily by developers whereas Chrome OS is the operating system shipped out in Chromebooks.
Microsoft Windows.
Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers. The latest version is Windows 10.
In 2011, Windows 7 overtook Windows XP as most common version in use.
Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS and 16 bits Windows 3.x drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and 32-bit ARM microprocessors. In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures.
Server editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. However, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.
Other.
There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; Mac OS, the non-Unix precursor to Apple's Mac OS X; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. OpenVMS, formerly from DEC, is still under active development by Hewlett-Packard. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research.
Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9.
Components.
The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
Kernel.
With the aid of the firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.
Program execution.
The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices.
Interrupts.
Interrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment. The alternative having the operating system "watch" the various sources of input for events (polling) that require action can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.
When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call. In modern operating systems, interrupts are handled by the operating system's kernel. Interrupts may come from either the computer's hardware or the running program.
When a hardware device triggers an interrupt, the operating system's kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called a device driver, which may be part of the operating system's kernel, part of another program, or both. Device drivers may then relay information to a running program by various means.
A program may also trigger an interrupt to the operating system. If a program wishes to access hardware, for example, it may interrupt the operating system's kernel, which causes control to be passed back to the kernel. The kernel then processes the request. If a program wishes additional resources (or wishes to shed resources) such as memory, it triggers an interrupt to get the kernel's attention.
Modes.
Modern CPUs support multiple modes of operation. CPUs with this capability use at least two modes: protected mode and supervisor mode. The supervisor mode is used by the operating system's kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is written and erased, and communication with devices like graphics cards. Protected mode, in contrast, is used for almost everything else. Applications operate within protected mode, and can only use hardware by communicating with the kernel, which controls everything in supervisor mode. CPUs might have other modes similar to protected mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.
When a computer first starts up, it is automatically running in supervisor mode. The first few programs to run on the computer, being the BIOS or EFI, bootloader, and the operating system have unlimited access to hardware and this is required because, by definition, initializing a protected environment can only be done outside of one. However, when the operating system passes control to another program, it can place the CPU into protected mode.
In protected mode, programs may have access to a more limited set of the CPU's instructions. A user program may leave protected mode only by triggering an interrupt, causing control to be passed back to the kernel. In this way the operating system can maintain exclusive control over things like access to hardware and memory.
The term "protected mode resource" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting (for example, by killing the program).
Memory management.
Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn't exist in all computers.
In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt which cause the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.
Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.
Virtual memory.
The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.
When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.
In modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
"Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.
Multitasking.
Multitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.
An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.
Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.
The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)
On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The AmigaOS is an exception, having preemptive multitasking from its very first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals).
Disk access and file systems.
Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.
Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.
While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.
A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.
When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.
Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ext3 and ReiserFS in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).
Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.
Device drivers.
A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.
Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.
Networking.
Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's network address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.
Security.
A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.
The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between "privileged" and "non-privileged", systems commonly have a form of requester "identity", such as a user name. To establish identity there may be a process of "authentication". Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is "authorization"; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.
In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, "who has been reading this file?"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.
External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the "Trusted Computer System Evaluation Criteria" (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.
Network services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.
An alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java.
Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing.
User interface.
Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
Graphical user interfaces.
Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of Mac OS, the GUI is integrated into the kernel.
While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and Mac OS X are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.
Many computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma Desktop is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.
Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).
Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.
Real-time operating systems.
A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.
Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase. Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.
Operating system development as a hobby.
Operating system development is one of the most complicated activities in which a computing hobbyist may engage. A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.
In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
Examples of a hobby operating system include ReactOS and Syllable.
Diversity of operating systems and portability.
Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
Unix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11.
This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms like Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
Market share.
Source: Gartner
In 2014, Android was first (currently not replicated by others, in a single year) operating system ever to ship on a billion devices, becoming the most popular operating system by installed base.

</doc>
<doc id="22196" url="https://en.wikipedia.org/wiki?curid=22196" title="Orson Welles">
Orson Welles

George Orson Welles (; May 6, 1915 – October 10, 1985) was an American actor, director, writer, and producer who worked in theatre, radio, and film. He is remembered for his innovative work in all three: in theatre, most notably "Caesar" (1937), a Broadway adaptation of William Shakespeare's "Julius Caesar"; in radio, the 1938 broadcast "The War of the Worlds", one of the most famous in the history of radio; and in film, "Citizen Kane" (1941), consistently ranked as one of the all-time greatest films.
Welles directed a number of high-profile stage productions for the Federal Theatre Project in his early twenties, including an innovative adaptation of "Macbeth" with an entirely African American cast, and the political musical "The Cradle Will Rock". In 1937 he and John Houseman founded the Mercury Theatre, an independent repertory theatre company that presented an acclaimed series of productions on Broadway through 1941. Welles found national and international fame as the director and narrator of a 1938 radio adaptation of H. G. Wells' novel "The War of the Worlds" performed for his radio anthology series "The Mercury Theatre on the Air". It reportedly caused widespread panic when listeners thought that an invasion by extraterrestrial beings was occurring. Although some contemporary sources claim these reports of panic were mostly false and overstated, they rocketed Welles to notoriety.
His first film was "Citizen Kane" (1941), which he co-wrote, produced, directed, and starred in as Charles Foster Kane. Welles was an outsider to the studio system and directed only 13 full-length films in his career. He struggled for creative control on his projects early on with the major film studios and later in life with a variety of independent financiers, and his films were either heavily edited or remained unreleased. His distinctive directorial style featured layered and nonlinear narrative forms, innovative uses of lighting such as chiaroscuro, unusual camera angles, sound techniques borrowed from radio, deep focus shots, and long takes. He has been praised as a major creative force and as "the ultimate auteur".
Welles followed up "Citizen Kane" with critically acclaimed films including "The Magnificent Ambersons" in 1942 and "Touch of Evil" in 1958. Although these three are his most acclaimed films, critics have argued other works of his, such as "The Lady from Shanghai" (1947) and "Chimes at Midnight" (1966), are underappreciated.
In 2002, Welles was voted the greatest film director of all time in two British Film Institute polls among directors and critics, and a wide survey of critical consensus, best-of lists, and historical retrospectives calls him the most acclaimed director of all time. Well known for his baritone voice, Welles was a well-regarded actor in radio and film, a celebrated Shakespearean stage actor, and an accomplished magician noted for presenting troop variety shows in the war years.
Early life.
George Orson Welles was born May 6, 1915, in Kenosha, Wisconsin, son of Richard Head Welles (b. Richard Hodgdon Wells, November 12, 1872, near St. Joseph, Missouri; d. December 28, 1930, Chicago, Illinois) and Beatrice Ives Welles (b. September 1, 1881, Springfield, Illinois; d. May 10, 1924, Chicago). He was named after his paternal great-grandfather, influential Kenosha attorney Orson S. Head, and his brother George Head.
Despite his family's affluence, Welles encountered hardship in childhood. His parents separated and moved to Chicago in 1919. His father, who made a fortune as the inventor of a popular bicycle lamp, became an alcoholic and stopped working. Welles's mother, a pianist, played during lectures by Dudley Crafts Watson at the Art Institute of Chicago to support her son and herself; the oldest Welles boy, "Dickie," was institutionalized at an early age because he had learning difficulties. Beatrice died of hepatitis in a Chicago hospital May 10, 1924, aged 42, just after Welles's ninth birthday. The Gordon String Quartet, which had made its first appearance at her home in 1921, played at Beatrice's funeral.
After his mother's death Welles ceased pursuing music. It was decided that he would spend the summer with the Watson family at a private art colony in Wyoming, New York, established by Lydia Avery Coonley Ward. There he played and became friends with the children of the Aga Khan, including the 12-year-old Prince Aly Khan. Then, in what Welles later described as "a hectic period" in his life, he lived in a Chicago apartment with both his father and Dr. Maurice Bernstein, a Chicago physician who had been a close friend of both his parents. Welles briefly attended public school before his alcoholic father left business altogether and took him along on his travels to Jamaica and the Far East. When they returned they settled in a hotel in Grand Detour, Illinois, that was owned by his father. When the hotel burned down, Welles and his father took to the road again.
"During the three years that Orson lived with his father, some observers wondered who took care of whom", wrote biographer Frank Brady.
"In some ways, he was never really a young boy, you know," said Roger Hill, who became Welles's teacher and lifelong friend.
Welles briefly attended public school in Madison, Wisconsin, enrolled in the fourth grade. On September 15, 1926, he entered the Todd Seminary for Boys, an expensive independent school in Woodstock, Illinois, that his older brother, Richard Ives Welles, had attended ten years before but was expelled for misbehavior. At Todd School Welles came under the influence of Roger Hill, a teacher who was later Todd's headmaster. Hill provided Welles with an "ad hoc" educational environment that proved invaluable to his creative experience, allowing Welles to concentrate on subjects that interested him. Welles performed and staged theatrical experiments and productions there.
"Todd provided Welles with many valuable experiences", wrote critic Richard France. "He was able to explore and experiment in an atmosphere of acceptance and encouragement. In addition to a theater the school's own radio station was at his disposal." Welles's first radio performance was on the Todd station, an adaptation of "Sherlock Holmes" that he also wrote.
On December 28, 1930, when Welles was 15, his father died of heart and kidney failure at the age of 58, alone in a hotel in Chicago. Shortly before this, Welles had announced to his father that he would stop seeing him, believing it would prompt his father to refrain from drinking. As a result, Orson felt guilty because he believed his father had drunk himself to death because of him. His father's will left it to Orson to name his guardian. When Roger Hill declined, Welles chose Maurice Bernstein.
Following graduation from Todd in May 1931, Welles was awarded a scholarship to Harvard University, while his mentor Roger Hill advocated he attend Cornell College in Iowa. Rather than enrolling, he chose travel. He studied for a few weeks at the Art Institute of Chicago with Boris Anisfeld, who encouraged him to pursue painting.
Welles would occasionally return to Woodstock, the place he eventually named when he was asked in a 1960 interview, "Where is home?" Welles replied, "I suppose it's Woodstock, Illinois, if it's anywhere. I went to school there for four years. If I try to think of a home, it's that."
Early career (1931–1935).
After his father's death, Welles traveled to Europe using a small portion of his inheritance. Welles said that while on a walking and painting trip through Ireland, he strode into the Gate Theatre in Dublin and claimed he was a Broadway star. The manager of Gate, Hilton Edwards, later said he had not believed him but was impressed by his brashness and an impassioned audition he gave. Welles made his stage debut at the Gate Theatre on October 13, 1931, appearing in Ashley Dukes's adaptation of "Jew Suss" as Duke Karl Alexander of Württemberg. He performed small supporting roles in subsequent Gate productions, and he produced and designed productions of his own in Dublin. In March 1932 Welles performed in W. Somerset Maugham's "The Circle" at Dublin's Abbey Theatre and travelled to London to find additional work in the theatre. Unable to obtain a work permit, he returned to the U.S.
Welles found his fame ephemeral and turned to a writing project at Todd School that would become the immensely successful, first entitled "Everybody's Shakespeare" and subsequently, "The Mercury Shakespeare". Welles traveled to North Africa while working on thousands of illustrations for the "Everybody's Shakespeare" series of educational books, a series that remained in print for decades.
In 1933, Roger and Hortense Hill invited Welles to a party in Chicago, where Welles met Thornton Wilder. Wilder arranged for Welles to meet Alexander Woollcott in New York, in order that he be introduced to Katharine Cornell, who was assembling a repertory theatre company. Cornell's husband, director Guthrie McClintic, immediately put Welles under contract and cast him in three plays. "Romeo and Juliet", "The Barretts of Wimpole Street" and "Candida" toured in repertory for 36 weeks beginning in November 1933, with the first of more than 200 performances taking place in Buffalo, New York.
In 1934, Welles got his first job on radio—on "The American School of the Air"—through actor-director Paul Stewart, who introduced him to director Knowles Entrikin. That summer Welles staged a drama festival with the Todd School in Woodstock, Illinois, inviting Micheál Mac Liammóir and Hilton Edwards from Dublin's Gate Theatre to appear along with New York stage luminaries in productions including "Trilby", "Hamlet", "The Drunkard" and "Tsar Paul". At the old firehouse in Woodstock he also shot his first film, an eight-minute short titled "The Hearts of Age".
A revised production of Katharine Cornell's "Romeo and Juliet" opened December 20, 1934, at the Martin Beck Theatre in New York. The Broadway production brought the 19-year-old Welles (now playing Tybalt) to the notice of John Houseman, a theatrical producer who was casting the lead role in the debut production of Archibald MacLeish's verse play, "Panic".
On November 14, 1934, Welles married Chicago socialite and actress Virginia Nicolson (often misspelled "Nicholson") in a civil ceremony in New York. To appease the Nicolsons, who were furious at the couple's elopement, a formal ceremony took place December 23, 1934, at the New Jersey mansion of the bride's godmother. Welles wore a cutaway borrowed from his friend George Macready.
By 1935 Welles was supplementing his earnings in the theater as a radio actor in Manhattan, working with many actors who would later form the core of his Mercury Theatre on programs including "America's Hour", "Cavalcade of America", "Columbia Workshop" and "The March of Time". "Within a year of his debut Welles could claim membership in that elite band of radio actors who commanded salaries second only to the highest paid movie stars," wrote critic Richard France.
Theatre (1936–1938).
Federal Theatre Project.
Part of the Works Progress Administration, the Federal Theatre Project (1935–39) was a New Deal program to fund theatre and other live artistic performances and entertainment programs in the United States during the Great Depression. It was created as a relief measure to employ artists, writers, directors and theater workers. Under national director Hallie Flanagan it was shaped into a true national theatre that created relevant art, encouraged experimentation and innovation, and made it possible for millions of Americans to see live theatre for the first time.
John Houseman, director of the Negro Theatre Unit in New York, invited Welles to join the Federal Theatre Project in 1935. Far from unemployed — "I was so employed I forgot how to sleep" — Welles put a large share of his $1,500-a-week radio earnings into his stage productions, bypassing administrative red tape and mounting the projects more quickly and professionally. "Roosevelt once said that I was the only operator in history who ever illegally siphoned money "into" a Washington project," Welles said.
The Federal Theatre Project was the ideal environment in which Welles could develop his art. Its purpose was employment, so he was able to hire any number of artists, craftsmen and technicians, and he filled the stage with performers. The company for the first production, an adaptation of William Shakespeare's "Macbeth" with an entirely African-American cast, numbered 150. The production became known as the "Voodoo Macbeth" because Welles changed the setting to a mythical island suggesting the Haitian court of King Henri Christophe, with Haitian "vodou" fulfilling the rôle of Scottish witchcraft. The play opened April 14, 1936, at the Lafayette Theatre in Harlem and was received rapturously. At 20, Welles was hailed as a prodigy. The production then made a 4,000-mile national tour that included two weeks at the Texas Centennial Exposition in Dallas.
Welles next mounted the farce "Horse Eats Hat", an adaptation by Welles and Edwin Denby of Eugène Labiche's play, "Un Chapeau de Paille d'Italie". The play was presented September 26 – December 5, 1936, at Maxine Elliott's Theatre, New York, and featured Joseph Cotten in his first starring role. It was followed by an adaptation of "Dr. Faustus" that used light as a prime unifying scenic element in a nearly black stage, presented January 8 – May 9, 1937, at Maxine Elliott's Theatre.
Outside the scope of the Federal Theatre Project, American composer Aaron Copland chose Welles to direct "The Second Hurricane" (1937), an operetta with a libretto by Edwin Denby. Presented at the Henry Street Settlement Music School in New York for the benefit of high school students, the production opened April 21, 1937, and ran its scheduled three performances.
In 1937, Welles rehearsed Marc Blitzstein's political operetta, "The Cradle Will Rock". It was originally scheduled to open June 16, 1937, in its first public preview. Because of severe federal cutbacks in the Works Progress projects, the show's premiere at the Maxine Elliott Theatre was canceled. The theater was locked and guarded to prevent any government-purchased materials from being used for a commercial production of the work. In a last-minute move, Welles announced to waiting ticket-holders that the show was being transferred to the Venice, 20 blocks away. Some cast, and some crew and audience, walked the distance on foot. The union musicians refused to perform in a commercial theater for lower non-union government wages. The actors' union stated that the production belonged to the Federal Theater Project and could not be performed outside that context without permission. Lacking the participation of the union members, "The Cradle Will Rock" began with Blitzstein introducing the show and playing the piano accompaniment on stage with some cast members performing from the audience. This impromptu performance was well received by its audience.
Mercury Theatre.
Breaking with the Federal Theatre Project in 1937, Welles and Houseman founded their own repertory company, which they called the Mercury Theatre. The name was inspired by the title of the iconoclastic magazine, "The American Mercury". Welles was executive producer, and the original company included such actors as Joseph Cotten, George Coulouris, Geraldine Fitzgerald, Arlene Francis, Martin Gabel, John Hoyt, Norman Lloyd, Vincent Price, Stefan Schnabel and Hiram Sherman.
"I think he was the greatest directorial talent we've ever had in the theater," Lloyd said of Welles in a 2014 interview. "When you saw a Welles production, you saw the text had been affected, the staging was remarkable, the sets were unusual, music, sound, lighting, a totality of everything. We had not had such a man in our theater. He was the first and remains the greatest."
The Mercury Theatre opened November 11, 1937, with "Caesar", Welles's modern-dress adaptation of Shakespeare's tragedy "Julius Caesar" — streamlined into an anti-fascist tour de force that Joseph Cotten later described as "so vigorous, so contemporary that it set Broadway on its ear." The set was completely open with no curtain, and the brick stage wall was painted dark red. Scene changes were achieved by lighting alone. On the stage was a series of risers; squares were cut into one at intervals and lights were set beneath it, pointing straight up to evoke the "cathedral of light" at the Nuremberg Rallies. "He staged it like a political melodrama that happened the night before," said Lloyd.
Beginning January 1, 1938, "Caesar" was performed in repertory with "The Shoemaker's Holiday"; both productions moved to the larger National Theatre. They were followed by "Heartbreak House" (April 29, 1938) and "Danton's Death" (November 5, 1938). As well as being presented in a pared-down oratorio version at the Mercury Theatre on Sunday nights in December 1937, "The Cradle Will Rock" was at the Windsor Theatre for 13 weeks (January 4–April 2, 1938). Such was the success of the Mercury Theatre that Welles appeared on the cover of "Time" magazine, in full makeup as Captain Shotover in "Heartbreak House", in the issue dated May 9, 1938—three days after his 23rd birthday.
Radio (1936–1940).
Simultaneously with his work in the theatre, Welles worked extensively in radio as an actor, writer, director and producer, often without credit. Between 1935 and 1937 he was earning as much as $2,000 a week, shuttling between radio studios at such a pace that he would arrive barely in time for a quick scan of his lines before he was on the air. While he was directing the "Voodoo Macbeth" Welles was dashing between Harlem and midtown Manhattan three times a day to meet his radio commitments.
In addition to continuing as a repertory player on "The March of Time", in the fall of 1936 Welles adapted and performed "Hamlet" in an early two-part episode of CBS Radio's "Columbia Workshop". His performance as the announcer in the series' April 1937 presentation of Archibald MacLeish's verse drama "The Fall of the City" was an important development in his radio career and made the 21-year-old Welles an overnight star.
In July 1937, the Mutual Network gave Welles a seven-week series to adapt "Les Misérables". It was his first job as a writer-director for radio, the radio debut of the Mercury Theatre, and one of Welles's earliest and finest achievements. He invented the use of narration in radio.
"By making himself the center of the storytelling process, Welles fostered the impression of self-adulation that was to haunt his career to his dying day," wrote critic Andrew Sarris. "For the most part, however, Welles was singularly generous to the other members of his cast and inspired loyalty from them above and beyond the call of professionalism."
That September, Mutual chose Welles to play Lamont Cranston, also known as "The Shadow". He performed the role anonymously through mid-September 1938.
"The Mercury Theatre on the Air".
After the theatrical successes of the Mercury Theatre, CBS Radio invited Orson Welles to create a summer show for 13 weeks. The series began July 11, 1938, initially titled "First Person Singular", with the formula that Welles would play the lead in each show. Some months later the show was called "The Mercury Theatre on the Air". The weekly hour-long show presented radio plays based on classic literary works, with original music composed and conducted by Bernard Herrmann.
The Mercury Theatre's radio adaptation of "The War of the Worlds" by H. G. Wells October 30, 1938, brought Welles instant fame. The combination of the news bulletin form of the performance with the between-breaks dial spinning habits of listeners was later reported to have created widespread confusion among listeners who failed to hear the introduction, although the extent of this confusion has come into question. Panic was reportedly spread among listeners who believed the fictional news reports of a Martian invasion. The myth of the result created by the combination was reported as fact around the world and disparagingly mentioned by Adolf Hitler in a public speech some months later.
Welles's growing fame drew Hollywood offers, lures that the independent-minded Welles resisted at first. "The Mercury Theatre on the Air," which had been a sustaining show (without sponsorship) was picked up by Campbell Soup and renamed "The Campbell Playhouse." "The Mercury Theatre on the Air" made its last broadcast on December 4, 1938, and "The Campbell Playhouse" began five days later.
Welles began commuting from Hollywood to New York for the two Sunday broadcasts of "The Campbell Playhouse" after signing a film contract with RKO Pictures in August 1939. In November 1939, production of the show moved from New York to Los Angeles.
After 20 shows, Campbell began to exercise more creative control and had complete control over story selection. As his contract with Campbell came to an end, Welles chose not to sign on for another season. After the broadcast of March 31, 1940, Welles and Campbell parted amicably.
Hollywood (1939–1948).
RKO Radio Pictures president George Schaefer eventually offered Welles what generally is considered the greatest contract offered to a filmmaker, much less to one who was untried. Engaging him to write, produce, direct and perform in two motion pictures, the contract subordinated the studio's financial interests to Welles's creative control, and broke all precedent by granting Welles the right of final cut. After signing a summary agreement with RKO on July 22, Welles signed a full-length 63-page contract August 21, 1939. The agreement was bitterly resented by the Hollywood studios and persistently mocked in the trade press.
"Citizen Kane".
RKO rejected Welles's first two movie proposals, but agreed on the third offer—"Citizen Kane". Welles co-wrote, produced and directed the film, and performed the lead role. Welles conceived the project with screenwriter Herman J. Mankiewicz, who was writing radio plays for "The Campbell Playhouse". Mankiewicz based the original outline on the life of William Randolph Hearst, whom he knew socially and came to hate after being exiled from Hearst's circle.
After agreeing on the storyline and character, Welles supplied Mankiewicz with 300 pages of notes and put him under contract to write the first draft screenplay under the supervision of John Houseman. Welles wrote his own draft, then drastically condensed and rearranged both versions and added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, "At the end, naturally, I was the one making the picture, after all—who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own."
Welles's project attracted some of Hollywood's best technicians, including cinematographer Gregg Toland. For the cast, Welles primarily used actors from his Mercury Theatre. Filming "Citizen Kane" took ten weeks.
Hearst's newspapers barred all reference to "Citizen Kane" and exerted enormous pressure on the Hollywood film community to force RKO to shelve the film. RKO chief George Schaefer received a cash offer from MGM's Louis B. Mayer and other major studio executives if he would destroy the negative and existing prints of the film.
While waiting for "Citizen Kane" to be released, Welles directed the original Broadway production of "Native Son", a drama written by Paul Green and Richard Wright based on Wright's novel. Starring Canada Lee, the show ran March 24 – June 28, 1941, at the St. James Theatre. The Mercury Production was the last time Welles and Houseman worked together.
"Citizen Kane" was given a limited release and the film received overwhelming critical praise. It was voted the best picture of 1941 by the National Board of Review and the New York Film Critics Circle. The film garnered nine Academy Award nominations but won only for Best Original Screenplay, shared by Mankiewicz and Welles. "Variety" reported that block voting by screen extras deprived "Citizen Kane" of Oscars for Best Picture and Best Actor (Welles), and similar prejudices were likely to have been responsible for the film receiving no technical awards.
The delay in the film's release and uneven distribution contributed to mediocre results at the box office. After it ran its course theatrically, "Citizen Kane" was retired to the vault in 1942. In postwar France, however, the film's reputation grew after it was seen for the first time in 1946. In the United States, it began to be re-evaluated after it began to appear on television in 1956. That year it was also re-released theatrically, and film critic Andrew Sarris described it as "the great American film" and "the work that influenced the cinema more profoundly than any American film since "Birth of a Nation"." "Citizen Kane" is now hailed as one of the greatest films ever made.
"The Magnificent Ambersons".
Welles's second film for RKO was "The Magnificent Ambersons", adapted by Welles from the Pulitzer Prize-winning novel by Booth Tarkington. Toland was not available, so Stanley Cortez was named cinematographer. The meticulous Cortez worked slowly and the film lagged behind schedule and over budget. Prior to production, Welles's contract was renegotiated, revoking his right to control the final cut. "The Magnificent Ambersons" was in production October 28, 1941 – January 22, 1942.
Throughout the shooting of the film Welles was also producing a weekly half-hour radio series, "The Orson Welles Show". Many of the "Ambersons" cast participated in the CBS Radio series, which ran September 15, 1941 – February 2, 1942.
"Journey into Fear".
At RKO's request, Welles worked on an adaptation of Eric Ambler's spy thriller, "Journey into Fear", co-written with Joseph Cotten. In addition to acting in the film, Welles was the producer. Direction was credited to Norman Foster. Welles later said that they were in such a rush that the director of each scene was determined by whoever was closest to the camera.
"Journey into Fear" was in production January 6–March 12, 1942.
War work.
Goodwill ambassador.
In late November 1941, Welles was appointed as a goodwill ambassador to Latin America by Nelson Rockefeller, U.S. Coordinator of Inter-American Affairs and a principal stockholder in RKO Radio Pictures. The mission of the OCIAA was cultural diplomacy, promoting hemispheric solidarity and countering the growing influence of the Axis powers in Latin America. John Hay Whitney, head of the agency's Motion Picture Division, was asked by the Brazilian government to produce a documentary of the annual Rio Carnival celebration taking place in early February 1942. In a telegram December 20, 1941, Whitney wrote Welles, "Personally believe you would make great contribution to hemisphere solidarity with this project."
The OCIAA sponsored cultural tours to Latin America and appointed goodwill ambassadors including George Balanchine and the American Ballet, Bing Crosby, Aaron Copland, Walt Disney, John Ford and Rita Hayworth. Welles was thoroughly briefed in Washington, D.C., immediately before his departure for Brazil, and film scholar Catherine L. Benamou, a specialist in Latin American affairs, finds it "not unlikely" that he was among the goodwill ambassadors who were asked to gather intelligence for the U.S. government in addition to their cultural duties. She concludes that Welles's acceptance of Whitney's request was "a logical and patently patriotic choice".
In addition to working on his ill-fated film project, "It's All True", Welles was responsible for radio programs, lectures, interviews and informal talks as part of his OCIAA-sponsored cultural mission, which was regarded as a success. He spoke on topics ranging from Shakespeare to visual art at gatherings of Brazil's elite, and his two intercontinental radio broadcasts in April 1942 were particularly intended to tell U.S. audiences that President Vargas was a partner with the Allies. Welles's ambassadorial mission was extended to permit his travel to other nations including Argentina, Bolivia, Chile, Colombia, Ecuador, Guatemala, Mexico, Peru and Uruguay. Welles worked for more than half a year with no compensation.
Welles's own expectations for the film were modest. ""It's All True" was not going to make any cinematic history, nor was it intended to," he later said. "It was intended to be a perfectly honorable execution of my job as a goodwill ambassador, bringing entertainment to the Northern Hemisphere that showed them something about the Southern one."
"It's All True".
In July 1941, Welles conceived "It's All True" as an omnibus film mixing documentary and docufiction in a project that emphasized the dignity of labor and celebrated the cultural and ethnic diversity of North America. It was to have been his third film for RKO, following "Citizen Kane" (1941) and "The Magnificent Ambersons" (1942). Duke Ellington was put under contract to score a segment with the working title, "The Story of Jazz", drawn from Louis Armstrong's 1936 autobiography, "Swing That Music". Armstrong was cast to play himself in the brief dramatization of the history of jazz performance, from its roots to its place in American culture in the 1940s. "The Story of Jazz" was to go into production in December 1941.
Mercury Productions purchased the stories for two other segments—"My Friend Bonito" and "The Captain's Chair"—from documentary filmmaker Robert J. Flaherty. Adapted by Norman Foster and John Fante, "My Friend Bonito" was the only segment of the original "It's All True" to go into production. Filming took place in Mexico September–December 1941, with Norman Foster directing under Welles's supervision.
In December 1941, the Office of the Coordinator of Inter-American Affairs asked Welles to make a film in Brazil that would showcase the Carnaval in Rio de Janeiro. With filming of "My Friend Bonito" about two-thirds complete, Welles decided he could shift the geography of "It's All True" and incorporate Flaherty's story into an omnibus film about Latin America—supporting the Roosevelt administration's Good Neighbor policy, which Welles strongly advocated. In this revised concept, "The Story of Jazz" was replaced by the story of samba, a musical form with a comparable history and one that came to fascinate Welles. He also decided to do a ripped-from-the-headlines episode about the epic voyage of four poor Brazilian fishermen, the jangadeiros, who had become national heroes. Welles later said this was the most valuable story.
Required to film the Carnaval in Rio de Janeiro in early February 1942, Welles rushed to edit "The Magnificent Ambersons" and finish his acting scenes in "Journey into Fear". He ended his lucrative CBS radio show February 2, flew to Washington, D.C., for a briefing, and then lashed together a rough cut of "Ambersons" in Miami with editor Robert Wise. Welles recorded the film's narration the night before he left for South America: "I went to the projection room at about four in the morning, did the whole thing, and then got on the plane and off to Rio—and the end of civilization as we know it."
Welles left for Brazil on February 4 and began filming in Rio February 8, 1942. At the time it did not seem that Welles's other film projects would be disrupted, but as film historian Catherine L. Benamou wrote, "the ambassadorial appointment would be the first in a series of turning points leading—in 'zigs' and 'zags,' rather than in a straight line—to Welles's loss of complete directorial control over both "The Magnificent Ambersons" and "It's All True", the cancellation of his contract at RKO Radio Studio, the expulsion of his company Mercury Productions from the RKO lot, and, ultimately, the total suspension of "It's All True".
In 1942 RKO Pictures underwent major changes under new management. Nelson Rockefeller, the primary backer of the Brazil project, left its board of directors, and Welles's principal sponsor at RKO, studio president George Schaefer, resigned. RKO took control of "Ambersons" and edited the film into what the studio considered a commercial format. Welles's attempts to protect his version ultimately failed. In South America, Welles requested resources to finish "It's All True". Given a limited amount of black-and-white film stock and a silent camera, he was able to finish shooting the episode about the jangadeiros, but RKO refused to support further production on the film.
"So I was fired from RKO," Welles later recalled. "And they made a great publicity point of the fact that I had gone to South America without a script and thrown all this money away. I never recovered from that attack." Later in 1942 when RKO Pictures began promoting its new corporate motto, "Showmanship In Place of Genius: A New Deal at RKO", Welles understood it as a reference to himself.
Radio (1942–43).
Welles returned to the United States August 22, 1942, after more than six months in South America. A week after his return he produced and emceed the first two hours of a seven-hour coast-to-coast War Bond drive broadcast titled "I Pledge America". Airing August 29, 1942, on the Blue Network, the program was presented in cooperation with the United States Department of the Treasury, Western Union (which wired bond subscriptions free of charge) and the American Women's Voluntary Services. Featuring 21 dance bands and a score of stage and screen and radio stars, the broadcast raised more than $10 million—more than $146 million today—for the war effort.
On October 12, 1942, "Cavalcade of America" presented Welles's radio play, "Admiral of the Ocean Sea", an entertaining and factual look at the legend of Christopher Columbus.
"It belongs to a period when hemispheric unity was a crucial matter and many programs were being devoted to the common heritage of the Americas," wrote broadcasting historian Erik Barnouw. "Many such programs were being translated into Spanish and Portuguese and broadcast to Latin America, to counteract many years of successful Axis propaganda to that area. The Axis, trying to stir Latin America against Anglo-America, had constantly emphasized the differences between the two. It became the job of American radio to emphasize their common experience and essential unity."
"Admiral of the Ocean Sea", also known as "Columbus Day", begins with the words, "Hello Americans"—the title Welles would choose for his own series five weeks later.
"Hello Americans", a CBS Radio series broadcast November 15, 1942 – January 31, 1943, was produced, directed and hosted by Welles under the auspices of the Office of the Coordinator for Inter-American Affairs. The 30-minute weekly program promoted inter-American understanding and friendship, drawing upon the research amassed for the ill-fated film, "It's All True". The series was produced concurrently with Welles's other CBS series, "Ceiling Unlimited" (November 9, 1942 – February 1, 1943), sponsored by the Lockheed-Vega Corporation. The program was conceived to glorify the aviation industry and dramatize its role in World War II. Welles's shows were regarded as significant contributions to the war effort.
Throughout the war Welles worked on patriotic radio programs including "Command Performance", "G.I. Journal", "Mail Call", "Nazi Eyes on Canada", "Stage Door Canteen" and "Treasury Star Parade".
"The Mercury Wonder Show".
In early 1943, the two concurrent radio series ("Ceiling Unlimited", "Hello Americans") that Orson Welles created for CBS to support the war effort had ended. Filming also had wrapped on the 1943 film adaptation of "Jane Eyre" and that fee, in addition to the income from his regular guest-star roles in radio, made it possible for Welles to fulfill a lifelong dream. He approached the War Assistance League of Southern California and proposed a show that evolved into a big-top spectacle, part circus and part magic show. He offered his services as magician and director, and invested some $40,000 of his own money in an extravaganza he co-produced with his friend Joseph Cotten: "The Mercury Wonder Show for Service Men". Members of the U.S. armed forces were admitted free of charge, while the general public had to pay. The show entertained more than 1,000 service members each night, and proceeds went to the War Assistance League, a charity for military service personnel.
The development of the show coincided with the resolution of Welles's oft-changing draft status in May 1943, when he was finally declared 4-F—unfit for military service—for a variety of medical reasons. "I felt guilty about the war," Welles told biographer Barbara Leaming. "I was guilt-ridden about my civilian status." He had been publicly hounded about his patriotism since "Citizen Kane", when the Hearst press began persistent inquiries about why Welles had not been drafted.
"The Mercury Wonder Show" ran August 3 – September 9, 1943, in an 80-by-120-foot tent located at 9000 Cahuenga Boulevard, in the heart of Hollywood.
At intermission September 7, 1943, KMPC radio interviewed audience and cast members of "The Mercury Wonder Show"—including Welles and Rita Hayworth, who were married earlier that day. Welles remarked that "The Mercury Wonder Show" had been performed for approximately 48,000 members of the U.S. armed forces.
Radio (1944–45).
The idea of doing a radio variety show occurred to Welles after his success as substitute host of four consecutive episodes (March 14 – April 4, 1943) of "The Jack Benny Program", radio's most popular show, when Benny contracted pneumonia on a performance tour of military bases. A half-hour variety show broadcast January 26 – July 19, 1944, on the Columbia Pacific Network, "The Orson Welles Almanac" presented sketch comedy, magic, mindreading, music and readings from classic works. Many of the shows originated on U.S. military camps, where Welles and his repertory company and guests entertained the troops with a reduced version of "The Mercury Wonder Show". The performances of the all-star jazz group Welles brought together for the show were so popular that the band became a regular feature and was an important force in reviving interest in traditional New Orleans jazz.
Welles was placed on the U.S. Treasury payroll on May 15, 1944, as an expert consultant for the duration of the war, with a retainer of $1 a year. On the recommendation of President Franklin D. Roosevelt, Secretary of the Treasury Henry Morgenthau asked Welles to lead the Fifth War Loan Drive, which opened June 12 with a one-hour radio show on all four networks, broadcast from Texarkana, Texas. Including a statement by the President, the program defined the causes of the war and encouraged Americans to buy $16 billion in bonds to finance the Normandy landings and the most violent phase of World War II. Welles produced additional war loan drive broadcasts on June 14 from the Hollywood Bowl, and June 16 from Soldier Field, Chicago. Americans purchased $20.6 billion in War Bonds during the Fifth War Loan Drive, which ended on July 8, 1944.
Welles campaigned ardently for Roosevelt in 1944. A longtime supporter and campaign speaker for FDR, he occasionally sent the president ideas and phrases that were sometimes incorporated into what Welles characterized as "less important speeches". One of these ideas was the joke in what came to be called the Fala speech, Roosevelt's nationally broadcast September 23 address to the International Teamsters Union which opened the 1944 presidential campaign. Welles campaigned for the Roosevelt–Truman ticket almost full-time in the fall of 1944, traveling to nearly every state to the detriment of his own health and at his own expense. In addition to his radio addresses he filled in for Roosevelt, opposite Republican presidential nominee Thomas E. Dewey, at "The New York Herald Tribune Forum" broadcast October 18 on the Blue Network. Welles accompanied FDR to his last campaign rally, speaking at an event November 4 at Boston's Fenway Park before 40,000 people, and took part in a historic election-eve campaign broadcast November 6 on all four radio networks.
On November 21, 1944, Welles began his association with "This Is My Best", a CBS radio series he would briefly produce, direct, write and host (March 13 – April 24, 1945). He wrote a political column called "Orson Welles' Almanac" (later titled "Orson Welles Today") for "The New York Post" January–November 1945, and advocated the continuation of FDR's New Deal policies and his international vision, particularly the establishment of the United Nations and the cause of world peace.
On April 12, 1945, the day Franklin D. Roosevelt died, the Blue-ABC network marshalled its entire executive staff and national leaders to pay homage to the late president. "Among the outstanding programs which attracted wide attention was a special tribute delivered by Orson Welles", reported "Broadcasting" magazine. Welles spoke at 10:10 p.m Eastern War Time, from Hollywood, and stressed the importance of continuing FDR's work: "He has no need for homage and we who loved him have no time for tears … Our fighting sons and brothers cannot pause tonight to mark the death of him whose name will be given to the age we live in."
Welles presented another special broadcast on the death of Roosevelt the following evening: "We must move on beyond mere death to that free world which was the hope and labor of his life."
He dedicated the April 17 episode of "This Is My Best" to Roosevelt and the future of America on the eve of the United Nations Conference on International Organization. Welles was an advisor and correspondent for the Blue-ABC radio network's coverage of the San Francisco conference that formed the UN, taking place April 24 – June 23, 1945. He presented a half-hour dramatic program written by Ben Hecht on the opening day of the conference, and on Sunday afternoons (April 29 – June 10) he led a weekly discussion from the San Francisco Civic Auditorium.
"The Stranger".
In the fall of 1945 Welles began work on "The Stranger" (1946), a film noir drama about a war crimes investigator who tracks a high-ranking Nazi fugitive to an idyllic New England town. Edward G. Robinson, Loretta Young and Welles star.
Producer Sam Spiegel initially planned to hire director John Huston, who had rewritten the screenplay by Anthony Veiller. When Huston entered the military, Welles was given the chance to direct and prove himself able to make a film on schedule and under budget—something he was so eager to do that he accepted a disadvantageous contract. One of its concessions was that he would defer to the studio in any creative dispute.
"The Stranger" was Welles's first job as a film director in four years. He was told that if the film was successful he could sign a four-picture deal with International Pictures, making films of his own choosing. Welles was given some degree of creative control, and he endeavored to personalize the film and develop a nightmarish tone. He worked on the general rewrite of the script and wrote scenes at the beginning of the picture that were shot but subsequently cut by the producers. He filmed in long takes that largely thwarted the control given to editor Ernest J. Nims under the terms of the contract.
"The Stranger" was the first commercial film to use documentary footage from the Nazi concentration camps. Welles had seen the footage in early May 1945 in San Francisco, as a correspondent and discussion moderator at the UN Conference on International Organization. He wrote of the Holocaust footage in his syndicated "New York Post" column May 7, 1945.
Completed a day ahead of schedule and under budget, "The Stranger" was the only film made by Welles to have been a "bona fide" box office success upon its release. Its cost was $1.034 million; 15 months after its release it had grossed $3.216 million. Within weeks of the completion of the film, International Pictures backed out of its promised four-picture deal with Welles. No reason was given, but the impression was left that "The Stranger" would not make money.
"Around the World".
In the summer of 1946, Welles moved to New York to direct the Broadway musical "Around the World", a stage adaptation of the Jules Verne novel "Around the World in Eighty Days" with a book by Welles and music by Cole Porter. Producer Mike Todd, who would later produce the successful 1956 film adaptation, pulled out from the lavish and expensive production, leaving Welles to support the finances. When Welles ran out of money he convinced Columbia Pictures president Harry Cohn to send enough money to continue the show, and in exchange Welles promised to write, produce, direct and star in a film for Cohn for no further fee. The stage show soon failed due to poor box-office, with Welles unable to claim the losses on his taxes.
Radio (1946).
In 1946, Welles began two new radio series—"The Mercury Summer Theatre on the Air" for CBS, and "Orson Welles Commentaries" for ABC. While "Mercury Summer Theatre" featured half-hour adaptations of some classic Mercury radio shows from the 1930s, the first episode was a condensation of his "Around the World" stage play, and is the only record of Cole Porter's music for the project. Several original Mercury actors returned for the series, as well as Bernard Herrmann. Welles invested his earnings into his failing stage play. "Commentaries" was a political vehicle for him, continuing the themes from his "New York Post" column. Again, Welles lacked a clear focus, until the NAACP brought to his attention the case of Isaac Woodard. Welles brought significant attention to Woodard's cause.
The last broadcast of "Orson Welles Commentaries" on October 6, 1946, marked the end of Welles's own radio shows.
"The Lady from Shanghai".
The film that Welles was obliged to make in exchange for Harry Cohn's help in financing the stage production "Around the World" was "The Lady from Shanghai", filmed in 1947 for Columbia Pictures. Intended as a modest thriller, the budget skyrocketed after Cohn suggested that Welles's then-estranged second wife Rita Hayworth co-star.
Cohn disliked Welles's rough-cut, particularly the confusing plot and lack of close-ups, and was not in sympathy with Welles's Brechtian use of irony and black comedy, especially in a farcical courtroom scene. Cohn ordered extensive editing and re-shoots. After heavy editing by the studio, approximately one hour of Welles's first cut was removed, including much of a climactic confrontation scene in an amusement park funhouse. While expressing displeasure at the cuts, Welles was appalled particularly with the musical score. The film was considered a disaster in America at the time of release, though the closing shootout in a hall of mirrors has since become a touchstone of film noir. Not long after release, Welles and Hayworth finalized their divorce.
Although "The Lady From Shanghai" was acclaimed in Europe, it was not embraced in the U.S. until decades later. A similar difference in reception on opposite sides of the Atlantic followed by greater American acceptance befell the Welles-inspired Chaplin film "Monsieur Verdoux", originally to be directed by Welles starring Chaplin, then directed by Chaplin with the idea credited to Welles.
"Macbeth".
Prior to 1948, Welles convinced Republic Pictures to let him direct a low-budget version of "Macbeth", which featured highly stylized sets and costumes, and a cast of actors lip-syncing to a pre-recorded soundtrack, one of many innovative cost-cutting techniques Welles deployed in an attempt to make an epic film from B-movie resources. The script, adapted by Welles, is a violent reworking of Shakespeare's original, freely cutting and pasting lines into new contexts via a collage technique and recasting "Macbeth" as a clash of pagan and proto-Christian ideologies. Some voodoo trappings of the famous Welles/Houseman Negro Theatre stage adaptation are visible, especially in the film's characterization of the Weird Sisters, who create an effigy of Macbeth as a charm to enchant him. Of all Welles's post-"Kane" Hollywood productions, "Macbeth" is stylistically closest to "Citizen Kane" in its long takes and deep focus photography.
Republic initially trumpeted the film as an important work but decided it did not care for the Scottish accents and held up general release for almost a year after early negative press reaction, including "Life"'s comment that Welles's film "doth foully slaughter Shakespeare." Welles left for Europe, while co-producer and lifelong supporter Richard Wilson reworked the soundtrack. Welles returned and cut 20 minutes from the film at Republic's request and recorded narration to cover some gaps. The film was decried as a disaster. "Macbeth" had influential fans in Europe, especially the French poet and filmmaker Jean Cocteau, who hailed the film's "crude, irreverent power" and careful shot design, and described the characters as haunting "the corridors of some dreamlike subway, an abandoned coal mine, and ruined cellars oozing with water."
Europe (1948–1956).
In Italy he starred as Cagliostro in the 1948 film "Black Magic". His co-star, Akim Tamiroff, impressed Welles so much that Tamiroff would appear in four of Welles's productions during the 1950s and 1960s.
The following year, Welles starred as Harry Lime in Carol Reed's "The Third Man", alongside Joseph Cotten, his friend and co-star from "Citizen Kane", with a script by Graham Greene and a memorable score by Anton Karas.
A few years later, British radio producer Harry Alan Towers would resurrect the Lime character in the radio series "The Adventures of Harry Lime".
Welles appeared as Cesare Borgia in the 1949 Italian film "Prince of Foxes", with Tyrone Power and Mercury Theatre alumnus Everett Sloane, and as the Mongol warrior Bayan in the 1950 film version of the novel "The Black Rose" (again with Tyrone Power).
"Othello".
During this time, Welles was channeling his money from acting jobs into a self-financed film version of Shakespeare's play "Othello". From 1949 to 1951, Welles worked on "Othello", filming on location in Europe and Morocco. The film featured Welles's friends, Micheál Mac Liammóir as Iago and Hilton Edwards as Desdemona's father Brabantio. Suzanne Cloutier starred as Desdemona and Campbell Playhouse alumnus Robert Coote appeared as Iago's associate Roderigo.
Filming was suspended several times as Welles ran out of funds and left for acting jobs, accounted in detail in MacLiammóir's published memoir "Put Money in Thy Purse". The American release prints had a technically flawed soundtrack, suffering from a drop-out of sound at every quiet moment. Welles's daughter, Beatrice Welles-Smith, restored "Othello" in 1992 for a wide re-release. The restoration included reconstructing Angelo Francesco Lavagnino's original musical score, which was originally inaudible, and adding ambient stereo sound effects, which were not in the original film. The restoration went on to a successful theatrical run in America.
In 1952, Welles continued finding work in England after the success of the "Harry Lime" radio show. Harry Alan Towers offered Welles another series, "The Black Museum", which ran for 52 weeks with Welles as host and narrator. Director Herbert Wilcox offered Welles the part of the murdered victim in "Trent's Last Case", based on the novel by E. C. Bentley. In 1953, the BBC hired Welles to read an hour of selections from Walt Whitman's epic poem "Song of Myself". Towers hired Welles again, to play Professor Moriarty in the radio series, "The Adventures of Sherlock Holmes", starring John Gielgud and Ralph Richardson.
Welles briefly returned to America to make his first appearance on television, starring in the "Omnibus" presentation of "King Lear", broadcast live on CBS October 18, 1953. Directed by Peter Brook, the production costarred Natasha Parry, Beatrice Straight and Arnold Moss.
In 1954, director George More O'Ferrall offered Welles the title role in the 'Lord Mountdrago' segment of "Three Cases of Murder", co-starring Alan Badel. Herbert Wilcox cast Welles as the antagonist in "Trouble in the Glen" opposite Margaret Lockwood, Forrest Tucker and Victor McLaglen. Old friend John Huston cast him as Father Mapple in his 1956 film adaptation of Herman Melville's "Moby-Dick", starring Gregory Peck.
"Mr. Arkadin".
Welles's next turn as director was the film "Mr. Arkadin" (1955), which was produced by his political mentor from the 1940s, Louis Dolivet. It was filmed in France, Germany, Spain and Italy on a very limited budget. Based loosely on several episodes of the Harry Lime radio show, it stars Welles as a billionaire who hires a man to delve into the secrets of his past. The film stars Robert Arden, who had worked on the Harry Lime series; Welles's third wife, Paola Mori, whose voice was dubbed by actress Billie Whitelaw; and guest stars Akim Tamiroff, Michael Redgrave, Katina Paxinou and Mischa Auer. Frustrated by his slow progress in the editing room, producer Dolivet removed Welles from the project and finished the film without him. Eventually five different versions of the film would be released, two in Spanish and three in English. The version that Dolivet completed was retitled "Confidential Report". In 2005 Stefan Droessler of the Munich Film Museum oversaw a reconstruction of the surviving film elements.
Television projects.
In 1955, Welles also directed two television series for the BBC. The first was "Orson Welles' Sketch Book", a series of six 15-minute shows featuring Welles drawing in a sketchbook to illustrate his reminiscences for the camera (including such topics as the filming of "It's All True" and the Isaac Woodard case), and the second was "Around the World with Orson Welles", a series of six travelogues set in different locations around Europe (such as Venice, the Basque Country between France and Spain, and England). Welles served as host and interviewer, his commentary including documentary facts and his own personal observations (a technique he would continue to explore in later works).
In 1956, Welles completed "Portrait of Gina". The film cans would remain in a lost-and-found locker at the hotel for several decades, where they were discovered after Welles's death.
Return to Hollywood (1956–1959).
In 1956, Welles returned to Hollywood.
He began filming a projected pilot for Desilu, owned by Lucille Ball and her husband Desi Arnaz, who had recently purchased the former RKO studios. The film was "The Fountain of Youth", based on a story by John Collier. Originally deemed not viable as a pilot, the film was not aired until 1958—and won the Peabody Award for excellence.
Welles guest starred on television shows including "I Love Lucy". On radio, he was narrator of "Tomorrow" (October 17, 1956), a nuclear holocaust drama produced and syndicated by ABC and the Federal Civil Defense Administration.
Welles's next feature film role was in "Man in the Shadow" for Universal Pictures in 1957, starring Jeff Chandler.
"Touch of Evil".
Welles stayed on at Universal to direct (and co-star with) Charlton Heston in the 1958 film "Touch of Evil", based on Whit Masterson's novel "Badge of Evil". Originally only hired as an actor, Welles was promoted to director by Universal Studios at the insistence of Charlton Heston. The film reunited many actors and technicians with whom Welles had worked in Hollywood in the 1940s, including cameraman Russell Metty ("The Stranger"), makeup artist Maurice Seiderman ("Citizen Kane"), and actors Joseph Cotten, Marlene Dietrich and Akim Tamiroff. Filming proceeded smoothly, with Welles finishing on schedule and on budget, and the studio bosses praising the daily rushes. Nevertheless, after the end of production, the studio re-edited the film, re-shot scenes, and shot new exposition scenes to clarify the plot. Welles wrote a 58-page memo outlining suggestions and objections, stating that the film was no longer his version—it was the studio's, but as such, he was still prepared to help with it.
In 1978, a longer preview version of the film was discovered and released.
As Universal reworked "Touch of Evil", Welles began filming his adaptation of Miguel de Cervantes' novel "Don Quixote" in Mexico, starring Mischa Auer as Quixote and Akim Tamiroff as Sancho Panza.
Return to Europe (1959–1970).
He continued shooting "Don Quixote" in Spain and Italy, but replaced Mischa Auer with Francisco Reiguera, and resumed acting jobs.
In Italy in 1959, Welles directed his own scenes as King Saul in Richard Pottier's film "David and Goliath". In Hong Kong he co-starred with Curt Jürgens in Lewis Gilbert's film "Ferry to Hong Kong". In 1960, in Paris he co-starred in Richard Fleischer's film "Crack in the Mirror". In Yugoslavia he starred in Richard Thorpe's film "The Tartars" and Veljko Bulajić's "Battle of Neretva".
Throughout the 1960s, filming continued on "Quixote" on-and-off until the decade, as Welles evolved the concept, tone and ending several times. Although he had a complete version of the film shot and edited at least once, he would continue toying with the editing well into the 1980s, he never completed a version film he was fully satisfied with, and would junk existing footage and shoot new footage. (In one case, he had a complete cut ready in which Quixote and Sancho Panza end up going to the moon, but he felt the ending was rendered obsolete by the 1969 moon landings, and burned 10 reels of this version.) As the process went on, Welles gradually voiced all of the characters himself and provided narration. In 1992, the director Jesús Franco constructed a film out of the portions of "Quixote" left behind by Welles. Some of the film stock had decayed badly. While the Welles footage was greeted with interest, the post-production by Franco was met with harsh criticism.
In 1961, Welles directed "In the Land of Don Quixote", a series of eight half-hour episodes for the Italian television network RAI. Similar to the "Around the World with Orson Welles" series, they presented travelogues of Spain and included Welles's wife, Paola, and their daughter, Beatrice. Though Welles was fluent in Italian, the network was not interested in him providing Italian narration because of his accent, and the series sat unreleased until 1964, by which time the network had added Italian narration of its own. Ultimately, versions of the episodes were released with the original musical score Welles had approved, but without the narration.
"The Trial".
In 1962, Welles directed his adaptation of "The Trial", based on the novel by Franz Kafka and produced by Michael and Alexander Salkind. The cast included Anthony Perkins as Josef K, Jeanne Moreau, Romy Schneider, Paola Mori and Akim Tamiroff. While filming exteriors in Zagreb, Welles was informed that the Salkinds had run out of money, meaning that there could be no set construction. No stranger to shooting on found locations, Welles soon filmed the interiors in the Gare d'Orsay, at that time an abandoned railway station in Paris. Welles thought the location possessed a "Jules Verne modernism" and a melancholy sense of "waiting", both suitable for Kafka. To remain in the spirit of Kafka Welles set up the cutting room together with the Film Editor, Frederick Muller (as Fritz Muller), in the old un-used, cold, depressing, station master office. The film failed at the box-office. Peter Bogdanovich would later observe that Welles found the film riotously funny. Welles also told a BBC interviewer that it was his best film. While filming "The Trial" Welles met Oja Kodar, who later became his mistress and collaborator for the last 20 years of his life.
Welles played a film director in "La Ricotta" (1963)—Pier Paolo Pasolini's segment of the "Ro.Go.Pa.G." movie, although his renowned voice was dubbed by Italian writer Giorgio Bassani. He continued taking what work he could find acting, narrating or hosting other people's work, and began filming "Chimes at Midnight", which was completed in 1966.
"Chimes at Midnight".
Filmed in Spain, "Chimes at Midnight" was based on Welles's play, "Five Kings", in which he drew material from six Shakespeare plays to tell the story of Sir John Falstaff (Welles) and his relationship with Prince Hal (Keith Baxter). The cast includes John Gielgud, Jeanne Moreau, Fernando Rey and Margaret Rutherford; the film's narration, spoken by Ralph Richardson, is taken from the chronicler Raphael Holinshed. Welles held the film in high regard: "It's my favorite picture, yes. If I wanted to get into heaven on the basis of one movie, that's the one I would offer up."
In 1966, Welles directed a film for French television, an adaptation of "The Immortal Story", by Karen Blixen. Released in 1968, it stars Jeanne Moreau, Roger Coggio and Norman Eshley. The film had a successful run in French theaters. At this time Welles met Oja Kodar again, and gave her a letter he had written to her and had been keeping for four years; they would not be parted again. They immediately began a collaboration both personal and professional. The first of these was an adaptation of Blixen's "The Heroine", meant to be a companion piece to "The Immortal Story" and starring Kodar. Unfortunately, funding disappeared after one day's shooting. After completing this film, he appeared in a brief cameo as Cardinal Wolsey in Fred Zinnemann's adaptation of "A Man for All Seasons"—a role for which he won considerable acclaim.
In 1967, Welles began directing "The Deep", based on the novel "Dead Calm" by Charles Williams and filmed off the shore of Yugoslavia. The cast included Jeanne Moreau, Laurence Harvey and Kodar. Personally financed by Welles and Kodar, they could not obtain the funds to complete the project, and it was abandoned a few years later after the death of Harvey. The surviving footage was eventually edited and released by the Filmmuseum München. In 1968 Welles began filming a TV special for CBS under the title "Orson's Bag", combining travelogue, comedy skits and a condensation of Shakespeare's play "The Merchant of Venice" with Welles as Shylock. In 1969 Welles called again the Film Editor Frederick Muller to work with him re-editing the material and they set up cutting rooms at the Safa Palatino Studios in Rome. Funding for the show sent by CBS to Welles in Switzerland was seized by the IRS. Without funding, the show was not completed. The surviving film clips portions were eventually released by the Filmmuseum München.
In 1969, Welles authorized the use of his name for a cinema in Cambridge, Massachusetts. The Orson Welles Cinema remained in operation until 1986, with Welles making a personal appearance there in 1977. Also in 1969 he played a supporting role in John Huston's "The Kremlin Letter". Drawn by the numerous offers he received to work in television and films, and upset by a tabloid scandal reporting his affair with Kodar, Welles abandoned the editing of "Don Quixote" and moved back to America in 1970.
Later career (1970–1985).
Welles returned to Hollywood, where he continued to self-finance his film and television projects. While offers to act, narrate and host continued, Welles also found himself in great demand on television talk shows. He made frequent appearances for Dick Cavett, Johnny Carson, Dean Martin and Merv Griffin.
Welles's primary focus during his final years was "The Other Side of the Wind", an unfinished project that was filmed intermittently between 1970 and 1976. Written by Welles, it is the story of an aging film director (John Huston) looking for funds to complete his final film. The cast includes Peter Bogdanovich, Susan Strasberg, Norman Foster, Edmond O'Brien, Cameron Mitchell and Dennis Hopper. Financed by Iranian backers, ownership of the film fell into a legal quagmire after the Shah of Iran was deposed. While there have been several reports of all the legal disputes concerning ownership of the film being settled, enough disputes still exist to prevent its release.
Welles portrayed Louis XVIII of France in the 1970 film "Waterloo", and narrated the beginning and ending scenes of the historical comedy "Start the Revolution Without Me" (1970).
In 1971, Welles directed a short adaptation of "Moby-Dick", a one-man performance on a bare stage, reminiscent of his 1955 stage production "Moby Dick—Rehearsed". Never completed, it was eventually released by the Filmmuseum München. He also appeared in "Ten Days' Wonder", co-starring with Anthony Perkins and directed by Claude Chabrol, based on a detective novel by Ellery Queen. That same year, the Academy of Motion Picture Arts and Sciences gave him an honorary award "For superlative artistry and versatility in the creation of motion pictures". Welles pretended to be out of town and sent John Huston to claim the award, thanking the Academy on film. Huston criticized the Academy for awarding Welles, even while they refused to give Welles any work.
In 1972, Welles acted as on-screen narrator for the film documentary version of Alvin Toffler's 1970 book "Future Shock". Working again for a British producer, Welles played Long John Silver in director John Hough's "Treasure Island" (1972), an adaptation of the Robert Louis Stevenson novel, which had been the second story broadcast by "The Mercury Theatre on the Air" in 1938. This was the last time he played the lead role in a major film. Welles also contributed to the script, his writing credit was attributed to the pseudonym 'O. W. Jeeves'. Some of Welles' original recorded dialog was redubbed by Robert Rietty.
In 1973, Welles completed "F for Fake", a personal essay film about art forger Elmyr de Hory and the biographer Clifford Irving. Based on an existing documentary by François Reichenbach, it included new material with Oja Kodar, Joseph Cotten, Paul Stewart and William Alland. An excerpt of Welles's 1930s "War of the Worlds" broadcast was recreated for this film; however, none of the dialogue heard in the film actually matches what was originally broadcast. Welles filmed a five-minute trailer, rejected in the U.S., that featured several shots of a topless Kodar.
Welles hosted a British syndicated anthology series, "Orson Welles's Great Mysteries", during the 1973–74 television season. His brief introductions to the 26 half-hour episodes were shot in July 1973 by Gary Graver. The year 1974 also saw Welles lending his voice for that year's remake of Agatha Christie's classic thriller "Ten Little Indians" produced by his former associate, Harry Alan Towers and starring an international cast that included Oliver Reed, Elke Sommer and Herbert Lom.
In 1975, Welles narrated the documentary "", focusing on Warner Bros. cartoons from the 1940s. Also in 1975, the American Film Institute presented Welles with its third Lifetime Achievement Award (the first two going to director John Ford and actor James Cagney). At the ceremony, Welles screened two scenes from the nearly finished "The Other Side of the Wind".
In 1976, Paramount Television purchased the rights for the entire set of Rex Stout's Nero Wolfe stories for Orson Welles. Welles had once wanted to make a series of Nero Wolfe movies, but Rex Stout – who was leery of Hollywood adaptations during his lifetime after two disappointing 1930s films – turned him down. Paramount planned to begin with an ABC-TV movie and hoped to persuade Welles to continue the role in a mini-series. Frank D. Gilroy was signed to write the television script and direct the TV movie on the assurance that Welles would star, but by April 1977 Welles had bowed out. In 1980 the Associated Press reported "the distinct possibility" that Welles would star in a Nero Wolfe TV series for NBC television. Again, Welles bowed out of the project due to creative differences and William Conrad was cast in the role.
In 1979, Welles completed his documentary "Filming Othello", which featured Michael MacLiammoir and Hilton Edwards. Made for West German television, it was also released in theaters. That same year, Welles completed his self-produced pilot for "The Orson Welles Show" television series, featuring interviews with Burt Reynolds, Jim Henson and Frank Oz and guest-starring The Muppets and Angie Dickinson. Unable to find network interest, the pilot was never broadcast. Also in 1979, Welles appeared in the biopic "The Secret of Nikola Tesla", and a cameo in "The Muppet Movie" as Lew Lord.
Beginning in the late 1970s, Welles participated in a series of famous television commercial advertisements. For two years he was on-camera spokesman for the Paul Masson Vineyards, and sales grew by one third during the time Welles intoned what became a popular catchphrase: "We will sell no wine before its time." He was also the voice behind the long-running Carlsberg "Probably the best lager in the world" campaign, promoted Domecq sherry on British television and provided narration on adverts for Findus, though the actual adverts have been overshadowed by a famous blooper reel of voice recordings, known as the Frozen Peas reel. He also did commercials for the Preview Subscription Television Service seen on stations around the country including WCLQ/Cleveland, KNDL/St. Louis and WSMW/Boston.
In 1981, Welles hosted the documentary "The Man Who Saw Tomorrow", about Renaissance-era prophet Nostradamus. In 1982, the BBC broadcast "The Orson Welles Story" in the "Arena" series. Interviewed by Leslie Megahey, Welles examined his past in great detail, and several people from his professional past were interviewed as well. It was reissued in 1990 as "With Orson Welles: Stories of a Life in Film". Welles provided narration for the tracks "Defender" from Manowar's 1987 album "Fighting the World" and "Dark Avenger" on their 1982 album, "Battle Hymns". His name was misspelled on the latter album, as he was credited as "Orson Wells".
During the 1980s, Welles worked on such film projects as "The Dreamers", based on two stories by Isak Dinesen and starring Oja Kodar, and "Orson Welles' Magic Show", which reused material from his failed TV pilot. Another project he worked on was "Filming The Trial", the second in a proposed series of documentaries examining his feature films. While much was shot for these projects, none of them was completed. All of them were eventually released by the Filmmuseum München.
In 1984, Welles narrated the short-lived television series "Scene of the Crime". During the early years of "Magnum, P.I.", Welles was the voice of the unseen character Robin Masters, a famous writer and playboy. Welles's death forced this minor character to largely be written out of the series. In an oblique homage to Welles, the "Magnum, P.I." producers ambiguously concluded that story arc by having one character accuse another of having hired an actor to portray Robin Masters. He also, in this penultimate year released a music single, titled "I Know What It Is To Be Young (But You Don't Know What It Is To Be Old)", which he recorded under Italian label Compagnia Generale del Disco. The song was performed with the Nick Perito Orchestra and the Ray Charles Singers and produced by Jerry Abbott who was father to famed metal guitarist Dimebag Darrell.
The last film roles before Welles's death included voice work in the animated films "Enchanted Journey" (1984) and "" (1986), in which he played the planet-eating robot Unicron. His last film appearance was in Henry Jaglom's 1987 independent film "Someone to Love", released after his death but produced before his voice-over in "Transformers: The Movie". His last television appearance was on the television show "Moonlighting". He recorded an introduction to an episode entitled "The Dream Sequence Always Rings Twice", which was partially filmed in black and white. The episode aired five days after his death and was dedicated to his memory.
In the mid-1980s, Henry Jaglom taped lunch conversations with Welles at Los Angeles's Ma Maison as well as in New York. Edited transcripts of these sessions appear in Peter Biskind's 2013 book "My Lunches With Orson: Conversations Between Henry Jaglom and Orson Welles".
Personal life.
Relationships and family.
Orson Welles and Chicago-born actress and socialite Virginia Nicolson (1916–1996) were married on November 14, 1934. The couple separated in December 1939, and were divorced on February 1, 1940. After bearing with Welles's romances in New York, Virginia had learned that Welles had fallen in love with Mexican actress Dolores del Río.
Infatuated with her since adolescence, Welles met del Río at Darryl Zanuck's ranch soon after he moved to Hollywood in 1939. Their relationship was kept secret until 1941, when del Río filed for divorce from her second husband. They openly appeared together in New York while Welles was directing the Mercury stage production, "Native Son". They acted together in the movie "Journey into Fear" (1943). Their relationship came to an end, among other things, due to the infidelities of Welles. Del Río returned to México in 1943, shortly before Welles married Rita Hayworth.
Welles married Rita Hayworth on September 7, 1943. They were divorced on November 10, 1947. During his last interview, recorded for "The Merv Griffin Show" on the evening before his death, Welles called Hayworth "one of the dearest and sweetest women that ever lived … and we were a long time together—I was lucky enough to have been with her longer than any of the other men in her life."
In 1955, Welles married actress Paola Mori (née Countess Paola di Girifalco), an Italian aristocrat who starred as Raina Arkadin in his 1955 film, "Mr. Arkadin". The couple had embarked on a passionate affair, and they were married at her parents' insistence. They were wed in London May 8, 1955, and never divorced.
Croatian-born artist and actress Oja Kodar became Welles's longtime companion both personally and professionally from 1966 onwards, and they lived together for some of the last 20 years of his life.
Welles had three daughters from his marriages: Christopher Welles Feder (born March 27, 1938, with Virginia Nicolson); Rebecca Welles Manning (December 17, 1944 – October 17, 2004, with Rita Hayworth); and Beatrice Welles (born November 13, 1955, with Paola Mori).
Welles is thought to have had a son, British director Michael Lindsay-Hogg (born May 5, 1940), with Irish actress Geraldine Fitzgerald, then the wife of Sir Edward Lindsay-Hogg, 4th baronet. When Lindsay-Hogg was 16 his mother reluctantly divulged that there were pervasive rumors that his father was Welles, and she denied them—but in such detail that he doubted her veracity. Fitzgerald evaded the subject for the rest of her life. Lindsay-Hogg knew Welles, worked with him in the theatre and met him at intervals throughout Welles's life. After he learned that Welles's oldest daughter Chris, his childhood playmate, had long suspected that he was her brother, Lindsay-Hogg initiated a DNA test that proved inconclusive. In his 2011 autobiography Lindsay-Hogg reported that his questions were resolved by his mother's close friend Gloria Vanderbilt, who wrote that Fitzgerald had told her that Welles was his father. A 2015 Welles biography by Patrick McGilligan, however, reports the impossibility of Welles's paternity: Fitzgerald left the U.S. for Ireland in May 1939 and her son was conceived before her return in late October, while Welles did not travel overseas during that period.
After the death of Rebecca Welles Manning, a man named Marc McKerrow was revealed to be her son, and therefore the direct descendant of Orson Welles and Rita Hayworth. McKerrow's reactions to the revelation and his meeting with Oja Kodar are documented in the 2008 film "Prodigal Sons". McKerrow died on June 18, 2010.
Despite an urban legend promoted by Welles himself, he was not related to Abraham Lincoln's wartime Secretary of the Navy, Gideon Welles. The myth dates back to the first newspaper feature ever written about Welles—"Cartoonist, Actor, Poet and only 10"—in the February 19, 1926, issue of "The Capital Times". The article falsely states that he was descended from "Gideon Welles, who was a member of President Lincoln's cabinet". As presented by Charles Higham in a genealogical chart that introduces his 1985 biography of Welles, Orson Welles's father was Richard Head Welles (born Wells), son of Richard Jones Wells, son of Henry Hill Wells (who had an uncle named Gideon "Wells"), son of William Hill Wells, son of Richard Wells (1734–1801).
Physical characteristics.
In his 1956 biography, Peter Noble describes Welles as "a magnificent figure of a man, over six feet tall, handsome, with flashing eyes and a gloriously resonant speaking-voice". Welles said that a voice specialist once told him he was born to be a heldentenor, a heroic tenor, but that when he was young and working at the Gate Theatre he forced his voice down into a bass-baritone.
Even as a baby Welles was prone to illness, including diphtheria, measles, whooping cough and malaria. From infancy he suffered from asthma, sinus headaches, and backache that was later found to be caused by congenital anomalies of the spine. Foot and ankle trouble throughout his life was the result of flat feet. "As he grew older," Brady wrote, "his ill health was exacerbated by the late hours he was allowed to keep an early penchant for alcohol and tobacco".
In 1928, at age 13, Welles was already more than six feet tall and weighed over 180 pounds. His passport recorded his height as six feet three inches, with brown hair and green eyes.
"Crash diets, drugs, and corsets had slimmed him for his early film roles," wrote biographer Barton Whaley. "Then always back to
gargantuan consumption of high-caloric food and booze. By summer 1949, when he was 34, his weight had crept up to a stout 230 pounds. In 1953 he ballooned from 250 to 275 pounds. After 1960 he remained permanently obese."
Religious beliefs.
When Peter Bogdanovich once asked him about his religion, Orson Welles gruffly replied that it was none of his business, then misinformed him that he was raised Catholic.
Although the Welles family was no longer devout, it was fourth-generation Protestant Episcopalian and, before that, Quaker and Puritan. Welles's earliest paternal forebear in America, Richard Wells, was a leader of the Quaker community in Pennsylvania. His earliest maternal ancestor in America was John Alden, a crew member on the Pilgrim ship "Mayflower".
The funeral of Welles's father Richard H. Welles was Episcopalian.
In April 1982, when interviewer Merv Griffin asked him about his religious beliefs, Welles replied, "I try to be a Christian. I don't pray really, because I don't want to bore God." Near the end of his life Welles was dining at Ma Maison, his favorite restaurant in Los Angeles, when proprietor Patrick Terrail conveyed an invitation from the head of the Greek Orthodox Church, who asked Welles to be his guest of honor at divine liturgy at Saint Sophia Cathedral. Welles replied, "Please tell him I really appreciate that offer, but I am an atheist."
"Orson never joked or teased about the religious beliefs of others," wrote biographer Barton Whaley. "He accepted it as a cultural artifact, suitable for the births, deaths, and marriages of strangers and even some friends—but without emotional or intellectual meaning for himself."
Politics.
Welles was politically active from the beginning of his career. He remained aligned with the left throughout his life, and always defined his political orientation as "progressive". He was a strong supporter of Franklin D. Roosevelt and the New Deal, and often spoke out on radio in support of progressive politics. He campaigned heavily for Roosevelt in the 1944 election.
"During a White House dinner," Welles recalled in a 1983 conversation with his friend Roger Hill, "when I was campaigning for Roosevelt, in a toast, with considerable tongue in cheek, he said, 'Orson, you and I are the two greatest actors alive today'. In private that evening, and on several other occasions, he urged me to run for a Senate seat either in California or Wisconsin. He wasn't alone."
For several years, he wrote a newspaper column on political issues and considered running for the U.S. Senate in 1946, representing his home state of Wisconsin (a seat that was ultimately won by Joseph McCarthy).
Welles' name and political activities are reported on pages 155–157 of Red Channels, the anti-Communist publication that, in part, fueled the already flourishing Hollywood Blacklist. He was in Europe during the height of the Red Scare, thereby nullifying more reasons for the Hollywood establishment to ostracize him.
In 1970, Welles narrated (but did not write) a satirical political record on the administration of President Richard Nixon titled "The Begatting of the President".
He was also an early and outspoken critic of American racism and the practice of segregation.
Death and tributes.
On the evening of October 9, 1985, Welles recorded his final interview on the syndicated TV program, "The Merv Griffin Show", appearing with biographer Barbara Leaming. "Both Welles and Leaming talked of Welles's life and the segment was a nostalgic interlude," wrote biographer Frank Brady. Welles returned to his house in Hollywood and worked into the early hours typing stage directions for the project he and Gary Graver were planning to shoot at UCLA the following day. Welles died sometime on the morning of October 10, following a heart attack. He was found by his chauffeur at around 10 a.m.; the first of Welles's friends to arrive was Paul Stewart.
Welles was cremated by prior agreement with the executor of his estate, Greg Garrison, whose advice about making lucrative TV appearances in the 1970s made it possible for Welles to pay off a portion of the taxes he owed the IRS. A brief private funeral was attended by Paola Mori and Welles's three daughters—the first time they had ever been together. Only a few close friends were invited: Garrison, Graver, Roger Hill and Prince Alessandro Tasca di Cuto. Chris Welles Feder later described the funeral as an awful experience.
A public memorial tribute took place November 2, 1985, at the Directors Guild of America Theater in Los Angeles. Host Peter Bogdanovich introduced speakers including Charles Champlin, Geraldine Fitzgerald, Greg Garrison, Charlton Heston, Roger Hill, Henry Jaglom, Arthur Knight, Oja Kodar, Barbara Leaming, Janet Leigh, Norman Lloyd, Dan O'Herlihy, Patrick Terrail and Robert Wise.
"I know what his feelings were regarding his death," Joseph Cotten later wrote. "He did not want a funeral; he wanted to be buried quietly in a little place in Spain. He wanted no memorial services …" Cotten declined to attend the memorial program; instead he sent a short message, ending with the last two lines of a Shakespeare sonnet that Welles had sent him on his most recent birthday:
But if the while I think on thee, dear friend,
In 1987 the cremated remains of Welles and Mori (killed in a 1986 car crash) were taken to Ronda, Spain, and buried in an old well covered by flowers on the rural estate of a longtime friend, retired bullfighter Antonio Ordóñez.
Unfinished projects.
Welles's reliance on self-production meant that many of his later projects were filmed piecemeal or were not completed. Welles financed his later projects through his own fundraising activities. He often also took on other work to obtain money to fund his own films.
"Don Quixote".
In the mid-1950s, Welles began work on "Don Quixote", initially a commission from CBS television. Welles expanded the film to feature length, developing the screenplay to take Quixote and Sancho Panza into the modern age. Filming stopped with the death of Francisco Reiguera, the actor playing Quixote, in 1969. Orson Welles continued editing the film into the early 1970s. At the time of his death, the film remained largely a collection of footage in various states of editing. The project and more importantly Welles's conception of the project changed radically over time. A version of the film was created from available fragments in 1992 and released to a very negative reception.
A version Oja Kodar supervised, with help from Jess Franco, assistant director during production, was released in 2008 to mixed reactions.
Frederick Muller - the film editor for The Trial, Chimes at Midnight and the CBS Special "Orson Bag" was fortunate to work on editing three reels of the original, unadulterated version - was asked for his opinion in 2013 from a journalist of Time Out, his reply was he felt that if released without image re-editing but with the addition of ad hoc sound and music it probably would have been rather successful.
"The Merchant of Venice".
In 1969, Welles was given another TV commission to film a condensed adaptation of "The Merchant of Venice". Although Welles had actually completed the film by 1970 the finished negative was later mysteriously stolen from his Rome production office. A restored and reconstructed version of the film, made by using the original script and composer's notes, premiered at the 72nd Venice International Film Festival alongside "Othello" as part of the pre-opening ceremonies.
"The Other Side of the Wind".
In 1970, Welles began shooting "The Other Side of the Wind". The film relates the efforts of a film director (played by John Huston) to complete his last Hollywood picture and is largely set at a lavish party. By 1972 the filming was reported by Welles as being "96% complete", though it is likely that Welles had only edited about 40 minutes of the film by 1979. In that year, legal complications over the ownership of the film forced the negative into a Paris vault. In 2004 director Peter Bogdanovich, who acted in the film, announced his intention to complete the production. As of 2009, legal complications over the Welles estate had kept the film from being finished or released.
On October 28, 2014, the Los Angeles-based production company Royal Road Entertainment announced that it had negotiated an agreement, with the assistance of producer Frank Marshall, and would purchase the rights to complete and release "The Other Side of the Wind". Bogdanovich and Marshall will complete Welles's nearly finished film in Los Angeles, aiming to have it ready for screening May 6, 2015 — the 100th anniversary of Welles's birth. Royal Road Entertainment and German producer Jens Koethner Kaul acquired the rights held by Les Films de l'Astrophore and the late Mehdi Boushehri. They reached an agreement with Oja Kodar, who inherited Welles's ownership of the film, and Beatrice Welles, manager of the Welles estate; but at the end of 2015, efforts to complete the film were at an impasse.
Some footage is included in the documentaries "Working with Orson Welles" (1993) and "Orson Welles: One Man Band" (1995).

</doc>
<doc id="22197" url="https://en.wikipedia.org/wiki?curid=22197" title="Open content">
Open content

Open content is a neologism coined by David Wiley in 1998 which describes a creative work that others can copy or modify. The term evokes the related concept of open source software.
History.
Originally, the Open content concept and term was evangelized via the "Open Content Project" by David A. Wiley in 1998, and described works licensed under the Open Content License (a non-free share-alike license, see 'Free content' below) and other works licensed under similar terms.
It has since come to describe a broader class of content without conventional copyright restrictions. The openness of content can be assessed under the '5Rs Framework' based on the extent to which it can be reused, revised, remixed and redistributed by members of the public without violating copyright law. Unlike open source and free content, there is no clear threshold that a work must reach to qualify as 'open content'.
Although open content has been described as a counterbalance to copyright, open content licenses rely on a copyright holder's power to license their work, similarly as copyleft which also utilizes copyright for such an purpose.
In 2003 announced Wiley that the Open Content Project has been succeeded by Creative Commons and their licenses, where he joined as "Director of Educational Licenses".
In 2006 the Creative Commons' successor project was the "Definition of Free Cultural Works" for free content, put forth by Erik Möller, Richard Stallman, Lawrence Lessig, Benjamin Mako Hill, Angela Beesley, and others. The "Definition of Free Cultural Works" is used by the Wikimedia Foundation. In 2008, the Attribution and Attribution-ShareAlike Creative Commons licenses were marked as "Approved for Free Cultural Works" among other licenses.
Another successor project is the "Open Knowledge Foundation" ("OKF"), founded by Rufus Pollock in Cambridge, UK in 2004 as a global non-profit network to promote and share open content and data. In 2007 the Open Knowledge Foundation gave a "Open Knowledge Definition" for ""Content such as music, films, books; Data be it scientific, historical, geographic or otherwise; Government and other administrative information"". In October 2014 with version 2.0 "Open Works" and "Open Licenses" were defined and "open" is described as synonymous to the definitions of open/free in the Open Source Definition, the Free Software Definition and the Definition of Free Cultural Works. A distinct difference is the focus given to the public domain and that it focuses also on the accessibility ("Open access") and the readability ("open formats"). Among several conformant licenses, six are recommended, three own (Open Data Commons Public Domain Dedication and Licence (PDDL), Open Data Commons Attribution License (ODC-BY), Open Data Commons Open Database License (ODbL)) and the CC BY, CC BY-SA, and CC0 creative commons licenses.
"Open Content" definition.
The OpenContent website once defined OpenContent as 'freely available for modification, use and redistribution under a license similar to those used by the Open Source / Free Software community'. However, such a definition would exclude the Open Content License (OPL) because that license forbade charging 'a fee for the itself', a right required by free and open source software licenses.
The term since shifted in meaning. OpenContent ""is licensed in a manner that provides users with free and perpetual permission to engage in the 5R activities.""
The 5Rs are put forward on the OpenContent website as a framework for assessing the extent to which content is open:
This broader definition distinguishes open content from open source software, since the latter must be available for commercial use by the public. However, it is similar to several definitions for open educational resources, which include resources under noncommercial and verbatim licenses.
The later "Open Definition" by the Open Knowledge Foundation define open knowledge with open content and open data as sub-elements and draws heavily on the Open Source Definition; it preserves the limited sense of open content as free content, unifying both.
Open access.
"Open access" refers to toll-free or gratis access to content, consisting mainly of published peer-reviewed scholarly journal articles. Some open access works are also licensed for reuse and redistribution, which would qualify them as open content.
Open content and education.
Over the past decade, open content has been used to develop alternative routes towards higher education. Traditional universities are expensive, and their tuition rates are increasing. Open content allows a free way of obtaining higher education that is "focused on collective knowledge and the sharing and reuse of learning and scholarly content."
There are multiple projects and organizations that promote learning through open content, including OpenCourseWare Initiative, The Saylor Foundation and Khan Academy. Some universities, like MIT, Yale, and Tufts are making their courses freely available on the internet.
Textbooks.
The textbook industry is one of the educational industries in which open content can make the biggest impact. Traditional textbooks, aside from being expensive can also be inconvenient and out of date, because of publishers' tendency to constantly print new editions. Open textbooks help to eliminate this problem, because they are online and thus easily updatable. Being openly licensed and online can be helpful to teachers, because it allows the textbook to be modified according to the teacher's unique curriculum. There are multiple organizations promoting the creation of openly licensed textbooks. Some of these organizations and projects include The University of Minnesota's Open Textbook Library, Connexions, OpenStax College, The Saylor Foundation Open Textbook Challenge and Wikibooks
Licenses.
According to the current definition of open content on the OpenContent website, any general, royalty-free copyright license would qualify as an open license because it 'provides users with the right to make more kinds of uses than those normally permitted under the law. These permissions are granted to users free of charge.'
However, the narrower definition used in the Open Definition effectively limits open content to libre content, any free content license, defined by the Definition of Free Cultural Works, would qualify as an open content license. According to this narrower criteria, the following still-maintained licenses qualify:

</doc>
<doc id="22199" url="https://en.wikipedia.org/wiki?curid=22199" title="Ohio">
Ohio

Ohio is a state in the midwestern region of the United States. Ohio is the 34th largest by area, the 7th most populous, and the 10th most densely populated of the 50 United States. The state's capital and largest city is Columbus.
The state takes its name from the Ohio River. The name originated from the Iroquois word "ohi-yo’", meaning "great river" or "large creek." Partitioned from the Northwest Territory, the state was admitted to the Union as the 17th state (and the first under the Northwest Ordinance) on March 1, 1803. Ohio is historically known as the "Buckeye State" after its Ohio buckeye trees, and Ohioans are also known as "Buckeyes."
The government of Ohio is composed of the executive branch, led by the Governor; the legislative branch, which comprises the Ohio General Assembly; and the judicial branch, which is led by the state Supreme Court. Ohio occupies 16 seats in the United States House of Representatives. Ohio is known for its status as both a swing state and a bellwether in national elections. Six Presidents of the United States have been elected who had Ohio as their home state.
Geography.
Ohio's geographic location has proven to be an asset for economic growth and expansion. Because Ohio links the Northeast to the Midwest, much cargo and business traffic passes through its borders along its well-developed highways. Ohio has the nation's 10th largest highway network, and is within a one-day drive of 50% of North America's population and 70% of North America's manufacturing capacity. To the north, Lake Erie gives Ohio of coastline, which allows for numerous cargo ports. Ohio's southern border is defined by the Ohio River (with the border being at the 1793 low-water mark on the north side of the river), and much of the northern border is defined by Lake Erie. Ohio's neighbors are Pennsylvania to the east, Michigan to the northwest, Ontario Canada, to the north, Indiana to the west, Kentucky on the south, and West Virginia on the southeast. Ohio's borders were defined by metes and bounds in the Enabling Act of 1802 as follows:
Ohio is bounded by the Ohio River, but nearly all of the river itself belongs to Kentucky and West Virginia. In 1980, the U.S. Supreme Court held that, based on the wording of the cessation of territory by Virginia (which at that time included what is now Kentucky and West Virginia), the boundary between Ohio and Kentucky (and, by implication, West Virginia) is the northern low-water mark of the river as it existed in 1792. Ohio has only that portion of the river between the river's 1792 low-water mark and the present high-water mark.
The border with Michigan has also changed, as a result of the Toledo War, to angle slightly northeast to the north shore of the mouth of the Maumee River.
Much of Ohio features glaciated plains, with an exceptionally flat area in the northwest being known as the Great Black Swamp. This glaciated region in the northwest and central state is bordered to the east and southeast first by a belt known as the glaciated Allegheny Plateau, and then by another belt known as the unglaciated Allegheny Plateau. Most of Ohio is of low relief, but the unglaciated Allegheny Plateau features rugged hills and forests.
The rugged southeastern quadrant of Ohio, stretching in an outward bow-like arc along the Ohio River from the West Virginia Panhandle to the outskirts of Cincinnati, forms a distinct socio-economic unit. Geologically similar to parts of West Virginia and southwestern Pennsylvania, this area's coal mining legacy, dependence on small pockets of old manufacturing establishments, and distinctive regional dialect set this section off from the rest of the state. In 1965 the United States Congress passed the Appalachian Regional Development Act, at attempt to "address the persistent poverty and growing economic despair of the Appalachian Region." This act defines 29 Ohio counties as part of Appalachia. While 1/3 of Ohio's land mass is part of the federally defined Appalachian region, only 12.8% of Ohioans live there (1.476 million people.)
Significant rivers within the state include the Cuyahoga River, Great Miami River, Maumee River, Muskingum River, and Scioto River. The rivers in the northern part of the state drain into the northern Atlantic Ocean via Lake Erie and the St. Lawrence River, and the rivers in the southern part of the state drain into the Gulf of Mexico via the Ohio River and then the Mississippi.
The worst weather disaster in Ohio history occurred along the Great Miami River in 1913. Known as the Great Dayton Flood, the entire Miami River watershed flooded, including the downtown business district of Dayton. As a result, the Miami Conservancy District was created as the first major flood plain engineering project in Ohio and the United States.
Grand Lake St. Marys in the west central part of the state was constructed as a supply of water for canals in the canal-building era of 1820–1850. For many years this body of water, over , was the largest artificial lake in the world. It should be noted that Ohio's canal-building projects were not the economic fiasco that similar efforts were in other states. Some cities, such as Dayton, owe their industrial emergence to location on canals, and as late as 1910 interior canals carried much of the bulk freight of the state.
Climate.
The climate of Ohio is a humid continental climate (Köppen climate classification "Dfa") throughout most of the state except in the extreme southern counties of Ohio's Bluegrass region section which are located on the northern periphery of the humid subtropical climate and Upland South region of the United States. Summers are typically hot and humid throughout the state, while winters generally range from cool to cold. Precipitation in Ohio is moderate year-round. Severe weather is not uncommon in the state, although there are typically fewer tornado reports in Ohio than in states located in what is known as the Tornado Alley. Severe lake effect snowstorms are also not uncommon on the southeast shore of Lake Erie, which is located in an area designated as the Snowbelt.
Although predominantly not in a subtropical climate, some warmer-climate flora and fauna does reach well into Ohio. For instance, a number of trees with more southern ranges, such as the blackjack oak, "Quercus marilandica", are found at their northernmost in Ohio just north of the Ohio River. Also evidencing this climatic transition from a subtropical to continental climate, several plants such as the Southern magnolia "(Magnolia grandiflora)", Albizia julibrissin (mimosa), Crape Myrtle, and even the occasional Needle Palm are hardy landscape materials regularly used as street, yard, and garden plantings in the Bluegrass region of Ohio; but these same plants will simply not thrive in much of the rest of the State. This interesting change may be observed while traveling through Ohio on Interstate 75 from Cincinnati to Toledo; the observant traveler of this diverse state may even catch a glimpse of Cincinnati's common wall lizard, one of the few examples of permanent "subtropical" fauna in Ohio.
Records.
The highest recorded temperature was , near Gallipolis on July 21, 1934.
The lowest recorded temperature was , at Milligan on February 10, 1899, during the Great Blizzard of 1899.
Earthquakes.
Although few have registered as noticeable to the average resident, more than 30 earthquakes occurred in Ohio between 2002 and 2007, and more than 200 quakes with a magnitude of 2.0 or higher have occurred since 1776.
The most substantial known earthquake in Ohio history was the Anna (Shelby County) earthquake, which occurred on March 9, 1937. It was centered in western Ohio, and had a magnitude of 5.4, and was of intensity VIII.
Other significant earthquakes in Ohio include: one of magnitude 4.8 near Lima on September 19, 1884; one of magnitude 4.2 near Portsmouth on May 17, 1901; and one of 5.0 in LeRoy Township in Lake County on January 31, 1986, which continued to trigger 13 aftershocks of magnitude 0.5 to 2.4 for two months.
The most recent earthquake in Ohio of any appreciable magnitude occurred on December 31, 2011, at 3:05pm EST. It had a magnitude of 4.0, and its epicenter was located approximately 4 kilometres northwest of Youngstown (), near the Trumbull/Mahoning county border.
The Ohio Seismic Network (OhioSeis), a group of seismograph stations at several colleges, universities, and other institutions, and coordinated by the Division of Geological Survey of the Ohio Department of Natural Resources, maintains an extensive catalog of Ohio earthquakes from 1776 to the present day, as well as earthquakes located in other states whose effects were felt in Ohio.
Major cities.
Columbus (home of The Ohio State University, Franklin University, Capital University, and Ohio Dominican University) is the capital of Ohio, near the geographic center of the state.
Other Ohio cities functioning as centers of United States metropolitan areas include:
Note: The Cincinnati metropolitan area extends into Kentucky and Indiana, the Steubenville metropolitan area extends into West Virginia, and the Youngstown metropolitan area extends into Pennsylvania.
Ohio cities that function as centers of United States micropolitan areas include:
History.
Native Americans.
Archeological evidence suggests that the Ohio Valley was inhabited by nomadic people as early as 13,000 BC. These early nomads disappeared from Ohio by 1,000 BC, "but their material culture provided a base for those who followed them". Between 1,000 and 800 BC, the sedentary Adena culture emerged. As Ohio historian George W. Knepper notes, this sophisticated culture was "so named because evidences of their culture were excavated in 1902 on the grounds of Adena, Thomas Worthington's estate located near Chillicothe". The Adena were able to establish "semi-permanent" villages because they domesticated plants, which included squash, sunflowers, and perhaps corn. Cultivation of these in addition to hunting and gathering supported more settled, complex villages. The most spectacular remnant of the Adena culture is the Great Serpent Mound, located in Adams County, Ohio.
Around 100 BC, the Adena were joined in Ohio Country by the Hopewell people, who were named for the farm owned by Captain M. C. Hopewell, where evidence of their unique culture was discovered. Like the Adena, the Hopewell people participated in a mound-building culture. Their complex, large and technologically sophisticated earthworks can be found in modern-day Marietta, Newark, and Circleville. The Hopewell, however, disappeared from the Ohio Valley in about 600 AD. Little is known about the people who replaced them. Researchers have identified two additional, distinct prehistoric cultures: the Fort Ancient people and the Whittlesey Focus people. Both cultures apparently disappeared in the 17th century, perhaps decimated by infectious diseases spread in epidemics from early European contact. The Native Americans had no immunity to common European diseases. Some scholars believe that the Fort Ancient people "were ancestors of the historic Shawnee people, or that, at the very least, the historic Shawnees absorbed remnants of these older peoples."
American Indians in the Ohio Valley were greatly affected by the aggressive tactics of the Iroquois Confederation, based in central and western New York. After the so-called Beaver Wars in the mid-17th century, the Iroquois claimed much of the Ohio country as hunting and, more importantly, beaver-trapping ground. After the devastation of epidemics and war in the mid-17th century, which largely emptied the Ohio country of indigenous people by the mid-to-late 17th century, the land gradually became repopulated by the mostly Algonquian-speaking descendants of its ancient inhabitants, that is, descendants of the Adena, Hopewell, and Mississippian cultures. Many of these Ohio-country nations were multi-ethnic (sometimes multi-linguistic) societies born out of the earlier devastation brought about by disease, war, and subsequent social instability. They subsisted on agriculture (corn, sunflowers, beans, etc.) supplemented by seasonal hunts. By the 18th century, they were part of a larger global economy brought about by European entry into the fur trade.
The indigenous nations to inhabit Ohio in the historical period included the Miamis (a large confederation); Wyandots (made up of refugees, especially from the fractured Huron confederacy); Delawares (pushed west from their historic homeland in New Jersey); Shawnees (also pushed west, although they may have been descended from the Fort Ancient people of Ohio); Ottawas (more commonly associated with the upper Great Lakes region); Mingos (like the Wyandot, a group recently formed of refugees from Iroquois); and Eries (gradually absorbed into the new, multi-ethnic "republics," namely the Wyandot). Ohio country was also the site of Indian massacres, such as the Yellow Creek Massacre, Gnadenhutten and Pontiac's Rebellion school massacre.
Colonial and Revolutionary eras.
During the 18th century, the French set up a system of trading posts to control the fur trade in the region. In 1754, France and Great Britain fought a war that was known in North America as the French and Indian War and in Europe as the Seven Years' War. As a result of the Treaty of Paris, the French ceded control of Ohio and the remainder of the Old Northwest to Great Britain.
Pontiac's Rebellion in the 1760s, however, posed a challenge to British military control. This came to an end with the colonists' victory in the American Revolution. In the Treaty of Paris in 1783, Britain ceded all claims to Ohio country to the United States.
Northwest Territory: 1787–1803.
The United States created the Northwest Territory under the Northwest Ordinance of 1787. Slavery was not permitted in the new territory. Settlement began with the founding of Marietta by the Ohio Company of Associates, which had been formed by a group of American Revolutionary War veterans. Following the Ohio Company, the Miami Company (also referred to as the "Symmes Purchase") claimed the southwestern section, and the Connecticut Land Company surveyed and settled the Connecticut Western Reserve in present-day Northeast Ohio.
The old Northwest Territory originally included areas previously known as Ohio Country and Illinois Country. As Ohio prepared for statehood, the Indiana Territory was created, reducing the Northwest Territory to approximately the size of present-day Ohio plus the eastern half of the Lower Peninsula of Michigan and the eastern tip of the Upper Peninsula.
Under the Northwest Ordinance, areas of the territory could be defined and admitted as states once their population reached 60,000. Although Ohio's population numbered only 45,000 in December 1801, Congress determined that the population was growing rapidly and Ohio could begin the path to statehood. The assumption was that it would exceed 60,000 residents by the time it was admitted as a state. Furthermore, in regards to the Leni Lenape Native Americans living in the region, Congress decided that 10,000 acres on the Muskingum River in the present state of Ohio would "be set apart and the property thereof be vested in the Moravian Brethren . . . or a society of the said Brethren for civilizing the Indians and promoting Christianity."
Statehood: 1803–present.
On February 19, 1803, US President Thomas Jefferson signed an act of Congress that approved Ohio's boundaries and constitution. However, Congress had never passed a resolution formally admitting Ohio as the 17th state. The current custom of Congress declaring an official date of statehood did not begin until 1812, with Louisiana's admission as the 18th state. Although no formal resolution of admission was required, when the oversight was discovered in 1953, Ohio congressman George H. Bender introduced a bill in Congress to admit Ohio to the Union retroactive to March 1, 1803, the date on which the Ohio General Assembly first convened. At a special session at the old state capital in Chillicothe, the Ohio state legislature approved a new petition for statehood that was delivered to Washington, D.C. on horseback. On August 7, 1953 (the year of Ohio's 150th anniversary), President Eisenhower signed a congressional joint resolution that officially declared March 1, 1803 the date of Ohio's admittance into the Union.
Ohio has had three capital cities: Chillicothe, Zanesville, and Columbus. Chillicothe was the capital from 1803 to 1810. The capital was then moved to Zanesville for two years, as part of a state legislative compromise, in order to get a bill passed. The capital was then moved back to Chillicothe, which was the capital from 1812 to 1816. Finally, the capital was moved to Columbus, in order to have it near the geographic center of the state, where it would be more accessible to most citizens.
Although many Native Americans had migrated west to evade American encroachment, others remained settled in the state, sometimes assimilating in part. In 1830 under President Andrew Jackson, the US government forced Indian Removal of most tribes to the Indian Territory west of the Mississippi River.
In 1835, Ohio fought with Michigan in the Toledo War, a mostly bloodless boundary war over the Toledo Strip. Congress intervened, making Michigan's admittance as a state conditional on ending the conflict. In exchange for giving up its claim to the Toledo Strip, Michigan was given the western two-thirds of the Upper Peninsula, in addition to the eastern third that was already considered part of the state.
Ohio's central position and its population gave it an important place during the Civil War. The Ohio River was a vital artery for troop and supply movements, as were Ohio's railroads. Ohio contributed more soldiers per-capita than any other state in the Union. In 1862, the state's morale was badly shaken in the aftermath of the battle of Shiloh, a costly victory in which Ohio forces suffered 2,000 casualties. Later that year, when Confederate troops under the leadership of Stonewall Jackson threatened Washington, D.C., Ohio governor David Tod still could recruit 5,000 volunteers to provide three months of service. Almost 35,000 Ohioans died in the conflict, and thirty thousand were physically wounded. By the end of the Civil War, the Union's top three generals–Ulysses S. Grant, William Tecumseh Sherman, and Philip Sheridan–were all from Ohio.
In 1912 a Constitutional Convention was held with Charles B. Galbreath as secretary. The result reflected the concerns of the Progressive Era. It introduced the initiative and the referendum. In addition, it allowed the General Assembly to put questions on the ballot for the people to ratify laws and constitutional amendments originating in the Legislature. Under the Jeffersonian principle that laws should be reviewed once a generation, the constitution provided for a recurring question to appear on Ohio's general election ballots every 20 years. The question asks whether a new convention is required. Although the question has appeared in 1932, 1952, 1972, and 1992, it has never been approved. Instead constitutional amendments have been proposed by petition to the legislature hundreds of times and adopted in a majority of cases.
Eight US Presidents hailed from Ohio at the time of their elections, giving rise to its nickname "Mother of Presidents", a sobriquet it shares with Virginia. It is also termed "Modern Mother of Presidents," in contrast to Virginia's status as the origin of presidents earlier in American history. Seven Presidents were born in Ohio, making it second to Virginia's eight. Virginia-born William Henry Harrison lived most of his life in Ohio and is also buried there. Harrison conducted his political career while living on the family compound, founded by his father-in-law, John Cleves Symmes, in North Bend, Ohio. The seven presidents born in Ohio were Ulysses S. Grant, Rutherford B. Hayes, James A. Garfield, Benjamin Harrison (grandson of William Henry Harrison), William McKinley, William Howard Taft and Warren G. Harding.
Demographics.
Population.
From just over 45,000 residents in 1800, Ohio's population grew at rates of over 10% per decade until the 1970 census, which recorded just over 10.65 million Ohioans. Growth then slowed for the next four decades. The United States Census Bureau estimates that the population of Ohio was 11,613,423 on July 1, 2015, a 0.67% increase since the 2010 United States Census. Ohio's population growth lags that of the entire United States, and Caucasians are found in a greater density than the United States average. , Ohio's center of population is located in Morrow County, in the county seat of Mount Gilead. This is approximately south and west of Ohio's population center in 1990.
As of 2011, 27.6% of Ohio's children under the age of 1 belonged to minority groups.
6.2% of Ohio's population is under 5 years of age, 23.7 percent under 18 years of age, and 14.1 percent were 65 or older. Females made up approximately 51.2 percent of the population.
Racial and ancestry groups.
According to the 2010 United States census, the racial composition of Ohio was the following:
In 2010, there were 469,700 foreign-born residents in Ohio, corresponding to 4.1% of the total population. Of these, 229,049 (2.0%) were naturalized US citizens and 240,699 (2.1%) were not. The largest groups were: Mexico (54,166), India (50,256), China (34,901), Germany (19,219), Philippines (16,410), United Kingdom (15,917), Canada (14,223), Russia (11,763), South Korea (11,307), and Ukraine (10,681). Though, predominantly white, Ohio has large black populations in all major metropolitan areas throughout the state, Ohio has a significant Hispanic population made up of Mexicans in Toledo and Columbus, and Puerto Ricans in Cleveland and Columbus, and also has a significant and diverse Asian population in Columbus.
The largest ancestry groups (which the Census defines as not including racial terms) in the state are:
Ancestries claimed by less than 1% of the population include Sub-Saharan African, Puerto Rican, Swiss, Swedish, Arab, Greek, Norwegian, Romanian, Austrian, Lithuanian, Finnish, West Indian, and Portuguese.
Languages.
About 6.7% of the population age 5 years and over reported speaking a language other than English, with 2.2% of the population speaking Spanish, 2.6% speaking other Indo-European languages, 1.1% speaking Asian and Austronesian languages, and 0.8% speaking other languages. Numerically: 10,100,586 spoke English, 239,229 Spanish, 55,970 German, 38,990 Chinese, 33,125 Arabic, and 32,019 French. In addition 59,881 spoke a Slavic language and 42,673 spoke another West Germanic language according to the 2010 Census. Ohio also had the nation's largest population of Slovene speakers, second largest of Slovak speakers, second largest of Pennsylvania Dutch (German) speakers, and the third largest of Serbian speakers.
Religion.
According to a Pew Forum poll, as of 2008, 76% of Ohioans identified as Christian. Specifically, 26% of Ohio's population identified as Evangelical Protestant, 22% as Mainline Protestant, and 21% as Roman Catholic. 17% of the population is unaffiliated with any religious body. 1.3% (148,380) were Jewish. There are also small minorities of Jehovah's Witnesses (1%), Muslims (1%), Hindus (<0.5%), Buddhists (<0.5%), Mormons (<0.5%), and other faiths (1-1.5%).
According to the Association of Religion Data Archives (ARDA), in 2010 the largest denominations by adherents were the Roman Catholic Church with 1,992,567; the United Methodist Church with 496,232; the Evangelical Lutheran Church in America with 223,253, the Southern Baptist Convention with 171,000, the Christian Churches and Churches of Christ with 141,311, the United Church of Christ with 118,000, and the Presbyterian Church (USA) with 110,000. With about 70,000 people in 2015 Ohio had the largest Amish population of all states of the US.
According to the same data, a majority of Ohioans, 55%, feel that religion is "very important," 30% say that it is "somewhat important," and 15% responded that religion is "not too important/not important at all." 36% of Ohioans indicate that they attend religious services at least once weekly, 35% attend occasionally, and 27% seldom or never participate in religious services.
Economy.
In 2010, Ohio was ranked No. 2 in the country for best business climate by Site Selection magazine, based on a business-activity database. The state has also won three consecutive Governor's Cup awards from the magazine, based on business growth and developments. , Ohio's gross domestic product (GDP) was $478 billion. This ranks Ohio's economy as the seventh-largest of all fifty states and the District of Columbia.
The Small Business & Entrepreneurship Council ranked the state No. 10 for best business-friendly tax systems in their Business Tax Index 2009, including a top corporate tax and capital gains rate that were both ranked No. 6 at 1.9%. Ohio was ranked No. 11 by the council for best friendly-policy states according to their Small Business Survival Index 2009. The Directorship's Boardroom Guide ranked the state No. 13 overall for best business climate, including No. 7 for best litigation climate. Forbes ranked the state No. 8 for best regulatory environment in 2009. Ohio has 5 of the top 115 colleges in the nation, according to U.S. News and World Report's 2010 rankings, and was ranked No. 8 by the same magazine in 2008 for best high schools.
Ohio's unemployment rate stands at 5.2% as of April 2015, down from 10.7% in May 2010. The state still lacks 45,000 jobs compared to the prerecession numbers of 2007. The labor force participation as of April 2015 is 63%, slightly above the national average. Ohio's per capita income stands at $34,874. , Ohio's median household income is $46,645, and 13.1% of the population is below the poverty line, slightly above the national rate of 13%.
The manufacturing and financial activities sectors each compose 18.3% of Ohio's GDP, making them Ohio's largest industries by percentage of GDP. Ohio has the largest bioscience sector in the Midwest, and is a national leader in the "green" economy. Ohio is the largest producer in the country of plastics, rubber, fabricated metals, electrical equipment, and appliances. 5,212,000 Ohioans are currently employed by wage or salary.
By employment, Ohio's largest sector is trade/transportation/utilities, which employs 1,010,000 Ohioans, or 19.4% of Ohio's workforce, while the health care and education sector employs 825,000 Ohioans (15.8%). Government employs 787,000 Ohioans (15.1%), manufacturing employs 669,000 Ohioans (12.9%), and professional and technical services employs 638,000 Ohioans (12.2%). Ohio's manufacturing sector is the third-largest of all fifty United States states in terms of gross domestic product. Fifty-nine of the United States' top 1,000 publicly traded companies (by revenue in 2008) are headquartered in Ohio, including Procter & Gamble, Goodyear Tire & Rubber, AK Steel, Timken, Abercrombie & Fitch, and Wendy's.
Ohio is also one of 41 states with its own lottery, the Ohio Lottery. The Ohio Lottery has contributed over $15.5 billion to public education in its 34-year history.
Transportation.
Ground travel.
Many major east-west transportation corridors go through Ohio. One of those pioneer routes, known in the early 20th century as "Main Market Route 3", was chosen in 1913 to become part of the historic Lincoln Highway which was the first road across America, connecting New York City to San Francisco. In Ohio, the Lincoln Highway linked many towns and cities together, including Canton, Mansfield, Wooster, Lima, and Van Wert. The arrival of the Lincoln Highway to Ohio was a major influence on the development of the state. Upon the advent of the federal numbered highway system in 1926, the Lincoln Highway through Ohio became U.S. Route 30.
Ohio also is home to of the Historic National Road, now U.S. Route 40.
Ohio has a highly developed network of roads and interstate highways. Major east-west through routes include the Ohio Turnpike (I-80/I-90) in the north, I-76 through Akron to Pennsylvania, I-70 through Columbus and Dayton, and the Appalachian Highway (State Route 32) running from West Virginia to Cincinnati. Major north-south routes include I-75 in the west through Toledo, Dayton, and Cincinnati, I-71 through the middle of the state from Cleveland through Columbus and Cincinnati into Kentucky, and I-77 in the eastern part of the state from Cleveland through Akron, Canton, New Philadelphia and Marietta down into West Virginia. Interstate 75 between Cincinnati and Dayton is one of the heaviest traveled sections of interstate in Ohio.
Ohio also has a highly developed network of signed state bicycle routes. Many of them follow rail trails, with conversion ongoing. The Ohio to Erie Trail (route 1) connects Cincinnati, Columbus, and Cleveland. U.S. Bicycle Route 50 traverses Ohio from Steubenville to the Indiana state line outside Richmond.
Air travel.
Ohio has 5 international airports, 4 commercial and 2 military. The 5 international includes Cleveland Hopkins International Airport, Port Columbus International Airport, and Dayton International Airport, Ohio's third largest airport. Akron Fulton International Airport handles cargo and for private use. Rickenbacker International Airport is one of two military airfields which is also home to the 7th largest FedEx building in America. The other military airfield is Wright Patterson Air Force Base which is one of the largest Air Force bases in the United States. Other major airports are located in Toledo and Akron.
Cincinnati/Northern Kentucky International Airport is in Hebron, Kentucky and therefore is not listed above.
Law and government.
The state government of Ohio consists of the executive, judicial, and legislative branches.
Executive branch.
The executive branch is headed by the Governor of Ohio. The current governor is John Kasich, a Republican elected in 2010. A lieutenant governor succeeds the governor in the event of any removal from office, and performs any duties assigned by the governor. The current lieutenant governor is Mary Taylor. The other elected constitutional offices in the executive branch are the secretary of state (Jon A. Husted), auditor (Dave Yost), treasurer (Josh Mandel), and attorney general (Mike DeWine).
Judicial branch.
There are three levels of the Ohio state judiciary. The lowest level is the court of common pleas: each county maintains its own constitutionally mandated court of common pleas, which maintain jurisdiction over "all justiciable matters." The intermediate-level court system is the district court system. Twelve courts of appeals exist, each retaining jurisdiction over appeals from common pleas, municipal, and county courts in a set geographical area. A case heard in this system is decided by a three-judge panel, and each judge is elected.
The highest-ranking court, the Ohio Supreme Court, is Ohio's "court of last resort." A seven-justice panel composes the court, which, by its own discretion, hears appeals from the courts of appeals, and retains original jurisdiction over limited matters.
Legislative branch.
The Ohio General Assembly is a bicameral legislature consisting of the Senate and House of Representatives. The Senate is composed of 33 districts, each of which is represented by one senator. Each senator represents approximately 330,000 constituents. The House of Representatives is composed of 99 members.
National politics.
Ohio, nicknamed the "Mother of Presidents," has sent seven of its native sons (Ulysses S. Grant, Rutherford B. Hayes, James A. Garfield, Benjamin Harrison, William McKinley, William Howard Taft, and Warren G. Harding) to the White House. All seven were Republicans. Virginia native William Henry Harrison, a Whig, resided in Ohio. Historian R. Douglas Hurt asserts that not since Virginia 'had a state made such a mark on national political affairs.' "The Economist" notes that "This slice of the mid-west contains a bit of everything American — part north-eastern and part southern, part urban and part rural, part hardscrabble poverty and part booming
suburb," Ohio is the only state that has voted for the winning Presidential candidate in each election since 1964, and in 33 of the 37 held since the Civil War. No Republican has ever won the presidency without winning Ohio.
, Ohio's voter demographic leans towards the Democratic Party. An estimated 2,408,178 Ohioans are registered to vote as Democrats, while 1,471,465 Ohioans are registered to vote as Republicans. These are changes from 2004 of 72% and 32%, respectively, and Democrats have registered over 1,000,000 new Ohioans since 2004. Unaffiliated voters have an attrition of 15% since 2004, losing an estimated 718,000 of their kind. The total now rests at 4,057,518 Ohioans. In total, there are 7,937,161 Ohioans registered to vote. In the United States presidential election of 2008, then-Senator Barack Obama of Illinois won 51.50% of Ohio's popular vote, 4.59 percentage points more than his nearest rival, Senator John McCain of Arizona (with 46.91% of the popular vote). However, Obama won only 22 of Ohio's 88 counties. Since 2010, the Republicans have largely controlled Ohio state politics, including a super-majority in the state's House, a majority in the state Senate, the Governorship, etc. As of 2014, the state Senate is 1 Republican away from a super-majority.
Following the 2000 census, Ohio lost one congressional district in the United States House of Representatives, which leaves Ohio with 18 districts, and consequently, 18 representatives. The state lost two more seats following the 2010 Census, leaving it with 18 votes for the next 3 presidential elections in 2012, 2016 and 2020. The 2008 elections, Democrats gained three seats in Ohio's delegation to the House of Representatives. This leaves eight Republican-controlled seats in the Ohio delegation. Ohio's U.S. Senators in the 112th Congress are Republican Rob Portman and Democrat Sherrod Brown. Marcy Kaptur (D-9) is the dean, or most senior member, of the Ohio delegation to the United States House of Representatives.
Education.
Ohio's system of public education is outlined in Article VI of the state constitution, and in Title XXXIII of the Ohio Revised Code. Ohio University, the first university in the Northwest Territory, was also the first public institution in Ohio. Substantively, Ohio's system is similar to those found in other states. At the State level, the Ohio Department of Education, which is overseen by the Ohio State Board of Education, governs primary and secondary educational institutions. At the municipal level, there are approximately 700 school districts statewide. The Ohio Board of Regents coordinates and assists with Ohio's institutions of higher education which have recently been reorganized into the University System of Ohio under Governor Strickland. The system averages an annual enrollment of over 400,000 students, making it one of the five largest state university systems in the U.S.
Libraries.
Ohio is home to some of the nation's highest-ranked public libraries. The 2008 study by Thomas J. Hennen Jr. ranked Ohio as number one in a state-by-state comparison. For 2008, 31 of Ohio's library systems were all ranked in the top ten for American cities of their population category.
The Ohio Public Library Information Network (OPLIN) is an organization that provides Ohio residents with internet access to their 251 public libraries. OPLIN also provides Ohioans with free home access to high-quality, subscription research databases.
Ohio also offers the OhioLINK program, allowing Ohio's libraries (particularly those from colleges and universities) access to materials for the other libraries. The program is largely successful in allowing researchers for access to books and other media that might not be otherwise available.
Sports.
Professional sports leagues.
Ohio is home to major professional sports teams in baseball, basketball, football, hockey, lacrosse and soccer. The state's major professional sporting teams include: Cincinnati Reds (Major League Baseball), Ohio Machine (Major League Lacrosse), Cleveland Indians (Major League Baseball), Cincinnati Bengals (National Football League), Cleveland Browns (National Football League), Cleveland Cavaliers (National Basketball Association), Columbus Blue Jackets (National Hockey League), and the Columbus Crew (Major League Soccer).
Ohio played a central role in the development of both Major League Baseball and the National Football League. Baseball's first fully professional team, the Cincinnati Red Stockings of 1869, were organized in Ohio. An informal early 20th century American football association, the Ohio League, was the direct predecessor of the NFL, although neither of Ohio's modern NFL franchises trace their roots to an Ohio League club. The Pro Football Hall of Fame is located in Canton.
On a smaller scale, Ohio hosts minor league baseball, arena football, indoor football, mid-level hockey, and lower division soccer.
Individual sports.
The Mid-Ohio Sports Car Course has hosted several auto racing championships, including CART World Series, IndyCar Series, NASCAR Nationwide Series, Can-Am, Formula 5000, IMSA GT Championship, American Le Mans Series and Rolex Sports Car Series.
The Grand Prix of Cleveland also hosted CART races from 1982 to 2007. The Eldora Speedway is a major dirt oval that hosts NASCAR Camping World Truck Series, World of Outlaws Sprint Cars and USAC Silver Crown Series races.
Ohio hosts two PGA Tour events, the WGC-Bridgestone Invitational and Memorial Tournament.
The Cincinnati Masters is an ATP World Tour Masters 1000 and WTA Premier 5 tennis tournament.
College football (NCAA DI-A).
Ohio has eight NCAA Division I-A college football teams, divided among three different conferences. It has also experienced considerable success in the secondary and tertiary tiers of college football divisions.
In Division I-A, representing the Big Ten, the Ohio State Buckeyes football team ranks 5th among all-time winningest programs, with seven national championships and seven Heisman Trophy winners. Their biggest rivals are the Michigan Wolverines, whom they traditionally play each year as the last game of their regular season schedule.
Ohio has six teams represented in the Mid-American Conference: the University of Akron, Bowling Green, Kent State, Miami University, Ohio University and the University of Toledo. The MAC headquarters are based in Cleveland.
The University of Cincinnati Bearcats represent Ohio in the American Athletic Conference.
State symbols.
Ohio's state symbols:

</doc>
<doc id="22201" url="https://en.wikipedia.org/wiki?curid=22201" title="Orbital">
Orbital

Orbital may refer to:
In chemistry and physics:
In astronomy and space flight:
In entertainment:
In other fields:

</doc>
<doc id="22203" url="https://en.wikipedia.org/wiki?curid=22203" title="Organic compound">
Organic compound

An organic compound is any member of a large class of gaseous, liquid, or solid chemical compounds whose molecules contain carbon. For historical reasons discussed below, a few types of carbon-containing compounds, such as carbides, carbonates, simple oxides of carbon (such as CO and CO2), and cyanides are considered inorganic. The distinction between "organic" and "inorganic" carbon compounds, while "useful in organizing the vast subject of chemistry... is somewhat arbitrary".
Organic chemistry is the science concerned with all aspects of organic compounds. Organic synthesis is the methodology of their preparation.
History.
Vitalism.
The word "organic" is historical, dating to the 1st century. For many centuries, Western alchemists believed in vitalism. This is the theory that certain compounds could be synthesized only from their classical elements—earth, water, air, and fire—by the action of a "life-force" ("vis vitalis") that only organisms possessed. Vitalism taught that these "organic" compounds were fundamentally different from the "inorganic" compounds that could be obtained from the elements by chemical manipulation.
Vitalism survived for a while even after the rise of modern atomic theory and the replacement of the Aristotelian elements by those we know today. It first came under question in 1824, when Friedrich Wöhler synthesized oxalic acid, a compound known to occur only in living organisms, from cyanogen. A more decisive experiment was Wöhler's 1828 synthesis of urea from the inorganic salts potassium cyanate and ammonium sulfate. Urea had long been considered an "organic" compound, as it was known to occur only in the urine of living organisms. Wöhler's experiments were followed by many others, where increasingly complex "organic" substances were produced from "inorganic" ones without the involvement of any living organism.
Modern classification.
Even though vitalism has been discredited, scientific nomenclature retains the distinction between "organic" and "inorganic" compounds. The modern meaning of "organic compound" is any compound that contains a significant amount of carbon—even though many of the organic compounds known today have no connection to any substance found in living organisms.
There is no single "official" definition of an organic compound. Some textbooks define an organic compound as one that contains one or more C-H bonds. Others include C-C bonds in the definition. Others state that if a molecule contains carbon―it is organic.
Even the broader definition of "carbon-containing molecules" requires the exclusion of carbon-containing alloys (including steel), a relatively small number of carbon-containing compounds, such as metal carbonates and carbonyls, simple oxides of carbon and cyanides, as well as the allotropes of carbon and simple carbon halides and sulfides, which are usually considered inorganic.
The "C-H" definition excludes compounds that are historically and practically considered organic. Neither urea nor oxalic acid is organic by this definition, yet they were two key compounds in the vitalism debate. The IUPAC Blue Book on organic nomenclature specifically mentions urea and oxalic acid. Other compounds lacking C-H bonds that are also traditionally considered organic include benzenehexol, mesoxalic acid, and carbon tetrachloride. Mellitic acid, which contains no C-H bonds, is considered a possible organic substance in Martian soil. C-C bonds are found in most organic compounds, except some small molecules like methane and methanol, which have only one carbon atom in their structure.
The "C-H bond-only" rule also leads to somewhat arbitrary divisions in sets of carbon-fluorine compounds, as, for example, Teflon is considered by this rule to be "inorganic", whereas Tefzel is considered to be organic. Likewise, many Halons are considered inorganic, whereas the rest are considered organic. For these and other reasons, most sources believe that C-H compounds are only a subset of "organic" compounds.
In summary, most carbon-containing compounds are organic, and almost all organic compounds contain at least a C-H bond or a C-C bond. A compound does not need to contain C-H bonds to be considered organic (e.g., urea), but most organic compounds do.
Classification.
Organic compounds may be classified in a variety of ways. One major distinction is between natural and synthetic compounds. Organic compounds can also be classified or subdivided by the presence of heteroatoms, e.g., organometallic compounds, which feature bonds between carbon and a metal, and organophosphorus compounds, which feature bonds between carbon and a phosphorus.
Another distinction, based on the size of organic compounds, distinguishes between small molecules and polymers.
Natural compounds.
Natural compounds refer to those that are produced by plants or animals. Many of these are still extracted from natural sources because they would be more expensive to produce artificially. Examples include most sugars, some alkaloids and terpenoids, certain nutrients such as vitamin B12, and, in general, those natural products with large or stereoisometrically complicated molecules present in reasonable concentrations in living organisms.
Further compounds of prime importance in biochemistry are antigens, carbohydrates, enzymes, hormones, lipids and fatty acids, neurotransmitters, nucleic acids, proteins, peptides and amino acids, lectins, vitamins, and fats and oils.
Synthetic compounds.
Compounds that are prepared by reaction of other compounds are known as "synthetic". They may be either compounds that already are found in plants or animals or those that do not occur naturally.
Most polymers (a category that includes all plastics and rubbers), are organic synthetic or semi-synthetic compounds.
Biotechnology.
Several compounds are industrially manufactured utilizing the biochemistry of organisms such as bacteria and yeast. Two examples are ethanol and insulin. Regularly, the DNA of the organism is altered to express desired compounds that are often not ordinarily produced by that organism. Sometimes the biotechnologically engineered compounds were never present in nature in the first place.
Nomenclature.
The IUPAC nomenclature of organic compounds slightly differs from the CAS nomenclature.
Databases.
There is a great number of more specialized databases for diverse branches of organic chemistry.
Structure determination.
Today, the main tools are proton and carbon-13 NMR spectroscopy, IR Spectroscopy, Mass spectrometry, UV/Vis Spectroscopy and X-ray crystallography.

</doc>
<doc id="22204" url="https://en.wikipedia.org/wiki?curid=22204" title="Oligopoly">
Oligopoly

An oligopoly () is a market form in which a market or industry is dominated by a small number of sellers (oligopolists). Oligopolies can result from various forms of collusion which reduce competition and lead to higher prices for consumers. Oligopoly has its own market structure.
With few sellers, each oligopolist is likely to be aware of the actions of the others. According to game theory, the decisions of one firm therefore influence and are influenced by decisions of other firms. Strategic planning by oligopolists needs to take into account the likely responses of the other market participants.
Description.
Oligopoly is a common market form where a number of firms are in competition. As a quantitative description of oligopoly, the four-firm concentration ratio is often utilized. This measure expresses the market share of the four largest firms in an industry as a percentage. For example, as of fourth quarter 2008, Verizon, AT&T, Sprint, and T-Mobile together control 97% of the US cellular phone market.
Oligopolistic competition can give rise to a wide range of different outcomes. In some situations, the firms may employ restrictive trade practices (collusion, market sharing etc.) to raise prices and restrict production in much the same way as a monopoly. Where there is a formal agreement for such collusion, this is known as a cartel. A primary example of such a cartel is OPEC which has a profound influence on the international price of oil.
Firms often collude in an attempt to stabilize unstable markets, so as to reduce the risks inherent in these markets for investment and product development. There are legal restrictions on such collusion in most countries. There does not have to be a formal agreement for collusion to take place (although for the act to be illegal there must be actual communication between companies)–for example, in some industries there may be an acknowledged market leader which informally sets prices to which other producers respond, known as price leadership.
In other situations, competition between sellers in an oligopoly can be fierce, with relatively low prices and high production. This could lead to an efficient outcome approaching perfect competition. The competition in an oligopoly can be greater when there are more firms in an industry than if, for example, the firms were only regionally based and did not compete directly with each other.
Oligopoly theory makes heavy use of game theory to model the behavior of oligopolies:
Modeling.
There is no single model describing the operation of an oligopolistic market. The variety and complexity of the models exist because you can have two to 10 firms competing on the basis of price, quantity, technological innovations, marketing, and reputation. Fortunately, there are a series of simplified models that attempt to describe market behavior by considering certain circumstances. Some of the better-known models are the dominant firm model, the Cournot–Nash model, the Bertrand model and the kinked demand model.
Cournot–Nash model.
The Cournot–Nash model is the simplest oligopoly model. The model assumes that there are two “equally positioned firms”; the firms compete on the basis of quantity rather than price and each firm makes an “output decision assuming that the other firm’s behavior is fixed.” The market demand curve is assumed to be linear and marginal costs are constant. To find the Cournot–Nash equilibrium one determines how each firm reacts to a change in the output of the other firm. The path to equilibrium is a series of actions and reactions. The pattern continues until a point is reached where neither firm desires “to change what it is doing, given how it believes the other firm will react to any change.” The equilibrium is the intersection of the two firm’s reaction functions. The reaction function shows how one firm reacts to the quantity choice of the other firm. For example, assume that the firm 1’s demand function is "P" = ("M" − "Q"2) − "Q"1 where "Q"2 is the quantity produced by the other firm and Q1 is the amount produced by firm 1, and M=60 is the market. Assume that marginal cost is CM=12. Firm 1 wants to know its maximizing quantity and price. Firm 1 begins the process by following the profit maximization rule of equating marginal revenue to marginal costs. Firm 1’s total revenue function is "R""T" = "Q"1 "P" = "Q"1("M" − "Q"2 − "Q"1) = "MQ"1 − "Q"1 "Q"2 − "Q"12. The marginal revenue function is formula_1.
Equation 1.1 is the reaction function for firm 1. Equation 1.2 is the reaction function for firm 2.
To determine the Cournot–Nash equilibrium you can solve the equations simultaneously. The equilibrium quantities can also be determined graphically. The equilibrium solution would be at the intersection of the two reaction functions. Note that if you graph the functions the axes represent quantities. The reaction functions are not necessarily symmetric. The firms may face differing cost functions in which case the reaction functions would not be identical nor would the equilibrium quantities.
Bertrand model.
The Bertrand model is essentially the Cournot–Nash model except the strategic variable is price rather than quantity.
The model assumptions are:
The only Nash equilibrium is PA = PB = MC.
Neither firm has any reason to change strategy. If the firm raises prices it will lose all its customers. If the firm lowers price P < MC then it will be losing money on every unit sold.
The Bertrand equilibrium is the same as the competitive result. Each firm will produce where P = marginal costs and there will be zero profits. A generalization of the Bertrand model is the Bertrand–Edgeworth model that allows for capacity constraints and more general cost functions.
Kinked demand curve model.
According to this model, each firm faces a demand curve kinked at the existing price. The conjectural assumptions of the model are; if the firm raises its price above the current existing price, competitors will not follow and the acting firm will lose market share and second if a firm lowers prices below the existing price then their competitors will follow to retain their market share and the firm's output will increase only marginally.
If the assumptions hold then: 
The gap in the marginal revenue curve means that marginal costs can fluctuate without changing equilibrium price and quantity. Thus prices tend to be rigid.
Examples.
In industrialized economies, barriers to entry have resulted in oligopolies forming in many sectors, with unprecedented levels of competition fueled by increasing globalization. Market shares in an oligopoly are typically determined by product development and advertising. For example, there are now only a small number of manufacturers of civil passenger aircraft, though Brazil (Embraer) and Canada (Bombardier) have participated in the small passenger aircraft market sector. Oligopolies have also arisen in heavily-regulated markets such as wireless communications: in some areas only two or three providers are licensed to operate.
Demand curve.
In an oligopoly, firms operate under imperfect competition. With the fierce price competitiveness created by this sticky-upward demand curve, firms use non-price competition in order to accrue greater revenue and market share.
"Kinked" demand curves are similar to traditional demand curves, as they are downward-sloping. They are distinguished by a hypothesized convex bend with a discontinuity at the bend–"kink". Thus the first derivative at that point is undefined and leads to a jump discontinuity in the marginal revenue curve.
Classical economic theory assumes that a profit-maximizing producer with some market power (either due to oligopoly or monopolistic competition) will set marginal costs equal to marginal revenue. This idea can be envisioned graphically by the intersection of an upward-sloping marginal cost curve and a downward-sloping marginal revenue curve (because the more one sells, the lower the price must be, so the less a producer earns per unit). In classical theory, any change in the marginal cost structure (how much it costs to make each additional unit) or the marginal revenue structure (how much people will pay for each additional unit) will be immediately reflected in a new price and/or quantity sold of the item. This result does not occur if a "kink" exists. Because of this jump discontinuity in the marginal revenue curve, marginal costs could change without necessarily changing the price or quantity.
The motivation behind this kink is the idea that in an oligopolistic or monopolistically competitive market, firms will not raise their prices because even a small price increase will lose many customers. This is because competitors will generally ignore price increases, with the hope of gaining a larger market share as a result of now having comparatively lower prices. However, even a large price decrease will gain only a few customers because such an action will begin a price war with other firms. The curve is therefore more price-elastic for price increases and less so for price decreases. Theory predicts that firms will enter the industry in the long run.

</doc>
<doc id="22205" url="https://en.wikipedia.org/wiki?curid=22205" title="Oasis">
Oasis

In geography, an oasis (plural: oases) or cienega (Southwestern United States) is an isolated area of vegetation in a desert, typically surrounding a spring or similar water source, such as a pond or small lake. Oases also provide habitat for animals and even humans if the area is big enough. The location of oases has been of critical importance for trade and transportation routes in desert areas; caravans must travel via oases so that supplies of water and food can be replenished. Thus, political or military control of an oasis has in many cases meant control of trade on a particular route. For example, the oases of Awjila, Ghadames, and Kufra, situated in modern-day Libya, have at various times been vital to both North-South and East-West trade in the Sahara Desert.
Oases are formed from underground rivers or aquifers such as an artesian aquifer, where water can reach the surface naturally by pressure or by man-made wells. Occasional brief thunderstorms provide subterranean water to sustain natural oases, such as the Tuat. Substrata of impermeable rock and stone can trap water and retain it in pockets, or on long faulting subsurface ridges or volcanic dikes water can collect and percolate to the surface. Any incidence of water is then used by migrating birds, which also pass seeds with their droppings which will grow at the water's edge forming an oasis.
Etymology.
The word "oasis" came into English via from , which in turn is a direct borrowing from Demotic Egyptian. The word for "oasis" in the later attested Coptic language (the descendant of Demotic Egyptian) is "wahe" or "ouahe" which means a "dwelling place".
Growing plants.
People who live in an oasis must manage land and water use carefully; fields must be irrigated to grow plants like apricots, dates, figs, and olives. The most important plant in an oasis is the date palm, which forms the upper layer. These palm trees provide shade for smaller trees like peach trees, which form the middle layer. By growing plants in different layers, the farmers make best use of the soil and water. Many vegetables are also grown and some cereals, such as barley, millet, and wheat, are grown where there is more moisture.

</doc>
