<doc id="25312" url="https://en.wikipedia.org/wiki?curid=25312" title="Quantum gravity">
Quantum gravity

Quantum gravity (QG) is a field of theoretical physics that seeks to describe the force of gravity according to the principles of quantum mechanics, and where quantum effects cannot be ignored.
The current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. On the other hand, the nongravitational forces are described within the framework of quantum mechanics, a radically different formalism for describing physical phenomena based on the wave-like nature of matter. The necessity of a quantum mechanical description of gravity follows from the fact that one cannot consistently couple a classical system to a quantum one.
Although a quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, difficulties arise when one attempts to apply the usual prescriptions of quantum field theory to the force of gravity. From a technical point of view, the problem is that the theory one gets in this way is not renormalizable and therefore cannot be used to make meaningful physical predictions. As a result, theorists have taken up more radical approaches to the problem of quantum gravity, the most popular approaches being string theory and loop quantum gravity. A recent development is the theory of causal fermion systems which gives quantum mechanics, general relativity, and quantum field theory as limiting cases.
Strictly speaking, the aim of quantum gravity is only to describe the quantum behavior of the gravitational field and should not be confused with the objective of unifying all fundamental interactions into a single mathematical framework. While any substantial improvement into the present understanding of gravity would aid further work towards unification, study of quantum gravity is a field in its own right with various branches having different approaches to unification. Although some quantum gravity theories, such as string theory, try to unify gravity with the other fundamental forces, others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces. A theory of quantum gravity that is also a grand unification of all known interactions is sometimes referred to as a theory of everything (TOE).
One of the difficulties of quantum gravity is that quantum gravitational effects are only expected to become apparent near the Planck scale, a scale far smaller in distance (equivalently, far larger in energy) than what is currently accessible at high energy particle accelerators. As a result, quantum gravity is a mainly theoretical enterprise, although there are speculations about how quantum gravity effects might be observed in existing experiments.
Overview.
Much of the difficulty in meshing these theories at all energy scales comes from the different assumptions that these theories make on how the universe works. Quantum field theory depends on particle fields embedded in the flat space-time of special relativity. General relativity models gravity as a curvature within space-time that changes as a gravitational mass moves. Historically, the most obvious way of combining the two (such as treating gravity as simply another particle field) ran quickly into what is known as the renormalization problem. In the old-fashioned understanding of renormalization, gravity particles would attract each other and adding together all of the interactions results in many infinite values which cannot easily be cancelled out mathematically to yield sensible, finite results. This is in contrast with quantum electrodynamics where, given that the series still do not converge, the interactions sometimes evaluate to infinite results, but those are few enough in number to be removable via renormalization.
Effective field theories.
Quantum gravity can be treated as an effective field theory. Effective quantum field theories come with some high-energy cutoff, beyond which we do not expect that the theory provides a good description of nature. The "infinities" then become large but finite quantities depending on this finite cutoff scale, and correspond to processes that involve very high energies near the fundamental cutoff. These quantities can then be absorbed into an infinite collection of coupling constants, and at energies well below the fundamental cutoff of the theory, to any desired precision; only a finite number of these coupling constants need to be measured in order to make legitimate quantum-mechanical predictions. This same logic works just as well for the highly successful theory of low-energy pions as for quantum gravity. Indeed, the first quantum-mechanical corrections to graviton-scattering and Newton's law of gravitation have been explicitly computed (although they are so infinitesimally small that we may never be able to measure them). In fact, gravity is in many ways a much better quantum field theory than the Standard Model, since it appears to be valid all the way up to its cutoff at the Planck scale.
While confirming that quantum mechanics and gravity are indeed consistent at reasonable energies, it is clear that near or above the fundamental cutoff of our effective quantum theory of gravity (the cutoff is generally assumed to be of the order of the Planck scale), a new model of nature will be needed. Specifically, the problem of combining quantum mechanics and gravity becomes an issue only at very high energies, and may well require a totally new kind of model.
Quantum gravity theory for the highest energy scales.
The general approach to deriving a quantum gravity theory that is valid at even the highest energy scales is to assume that such a theory will be simple and elegant and, accordingly, to study symmetries and other clues offered by current theories that might suggest ways to combine them into a comprehensive, unified theory. One problem with this approach is that it is unknown whether quantum gravity will actually conform to a simple and elegant theory, as it should resolve the dual conundrums of special relativity with regard to the uniformity of acceleration and gravity, and general relativity with regard to spacetime curvature.
Such a theory is required in order to understand problems involving the combination of very high energy and very small dimensions of space, such as the behavior of black holes, and the origin of the universe.
Quantum mechanics and general relativity.
The graviton.
At present, one of the deepest problems in theoretical physics is harmonizing the theory of general relativity, which describes gravitation, and applications to large-scale structures (stars, planets, galaxies), with quantum mechanics, which describes the other three fundamental forces acting on the atomic scale. This problem must be put in the proper context, however. In particular, contrary to the popular claim that quantum mechanics and general relativity are fundamentally incompatible, one can demonstrate that the structure of general relativity essentially follows inevitably from the quantum mechanics of interacting theoretical spin-2 massless particles 
(called gravitons).
While there is no concrete proof of the existence of gravitons, quantized theories of matter may necessitate their existence. Supporting this theory is the observation that all fundamental forces except gravity have one or more known messenger particles, leading researchers to believe that at least one most likely does exist; they have dubbed this hypothetical particle the "graviton". The predicted find would result in the classification of the graviton as a "force particle" similar to the photon of the electromagnetic field. Many of the accepted notions of a unified theory of physics since the 1970s assume, and to some degree depend upon, the existence of the graviton. These include string theory, superstring theory, M-theory, and loop quantum gravity. Detection of gravitons is thus vital to the validation of various lines of research to unify quantum mechanics and relativity theory. 
The dilaton.
The dilaton made its first appearance in Kaluza–Klein theory, a five-dimensional theory that combined gravitation and electromagnetism. Generally, it appears in string theory. More recently, however, it's become central to the lower-dimensional many-bodied gravity problem based on the field theoretic approach of Roman Jackiw. The impetus arose from the fact that complete analytical solutions for the metric of a covariant "N"-body system have proven elusive in general relativity. To simplify the problem, the number of dimensions was lowered to "(1+1)", i.e., one spatial dimension and one temporal dimension. This model problem, known as "R=T" theory (as opposed to the general "G=T" theory) was amenable to exact solutions in terms of a generalization of the Lambert W function. It was also found that the field equation governing the dilaton (derived from differential geometry) was the Schrödinger equation and consequently amenable to quantization.
Thus, one had a theory which combined gravity, quantization, and even the electromagnetic interaction, promising ingredients of a fundamental physical theory. It is worth noting that this outcome revealed a previously unknown and already existing "natural link" between general relativity and quantum mechanics. For some time, a generalization of this theory to "(3+1)" dimensions was unclear. However, a recent derivation in "(3+1)" dimensions under the right coordinate conditions yields a formulation similar to the earlier "(1+1)" namely a dilaton field governed by the logarithmic Schrödinger equation which is seen in condensed matter physics and superfluids. The field equations are indeed amenable to such a generalization (as shown with the inclusion of a one-graviton process) and yield the correct Newtonian limit in "d" dimensions but only if a dilaton is included. Furthermore, the results become even more tantalizing in view of the apparent resemblance between the dilaton and the Higgs boson. However, more experimentation is needed to resolve the relationhip between these two particles.
Since this theory can combine gravitational, electromagnetic and quantum effects, their coupling could potentially lead to a means of vindicating the theory, through cosmology and even, perhaps, experimentally. When the equation E=mC^2 was solved through the Lorentz derivative, and applied to the velocity of the electron in relationship to micro time dilation, a gravitational force was discovered as space was bent due to near C velocity of the electron and the discovery that going light speed and gravity were linked unquestionably by Einstein. It has come into question how light speed is linked to gravity, and experiment was done with atomic clocks in a centrifuge where the acceleration in the disk was causing macroscopic time dilation according to Einsteins well known works. when cesium clocks were used in the 50s on the ground and a commercial airliner, the difference in the two clocks was measured to be about 45.9 microseconds time dilation for 24 hours of flight, the clock at sea level being slower.
Nonrenormalizability of gravity.
General relativity, like electromagnetism, is a classical field theory. One might expect that, as with electromagnetism, the gravitational force should also have a corresponding quantum field theory.
However, gravity is perturbatively nonrenormalizable. For a quantum field theory to be well-defined according to this understanding of the subject, it must be asymptotically free or asymptotically safe. The theory must be characterized by a choice of "finitely many" parameters, which could, in principle, be set by experiment. For example, in quantum electrodynamics these parameters are the charge and mass of the electron, as measured at a particular energy scale.
On the other hand, in quantizing gravity there are, in perturbation theory, "infinitely many independent parameters" (counterterm coefficients) needed to define the theory. For a given choice of those parameters, one could make sense of the theory, but since it's impossible to conduct infinite experiments to fix the values of every parameter, it has been argued that one does not, in perturbation theory, have a meaningful physical theory:
If we treat QG as an effective field theory, there is a way around this problem.
That is, the meaningful theory of quantum gravity (that makes sense and is predictive at all energy levels) inherently implies some deep principle that reduces the infinitely many unknown parameters to a finite number that can then be measured:
QG as an effective field theory.
In an effective field theory, all but the first few of the infinite set of parameters in a non-renormalizable theory are suppressed by huge energy scales and hence can be neglected when computing low-energy effects. Thus, at least in the low-energy regime, the model is indeed a predictive quantum field theory. (A very similar situation occurs for the very similar effective field theory of low-energy pions.) Furthermore, many theorists agree that even the Standard Model should really be regarded as an effective field theory as well, with "nonrenormalizable" interactions suppressed by large energy scales and whose effects have consequently not been observed experimentally.
Recent work has shown that by treating general relativity as an effective field theory, one can actually make legitimate predictions for quantum gravity, at least for low-energy phenomena. An example is the well-known calculation of the tiny first-order quantum-mechanical correction to the classical Newtonian gravitational potential between two masses.
Spacetime background dependence.
A fundamental lesson of general relativity is that there is no fixed spacetime background, as found in Newtonian mechanics and special relativity; the spacetime geometry is dynamic. While easy to grasp in principle, this is the hardest idea to understand about general relativity, and its consequences are profound and not fully explored, even at the classical level. To a certain extent, general relativity can be seen to be a relational theory, in which the only physically relevant information is the relationship between different events in space-time.
On the other hand, quantum mechanics has depended since its inception on a fixed background (non-dynamic) structure. In the case of quantum mechanics, it is time that is given and not dynamic, just as in Newtonian classical mechanics. In relativistic quantum field theory, just as in classical field theory, Minkowski spacetime is the fixed background of the theory.
String theory.
String theory can be seen as a generalization of quantum field theory where instead of point particles, string-like objects propagate in a fixed spacetime background, although the interactions among closed strings give rise to space-time in a dynamical way.
Although string theory had its origins in the study of quark confinement and not of quantum gravity, it was soon discovered that the string spectrum contains the graviton, and that "condensation" of certain vibration modes of strings is equivalent to a modification of the original background. In this sense, string perturbation theory exhibits exactly the features one would expect of a perturbation theory that may exhibit a strong dependence on asymptotics (as seen, for example, in the AdS/CFT correspondence) which is a weak form of background dependence.
Background independent theories.
Loop quantum gravity is the fruit of an effort to formulate a background-independent quantum theory.
Topological quantum field theory provided an example of background-independent quantum theory, but with no local degrees of freedom, and only finitely many degrees of freedom globally. This is inadequate to describe gravity in 3+1 dimensions, which has local degrees of freedom according to general relativity. In 2+1 dimensions, however, gravity is a topological field theory, and it has been successfully quantized in several different ways, including spin networks.
Semi-classical quantum gravity.
Quantum field theory on curved (non-Minkowskian) backgrounds, while not a full quantum theory of gravity, has shown many promising early results. In an analogous way to the development of quantum electrodynamics in the early part of the 20th century (when physicists considered quantum mechanics in classical electromagnetic fields), the consideration of quantum field theory on a curved background has led to predictions such as black hole radiation.
Phenomena such as the Unruh effect, in which particles exist in certain accelerating frames but not in stationary ones, do not pose any difficulty when considered on a curved background (the Unruh effect occurs even in flat Minkowskian backgrounds). The vacuum state is the state with the least energy (and may or may not contain particles).
See Quantum field theory in curved spacetime for a more complete discussion.
Points of tension.
There are other points of tension between quantum mechanics and general relativity.
Candidate theories.
There are a number of proposed quantum gravity theories. Currently, there is still no complete and consistent quantum theory of gravity, and the candidate models still need to overcome major formal and conceptual problems. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests, although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.
String theory.
One suggested starting point is ordinary quantum field theories which, after all, are successful in describing the other three basic fundamental forces in the context of the standard model of elementary particle physics. However, while this leads to an acceptable effective (quantum) field theory of gravity at low energies, gravity turns out to be much more problematic at higher energies. For ordinary field theories such as quantum electrodynamics, a technique known as renormalization is an integral part of deriving predictions which take into account higher-energy contributions, but gravity turns out to be nonrenormalizable: at high energies, applying the recipes of ordinary quantum field theory yields models that are devoid of all predictive power.
One attempt to overcome these limitations is to replace ordinary quantum field theory, which is based on the classical concept of a point particle, with a quantum theory of one-dimensional extended objects: string theory. At the energies reached in current experiments, these strings are indistinguishable from point-like particles, but, crucially, different modes of oscillation of one and the same type of fundamental string appear as particles with different (electric and other) charges. In this way, string theory promises to be a unified description of all particles and interactions. The theory is successful in that one mode will always correspond to a graviton, the messenger particle of gravity; however, the price of this success are unusual features such as six extra dimensions of space in addition to the usual three for space and one for time.
In what is called the , it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity. As presently understood, however, string theory admits a very large number (10500 by some estimates) of consistent vacua, comprising the so-called "string landscape". Sorting through this large family of solutions remains a major challenge.
Loop quantum gravity.
Loop quantum gravity seriously considers general relativity's insight that spacetime is a dynamical field and is therefore a quantum object. Its second idea is that the quantum discreteness that determines the particle-like behavior of other field theories (for instance, the photons of the electromagnetic field) also affects the structure of space.
The main result of loop quantum gravity is the derivation of a granular structure of space at the Planck length. This is derived from following considerations: In the case of electromagnetism, the quantum operator representing the energy of each frequency of the field has a discrete spectrum. Thus the energy of each frequency is quantized, and the quanta are the photons. In the case of gravity, the operators representing the area and the volume of each surface or space region likewise have discrete spectrum. Thus area and volume of any portion of space are also quantized, where the quanta are elementary quanta of space. It follows, then, that spacetime has an elementary quantum granular structure at the Planck scale, which cuts off the ultraviolet infinities of quantum field theory.
The quantum state of spacetime is described in the theory by means of a mathematical structure called spin networks. Spin networks were initially introduced by Roger Penrose in abstract form, and later shown by Carlo Rovelli and Lee Smolin to derive naturally from a non-perturbative quantization of general relativity. Spin networks do not represent quantum states of a field in spacetime: they represent directly quantum states of spacetime.
The theory is based on the reformulation of general relativity known as Ashtekar variables, which represent geometric gravity using mathematical analogues of electric and magnetic fields. 
In the quantum theory, space is represented by a network structure called a spin network, evolving over time in discrete steps.
The dynamics of the theory is today constructed in several versions. One version starts with the canonical quantization of general relativity. The analogue of the Schrödinger equation is a Wheeler–DeWitt equation, which can be defined within the theory. 
In the covariant, or spinfoam formulation of the theory, the quantum dynamics is obtained via a sum over discrete versions of spacetime, called spinfoams. These represent histories of spin networks.
Other approaches.
There are a number of other approaches to quantum gravity. The approaches differ depending on which features of general relativity and quantum theory are accepted unchanged, and which features are modified. Examples include:
Weinberg–Witten theorem.
In quantum field theory, the Weinberg–Witten theorem places some constraints on theories of composite gravity/emergent gravity. However, recent developments attempt to show that if locality is only approximate and the holographic principle is correct, the Weinberg–Witten theorem would not be valid.
Experimental tests.
As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.
The most widely pursued possibilities for quantum gravity phenomenology include violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations in the space-time foam.
The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. If truly primordial, these waves were born as quantum fluctuations in gravity itself. Cosmologist Ken Olum (Tufts University) stated: "I think this is the only observational evidence that we have that actually shows that gravity is quantized...It's probably the only evidence of this that we will ever have."

</doc>
<doc id="25315" url="https://en.wikipedia.org/wiki?curid=25315" title="Quality of service">
Quality of service

Quality of service (QoS) is the overall performance of a telephony or computer network, particularly the performance seen by the users of the network.
To quantitatively measure quality of service, several related aspects of the network service are often considered, such as error rates, bit rate, throughput, transmission delay, availability, jitter, etc.
Quality of service is particularly important for the transport of traffic with special requirements. In particular, much technology has been developed to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter service demands.
Definitions.
In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on. A subset of telephony QoS is grade of service (GoS) requirements, which comprises aspects of a connection relating to capacity and coverage of a network, for example guaranteed maximum blocking probability and outage probability.
In the field of computer networking and other packet-switched telecommunication networks, the traffic engineering term refers to resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow. For example, a required bit rate, delay, jitter, packet dropping probability and/or bit error rate may be guaranteed. Quality of service guarantees are important if the network capacity is insufficient, especially for real-time streaming multimedia applications such as voice over IP, online games and IP-TV, since these often require fixed bit rate and are delay sensitive, and in networks where the capacity is a limited resource, for example in cellular data communication.
A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes. It may release the reserved capacity during a tear down phase.
A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load. The resulting absence of network congestion eliminates the need for QoS mechanisms.
QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e. the guaranteed service quality. High QoS is often confused with a high level of performance or achieved service quality, for example high bit rate, low latency and low bit error probability.
An alternative and disputable definition of QoS, used especially in application layer services such as telephony and streaming video, is requirements on a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE) subjective business concept, the required “user perceived performance”, the required “degree of satisfaction of the user” or the targeted “number of happy customers”. Examples of measures and measurement methods are mean opinion score (MOS), perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ). See also Subjective video quality.
History.
Conventional Internet routers and LAN switches operate on a best effort basis. This equipment is less expensive, less complex and faster and thus more popular than competing more complex technologies that provided QoS mechanisms. There were four “Type of service” bits and three “Precedence” bits provided in each IP packet header, but they were not generally respected. These bits were later re-defined as Differentiated services code points (DSCP).
With the advent of IPTV and IP telephony, QoS mechanisms are increasingly available to the end user.
A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3). Despite these network technologies remaining in use today, this kind of network lost attention after the advent of Ethernet networks. Today Ethernet is, by far, the most popular layer 2 technology. Ethernet uses 802.1p to signal the priority of a frame.
Qualities of traffic.
In packet-switched networks, quality of service is affected by various factors, which can be divided into “human” and “technical” factors. Human factors include: stability of service, availability of service, delays, user information. Technical factors include: reliability, scalability, effectiveness, maintainability, grade of service, etc.
Many things can happen to packets as they travel from origin to destination, resulting in the following problems as seen from the point of view of the sender and receiver:
Applications.
A defined quality of service may be desired or required for certain types of network traffic, for example:
These types of service are called "inelastic", meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, "elastic" applications can take advantage of however much or little bandwidth is available. Bulk file transfer applications that rely on TCP are generally elastic.
Mechanisms.
Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol and do not need additional procedures to achieve it. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.
When the expense of mechanisms to provide QoS is justified, network customers and providers can enter into a contractual agreement termed a service level agreement (SLA) which specifies guarantees for the ability of a network/protocol to give guaranteed performance/throughput/latency bounds based on mutually agreed measures, usually by prioritizing traffic.
In other approaches, resources are reserved at each step on the network for the call as it is set up.
Over-provisioning.
An alternative to complex QoS control mechanisms is to provide high quality communication by generously over-provisioning a network so that capacity is based on peak traffic load estimates. This approach is simple for networks with predictable peak loads. The performance is reasonable for many applications. This might include demanding applications that can compensate for variations in bandwidth and delay with large receive buffers, which is often possible for example in video streaming. Over-provisioning can be of limited use, however, in the face of transport protocols (such as TCP) that over time exponentially increase the amount of data placed on the network until all available bandwidth is consumed and packets are dropped. Such greedy protocols tend to increase latency and packet loss for all users.
Commercial VoIP services are often competitive with traditional telephone service in terms of call quality even though QoS mechanisms are usually not in use on the user's connection to their ISP and the VoIP provider's connection to a different ISP. Under high load conditions, however, VoIP may degrade to cell-phone quality or worse. The mathematics of packet traffic indicate that network requires just 60% more raw capacity under conservative assumptions.
The amount of over-provisioning in interior links required to replace QoS depends on the number of users and their traffic demands. This limits usability of over-provisioning. Newer more bandwidth intensive applications and the addition of more users results in the loss of over-provisioned networks. This then requires a physical update of the relevant network links which is an expensive process. Thus over-provisioning cannot be blindly assumed on the Internet.
IP and Ethernet efforts.
Unlike single-owner networks, the Internet is a series of exchange points interconnecting private networks. Hence the Internet's core is owned and managed by a number of different network service providers, not a single entity. Its behavior is much more stochastic or unpredictable. Therefore, research continues on QoS procedures that are deployable in large, diverse networks.
There are two principal approaches to QoS in modern packet-switched IP networks, a parameterized system based on an exchange of application requirements with the network, and a prioritized system where each packet identifies a desired service level to the network.
Early work used the integrated services (IntServ) philosophy of reserving network resources. In this model, applications used the Resource reservation protocol (RSVP) to request and reserve resources through a network. While IntServ mechanisms do work, it was realized that in a broadband network typical of a larger service provider, Core routers would be required to accept, maintain, and tear down thousands or possibly tens of thousands of reservations. It was believed that this approach would not scale with the growth of the Internet, and in any event was antithetical to the notion of designing networks so that Core routers do little more than simply switch packets at the highest possible rates.
In response to these markings, routers and switches use various queuing strategies to tailor performance to requirements. At the IP layer, DSCP markings use the 6 bits in the IP packet header. At the MAC layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to carry essentially the same information.
Routers supporting DiffServ configure their network scheduler to use multiple queues for packets awaiting transmission from bandwidth constrained (e.g., wide area) interfaces. Router vendors provide different capabilities for configuring this behavior, to include the number of queues supported, the relative priorities of queues, and bandwidth reserved for each queue.
In practice, when a packet must be forwarded from an interface with queuing, packets requiring low jitter (e.g., VoIP or videoconferencing) are given priority over packets in other queues. Typically, some bandwidth is allocated by default to network control packets (such as Internet Control Message Protocol and routing protocols), while best effort traffic might simply be given whatever bandwidth is left over.
At the Media Access Control (MAC) layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to distinguish between Ethernet frames and classify them. Queueing theory models have been developed on performance analysis and QoS for MAC layer protocols.
Cisco IOS NetFlow and the Cisco Class Based QoS (CBQoS) Management Information Base (MIB) are marketed by Cisco Systems.
One compelling example of the need for QoS on the Internet relates to congestion collapse. The Internet relies on congestion avoidance protocols, as built into Transmission Control Protocol (TCP), to reduce traffic under conditions that would otherwise lead to "meltdown". QoS applications such as VoIP and IPTV, because they require largely constant bitrates and low latency cannot use TCP and cannot otherwise reduce their traffic rate to help prevent congestion. QoS contracts limit traffic that can be offered to the Internet and thereby enforce traffic shaping that can prevent it from becoming overloaded, and are hence an indispensable part of the Internet's ability to handle a mix of real-time and non-real-time traffic without meltdown.
End-to-end quality of service.
End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another.
The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation, as a proposed standard in 1997.
RSVP is an end-to-end bandwidth reservation protocol. The traffic engineering version, RSVP-TE, is used in many networks to establish traffic-engineered Multiprotocol Label Switching (MPLS) label-switched paths.
The IETF also defined Next Steps in Signaling (NSIS) with QoS signalling as a target. NSIS is a development and simplification of RSVP.
Research consortia such as "end-to-end quality of service support over heterogeneous networks" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services.
EuQoS conducted experiments to integrate Session Initiation Protocol, Next Steps in Signaling and IPsphere's SSS with an estimated cost of about 15.6 million Euro and published a book.
A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007.
Another research project named PlaNetS was proposed for European funding circa 2005.
A broader European project called "Architecture and design for the future Internet" known as 4WARD had a budgest estimated at 23.4 million Euro and was funded from January 2008 through June 2010.
It included a "Quality of Service Theme" and published a book.
Another European project, called WIDENS (Wireless Deployable Network System) proposed a bandwidth reservation approach for mobile wireless multirate adhoc networks.<br>
In the services domain, end-to-end Quality of Service has also been discussed in the case of composite services (consisting of atomic services) or applications (consisting of application components). Moreover, in cloud computing end-to-end QoS has been the focus of various research efforts aiming at the provision of QoS guarantees across the cloud service models.
Circumvention.
Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them. As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers. Yet, encrypted traffic is otherwise unable to undergo deep packet inspection for QoS.
Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.
Doubts about quality of service over IP.
The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time.
Equipment available at the time relied on software to implement QoS. The group also predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.
They believed that the economics would encourage network providers to deliberately erode the quality of best effort traffic as a way to push customers to higher priced QoS services. Instead they proposed over-provisioning of capacity as more cost-effective at the time.
The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined.
Bachula's testimony has been cited by proponents of a law banning quality of service as proof that no legitimate purpose is served by such an offering. This argument is dependent on the assumption that over-provisioning isn't a form of QoS and that it is always possible. Cost and other factors affect the ability of carriers to build and maintain permanently over-provisioned networks.
Mobile (cellular) QoS.
Mobile cellular service providers may offer mobile QoS to customers just as the fixed line PSTN services providers and Internet Service Providers (ISP) may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for non-elastic services, for example streaming multimedia.
Mobility adds complication to the QoS mechanisms, for several reasons:
Standards.
Quality of service in the field of telephony, was first defined in 1994 in the ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security.
A 1995 recommendation X.902 included a definition is the OSI reference model.
In 1998 the ITU published a document discussing QoS in the field of data networking. X.641 offers a means of developing or enhancing standards related to QoS and provide concepts and terminology that will assist in maintaining the consistency of related standards.
Some QoS-related IETF Request For Comments (RFC)s are Definition of the Differentiated services Field (DS Field) in the IPv4 and IPv6 Headers (RFC 2474), and Resource ReSerVation Protocol (RSVP) (RFC 2205); both these are discussed above. The IETF has also published two RFCs giving background on QoS: RFC 2990: Next Steps for the IP QoS Architecture, and RFC 3714: IAB Concerns Regarding Congestion Control for Voice Traffic in the Internet.
The IETF has also published RFC 4594 Configuration Guidelines for DiffServ Service Classes as an informative or "best practices" document about the practical aspects of designing a QoS solution for a DiffServ network. They try to identify which types of applications are commonly run over an IP network to group them into traffic classes, study what treatment do each of these classes need from the network, and suggest which of the QoS mechanisms commonly available in routers can be used to implement those treatments.

</doc>
<doc id="25316" url="https://en.wikipedia.org/wiki?curid=25316" title="Quadrature amplitude modulation">
Quadrature amplitude modulation

Quadrature amplitude modulation (QAM) is both an analog and a digital modulation scheme. It conveys two analog message signals, or two digital bit streams, by changing ("modulating") the amplitudes of two carrier waves, using the amplitude-shift keying (ASK) digital modulation scheme or amplitude modulation (AM) analog modulation scheme. The two carrier waves of the same frequency, usually sinusoids, are out of phase with each other by 90° and are thus called quadrature carriers or quadrature components — hence the name of the scheme. The modulated waves are summed, and the final waveform is a combination of both phase-shift keying (PSK) and amplitude-shift keying (ASK), or, in the analog case, of phase modulation (PM) and amplitude modulation. In the digital QAM case, a finite number of at least two phases and at least two amplitudes are used. PSK modulators are often designed using the QAM principle, but are not considered as QAM since the amplitude of the modulated carrier signal is constant. QAM is used extensively as a modulation scheme for digital telecommunication systems. Arbitrarily high spectral efficiencies can be achieved with QAM by setting a suitable constellation size, limited only by the noise level and linearity of the communications channel.
QAM is being used in optical fiber systems as bit rates increase; QAM16 and QAM64 can be optically emulated with a 3-path interferometer.
Introduction.
Like all modulation schemes, QAM conveys data by changing some aspect of a carrier signal, or the carrier wave, (usually a sinusoid) in response to a data signal. In the case of QAM, the amplitude of two waves of the same frequency, 90° out-of-phase with each other (in quadrature) are changed ("modulated" or "keyed") to represent the data signal. Amplitude modulating two carriers in quadrature can be equivalently viewed as both amplitude modulating and phase modulating a single carrier.
Phase modulation (analog PM) and phase-shift keying (digital PSK) can be regarded as a special case of QAM, where the magnitude of the modulating signal is a constant, with only the phase varying. This can also be extended to frequency modulation (FM) and frequency-shift keying (FSK), for these can be regarded as a special case of phase modulation.
Analog QAM.
When transmitting two signals by modulating them with QAM, the transmitted signal will be of the form:
where formula_2, formula_3 and formula_4 are the modulating signals, formula_5 is the carrier frequency and formula_6 is the real part.
At the receiver, these two modulating signals can be demodulated using a coherent demodulator. Such a receiver multiplies the received signal separately with both a cosine and sine signal to produce the received estimates of formula_3 and formula_4 respectively. Because of the orthogonality property of the carrier signals, it is possible to detect the modulating signals independently.
In the ideal case formula_3 is demodulated by multiplying the transmitted signal with a cosine signal:
Using standard trigonometric identities, we can write it as:
Low-pass filtering formula_12 removes the high frequency terms (containing formula_13), leaving only the formula_3 term. This filtered signal is unaffected by formula_4, showing that the in-phase component can be received independently of the quadrature component. Similarly, we may multiply formula_16 by a sine wave and then low-pass filter to extract formula_4.
Analog QAM suffers from the same problem as Single-sideband modulation: the exact phase of the carrier is required for correct demodulation at the receiver. If the demodulating phase is even a little off, it results in crosstalk between the modulated signals. This issue of carrier synchronization at the receiver must be handled somehow in QAM systems. The coherent demodulator needs to be exactly in phase with the received signal, or otherwise the modulated signals cannot be independently received. This is achieved typically by transmitting a burst subcarrier or a Pilot signal.
Analog QAM is used in:
Fourier analysis of QAM.
In the frequency domain, QAM has a similar spectral pattern to DSB-SC modulation. Using the properties of the Fourier transform, we find that:
where "S"("f"), "M""I"("f") and "M""Q"("f") are the Fourier transforms (frequency-domain representations) of "s"("t"), "I"("t") and "Q"("t"), respectively.
Quantized QAM.
As in many digital modulation schemes, the constellation diagram is useful for QAM. In QAM, the constellation points are usually arranged in a square grid with equal vertical and horizontal spacing, although other configurations are possible (e.g. Cross-QAM). Since in digital telecommunications the data are usually binary, the number of points in the grid is usually a power of 2 (2, 4, 8, …). Since QAM is usually square, some of these are rare—the most common forms are 16-QAM, 64-QAM and 256-QAM. By moving to a higher-order constellation, it is possible to transmit more bits per symbol. However, if the mean energy of the constellation is to remain the same (by way of making a fair comparison), the points must be closer together and are thus more susceptible to noise and other corruption; this results in a higher bit error rate and so higher-order QAM can deliver more data less reliably than lower-order QAM, for constant mean constellation energy. Using higher-order QAM without increasing the bit error rate requires a higher signal-to-noise ratio (SNR) by increasing signal energy, reducing noise, or both.
If data-rates beyond those offered by 8-PSK are required, it is more usual to move to QAM since it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly. The complicating factor is that the points are no longer all the same amplitude and so the demodulator must now correctly detect both phase and amplitude, rather than just phase.
64-QAM and 256-QAM are often used in digital cable television and cable modem applications. In the United States, 64-QAM and 256-QAM are the mandated modulation schemes for digital cable (see QAM tuner) as standardised by the SCTE in the standard ANSI/SCTE 07 2013. Note that many marketing people will refer to these as QAM-64 and QAM-256. In the UK, 64-QAM is used for digital terrestrial television (Freeview) whilst 256-QAM is used for Freeview-HD.
Communication systems designed to achieve very high levels of spectral efficiency usually employ very dense QAM constellations. For example, current Homeplug AV2 500-Mbit powerline Ethernet devices use 1024-QAM and 4096-QAM, as well as future devices using ITU-T G.hn standard for networking over existing home wiring (coaxial cable, phone lines and power lines); 4096-QAM provides 12 bits/symbol. Another example is ADSL technology for copper twisted pairs, whose constellation size goes up to 32768-QAM (in ADSL terminology this is referred to as bit-loading, or bit per tone, 32768-QAM being equivalent to 15 bits per tone).
Ultra-high capacity Microwave Backhaul Systems also use 1024-QAM. With 1024-QAM, Adaptive Coding and Modulation (ACM), and XPIC, Vendors can obtain Gigabit capacity in a single 56 MHz channel.
Ideal structure.
Transmitter.
The following picture shows the ideal structure of a QAM transmitter, with a carrier center frequency formula_5 and the frequency response of the transmitter's filter formula_20:
First the flow of bits to be transmitted is split into two equal parts: this process generates two independent signals to be transmitted. They are encoded separately just like they were in an amplitude-shift keying (ASK) modulator. Then one channel (the one "in phase") is multiplied by a cosine, while the other channel (in "quadrature") is multiplied by a sine. This way there is a phase of 90° between them. They are simply added one to the other and sent through the real channel.
The sent signal can be expressed in the form:
where formula_22 are the voltages applied in response to the formula_23th symbol to the cosine and sine waves respectively.
Receiver.
The receiver simply performs the inverse operation of the transmitter. Its ideal structure is shown in the picture below with formula_24 the receive filter's frequency response :
Multiplying by a cosine (or a sine) and by a low-pass filter it is possible to extract the component in phase (or in quadrature). Then there is only an ASK demodulator and the two flows of data are merged back.
In practice, there is an unknown phase delay between the transmitter and receiver that must be compensated by "synchronization" of the receivers local oscillator; i.e., the sine and cosine functions in the above figure. In mobile applications, there will often be an offset in the relative "frequency" as well, due to the possible presence of a Doppler shift proportional to the relative velocity of the transmitter and receiver. Both the phase and frequency variations introduced by the channel must be compensated by properly tuning the sine and cosine components, which requires a "phase reference", and is typically accomplished using a Phase-Locked Loop (PLL).
In any application, the low-pass filter and the receive formula_24 filter will be implemented as a single combined filter. Here they are shown as separate just to be clearer.
Quantized QAM performance.
The following definitions are needed in determining error rates:
formula_36 is related to the complementary Gaussian error function by:
formula_37, which is the probability that "x" will be under the tail of the Gaussian PDF towards positive infinity.
The error rates quoted here are those in additive white Gaussian noise (AWGN).
Where coordinates for constellation points are given in this article, note that they represent a "non-normalised" constellation. That is, if a particular mean average energy were required (e.g. unit average energy), the constellation would need to be linearly scaled.
Rectangular QAM.
Rectangular QAM constellations are, in general, sub-optimal in the sense that they do not maximally space the constellation points for a given energy. However, they have the considerable advantage that they may be easily transmitted as two pulse amplitude modulation (PAM) signals on quadrature carriers, and can be easily demodulated. The non-square constellations, dealt with below, achieve marginally better bit-error rate (BER) but are harder to modulate and demodulate.
The first rectangular QAM constellation usually encountered is 16-QAM, the constellation diagram for which is shown here. A Gray coded bit-assignment is also given. The reason that 16-QAM is usually the first is that a brief consideration reveals that 2-QAM and 4-QAM are in fact binary phase-shift keying (BPSK) and quadrature phase-shift keying (QPSK), respectively. Also, the error-rate performance of 8-QAM is close to that of 16-QAM (only about 0.5 dB better), but its data rate is only three-quarters that of 16-QAM.
Expressions for the symbol-error rate of rectangular QAM are not hard to derive but yield rather unpleasant expressions. For an even number of bits per symbol, formula_38, exact expressions are available. They are most easily expressed in a "per carrier" sense:
so
The bit-error rate depends on the bit to symbol mapping, but for formula_41 and a Gray-coded assignment—so that we can assume each symbol error causes only one bit error—the bit-error rate is approximately
Since the carriers are independent, the overall bit error rate is the same as the per-carrier error rate, just like BPSK and QPSK.
An exact and general closed-form expression of the
Bit Error Rates (BER) for rectangular type of Quadrature
Amplitude Modulation (QAM) over AWGN and slow, flat, Rician fading
channels were derived analytically. Consider a (L×M)-QAM system with 2 · log2 L levels and
2 · log2M levels in the I-channel and Q-channel, respectively
and a two-dimensional grey code mapping employed. It was
shown that the generalized expression for the conditional
BER on SNR formula_44 over AWGN channel is
where
Odd-"k" QAM.
For odd formula_38, such as 8-QAM (formula_48) it is harder to obtain symbol-error rates, but a tight upper bound is:
Two rectangular 8-QAM constellations are shown below without bit assignments. These both have the same minimum distance between symbol points, and thus the same symbol-error rate (to a first approximation).
The exact bit-error rate, formula_31 will depend on the bit-assignment.
Note that both of these constellations are seldom used in practice, as the non-rectangular version of 8-QAM is optimal.
Non-rectangular QAM.
It is the nature of QAM that most orders of constellations can be constructed in many different ways and it is neither possible nor instructive to cover them all here. This article instead presents two, lower-order constellations.
Two diagrams of circular QAM constellation are shown, for 8-QAM and 16-QAM. The circular 8-QAM constellation is known to be the optimal 8-QAM constellation in the sense of requiring the least mean power for a given minimum Euclidean distance. The 16-QAM constellation is suboptimal although the optimal one may be constructed along the same lines as the 8-QAM constellation. The circular constellation highlights the relationship between QAM and PSK. Other orders of constellation may be constructed along similar (or very different) lines. It is consequently hard to establish expressions for the error rates of non-rectangular QAM since it necessarily depends on the constellation. Nevertheless, an obvious upper bound to the rate is related to the minimum Euclidean distance of the constellation (the shortest straight-line distance between two points):
Again, the bit-error rate will depend on the assignment of bits to symbols.
Although, in general, there is a non-rectangular constellation that is optimal for a particular formula_26, they are not often used since the rectangular QAMs are much easier to modulate and demodulate.
Hierarchical QAM.
Hierarchical QAM is a form of Hierarchical modulation. For example, Hierarchical QAM is used in DVB, where the constellation points are grouped into a high-priority QPSK stream and a low-priority 16-QAM stream. The irregular distribution of constellation points improves the reception probability of the high-priority stream in low SNR conditions, at the expense of higher SNR requirements for the low-priority stream.
Interference and noise.
In moving to a higher order QAM constellation (higher data rate and mode) in hostile RF/microwave QAM application environments, such as in broadcasting or telecommunications, multipath interference typically increases. There is a spreading of the spots in the constellation, decreasing the separation between adjacent states, making it difficult for the receiver to decode the signal appropriately. In other words, there is reduced noise immunity. There are several test parameter measurements which help determine an optimal QAM mode for a specific operating environment. The following three are most significant:
References.
5. Jonqyin (Russell) Sun "Linear diversity analysis for QAM in Rician fading channels", IEEE WOCC 2014
The notation used here has mainly (but not exclusively) been taken from

</doc>
<doc id="25317" url="https://en.wikipedia.org/wiki?curid=25317" title="QAM (disambiguation)">
QAM (disambiguation)

QAM may refer to:

</doc>
<doc id="25319" url="https://en.wikipedia.org/wiki?curid=25319" title="Quetzalcoatlus">
Quetzalcoatlus

Quetzalcoatlus was a pterosaur known from the Late Cretaceous of North America (Maastrichtian stage) and one of the largest known flying animals of all time. It was a member of the Azhdarchidae, a family of advanced toothless pterosaurs with unusually long, stiffened necks. Its name comes from the Mesoamerican feathered serpent god Quetzalcoatl.
Description.
Size.
When it was first discovered, scientists estimated that the largest "Quetzalcoatlus" fossils came from an individual with a wingspan as large as , choosing the middle of three extrapolations from the proportions of other pterosaurs that gave an estimate of 11, 15.5 and 21 meters respectively (36 feet, 50.85 feet, 68.9 feet). In 1981, further study lowered these estimates to . More recent estimates based on greater knowledge of azhdarchid proportions place its wingspan at .
Mass estimates for giant azhdarchids are extremely problematic because no existing species share a similar size or body plan, and in consequence published results vary widely. While some studies have historically found extremely low weight estimates for "Quetzalcoatlus", as low as for a individual, a majority of estimates published since the 2000s have been higher, around .
Skull.
Skull material (from smaller specimens, possibly a related species) shows that "Quetzalcoatlus" had a very sharp and pointed beak. That is contrary to some earlier reconstructions that showed a blunter snout, based on the inadvertent inclusion of jaw material from another pterosaur species, possibly a tapejarid or a form related to "Tupuxuara". A skull crest was also present but its exact form and size are still unknown.
Discovery and species.
The first "Quetzalcoatlus" fossils were discovered in Texas, United States, from the Maastrichtian Javelina Formation at Big Bend National Park (dated to around 68 million years ago) in 1971 by a geology graduate student from the University of Texas at Austin's Jackson School of Geosciences, Douglas A. Lawson. The specimen consisted of a partial wing (in pterosaurs composed of the forearms and elongated fourth finger), from an individual later estimated at over in wingspan. Lawson discovered a second site of the same age, about forty kilometers from the first, where between 1972 and 1974 he and Professor Wann Langston Jr. of the Texas Memorial Museum unearthed three fragmentary skeletons of much smaller individuals. Lawson in 1975 announced the find in an article in "Science". That same year, in a subsequent letter to the same journal, he made the original large specimen, TMM 41450-3, the holotype of a new genus and species, Quetzalcoatlus northropi. The genus name refers to the Aztec feathered serpent god Quetzalcoatl. The specific name honors John Knudsen Northrop, the founder of Northrop, who was interested in large tailless flying wing aircraft designs resembling "Quetzalcoatlus". At first it was assumed that the smaller specimens were juvenile or subadult forms of the larger type. Later, when more remains were found, it was realized they could have been a separate species. This possible second species from Texas was provisionally referred to as a "Quetzalcoatlus" sp. by Alexander Kellner and Langston in 1996, indicating that its status was too uncertain to give it a full new species name. The smaller specimens are more complete than the "Q. northropi" holotype, and include four partial skulls, though they are much less massive, with an estimated wingspan of .
The holotype specimen of "Q. northropi" has yet to be properly described and diagnosed. Where the known remains overlap, it has been considered by Mark Witton and colleagues (2010) to be indistinguishable from its Romanian contemporary "Hatzegopteryx". If "Q. northropi" is complete enough to be distinguished from other pterosaurs (i.e., if it is not a "nomen dubium"), "Hatzegopteryx" may represent the same animal. It is likely that huge pterosaurs such as "Q. northropi" would have had very large, transcontinental ranges, making its presence in both North America and Europe unsurprising. Mark Witton "et al." argued that the skull material of "Hatzegopteryx" and "Q." sp. differ enough that they cannot be regarded as the same animal, making it likely that "Q." sp., if not identical to "Quetzalcoatlus northropi", represents a distinct genus.
An azhdarchid neck vertebra, discovered in 2002 from the Maastrichtian age Hell Creek Formation, may also belong to "Quetzalcoatlus". The specimen (BMR P2002.2) was recovered accidentally when it was included in a field jacket prepared to transport part of a tyrannosaur specimen. Despite this association with the remains of a large carnivorous dinosaur, the vertebra shows no evidence that it was chewed on by the dinosaur. The bone came from an individual azhdarchid pterosaur estimated to have had a wingspan of .
Classification.
Below is a cladogram showing the phylogenetic placement of "Quetzalcoatlus" within Neoazhdarchia from Andres and Myers (2013).
Paleobiology.
"Quetzalcoatlus" was abundant in Texas during the Lancian in a fauna dominated by "Alamosaurus". The "Alamosaurus"-"Quetzalcoatlus" association probably represents semi-arid inland plains. "Quetzalcoatlus" had precursors in North America and its apparent rise to widespreadness may represent the expansion of its preferred habitat rather than an immigration event, as some experts have suggested.
Feeding.
There have been a number of different ideas proposed about the lifestyle of "Quetzalcoatlus". Because the area of the fossil site was four hundred kilometers removed from the coastline and there were no indications of large rivers or deep lakes nearby at the end of the Cretaceous, Lawson in 1975 rejected a fish-eating lifestyle, instead suggesting that "Quetzalcoatlus" scavenged like the marabou stork, but then on the carcasses of titanosaur sauropods such as "Alamosaurus". Lawson had found the remains of the giant pterosaur while searching for the bones of this dinosaur, which formed an important part of its ecosystem.
In 1996, Thomas Lehman and Langston rejected the scavenging hypothesis, pointing out that the lower jaw bent so strongly downwards that even when it closed completely a gap of over five centimeters remained between it and the upper jaw, very different from the hooked beaks of specialized scavenging birds. They suggested that with its long neck vertebrae and long toothless jaws "Quetzalcoatlus" fed like modern-day skimmers, catching fish during flight while cleaving the waves with its beak. While this skim-feeding view became widely accepted, it was not subjected to scientific research until 2007 when a study showed that for such large pterosaurs it was not a viable method because the energy costs would be too high due to excessive drag. In 2008 pterosaur workers Mark Paul Witton and Darren Naish published an examination of possible feeding habits and ecology of azhdarchids. Witton and Naish noted that most azhdarchid remains are found in inland deposits far from seas or other large bodies of water required for skimming. Additionally, the beak, jaw, and neck anatomy are unlike those of any known skimming animal. Rather, they concluded that azhdarchids were more likely terrestrial stalkers, similar to modern storks, and probably hunted small vertebrates on land or in small streams. Though "Quetzalcoatlus", like other pterosaurs, was a quadruped when on the ground, "Quetzalcoatlus" and other azhdarchids have fore and hind limb proportions more similar to modern running ungulate mammals than to their smaller cousins, implying that they were uniquely suited to a terrestrial lifestyle.
Flight.
The nature of flight in "Quetzalcoatlus" and other giant azhdarchids was poorly understood until serious biomechanical studies were conducted in the 21st century. One early (1984) experiment by Paul MacCready used practical aerodynamics to test the flight of "Quetzalcoatlus". MacCready constructed a model flying machine or ornithopter with a simple computer functioning as an autopilot. The model successfully flew with a combination of soaring and wing flapping; however, the model was half scale based on a then-current weight estimate of around 80 kg, far lower than more modern estimates of over 200 kg. The method of flight in these pterosaurs depends largely on weight, which has been controversial, and widely differing masses have been favored by different scientists. Some researchers have suggested that these animals employed slow, soaring flight, while others have concluded that their flight was fast and dynamic. In 2010, Donald Henderson argued that the mass of "Q. northropi" had been underestimated, even the highest estimates, and that it was too massive to have achieved powered flight. He estimated it in his 2010 paper as 540 kg. Henderson argued that it may have been flightless.
However, most other flight capability estimates have disagreed with Henderson's research, suggesting instead an animal superbly adapted to long-range, extended flight. In 2010, Mike Habib, a professor of biomechanics at Chatham University, and Mark Witton, a British paleontologist, undertook further investigation into the claims of flightlessness in large pterosaurs. After factoring wingspan, body weight, and aerodynamics, a computer model led the two researchers to conclude that "Q. northropi" was capable of flight "up to 80 miles an hour for 7 to 10 days at altitudes of 15,000 feet". Mike Habib further suggested a maximum flight range of 8,000 to 12,000 miles for "Q. northropi". Henderson's work was further criticized by Habib, who pointed out that although Henderson used excellent mass estimations, they were based on outdated pterosaur models, and that anatomical study of "Q. northropi" and other large pterosaur forelimbs show a higher degree of robustness than would be expected if they were purely quadrupedal. Habib believes that large pterosaurs most likely utilized a short burst of powered flight in order to then transition to thermal soaring.
Cultural significance.
"Quetzalcoatlus" has been featured in documentaries, both in cinemas and on television, since the 1980s. The Smithsonian project to build a working model of "Q. northropi" was the subject of the 1986 IMAX documentary "On the Wing", shown at the National Air and Space Museum in Washington, D.C.. It has also been featured in television programs such as the BBC's "Walking with Dinosaurs" in 1999 depicted as a giant fish eating pterosaur and it was shown with an inaccurate spike instead of a crest and teeth instead of the toothless beak it had in life, and Dangerous, Ltd.'s "Clash of the Dinosaurs" in 2009. The later program featured traits invented by the producers to heighten entertainment value, including a depiction of "Quetzalcoatlus" with the ability to use ultraviolet vision to locate dinosaur urine when hunting in the air. It was also depicted in the 2011 documentary "March of the Dinosaurs", where it was erroneously depicted as a clawless, bipedal scavenger, and in the 2009 series "Animal Armageddon", where it was correctly portrayed with pycnofibres. In 2010, "Quetzalocotlus" appears in the BBC film "Flying Monsters 3D" where it was depicted as a giant vulture. In the "Return to Jurassic Park" bonus feature of the 2011 Blu-ray release of the "Jurassic Park" film series, John R. Horner describes "Quetzalcoatlus" as the pterosaur that most accurately represented and matched the size of the pterosaurs that are featured in the films. It was also depicted in the 2001 documentary "When Dinosaurs Roamed America" where it was also depicted as a vulture.
In June 2010, several life-sized models of "Q. northropi" were put on display on London's South Bank as the centerpiece exhibit for the Royal Society’s 350th anniversary exhibition. The models, which included both flying and standing individuals with wingspans of , were intended to help build interest in science among the public. The models were created by scientists from the University of Portsmouth, including David Martill, Bob Loveridge and Mark Witton, and engineers Bob and Jack Rushton from Griffon Hoverwork. The display presented to the public the most accurate pterosaur models constructed at the time, taking into account anatomical and footprint evidence based on skeletal and trace fossils from related pterosaurs.
In 1985, the U.S. Defense Advanced Research Projects Agency (DARPA) and AeroVironment used "Quetzalcoatlus northropi" as the basis for an experimental ornithopter UAV. They produced a half-scale model weighing , with a wingspan of . Coincidentally, Douglas A. Lawson, who discovered "Q. northropi" in Texas in 1971, named it for John "Jack" Northrop, a developer of tailless flying wing aircraft in the 1940s. The replica of "Q. northropi" incorporates a "flight control system/autopilot which processes pilot commands and sensor inputs, implements several feedback loops, and delivers command signals to its various servo-actuators". It is on exhibit at the National Air and Space Museum.

</doc>
<doc id="25320" url="https://en.wikipedia.org/wiki?curid=25320" title="Quedlinburg">
Quedlinburg

Quedlinburg () is a medieval German town situated just north of the Harz mountains, in the district of Harz in the west of Saxony-Anhalt, Germany. In 1994, both the medieval court and the old town were added to the prestigious UNESCO world heritage list.
The town was the capital of the county of Quedlinburg until 2007, when the district was dissolved. Several locations in the town are designated stops along the scenic holiday route, the Romanesque Road.
History.
The town of Quedlinburg is known to have existed since at least the early 9th century, when there was a settlement known as Gross Orden on the eastern bank of the River Bode. It was first mentioned as a town in 922 as part of a donation by King Henry the Fowler. The records of this donation were held by the abbey of Corvey.
After Henry's death in 936, his widow Saint Matilda founded a religious community for women (""Frauenstift"") on the castle hill, where daughters of the higher nobility were educated. The main task of this collegiate foundation, Quedlinburg Abbey, was to pray for the memory of King Henry and the rulers who came after him. The "Annals of Quedlinburg" were also compiled there. The first abbess was Matilda, a granddaughter of King Henry and St. Matilda.
The Quedlinburg castle complex, founded by King Henry and built up by Emperor Otto I in 936, was an imperial palatinate of the Saxon emperors. The palatinate, including the male convent, was in the valley, where today the Roman Catholic Church of St. Wiperti is situated, while the women's convent was located on the castle hill.
In 961 and 963, a canon's monastery was established in St. Wiperti, south of the castle hill. It was abandoned in the 16th century, and at one time the church, which boasts a magnificent crypt from the 10th century, was even used as a barn and a pigsty before being restored in the 1950s.
In 973, shortly before the death of Emperor Otto I, a "Reichstag" (Imperial Convention) was held at the imperial court in which Mieszko, duke of Poland, and Boleslav, duke of Bohemia, as well as numerous other nobles from as far away as Byzantium and Bulgaria, gathered to pay homage to the emperor. On the occasion, Otto the Great introduced his new daughter-in-law Theophanu, a Byzantine princess whose marriage to Otto II brought hope for recognition and continued peace between the rulers of the Eastern and Western empires.
In 994, Otto III granted the right of market, tax, and coining, and established the first market place to the north of the castle hill.
The town became a member of the Hanseatic League in 1426. Quedlinburg Abbey frequently disputed the independence of Quedlinburg, which sought the aid of the Bishopric of Halberstadt. In 1477, Abbess Hedwig, aided by her brothers Ernest and Albert, broke the resistance of the town and expelled the bishop's forces. Quedlinburg was forced to leave the Hanseatic League and was subsequently protected by the Electorate of Saxony. Both town and abbey converted to Lutheranism in 1539 during the Protestant Reformation.
In 1697, Elector Frederick Augustus I of Saxony sold his rights to Quedlinburg to Elector Frederick III of Brandenburg for 240,000 thalers. Quedlinburg Abbey contested Brandenburg-Prussia's claims throughout the 18th century, however. The abbey was secularized in 1802 during the German Mediatisation, and Quedlinburg passed to the Kingdom of Prussia as part of the Principality of Quedlinburg Part of the Napoleonic Kingdom of Westphalia from 1807–13, it was included within the new Prussian Province of Saxony in 1815. In all this time, great ladies ruled Quedlinburg as abbesses without "taking the veil"; they were free to marry. The last of these great ladies was a Swedish princess, an early fighter for women's rights, Sofia Albertina.
During the Nazi regime, the memory of Henry I became a sort of cult, as Heinrich Himmler saw himself as the reincarnation of the "most German of all German" rulers. The collegiate church and castle were to be turned into a shrine for Nazi Germany. The Nazi Party tried to create a new religion. The cathedral was closed from 1938 and during the war. The local crematory was kept busy burning the victims of the Langenstein-Zwieberge concentration camp. Liberation in 1945 brought back the Protestant bishop and the church bells, and the Nazi-style eagle was taken down from the tower. Georg Ay was local party chief from 1931 until the end of the war.
Quedlinburg was administered within Bezirk Halle while part of the Communist East Germany from 1949 to 1990. It became part of the state of Saxony-Anhalt upon German reunification in 1990.
During Quedlinburg's Communist era as part of the GDR (1949–1990), restoration specialists from Poland were called in during the 1980s to carry out repairs on the old architecture. As in all German cities, the "Altstadt" (old city) medieval sections are the most popular attractions of any town. Now Quedlinburg is a center of restoration of "Fachwerk" houses.
During the last months of World War II, the United States military occupied Quedlinburg. In the 1980s, upon the death of one of the US military men, the theft of medieval art from Quedlinburg came to light.
Main sights.
In the innermost parts of the town, a wide selection of half-timbered buildings from at least five different centuries are to be found (including a 14th-century structure, one of Germany's oldest), while around the outer fringes of the old town are wonderful examples of "Jugendstil" buildings, dating from the late 19th and early 20th centuries.
Since December 1994, the old town of Quedlinburg and the castle mount with the collegiate church are listed as one of UNESCO's World Heritage Sites. Quedlinburg is one of the best-preserved medieval and renaissance towns in Europe, having escaped major damage in World War II.
In 2006, the Selke valley branch of the Harz Narrow Gauge Railways was extended to Quedlinburg from Gernrode, giving access to the historic steam narrow gauge railway, Alexisbad and the high Harz plateau.
The castle and the cathedral still tower above the town the way they dominated it in the early Middle Ages. The cathedral is a prime example of German Romanesque style. The "Domschatz", the cathedral treasure containing ancient Christian religious artefacts and books, was stolen by an American soldier and finally brought back to Quedlinburg in 1993 and is again on display here.
Geography.
The town is located north of the Harz mountains, about 123 m above sea level. The nearest mountains reach 181 m above sea level. The largest part of the town is located in the western part of the Bode river bed. This river comes from the Harz mountains and flows into the river Saale and further into the river Elbe. The municipal area of Quedlinburg is 120.42 km²; before the incorporation of the two (previously independent) municipalities of Gernrode and Bad Suderode in January 2014 it was only 78.14 km².
Climate.
Quedlinburg has a oceanic climate (Cfb) resulting from prevailing westerlies, blowing from the high-pressure area in the central Atlantic towards Scandinavia. Snowfall occurs almost every winter. January and February are the coldest months of the year, with an average temperature of 0.1 °C and 0.4 °C. July and August are the hottest months, with an average temperature of 17.8 °C (63 °F) and 17.2 °C. The average annual precipitation is close to 438 mm with rain occurring usually from May to September. This precipitation is one of the lowest in Germany, which has an annual average close to 700 mm. In August 2010, Quedlinburg was the driest place in Germany, with only 72,4 l/m2.
Transport.
Air.
The nearest airports to Quedlinburg are Hannover, 120 km northwest, and Leipzig/Halle Airport, 90 km southeast. Much closer, but only served by a few airlines, is Magdeburg-Cochstedt. An airfield is located at Ballenstedt-Assmussstedt for general aviation. 
Train.
Regional trains run on the standard-gauge Magdeburg–Thale line by Deutsche Bahn and the private company Transdev connect Quedlinburg with Magdeburg, Thale, and Halberstadt.
In 2006, the Selke Valley branch of the Harz Narrow Gauge Railways was extended into Quedlinburg from Gernrode, giving access to the historic steam narrow-gauge railway, Alexisbad, and high Harz plateau.
Bus.
Quedlinburg is connected by regional buses to the surrounding villages and small towns. Additionally, buses to Berlin are run by the company BerlinLinienBus.
Notable people.
Jordanus of Quedlinburg, a 14th-century preacher and monk, wrote texts about contemporary devoutness.
In the 18th century, Dorothea Erxleben was the first female medical doctor in Germany. Born in 1715, she was the first women to receive a full M.D. from a German university (University of Halle), with the help of Frederick the Great. Trained originally by her father, the town's physician, she had been practicing as a physician, but without the Masters degree, until she was accused of witchcraft. She demanded a chance to defend her knowledge. Officials debated for a year over whether a woman so often pregnant could practice medicine. They finally allowed her to take the exams – after the birth of her fourth child – and she passed with flying colours.
Friedrich Gottlieb Klopstock was a German poet and contemporary of Johann Wolfgang von Goethe.
Sister cities.
Quedlinburg is twinned with:

</doc>
<doc id="25321" url="https://en.wikipedia.org/wiki?curid=25321" title="Quantization">
Quantization

__NOTOC__
Quantization is the procedure of constraining something from a continuous set of values (such as the real numbers) to a relatively small discrete set (such as the integers).

</doc>
<doc id="25322" url="https://en.wikipedia.org/wiki?curid=25322" title="Quantum theory">
Quantum theory

Quantum theory may mean:
In science:
In popular culture:

</doc>
<doc id="25323" url="https://en.wikipedia.org/wiki?curid=25323" title="QRP operation">
QRP operation

In amateur radio, QRP operation refers to transmitting at reduced power while attempting to maximize one's effective range. The term QRP derives from the standard Q code used in radio communications, where "QRP" and "QRP?" are used to request, "Reduce power", and ask "Should I reduce power?" respectively. The opposite of QRP is QRO, or high-power operation.
Philosophy.
Most amateurs use approximately 100 watts on HF and 50 watts on VHF/UHF , but in some parts of the world, like the U.S., they can use up to 1,500 watts. QRP enthusiasts contend that this is not always necessary, and doing so wastes power, increases the likelihood of causing interference to nearby televisions, radios, and telephones and, for United States' amateurs, is incompatible with FCC Part 97 rule, which states that one must use "the minimum power necessary to carry out the desired communications".
The current record for a QRP connection is 1 µW for 1,650 miles on 10-meter band.
Practice.
There is not complete agreement on what constitutes QRP power. While most QRP enthusiasts agree that for CW, AM, FM, and data modes, the transmitter output power should be 5 watts (or less), the maximum output power for SSB (single sideband) is not always agreed upon. Some believe that the power should be no more than 10 watts peak envelope power (PEP), while others strongly hold that the power limit should be 5 watts. QRPers are known to use even less than five watts, sometimes operating with as little as 100 milliwatts or even less. Extremely low power—1 watt and below—is often referred to by hobbyists as QRPp.
Communicating using QRP can be difficult since the QRPer must face the same challenges of radio propagation faced by amateurs using higher power levels, but with the inherent disadvantages associated with having a weaker signal on the receiving end, all other things being equal. QRP aficionados try to make up for this through more efficient antenna systems and enhanced operating skills.
QRP is especially popular with CW operators and those using the newer digital modes. PSK31 is a highly efficient, narrow-band mode that is very suitable to QRP operation.
QRSS.
Some extreme QRP enthusiasts use QRSS—transmitting extremely slowly—to compensate for the decreased signal-to-noise ratio involved in QRP operation.
QRSS derives from the standard Q code used in radio communications, where "QRS?" asks "Shall I send more slowly?" and "QRS" requests "Send more slowly".
Rather than directly listening to such slow transmissions, many QRSS enthusiasts record the transmission for later analysis, later decoding "by ear" while playing it back at much faster rates (time compression), or decoding "by eye" on the waterfall display of a spectrum analyzer.
QRSS enthusiasts typically use some form of Morse code, except much slower—rather than a typical second "dit" time, QRSS transmissions may use a full second for the "dit" time, or in extreme cases, a full minute for a single "dit" time.
A few people apply QRSS techniques to other narrow-band communication codes or protocols, such as the "Slowfeld" variant of Hellschreiber, slow-scan television, MT63, etc.
Equipment.
Many of the larger, more powerful commercial transceivers permit the operator to lower their output level to QRP levels. Commercial transceivers specially designed to operate at or near QRP power levels have been commercially available since the late 1960s. In 1969, American manufacturer, Ten-Tec, produced the Powermite-1. This radio was one of Ten-Tec's first assembled transceivers. (The MR-1 was available, and it was essentially the same radio, albeit in kit form.) This radio featured modular construction (all stages of the transceiver were on individual circuit boards): the transmitter was capable of about one or two watts of RF, and the receiver was a direct-conversion unit, similar to that found in the Heathkit HW-7 and HW-8 lines. Many amateurs became quite adept at QRP'ing through their use of these early, trend-setting radios . As QRP has become more popular in recent years , radio manufacturers have introduced radios specifically intended for the QRP enthusiast. Popular US models include Elecraft KX3, K2 and K1, the Yaesu Yaesu FT-817, the Icom IC-703, and the 516 Argonaut V and the new 539 Argonaut VI from TenTec. Another popular source is Hendricks QRP Kits, which offers a variety of popular kits. Enthusiasts operate QRP radios on the HF bands in portable modes, usually carrying the radios in backpacks, with whip antennas. Some QRPers prefer to construct their equipment from kits or homebrew it from scratch. Many popular designs are based on the NE612 mixer IC, i.e. the K1, K2, ATS series and the Softrock SDR.
Contests and awards.
There are specific operating awards, contests, clubs, and conventions devoted to QRP enthusiasts.
In the USA, the November Sweepstakes, June and September VHF QSO Parties, January VHF Sweepstakes, and the ARRL International DX Contest, as well as many major international contests have designated special QRP categories. For example, during the annual ARRL's Field Day contest, making a QSO (ham-to-ham contact) using "QRP battery power" is worth five times as many points as a contact made by conventional means.
The QRP ARCI club sponsors 12 contests during the year specifically for QRP operators. QRP-ARCI Contests
Typical awards include the QRP ARCI club's "thousand-miles-per-watt" award, available to anyone presenting evidence of a qualifying contact. QRP ARCI also offers special awards for achieving the ARRL's Worked All States, Worked All Continents, and DX Century Club awards under QRP conditions. Other QRP clubs also offer similar versions of these awards, as well as general QRP operating achievement awards.

</doc>
<doc id="25327" url="https://en.wikipedia.org/wiki?curid=25327" title="QCD (disambiguation)">
QCD (disambiguation)

The initialism QCD may refer to:

</doc>
<doc id="25328" url="https://en.wikipedia.org/wiki?curid=25328" title="Quicksilver">
Quicksilver

Quicksilver is the chemical element mercury.
Quicksilver also may refer to

</doc>
<doc id="25330" url="https://en.wikipedia.org/wiki?curid=25330" title="Quartet">
Quartet

In music, a quartet or quartette (, , , , ) is an ensemble of four singers or instrumental performers; or a musical composition for four voices or instruments.
Classical.
String quartet.
In Classical music, the most important combination of four instruments in chamber music is the string quartet. String quartets most often consist of two violins, a viola, and a cello. The particular choice and number of instruments derives from the registers of the human voice: soprano, alto, tenor and bass. In the string quartet, two violins play the soprano and alto vocal registers, the viola plays the tenor register and the cello plays the bass register. 
Composers of notable string quartets include Joseph Haydn (68 compositions), Wolfgang Amadeus Mozart (23), Ludwig van Beethoven (17), Felix Mendelssohn (6), Franz Schubert (15), Johannes Brahms (3), Antonín Dvořák (14), Alexander Borodin (2), Béla Bartók (6), and Dmitri Shostakovich (15). The Italian composer Luigi Boccherini (1743–1805), wrote more than 100 string quartets. 
Less often, string quartets are written for other combinations of the standard string ensemble. These include quartets for one violin, two violas, and one cello, notably by Carl Stamitz (6 compositions) and others; and for one violin, one viola, and two cellos, by Johann Georg Albrechtsberger and others.
Piano quartet.
Another common standard classical quartet is the piano quartet, consisting of violin, viola, cello, and piano. Romantic composers Beethoven, Brahms, and Mendelssohn each wrote three important compositions in this form, and Mozart, Dvořák, and Gabriel Fauré each wrote two.
Other instrumental quartets.
Wind quartets are scored either the same as a string quartet with the wind instrument replacing the first violin (i.e. scored for wind, violin, viola and cello) or are groups of four wind instruments. Among the latter, the SATB format woodwind quartet of flute, oboe, clarinet, and bassoon is relatively common.
An example of a wind quartet featuring four of the same types of wind instruments is the saxophone quartet, consisting of soprano saxophone, alto saxophone, tenor saxophone and baritone saxophone or (SATB). Often a second alto may be substituted for the soprano part (AATB) or a bass saxophone may be substituted for the baritone.
Vocal quartet.
Compositions for four singers have been written for quartets a cappella; accompanied by instruments, such as a piano; and accompanied by larger vocal forces, such as a choir. Brahms and Schubert wrote numerous pieces for four voices that were once popular in private salons, although they are seldom performed today. Vocal quartets also feature within larger classical compositions, such as opera, choral works, and symphonic compositions. The final movement of Beethoven's Ninth Symphony and the Verdi Requiem are two examples of renowned concert works that include vocal quartets.
Typically, a vocal quartet is composed of:
Baroque quartet.
The baroque quartet is a form of music composition similar to the trio sonata, but with four music parts performed by three solo melodic instruments and basso continuo. The solo instruments could be strings or wind instruments.
Examples of baroque quartets are Telemann's Paris quartets.
Jazz.
Quartets are popular in jazz and jazz fusion music. Jazz quartet ensembles are often composed of a horn (e.g., saxophone, trumpet, etc.), a chordal instrument (e.g., electric guitar, piano, Hammond organ, etc.), a bass instrument (e.g., double bass or bass guitar) and a drum kit. This configuration is sometimes modified by using a second horn replacing the chordal instrument, such as a trumpet and saxophone with string bass and drum kit, popularized by Miles Davis, or by using two chordal instruments (e.g., piano and electric guitar).
Popular music.
In 20th century Western popular music, the term "vocal quartet" usually refers to ensembles of four singers of the same gender. This is particularly common for barbershop quartets and Gospel quartets. 
Some well-known female US vocal quartets include The Carter Sisters; The Forester Sisters; The Chiffons; The Chordettes; The Lennon Sisters; and En Vogue. Some well-known male US vocal quartets include The Statler Brothers; The Ames Brothers; The Chi-Lites; Crosby Stills Nash & Young; The Dixie Hummingbirds; The Four Aces; Four Freshmen; The Four Seasons; The Four Tops; The Cathedral Quartet; Ernie Haase and Signature Sound; The Golden Gate Quartet; The Hilltoppers; The Jordanaires; Mills Brothers; The Rascals; and The Skylarks. The only known U.S. drag quartet is The Kinsey Sicks. Some mixed-gender vocal quartets include The Pied Pipers; The Mamas & the Papas; The Merry Macs; and The Weavers.
The quartet lineup also is very common in pop and rock music. A standard quartet formation in pop and rock music is an ensemble consisting of two electric guitars, a bass guitar, and a drum kit. This configuration is sometimes modified by using a keyboard instrument (e.g., organ, piano, synthesizer) or a soloing instrument (e.g., saxophone) in place of the second electric guitar.

</doc>
<doc id="25336" url="https://en.wikipedia.org/wiki?curid=25336" title="Quantum entanglement">
Quantum entanglement

Quantum entanglement is a physical phenomenon that occurs when pairs or groups of particles are generated or interact in ways such that the quantum state of each particle cannot be described independently — instead, a quantum state must be described for the system as a whole.
Measurements of physical properties such as position, momentum, spin, polarization, etc., performed on entangled particles are found to be appropriately correlated. For example, if a pair of particles are generated in such a way that their total spin is known to be zero, and one particle is found to have clockwise spin on a certain axis, then the spin of the other particle, measured on the same axis, will be found to be counterclockwise, as to be expected due to their entanglement. However, this behavior gives rise to paradoxical effects: any measurement of a property of a particle can be seen as acting on that particle (e.g., by collapsing a number of superposed states) and will change the original quantum property by some unknown amount; and in the case of entangled particles, such a measurement will be on the entangled system as a whole. It thus appears that one particle of an entangled pair "knows" what measurement has been performed on the other, and with what outcome, even though there is no known means for such information to be communicated between the particles, which at the time of measurement may be separated by arbitrarily large distances.
Such phenomena were the subject of a 1935 paper by Albert Einstein, Boris Podolsky, and Nathan Rosen, and several papers by Erwin Schrödinger shortly thereafter, describing what came to be known as the EPR paradox. Einstein and others considered such behavior to be impossible, as it violated the local realist view of causality (Einstein referring to it as "spooky action at a distance") and argued that the accepted formulation of quantum mechanics must therefore be incomplete. Later, however, the counterintuitive predictions of quantum mechanics were verified experimentally. Experiments have been performed involving measuring the polarization or spin of entangled particles in different directions, which — by producing violations of Bell's inequality — demonstrate statistically that the local realist view cannot be correct. This has been shown to occur even when the measurements are performed more quickly than light could travel between the sites of measurement: there is no lightspeed or slower influence that can pass between the entangled particles. Recent experiments have measured entangled particles within less than one one-hundredth of a percent of the travel time of light between them. According to the formalism of quantum theory, the effect of measurement happens instantly. It is not possible, however, to use this effect to transmit classical information at faster-than-light speeds (see Faster-than-light § Quantum mechanics).
Quantum entanglement is an area of extremely active research by the physics community, and its effects have been demonstrated experimentally with photons, electrons, molecules the size of buckyballs, and even small diamonds. Research is also focused on the utilization of entanglement effects in communication and computation.
History.
The counterintuitive predictions of quantum mechanics about strongly correlated systems were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen. In this study, they formulated the EPR paradox (Einstein, Podolsky, Rosen paradox), a thought experiment that attempted to show that quantum mechanical theory was incomplete. They wrote: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."
However, they did not coin the word "entanglement", nor did they generalize the special properties of the state they considered. Following the EPR paper, Erwin Schrödinger wrote a letter (in German) to Einstein in which he used the word "Verschränkung" (translated by himself as "entanglement") "to describe the correlations between two particles that interact and then separate, as in the EPR experiment." He shortly thereafter published a seminal paper defining and discussing the notion, and terming it "entanglement." In the paper he recognized the importance of the concept, and stated: "I would not call "one" but rather "the" characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought."
Like Einstein, Schrödinger was dissatisfied with the concept of entanglement, because it seemed to violate the speed limit on the transmission of information implicit in the theory of relativity. Einstein later famously derided entanglement as ""spukhafte Fernwirkung"" or "spooky action at a distance."
The EPR paper generated significant interest among physicists and inspired much discussion about the foundations of quantum mechanics (perhaps most famously Bohm's interpretation of quantum mechanics), but produced relatively little other published work. So, despite the interest, the weak point in EPR's argument was not discovered until 1964, when John Stewart Bell proved that one of their key assumptions, the principle of locality, which underlies the kind of hidden variables interpretation hoped for by EPR, was mathematically inconsistent with the predictions of quantum theory. Specifically, he demonstrated an upper limit, seen in Bell's inequality, regarding the strength of correlations that can be produced in any theory obeying local realism, and he showed that quantum theory predicts violations of this limit for certain entangled systems. His inequality is experimentally testable, and there have been numerous relevant experiments, starting with the pioneering work of Stuart Freedman and John Clauser in 1972 and Alain Aspect's experiments in 1982. They have all shown agreement with quantum mechanics rather than the principle of local realism. However, the issue is not finally settled, as each of these experimental tests has left open at least one loophole by which it is possible to question the validity of the results.
The work of Bell raised the possibility of using these super-strong correlations as a resource for communication. It led to the discovery of quantum key distribution protocols, most famously BB84 by Charles H. Bennett and Gilles Brassard and E91 by Artur Ekert. Although BB84 does not use entanglement, Ekert's protocol uses the violation of a Bell's inequality as a proof of security.
Concept.
Meaning of entanglement.
An entangled system is defined to be one whose quantum state cannot be factored as a product of states of its local constituents, that is to say, they are not individual particles but are an inseparable whole. If entangled, one constituent cannot be fully described without considering the other(s). Note that the state of a composite system is always expressible as a "sum", or superposition, of products of states of local constituents; it is entangled if this sum necessarily has more than one term.
Quantum systems can become entangled through various types of interactions. For some ways in which entanglement may be achieved for experimental purposes, see the section below on methods. Entanglement is broken when the entangled particles decohere through interaction with the environment; for example, when a measurement is made.
As an example of entanglement: a subatomic particle decays into an entangled pair of other particles. The decay events obey the various conservation laws, and as a result, the measurement outcomes of one daughter particle must be highly correlated with the measurement outcomes of the other daughter particle (so that the total momenta, angular momenta, energy, and so forth remains roughly the same before and after this process). For instance, a spin-zero particle could decay into a pair of spin-½ particles. Since the total spin before and after this decay must be zero (conservation of angular momentum), whenever the first particle is measured to be spin up on some axis, the other, when measured on the same axis, is always found to be spin down. (This is called the "spin anti-correlated" case; and if the prior probabilities for measuring each spin are equal, the pair is said to be in the singlet state.)
The special property of entanglement can be better observed if we separate the said two particles. Let's put one of them in the White House in Washington and the other in UC Berkeley (think about this as a thought experiment, not an actual one). Now, if we measure a particular characteristic of one of these particles (say, for example, spin), get a result, and then measure the other particle using the same criterion (spin along the same axis), we find that the result of the measurement of the second particle will match (in a complementary sense) the result of the measurement of the first particle, in that they will be opposite in their values. 
The above result may or may not be perceived as surprising. A classical system would display the same property, and a hidden variable theory (see below) would certainly be "required" to do so, based on conservation of angular momentum in classical and quantum mechanics alike. The difference is that a classical system has definite values for all the observables all along while the quantum system does not. In a sense to be discussed below, the quantum system considered here seems to "acquire" a probability distribution for the outcome of a measurement of the spin along "any" axis of the "other" particle upon measurement of the "first" particle. This probability distribution is in general "different" from what it would be "without" measurement of the first particle. This may certainly be perceived as surprising in the case of spatially separated entangled particles.
Paradox.
The paradox is that a measurement made on either of the particles apparently collapses the state of the entire entangled system — and does so instantaneously, before any information about the measurement result could have been communicated to the other particle (assuming that information cannot travel faster than light) and hence assured the "proper" outcome of the measurement of the other part of the entangled pair. In the quantum formalism, the result of a spin measurement on one of the particles is a collapse into a state in which each particle has a definite spin (either up or down) along the axis of measurement. The outcome is taken to be random, with each possibility having a probability of 50%. However, if both spins are measured along the same axis, they are found to be anti-correlated. This means that the random outcome of the measurement made on one particle seems to have been transmitted to the other, so that it can make the "right choice" when it too is measured. 
The distance and timing of the measurements can be chosen so as to make the interval between the two measurements spacelike, hence, a message connecting the events would have to travel faster than light. According to the principles of special relativity, it is not possible for any information to travel between two such measuring events. It is not even possible to unambiguously say which of the measurements came first. For two spacelike separated events and there are inertial systems in which is first and others in which is first. Therefore, the correlation between the two measurements cannot appropriately be explained as one measurement determining the other. Different observers would disagree about the role of cause and effect.
In conclusion, the result shows that it is quite impossible for the measurement of one particle's properties to determine the other's properties, even though it seems that way at first, as the speed of light is the universal speed limit, and nothing can travel faster than that speed.
Hidden variables theory.
A possible resolution to the paradox might be to assume that the state of the particles contains some hidden variables, whose values effectively determine, right from the moment of separation, what the outcomes of the spin measurements are going to be. This would mean that each particle carries all the required information with it, and nothing needs to be transmitted from one particle to the other at the time of measurement. It was originally believed by Einstein and others (see the previous section) that this was the only way out of the paradox, and therefore that the accepted quantum mechanical description (with a random measurement outcome) must be incomplete. (In fact similar paradoxes can arise even without entanglement: the position of a single particle is spread out over space, and two widely separated detectors attempting to detect the particle in two different places must instantaneously attain appropriate correlation, so that they do not "both" detect the particle.)
Violations of Bell's inequality.
The hidden variables theory fails, however, when we consider measurements of the spin of entangled particles along different axes (for example, along any of three axes which make angles of 120 degrees). If a large number of pairs of such measurements are made (on a large number of pairs of entangled particles), then statistically, if the local realist or hidden variables view were correct, the results would always satisfy Bell's inequality. A number of experiments have shown in practice that Bell's inequality is not satisfied. However, all experiments have loophole problems. When measurements of the entangled particles are made in moving relativistic reference frames, in which each measurement (in its own relativistic time frame) occurs before the other, the measurement results remain correlated.
The fundamental issue about measuring spin along different axes is that these measurements cannot have definite values at the same time―they are incompatible in the sense that these measurements' maximum simultaneous precision is constrained by the uncertainty principle. This is contrary to what is found in classical physics, where any number of properties can be measured simultaneously with arbitrary accuracy. It has been proven mathematically that compatible measurements cannot show Bell-inequality-violating correlations, and thus entanglement is a fundamentally non-classical phenomenon.
Other types of experiments.
In a 2012 experiment, "delayed-choice entanglement swapping" was used to decide whether two particles were entangled or not after they had been measured.
In a 2013 experiment, entanglement swapping has been used to create entanglement of photons that never coexisted in time, thus demonstrating that "the nonlocality of quantum mechanics, as manifested by entanglement, does not apply only to particles with spacelike separation, but also to particles with timelike temporal separation". What this means is that two particles can be entangled even if they are distanced from each other in time. Two entangled particles will thus show the property of entanglement even if they are measured in two different times.
In three independent experiments it was shown that classically-communicated separable quantum states can be used to carry entangled states.
In August 2014, researcher Gabriela Barreto Lemos and team were able to "take pictures" of objects using photons that have not interacted with the subjects, but were entangled with photons that did interact with such objects. Lemos, from the University of Vienna, is confident that this new quantum imaging technique could find application where low light imaging is imperative, in fields like biological or medical imaging.
Mystery of time.
There are physicists who say that time is an emergent phenomenon that is a side effect of quantum entanglement. 
In other words, time is an entanglement phenomenon, which places all equal clock readings (of correctly prepared clocks - or of any objects usable as clocks) into the same history. This was first understood by physicist Don Page and William Wootters in 1983. 
The Wheeler–DeWitt equation that combines general relativity and quantum mechanics — by leaving out time altogether — was introduced in the 1960s and it was taken up again in 1983, when the theorists Don Page and William Wootters made a solution based on the quantum phenomenon of entanglement. Page and Wootters argued that entanglement can be used to measure time.
In 2013, at the Istituto Nazionale di Ricerca Metrologica (INRIM) in Turin, Italy, researchers performed the first experimental test of Page and Wootters' ideas. Their result has been interpreted to confirm that time is an emergent phenomenon for internal observers but absent for external observers of the universe just as the Wheeler-DeWitt equation predicts.
Source for the arrow of time.
Physicist Seth Lloyd says that quantum uncertainty gives rise to "entanglement", the putative source of the arrow of time. According to Lloyd; "The arrow of time is an arrow of increasing correlations." The approach to entanglement would be from the perspective of the causal arrow of time, with the assumption that the cause of the measurement of one particle determines the effect of the result of the other particle's measurement.
Non-locality and hidden variables.
There is much confusion about the meaning of entanglement, non-locality and hidden variables and how they relate to each other. As described above, entanglement is an experimentally verified and accepted property of nature, which has critical implications for the interpretations of quantum mechanics. Entanglement is also mathematically well-defined (see below). The question becomes, "How can one account for something that was at one point indefinite with regard to its spin (or whatever is in this case the subject of investigation) suddenly becoming definite in that regard even though no physical interaction with the second object occurred, and, if the two objects are sufficiently far separated, could not even have had the time needed for such an interaction to proceed from the first to the second object?" The latter question involves the issue of locality, i.e. does the agent of change have to be in physical contact (at least via some intermediary such as a field force), for a change to occur in some other thing? The study of entanglement brings into sharp focus the dilemma between locality and the completeness or lack of completeness of quantum mechanics.
Bell's theorem and related results rule out a local realistic explanation for quantum mechanics (one which obeys the principle of locality while also ascribing definite values to quantum observables). However, in other interpretations, the experiments that demonstrate the apparent non-locality can also be described in local terms: If each distant observer regards the other as a quantum system, communication between the two must then be treated as a measurement process, and this communication is strictly local. In particular, in the Many-worlds interpretation, the underlying description is fully local. More generally, the question of locality in quantum physics is extraordinarily subtle and sometimes hinges on precisely how it is defined.
In the media and popular science, quantum non-locality is often portrayed as being equivalent to entanglement. While it is true that a bipartite quantum state must be entangled in order for it to produce non-local correlations, there exist entangled states that do not produce such correlations. A well-known example of this is the Werner state that is entangled for certain values of formula_1, but can always be described using local hidden variables. In short, entanglement of a two-party state is necessary but not sufficient for that state to be non-local. Moreover, it was shown that, for arbitrary number of party, there exist states that are genuinely entangled but admits a fully local strategy. It is important to recognize that entanglement is more commonly viewed as an algebraic concept, noted for being a precedent to non-locality as well as to quantum teleportation and to superdense coding, whereas non-locality is defined according to experimental statistics and is much more involved with the foundations and interpretations of quantum mechanics.
Quantum mechanical framework.
The following subsections are for those with a good working knowledge of the formal, mathematical description of quantum mechanics, including familiarity with the formalism and theoretical framework developed in the articles: bra–ket notation and mathematical formulation of quantum mechanics.
Pure states.
Consider two noninteracting systems and , with respective Hilbert spaces and . The Hilbert space of the composite system is the tensor product
If the first system is in state formula_3 and the second in state formula_4, the state of the composite system is
States of the composite system which can be represented in this form are called "separable states", or (in the simplest case) "product states".
Not all states are separable states (and thus product states). Fix a basis formula_6 for and a basis formula_7 for . The most general state in is of the form
This state is separable if there exist vectors formula_9 yielding formula_10 and formula_11 It is inseparable if for any vectors formula_12 at least for one pair of coordinates formula_13 we have formula_14 If a state is inseparable, it is called an "entangled state".
For example, given two basis vectors formula_15 of and two basis vectors formula_16 of , the following is an entangled state:
If the composite system is in this state, it is impossible to attribute to either system or system a definite pure state. Another way to say this is that while the von Neumann entropy of the whole state is zero (as it is for any pure state), the entropy of the subsystems is greater than zero. In this sense, the systems are "entangled". This has specific empirical ramifications for interferometry. It is worthwhile to note that the above example is one of four Bell states, which are (maximally) entangled pure states (pure states of the space, but which cannot be separated into pure states of each and ).
Now suppose Alice is an observer for system , and Bob is an observer for system . If in the entangled state given above Alice makes a measurement in the formula_18 eigenbasis of , there are two possible outcomes, occurring with equal probability:
If the former occurs, then any subsequent measurement performed by Bob, in the same basis, will always return 1. If the latter occurs, (Alice measures 1) then Bob's measurement will return 0 with certainty. Thus, system has been altered by Alice performing a local measurement on system . This remains true even if the systems and are spatially separated. This is the foundation of the EPR paradox.
The outcome of Alice's measurement is random. Alice cannot decide which state to collapse the composite system into, and therefore cannot transmit information to Bob by acting on her system. Causality is thus preserved, in this particular scheme. For the general argument, see no-communication theorem.
Ensembles.
As mentioned above, a state of a quantum system is given by a unit vector in a Hilbert space. More generally, if one has a large number of copies of the same system, then the state of this "ensemble" is described by a density matrix, which is a positive-semidefinite matrix, or a trace class when the state space is infinite-dimensional, and has trace 1. Again, by the spectral theorem, such a matrix takes the general form:
where the "w"i are positive-valued probabilities (they sum up to 1), the vectors are unit vectors, and in the infinite-dimensional case, we would take the closure of such states in the trace norm. We can interpret as representing an ensemble where is the proportion of the ensemble whose states are formula_22. When a mixed state has rank 1, it therefore describes a "pure ensemble". When there is less than total information about the state of a quantum system we need density matrices to represent the state.
Experimentally, a mixed ensemble might be realized as follows. Consider a "black box" apparatus that spits electrons towards an observer. The electrons' Hilbert spaces are identical. The apparatus might produce electrons that are all in the same state; in this case, the electrons received by the observer are then a pure ensemble. However, the apparatus could produce electrons in different states. For example, it could produce two populations of electrons: one with state formula_23 with spins aligned in the positive direction, and the other with state formula_24 with spins aligned in the negative direction. Generally, this is a mixed ensemble, as there can be any number of populations, each corresponding to a different state.
Following the definition above, for a bipartite composite system, mixed states are just density matrices on . That is, it has the general form
where the "w"i are positively valued probabilities, formula_26, and the vectors are unit vectors. This is self-adjoint and positive and has trace 1.
Extending the definition of separability from the pure case, we say that a mixed state is separable if it can be written as
where the are positively valued probabilities and the formula_28's and formula_29's are themselves mixed states (density operators) on the subsystems and respectively. In other words, a state is separable if it is a probability distribution over uncorrelated states, or product states. By writing the density matrices as sums of pure ensembles and expanding, we may assume without loss of generality that formula_28 and formula_29 are themselves pure ensembles. A state is then said to be "entangled" if it is not separable.
In general, finding out whether or not a mixed state is entangled is considered difficult. The general bipartite case has been shown to be NP-hard. For the and cases, a necessary and sufficient criterion for separability is given by the famous Positive Partial Transpose (PPT) condition.
Reduced density matrices.
The idea of a reduced density matrix was introduced by Paul Dirac in 1930. Consider as above systems and each with a Hilbert space . Let the state of the composite system be
As indicated above, in general there is no way to associate a pure state to the component system . However, it still is possible to associate a density matrix. Let
which is the projection operator onto this state. The state of is the partial trace of over the basis of system :
For example, the reduced density matrix of for the entangled state
discussed above is
This demonstrates that, as expected, the reduced density matrix for an entangled pure ensemble is a mixed ensemble. Also not surprisingly, the density matrix of for the pure product state formula_37 discussed above is
In general, a bipartite pure state ρ is entangled if and only if its reduced states are mixed rather than pure.
Two applications that use them.
Reduced density matrices were explicitly calculated in different spin chains with unique ground state. An example is the one-dimensional AKLT spin chain: the ground state can be divided into a block and an environment. The reduced density matrix of the block is proportional to a projector to a degenerate ground state of another Hamiltonian.
The reduced density matrix also was evaluated for XY spin chains, where it has full rank. It was proved that in the thermodynamic limit, the spectrum of the reduced density matrix of a large block of spins is an exact geometric sequence in this case.
Entropy.
In this section, the entropy of a mixed state is discussed as well as how it can be viewed as a measure of quantum entanglement.
Definition.
In classical information theory, the Shannon entropy, is associated to a probability distribution,formula_39, in the following way:
Since a mixed state is a probability distribution over an ensemble, this leads naturally to the definition of the von Neumann entropy:
In general, one uses the Borel functional calculus to calculate a non-polynomial function such as . If the nonnegative operator acts on a finite-dimensional Hilbert space and has eigenvalues formula_42, turns out to be nothing more than the operator with the same eigenvectors, but the eigenvalues formula_43. The Shannon entropy is then:
Since an event of probability 0 should not contribute to the entropy, and given that
the convention is adopted. This extends to the infinite-dimensional case as well: if has spectral resolution
assume the same convention when calculating
As in statistical mechanics, the more uncertainty (number of microstates) the system should possess, the larger the entropy. For example, the entropy of any pure state is zero, which is unsurprising since there is no uncertainty about a system in a pure state. The entropy of any of the two subsystems of the entangled state discussed above is (which can be shown to be the maximum entropy for mixed states).
As a measure of entanglement.
Entropy provides one tool which can be used to quantify entanglement, although other entanglement measures exist. If the overall system is pure, the entropy of one subsystem can be used to measure its degree of entanglement with the other subsystems.
For bipartite pure states, the von Neumann entropy of reduced states is the unique measure of entanglement in the sense that it is the only function on the family of states that satisfies certain axioms required of an entanglement measure.
It is a classical result that the Shannon entropy achieves its maximum at, and only at, the uniform probability distribution {1/"n"...,1/"n"}. Therefore, a bipartite pure state is said to be a maximally entangled state if the reduced state of is the diagonal matrix
For mixed states, the reduced von Neumann entropy is not the only reasonable entanglement measure.
As an aside, the information-theoretic definition is closely related to entropy in the sense of statistical mechanics (comparing the two definitions, we note that, in the present context, it is customary to set the Boltzmann constant ). For example, by properties of the Borel functional calculus, we see that for any unitary operator ,
Indeed, without this property, the von Neumann entropy would not be well-defined.
In particular, could be the time evolution operator of the system, i.e.,
where is the Hamiltonian of the system. Here the entropy is unchanged.
The reversibility of a process is associated with the resulting entropy change, i.e., a process is reversible if, and only if, it leaves the entropy of the system invariant. Therefore, the march of the arrow of time towards thermodynamic equilibrium is simply the growing spread of quantum entanglement.
This provides a connection between quantum information theory and thermodynamics.
Rényi entropy also can be used as a measure of entanglement.
Entanglement measures.
Entanglement measures quantify the amount of entanglement in a (often viewed as a bipartite) quantum state. As aforementioned, entanglement entropy is the standard measure of entanglement for pure states (but no longer a measure of entanglement for mixed states). For mixed states, there are some entanglement measures in the literature and no single one is standard.
Most (but not all) of these entanglement measures reduce for pure states to entanglement entropy, and are difficult (NP-hard) to compute.
Quantum field theory.
The Reeh-Schlieder theorem of quantum field theory is sometimes seen as an analogue of quantum entanglement.
Applications.
Entanglement has many applications in quantum information theory. With the aid of entanglement, otherwise impossible tasks may be achieved.
Among the best-known applications of entanglement are superdense coding and quantum teleportation.
Most researchers believe that entanglement is necessary to realize quantum computing (although this is disputed by some).
Entanglement is used in some protocols of quantum cryptography. This is because the "shared noise" of entanglement makes for an excellent one-time pad. Moreover, since measurement of either member of an entangled pair destroys the entanglement they share, entanglement-based quantum cryptography allows the sender and receiver to more easily detect the presence of an interceptor.
In interferometry, entanglement is necessary for surpassing the standard quantum limit and achieving the Heisenberg limit.
Entangled states.
There are several canonical entangled states that appear often in theory and experiments.
For two qubits, the Bell states are
These four pure states are all maximally entangled (according to the entropy of entanglement) and form an orthonormal basis (linear algebra) of the Hilbert space of the two qubits. They play a fundamental role in Bell's theorem.
For M>2 qubits, the GHZ state is
which reduces to the Bell state formula_54 for formula_55. The traditional GHZ state was defined for formula_56. GHZ states are occasionally extended to "qudits", i.e., systems of "d" rather than 2 dimensions.
Also for M>2 qubits, there are spin squeezed states. Spin squeezed states are a class of squeezed coherent states satisfying certain restrictions on the uncertainty of spin measurements, and are necessarily entangled. Spin squeezed states are good candidates for enhancing precision measurements using quantum entanglement.
For two bosonic modes, a NOON state is
This is like a Bell state formula_54 except the basis kets 0 and 1 have been replaced with "the "N" photons are in one mode" and "the "N" photons are in the other mode".
Finally, there also exist twin Fock states for bosonic modes, which can be created by feeding a Fock state into two arms leading to a beam splitter. They are the sum of multiple of NOON states, and can used to achieve the Heisenberg limit.
For the appropriately chosen measure of entanglement, Bell, GHZ, and NOON states are maximally entangled while spin squeezed and twin Fock states are only partially entangled. The partially entangled states are generally easier to prepare experimentally.
Methods of creating entanglement.
Entanglement is usually created by direct interactions between subatomic particles. These interactions can take numerous forms. One of the most commonly used methods is spontaneous parametric down-conversion to generate a pair of photons entangled in polarisation. Other methods include the use of a fiber coupler to confine and mix photons, the use of quantum dots to trap electrons until decay occurs, the use of the Hong-Ou-Mandel effect, etc., In the earliest tests of Bell's theorem, the entangled particles were generated using atomic cascades.
It is also possible to create entanglement between quantum systems that never directly interacted, through the use of entanglement swapping.
Testing a system for entanglement.
Systems which contain no entanglement are said to be separable. For 2-Qubit and Qubit-Qutrit systems (2 × 2 and 2 × 3 respectively) the simple Peres–Horodecki criterion provides both a necessary and a sufficient criterion for separability, and thus for detecting entanglement. However, for the general case, the criterion is merely a sufficient one for separability, as the problem becomes NP-hard. A numerical approach to the problem is suggested by Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement". Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, and checking if the target state can indeed be reached. An implementation of the algorithm (including a built in Peres-Horodecki criterion testing) is brought in the "StateSeparator" web-app.
Naturally entangled systems.
The electron shell of multi-electron atoms always consists of entangled electrons.
The correct ionization energy can be calculated only by consideration of electron 
entanglement.
It has been shown by femtosecond transition spectroscopy, that in the photosystem of plants,
entangled photons exist. An efficient conversion of the photon energy into 
chemical energy is possible only due to this entanglement.

</doc>
<doc id="25343" url="https://en.wikipedia.org/wiki?curid=25343" title="Quasi-War">
Quasi-War

The Quasi-War () was an undeclared war fought almost entirely at sea between the United States of America and the French Republic from 1798 to 1800. After the toppling of the French crown during its revolutionary wars, the United States refused to continue repaying its debt to France on the grounds that it had been owed to a previous regime. French outrage led to a series of attacks on American shipping, ultimately leading to retaliation from the Americans and the end of hostilities with the signing of the Convention of 1800 shortly thereafter.
Background.
The Kingdom of France had been a crucial ally of the United States in the American Revolutionary War since the spring of 1776, and had signed in 1778 a treaty of alliance with the United States of America against Great Britain. But in 1794, after the French Revolution toppled that country's monarchy, the American government came to an agreement with the Kingdom of Great Britain, the Jay Treaty, that resolved several points of contention between the United States and Great Britain that had lingered after the end of the American Revolutionary War. It also contained economic clauses.
The United States had already declared neutrality in the conflict between Great Britain and revolutionary France, and American legislation was being passed for a trade deal with Britain. When the U.S. refused to continue repaying its debt using the argument that the debt was owed to the previous government, not to the French First Republic, French outrage led to a series of responses. First, French privateers began seizing American ships trading with Britain and bringing them in as prizes to be sold. Next, the French government refused to receive Charles Cotesworth Pinckney, the new U.S. Minister, when he arrived in Paris in December 1796. In his annual message to Congress at the close of 1797, President John Adams reported on France's refusal to negotiate a settlement and spoke of the need "to place our country in a suitable posture of defense." In April 1798, President Adams informed Congress of the "XYZ Affair", in which French agents demanded a large bribe before engaging in substantive negotiations with United States diplomats.
Meanwhile, the French Navy was inflicting substantial losses on American shipping. On 21 February 1797, Secretary of State Timothy Pickering told Congress that during the previous eleven months, France had seized 316 American merchant ships. French marauders now cruised the length of the Atlantic seaboard virtually unopposed. The United States government had nothing to combat them, as the Navy had been abolished at the end of the Revolutionary War and its last warship was sold in 1785. The United States had only a flotilla of small revenue cutters and a few somewhat neglected coastal forts.
Increased depredations by French privateers led to the rebirth of the United States Navy and the creation of the United States Marine Corps to defend the expanding American merchant fleet. Congress authorized the president to acquire, arm, and man not more than 12 ships of up to 22 guns each. Several merchantmen were immediately purchased and refitted as ships of war, and construction of the frigate resumed.
Congress rescinded the treaties with France on 7 July 1798; that date is now considered as the beginning of the Quasi-War. This was followed two days later with the passage of the Congressional authorization of attacks on French warships in American waters.
Naval engagements.
The U.S. Navy operated with a battle fleet of about 25 vessels, which patrolled the southern coast of the United States and throughout the Caribbean, hunting down French privateers. Captain Thomas Truxtun's insistence on the highest standards of crew training paid dividends when the frigate captured the French Navy's frigate "L'Insurgente" and severely damaged the frigate "La Vengeance". French privateers generally resisted, as did "La Croyable", which was captured on 7 July 1798, by outside of Egg Harbor, New Jersey. captured eight privateers and freed 11 American merchant ships from captivity. captured the French privateers "Deux Amis" and "Diane". Numerous American merchantmen were recaptured by "Experiment". forced "Le Berceau" into submission. Silas Talbot engineered an expedition to Puerto Plata harbor in the Colony of Santo Domingo, a possession of France's ally Spain, on 11 May 1800. Sailors and marines from under Lieutenant Isaac Hull captured the French privateer "Sandwich" in the harbor and spiked the guns of the Spanish fort.
The U.S. Navy lost only one ship to the French, , which was later recaptured. She was the captured privateer "La Croyable", recently purchased by the U.S. Navy. "Retaliation" departed Norfolk on 28 October 1798, with and , and cruised in the West Indies protecting American commerce. On 20 November 1798, the French frigates "L’Insurgente" and "Volontaire" overtook "Retaliation" while her consorts were away and forced commanding officer Lieutenant William Bainbridge to surrender the out-gunned schooner. "Montezuma" and "Norfolk" escaped after Bainbridge convinced the senior French commander that those American warships were too powerful for his frigates and persuaded him to abandon the chase. Renamed "Magicienne" by the French, the schooner again came into American hands on 28 June, when a broadside from forced her to haul down her colors.
Revenue cutters in the service of the United States Revenue-Marine, the predecessor to the United States Coast Guard, also took part in the conflict. The cutter USRC "Pickering", commanded by Edward Preble, made two cruises to the West Indies and captured 10 prizes. Preble turned command of "Pickering" over to Benjamin Hillar, who captured the much larger and more heavily armed French privateer "lEgypte Conquise" after a nine-hour battle. In September 1800, Hillar, "Pickering", and her entire crew were lost at sea in a storm. Preble next commanded the frigate "Essex", which he sailed around Cape Horn into the Pacific to protect American merchantmen in the East Indies. He recaptured several American ships that had been seized by French privateers.
American naval losses may have been light, but the French had successfully seized many American merchant ships by the war's end in 1800—more than 2,000, according to one source.
Although they were fighting the same enemy, the Royal Navy and the United States Navy did not cooperate operationally or share operational plans. There were no mutual understandings about deployment between their forces. However, the British did sell naval stores and munitions to the American government. In addition, the two navies shared a signal system so they could recognize the other's warships at sea and allowed their merchantmen to join each other's convoys for safety.
Conclusion of hostilities.
By the autumn of 1800, the United States Navy and the Royal Navy, combined with a more conciliatory diplomatic stance by the government of First Consul Napoleon Bonaparte, had reduced the activity of the French privateers and warships. The Convention of 1800, signed on 30 September, ended the Franco-American War. Unfortunately for President Adams, the news did not arrive in time to help him secure a second term in the 1800 presidential election.

</doc>
<doc id="25345" url="https://en.wikipedia.org/wiki?curid=25345" title="Quality management system">
Quality management system

A quality management system (QMS) is a collection of business processes focused on consistently meeting customer requirements and enhancing their satisfaction. It is expressed as the organizational structure, policies, procedures, processes and resources needed to implement quality management. Early systems emphasized predictable outcomes of an industrial product production line, using simple statistics and random sampling. By the 20th century, labour inputs were typically the most costly inputs in most industrialized societies, so focus shifted to team cooperation and dynamics, especially the early signalling of problems via a continuous improvement cycle. In the 21st century, QMS has tended to converge with sustainability and transparency initiatives, as both investor and customer satisfaction and perceived quality is increasingly tied to these factors. Of QMS regimes, the ISO 9000 family of standards is probably the most widely implemented worldwide - the ISO 19011 audit regime applies to both, and deals with quality and sustainability and their integration.
Other QMS, e.g. Natural Step, focus on sustainability issues and assume that other quality problems will be reduced as result of the systematic thinking, transparency, documentation and diagnostic discipline.
Concept of quality - historical background.
The concept of quality as we think of it now first emerged from the Industrial Revolution. Previously goods had been made from start to finish by the same person or team of people, with handcrafting and tweaking the product to meet 'quality criteria'. Mass production brought huge teams of people together to work on specific stages of production where one person would not necessarily complete a product from start to finish. In the late 19th century pioneers such as Frederick Winslow Taylor and Henry Ford recognized the limitations of the methods being used in mass production at the time and the subsequent varying quality of output. Birland established Quality Departments to oversee the quality of production and rectifying of errors, and Ford emphasized standardization of design and component standards to ensure a standard product was produced, Management of quality was the responsibility of the Quality department and was implemented by Inspection of product output to 'catch' defects.
Application of statistical control came later as a result of World War production methods, and were advanced by the work done of W. Edwards Deming, a statistician, after whom the Deming Prize for quality is named. Joseph M. Juran focused more on managing for quality. The first edition of Juran's Quality Control Handbook was published in 1951. He also developed the "Juran's trilogy," an approach to cross-functional management that is composed of three managerial processes: quality planning, quality control and quality improvement. These functions all play a vital role when evaluating quality.
Quality, as a profession and the managerial process associated with the quality function, was introduced during the second-half of the 20th century, and has evolved since then. Over this period, few other disciplines have seen as many changes as the quality profession.
The quality profession grew from simple control, to engineering, to systems engineering. Quality control activities were predominant in the 1940s, 1950s, and 1960s. The 1970s were an era of quality engineering and the 1990s saw quality systems as an emerging field. Like medicine, accounting, and engineering, quality has achieved status as a recognized profession
As Lee and Dale (1998) state, there are many organisations that are striving to assess the methods and ways in which their overall productivity, the quality of their products and services and the required operations to achieve them are done.
Quality system for medical devices.
ISO 13485 is considered state of the art for medical device manufacturers QMS and related services. The standard is harmonised in the EU to the medical devices directive (93/42/EEC) as well as the IVD and AIMD directives; the standard is also used in other jurisdictions such as Japan (JPAL) and Canada (via the CMDCAS scheme). 
Quality System requirements for medical devices have been internationally recognized as a way to assure product safety and efficacy and customer satisfaction since at least 1983, and were instituted as requirements in a final rule published on October 7, 1996. The U.S. Food and Drug Administration (FDA) had documented design defects in medical devices that contributed to recalls from 1983 to 1989 that would have been prevented if Quality Systems had been in place. The rule is promulgated at 21 CFR 820.
According to current Good Manufacturing Practice (GMP), medical device manufacturers have the responsibility to use good judgment when developing their quality system and apply those sections of the FDA Quality System (QS) Regulation that are applicable to their specific products and operations, in Part 820 of the QS regulation. As with GMP, operating within this flexibility, it is the responsibility of each manufacturer to establish requirements for each type or family of devices that will result in devices that are safe and effective, and to establish methods and procedures to design, produce, and distribute devices that meet the quality system requirements.
The FDA has identified in the QS regulation the essential elements that a quality system shall embody for design, production and distribution, without prescribing specific ways to establish these elements. These elements include:
all overseen by management and quality audits.
Because the QS regulation covers a broad spectrum of devices and production processes, it allows some leeway in the details of quality system elements. It is left to manufacturers to determine the necessity for, or extent of, some quality elements and to develop and implement procedures tailored to their particular processes and devices. For example, if it is impossible to mix up labels at a manufacturer because there is only one label to each product, then there is no necessity for the manufacturer to comply with all of the GMP requirements under device labeling.
Drug manufactures are regulated under a different section of the Code of Federal Regulations:
Quality management organizations and awards.
The International Organization for Standardization's ISO 9001:2008 series describes standards for a QMS addressing the principles and processes surrounding the design, development and delivery of a general product or service. Organizations can participate in a continuing certification process to ISO 9001:2008 to demonstrate their compliance with the standard, which includes a requirement for continual (i.e. planned) improvement of the QMS, as well as more foundational QMS components such as failure mode and effects analysis (FMEA). 
(ISO 9000:2005 provides information on the fundamentals and vocabulary used in quality management systems. ISO 9004:2009 provides guidance on quality management approach for the sustained success of an organization. Neither of these standards can be used for certification purposes as they provide guidance, not requirements).
The Baldrige Performance Excellence Program educates organizations in improving their performance and administers the Malcolm Baldrige National Quality Award. The Baldrige Award recognizes U.S. organizations for performance excellence based on the Baldrige Criteria for Performance Excellence. The Criteria address critical aspects of management that contribute to performance excellence: leadership; strategy; customers; measurement, analysis, and knowledge management; workforce; operations; and results.
The European Foundation for Quality Management's EFQM Excellence Model supports an award scheme similar to the Baldrige Award for European companies.
In Canada, the National Quality Institute presents the 'Canada Awards for Excellence' on an annual basis to organisations that have displayed outstanding performance in the areas of Quality and Workplace Wellness, and have met the Institute's criteria with documented overall achievements and results.
EQUASS is a sector-specific quality system designed for the social services sector, and addresses quality principles that are specific to service delivery to vulnerable groups, such as empowerment, rights and person-centredness. 
The Alliance for Performance Excellence is a network of state and local organizations that use the Baldrige Criteria for Performance Excellence at the grassroots level to improve the performance of local organizations and economies. browsers can find Alliance members in their state and get the latest news and events from the Baldrige community.
Quality Management System process.
A QMS process is an element of an organizational QMS. The ISO9001:2000 standard requires organizations seeking compliance or certification to define the processes which form the QMS and the sequence and interaction of these processes. Butterworth-Heinemann and other publishers have offered several books which provide step-by-step guides to whom seeking the quality certifications of their products , , , , .
Examples of such processes include:
ISO9001 requires that the performance of these processes be measured, analysed and continually improved, and the results of this form an input into the management review process.
References.
Business School Press
21. Lee, R., and Dale, B. (1998) Business process management: a review and evaluation, Business Process Re-engineering & Management Journal, 4 (3), 214–225

</doc>
<doc id="25346" url="https://en.wikipedia.org/wiki?curid=25346" title="Québécois (word)">
Québécois (word)

Québécois (pronounced ; feminine: Québécoise (pronounced ), ' (fem.: '), or (fem.: ) is a word used primarily to refer to a French-speaking native or inhabitant of the Canadian province of Quebec. It can refer to French spoken in Quebec. It may also be used, with an upper or lower case initial, as an adjective relating to Quebec, or to the French culture of Quebec. A resident or native of Quebec is usually referred to in English as a Quebecer or Quebecker. In French, Québécois or Québécoise usually refers to any native or resident of Quebec. Its use became more prominent in the 1960s as French Canadians from Quebec increasingly self-identified as Québécois.
Etymology.
The name "Quebec" comes from a Mi'kmaq word "k'webeq" meaning "where the waters get narrow" and originally referred to the area around Quebec City, where the Saint Lawrence River narrows to a cliff-lined gap. French explorer Samuel de Champlain chose this name in 1608 for the colonial outpost he would use as the administrative seat for the French colony of Canada and New France. The Province of Quebec was first founded as a British colony in the Royal Proclamation of 1763 after the Treaty of Paris formally transferred the French colony of New France to Britain after the Seven Years' War. Quebec City remained the capital. In 1774, Guy Carleton obtained from the British Government the Quebec Act, which gave Canadiens most of the territory they held before 1763; the right of religion; and their right of language and culture. The British Government did this to in order to keep their loyalty, in the face of a growing menace of independence from the 13 original British colonies.
Québécois identity.
The term became more common in English as "Québécois" largely replacing "French Canadian" as an expression of cultural and national identity among French Canadians living in Quebec during the Quiet Revolution of the 1960s. The predominant French Canadian nationalism and identity of previous generations was based on the protection of the French language, the Roman Catholic Church, and Church-run institutions across Canada and in parts of the United States. In contrast, the modern Québécois identity is secular and based on a social democratic ideal of an active Quebec government promoting the French language and French-speaking culture in the arts, education, and business within the Province of Quebec. Politically, this resulted in a push towards more autonomy for Quebec and an internal debate on Quebec independence and identity that continues to this day. The emphasis on the French language and Quebec autonomy means that French-speakers across Canada now self-identify more specifically with provincial or regional identity-tags, such as "acadienne", or "franco-canadienne", "franco-manitobaine", "franco-ontarienne" or "fransaskoise". As a result, francophone and anglophones now borrow the French terms when discussing issues of francophone linguistic and cultural identity in English, though outside of Quebec terms such as Franco-Ontarian, acadian and Franco-Manitoban are still predominant.
Québécois nation.
The political shift towards a new Quebec nationalism in the 1960s led to Québécois increasingly referring to provincial institutions as being national. This was reflected in the change of the provincial "Legislative Assembly" to "National Assembly" in 1968. Nationalism reached an apex the 1970s and 1990s, with contentious constitutional debates resulting in close to half of all of French-speaking Québécois seeking recognition of nation status through tight referendums on Quebec sovereignty in 1980 and 1995. Having lost both referendums, the sovereigntist Parti Québécois government renewed the push for recognition as a nation through symbolic motions that gained the support of all parties in the National Assembly. They affirmed the right to determine the independent status of Quebec. They also renamed the area around Quebec City the "Capitale-Nationale" (national capital) region and renamed provincial parks "Parcs Nationaux" (national parks). In opposition in October 2003, the Parti Québécois tabled a motion that was unanimously adopted in the National Assembly affirming that the Quebec people formed a nation. Bloc Québécois leader Gilles Duceppe scheduled a similar motion in the House of Commons for November 23, 2006, that would have recognized "Quebecers as a nation". Conservative Prime Minister Stephen Harper tabled the "Québécois nation motion" the day before the Bloc Québécois resolution came to a vote. The English version changed the word "Quebecer" to "Québécois" and added "within a united Canada" at the end of the Bloc motion.
The "Québécois nation" was recognized by the Canadian House of Commons on November 27, 2006. The Prime Minister specified that the motion used the ""cultural"" and ""sociological"" as opposed to the ""legal"" sense of the word ""nation"". According to Harper, the motion was of a symbolic political nature, representing no constitutional change, no recognition of Quebec sovereignty, and no legal change in its political relations within the federation. The Prime Minister has further elaborated, stating that the motion's definition of Québécois relies on personal decisions to self-identify as Québécois, and therefore is a personal choice.
Despite near-universal support in the House of Commons, several important dissenters criticized the motion. Intergovernmental Affairs minister Michael Chong resigned from his position and abstained from voting, arguing that this motion was too ambiguous and had the potential of recognizing a destructive ethnic nationalism in Canada. Liberals were the most divided on the issue and represented 15 of the 16 votes against the motion. Liberal MP Ken Dryden summarized the view of many of these dissenters, maintaining that it was a game of semantics that cheapened issues of national identity. A survey by Leger Marketing in November 2006 showed that Canadians were deeply divided on this issue. When asked if Québécois are a nation, only 53 per cent of Canadians agreed, 47 per cent disagreed, with 33 per cent strongly disagreeing; 78 per cent of French-speaking Canadians agreed that Québécois are a nation, compared with 38 per cent of English-speaking Canadians. As well, 78 per cent of 1,000 Québécois polled thought that Québécois should be recognized as a nation.
Québécois in census and ethnographic studies.
The Québécois self-identify as an ethnic group in both the English and French versions of the Canadian census and in demographic studies of ethnicity in Canada. In the 2001 Census of Canada, 98,670 Canadians, or just over 1% of the population of Quebec identified "Québécois" as their ethnicity, ranking "Québécois" as the 37th most common response. These results were based on a question on residents in each household in Canada: ""To which ethnic or cultural group(s) did this person's ancestors belong?"", along with a list of sample choices ("Québécois" did not appear among the various sample choices). The most common ethnicity,""Canadien"" or Canadian, did appear as an example on the questionnaire, and was selected by 4.9 million people or 68.2% of the Quebec population.
In the more detailed "Ethnic Diversity Survey",
Québécois was the most common ethnic identity in Quebec, reported by 37% of
Quebec’s population aged 15 years and older, either as their only identity or alongside
other identities. The survey, based on interviews, asked the following questions: ""1) I would now like to ask you about your ethnic ancestry, heritage or background. What were the ethnic or cultural origins of your ancestors? 2) In addition to "Canadian", what were the other ethnic or cultural origins of your ancestors on first coming to North America?"" This survey did not list possible choices of ancestry and permitted multiple answers.
In census ethnic surveys, French-speaking Canadians identify their ethnicity most often as French, "Canadien", "Québécois", or French Canadian, with the latter three referred to by Jantzen (2005) as "French New World" ancestries because they originate in Canada. Jantzen (2005) distinguishes the English "Canadian", meaning "someone whose family has been in Canada for multiple generations", and the French "Canadien", used to refer to descendants of the original settlers of New France in the 17th and 18th centuries.
Those reporting "French New World" ancestries overwhelmingly had ancestors that went back at least 4 generations in Canada: specifically, 90% of "Québécois" traced their ancestry back this far. Fourth generation Canadiens and Québécois showed considerable attachment to their ethno-cultural group, with 70% and 61% respectively reporting a strong sense of belonging.
The generational profile and strength of identity of French New World ancestries contrast with those of British or Canadian ancestries, which represent the largest ethnic identities in Canada. Although deeply rooted Canadians express a deep attachment to their ethnic identity, most English-speaking Canadians of British ancestry generally cannot trace their ancestry as far back in Canada as French-speakers. As a result, their identification with their ethnicity is weaker tending to have a more broad based cultural identification: for example, only 50% of third generation "Canadians" strongly identify as such, bringing down the overall average. The survey report notes that 80% of Canadians whose families had been in Canada for three or more generations reported "Canadian and provincial or regional ethnic identities". These identities include "Québécois" (37% of Quebec population), "Acadian" (6% of Atlantic provinces) and "Newfoundlander" (38% of Newfoundland and Labrador).
English usage.
English expressions employing the term may imply specific reference to francophones; such as "Québécois literature"
French usage.
Most French usage employs references to people and things of Quebec origin.
Possible use as an ethnic designation in French.
Dictionaries.
The dictionary "Le Petit Robert", published in France, states that the adjective "québécois", in addition to its territorial meaning, may refer specifically to francophone or French Canadian culture in Quebec. The dictionary gives as examples "cinéma québécois" and "littérature québécoise".
However, an ethnic or linguistic sense is absent from "Le Petit Larousse", also published in France, as well as from French dictionaries published in Canada such as "Le Dictionnaire québécois d'aujourd'hui" and "Le Dictionnaire du français Plus", which indicate instead "Québécois francophone" "francophone Quebecer" in the linguistic sense. These dictionaries also include phrases like "cinéma québécois" "Quebec cinema", but do not classify them as relating to language or ethnicity.
The online dictionary "Grand dictionnaire terminologique" of the Office québécois de la langue française mentions only a territorial meaning for "Québécois".
Other opinion.
Newspaper editor Lysiane Gagnon has referred to an ethnic sense of the word "Québécois" in both English and French.
Special terms using 'Québécois'.
French expressions employing "Québécois" often appear in both French and English.

</doc>
<doc id="25348" url="https://en.wikipedia.org/wiki?curid=25348" title="Quantico, Virginia">
Quantico, Virginia

Quantico (formerly Potomac) is a town in Prince William County, Virginia. As of the 2010 United States Census, Quantico had a population of 480.
Quantico is bordered by the U.S. military installation of Marine Corps Base Quantico on three sides and the Potomac River on the fourth. Quantico is located south of the mouth of Quantico Creek on the Potomac. The word Quantico is a derivation of the name of a Doeg village recorded by English colonists as "Pamacocack".
Quantico is the site of one of the largest U.S. Marine Corps bases in the world, MCB Quantico. The base is the site of the Marine Corps Combat Development Command and HMX-1 (the presidential helicopter squadron), Officer Candidate School and The Basic School. The United States Drug Enforcement Administration's training academy, the FBI Academy, the FBI Laboratory, the Naval Criminal Investigative Service and the Air Force Office of Special Investigations headquarters are on the base. A replica of the USMC War Memorial stands in the entrance to the base (the original is at the north end of Arlington National Cemetery).
, the mayor is Kevin P. Brown.
Geography.
Quantico is at 38°31'19" North, 77°17'23" West (38.521871, −77.289757). According to the United States Census Bureau, the town has a total area of , of which, of it is land and none of the area is covered with water.
Climate.
Quantico has a humid subtropical climate (Köppen climate classification "Cfa").
Demographics.
As of the census of 2000, there are 561 people, 295 households, and 107 families living in the town. The population density is . There are 359 housing units at an average density of .
Racial composition.
The racial makeup is 51.32% White, 20.32% African American, 10.16% Asian, 5.53% Hispanic or Latino, 0.36% Native American, 2.32% from other races, and 5.53% from two or more races.
Households.
There are 295 households out of which 19.7% have children under the age of 18 living with them, 21.4% are married couples living together, 11.2% have a female householder with no husband present, and 63.4% are non-families. Of all households, 53.2% are made up of individuals and 9.2% have someone living alone who is 65 years of age or older. The average household size is 1.90 and the average family size is 3.02.
Ages.
The population is spread out, with 20.9% under the age of 18, 11.6% from 18 to 24, 39.8% from 25 to 44, 19.4% from 45 to 64, and 8.4% who are 65 years of age or older. The median age is 35 years. For every 100 females there are 122.6 males. For every 100 females age 18 and over, there are 130.1 males.
Income.
The median income for a male is $29,615 versus $23,125 for females. The per capita income for the town is $19,087. 21.4% of the population and 22.4% of families are below the poverty line. Out of the total population, 39.4% of those under the age of 18 and none of those 65 and older are living below the poverty line.
In popular culture.
The FBI Academy in Quantico was the setting of fifteen episodes of "The X-Files" and several scenes from Thomas Harris' book "The Silence of the Lambs" and the film of the same name. "Criminal Minds" is based out of Quantico. The CBS drama "" is also set in Quantico. A new drama series, "Quantico", set at the FBI Academy, broadcast in 2015 on ABC. Major Jack Reacher makes numerous references to Quantico. Especially in Lee Childs novel "The Visitor"

</doc>
<doc id="25349" url="https://en.wikipedia.org/wiki?curid=25349" title="QSIG">
QSIG

QSIG is an ISDN based signaling protocol for signaling between private branch exchanges (PBXs) in a private integrated services network (PISN). It makes use of the connection-level Q.931 protocol and the application-level ROSE protocol. ISDN "proper" functions as the physical link layer.
QSIG was originally developed by Ecma International, adopted by ETSI and is defined by a set of ISO standard documents, so is not owned by any company. This allows interoperability between communications platforms provided by disparate vendors. 
QSIG has two layers, called BC (basic call) and GF (generic function). QSIG BC describes how to set up calls between PBXs. QSIG GF provides supplementary services for large-scale corporate, educational, and government networks, such as line identification, call intrusion and call forwarding. Thus for a large or very distributed company that requires multiple PBXs, users can receive the same services across the network and be unaware of the switch that their telephone is connected to. This greatly eases the problems of management of large networks.
QSIG will likely never rival each vendor's private network protocols, but it does provide an option for a higher level of integration than that of the traditional choices.
List of QSIG standards.
Note: This list is not complete. See the "source" after the list for more information.
Source : ECMA - list of standards (search the list for PISN to find all QSIG related standards at ECMA)
QSIG basically uses ROSE to invoke specific supplementary service at the remote PINX. These ROSE operations are coded in a Q.931 FACILITY info element. Here a list of QSIG opcodes:
List of ISDN standards.
Source : European Telecommunications Standards Institute (ETSI)
Source : International Telecommunications Union (ITU)

</doc>
<doc id="25350" url="https://en.wikipedia.org/wiki?curid=25350" title="Quasicrystal">
Quasicrystal

A quasiperiodic crystal, or quasicrystal, is a structure that is ordered but not periodic. A quasicrystalline pattern can continuously fill all available space, but it lacks translational symmetry. While crystals, according to the classical crystallographic restriction theorem, can possess only two, three, four, and six-fold rotational symmetries, the Bragg diffraction pattern of quasicrystals shows sharp peaks with other symmetry orders, for instance five-fold.
Aperiodic tilings were discovered by mathematicians in the early 1960s, and, some twenty years later, they were found to apply to the study of quasicrystals. The discovery of these aperiodic forms in nature has produced a paradigm shift in the fields of crystallography. Quasicrystals had been investigated and observed earlier, but, until the 1980s, they were disregarded in favor of the prevailing views about the atomic structure of matter. In 2009, after a dedicated search, a mineralogical finding, icosahedrite, offered evidence for the existence of natural quasicrystals.
Roughly, an ordering is non-periodic if it lacks translational symmetry, which means that a shifted copy will never match exactly with its original. The more precise mathematical definition is that there is never translational symmetry in more than "n" – 1 linearly independent directions, where "n" is the dimension of the space filled, e.g., the three-dimensional tiling displayed in a quasicrystal may have translational symmetry in two dimensions. The ability to diffract comes from the existence of an indefinitely large number of elements with a regular spacing, a property loosely described as long-range order. Experimentally, the aperiodicity is revealed in the unusual symmetry of the diffraction pattern, that is, symmetry of orders other than two, three, four, or six. 
In 1982 materials scientist Dan Shechtman observed that certain aluminium-manganese alloys produced the unusual diffractograms which today are seen as revelatory of quasicrystal structures. Due to fear of the scientific community's reaction, it took him two years to publish the results for which he was awarded the Nobel Prize in Chemistry in 2011.
History.
In 1961, Hao Wang asked whether determining if a set of tiles admits a tiling of the plane is an algorithmically unsolvable problem or not. He conjectured that it is solvable, relying on the hypothesis that every set of tiles that can tile the plane can do it "periodically" (hence, it would suffice to try to tile bigger and bigger patterns until obtaining one that tiles periodically). Nevertheless, two years later, his student Robert Berger constructed a set of some 20,000 square tiles (now called Wang tiles) that can tile the plane but not in a periodic fashion. As further aperiodic sets of tiles were discovered, sets with fewer and fewer shapes were found. in 1976 Roger Penrose discovered a set of just two tiles, now referred to as Penrose tiles, that produced only non-periodic tilings of the plane. These tilings displayed instances of fivefold symmetry. One year later Alan Mackay showed experimentally that the diffraction pattern from the Penrose tiling had a two-dimensional Fourier transform consisting of sharp 'delta' peaks arranged in a fivefold symmetric pattern. Around the same time Robert Ammann created a set of aperiodic tiles that produced eightfold symmetry.
Mathematically, quasicrystals have been shown to be derivable from a general method that treats them as projections of a higher-dimensional lattice. Just as circles, ellipses, and hyperbolic curves in the plane can be obtained as sections from a three-dimensional double cone, so too various (aperiodic or periodic) arrangements in two and three dimensions can be obtained from postulated hyperlattices with four or more dimensions. Icosahedral quasicrystals in three dimensions were projected from a six-dimensional hypercubic lattice by Peter Kramer and Roberto Neri in 1984. The tiling is formed by two tiles with rhombohedral shape.
Shechtman first observed ten-fold electron diffraction patterns in 1982, as described in his notebook. The observation was made during a routine investigation, by electron microscopy, of a rapidly cooled alloy of aluminium and manganese prepared at the US National Bureau of Standards (later NIST).
In the summer of the same year Shechtman visited Ilan Blech and related his observation to him. Blech responded that such diffractions had been seen before. Around that time, Shechtman also related his finding to John Cahn of NIST who did not offer any explanation and challenged him to solve the observation. Shechtman quoted Cahn as saying: "Danny, this material is telling us something and I challenge you to find out what it is".
The observation of the ten-fold diffraction pattern lay unexplained for two years until the spring of 1984, when Blech asked Shechtman to show him his results again. A quick study of Shechtman's results showed that the common explanation for a ten-fold symmetrical diffraction pattern, the existence of twins, was ruled out by his experiments.
Since periodicity and twins were ruled out, Blech, unaware of the two-dimensional tiling work, was looking for another possibility: a completely new structure containing cells connected to each other by defined angles and distances but without translational periodicity. Blech decided to use a computer simulation to calculate the diffraction intensity from a cluster of such a material without long-range translational order but still not random. He termed this new structure multiple polyhedral.
The idea of a new structure was the necessary paradigm shift to break the impasse. The “Eureka moment” came when the computer simulation showed sharp ten-fold diffraction patterns, similar to the observed ones, emanating from the three-dimensional structure devoid of periodicity. The multiple polyhedral structure was termed later by many researchers as icosahedral glass but in effect it embraces "any arrangement of polyhedra connected with definite angles and distances" (this general definition includes tiling, for example).
Shechtman accepted Blech's discovery of a new type of material and it gave him the courage to publish his experimental observation. Shechtman and Blech jointly wrote a paper entitled "The Microstructure of Rapidly Solidified Al6Mn" and sent it for publication around June 1984 to the Journal of Applied Physics (JAP). The JAP editor promptly rejected the paper as being better fit for a metallurgical readership. As a result, the same paper was re-submitted for publication to the Metallurgical Transactions A, where it was accepted. Although not noted in the body of the published text, the published paper was slightly revised prior to publication.
Meanwhile, on seeing the draft of the Shechtman-Blech paper in the summer of 1984, John Cahn suggested that Shechtman's experimental results merit a fast publication in a more appropriate scientific journal. Shechtman agreed and, in hindsight, called this fast publication "a winning move”. This paper, published in the "Physical Review Letters" (PRL), repeated Shechtman's observation and used the same illustrations as the original Shechtman-Blech paper in the "Metallurgical Transactions A". The PRL paper, the first to appear in print, caused considerable excitement in the scientific community.
Next year Ishimasa "et al." reported twelvefold symmetry in Ni-Cr particles. Soon, eightfold diffraction patterns were recorded in V-Ni-Si and Cr-Ni-Si alloys. Over the years, hundreds of quasicrystals with various compositions and different symmetries have been discovered. The first quasicrystalline materials were thermodynamically unstable—when heated, they formed regular crystals. However, in 1987, the first of many stable quasicrystals were discovered, making it possible to produce large samples for study and opening the door to potential applications. In 2009, following a 10-year systematic search, scientists reported the first natural quasicrystal, a mineral found in the Khatyrka River in eastern Russia. This natural quasicrystal exhibits high crystalline quality, equalling the best artificial examples. The natural quasicrystal phase, with a composition of Al63Cu24Fe13, was named icosahedrite and it was approved by the International Mineralogical Association in 2010. Furthermore, analysis indicates it may be meteoritic in origin, possibly delivered from a carbonaceous chondrite asteroid.
A further study of Khatyrka meteorites revealed micron-sized grains of another natural quasicrystal, which has a ten-fold symmetry and a chemical formula of Al71Ni24Fe5. This quasicrystal is stable in a narrow temperature range, from 1120 to 1200 K at ambient pressure, which suggests that natural quasicrystals are formed by rapid quenching of a meteorite heated during an impact-induced shock.
In 1972 de Wolf and van Aalst reported that the diffraction pattern produced by a crystal of sodium carbonate cannot be labeled with three indices but needed one more, which implied that the underlying structure had four dimensions in reciprocal space. Other puzzling cases have been reported, but until the concept of quasicrystal came to be established, they were explained away or denied. However, at the end of the 1980s the idea became acceptable, and in 1992 the International Union of Crystallography altered its definition of a crystal, broadening it as a result of Shechtman’s findings, reducing it to the ability to produce a clear-cut diffraction pattern and acknowledging the possibility of the ordering to be either periodic or aperiodic. Now, the symmetries compatible with translations are defined as "crystallographic", leaving room for other "non-crystallographic" symmetries. Therefore, aperiodic or quasiperiodic structures can be divided into two main classes: those with crystallographic point-group symmetry, to which the incommensurately modulated structures and composite structures belong, and those with non-crystallographic point-group symmetry, to which quasicrystal structures belong.
Originally, the new form of matter was dubbed "Shechtmanite". The term "quasicrystal" was first used in print by Steinhardt and Levine shortly after Shechtman's paper was published.
The adjective "quasicrystalline" had already been in use, but now it came to be applied to any pattern with unusual symmetry. 'Quasiperiodical' structures were claimed to be observed in some decorative tilings devised by medieval Islamic architects. For example, Girih tiles in a medieval Islamic mosque in Isfahan, Iran, are arranged in a two-dimensional quasicrystalline pattern. These claims have, however, been under some debate.
Shechtman was awarded the Nobel Prize in Chemistry in 2011 for his work on quasicrystals. "His discovery of quasicrystals revealed a new principle for packing of atoms and molecules," stated the Nobel Committee and pointed that "this led to a paradigm shift within chemistry." 
Mathematics.
There are several ways to mathematically define quasicrystalline patterns. One definition, the "cut and project" construction, is based on the work of Harald Bohr (mathematician brother of Niels Bohr). The concept of an almost periodic function (also called a quasiperiodic function) was studied by Bohr, including work of Bohl and Escanglon.
He introduced the notion of a superspace. Bohr showed that quasiperiodic functions arise as restrictions of high-dimensional periodic functions to an irrational slice (an intersection with one or more hyperplanes), and discussed their Fourier point spectrum. These functions are not exactly periodic, but they are arbitrarily close in some sense, as well as being a projection of an exactly periodic function.
In order that the quasicrystal itself be aperiodic, this slice must avoid any lattice plane of the higher-dimensional lattice. De Bruijn showed that Penrose tilings can be viewed as two-dimensional slices of five-dimensional hypercubic structures. Equivalently, the Fourier transform of such a quasicrystal is nonzero only at a dense set of points spanned by integer multiples of a finite set of basis vectors (the projections of the primitive reciprocal lattice vectors of the higher-dimensional lattice).
The intuitive considerations obtained from simple model aperiodic tilings are formally expressed in the concepts of Meyer and Delone sets. The mathematical counterpart of physical diffraction is the Fourier transform and the qualitative description of a diffraction picture as 'clear cut' or 'sharp' means that singularities are present in the Fourier spectrum. There are different methods to construct model quasicrystals. These are the same methods that produce aperiodic tilings with the additional constraint for the diffractive property. Thus, for a substitution tiling the eigenvalues of the substitution matrix should be Pisot numbers. The aperiodic structures obtained by the cut-and-project method are made diffractive by choosing a suitable orientation for the construction; this is a geometric approach that has also a great appeal for physicists.
Classical theory of crystals reduces crystals to point lattices where each point is the center of mass of one of the identical units of the crystal. The structure of crystals can be analyzed by defining an associated group. Quasicrystals, on the other hand, are composed of more than one type of unit, so, instead of lattices, quasilattices must be used. Instead of groups, groupoids, the mathematical generalization of groups in category theory, is the appropriate tool for studying quasicrystals.
Using mathematics for construction and analysis of quasicrystal structures is a difficult task for most experimentalists. Computer modeling, based on the existing theories of quasicrystals, however, greatly facilitated this task. Advanced programs have been developed allowing one to construct, visualize and analyze quasicrystal structures and their diffraction patterns.
Interacting spins were also analyzed in quasicrystals: AKLT Model and 8-vertex model were solved in quasicrystals analytically 
Materials science.
Since the original discovery by Dan Shechtman, hundreds of quasicrystals have been reported and confirmed. Undoubtedly, the quasicrystals are no longer a unique form of solid; they exist
universally in many metallic alloys and some polymers. Quasicrystals are found most often in aluminium alloys (Al-Li-Cu, Al-Mn-Si, Al-Ni-Co, Al-Pd-Mn, Al-Cu-Fe, Al-Cu-V, etc.), but numerous other compositions are also known (Cd-Yb, Ti-Zr-Ni, Zn-Mg-Ho, Zn-Mg-Sc, In-Ag-Yb, Pd-U-Si, etc.).
Two types of quasicrystals are known. The first type, polygonal (dihedral) quasicrystals, have an axis of 8, 10, or 12-fold local symmetry (octagonal, decagonal, or dodecagonal quasicrystals, respectively). They are periodic along this axis and quasiperiodic in planes normal to it. The second type, icosahedral quasicrystals, are aperiodic in all directions.
Quasicrystals fall into three groups of different thermal stability:
Except for the Al–Li–Cu system, all the stable quasicrystals are almost free of defects and disorder, as evidenced by X-ray and electron diffraction revealing peak widths as sharp as those of perfect crystals such as Si. Diffraction patterns exhibit fivefold, threefold, and twofold symmetries, and reflections are arranged quasiperiodically in three dimensions.
The origin of the stabilization mechanism is different for the stable and metastable quasicrystals. Nevertheless, there is a common feature observed in most quasicrystal-forming liquid alloys or their undercooled liquids: a local icosahedral order. The icosahedral order is in equilibrium in the "liquid state" for the stable quasicrystals, whereas the icosahedral order prevails in the "undercooled liquid state" for the metastable quasicrystals.
A nanoscale icosahedral phase was formed in Zr-, Cu- and Hf-based bulk metallic glasses alloyed with noble metals.
Most quasicrystals have ceramic-like properties including high thermal and electrical resistance, hardness and brittleness, resistance to corrosion, and non-stick
properties. Many metallic quasicrystalline substances are impractical for most applications due to their thermal instability; the Al-Cu-Fe ternary system and the Al-Cu-Fe-Cr and Al-Co-Fe-Cr quaternary systems, thermally stable up to 700 °C, are notable exceptions.
Applications.
Quasicrystalline substances have potential applications in several forms. The tendency to brittleness is a problem that must be overcome.
Quasicrystalline coatings benefit from hardness while avoiding the brittleness in bulk material.
Metallic quasicrystalline coatings can be applied by plasma-coating or magnetron sputtering. A problem that must be resolved is the tendency for cracking due to the materials' extreme brittleness.
An application was the use of low-friction Al-Cu-Fe-Cr quasicrystals as a coating for frying pans. Food did not stick to it as much as to stainless steel making the pan moderately non-stick and easy to clean; heat transfer and durability were better than PTFE non-stick cookware and the pan was free from perfluorooctanoic acid (PFOA); the surface was very hard, claimed to be ten times harder than stainless steel, and not harmed by metal utensils or cleaning in a dishwasher; and the pan could withstand temperatures of without harm. However, cooking with a lot of salt would etch the quasicrystalline coating used, and the pans were eventually withdrawn from production. Shechtman had one of these pans.
The Nobel citation said that quasicrystals, while brittle, could reinforce steel "like armor". When Shechtman was asked about potential applications of quasicrstals he said that a precipitation-hardened stainless steel is produced that is strengthened by small quasicrystalline particles. It does not corrode and is extremely strong, suitable for razor blades and surgery instruments. The small quasicrystalline particles impede the motion of dislocation in the material.
Quasicrystals were also being used to develop heat insulation, LEDs, diesel engines, and new materials that convert heat to electricity. Shechtman suggested new applications taking advantage of the low coefficient of friction and the hardness of some quasicrystalline materials, for example embedding particles in plastic to make strong, hard-wearing, low-friction plastic gears. The low heat conductivity of some quasicrystals makes them good for heat insulating coatings.
Other potential applications include selective solar absorbers for power conversion, broad-wavelength reflectors, and bone repair and prostheses applications where biocompatibility, low friction and corrosion resistance are required. Magnetron sputtering can be readily applied to other stable quasicrystalline alloys such as Al-Pd-Mn.
While saying that the discovery of icosahedrite, the first quasicrystal found in nature, was important, Shechtman saw no practical applications.

</doc>
<doc id="25381" url="https://en.wikipedia.org/wiki?curid=25381" title="Recreation">
Recreation

Recreation is an activity of leisure, leisure being discretionary time. The "need to do something for recreation" is an essential element of human biology and psychology. Recreational activities are often done for enjoyment, amusement, or pleasure and are considered to be "fun".
Etymology.
The term "recreation" appears to have been used in English first in the late 14th century, first in the sense of "refreshment or curing of a sick person", and derived turn from Latin ("re": "again", "creare": "to create, bring forth, beget.).
Prerequisites to leisure.
Humans spend their time in activities of daily living, work, sleep, social duties, and leisure, the latter time being free from prior commitments to physiologic or social needs, a prerequisite of recreation. Leisure has increased with increased longevity and, for many, with decreased hours spent for physical and economic survival, yet others argue that time pressure has increased for modern people, as they are committed to too many tasks. Other factors that account for an increased role of recreation are affluence, population trends, and increased commercialization of recreational offerings. While one perception is that leisure is just "spare time", time not consumed by the necessities of living, another holds that leisure is a force that allows individuals to consider and reflect on the values and realities that are missed in the activities of daily life, thus being an essential element of personal development and civilization. This direction of thought has even been extended to the view that leisure is the purpose of work, and a reward in itself, and "leisure life" reflects the values and character of a nation. Leisure is considered a human right under the Universal Declaration of Human Rights.
Play, recreation and work.
Recreation is difficult to separate from the general concept of play, which is usually the term for children's recreational activity. Children may playfully imitate activities that reflect the realities of adult life. It has been proposed that play or recreational activities are outlets of or expression of excess energy, channeling it into socially acceptable activities that fulfill individual as well as societal needs, without need for compulsion, and providing satisfaction and pleasure for the participant. A traditional view holds that work is supported by recreation, recreation being useful to "recharge the battery" so that work performance is improved. Work, an activity generally performed out of economic necessity and useful for society and organized within the economic framework, however can also be pleasurable and may be self-imposed thus blurring the distinction to recreation. Many activities may be work for one person and recreation for another, or, at an individual level, over time recreational activity may become work, and vice versa. Thus, for a musician, playing an instrument may be at one time a profession, and at another a recreation. Similarly, it may be difficult to separate education from recreation as in the case of recreational mathematics.
Recreational activities.
Recreation is an essential part of human life and finds many different forms which are shaped naturally by individual interests but also by the surrounding social construction. Recreational activities can be communal or solitary, active or passive, outdoors or indoors, healthy or harmful, and useful for society or detrimental. A significant section of recreational activities are designated as hobbies which are activities done for pleasure on a regular basis. A list of typical activities could be almost endless including most human activities, a few examples being reading, playing or listening to music, watching movies or TV, gardening, hunting, sports, studies, and travel. Some recreational activities - such as gambling, recreational drug use, or delinquent activities - may violate societal norms and laws.
Public space such as parks and beaches are essential venues for many recreational activities. Tourism has recognized that many visitors are specifically attracted by recreational offerings. In support of recreational activities government has taken an important role in their creation, maintenance, and organization, and whole industries have developed merchandise or services. Recreation-related business is an important factor in the economy; it has been estimated that the outdoor recreation sector alone contributes $730 billion annually to the U.S. economy and generates 6.5 million jobs.
Organized recreation.
Many recreational activities are organized, typically by public institutions, voluntary group-work agencies, private groups supported by membership fees, and commercial enterprises. Examples of each of these are the National Park Service, the YMCA, the Kiwanis, and Disney World.
Health and recreation.
Recreation has many health benefits, and, accordingly, Therapeutic Recreation has been developed to take advantage of this effect. The National Council for Therapeutic Recreation Certification (NCTRC) is the nationally recognized credentialing organization for the profession of Therapeutic Recreation. Professionals in the field of Therapeutic Recreation who are certified by the NCTRC are called "Certified Therapeutic Recreation Specialists". The job title "Recreation Therapist" is identified in the U.S. Dept of Labor's Occupation Outlook. Such therapy is applied in rehabilitation, psychiatric facilities for youth and adults, and in the care of the elderly, the disabled, or people with chronic diseases. Recreational physical activity is important to reduce obesity, and the risk of osteoporosis and of cancer, most significantly in men that of colon and prostate, and in women that of the breast; however, not all malignancies are reduced as outdoor recreation has been linked to a higher risk of melanoma. Extreme adventure recreation naturally carries its own hazards.
Recreation as a career.
A recreation specialist would be expected to meet the recreational needs of a community or assigned interest group. Educational institutions offer courses that lead to a degree as a Bachelor of Arts in recreation management. People with such degrees often work in parks and recreation centers in towns, on community projects and activities. Networking with instructors, budgeting, and evaluation of continuing programs are common job duties.
In the United States, most states have a professional organization for continuing education and certification in recreation management. The National Recreation and Park Association administers a certification program called the CPRP (Certified Park and Recreation Professional) that is considered a national standard for professional recreation specialist practices.

</doc>
<doc id="25382" url="https://en.wikipedia.org/wiki?curid=25382" title="Recession">
Recession

In economics, a recession is a business cycle contraction which results in a general slowdown in economic activity. Macroeconomic indicators such as GDP (gross domestic product), investment spending, capacity utilization, household income, business profits, and inflation fall, while bankruptcies and the unemployment rate rise.
Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock or the bursting of an economic bubble. Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply, increasing government spending and decreasing taxation.
Definition.
In a 1979 "New York Times" article, economic statistician Julius Shiskin suggested several rules of thumb for defining a recession, one of which was two down consecutive quarters of GDP. In time, the other rules of thumb were forgotten. Some economists prefer a definition of a 1.5-2 percentage points rise in unemployment within 12 months.
In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) is generally seen as the authority for dating US recessions. The NBER defines an economic recession as: "a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales." Almost universally, academics, economists, policy makers, and businesses defer to the determination by the NBER for the precise dating of a recession's onset and end.
In the United Kingdom, recessions are generally defined as two consecutive quarters of negative economic growth, as measured by the seasonal adjusted quarter-on-quarter figures for real GDP. The exact same recession definition applies for all member states of the European Union.
Attributes.
A recession has many attributes that can occur simultaneously and includes declines in component measures of economic activity (GDP) such as consumption, investment, government spending, and net export activity. These summary measures reflect underlying drivers such as employment levels and skills, household savings rates, corporate investment decisions, interest rates, demographics, and government policies.
Economist Richard C. Koo wrote that under ideal conditions, a country's economy should have the household sector as net savers and the corporate sector as net borrowers, with the government budget nearly balanced and net exports near zero. When these relationships become imbalanced, recession can develop within the country or create pressure for recession in another country. Policy responses are often designed to drive the economy back towards this ideal state of balance.
A severe (GDP down by 10%) or prolonged (three or four years) recession is referred to as an economic depression, although some argue that their causes and cures can be different. As an informal shorthand, economists sometimes refer to different recession shapes, such as V-shaped, U-shaped, L-shaped and W-shaped recessions.
Type of recession or shape.
The type and shape of recessions are distinctive. In the US, V-shaped, or short-and-sharp contractions followed by rapid and sustained recovery, occurred in 1954 and 1990–91; U-shaped (prolonged slump) in 1974–75, and W-shaped, or double-dip recessions in 1949 and 1980–82. Japan’s 1993–94 recession was U-shaped and its 8-out-of-9 quarters of contraction in 1997–99 can be described as L-shaped. Korea, Hong Kong and South-east Asia experienced U-shaped recessions in 1997–98, although Thailand’s eight consecutive quarters of decline should be termed L-shaped.
Psychological aspects.
Recessions have psychological and confidence aspects. For example, if companies expect economic activity to slow, they may reduce employment levels and save money rather than invest. Such expectations can create a self-reinforcing downward cycle, bringing about or worsening a recession. Consumer confidence is one measure used to evaluate economic sentiment. The term animal spirits has been used to describe the psychological factors underlying economic activity. Economist Robert J. Shiller wrote that the term "...refers also to the sense of trust we have in each other, our sense of fairness in economic dealings, and our sense of the extent of corruption and bad faith. When animal spirits are on ebb, consumers do not want to spend and businesses do not want to make capital expenditures or hire people."
Balance sheet recession.
High levels of indebtedness or the bursting of a real estate or financial asset price bubble can cause what is called a "balance sheet recession." This is when large numbers of consumers or corporations pay down debt (i.e., save) rather than spend or invest, which slows the economy. The term balance sheet derives from an accounting identity that holds that assets must always equal the sum of liabilities plus equity. If asset prices fall below the value of the debt incurred to purchase them, then the equity must be negative, meaning the consumer or corporation is insolvent. Economist Paul Krugman wrote in 2014 that "the best working hypothesis seems to be that the financial crisis was only one manifestation of a broader problem of excessive debt--that it was a so-called "balance sheet recession." In Krugman's view, such crises require debt reduction strategies combined with higher government spending to offset declines from the private sector as it pays down its debt.
For example, economist Richard Koo wrote that Japan's "Great Recession" that began in 1990 was a "balance sheet recession." It was triggered by a collapse in land and stock prices, which caused Japanese firms to have negative equity, meaning their assets were worth less than their liabilities. Despite zero interest rates and expansion of the money supply to encourage borrowing, Japanese corporations in aggregate opted to pay down their debts from their own business earnings rather than borrow to invest as firms typically do. Corporate investment, a key demand component of GDP, fell enormously (22% of GDP) between 1990 and its peak decline in 2003. Japanese firms overall became net savers after 1998, as opposed to borrowers. Koo argues that it was massive fiscal stimulus (borrowing and spending by the government) that offset this decline and enabled Japan to maintain its level of GDP. In his view, this avoided a U.S. type Great Depression, in which U.S. GDP fell by 46%. He argued that monetary policy was ineffective because there was limited demand for funds while firms paid down their liabilities. In a balance sheet recession, GDP declines by the amount of debt repayment and un-borrowed individual savings, leaving government stimulus spending as the primary remedy.
Krugman discussed the balance sheet recession concept during 2010, agreeing with Koo's situation assessment and view that sustained deficit spending when faced with a balance sheet recession would be appropriate. However, Krugman argued that monetary policy could also affect savings behavior, as inflation or credible promises of future inflation (generating negative real interest rates) would encourage less savings. In other words, people would tend to spend more rather than save if they believe inflation is on the horizon. In more technical terms, Krugman argues that the private sector savings curve is elastic even during a balance sheet recession (responsive to changes in real interest rates) disagreeing with Koo's view that it is inelastic (non-responsive to changes in real interest rates).
A July 2012 survey of balance sheet recession research reported that consumer demand and employment are affected by household leverage levels. Both durable and non-durable goods consumption declined as households moved from low to high leverage with the decline in property values experienced during the subprime mortgage crisis. Further, reduced consumption due to higher household leverage can account for a significant decline in employment levels. Policies that help reduce mortgage debt or household leverage could therefore have stimulative effects.
Liquidity trap.
A liquidity trap is a Keynesian theory that a situation can develop in which interest rates reach near zero (zero interest-rate policy) yet do not effectively stimulate the economy. In theory, near-zero interest rates should encourage firms and consumers to borrow and spend. However, if too many individuals or corporations focus on saving or paying down debt rather than spending, lower interest rates have less effect on investment and consumption behavior; the lower interest rates are like "pushing on a string." Economist Paul Krugman described the U.S. 2009 recession and Japan's lost decade as liquidity traps. One remedy to a liquidity trap is expanding the money supply via quantitative easing or other techniques in which money is effectively printed to purchase assets, thereby creating inflationary expectations that cause savers to begin spending again. Government stimulus spending and mercantilist policies to stimulate exports and reduce imports are other techniques to stimulate demand. He estimated in March 2010 that developed countries representing 70% of the world's GDP were caught in a liquidity trap.
Paradoxes of thrift and deleveraging.
Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.
During April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: "Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."
Predictors.
There are no known completely reliable predictors, but the following are considered possible predictors.
Government responses.
Most mainstream economists believe that recessions are caused by inadequate aggregate demand in the economy, and favor the use of expansionary macroeconomic policy during recessions. Strategies favored for moving an economy out of a recession vary depending on which economic school the policymakers follow. Monetarists would favor the use of expansionary monetary policy, while Keynesian economists may advocate increased government spending to spark economic growth. Supply-side economists may suggest tax cuts to promote business capital investment. When interest rates reach the boundary of an interest rate of zero percent (zero interest-rate policy) conventional monetary policy can no longer be used and government must use other measures to stimulate recovery. Keynesians argue that fiscal policy—tax cuts or increased government spending—works when monetary policy fails. Spending is more effective because of its larger multiplier but tax cuts take effect faster.
For example, Paul Krugman wrote in December 2010 that significant, sustained government spending was necessary because indebted households were paying down debts and unable to carry the U.S. economy as they had previously: "The root of our current troubles lies in the debt American families ran up during the Bush-era housing bubble...highly indebted Americans not only can’t spend the way they used to, they’re having to pay down the debts they ran up in the bubble years. This would be fine if someone else were taking up the slack. But what’s actually happening is that some people are spending much less while nobody is spending more — and this translates into a depressed economy and high unemployment. What the government should be doing in this situation is spending more while the private sector is spending less, supporting employment while those debts are paid down. And this government spending needs to be sustained..."
Stock market.
Some recessions have been anticipated by stock market declines. In "Stocks for the Long Run", Siegel mentions that since 1948, ten recessions were preceded by a stock market decline, by a lead time of 0 to 13 months (average 5.7 months), while ten stock market declines of greater than 10% in the Dow Jones Industrial Average were not followed by a recession.
The real-estate market also usually weakens before a recession. However real-estate declines can last much longer than recessions.
Since the business cycle is very hard to predict, Siegel argues that it is not possible to take advantage of economic cycles for timing investments. Even the National Bureau of Economic Research (NBER) takes a few months to determine if a peak or trough has occurred in the US.
During an economic decline, high yield stocks such as fast-moving consumer goods, pharmaceuticals, and tobacco tend to hold up better. However, when the economy starts to recover and the bottom of the market has passed (sometimes identified on charts as a MACD), growth stocks tend to recover faster. There is significant disagreement about how health care and utilities tend to recover. Diversifying one's portfolio into international stocks may provide some safety; however, economies that are closely correlated with that of the U.S. may also be affected by a recession in the U.S.
There is a view termed the "halfway rule" according to which investors start discounting an economic recovery about halfway through a recession. In the 16 U.S. recessions since 1919, the average length has been 13 months, although the recent recessions have been shorter. Thus if the 2008 recession followed the average, the downturn in the stock market would have bottomed around November 2008. The actual US stock market bottom of the 2008 recession was in March 2009.
Politics.
Generally an administration gets credit or blame for the state of economy during its time. This has caused disagreements about when a recession actually started. In an economic cycle, a downturn can be considered a consequence of an expansion reaching an unsustainable state, and is corrected by a brief decline. Thus it is not easy to isolate the causes of specific phases of the cycle.
The 1981 recession is thought to have been caused by the tight-money policy adopted by Paul Volcker, chairman of the Federal Reserve Board, before Ronald Reagan took office. Reagan supported that policy. Economist Walter Heller, chairman of the Council of Economic Advisers in the 1960s, said that "I call it a Reagan-Volcker-Carter recession. The resulting taming of inflation did, however, set the stage for a robust growth period during Reagan's administration.
Economists usually teach that to some degree recession is unavoidable, and its causes are not well understood. Consequently, modern government administrations attempt to take steps, also not agreed upon, to soften a recession.
Consequences.
Unemployment.
Unemployment is particularly high during a recession. Many economists working within the neoclassical paradigm argue that there is a natural rate of unemployment which, when subtracted from the actual rate of unemployment, can be used to calculate the negative GDP gap during a recession. In other words, unemployment never reaches 0 percent, and thus is not a negative indicator of the health of an economy unless above the "natural rate," in which case it corresponds directly to a loss in gross domestic product, or GDP.
The full impact of a recession on employment may not be felt for several quarters. Research in Britain shows that low-skilled, low-educated workers and the young are most vulnerable to unemployment in a downturn. After recessions in Britain in the 1980s and 1990s, it took five years for unemployment to fall back to its original levels. Many companies often expect employment discrimination claims to rise during a recession.
Business.
Productivity tends to fall in the early stages of a recession, then rises again as weaker firms close. The variation in profitability between firms rises sharply. Recessions have also provided opportunities for anti-competitive mergers, with a negative impact on the wider economy: the suspension of competition policy in the United States in the 1930s may have extended the Great Depression.
Social effects.
The living standards of people dependent on wages and salaries are not more affected by recessions than those who rely on fixed incomes or welfare benefits. The loss of a job is known to have a negative impact on the stability of families, and individuals' health and well-being. Fixed income benefits receive small cuts which make it tougher to survive.
History.
Global.
According to the International Monetary Fund (IMF), "Global recessions seem to occur over a cycle lasting between eight and 10 years." The IMF takes many factors into account when defining a global recession. Until April 2009, IMF several times communicated to the press, that a global annual real GDP growth of 3.0 percent or less in their view was "...equivalent to a global recession."
By this measure, six periods since 1970 qualify: 1974–1975, 1980–1983, 1990–1993, 1998, 2001–2002, and 2008–2009. During what IMF in April 2002 termed the past three global recessions of the last three decades, global per capita output growth was zero or negative, and IMF argued—at that time—that because of the opposite being found for 2001, the economic state in this year by itself did not qualify as a "global recession".
In April 2009, IMF changed their Global recession definition to: 
By this new definition, a total of four global recessions took place since World War II: 1975, 1982, 1991 and 2009. All of them only lasted one year, although the third would have lasted three years (1991–93) if IMF as criteria had used the normal exchange rate weighted percapita real World GDP rather than the purchase power parity weighted percapita real World GDP.
Australia.
The worst recession Australia has ever suffered happened in the beginning of the 1930s. As a result of late 1920s profit issues in agriculture and cutbacks, 1931-1932 saw Australia’s biggest recession in its entire history. It fared better than other nations, that underwent depressions, but their poor economic states influenced Australia’s as well, that depended on them for export, as well as foreign investments. The nation also benefited from bigger productivity in manufacturing, facilitated by trade protection, which also helped with feeling the effects less.
Australia was facing a rising level of inflation in 1973, caused partially by the oil crisis happening in that same year, which brought inflation at a 13% increase. Economic recession hit by the middle of the year 1974, with no change in policy enacted by the government as a measure to counter the economic situation of the country. Consequently, the unemployment level rose and the trade deficit increased significantly.
Another recession – the most recent one to date – came in the 1990s, at the beginning of the decade. It was the result of a major stock collapse in 1987, in October, referred to now as Black Monday. Although the collapse was larger than the one in 1929, the global economy recovered quickly, but North America still suffered a decline in lumbering savings and loans, which led to a crisis. The recession wasn’t limited to only America, but it also affected partnering nations, such as Australia. The unemployment level increased to 10.8%, employment declined by 3.4% and the GDP also decreased as much as 1.7%. Inflation, however, was successfully reduced.
United Kingdom.
The most recent recession to affect the United Kingdom was the late-2000s recession.
United States.
According to economists, since 1854, the U.S. has encountered 32 cycles of expansions and contractions, with an average of 17 months of contraction and 38 months of expansion. However, since 1980 there have been only eight periods of negative economic growth over one fiscal quarter or more, and four periods considered recessions:
For the past three recessions, the NBER decision has approximately conformed with the definition involving two consecutive quarters of decline. While the 2001 recession did not involve two consecutive quarters of decline, it was preceded by two quarters of alternating decline and weak growth.
Late 2000s.
Official economic data shows that a substantial number of nations were in recession as of early 2009. The US entered a recession at the end of 2007, and 2008 saw many other nations follow suit. The US recession of 2007 ended in June 2009 as the nation entered the current economic recovery.
United States.
The United States housing market correction (a possible consequence of United States housing bubble) and subprime mortgage crisis significantly contributed to a recession.
The 2007–2009 recession saw private consumption fall for the first time in nearly 20 years. This indicates the depth and severity of the current recession. With consumer confidence so low, recovery takes a long time. Consumers in the U.S. have been hard hit by the current recession, with the value of their houses dropping and their pension savings decimated on the stock market. Not only have consumers watched their wealth being eroded – they are now fearing for their jobs as unemployment rises.
U.S. employers shed 63,000 jobs in February 2008, the most in five years. Former Federal Reserve chairman Alan Greenspan said on 6 April 2008 that "There is more than a 50 percent chance the United States could go into recession." On 1 October, the Bureau of Economic Analysis reported that an additional 156,000 jobs had been lost in September. On 29 April 2008, Moody's declared that nine US states were in a recession. In November 2008, employers eliminated 533,000 jobs, the largest single month loss in 34 years. For 2008, an estimated 2.6 million U.S. jobs were eliminated.
The unemployment rate in the US grew to 8.5 percent in March 2009, and there were 5.1 million job losses until March 2009 since the recession began in December 2007. That was about five million more people unemployed compared to just a year prior, which was the largest annual jump in the number of unemployed persons since the 1940s.
Although the US Economy grew in the first quarter by 1%, by June 2008 some analysts stated that due to a protracted credit crisis and "...rampant inflation in commodities such as oil, food, and steel," the country was nonetheless in a recession. The third quarter of 2008 brought on a GDP retraction of 0.5% the biggest decline since 2001. The 6.4% decline in spending during Q3 on non-durable goods, like clothing and food, was the largest since 1950.
A 17 November 2008 report from the Federal Reserve Bank of Philadelphia based on the survey of 51 forecasters, suggested that the recession started in April 2008 and would last 14 months. They project real GDP declining at an annual rate of 2.9% in the fourth quarter and 1.1% in the first quarter of 2009. These forecasts represent significant downward revisions from the forecasts of three months ago.
A 1 December 2008, report from the National Bureau of Economic Research stated that the U.S. has been in a recession since December 2007 (when economic activity peaked), based on a number of measures including job losses, declines in personal income, and declines in real GDP. By July 2009 a growing number of economists believed that the recession may have ended. The National Bureau of Economic Research announced on 20 September 2010 that the 2008/2009 recession ended in June 2009, making it the longest recession since World War II.
Other countries.
Many other countries, particularly in Europe, have undergone decreasing rates of GDP growth. Some countries have been able to avoid a recession but have still experienced slower economic activity, such as China. India and Australia were able to maintain positive growth throughout the late-2000s recession.
China had their stock market crash, which began with the popping of the stock market bubble on 12 July 2015.
Canada officially declared a recession in 2015 after two quarters of shrinking GDP.

</doc>
<doc id="25385" url="https://en.wikipedia.org/wiki?curid=25385" title="RSA (cryptosystem)">
RSA (cryptosystem)

RSA is one of the first practical public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and differs from the decryption key which is kept secret. In RSA, this asymmetry is based on the practical difficulty of factoring the product of two large prime numbers, the factoring problem. RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1977. Clifford Cocks, an English mathematician working for the UK intelligence agency GCHQ, had developed an equivalent system in 1973, but it was not declassified until 1997.
A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, if the public key is large enough, only someone with knowledge of the prime numbers can feasibly decode the message.
Breaking RSA encryption is known as the RSA problem; whether it is as hard as the factoring problem remains an open question.
RSA is a relatively slow algorithm, and because of this it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.
History.
The idea of an asymmetric public-private key cryptosystem is attributed to Diffie and Hellman, who published the concept in 1976. The same two also introduced digital signatures and attempted to apply number theory. Their formulation used a shared secret key created from exponentiation of some number, modulo a prime number. However, they left open the problem of realizing a one-way function, possibly because the difficulty of factoring was not well studied at the time.
Ron Rivest, Adi Shamir, and Leonard Adleman at MIT made several attempts over the course of a year to create a one-way function that is hard to invert. Rivest and Shamir, as computer scientists, proposed many potential functions while Adleman, as a mathematician, was responsible for finding their weaknesses. They tried many approaches including "knapsack-based" and "permutation polynomials". For a time they thought it was impossible for what they wanted to achieve due to contradictory requirements. In April 1977, they spent Passover at the house of a student and drank a good deal of Manischewitz wine before returning to their home at around midnight. Rivest, unable to sleep, lay on the couch with a math textbook and started thinking about their one-way function. He spent the rest of the night formalizing his idea and had much of the paper ready by daybreak. The algorithm is now known as RSA – the initials of their surnames in same order as their paper.
Clifford Cocks, an English mathematician working for the UK intelligence agency GCHQ, described an equivalent system in an internal document in 1973. However, given the relatively expensive computers needed to implement it at the time, it was mostly considered a curiosity and, as far as is publicly known, was never deployed. His discovery, however, was not revealed until 1997 due to its top-secret classification.
Kid-RSA (KRSA) is a simplified public-key cipher published in 1997, designed for educational purposes. Some people feel that learning Kid-RSA gives insight into RSA and other public-key ciphers, analogous to simplified DES.
Patent.
MIT was granted for a "Cryptographic communications system and method" that used the algorithm, on September 20, 1983. Though the patent was going to expire on September 21, 2000 (the term of patent was 17 years at the time), the algorithm was released to the public domain by RSA Security on September 6, 2000, two weeks earlier. Since a paper describing the algorithm had been published in August 1977, prior to the December 1977 filing date of the patent application, regulations in much of the rest of the world precluded patents elsewhere and only the US patent was granted. Had Cocks' work been publicly known, a patent in the US would not have been possible either.
From the DWPI's abstract of the patent,
Operation.
The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption.
RSA involves a "public key" and a "private key." The public key can be known by everyone and is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time using the private key. 
The basic principle behind RSA is the observation that it is practical to find three very large positive integers , and such that with modular exponentiation for all :
and that even knowing and or even it can be extremely difficult to find .
Additionally, for some operations it is convenient that the order of the two exponentiations can be changed and that this relation also implies:
Key distribution.
To enable Bob to send his encrypted messages, Alice transmits her public key to Bob via a reliable, but not necessarily secret route. The private key is never distributed.
Encryption.
Suppose that Bob would like to send message to Alice.
He first turns into an integer , such that and by using an agreed-upon reversible protocol known as a padding scheme. He then computes the ciphertext , using Alice's public key , corresponding to
This can be done efficiently, even for 500-bit numbers, using modular exponentiation. Bob then transmits to Alice.
Decryption.
Alice can recover from by using her private key exponent by computing
Given , she can recover the original message by reversing the padding scheme.
Key generation.
The keys for the RSA algorithm are generated the following way:
The "public key" consists of the modulus "n" and the public (or encryption) exponent "e". The "private key" consists of the modulus "n" and the private (or decryption) exponent "d", which must be kept secret. "p", "q", and φ("n") must also be kept secret because they can be used to calculate "d".
Since any common factors of (p − 1) and (q − 1) are present in the factorisation of pq − 1, it is recommended that (p − 1) and (q − 1) have only very small common factors, if any besides the necessary 2.
Note: The authors of RSA carry out the key generation by choosing d and then computing e as the modular multiplicative inverse of d (modulo φ(n)). Since it is beneficial to use a small value for e (i.e. 65,537) in order to speed up the encryption function, current implementations of RSA, such as PKCS#1 choose e and compute d instead.
Example.
Here is an example of RSA encryption and decryption. The parameters used here are artificially small, but one can also .
The public key is (, ). For a padded plaintext message "m", the encryption function is
The private key is (). For an encrypted ciphertext "c", the decryption function is
For instance, in order to encrypt , we calculate
To decrypt , we calculate
Both of these calculations can be computed efficiently using the square-and-multiply algorithm for modular exponentiation. In real-life situations the primes selected would be much larger; in our example it would be trivial to factor "n", 3233 (obtained from the freely available public key) back to the primes "p" and "q". "e", also from the public key, is then inverted to get "d", thus acquiring the private key.
Practical implementations use the Chinese remainder theorem to speed up the calculation using modulus of factors (mod "pq" using mod "p" and mod "q").
The values "d""p", "d""q" and "q"inv, which are part of the private key are computed as follows:
Here is how "d""p", "d""q" and "q"inv are used for efficient decryption. (Encryption is efficient by choice of a suitable "d" and "e" pair)
Code.
A working example in JavaScript.
<syntaxhighlight lang="javascript">
'use strict';
var RSA = {};
RSA.generate = function(){
RSA.encrypt = function(m, n, e){
 return bigInt(m).pow(e).mod(n); 
RSA.decrypt = function(mEnc, d, n){
 return bigInt(mEnc).pow(d).mod(n); 
</syntaxhighlight>
Signing messages.
Suppose Alice uses Bob's public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob's public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.
Suppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of "d" (modulo "n") (as she does when decrypting a message), and attaches it as a "signature" to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice's public key. He raises the signature to the power of "e" (modulo "n") (as he does when encrypting a message), and compares the resulting hash value with the message's actual hash value. If the two agree, he knows that the author of the message was in possession of Alice's private key, and that the message has not been tampered with since.
Proofs of correctness.
Proof using Fermat's little theorem.
The proof of the correctness of RSA is based on Fermat's little theorem. This theorem states that if "p" is prime and "p" does not divide an integer "a" then
We want to show that for every integer "m" when "p" and "q" are distinct prime numbers and "e" and "d" are positive integers satisfying
Since formula_21, we can write
for some nonnegative integer "h".
To check whether two numbers, like "med" and "m", are congruent mod "pq" it suffices (and in fact is equivalent) to check they are congruent mod "p" and mod "q" separately. (This is part of the Chinese remainder theorem, although it is not the significant part of that theorem.) To show , we consider two cases: and .
In the first case "med" is a multiple of "p", so . In the second case
where we used Fermat's little theorem to replace "m""p"−1 mod "p" with 1.
The verification that proceeds in a similar way, treating separately the cases and , using Fermat's little theorem for modulus "q" in the second case.
This completes the proof that, for any integer "m", and integers "e", "d" such that formula_24,
Proof using Euler's theorem.
Although the original paper of Rivest, Shamir, and Adleman used Fermat's little theorem to explain why RSA works, it is common to find proofs that rely instead on Euler's theorem.
We want to show that , where is a product of two different prime numbers and "e" and "d" are positive integers satisfying . Since "e" and "d" are positive, we can write for some non-negative integer "h". "Assuming" that "m" is relatively prime to "n", we have
where the second-last congruence follows from Euler's theorem.
When "m" is not relatively prime to "n", the argument just given is invalid. This is highly improbable (only a proportion of numbers have this property), but even in this case the desired congruence is still true. Either or , and these cases can be treated using the previous proof.
Padding.
Attacks against plain RSA.
There are a number of attacks against plain RSA as described above.
Padding schemes.
To avoid these problems, practical RSA implementations typically embed some form of structured, randomized padding into the value "m" before encrypting it. This padding ensures that "m" does not fall into the range of insecure plaintexts, and that a given message, once padded, will encrypt to one of a large number of different possible ciphertexts.
Standards such as PKCS#1 have been carefully designed to securely pad messages prior to RSA encryption. Because these schemes pad the plaintext "m" with some number of additional bits, the size of the un-padded message "M" must be somewhat smaller. RSA padding schemes must be carefully designed so as to prevent sophisticated attacks which may be facilitated by a predictable message structure. Early versions of the PKCS#1 standard (up to version 1.5) used a construction that appears to make RSA semantically secure. However, at Eurocrypt 2000, Coron et al. showed that for some types of messages, this padding does not provide a high enough level of security. Furthermore, at Crypto 1998, Bleichenbacher showed that this version is vulnerable to a practical adaptive chosen ciphertext attack. Later versions of the standard include Optimal Asymmetric Encryption Padding (OAEP), which prevents these attacks. As such, OAEP should be used in any new application, and PKCS#1 v1.5 padding should be replaced wherever possible. The PKCS#1 standard also incorporates processing schemes designed to provide additional security for RSA signatures, e.g. the Probabilistic Signature Scheme for RSA (RSA-PSS).
Secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption. Two US patents on PSS were granted (USPTO 6266771 and USPTO 70360140); however, these patents expired on 24 July 2009 and 25 April 2010, respectively. Use of PSS no longer seems to be encumbered by patents. Note that using different RSA key-pairs for encryption and signing is potentially more secure.
Security and practical considerations.
Using the Chinese remainder algorithm.
For efficiency many popular crypto libraries (like OpenSSL, Java and .NET) use the following optimization for decryption and signing based on the Chinese remainder theorem. The following values are precomputed and stored as part of the private key:
These values allow the recipient to compute the exponentiation more efficiently as follows:
This is more efficient than computing exponentiation by squaring even though two modular exponentiations have to be computed. The reason is that these two modular exponentiations both use a smaller exponent and a smaller modulus.
Integer factorization and RSA problem.
The security of the RSA cryptosystem is based on two mathematical problems: the problem of factoring large numbers and the RSA problem. Full decryption of an RSA ciphertext is thought to be infeasible on the assumption that both of these problems are hard, i.e., no efficient algorithm exists for solving them. Providing security against "partial" decryption may require the addition of a secure padding scheme.
The RSA problem is defined as the task of taking "e"th roots modulo a composite "n": recovering a value "m" such that , where is an RSA public key and "c" is an RSA ciphertext. Currently the most promising approach to solving the RSA problem is to factor the modulus "n". With the ability to recover prime factors, an attacker can compute the secret exponent "d" from a public key , then decrypt "c" using the standard procedure. To accomplish this, an attacker factors "n" into "p" and "q", and computes which allows the determination of "d" from "e". No polynomial-time method for factoring large integers on a classical computer has yet been found, but it has not been proven that none exists. "See integer factorization for a discussion of this problem".
Multiple polynomial quadratic sieve (MPQS) can be used to factor the public modulus "n". The time taken to factor 128-bit and 256-bit "n" on a desktop computer are respectively 2 seconds and 35 minutes.
A tool called yafu can be used to optimize this process. The automation within YAFU is state-of-the-art, combining factorization algorithms in an intelligent and adaptive methodology that minimizes the time to find the factors of arbitrary input integers. Most algorithm implementations are multi-threaded, allowing YAFU to fully utilize multi- or many-core processors (including SNFS, GNFS, SIQS, and ECM). YAFU is primarily a command-line driven tool. The time taken to factor "n" using yafu on the same computer was reduced to 103.1746 seconds. Yafu requires the GGNFS binaries to factor N that are 320 bits or larger. This is a very complicated software that requires a certain amount of technical skill to install and configure. It took about 5720s to factor "320bit-N" on the same computer.
In 2009, Benjamin Moody has factored an RSA-512 bit key in 73 days using only public software (GGNFS) and his desktop computer (dual-core Athlon64 at 1,900 MHz). Just under 5 gigabytes of disk was required and about 2.5 gigabytes of RAM for the sieving process. The first RSA-512 factorization in 1999 required the equivalent of 8,400 MIPS years over an elapsed time of about 7 months.
Rivest, Shamir and Adleman note that Miller has shown that – assuming the Extended Riemann Hypothesis – finding "d" from "n" and "e" is as hard as factoring "n" into "p" and "q" (up to a polynomial time difference). However, Rivest, Shamir and Adleman note (in section IX / D of their paper) that they have not found a proof that inverting RSA is equally hard as factoring.
, the largest factored RSA number was 768 bits long (232 decimal digits, see RSA-768). Its factorization, by a state-of-the-art distributed implementation, took around fifteen hundred CPU years (two years of real time, on many hundreds of computers). No larger RSA key is publicly known to have been factored. In practice, RSA keys are typically 1024 to 4096 bits long. Some experts believe that 1024-bit keys may become breakable in the near future or may already be breakable by a sufficiently well-funded attacker (though this is disputed); few see any way that 4096-bit keys could be broken in the foreseeable future. Therefore, it is generally presumed that RSA is secure if "n" is sufficiently large. If "n" is 300 bits or shorter, it can be factored in a few hours on a personal computer, using software already freely available. Keys of 512 bits have been shown to be practically breakable in 1999 when RSA-155 was factored by using several hundred computers and are now factored in a few weeks using common hardware. Exploits using 512-bit code-signing certificates that may have been factored were reported in 2011. A theoretical hardware device named TWIRL and described by Shamir and Tromer in 2003 called into question the security of 1024 bit keys. It is currently recommended that "n" be at least 2048 bits long.
In 1994, Peter Shor showed that a quantum computer (if one could ever be practically created for the purpose) would be able to factor in polynomial time, breaking RSA; see Shor's algorithm.
Faulty key generation.
Finding the large primes "p" and "q" is usually done by testing random numbers of the right size with probabilistic primality tests which quickly eliminate virtually all non-primes.
Numbers "p" and "q" should not be 'too close', lest the Fermat factorization for "n" be successful, if "p" − "q", for instance is less than 2"n"1/4 (which for even small 1024-bit values of "n" is ) solving for "p" and "q" is trivial. Furthermore, if either "p" − 1 or "q" − 1 has only small prime factors, "n" can be factored quickly by Pollard's "p" − 1 algorithm, and these values of "p" or "q" should therefore be discarded as well.
It is important that the private exponent "d" be large enough. Michael J. Wiener showed that if "p" is between "q" and 2"q" (which is quite typical) and , then "d" can be computed efficiently from "n" and "e".
There is no known attack against small public exponents such as , provided that proper padding is used. Coppersmith's Attack has many applications in attacking RSA specifically if the public exponent "e" is small and if the encrypted message is short and not padded. 65537 is a commonly used value for "e"; this value can be regarded as a compromise between avoiding potential small exponent attacks and still allowing efficient encryptions (or signature verification). The NIST Special Publication on Computer Security (SP 800-78 Rev 1 of August 2007) does not allow public exponents "e" smaller than 65537, but does not state a reason for this restriction.
Importance of strong random number generation.
A cryptographically strong random number generator, which has been properly seeded with adequate entropy, must be used to generate the primes "p" and "q". An analysis comparing millions of public keys gathered from the Internet was carried out in early 2012 by Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung and Christophe Wachter. They were able to factor 0.2% of the keys using only Euclid's algorithm.
They exploited a weakness unique to cryptosystems based on integer factorization. If is one public key and is another, then if by chance (but q is not equal to q'), then a simple computation of factors both "n" and "n"′, totally compromising both keys. Lenstra et al. note that this problem can be minimized by using a strong random seed of bit-length twice the intended security level, or by employing a deterministic function to choose "q" given "p", instead of choosing "p" and "q" independently.
Nadia Heninger was part of a group that did a similar experiment. They used an idea of Daniel J. Bernstein to compute the GCD of each RSA key "n" against the product of all the other keys "n"′ they had found (a 729 million digit number), instead of computing each gcd("n","n"′) separately, thereby achieving a very significant speedup since after one large division the GCD problem is of normal size.
Heninger says in her blog that the bad keys occurred almost entirely in embedded applications, including "firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones" from over 30 manufacturers. Heninger explains that the one-shared-prime problem uncovered by the two groups results from situations where the pseudorandom number generator is poorly seeded initially and then reseeded between the generation of the first and second primes. Using seeds of sufficiently high entropy obtained from key stroke timings or electronic diode noise or atmospheric noise from a radio receiver tuned between stations should solve the problem.
Strong random number generation is important throughout every phase of public key cryptography. For instance, if a weak generator is used for the symmetric keys that are being distributed by RSA, then an eavesdropper could bypass the RSA and guess the symmetric keys directly.
Timing attacks.
Kocher described a new attack on RSA in 1995: if the attacker Eve knows Alice's hardware in sufficient detail and is able to measure the decryption times for several known ciphertexts, she can deduce the decryption key "d" quickly. This attack can also be applied against the RSA signature scheme. In 2003, Boneh and Brumley demonstrated a more practical attack capable of recovering RSA factorizations over a network connection (e.g., from a Secure Sockets Layer (SSL)-enabled webserver) This attack takes advantage of information leaked by the Chinese remainder theorem optimization used by many RSA implementations.
One way to thwart these attacks is to ensure that the decryption operation takes a constant amount of time for every ciphertext. However, this approach can significantly reduce performance. Instead, most RSA implementations use an alternate technique known as cryptographic blinding. RSA blinding makes use of the multiplicative property of RSA. Instead of computing , Alice first chooses a secret random value "r" and computes . The result of this computation after applying Euler's Theorem is and so the effect of "r" can be removed by multiplying by its inverse. A new value of "r" is chosen for each ciphertext. With blinding applied, the decryption time is no longer correlated to the value of the input ciphertext and so the timing attack fails.
Adaptive chosen ciphertext attacks.
In 1998, Daniel Bleichenbacher described the first practical adaptive chosen ciphertext attack, against RSA-encrypted messages using the PKCS #1 v1 padding scheme (a padding scheme randomizes and adds structure to an RSA-encrypted message, so it is possible to determine whether a decrypted message is valid). Due to flaws with the PKCS #1 scheme, Bleichenbacher was able to mount a practical attack against RSA implementations of the Secure Socket Layer protocol, and to recover session keys. As a result of this work, cryptographers now recommend the use of provably secure padding schemes such as Optimal Asymmetric Encryption Padding, and RSA Laboratories has released new versions of PKCS #1 that are not vulnerable to these attacks.
Side-channel analysis attacks.
A side-channel attack using branch prediction analysis (BPA) has been described. Many processors use a branch predictor to determine whether a conditional branch in the instruction flow of a program is likely to be taken or not. Often these processors also implement simultaneous multithreading (SMT). Branch prediction analysis attacks use a spy process to discover (statistically) the private key when processed with these processors.
Simple Branch Prediction Analysis (SBPA) claims to improve BPA in a non-statistical way. In their paper, "On the Power of Simple Branch Prediction Analysis", the authors of SBPA (Onur Aciicmez and Cetin Kaya Koc) claim to have discovered 508 out of 512 bits of an RSA key in 10 iterations.
A power fault attack on RSA implementations has been described in 2010. The authors recovered the key by varying the CPU power voltage outside limits; this caused multiple power faults on the server.

</doc>
<doc id="25389" url="https://en.wikipedia.org/wiki?curid=25389" title="Robert A. Heinlein">
Robert A. Heinlein

Robert Anson Heinlein (; July 7, 1907 – May 8, 1988) was an American science fiction writer. Often called the "dean of science fiction writers", he was an influential and controversial author of the genre in his time.
He was one of the first science fiction writers to break into mainstream magazines such as "The Saturday Evening Post" in the late 1940s. He was one of the best-selling science fiction novelists for many decades, and he, Isaac Asimov, and Arthur C. Clarke are often considered to be the "Big Three" of science fiction authors.
A notable writer of science fiction short stories, Heinlein was one of a group of writers who came to prominence under the editorship of John W. Campbell, Jr. in his "Astounding Science Fiction" magazine—though Heinlein denied that Campbell influenced his writing to any great degree.
Within the framework of his science fiction stories, Heinlein repeatedly addressed certain social themes: the importance of individual liberty and self-reliance, the obligation individuals owe to their societies, the influence of organized religion on culture and government, and the tendency of society to repress nonconformist thought. He also speculated on the influence of space travel on human cultural practices.
Heinlein was named the first Science Fiction Writers Grand Master in 1974. He won Hugo Awards for four of his novels; in addition, fifty years after publication, three of his works were awarded "Retro Hugos"—awards given retrospectively for works that were published before the Hugo Awards came into existence. In his fiction, Heinlein coined terms that have become part of the English language, including "grok" and "waldo", and speculative fiction, as well as popularizing existing terms like "TANSTAAFL", "pay it forward", and space marine. He also anticipated Mechanical Computer Aided Design with "Drafting Dan" and described a modern version of a waterbed in his novel "The Door into Summer", though he never patented or built one. In the first chapter of the novel "Space Cadet" he anticipated the cell phone, 35 years before the technology was invented by Motorola. Several of Heinlein's works have been adapted for film and television.
Life.
Birth and childhood.
Heinlein was born on July 7, 1907 to Rex Ivar Heinlein (an accountant) and Bam Lyle Heinlein, in Butler, Missouri. He was a 6th-generation German-American: a family tradition had it that Heinleins fought in every American war starting with the War of Independence.
His childhood was spent in Kansas City, Missouri. The outlook and values of this time and place (in his own words, "The Bible Belt") had a definite influence on his fiction, especially his later works, as he drew heavily upon his childhood in establishing the setting and cultural atmosphere in works like "Time Enough for Love" and "To Sail Beyond the Sunset".
Navy.
Heinlein's experience in the U.S. Navy exerted a strong influence on his character and writing. Heinlein graduated from the U.S. Naval Academy in Annapolis, Maryland, in 1929 with a B.S. degree in naval engineering, and he served as an officer in the Navy. He was assigned to the new aircraft carrier in 1931, where he worked in radio communications, then in its earlier phases, with the carrier's aircraft. The captain of this carrier was Ernest J. King, who later served as the Chief of Naval Operations and Commander-in-Chief, U.S. Fleet during World War II. Heinlein was frequently interviewed during his later years by military historians who asked him about Captain King and his service as the commander of the U.S. Navy's first modern aircraft carrier.
Heinlein also served aboard the destroyer in 1933 and 1934, reaching the rank of lieutenant. His brother, Lawrence Heinlein, served in the U.S. Army, the U.S. Air Force, and the Missouri National Guard, and he rose to the rank of major general in the National Guard.
In 1929, Heinlein married Elinor Curry of Kansas City in Los Angeles, and their marriage lasted about a year. His second marriage in 1932 to Leslyn MacDonald (1904–1981) lasted for 15 years. MacDonald was, according to the testimony of Heinlein's Navy buddy, Caleb 'Cab' Lansing, "astonishingly intelligent, widely read, and extremely liberal, though a registered Republican," while Isaac Asimov later recalled that Heinlein was, at the time, "a flaming liberal". "(See section: Politics of Robert Heinlein.)"
California.
In 1934, Heinlein was discharged from the Navy due to pulmonary tuberculosis. During a lengthy hospitalization, he developed a design for a waterbed.
After his discharge, Heinlein attended a few weeks of graduate classes in mathematics and physics at the University of California at Los Angeles (UCLA), but he soon quit either because of his health or from a desire to enter politics.
Heinlein supported himself at several occupations, including real estate sales and silver mining, but for some years found money in short supply. Heinlein was active in Upton Sinclair's socialist End Poverty in California movement in the early 1930s. When Sinclair gained the Democratic nomination for Governor of California in 1934, Heinlein worked actively in the campaign. Heinlein himself ran for the California State Assembly in 1938, but he was unsuccessful.
Author.
While not destitute after the campaign—he had a small disability pension from the Navy—Heinlein turned to writing in order to pay off his mortgage. His first published story, "Life-Line", was printed in the August 1939 issue of "Astounding Science-Fiction". Originally written for a contest, it was instead sold to "Astounding" for significantly more than the contest's first-prize payoff. Another Future History story, "Misfit", followed in November. Heinlein was quickly acknowledged as a leader of the new movement toward "social" science fiction. In California he hosted the Mañana Literary Society, a 1940-41 series of informal gatherings of new authors. He was the guest of honor at Denvention, the 1941 Worldcon, held in Denver. During World War II, he did aeronautical engineering for the U.S. Navy, also recruiting Isaac Asimov and L. Sprague de Camp to work at the Philadelphia Naval Shipyard in Pennsylvania.
As the war wound down in 1945, Heinlein began re-evaluating his career. The atomic bombings of Hiroshima and Nagasaki, along with the outbreak of the Cold War, galvanized him to write nonfiction on political topics. In addition, he wanted to break into better-paying markets. He published four influential short stories for "The Saturday Evening Post" magazine, leading off, in February 1947, with "The Green Hills of Earth". That made him the first science fiction writer to break out of the "pulp ghetto". In 1950, the movie "Destination Moon"—the documentary-like film for which he had written the story and scenario, co-written the script, and invented many of the effects—won an Academy Award for special effects. Also, he embarked on a series of juvenile S.F. novels for the Charles Scribner's Sons publishing company that went from 1947 through 1959, at the rate of one book each autumn, in time for Christmas presents to teenagers. He also wrote for "Boys' Life" in 1952.
At the Philadelphia Naval Shipyard he had met and befriended a chemical engineer named Virginia "Ginny" Gerstenfeld. After the war, her engagement having fallen through, she moved to UCLA for doctoral studies in chemistry, and made contact again.
As his second wife's alcoholism gradually spun out of control, Heinlein moved out and the couple filed for divorce. Heinlein's friendship with Virginia turned into a relationship and on October 21, 1948 — shortly after the decree nisi came through — they married in the town of Raton, New Mexico shortly after having set up house in Colorado. They would remain married until Heinlein's death.
As Heinlein's increasing success as a writer resolved their initial financial woes, they had a house custom built with various innovative features, later described in an article in "Popular Mechanics". In 1965, after various chronic health problems of Virginia's were traced back to altitude sickness, they moved to Santa Cruz, California, at sea level, while they were building a new residence in the adjacent village of Bonny Doon, California. Their unique circular California house—which like their Colorado house, he designed along with Virginia and then built himself—is on Bonny Doon Road .
Ginny undoubtedly served as a model for many of his intelligent, fiercely independent female characters. She was a chemist, rocket test engineer, and held a higher rank in the Navy than Heinlein himself. She was also an accomplished college athlete, earning four letters. In 1953–1954, the Heinleins voyaged around the world (mostly via ocean liners and cargo liners, as Ginny detested flying), which Heinlein described in "Tramp Royale", and which also provided background material for science fiction novels set aboard spaceships on long voyages, such as "Podkayne of Mars" and "Friday". Ginny acted as the first reader of his manuscripts. Isaac Asimov believed that Heinlein made a swing to the right politically at the same time he married Ginny.
The Heinleins formed the small "Patrick Henry League" in 1958, and they worked in the 1964 Barry Goldwater Presidential campaign.
Heinlein had used topical materials throughout his juvenile series beginning in 1947, but in 1959, his novel "Starship Troopers" was considered by the editors and owners of Scribner's to be too controversial for one of its prestige lines, and it was rejected.
Heinlein found another publisher (Putnam), feeling himself released from the constraints of writing novels for children, and he began to write "my own stuff, my own way", and he wrote a series of challenging books that redrew the boundaries of science fiction, including "Stranger in a Strange Land" (1961) and "The Moon Is a Harsh Mistress" (1966).
Later life and death.
Beginning in 1970, Heinlein had a series of health crises, broken by strenuous periods of activity in his hobby of stonemasonry. (In a private correspondence, he referred to that as his "usual and favorite occupation between books.") The decade began with a life-threatening attack of peritonitis, recovery from which required more than two years, and treatment of which required multiple transfusions of Heinlein's rare blood type, A2 negative. As soon as he was well enough to write again, he began work on "Time Enough for Love" (1973), which introduced many of the themes found in his later fiction.
In the mid-1970s, Heinlein wrote two articles for the "Britannica Compton Yearbook". He and Ginny crisscrossed the country helping to reorganize blood donation in the United States in an effort to assist the system which had saved his life, and he was the guest of honor at the Worldcon for the third time at MidAmeriCon in Kansas City, Missouri, in 1976. While vacationing in Tahiti in early 1978, he suffered a transient ischemic attack. Over the next few months, he became more and more exhausted, and his health again began to decline. The problem was determined to be a blocked carotid artery, and he had one of the earliest known carotid bypass operations to correct it. Heinlein and Virginia had been smokers, and smoking appears often in his fiction, as do fictitious strikable self-lighting cigarettes.
In 1980 Robert Heinlein was a member of the Citizens Advisory Council on National Space Policy, chaired by Jerry Pournelle, which met at the home of SF writer Larry Niven to write space policy papers for the incoming Reagan Administration. Members included Buzz Aldrin, General Daniel Graham, rocket engineer Max Hunter, North American VP and Space Shuttle manager George Merrick, and other aerospace industry leaders. Policy recommendations from the Council included ballistic missile defense concepts which were later transformed into what was called the Strategic Defense Initiative by those who favored it, and "Star Wars" as a term of derision coined by Senator Ted Kennedy. Heinlein contributed to the Council contribution to the Reagan "Star Wars" speech of Spring 1983.
Asked to appear before a Joint Committee of the U.S. House and Senate that year, he testified on his belief that spin-offs from space technology were benefiting the infirm and the elderly. Heinlein's surgical treatment re-energized him, and he wrote five novels from 1980 until he died in his sleep from emphysema and heart failure on May 8, 1988.
At that time, he had been putting together the early notes for another "World as Myth" novel. Several of his other works have been published posthumously.
After his death, his wife Virginia Heinlein issued a compilation of Heinlein's correspondence and notes into a somewhat autobiographical examination of his career, published in 1989 under the title "Grumbles from the Grave". Heinlein's archive is housed by the Special Collections department of McHenry Library at the University of California at Santa Cruz. The collection includes manuscript drafts, correspondence, photographs and artifacts. A substantial portion of the archive has been digitized and it is available online through the Robert A. and Virginia Heinlein Archives.
Works.
Heinlein published 32 novels, 59 short stories, and 16 collections during his life. Four films, two television series, several episodes of a radio series, and a board game have been derived more or less directly from his work. He wrote a screenplay for one of the films. Heinlein edited an anthology of other writers' SF short stories.
Three nonfiction books and two poems have been published posthumously. "For Us, The Living: A Comedy of Customs" was published posthumously in 2003; "Variable Star", written by Spider Robinson based on an extensive outline by Heinlein, was published in September 2006. Four collections have been published posthumously.
Series.
Over the course of his career Heinlein wrote three somewhat overlapping series.
Early work, 1939–1958.
Heinlein began his career as a writer of stories for Astounding Science Fiction, a highly respected science fiction magazine, which was edited by John Campbell. The science fiction writer Frederik Pohl has described Heinlein as "that greatest of Campbell-era sf writers". Isaac Asimov said that, from the time of his first story, it was accepted that Heinlein was the best science fiction writer in existence, adding that he would hold this title through his lifetime.
Alexei and Cory Panshin noted that Heinlein's impact was immediately felt. In 1940, the year after selling 'Life-Line' to Campbell, he wrote three short novels, four novelettes, and seven short stories. They went on to say that "No one ever dominated the science fiction field as Bob did in the first few years of his career." Alexei expresses awe in Heinlein's ability to show readers a world so drastically different from the one we live in now, yet have so many similarities. He says that "We find ourselves not only in a world other than our own, but identifying with a living, breathing individual who is operating within its context, and thinking and acting according to its terms."
The first novel that Heinlein wrote, "" (1939), did not see print during his lifetime, but Robert James tracked down the manuscript and it was published in 2003. Though some regard it as a failure as a novel, considering it little more than a disguised lecture on Heinlein's social theories, some readers took a very different view. In a review of it, John Clute wrote: "I'm not about to suggest that if Heinlein had been able to publish works openly in the pages of Astounding in 1939, SF would have gotten the future right; I would suggest, however, that if Heinlein, and his colleagues, had been able to publish adult SF in Astounding and its fellow journals, then SF might not have done such a grotesquely poor job of prefiguring something of the flavor of actually living here at the onset of 2004."
"For Us, the Living" was intriguing as a window into the development of Heinlein's radical ideas about man as a social animal, including his interest in free love. The root of many themes found in his later stories can be found in this book. It also contained much material that could be considered background for his other novels, including a detailed description of the protagonist's treatment to avoid being banned to Coventry (a lawless land in the Heinlein mythos where unrepentant law-breakers are exiled).
It appears that Heinlein at least attempted to live in a manner consistent with these ideals, even in the 1930s, and had an open relationship in his marriage to his second wife, Leslyn. He was also a nudist; nudism and body taboos are frequently discussed in his work. At the height of the Cold War, he built a bomb shelter under his house, like the one featured in "Farnham's Freehold."
After "For Us, The Living", Heinlein began selling (to magazines) first short stories, then novels, set in a Future History, complete with a time line of significant political, cultural, and technological changes. A chart of the future history was published in the May 1941 issue of "Astounding". Over time, Heinlein wrote many novels and short stories that deviated freely from the Future History on some points, while maintaining consistency in some other areas. The Future History was eventually overtaken by actual events. These discrepancies were explained, after a fashion, in his later World as Myth stories.
Heinlein's first novel published as a book, "Rocket Ship Galileo", was initially rejected because going to the moon was considered too far out, but he soon found a publisher, Scribner's, that began publishing a Heinlein juvenile once a year for the Christmas season. Eight of these books were illustrated by Clifford Geary in a distinctive white-on-black scratchboard style. Some representative novels of this type are "Have Space Suit—Will Travel", "Farmer in the Sky", and "Starman Jones". Many of these were first published in serial form under other titles, e.g., "Farmer in the Sky" was published as "Satellite Scout" in the Boy Scout magazine "Boys' Life". There has been speculation that Heinlein's intense obsession with his privacy was due at least in part to the apparent contradiction between his unconventional private life and his career as an author of books for children, but "For Us, The Living" also explicitly discusses the political importance Heinlein attached to privacy as a matter of principle.
The novels that Heinlein wrote for a young audience are commonly called "the Heinlein juveniles", and they feature a mixture of adolescent and adult themes. Many of the issues that he takes on in these books have to do with the kinds of problems that adolescents experience. His protagonists are usually very intelligent teenagers who have to make their way in the adult society they see around them. On the surface, they are simple tales of adventure, achievement, and dealing with stupid teachers and jealous peers. Heinlein was a vocal proponent of the notion that juvenile readers were far more sophisticated and able to handle more complex or difficult themes than most people realized. His juvenile stories often had a maturity to them that made them readable for adults. "Red Planet", for example, portrays some very subversive themes, including a revolution in which young students are involved; his editor demanded substantial changes in this book's discussion of topics such as the use of weapons by children and the misidentified sex of the Martian character. Heinlein was always aware of the editorial limitations put in place by the editors of his novels and stories, and while he observed those restrictions on the surface, was often successful in introducing ideas not often seen in other authors' juvenile SF.
In 1957, James Blish wrote that one reason for Heinlein's success "has been the high grade of machinery which goes, today as always, into his story-telling. Heinlein seems to have known from the beginning, as if instinctively, technical lessons about fiction which other writers must learn the hard way (or often enough, never learn). He does not always operate the machinery to the best advantage, but he always seems to be aware of it."
1959–1960.
Heinlein decisively ended his juvenile novels with "Starship Troopers" (1959), a controversial work and his personal riposte to leftists calling for President Dwight D. Eisenhower to stop nuclear testing in 1958. "The "Patrick Henry" ad shocked 'em," he wrote many years later. ""Starship Troopers" outraged 'em." "Starship Troopers" is a coming-of-age story about duty, citizenship, and the role of the military in society. The book portrays a society in which suffrage is earned by demonstrated willingness to place society's interests before one's own, at least for a short time and often under onerous circumstances, in government service; in the case of the protagonist, this was military service.
Later, in "Expanded Universe", Heinlein said that it was his intention in the novel that service could include positions outside strictly military functions such as teachers, police officers, and other government positions. This is presented in the novel as an outgrowth of the failure of unearned suffrage government and as a very successful arrangement. In addition, the franchise was only awarded "after" leaving the assigned service, thus those serving their terms—in the military, or any other service—were excluded from exercising any franchise. Career military were completely disenfranchised until retirement.
The name "Starship Troopers" was licensed for an unrelated, B movie script called "Bug Hunt at Outpost Nine", which was then retitled to benefit from the book's credibility.Verhoeven went returned to genre territory, optioning a script from his ROBOCOP collaborator Ed Neumeier entitled BUG HUNT AT OUTPOST 9 and refashioning it with elements from Robert Heinlein’s STARSHIP TROOPERS. A loose adaptation at best, Verhoeven saw the potential in another science-fiction satire and pursued it head-on</ref> The resulting film, entitled "Starship Troopers" (1997), which was written by Ed Neumeier and directed by Paul Verhoeven, had little relationship to the book, beyond the inclusion of character names, the depiction of space marines, and the concept of suffrage earned by military service. Fans of Heinlein were critical of the movie, which they considered a betrayal of Heinlein's philosophy, presenting the society in which the story takes place as fascist. Christopher Weuve, an admirer of Heinlein, has said that the society depicted in the film showed only a superficial resemblance to the society that Heinlein describes in his book. Weuve summed up his critique of the film as follows. First, "while the Terran Federation in "Starship Troopers" novel is specifically stated to be a representative democracy, Ed Neumeier decided to make the government into a fascist state ... Second, the book was multiracial, but not so the movie: all the non-Anglo characters from the book have been replaced by characters who look like they stepped out of the Aryan edition of "GQ"... Third, there is real element of sadism present in the movie which simply isn't present in the book."
Likewise, the powered armor technology that is not only central to the book, but became a standard subgenre of science fiction thereafter, is completely absent in the movie, where the characters use World War II-technology weapons and wear light combat gear little more advanced than that. According to Verhoeven, this, and the fascist tone of the book, reflected his own experience in the Nazi-occupied Netherlands during World War II.It was an attempt to upgrade the old style Fox Movietone newsreels...and Third Reich propaganda films and even my old Marines documentaries that I did, because a lot of that was promotion and propaganda as well...That's why the relationship to the second world war is so important to me because it was probably the last war, and one of the few wars in history, where you can make the argument that it was good</ref>
In fact, Verhoeven had not even read the book, attempting to after he bought the rights to add to his existing movie, and disliking it: "I stopped after two chapters because it was so boring...It is really quite a bad book. I asked Ed Neumeier to tell me the story because I just couldn't read the thing"."I stopped after two chapters because it was so boring," says Verhoeven of his attempts to read Heinlein's opus. "It is really quite a bad book. I asked Ed Neumeier to tell me the story because I just couldn't read the thing. It's a very right-wing book. And with the movie we tried, and I think at least partially succeeded, in commenting on that at the same time. It would be eat your cake and have it. All the way through we were fighting with the fascism, the ultra-militarism. All the way through I wanted the audience to be asking, 'Are these people crazy?'"</ref>
Middle period work, 1961–1973.
From about 1961 ("Stranger in a Strange Land") to 1973 ("Time Enough for Love"), Heinlein explored some of his most important themes, such as individualism, libertarianism, and free expression of physical and emotional love. Three novels from this period, "Stranger in a Strange Land", "The Moon Is a Harsh Mistress", and "Time Enough for Love", won the Libertarian Futurist Society's Prometheus Hall of Fame Award, designed to honor classic libertarian fiction. Jeff Riggenbach described "The Moon is a Harsh Mistress" as "unquestionably one of the three or four most influential libertarian novels of the last century".
Heinlein did not publish "Stranger in a Strange Land" until some time after it was written, and the themes of free love and radical individualism are prominently featured in his long-unpublished first novel, "For Us, The Living: A Comedy of Customs".
"The Moon Is a Harsh Mistress" tells of a war of independence waged by the Lunar penal colonies, with significant comments from a major character, Professor La Paz, regarding the threat posed by government to individual freedom.
Although Heinlein had previously written a few short stories in the fantasy genre, during this period he wrote his first fantasy novel, "Glory Road", and in "Stranger in a Strange Land" and "I Will Fear No Evil", he began to mix hard science with fantasy, mysticism, and satire of organized religion. Critics William H. Patterson, Jr., and Andrew Thornton believe that this is simply an expression of Heinlein's longstanding philosophical opposition to positivism. Heinlein stated that he was influenced by James Branch Cabell in taking this new literary direction. The penultimate novel of this period, "I Will Fear No Evil", is according to critic James Gifford "almost universally regarded as a literary failure" and he attributes its shortcomings to Heinlein's near-death from peritonitis.
Later work, 1980–1987.
After a seven-year hiatus brought on by poor health, Heinlein produced five new novels in the period from 1980 ("The Number of the Beast") to 1987 ("To Sail Beyond the Sunset"). These books have a thread of common characters and time and place. They most explicitly communicated Heinlein's philosophies and beliefs, and many long, didactic passages of dialog and exposition deal with government, sex, and religion. These novels are controversial among his readers and one critic, Dave Langford, has written about them very negatively. Heinlein's four Hugo awards were all for books written before this period.
Some of these books, such as "The Number of the Beast" and "The Cat Who Walks Through Walls", start out as tightly constructed adventure stories, but transform into philosophical fantasias at the end. It is a matter of opinion whether this demonstrates a lack of attention to craftsmanship or a conscious effort to expand the boundaries of science fiction, either into a kind of magical realism, continuing the process of literary exploration that he had begun with "Stranger in a Strange Land", or into a kind of literary metaphor of quantum science ("The Number of the Beast" dealing with the Observer problem, and "The Cat Who Walks Through Walls" being a direct reference to the Schrödinger's cat thought experiment).
Most of the novels from this period are recognized by critics as forming an offshoot from the Future History series, and referred to by the term World as Myth.
The tendency toward authorial self-reference begun in "Stranger in a Strange Land" and "Time Enough for Love" becomes even more evident in novels such as "The Cat Who Walks Through Walls", whose first-person protagonist is a disabled military veteran who becomes a writer, and finds love with a female character.
The 1982 novel "Friday", a more conventional adventure story (borrowing a character and backstory from the earlier short story "Gulf", also containing suggestions of connection to "The Puppet Masters") continued a Heinlein theme of expecting what he saw as the continued disintegration of Earth's society, to the point where the title character is strongly encouraged to seek a new life off-planet. It concludes with a traditional Heinlein note, as in "The Moon is a Harsh Mistress" or "Time Enough for Love", that freedom is to be found on the frontiers.
The 1984 novel "" is a sharp satire of organized religion. Heinlein himself was agnostic.
Posthumous publications.
Several Heinlein works have been published since his death, including the aforementioned "" as well as 1989's "Grumbles from the Grave", a collection of letters between Heinlein and his editors and agent; 1992's "Tramp Royale", a travelogue of a southern hemisphere tour the Heinleins took in the 1950s; "Take Back Your Government", a how-to book about participatory democracy written in 1946; and a tribute volume called "Requiem: Collected Works and Tributes to the Grand Master", containing some additional short works previously unpublished in book form. "Off the Main Sequence", published in 2005, includes three short stories never before collected in any Heinlein book (Heinlein called them "stinkeroos").
Spider Robinson, a colleague, friend, and admirer of Heinlein, wrote "Variable Star", based on an outline and notes for a juvenile novel that Heinlein prepared in 1955. The novel was published as a collaboration, with Heinlein's name above Robinson's on the cover, in 2006.
A complete collection of Heinlein's published work, conformed and copy-edited by several Heinlein scholars including biographer William H. Patterson has been published by the Heinlein Trust as the "Virginia Edition", after his wife.
Views.
Heinlein's books probe a range of ideas about a range of topics such as sex, race, politics, and the military. Many were seen as radical or as ahead of their time in their social criticism. His books have inspired considerable debate about the specifics, and the evolution, of Heinlein's own opinions, and have earned him both lavish praise and a degree of criticism. He has also been accused of contradicting himself on various philosophical questions.
As Ted Gioia notes:
Brian Doherty cites William Patterson, saying that the best way to gain an understanding of Heinlein is as a "full-service iconoclast, the unique individual who decides that things do not have to be, and won't continue, as they are." He says this vision is "at the heart of Heinlein, science fiction, libertarianism, and America. Heinlein imagined how everything about the human world, from our sexual mores to our religion to our automobiles to our government to our plans for cultural survival, might be flawed, even fatally so."
The critic Elizabeth Anne Hull, for her part, has praised Heinlein for his interest in exploring fundamental life questions, especially questions about "political power—our responsibilities to one another" and about "personal freedom, particularly sexual freedom."
Politics.
Heinlein's political positions evolved throughout his life. Heinlein's early political leanings were liberal. In 1934 he worked actively for the Democratic campaign of Upton Sinclair for Governor of California. After Sinclair's loss, Heinlein became an anti-Communist Democratic activist. He made an unsuccessful bid for a California State Assembly seat in 1938. Heinlein's first novel, "For Us, The Living" (written 1939), consists largely of speeches advocating the Social Credit system, and the early story "Misfit" (1939) deals with an organization that seems to be Franklin D. Roosevelt's Civilian Conservation Corps translated into outer space.
Heinlein's fiction of the 1940s and 1950s, however, began to espouse conservative views. After 1945, he came to believe that a strong world government was the only way to avoid mutual nuclear annihilation. His 1949 novel "Space Cadet" describes a future scenario where a military-controlled global government enforces world peace. Heinlein ceased considering himself a Democrat in 1954.
Heinlein considered himself a libertarian, but in a letter to Judith Merril in 1967 (never sent) he also described himself as a philosophical anarchist or an "autarchist" 
"Stranger in a Strange Land" was embraced by the hippie counterculture, and libertarians have found inspiration in "The Moon Is a Harsh Mistress". Both groups found resonance with his themes of personal freedom in both thought and action.
Race.
Heinlein grew up in the era of racial segregation in the United States and wrote some of his most influential fiction at the height of the US civil rights movement. His early juveniles were very much ahead of their time both in their explicit rejection of racism and in their inclusion of protagonists of color—in the context of science fiction before the 1960s, the mere existence of characters of color was a remarkable novelty, with green occurring more often than brown. For example, his second juvenile, the 1948 "Space Cadet", explicitly uses aliens as a metaphor for minorities. In his juvenile, "Star Beast", the "de facto" foreign minister of the Terran government is an undersecretary, a Mr. Kiku, who is from Africa. Heinlein explicitly states his skin is "ebony black", and that Kiku is in an arranged marriage that is happy.
In a number of his stories, Heinlein challenges his readers' possible racial preconceptions by introducing a strong, sympathetic character, only to reveal much later that he or she is of African or other ancestry; in several cases, the covers of the books show characters as being light-skinned, when in fact the text states, or at least implies, that they are dark-skinned or of African ancestry. Heinlein repeatedly denounced racism in his non-fiction works, including numerous examples in "Expanded Universe".
Heinlein reveals in "Starship Troopers" that the novel's protagonist and narrator, Johnny Rico, the formerly disaffected scion of a wealthy family, is Filipino, actually named "Juan Rico" and speaks Tagalog in addition to English.
Race was a central theme in some of Heinlein's fiction. The most prominent and controversial example is "Farnham's Freehold", which casts a white family into a future in which white people are the slaves of cannibalistic black rulers. In the 1941 novel "Sixth Column" (also known as "The Day After Tomorrow"), a white resistance movement in the United States defends itself against an invasion by an Asian fascist state (the "Pan-Asians") using a "super-science" technology that allows ray weapons to be tuned to specific races. The book is sprinkled with racist slurs against Asian people, and blacks and Hispanics are not mentioned at all. The idea for the story was pushed on Heinlein by editor John W. Campbell, and Heinlein wrote later that he had "had to re-slant it to remove racist aspects of the original story line" and that he did not "consider it to be an artistic success." (However, the novel prompted a heated debate in the scientific community regarding the plausibility of developing ethnic bioweapons.)
It has been suggested that the strongly hierarchical and anti-individualistic "Bugs" in "Starship Troopers" were meant to represent the Chinese or Japanese, but Heinlein claimed to have written the book in response to "calls for the unilateral ending of nuclear testing by the United States." Heinlein suggests in the book that the Bugs are a good example of Communism being something that humans cannot successfully adhere to, since humans are strongly defined individuals, whereas the Bugs, being a collective, can all contribute to the whole without consideration of individual desire.
Heinlein's biographer William Patterson relates a number of instances in which Heinlein responded to anti-semitic remarks by (falsely) claiming to be half-Jewish himself and breaking off all further contact with the anti-semite. (Heinlein's actual ancestry is German-American on his father's side and Scots-Irish American on his mother's side, both going back to the Colonial era in the USA.)
Individualism and self-determination.
In keeping with his belief in individualism, his work for adults—and sometimes even his work for juveniles—often portrays both the oppressors and the oppressed with considerable ambiguity. Heinlein believed that individualism was incompatible with ignorance. He believed that an appropriate level of adult competence was achieved through a wide-ranging education, whether this occurred in a classroom or not. In his juvenile novels, more than once a character looks with disdain at a student's choice of classwork, saying, "Why didn't you study something useful?" In "Time Enough for Love", Lazarus Long gives a long list of capabilities that anyone should have, concluding, "Specialization is for insects." The ability of the individual to create himself is explored in stories such as "I Will Fear No Evil", "—All You Zombies—", and "By His Bootstraps".
Sexual issues.
For Heinlein, personal liberation included sexual liberation, and free love was a major subject of his writing starting in 1939, with "For Us, The Living". During his early period, Heinlein's writing for younger readers needed to take account of both editorial perceptions of sexuality in his novels, and potential perceptions among the buying public; as critic William H. Patterson has put it, his dilemma was "to sort out what was really objectionable from what was only excessive over-sensitivity to imaginary librarians".
By his middle period, sexual freedom and the elimination of sexual jealousy were a major theme of "Stranger in a Strange Land" (1961), in which the progressively minded but sexually conservative reporter, Ben Caxton, acts as a dramatic foil for the less parochial characters, Jubal Harshaw and Valentine Michael Smith (Mike). Another of the main characters, Jill, is homophobic.
Gary Westfahl points out that "Heinlein is a problematic case for feminists; on the one hand, his works often feature strong female characters and vigorous statements that women are equal to or even superior to men; but these characters and statements often reflect hopelessly stereotypical attitudes about typical female attributes. It is disconcerting, for example, that in "Expanded Universe" Heinlein calls for a society where all lawyers and politicians are women, essentially on the grounds that they possess a mysterious feminine practicality that men cannot duplicate." Also, in Heinlein's "Stranger in a Strange Land", Jill, one of the main characters, says, "nine times out of ten, if a girl gets raped it's partly her fault".
In books written as early as 1956, Heinlein dealt with incest and the sexual nature of children. Many of his books (including "Time for the Stars", "Glory Road", "Time Enough for Love", and "The Number of the Beast") dealt explicitly or implicitly with incest, sexual feelings and relations between adults and children, or both. The treatment of these themes include the romantic relationship and eventual marriage (once the girl becomes an adult via time-travel) of a 30-year-old engineer and an 11-year-old girl in "The Door into Summer" or the more overt intra-familial incest in "To Sail Beyond the Sunset" and "Farnham's Freehold". Peers such as L. Sprague de Camp and Damon Knight have commented critically on Heinlein's portrayal of incest and pedophilia in a lighthearted and even approving manner.
Philosophy.
In "To Sail Beyond the Sunset", Heinlein has the main character, Maureen, state that the purpose of metaphysics is to ask questions: Why are we here? Where are we going after we die? (and so on), and that you are not allowed to answer the questions. "Asking" the questions is the point of metaphysics, but "answering" them is not, because once you answer this kind of question, you cross the line into religion. Maureen does not state a reason for this; she simply remarks that such questions are "beautiful" but lack answers. Maureen's son/lover Lazarus Long makes a related remark in "Time Enough for Love". In order for us to answer the "big questions" about the universe, Lazarus states at one point, it would be necessary to stand "outside" the universe.
During the 1930s and 1940s, Heinlein was deeply interested in Alfred Korzybski's General Semantics and attended a number of seminars on the subject. His views on epistemology seem to have flowed from that interest, and his fictional characters continue to express Korzybskian views to the very end of his writing career. Many of his stories, such as "Gulf", "If This Goes On—", and "Stranger in a Strange Land", depend strongly on the premise, related to the well-known Sapir–Whorf hypothesis, that by using a correctly designed language, one can change or improve oneself mentally, or even realize untapped potential (as in the case of Joe Green in "Gulf").
When Ayn Rand's novel "The Fountainhead" was published, Heinlein was very favorably impressed, as quoted in "Grumbles..." and mentioned John Galt—the hero in Rand's "Atlas Shrugged"—as a heroic archetype in "The Moon Is a Harsh Mistress". He was also strongly affected by the religious philosopher P. D. Ouspensky. Freudianism and psychoanalysis were at the height of their influence during the peak of Heinlein's career, and stories such as "Time for the Stars" indulged in psychological theorizing.
However, he was skeptical about Freudianism, especially after a struggle with an editor who insisted on reading Freudian sexual symbolism into his juvenile novels. Heinlein was fascinated by the social credit movement in the 1930s. This is shown in "Beyond This Horizon" and in his 1938 novel "", which was finally published in 2003, long after his death. He was strongly committed to cultural relativism, and the sociologist Margaret Mader in his novel "Citizen of the Galaxy" is clearly a reference to Margaret Mead.
Pay it forward.
The term "pay it forward", though it was already in occasional use as a quotation, was popularized by Robert A. Heinlein in his book "Between Planets", published in 1951:
Heinlein was a mentor to Ray Bradbury, giving him help and quite possibly passing on the concept, made famous by the publication of a letter from him to Heinlein thanking him. In Bradbury's novel "Dandelion Wine", published in 1957, when the main character Douglas Spaulding is reflecting on his life being saved by Mr. Jonas, the Junkman:
Bradbury has also advised that writers he has helped thank him by helping other writers.
Heinlein both preached and practiced this philosophy; now the Heinlein Society, a humanitarian organization founded in his name, does so, attributing the philosophy to its various efforts, including Heinlein for Heroes, the Heinlein Society Scholarship Program, and Heinlein Society blood drives.The most important aspect of Robert Heinlein’s legacy that we at The Heinlein Society support and adhere to is his concept of paying it forward.</ref>
Author Spider Robinson made repeated reference to the doctrine, attributing it to his spiritual mentor Heinlein.
Influence and legacy.
The Dean of Science Fiction.
Heinlein is usually identified, along with Isaac Asimov and Arthur C. Clarke, as one of the three masters of science fiction to arise in the so-called Golden Age of science fiction, associated with John W. Campbell and his magazine "Astounding".
In the 1950s he was a leader in bringing science fiction out of the low-paying and less prestigious "pulp ghetto". Most of his works, including short stories, have been continuously in print in many languages since their initial appearance and are still available as new paperbacks decades after his death.
Robert Heinlein was also influenced by the American writer, philosopher and humorist Charles Fort who is credited as a major influence on most of the leading science-fiction writers of the 20th-century. Heinlein was a lifelong member of the International Fortean Organization also known as INFO, the successor to the original Fortean Society. Heinlein's letters were often displayed on the walls of the INFO offices, and his active participation in the organization is mentioned in the INFO Journal.
He was at the top of his form during, and himself helped to initiate, the trend toward social science fiction, which went along with a general maturing of the genre away from space opera to a more literary approach touching on such adult issues as politics and human sexuality. In reaction to this trend, hard science fiction began to be distinguished as a separate subgenre, but paradoxically Heinlein is also considered a seminal figure in hard science fiction, due to his extensive knowledge of engineering, and the careful scientific research demonstrated in his stories. Heinlein himself stated—with obvious pride—that in the days before pocket calculators, he and his wife Virginia once worked for several days on a mathematical equation describing an Earth-Mars rocket orbit, which was then subsumed in a single sentence of the novel "Space Cadet".
Influence among writers.
Heinlein has had a nearly ubiquitous influence on other science fiction writers. In a 1953 poll of leading science fiction authors, he was cited more frequently as an influence than any other modern writer. Critic James Gifford writes that "Although many other writers have exceeded Heinlein's output, few can claim to match his broad and seminal influence. Scores of science fiction writers from the prewar Golden Age through the present day loudly and enthusiastically credit Heinlein for blazing the trails of their own careers, and shaping their styles and stories."
Writer David Gerrold, responsible for creating the tribbles in Star Trek, also credited Heinlein as the inspiration for his "Dingilliad" series of novels. Gregory Benford refers to his novel Jupiter Project as a Heinlein tribute. Similarly, Charles Stross says his Hugo Award-nominated novel "Saturn's Children" is "a space opera and late-period Robert A. Heinlein tribute", referring to Heinlein's "Friday".
Words and phrases coined.
Outside the science fiction community, several words and phrases coined or adopted by Heinlein have passed into common English usage:
Inspiring culture and technology.
In 1962, Oberon Zell-Ravenheart (then still using his birth name, Tim Zell) founded the Church of All Worlds, a Neopagan religious organization modeled in many ways after the treatment of religion in the novel "Stranger in a Strange Land". This spiritual path included several ideas from the book, including non-mainstream family structures, social libertarianism, water-sharing rituals, an acceptance of all religious paths by a single tradition, and the use of several terms such as "grok", "Thou art God", and "Never Thirst". Though Heinlein was neither a member nor a promoter of the Church, there was a frequent exchange of correspondence between Zell and Heinlein, and he was a paid subscriber to their magazine, "Green Egg". This Church still exists as a 501(C)(3) religious organization incorporated in California, with membership worldwide, and it remains an active part of the neopagan community today.
Heinlein was influential in making space exploration seem to the public more like a practical possibility. His stories in publications such as "The Saturday Evening Post" took a matter-of-fact approach to their outer-space setting, rather than the "gee whiz" tone that had previously been common. The documentary-like film "Destination Moon" advocated a Space Race with an unspecified foreign power almost a decade before such an idea became commonplace, and was promoted by an unprecedented publicity campaign in print publications. Many of the astronauts and others working in the U.S. space program grew up on a diet of the Heinlein juveniles, best evidenced by the naming of a crater on Mars after him, and a tribute interspersed by the Apollo 15 astronauts into their radio conversations while on the moon.
Heinlein was also a guest commentator for Walter Cronkite during Neil Armstrong and Buzz Aldrin's Apollo 11 moon landing. He remarked to Cronkite during the landing that, "This is the greatest event in human history, up to this time. This is—today is New Year's Day of the Year One." Businessman and entrepreneur Elon Musk says that Heinlein's books have helped inspire his career.
Heinlein Society.
The Heinlein Society was founded by Virginia Heinlein on behalf of her husband, to "pay forward" the legacy of the writer to future generations of "Heinlein's Children." The foundation has programs to:
The Heinlein society also established the Robert A. Heinlein Award in 2003 "for outstanding published works in science fiction and technical writings to inspire the human exploration of space."
Honors.
In his lifetime, Heinlein received four Hugo Awards, for "Stranger in a Strange Land", "The Moon is a Harsh Mistress", "Starship Troopers", and "Double Star", and was nominated for four Nebula Awards, for "Stranger in a Strange Land", "Friday", "Time Enough for Love", and "Job: A Comedy of Justice". He was also given two posthumous Hugos, for "Farmer in the Sky" and "The Man Who Sold the Moon".
The Science Fiction Writers of America named Heinlein its first Grand Master in 1974, presented 1975. Officers and past presidents of the Association select a living writer for lifetime achievement (now annually and including fantasy literature).
Main-belt asteroid 6312 Robheinlein (1990 RH4), discovered on September 14, 1990 by H. E. Holt, at Palomar was named after him. Likewise, the Heinlein crater on Mars is named after him.Although there is no lunar feature named explicitly for Heinlein, this lack was rectified in 1994 when a major crater on Mars was named for him.</ref>
The Science Fiction and Fantasy Hall of Fame inducted Heinlein in 1998, its third class of two deceased and two living writers and editors.
In 2001 the United States Naval Academy created the Robert A. Heinlein Chair In Aerospace Engineering.
There was an active campaign to persuade the Secretary of the Navy to name the new "Zumwalt"-class destroyer DDG-1001 the USS "Robert A. Heinlein"; however, DDG-1001 will be named USS "Monsoor", after Michael Monsoor, a Navy SEAL who was posthumously awarded the Medal of Honor for his service in Iraq.
In December 2013 Heinlein was announced as an inductee to the Hall of Famous Missourians. His bronze bust, created by Kansas City sculptor, E. Spencer Schubert, will be one of forty-four on permanent display in the Missouri State Capitol in Jefferson City.
The Libertarian Futurist Society has honored six of Heinlein's novels with their Hall of Fame award. The first two during his lifetime for "The Moon Is A Harsh Mistress" and "Stranger in a Strange Land". Four have been awarded posthumously for "Red Planet", "Methuselah's Children", "Time Enough for Love" and "Requiem".
External links.
Biography and criticism
Bibliography and works

</doc>
<doc id="25391" url="https://en.wikipedia.org/wiki?curid=25391" title="Russia">
Russia

Russia (; ), also officially known as the Russian Federation (), is a sovereign state in northern Eurasia. It is a federal semi-presidential republic. At , Russia is the largest country in the world, covering more than one-eighth of the Earth's inhabited land area. Russia is the world's ninth most populous country with over 144 million people at the end of 2015.
Extending across the entirety of northern Asia and much of Eastern Europe, Russia spans eleven time zones and incorporates a wide range of environments and landforms. From northwest to southeast, Russia shares land borders with Norway, Finland, Estonia, Latvia, Lithuania and Poland (both with Kaliningrad Oblast), Belarus, Ukraine, Georgia, Azerbaijan, Kazakhstan, China, Mongolia, and North Korea. It shares maritime borders with Japan by the Sea of Okhotsk and the U.S. state of Alaska across the Bering Strait.
The nation's history began with that of the East Slavs, who emerged as a recognizable group in Europe between the 3rd and 8th centuries AD. Founded and ruled by a Varangian warrior elite and their descendants, the medieval state of Rus arose in the 9th century. In 988 it adopted Orthodox Christianity from the Byzantine Empire, beginning the synthesis of Byzantine and Slavic cultures that defined Russian culture for the next millennium. Rus' ultimately disintegrated into a number of smaller states; most of the Rus' lands were overrun by the Mongol invasion and became tributaries of the nomadic Golden Horde in the 13th century.
The Grand Duchy of Moscow gradually reunified the surrounding Russian principalities, achieved independence from the Golden Horde, and came to dominate the cultural and political legacy of Kievan Rus'. By the 18th century, the nation had greatly expanded through conquest, annexation, and exploration to become the Russian Empire, which was the third largest empire in history, stretching from Poland in Europe to Alaska in North America.
Following the Russian Revolution, the Russian Soviet Federative Socialist Republic became the largest and leading constituent of the Soviet Union abbreviated to USSR, the world's first constitutionally socialist state and a recognized world superpower, and a rival to the United States which played a decisive role in the Allied victory in World War II. The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first human-made satellite, and the first man in space. By the end of 1990, the Soviet Union had the world's second largest economy, largest standing military in the world and the largest stockpile of weapons of mass destruction (the state detonated in 1961 the Tsar Bomba, which was mankind's most powerful nuclear bomb ever built). Following the partition of the Soviet Union in 1991, fourteen independent republics emerged from the USSR; as the largest, most populous, and most economically developed republic, the Russian SFSR reconstituted itself as the Russian Federation and is recognized as the continuing legal personality (the sole successor state) of the Soviet Union.
The Russian economy ranks as the tenth largest by nominal GDP and sixth largest by purchasing power parity in 2015. Russia's extensive mineral and energy resources, the largest reserves in the world, have made it one of the largest producers of oil and natural gas globally. The country is one of the five recognized nuclear weapons states and possesses the largest stockpile of weapons of mass destruction. Russia was the world's second biggest exporter of major arms in 2010-14, according to SIPRI data.
Russia is a great power and a permanent member of the United Nations Security Council, a member of the G20, the Council of Europe, the Asia-Pacific Economic Cooperation (APEC), the Shanghai Cooperation Organisation (SCO), the Organization for Security and Co-operation in Europe (OSCE), and the World Trade Organization (WTO), as well as being the leading member of the Commonwealth of Independent States (CIS), the Collective Security Treaty Organization (CSTO) and one of the 5 members of the Eurasian Economic Union (EEU), along with Armenia, Belarus, Kazakhstan and Kyrgyzstan.
Etymology.
The name "Russia" is derived from Rus, a medieval state populated mostly by the East Slavs. However, this proper name became more prominent in the later history, and the country typically was called by its inhabitants "Русская Земля" (russkaja zemlja), which can be translated as "Russian Land" or "Land of Rus'". In order to distinguish this state from other states derived from it, it is denoted as "Kievan Rus"' by modern historiography. The name "Rus" itself comes from Rus people, a group of Varangians (possibly Swedish Vikings) who founded the state of Rus (Русь).
An old Latin version of the name Rus' was Ruthenia, mostly applied to the western and southern regions of Rus' that were adjacent to Catholic Europe. The current name of the country, Россия (Rossija), comes from the Byzantine Greek designation of the Kievan Rus', Ρωσσία "Rossía"—spelt Ρωσία ("Rosía" ) in Modern Greek.
The standard way to refer to citizens of Russia is "Russians".
History.
Prehistory.
Nomadic pastoralism developed in the Pontic-Caspian steppe beginning in the Chalcolithic. 
In classical antiquity, the Pontic Steppe was known as Scythia. Beginning in the 8th century BC, Ancient Greek traders brought their civilization to the trade emporiums in Tanais and Phanagoria. The Romans settled on the western part of the Caspian Sea, where their empire stretched towards the east. In the 3rd to 4th centuries AD a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by nomadic invasions led by warlike tribes, such as the Huns and Eurasian Avars. A Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas until the 10th century.
The ancestors of modern Russians are the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pinsk Marshes. The East Slavs gradually settled Western Russia in two waves: one moving from Kiev toward present-day Suzdal and Murom and another from Polotsk toward Novgorod and Rostov. From the 7th century onwards, the East Slavs constituted the bulk of the population in Western Russia and assimilated the native Finno-Ugric peoples, including the Merya, the Muromians, and the Meshchera.
Kievan Rus'.
The establishment of the first East Slavic states in the 9th century coincided with the arrival of Varangians, the traders, warriors and settlers from the Baltic Sea region. Primarily they were Vikings of Scandinavian origin, who ventured along the waterways extending from the eastern Baltic to the Black and Caspian Seas. According to the Primary Chronicle, a Varangian from Rus' people, named Rurik, was elected ruler of Novgorod in 862. In 882 his successor Oleg ventured south and conquered Kiev, which had been previously paying tribute to the Khazars, founding Kievan Rus'. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.
In the 10th to 11th centuries Kievan Rus' became one of the largest and most prosperous states in Europe. The reigns of Vladimir the Great (980–1015) and his son Yaroslav the Wise (1019–1054) constitute the Golden Age of Kiev, which saw the acceptance of Orthodox Christianity from Byzantium and the creation of the first East Slavic written legal code, the "Russkaya Pravda".
In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.
The age of feudalism and decentralization was marked by constant in-fighting between members of the Rurik Dynasty that ruled Kievan Rus' collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod Republic in the north-west and Galicia-Volhynia in the south-west.
Ultimately Kievan Rus' disintegrated, with the final blow being the Mongol invasion of 1237–40 that resulted in the destruction of Kiev and the death of about half the population of Rus'. The invading Mongol elite, together with their conquered Turkic subjects (Cumans, Kipchaks, Bulgars), became known as Tatars, forming the state of the Golden Horde, which pillaged the Russian principalities; the Mongols ruled the Cuman-Kipchak confederation and Volga Bulgaria (modern-day southern and central expanses of Russia) for over two centuries.
Galicia-Volhynia was eventually assimilated by the Kingdom of Poland, while the Mongol-dominated Vladimir-Suzdal and Novgorod Republic, two regions on the periphery of Kiev, established the basis for the modern Russian nation. The Novgorod together with Pskov retained some degree of autonomy during the time of the Mongol yoke and were largely spared the atrocities that affected the rest of the country. Led by Prince Alexander Nevsky, Novgorodians repelled the invading Swedes in the Battle of the Neva in 1240, as well as the Germanic crusaders in the Battle of the Ice in 1242, breaking their attempts to colonize the Northern Rus'.
Grand Duchy of Moscow.
The most powerful state to eventually arise after the destruction of Kievan Rus' was the Grand Duchy of Moscow ("Muscovy" in the Western chronicles), initially a part of Vladimir-Suzdal. While still under the domain of the Mongol-Tatars and with their connivance, Moscow began to assert its influence in the Central Rus' in the early 14th century, gradually becoming the main leading force in the process of the Rus' lands' reunification and expansion of Russia. Moscow's last rival, the Novgorod Republic, prospered as the chief fur trade center and the easternmost port of the Hanseatic League.
Times remained difficult, with frequent Mongol-Tatar raids and agriculture suffering from the beginning of the Little Ice Age. As in the rest of Europe, plague was a frequent occurrence between 1350 and 1490. However, because of the lower population density and better hygiene (widespread practicing of banya, the wet steam bath), the death rate from plague was not as severe as in Western Europe, and population numbers recovered by 1500.
Led by Prince Dmitry Donskoy of Moscow and helped by the Russian Orthodox Church, the united army of Russian principalities inflicted a milestone defeat on the Mongol-Tatars in the Battle of Kulikovo in 1380. Moscow gradually absorbed the surrounding principalities, including its formerly strong rivals such as Tver and Novgorod.
Ivan III ("the Great") finally threw off the control of the Golden Horde, consolidated the whole of Central and Northern Rus' under Moscow's dominion, and was the first to take the title "Grand Duke of all the Russias". After the fall of Constantinople in 1453, Moscow claimed succession to the legacy of the Eastern Roman Empire. Ivan III married Sophia Palaiologina, the niece of the last Byzantine emperor Constantine XI, and made the Byzantine double-headed eagle his own, and eventually Russia's, coat-of-arms.
Tsardom of Russia.
In development of the Third Rome ideas, the Grand Duke Ivan IV (the "Terrible") was officially crowned the first Tsar ("Caesar") of Russia in 1547. The Tsar promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor) and introduced local self-management into the rural regions.
During his long reign, Ivan the Terrible nearly doubled the already large Russian territory by annexing the three Tatar khanates (parts of the disintegrated Golden Horde): Kazan and Astrakhan along the Volga River, and the Siberian Khanate in southwestern Siberia. Thus, by the end of the 16th century Russia was transformed into a multiethnic, multidenominational and transcontinental state.
However, the Tsardom was weakened by the long and unsuccessful Livonian War against the coalition of Poland, Lithuania, and Sweden for access to the Baltic coast and sea trade. At the same time, the Tatars of the Crimean Khanate, the only remaining successor to the Golden Horde, continued to raid Southern Russia. In an effort to restore the Volga khanates, Crimeans and their Ottoman allies invaded central Russia and were even able to burn down parts of Moscow in 1571. But in the next year the large invading army was thoroughly defeated by Russians in the Battle of Molodi, forever eliminating the threat of an Ottoman–Crimean expansion into Russia. The slave raids of Crimeans, however, did not cease until the late 17th century though the construction of new fortification lines across Southern Russia, such as the Great Abatis Line, constantly narrowed the area accessible to incursions.
The death of Ivan's sons marked the end of the ancient Rurik Dynasty in 1598, and in combination with the famine of 1601–03 led to civil war, the rule of pretenders, and foreign intervention during the Time of Troubles in the early 17th century. The Polish-Lithuanian Commonwealth occupied parts of Russia, including Moscow. In 1612, the Poles were forced to retreat by the Russian volunteer corps, led by two national heroes, merchant Kuzma Minin and Prince Dmitry Pozharsky. The Romanov Dynasty acceded to the throne in 1613 by the decision of Zemsky Sobor, and the country started its gradual recovery from the crisis.
Russia continued its territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organized into military communities, resembling pirates and pioneers of the New World. In 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising in reaction to the social and religious oppression they had been suffering under Polish rule. In 1654, the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War. Finally, Ukraine was split along the Dnieper River, leaving the western part, right-bank Ukraine, under Polish rule and the eastern part (Left-bank Ukraine and Kiev) under Russian rule. Later, in 1670–71, the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga Region, but the Tsar's troops were successful in defeating the rebels.
In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian River Routes, and by the mid-17th century there were Russian settlements in Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648, the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.
Imperial Russia.
Under Peter the Great, Russia was proclaimed an Empire in 1721 and became recognized as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War, forcing it to cede West Karelia and Ingria (two regions lost by Russia in the Time of Troubles), as well as Estland and Livland, securing Russia's access to the sea and sea trade. On the Baltic Sea Peter founded a new capital called Saint Petersburg, later known as Russia's "Window to Europe". Peter the Great's reforms brought considerable Western European cultural influences to Russia.
The reign of Peter I's daughter Elizabeth in 1741–62 saw Russia's participation in the Seven Years' War (1756–63). During this conflict Russia annexed East Prussia for a while and even took Berlin. However, upon Elisabeth's death, all these conquests were returned to the Kingdom of Prussia by pro-Prussian Peter III of Russia.
Catherine II ("the Great"), who ruled in 1762–96, presided over the Age of Russian Enlightenment. She extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In the south, after successful Russo-Turkish Wars against Ottoman Turkey, Catherine advanced Russia's boundary to the Black Sea, defeating the Crimean Khanate. As a result of victories over Qajar Iran through the Russo-Persian Wars, by the first half of the 19th century Russia also made significant territorial gains in Transcaucasia and the North Caucasus, forcing the former to irrevocably cede what is nowadays Georgia, Dagestan, Azerbaijan and Armenia to Russia. This continued with Alexander I's (1801–25) wresting of Finland from the weakened kingdom of Sweden in 1809 and of Bessarabia from the Ottomans in 1812. At the same time, Russians colonized Alaska and even founded settlements in California, such as Fort Ross.
In 1803–1806, the first Russian circumnavigation was made, later followed by other notable Russian sea exploration voyages. In 1820 a Russian expedition discovered the continent of Antarctica.
In alliances with various European countries, Russia fought against Napoleon's France. The French invasion of Russia at the height of Napoleon's power in 1812 failed miserably as the obstinate resistance in combination with the bitterly cold Russian winter led to a disastrous defeat of invaders, in which more than 95% of the pan-European Grande Armée perished. Led by Mikhail Kutuzov and Barclay de Tolly, the Russian army ousted Napoleon from the country and drove through Europe in the war of the Sixth Coalition, finally entering Paris. Alexander I headed Russia's delegation at the Congress of Vienna that defined the map of post-Napoleonic Europe.
The officers of the Napoleonic Wars brought ideas of liberalism back to Russia with them and attempted to curtail the tsar's powers during the abortive Decembrist revolt of 1825. At the end of the conservative reign of Nicolas I (1825–55), a zenith period of Russia's power and influence in Europe was disrupted by defeat in the Crimean War. Between 1847 and 1851, about one million people died of Asiatic cholera.
Nicholas's successor Alexander II (1855–81) enacted significant changes in the country, including the emancipation reform of 1861. These "Great Reforms" spurred industrialization and modernized the Russian army, which had successfully liberated Bulgaria from Ottoman rule in the 1877–78 Russo-Turkish War.
The late 19th century saw the rise of various socialist movements in Russia. Alexander II was killed in 1881 by revolutionary terrorists, and the reign of his son
Alexander III (1881–94) was less liberal but more peaceful. The last Russian Emperor, Nicholas II (1894–1917), was unable to prevent the events of the Russian Revolution of 1905, triggered by the unsuccessful Russo-Japanese War and the demonstration incident known as Bloody Sunday. The uprising was put down, but the government was forced to concede major reforms, including granting the freedoms of speech and assembly, the legalization of political parties, and the creation of an elected legislative body, the State Duma of the Russian Empire. The Stolypin agrarian reform led to a massive peasant migration and settlement into Siberia. More than four million settlers arrived in that region between 1906 and 1914.
In 1914, Russia entered World War I in response to Austria-Hungary's declaration of war on Russia's ally Serbia, and fought across multiple fronts while isolated from its Triple Entente allies. In 1916, the Brusilov Offensive of the Russian Army almost completely destroyed the military of Austria-Hungary. However, the already-existing public distrust of the regime was deepened by the rising costs of war, high casualties, and rumors of corruption and treason. All this formed the climate for the Russian Revolution of 1917, carried out in two major acts.
Revolution and Russian Republic.
The February Revolution forced Nicholas II to abdicate; he and his family were imprisoned and later executed in Yekaterinburg during the Russian Civil War. The monarchy was replaced by a shaky coalition of political parties that declared itself the Provisional Government. An alternative socialist establishment existed alongside, the Petrograd Soviet, wielding power through the democratically elected councils of workers and peasants, called "Soviets". The rule of the new authorities only aggravated the crisis in the country, instead of resolving it. Eventually, the October Revolution, led by Bolshevik leader Vladimir Lenin, overthrew the Provisional Government and gave full governing power to the Soviets, leading to the creation of the world's first socialist state.
Soviet Russia and civil war.
Following the October Revolution, a civil war broke out between the anti-Communist White movement and the new Soviet regime with its Red Army. Bolshevist Russia lost its Ukrainian, Polish, Baltic, and Finnish territories by signing the Treaty of Brest-Litovsk that concluded hostilities with the Central Powers of World War I. The Allied powers launched an unsuccessful military intervention in support of anti-Communist forces. In the meantime both the Bolsheviks and White movement carried out campaigns of deportations and executions against each other, known respectively as the Red Terror and White Terror. By the end of the civil war, Russia's economy and infrastructure were heavily damaged. Millions became White émigrés, and the Povolzhye famine of 1921 claimed up to 5 million victims.
Soviet Union.
The Russian Soviet Federative Socialist Republic (called "Russian Socialist Federative Soviet Republic" at the time), together with the Ukrainian, Byelorussian, and Transcaucasian Soviet Socialist Republics, formed the Union of Soviet Socialist Republics (USSR), or Soviet Union, on 30 December 1922. Out of the 15 republics that would make up the USSR, the largest in size and over half of the total USSR population was the Russian SFSR, which came to dominate the union for its entire 69-year history.
Following Lenin's death in 1924, a troika was designated to govern the Soviet Union. However, Joseph Stalin, an elected General Secretary of the Communist Party, managed to suppress all opposition groups within the party and consolidate power in his hands. Leon Trotsky, the main proponent of world revolution, was exiled from the Soviet Union in 1929, and Stalin's idea of Socialism in One Country became the primary line. The continued internal struggle in the Bolshevik party culminated in the Great Purge, a period of mass repressions in 1937–38, during which hundreds of thousands of people were executed, including original party members and military leaders accused of coup d'état plots.
Under Stalin's leadership, the government launched a planned economy, industrialisation of the largely rural country, and collectivization of its agriculture. During this period of rapid economic and social change, millions of people were sent to penal labor camps, including many political convicts for their opposition to Stalin's rule; millions were deported and exiled to remote areas of the Soviet Union. The transitional disorganisation of the country's agriculture, combined with the harsh state policies and a drought, led to the Soviet famine of 1932–1933. The Soviet Union, though with a heavy price, was transformed from a largely agrarian economy to a major industrial powerhouse in a short span of time.
The Appeasement policy of Great Britain and France towards Adolf Hitler's annexation of Austria and Czechoslovakia did not stem an increase in the power of Nazi Germany and initiated a threat of war to the Soviet Union. Around the same time, the Third Reich allied with the Empire of Japan, a rival of the USSR in the Far East and an open enemy of the USSR in the Soviet–Japanese Border Wars in 1938–39.
In August 1939, after another failure of attempts to establish an anti-Nazi alliance with Britain and France, the Soviet government decided to improve relations with Germany by concluding the Molotov-Ribbentrop Pact, pledging non-aggression between the two countries and dividing Eastern Europe into their respective spheres of influence. While Hitler conquered Poland and France and other countries acted on a single front at the start of World War II, the USSR was able to build up its military and claim some of the former territories of the Russian Empire, Western Ukraine, Hertza region and Northern Bukovina as a result of the Soviet invasion of Poland, Winter War, occupation of the Baltic states and Soviet occupation of Bessarabia and Northern Bukovina.
On 22 June 1941, Nazi Germany broke the non-aggression treaty and invaded the Soviet Union with the largest and most powerful invasion force in human history, opening the largest theater of World War II. Although the German army had considerable early success, their attack was halted in the Battle of Moscow. Subsequently, the Germans were dealt major defeats first at the Battle of Stalingrad in the winter of 1942–43, and then in the Battle of Kursk in the summer of 1943. Another German failure was the Siege of Leningrad, in which the city was fully blockaded on land between 1941 and 1944 by German and Finnish forces, and suffered starvation and more than a million deaths, but never surrendered. Under Stalin's administration and the leadership of such commanders as Georgy Zhukov and Konstantin Rokossovsky, Soviet forces took Eastern Europe in 1944–45 and captured Berlin in May 1945. In August 1945 the Soviet Army ousted the Japanese from China's Manchukuo and North Korea, contributing to the allied victory over Japan.
The 1941–45 period of World War II is known in Russia as the "Great Patriotic War". The Soviet Union together with the United States, the United Kingdom and China were considered as the Big Four of Allied powers in World War II and later became the Four Policemen which was the foundation of the United Nations Security Council. During this war, which included many of the most lethal battle operations in human history, Soviet military and civilian deaths were 10.6 million and 15.9 million respectively, accounting for about a third of all World War II casualties. The full demographic loss to the Soviet peoples was even greater. The Soviet economy and infrastructure suffered massive devastation which caused the Soviet famine of 1946–47 but the Soviet Union emerged as an acknowledged military superpower on the continent.
After the war, Eastern and Central Europe including East Germany and part of Austria was occupied by Red Army according to the Potsdam Conference. Dependent socialist governments were installed in the Eastern Bloc satellite states. Becoming the world's second nuclear weapons power, the USSR established the Warsaw Pact alliance and entered into a struggle for global dominance, known as the Cold War, with the United States and NATO. The Soviet Union supported revolutionary movements across the world, including the newly formed People's Republic of China, the Democratic People's Republic of Korea and, later on, the Republic of Cuba. Significant amounts of Soviet resources were allocated in aid to the other socialist states.
After Stalin's death and a short period of collective rule, the new leader Nikita Khrushchev denounced the cult of personality of Stalin and launched the policy of de-Stalinization. The penal labor system was reformed and many prisoners were released and rehabilitated (many of them posthumously). The general easement of repressive policies became known later as the Khrushchev Thaw. At the same time, tensions with the United States heightened when the two rivals clashed over the deployment of the U.S. Jupiter missiles in Turkey and Soviet missiles in Cuba.
In 1957, the Soviet Union launched the world's first artificial satellite, "Sputnik 1", thus starting the Space Age. Russia's cosmonaut Yuri Gagarin became the first human to orbit the Earth, aboard the "Vostok 1" manned spacecraft on 12 April 1961.
Following the ousting of Khrushchev in 1964, another period of collective rule ensued, until Leonid Brezhnev became the leader. The era of the 1970s and the early 1980s was designated later as the Era of Stagnation, a period when economic growth slowed and social policies became static. The 1965 Kosygin reform aimed for partial decentralization of the Soviet economy and shifted the emphasis from heavy industry and weapons to light industry and consumer goods but was stifled by the conservative Communist leadership.
In 1979, after a Communist-led revolution in Afghanistan, Soviet forces entered that country at the request of the new regime. The occupation drained economic resources and dragged on without achieving meaningful political results. Ultimately, the Soviet Army was withdrawn from Afghanistan in 1989 due to international opposition, persistent anti-Soviet guerilla warfare, and a lack of support by Soviet citizens.
From 1985 onwards, the last Soviet leader Mikhail Gorbachev, who sought to enact liberal reforms in the Soviet system, introduced the policies of "glasnost" (openness) and "perestroika" (restructuring) in an attempt to end the period of economic stagnation and to democratise the government. This, however, led to the rise of strong nationalist and separatist movements. Prior to 1991, the Soviet economy was the second largest in the world, but during its last years it was afflicted by shortages of goods in grocery stores, huge budget deficits, and explosive growth in the money supply leading to inflation.
By 1991, economic and political turmoil began to boil over, as the Baltic republics chose to secede from the Soviet Union. On 17 March, a referendum was held, in which the vast majority of participating citizens voted in favour of changing the Soviet Union into a renewed federation. In August 1991, a coup d'état attempt by members of Gorbachev's government, directed against Gorbachev and aimed at preserving the Soviet Union, instead led to the end of the Communist Party of the Soviet Union. On 25 December 1991, the USSR was dissolved into 15 post-Soviet states.
Russian Federation.
In June 1991, Boris Yeltsin became the first directly elected President in Russian history when he was elected President of the Russian Soviet Federative Socialist Republic, which became the independent Russian Federation in December of that year. During and after the disintegration of the Soviet Union, wide-ranging reforms including privatization and market and trade liberalization were undertaken, including radical changes along the lines of "shock therapy" as recommended by the United States and the International Monetary Fund. All this resulted in a major economic crisis, characterized by a 50% decline in both GDP and industrial output between 1990 and 1995.
The privatization largely shifted control of enterprises from state agencies to individuals with inside connections in the government. Many of the newly rich moved billions in cash and assets outside of the country in an enormous capital flight. The depression of the economy led to the collapse of social services; the birth rate plummeted while the death rate skyrocketed. Millions plunged into poverty, from a level of 1.5% in the late Soviet era to 39–49% by mid-1993. The 1990s saw extreme corruption and lawlessness, the rise of criminal gangs and violent crime.
The 1990s were plagued by armed conflicts in the North Caucasus, both local ethnic skirmishes and separatist Islamist insurrections. From the time Chechen separatists declared independence in the early 1990s, an intermittent guerrilla war has been fought between the rebel groups and the Russian military. Terrorist attacks against civilians carried out by separatists, most notably the Moscow theater hostage crisis and Beslan school siege, caused hundreds of deaths and drew worldwide attention.
Russia took up the responsibility for settling the USSR's external debts, even though its population made up just half of the population of the USSR at the time of its dissolution. High budget deficits caused the 1998 Russian financial crisis and resulted in a further GDP decline.
On 31 December 1999, President Yeltsin unexpectedly resigned, handing the post to the recently appointed Prime Minister, Vladimir Putin, who then won the 2000 presidential election. Putin suppressed the Chechen insurgency although sporadic violence still occurs throughout the Northern Caucasus. High oil prices and the initially weak currency followed by increasing domestic demand, consumption, and investments has helped the economy grow for nine straight years, improving the standard of living and increasing Russia's influence on the world stage. While many reforms made during the Putin presidency have been generally criticized by Western nations as undemocratic, Putin's leadership over the return of order, stability, and progress has won him widespread admiration in Russia.
On 2 March 2008, Dmitry Medvedev was elected President of Russia while Putin became Prime Minister. Putin returned to the presidency following the 2012 presidential elections, and Medvedev was appointed Prime Minister.
In 2014, after President Viktor Yanukovych of Ukraine fled as a result of a revolution, Putin requested and received authorization from the Russian Parliament to deploy Russian troops to Ukraine. Following a Crimean referendum in which separation was favored by a large majority of voters, but not accepted internationally, the Russian leadership announced the accession of Crimea into the Russian Federation. On 27 March the United Nations General Assembly voted in favor of a non-binding resolution opposing the Russian annexation of Crimea by a vote of 100 in favour, 11 against and 58 abstentions.
In September 2015, Russia started military intervention in the Syrian Civil War, consisting of air strikes against militant groups of the Islamic State, al-Nusra Front (al-Qaeda in the Levant), and the Army of Conquest.
Politics.
Governance.
According to the Constitution of Russia, the country is a federation and semi-presidential republic, wherein the President is the head of state and the Prime Minister is the head of government. The Russian Federation is fundamentally structured as a multi-party representative democracy, with the federal government composed of three branches:
The president is elected by popular vote for a six-year term (eligible for a second term, but not for a third consecutive term). Ministries of the government are composed of the Premier and his deputies, ministers, and selected other individuals; all are appointed by the President on the recommendation of the Prime Minister (whereas the appointment of the latter requires the consent of the State Duma). Leading political parties in Russia include United Russia, the Communist Party, the Liberal Democratic Party, and A Just Russia. In 2013, Russia was ranked as 122nd of 167 countries in the Democracy Index, compiled by The Economist Intelligence Unit, while the World Justice Project currently ranks Russia 80th of 99 countries surveyed in terms of rule of law.
Foreign relations.
The Russian Federation is recognized in international law as a successor state of the former Soviet Union. Russia continues to implement the international commitments of the USSR, and has assumed the USSR's permanent seat in the UN Security Council, membership in other international organisations, the rights and obligations under international treaties, and property and debts. Russia has a multifaceted foreign policy. , it maintains diplomatic relations with 191 countries and has 144 embassies. The foreign policy is determined by the President and implemented by the Ministry of Foreign Affairs of Russia.
As the successor to a former superpower, Russia's geopolitical status has often been debated, particularly in relation to unipolar and multipolar views on the global political system. While Russia is commonly accepted to be a great power, in recent years it has been characterized by a number of world leaders, scholars, commentators and politicians as a currently reinstating or potential superpower.
As one of five permanent members of the UN Security Council, Russia plays a major role in maintaining international peace and security. The country participates in the Quartet on the Middle East and the Six-party talks with North Korea. Russia is a member of the G8 industrialized nations, the Council of Europe, OSCE, and APEC. Russia usually takes a leading role in regional organisations such as the CIS, EurAsEC, CSTO, and the SCO. Russia became the 39th member state of the Council of Europe in 1996. In 1998, Russia ratified the European Convention on Human Rights. The legal basis for EU relations with Russia is the Partnership and Cooperation Agreement, which came into force in 1997. The Agreement recalls the parties' shared respect for democracy and human rights, political and economic freedom and commitment to international peace and security. In May 2003, the EU and Russia agreed to reinforce their cooperation on the basis of common values and shared interests. Former President Vladimir Putin had advocated a strategic partnership with close integration in various dimensions including establishment of EU-Russia Common Spaces. Since the dissolution of the Soviet Union, Russia has developed a friendlier relationship with the United States and NATO. The NATO-Russia Council was established in 2002 to allow the United States, Russia and the 27 allies in NATO to work together as equal partners to pursue opportunities for joint collaboration.
Russia maintains strong and positive relations with other BRIC countries. India is the largest customer of Russian military equipment and the two countries share extensive defense and strategic relations. In recent years, the country has strengthened bilateral ties especially with the People's Republic of China by signing the Treaty of Friendship as well as building the Trans-Siberian oil pipeline and gas pipeline from Siberia to China.
An important aspect of Russia's relations with the West is the criticism of Russia's political system and human rights management (including LGBT rights, media freedom, and reports about killed journalists) by Western governments, the mass media and the leading democracy and human rights watchdogs. In particular, such organisations as the Amnesty International and Human Rights Watch consider Russia to have not enough democratic attributes and to allow few political rights and civil liberties to its citizens. Freedom House, an international organisation funded by the United States, ranks Russia as "not free", citing "carefully engineered elections" and "absence" of debate. Russian authorities dismiss these claims and especially criticise Freedom House. The Russian Ministry of Foreign Affairs has called the 2006 "Freedom in the World" report "prefabricated", stating that the human rights issues have been turned into a political weapon in particular by the United States. The ministry also claims that such organisations as Freedom House and Human Rights Watch use the same scheme of voluntary extrapolation of ""isolated facts that of course can be found in any country"" into "dominant tendencies".
Military.
The Russian military is divided into the Ground Forces, Navy, and Air Force. There are also three independent arms of service: Strategic Missile Troops, Aerospace Defence Forces, and the Airborne Troops. In 2006, the military had 1.037 million personnel on active duty. It is mandatory for all male citizens aged 18–27 to be drafted for a year of service in Armed Forces.
Russia has the largest stockpile of nuclear weapons in the world. It has the second largest fleet of ballistic missile submarines and is the only country apart from the United States with a modern strategic bomber force. Russia's tank force is the largest in the world, its surface navy and air force are among the largest ones.
The country has a large and fully indigenous arms industry, producing most of its own military equipment with only few types of weapons imported. Russia is one of the world's top supplier of arms, a spot it has held since 2001, accounting for around 30% of worldwide weapons sales and exporting weapons to about 80 countries. The Stockholm International Peace Research Institute, SIPRI, found that Russia was the second biggest exporter of arms in 2010-14, increasing their exports by 37 per cent from the period 2005-2009. In 2010-14, Russia delivered weapons to 56 states and to rebel forces in eastern Ukraine.
The Russian government's published 2014 military budget is about 2.49 trillion rubles (approximately US$69.3 billion), the third largest in the world behind the US and China. The official budget is set to rise to 3.03 trillion rubles (approximately US$83.7 billion) in 2015, and 3.36 trillion rubles (approximately US$93.9 billion) in 2016. However, unofficial estimates put the budget significantly higher, for example the Stockholm International Peace Research Institute (SIPRI) 2013 Military Expenditure Database estimated Russia's military expenditure in 2012 at US$90.749 billion. This estimate is an increase of more than US$18 billion on SIPRI's estimate of the Russian military budget for 2011 (US$71.9 billion). , Russia's military budget is higher than any other European nation.
According to 2012 Global Peace Index, Russia is the sixth least peaceful out of 162 countries in the world, principally because of its defense industry. Russia has historically ranked low on the index since its inception in 2007.
Political divisions.
According to the Constitution, the country comprises eighty-five federal subjects, including the Republic of Crimea and the federal city of Sevastopol, whose recent establishment is internationally disputed and criticized as illegal annexation. In 1993, when the Constitution was adopted, there were eighty-nine federal subjects listed, but later some of them were merged. These subjects have equal representation—two delegates each—in the Federation Council. However, they differ in the degree of autonomy they enjoy.
Federal subjects are grouped into nine federal districts, each administered by an envoy appointed by the President of Russia. Unlike the federal subjects, the federal districts are not a subnational level of government, but are a level of administration of the federal government. Federal districts' envoys serve as liaisons between the federal subjects and the federal government and are primarily responsible for overseeing the compliance of the federal subjects with the federal laws.
Geography.
Russia is the largest country in the world; its total area is . There are 23 UNESCO World Heritage Sites in Russia, 40 UNESCO biosphere reserves, 41 national parks and 101 nature reserves. It lies between latitudes 41° and 82° N, and longitudes 19° E and 169° W.
Russia's territorial expansion was achieved largely in the late 16th century under the Cossack Yermak Timofeyevich during the reign of Ivan the Terrible, at a time when competing city-states in the western regions of Russia had banded together to form one country. Yermak mustered an army and pushed eastward where he conquered nearly all the lands once belonging to the Mongols, defeating their ruler, Khan Kuchum.
Russia has a wide natural resource base, including major deposits of timber, petroleum, natural gas, coal, ores and other mineral resources.
Topography.
The two most widely separated points in Russia are about apart along a geodesic line. These points are: a long Vistula Spit the boundary with Poland separating the Gdańsk Bay from the Vistula Lagoon and the most southeastern point of the Kuril Islands. The points which are farthest separated in longitude are apart along a geodesic line. These points are: in the west, the same spit on the boundary with Poland, and in the east, the Big Diomede Island. The Russian Federation spans nine time zones.
Most of Russia consists of vast stretches of plains that are predominantly steppe to the south and heavily forested to the north, with tundra along the northern coast. Russia possesses 10% of the world's arable land. Mountain ranges are found along the southern borders, such as the Caucasus (containing Mount Elbrus, which at is the highest point in both Russia and Europe) and the Altai (containing Mount Belukha, which at the is the highest point of Siberia outside of the Russian Far East); and in the eastern parts, such as the Verkhoyansk Range or the volcanoes of Kamchatka Peninsula (containing Klyuchevskaya Sopka, which at the is the highest active volcano in Eurasia as well as the highest point of Asian Russia). The Ural Mountains, rich in mineral resources, form a north-south range that divides Europe and Asia.
Russia has an extensive coastline of over along the Arctic and Pacific Oceans, as well as along the Baltic Sea, Sea of Azov, Black Sea and Caspian Sea. The Barents Sea, White Sea, Kara Sea, Laptev Sea, East Siberian Sea, Chukchi Sea, Bering Sea, Sea of Okhotsk, and the Sea of Japan are linked to Russia via the Arctic and Pacific. Russia's major islands and archipelagos include Novaya Zemlya, the Franz Josef Land, the Severnaya Zemlya, the New Siberian Islands, Wrangel Island, the Kuril Islands, and Sakhalin. The Diomede Islands (one controlled by Russia, the other by the U.S.) are just apart, and Kunashir Island is about from Hokkaido, Japan.
Russia has thousands of rivers and inland bodies of water, providing it with one of the world's largest surface water resources. Its lakes contain approximately one-quarter of the world's liquid fresh water. The largest and most prominent of Russia's bodies of fresh water is Lake Baikal, the world's deepest, purest, oldest and most capacious fresh water lake. Baikal alone contains over one-fifth of the world's fresh surface water. Other major lakes include Ladoga and Onega, two of the largest lakes in Europe. Russia is second only to Brazil in volume of the total renewable water resources. Of the country's 100,000 rivers, the Volga is the most famous, not only because it is the longest river in Europe, but also because of its major role in Russian history. The Siberian rivers Ob, Yenisey, Lena and Amur are among the longest rivers in the world.
Climate.
The enormous size of Russia and the remoteness of many areas from the sea result in the dominance of the humid continental climate, which is prevalent in all parts of the country except for the tundra and the extreme southeast. Mountains in the south obstruct the flow of warm air masses from the Indian Ocean, while the plain of the west and north makes the country open to Arctic and Atlantic influences.
Most of Northern European Russia and Siberia has a subarctic climate, with extremely severe winters in the inner regions of Northeast Siberia (mostly the Sakha Republic, where the Northern Pole of Cold is located with the record low temperature of ), and more moderate winters elsewhere. Both the strip of land along the shore of the Arctic Ocean and the Russian Arctic islands have a polar climate.
The coastal part of Krasnodar Krai on the Black Sea, most notably in Sochi, possesses a humid subtropical climate with mild and wet winters. In many regions of East Siberia and the Far East, winter is dry compared to summer; other parts of the country experience more even precipitation across seasons. Winter precipitation in most parts of the country usually falls as snow. The region along the Lower Volga and Caspian Sea coast, as well as some areas of southernmost Siberia, possesses a semi-arid climate.
Throughout much of the territory there are only two distinct seasons—winter and summer—as spring and autumn are usually brief periods of change between extremely low and extremely high temperatures. The coldest month is January (February on the coastline); the warmest is usually July. Great ranges of temperature are typical. In winter, temperatures get colder both from south to north and from west to east. Summers can be quite hot, even in Siberia. The continental interiors are the driest areas.
Biodiversity.
From north to south the East European Plain, also known as Russian Plain, is clad sequentially in Arctic tundra, coniferous forest (taiga), mixed and broad-leaf forests, grassland (steppe), and semi-desert (fringing the Caspian Sea), as the changes in vegetation reflect the changes in climate. Siberia supports a similar sequence but is largely taiga. Russia has the world's largest forest reserves, known as "the lungs of Europe", second only to the Amazon Rainforest in the amount of carbon dioxide it absorbs.
There are 266 mammal species and 780 bird species in Russia. A total of 415 animal species have been included in the Red Data Book of the Russian Federation as of 1997 and are now protected.
Economy.
Russia has a developed, high-income market economy with enormous natural resources, particularly oil and natural gas. It has the 15th largest economy in the world by nominal GDP and the 6th largest by purchasing power parity (PPP). Since the turn of the 21st century, higher domestic consumption and greater political stability have bolstered economic growth in Russia. The country ended 2008 with its ninth straight year of growth, but growth has slowed with the decline in the price of oil and gas. Real GDP per capita, PPP (current international) was 19,840 in 2010. Growth was primarily driven by non-traded services and goods for the domestic market, as opposed to oil or mineral extraction and exports. The average nominal salary in Russia was $967 per month in early 2013, up from $80 in 2000. In March 2014 the average nominal monthly wages reached 30,000 RUR (or US$980), while tax on the income of individuals is payable at the rate of 13% on most incomes. Approximately 12.8% of Russians lived below the national poverty line in 2011, significantly down from 40% in 1998 at the worst point of the post-Soviet collapse. Unemployment in Russia was 5.4% in 2014, down from about 12.4% in 1999. The middle class has grown from just 8 million persons in 2000 to 104 million persons in 2013. However, after U.S.-led sanctions since 2014 and a collapse in oil prices, the proportion of middle-class could halve to 20%. Sugar imports reportedly dropped 82% between 2012 and 2013 as a result of the increase in domestic output.
Oil, natural gas, metals, and timber account for more than 80% of Russian exports abroad. Since 2003, the exports of natural resources started decreasing in economic importance as the internal market strengthened considerably. Despite higher energy prices, oil and gas only contribute to 5.7% of Russia's GDP and the government predicts this will be 3.7% by 2011. Oil export earnings allowed Russia to increase its foreign reserves from $12 billion in 1999 to $597.3 billion on 1 August 2008, the third largest foreign exchange reserves in the world. The macroeconomic policy under Finance Minister Alexei Kudrin was prudent and sound, with excess income being stored in the Stabilization Fund of Russia. In 2006, Russia repaid most of its formerly massive debts, leaving it with one of the lowest foreign debts among major economies. The Stabilization Fund helped Russia to come out of the global financial crisis in a much better state than many experts had expected.
A simpler, more streamlined tax code adopted in 2001 reduced the tax burden on people and dramatically increased state revenue. Russia has a flat tax rate of 13%. This ranks it as the country with the second most attractive personal tax system for single managers in the world after the United Arab Emirates. According to Bloomberg, Russia is considered well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry. The country has a higher proportion of higher education graduates than any other country in Eurasia.
The economic development of the country has been uneven geographically with the Moscow region contributing a very large share of the country's GDP. 
Inequality of household income and wealth has also been noted, with Credit Suisse finding Russian wealth distribution so much more extreme than other countries studied it "deserves to be placed in a separate category."
Another problem is modernisation of infrastructure, ageing and inadequate after years of being neglected in the 1990s; the government has said $1 trillion will be invested in development of infrastructure by 2020. In December 2011, Russia finally joined the World Trade Organisation, allowing it a greater access to overseas markets. Some analysts estimate that WTO membership could bring the Russian economy a bounce of up to 3% annually. Russia ranks as the second-most corrupt country in Europe (after Ukraine), according to the Corruption Perceptions Index. The Norwegian-Russian Chamber of Commerce also states that "orruption is one of the biggest problems both Russian and international companies have to deal with". The high rate of corruption acts as a hidden tax as businesses and individuals often have to pay money that is not part of the official tax rate. It is estimated that corruption is costing the Russian economy an estimated $2 billion (80 billion rubles) per year. In 2014, a book-length study by Professor Karen Dawisha was published concerning corruption in Russian under Putin's government.
The Russian central bank announced plans in 2013 to free float the Russian ruble in 2015. According to a stress test conducted by the central bank Russian financial system would be able to handle a currency decline of 25%–30% without major central bank interference. However, Russian economy began stagnating in late 2013 and in combination with the War in Donbass is in danger of entering stagflation, slow growth and high inflation. The Russian ruble collapsed by 24% from October 2013 to October 2014 entering the level where the central bank may need to intervene to strengthen the currency. Moreover, after bringing inflation down to 3.6% in 2012, the lowest rate since gaining independence from the Soviet Union, inflation in Russia jumped to nearly 7.5% in 2014, causing the central bank to increase its lending rate to 8% from 5.5% in 2013. In an October 2014 article in "Bloomberg Business Week", it was reported that Russia had significantly started shifting its economy towards China in response to increasing financial tensions following its annexation of Crimea and subsequent Western economic sanctions.
Agriculture.
Russia's total area of cultivated land is estimated at , the fourth largest in the world. From 1999 to 2009, Russia's agriculture grew steadily, and the country turned from a grain importer to the third largest grain exporter after the EU and the United States. The production of meat has grown from 6,813,000 tonnes in 1999 to 9,331,000 tonnes in 2008, and continues to grow.
This restoration of agriculture was supported by a credit policy of the government, helping both individual farmers and large privatized corporate farms that once were Soviet kolkhozes and which still own the significant share of agricultural land. While large farms concentrate mainly on grain production and husbandry products, small private household plots produce most of the country's potatoes, vegetables and fruits.
Since Russia borders three oceans (the Atlantic, Arctic, and Pacific), Russian fishing fleets are a major world fish supplier. Russia captured 3,191,068 tons of fish in 2005. Both exports and imports of fish and sea products grew significantly in recent years, reaching $2,415 and $2,036 million, respectively, in 2008.
Sprawling from the Baltic Sea to the Pacific Ocean, Russia has more than a fifth of the world's forests, which makes it the largest forest country in the world. However, according to a 2012 study by the Food and Agriculture Organization of the United Nations and the Government of the Russian Federation, the considerable potential of Russian forests is underutilized and Russia's share of the global trade in forest products is less than four percent.
Energy.
In recent years, Russia has frequently been described in the media as an energy superpower. The country has the world's largest natural gas reserves, the 8th largest oil reserves, and the second largest coal reserves. Russia is the world's leading natural gas exporter and second largest natural gas producer, while also the largest oil exporter and the largest oil producer.
Russia is the 3rd largest electricity producer in the world and the 5th largest renewable energy producer, the latter because of the well-developed hydroelectricity production in the country. Large cascades of hydropower plants are built in European Russia along big rivers like the Volga. The Asian part of Russia also features a number of major hydropower stations, however the gigantic hydroelectric potential of Siberia and the Russian Far East largely remains unexploited.
Russia was the first country to develop civilian nuclear power and to construct the world's first nuclear power plant. Currently the country is the 4th largest nuclear energy producer, with all nuclear power in Russia being managed by Rosatom State Corporation. The sector is rapidly developing, with an aim of increasing the total share of nuclear energy from current 16.9% to 23% by 2020. The Russian government plans to allocate 127 billion rubles ($5.42 billion) to a federal program dedicated to the next generation of nuclear energy technology. About 1 trillion rubles ($42.7 billion) is to be allocated from the federal budget to nuclear power and industry development before 2015.
In May 2014 on a two-day trip to Shanghai, President Putin signed a deal on behalf of Gazprom for the Russian energy giant to supply China with 38 billion cubic meters of natural gas per year. Construction of a pipeline to facilitate the deal was agreed whereby Russia would contribute $55bn to the cost, and China $22bn, in what Putin described as "the world's biggest construction project for the next four years." The natural gas would begin to flow sometime between 2018 and 2020 and would continue for 30 years at an ultimate cost to China of $400bn.
Transport.
Railway transport in Russia is mostly under the control of the state-run Russian Railways monopoly. The company accounts for over 3.6% of Russia's GDP and handles 39% of the total freight traffic (including pipelines) and more than 42% of passenger traffic. The total length of common-used railway tracks exceeds , second only to the United States. Over of tracks are electrified, which is the largest number in the world, and additionally there are more than of industrial non-common carrier lines. Railways in Russia, unlike in the most of the world, use broad gauge of , with the exception of on Sakhalin island using narrow gauge of . The most renowned railway in Russia is Trans-Siberian ("Transsib"), spanning a record 7 time zones and serving the longest single continuous services in the world, Moscow-Vladivostok (), Moscow–Pyongyang () and Kiev–Vladivostok ().
Much of Russia's inland waterways, which total , are made up of natural rivers or lakes. In the European part of the country the network of channels connects the basins of major rivers. Russia's capital, Moscow, is sometimes called "the port of the five seas", because of its waterway connections to the Baltic, White, Caspian, Azov and Black Seas.
Major sea ports of Russia include Rostov-on-Don on the Azov Sea, Novorossiysk on the Black Sea, Astrakhan and Makhachkala on the Caspian, Kaliningrad and St Petersburg on the Baltic, Arkhangelsk on the White Sea, Murmansk on the Barents Sea, Petropavlovsk-Kamchatsky and Vladivostok on the Pacific Ocean. In 2008 the country owned 1,448 merchant marine ships. The world's only fleet of nuclear-powered icebreakers advances the economic exploitation of the Arctic continental shelf of Russia and the development of sea trade through the Northern Sea Route between Europe and East Asia.
By total length of pipelines Russia is second only to the United States. Currently many new pipeline projects are being realized, including Nord Stream and South Stream natural gas pipelines to Europe, and the Eastern Siberia – Pacific Ocean oil pipeline (ESPO) to the Russian Far East and China.
Russia has 1,216 airports, the busiest being Sheremetyevo, Domodedovo, and Vnukovo in Moscow, and Pulkovo in St. Petersburg.
Typically, major Russian cities have well-developed systems of public transport, with the most common varieties of exploited vehicles being bus, trolleybus and tram. Seven Russian cities, namely Moscow, Saint Petersburg, Nizhny Novgorod, Novosibirsk, Samara, Yekaterinburg, and Kazan, have underground metros, while Volgograd features a metrotram. The total length of metros in Russia is . Moscow Metro and Saint Petersburg Metro are the oldest in Russia, opened in 1935 and 1955 respectively. These two are among the fastest and busiest metro systems in the world, and are famous for rich decorations and unique designs of their stations, which is a common tradition in Russian metros and railways.
Science and technology.
Science and technology in Russia blossomed since the Age of Enlightenment, when Peter the Great founded the Russian Academy of Sciences and Saint Petersburg State University, and polymath Mikhail Lomonosov established the Moscow State University, paving the way for a strong native tradition in learning and innovation. In the 19th and 20th centuries the country produced a large number of notable scientists and inventors.
The Russian physics school began with Lomonosov who proposed the law of conservation of matter preceding the energy conservation law. Russian discoveries and inventions in physics include the electric arc, electrodynamical Lenz's law, space groups of crystals, photoelectric cell, superfluidity, Cherenkov radiation, electron paramagnetic resonance, heterotransistors and 3D holography. Lasers and masers were co-invented by Nikolai Basov and Alexander Prokhorov, while the idea of tokamak for controlled nuclear fusion was introduced by Igor Tamm, Andrei Sakharov and Lev Artsimovich, leading eventually the modern international ITER project, where Russia is a party.
Since the time of Nikolay Lobachevsky (the "Copernicus of Geometry" who pioneered the non-Euclidean geometry) and a prominent tutor Pafnuty Chebyshev, the Russian mathematical school became one of the most influential in the world. Chebyshev's students included Aleksandr Lyapunov, who founded the modern stability theory, and Andrey Markov who invented the Markov chains. In the 20th century Soviet mathematicians, such as Andrey Kolmogorov, Israel Gelfand, and Sergey Sobolev, made major contributions to various areas of mathematics. Nine Soviet/Russian mathematicians were awarded with Fields Medal, a most prestigious award in mathematics. Recently Grigori Perelman was offered the first ever Clay Millennium Prize Problems Award for his final proof of the Poincaré conjecture in 2002.
Russian chemist Dmitry Mendeleev invented the Periodic table, the main framework of modern chemistry. Aleksandr Butlerov was one of the creators of the theory of chemical structure, playing a central role in organic chemistry. Russian biologists include Dmitry Ivanovsky who discovered viruses, Ivan Pavlov who was the first to experiment with the classical conditioning, and Ilya Mechnikov who was a pioneer researcher of the immune system and probiotics.
Many Russian scientists and inventors were émigrés, like Igor Sikorsky, who built the first airliners and modern-type helicopters; Vladimir Zworykin, often called the father of TV; chemist Ilya Prigogine, noted for his work on dissipative structures and complex systems; Nobel Prize-winning economists Simon Kuznets and Wassily Leontief; physicist Georgiy Gamov (an author of the Big Bang theory) and social scientist Pitirim Sorokin. Many foreigners worked in Russia for a long time, like Leonard Euler and Alfred Nobel.
Russian inventions include arc welding by Nikolay Benardos, further developed by Nikolay Slavyanov, Konstantin Khrenov and other Russian engineers. Gleb Kotelnikov invented the knapsack parachute, while Evgeniy Chertovsky introduced the pressure suit. Alexander Lodygin and Pavel Yablochkov were pioneers of electric lighting, and Mikhail Dolivo-Dobrovolsky introduced the first three-phase electric power systems, widely used today. Sergei Lebedev invented the first commercially viable and mass-produced type of synthetic rubber. The first ternary computer, "Setun", was developed by Nikolay Brusentsov.
In the 20th century a number of prominent Soviet aerospace engineers, inspired by the fundamental works of Nikolai Zhukovsky, Sergei Chaplygin and others, designed many hundreds of models of military and civilian aircraft and founded a number of "KBs" ("Construction Bureaus") that now constitute the bulk of Russian United Aircraft Corporation. Famous Russian aircraft include the civilian Tu-series, Su and MiG fighter aircraft, Ka and Mi-series helicopters; many Russian aircraft models are on the list of most produced aircraft in history.
Famous Russian battle tanks include T34, the most heavily produced tank design of World War II, and further tanks of T-series, including the most produced tank in history, T54/55. The AK47 and AK74 by Mikhail Kalashnikov constitute the most widely used type of assault rifle throughout the world—so much so that more AK-type rifles have been manufactured than all other assault rifles combined.
With all these achievements, however, since the late Soviet era Russia was lagging behind the West in a number of technologies, mostly those related to energy conservation and consumer goods production. The crisis of the 1990s led to the drastic reduction of the state support for science and a brain drain migration from Russia.
In the 2000s, on the wave of a new economic boom, the situation in the Russian science and technology has improved, and the government launched a campaign aimed into modernisation and innovation. Russian President Dmitry Medvedev formulated top priorities for the country's technological development:
Currently Russia has completed the GLONASS satellite navigation system. The country is developing its own fifth-generation jet fighter and constructing the first serial mobile nuclear plant in the world.
Space exploration.
Russian achievements in the field of space technology and space exploration are traced back to Konstantin Tsiolkovsky, the father of theoretical astronautics. His works had inspired leading Soviet rocket engineers, such as Sergey Korolyov, Valentin Glushko, and many others who contributed to the success of the Soviet space program on early stages of the Space Race and beyond.
In 1957 the first Earth-orbiting artificial satellite, "Sputnik 1", was launched; in 1961 the first human trip into space was successfully made by Yury Gagarin. Many other Soviet and Russian space exploration records ensued, including the first spacewalk performed by Alexey Leonov, Luna 9 was the first spacecraft to land on the Moon, Venera 7 was the first to land on another planet (Venus), Mars 3 then the first to land on Mars, the first space exploration rover "Lunokhod 1" and the first space station "Salyut 1" and "Mir".
After the collapse of the Soviet Union, some government-funded space exploration programs, including the Buran space shuttle program, were cancelled or delayed, while participation of the Russian space industry in commercial activities and international cooperation intensified.
Nowadays Russia is the largest satellite launcher. After the U.S. Space Shuttle program ended in 2011, Soyuz rockets became the only provider of transport for astronauts at the International Space Station.
Water supply and sanitation.
In Russia, approximately 70 per cent of drinking water comes from surface water and 30 per cent from groundwater. In 2004, water supply systems had a total capacity of 90 million cubic metres a day. The average residential water use was 248 litres per capita per day. One fourth of the world's fresh surface and groundwater is located in Russia. The water utilities sector is one of the largest industries in Russia serving the entire Russian population.
Demographics.
Ethnic Russians comprise 81% of the country's population. The Russian Federation is also home to several sizeable minorities. In all, 160 different other ethnic groups and indigenous peoples live within its borders. Though Russia's population is comparatively large, its density is low because of the country's enormous size. Population is densest in European Russia, near the Ural Mountains, and in southwest Siberia. 73% of the population lives in urban areas while 27% in rural ones. The results of the 2010 Census show a total population of 142,856,536.
Russia's population peaked at 148,689,000 in 1991, just before the dissolution of the Soviet Union. It began to experience a rapid decline starting in the mid-1990s. The decline has slowed to near stagnation in recent years because of reduced death rates, increased birth rates and increased immigration.
In 2009, Russia recorded annual population growth for the first time in fifteen years, with total growth of 10,500. 279,906 migrants arrived to the Russian Federation the same year, of which 93% came from CIS countries. The number of Russian emigrants steadily declined from 359,000 in 2000 to 32,000 in 2009. There are also an estimated 10 million illegal immigrants from the ex-Soviet states in Russia. Russia is home to approximately 116 million ethnic Russians and about 20 million ethnic Russians live outside Russia in the former republics of the Soviet Union, mostly in Ukraine and Kazakhstan.
The 2010 census recorded 81% of the population as ethnically Russian, and 19% as other ethnicities: 3.7% Tatars; 1.4% Ukrainians; 1.1% Bashkirs; 1% Chuvashes; 11.8% others and unspecified. According to the Census, 84.93% of the Russian population belongs to European ethnic groups (Slavic, Germanic, Finnic other than Ugric, Greek, and others). This is a decline from the 2002, when they constituted for more than 86% of the population.
Russia's birth rate is higher than that of most European countries (12.6 births per 1000 people in 2010 compared to the European Union average of 9.90 per 1000), but its death rate is also substantially higher (in 2010, Russia's death rate was 14.3 per 1000 people compared to the EU average of 10.28 per 1000). The Russian Ministry of Health and Social Affairs predicted that by 2011 the death rate would equal the birth rate because of increase in fertility and decline in mortality. The government is implementing a number of programs designed to increase the birth rate and attract more migrants. Monthly government child-assistance payments were doubled to US$55, and a one-time payment of US$9,200 was offered to women who had a second child since 2007.
In 2006, in a bid to compensate for the country's demographic decline, the Russian government started simplifying immigration laws and launched a state program "for providing assistance to voluntary immigration of ethnic Russians from former Soviet republics". In 2009 Russia experienced its highest birth rate since the dissolution of the Soviet Union. In 2012, the birth rate increased again. Russia recorded 1,896,263 births, the highest number since 1990, and even exceeding annual births during the period 1967–1969, with a TFR of about 1.7, the highest since 1991. (Source: Vital statistics table below)
In August 2012, as the country saw its first demographic growth since the 1990s, President Putin declared that Russia's population could reach 146 million by 2025, mainly as a result of immigration.
Language.
Russia's 160 ethnic groups speak some 100 languages. According to the 2002 Census, 142.6 million people speak Russian, followed by Tatar with 5.3 million and Ukrainian with 1.8 million speakers. Russian is the only official state language, but the Constitution gives the individual republics the right to establish their own state languages in addition to Russian.
Despite its wide distribution, the Russian language is homogeneous throughout the country. Russian is the most geographically widespread language of Eurasia, as well as the most widely spoken Slavic language. It belongs to the Indo-European language family and is one of the living members of the East Slavic languages, the others being Belarusian and Ukrainian (and possibly Rusyn). Written examples of Old East Slavic ("Old Russian") are attested from the 10th century onwards.
Russian is the second-most used language on the Internet after English, one of two official languages aboard the International Space Station and is one of the six official languages of the UN.
35 languages are officially recognized in Russia in various regions by local governments.
Religion.
The ancestors of many of today's Russians practised Orthodox Christianity since the 10th century. According to the Orthodox Church Tradition, Christianity was first brought to the territory of modern Belarus, Russia and Ukraine by Saint Andrew, the first Apostle of Jesus Christ. Following the Primary Chronicle, the definitive Christianization of Kievan Rus' dates from the year 988 (the year is disputed), when Vladimir the Great was baptized in Chersonesus and proceeded to baptize his family and people in Kiev. The latter events are traditionally referred to as baptism of Rus' (, ) in Russian and Ukrainian literature.
At the time of the 1917 Revolution, the Russian Orthodox Church was deeply integrated into the autocratic state, enjoying official status. This was a significant factor that contributed to the Bolshevik attitude to religion and the steps they took to control it. Bolsheviks consisted of many people with non-Russian, Communist Russians and influential Jewish backgrounds such as Vladimir Lenin, Leon Trotsky, Grigory Zinoviev, Lev Kamenev, Grigori Sokolnikov who were indifferent towards Christianity and based on the writings of Jewish philosopher Karl Marx with Marxism–Leninism as an ideology went on to form the Communist party. 
Thus the USSR became the first state to have, as an ideological objective, the elimination of religion and its replacement with universal atheism. The communist regime confiscated religious property, ridiculed religion, harassed believers, and propagated atheism in schools. The confiscation of religious assets was often based on accusations of illegal accumulation of wealth.
The vast majority of people in the Russian empire were, at the time of the revolution, religious believers, whereas the communists aimed to break the power of all religious institutions and eventually replace religious belief with atheism. "Science" was counterposed to "religious superstition" in the media and in academic writing. The main religions of pre-revolutionary Russia persisted throughout the entire Soviet period, but they were only tolerated within certain limits. Generally, this meant that believers were free to worship in private and in their respective religious buildings (churches, mosques, etc.), but public displays of religion outside of such designated areas were prohibited. In addition, religious institutions were not allowed to express their views in any type of mass media, and many religious buildings were demolished or used for other purposes.
State Atheism in the Soviet Union was known as "gosateizm", and was based on the ideology of Marxism–Leninism. Marxist–Leninist Atheism has consistently advocated the control, suppression, and elimination of religion. Within about a year of the revolution, the state expropriated all church property, including the churches themselves, and in the period from 1922 to 1926, 28 Russian Orthodox bishops and more than 1,200 priests were killed. Many more were persecuted.
Currently, there is no official census of religion in Russia, and estimates are based on surveys only. In August 2012, ARENA estimated that about 46.8% of Russians are Christians (including Orthodox, Catholic, Protestant, and non-denominational), while 25% believed in God but without any religion. However, later that year, the Levada Center estimated that 76% of Russians are Christians, and in June 2013, the Public Opinion Foundation estimated that 65% of Russians are Christians. These findings are in line with Pew Research Center's 2011 survey, which estimated that 73.6% of Russians are Christians, with Russian Public Opinion Research Center (VCIOM)'s 2010 survey (~77% Christian), and with Ipsos MORI's 2011 survey (69%). Orthodox Christianity, Islam, Judaism and Buddhism are Russia's traditional religions, and are all legally a part of Russia's "historical heritage".
Traced back to the Christianization of Kievan Rus' in the 10th century, Russian Orthodoxy is the dominant religion in the country; smaller Christian denominations such as Catholics, Armenian Gregorians and various Protestant churches also exist. The Russian Orthodox Church was the country's state religion prior to the Revolution and remains the largest religious body in the country. An estimated 95% of the registered Orthodox parishes belong to the Russian Orthodox Church while there are a number of smaller Orthodox Churches. However, the vast majority of Orthodox believers do not attend church on a regular basis. Easter is the most popular religious holiday in Russia, celebrated by a large segment of the Russian population, including large numbers of those who are non-religious. More than three-quarters of the Russian population celebrate Easter by making traditional Easter cakes, coloured eggs and paskha.
Islam is the second largest religion in Russia after Russian Orthodoxy. It is the traditional or predominant religion amongst some Caucasian ethnicities (notably the Chechens, the Ingush and the Circassians), and amongst some Turkic peoples (notably the Tatars and the Bashkirs). Altogether, there are 9,400,000 Muslims in Russia or 6.5% of the total population (the share of Muslims is probably much higher because the survey doesn't include detailed data for the traditionally Islamic states of Chechnya and Ingushetia). Notwithstanding, various differences split the Muslim population in different groups. According to the survey, most of the Muslims (precisely 6,700,000 or 4.6% of the total population) are "unaffiliated" to any Islamic schools and branches or Islamic organisation, this is mainly because it is not essential for Muslims to be affiliated with any specific sect or organization. Those who are affiliated are mostly Sunni Muslims, with Shia and Ahmadiyya minorities. Unaffiliated Muslims constitute significant numbers of over 10% in Kabardino-Balkaria (49%), Bashkortostan (38%), Karachay-Cherkessia (34%), Tatarstan (31%), Yamalia (13%), Orenburg Oblast (11%), Adygea (11%) and Astrakhan Oblast (11%). Most of the regions of Siberia have an unaffiliated Muslim population of 1% to 2%.
Buddhism is traditional in three regions of the Russian Federation: Buryatia, Tuva, and Kalmykia. Some residents of the Siberian and Far Eastern regions, such as Yakutia and Chukotka, practice shamanist, pantheistic, and pagan rites, along with the major religions. Induction into religion takes place primarily along ethnic lines. Slavs are significantly Orthodox Christian, Turkic speakers are predominantly Muslim, and Mongolic peoples are generally Buddhists.
According to various western purposive reports, the number of non-religious in Russia is between 16% and 48% of the population. The number of atheists has decreased significantly; according to the recent statistic, only seven percent declared themselves atheists, a decrease of 5% in three years. In a 2012 poll by Gallup International, 6% of Russian people reported that they were "convinced atheists which is lowest among European countries"
Health.
The Russian Constitution guarantees free, universal health care for all its citizens. In practice, however, free health care is partially restricted because of mandatory registration. While Russia has more physicians, hospitals, and health care workers than almost any other country in the world on a per capita basis, since the dissolution of the Soviet Union the health of the Russian population has declined considerably as a result of social, economic, and lifestyle changes; the trend has been reversed only in the recent years, with average life expectancy having increased 5.2 years for males and 3.1 years for females between 2006 and 2014.
, the average life expectancy in Russia was 65.29 years for males and 76.49 years for females. The biggest factor contributing to the relatively low life expectancy for males is a high mortality rate among working-age males. Deaths mostly occur because of preventable causes (e.g., alcohol poisoning, smoking, traffic accidents, violent crime). As a result of the large gender difference in life expectancy, and also because of the lasting effect of high casualties in World War II, the gender imbalance remains to this day; there are 0.859 males to every female.
Education.
Russia has the most college-level or higher graduates in terms of percentage of population in the world, at 54%. Russia has a free education system, which is guaranteed for all citizens by the Constitution, however entry to subsidized higher education is highly competitive. As a result of great emphasis on science and technology in education, Russian medical, mathematical, scientific, and aerospace research is generally of a high order.
Since 1990, the 11-year school education has been introduced. Education in state-owned secondary schools is free. University level education is free, with exceptions. A substantial share of students is enrolled for full pay (many state institutions started to open commercial positions in the last years).
In 2004, state spending for education amounted to 3.6% of the GDP, or 13% of the consolidated state budget. The Government allocates funding to pay the tuition fees within an established quota or number of students for each state institution. In higher education institutions, students are paid a small stipend and provided with free housing if they are from out of town.
The oldest and largest Russian universities are Moscow State University and Saint Petersburg State University. In the 2000s, in order to create higher education and research institutions of comparable scale in Russian regions, the government launched a program of establishing "federal universities", mostly by merging existing large regional universities and research institutes and providing them with a special funding. These new institutions include the Southern Federal University, Siberian Federal University, Kazan Volga Federal University, North-Eastern Federal University, and Far Eastern Federal University.
Culture.
Folk culture and cuisine.
There are over 160 different ethnic groups and indigenous peoples in Russia. The country's vast cultural diversity spans ethnic Russians with their Slavic Orthodox traditions, Tatars and Bashkirs with their Turkic Muslim culture, Buddhist nomadic Buryats and Kalmyks, Shamanistic peoples of the Extreme North and Siberia, highlanders of the Northern Caucasus, and Finno-Ugric peoples of the Russian North West and Volga Region.
Handicraft, like Dymkovo toy, khokhloma, gzhel and palekh miniature represent an important aspect of Russian folk culture. Ethnic Russian clothes include kaftan, kosovorotka and ushanka for men, sarafan and kokoshnik for women, with lapti and valenki as common shoes. The clothes of Cossacks from Southern Russia include burka and papaha, which they share with the peoples of the Northern Caucasus.
Russian cuisine widely uses fish, poultry, mushrooms, berries, and honey. Crops of rye, wheat, barley, and millet provide the ingredients for various breads, pancakes and cereals, as well as for kvass, beer and vodka drinks. Black bread is rather popular in Russia, compared to the rest of the world. Flavourful soups and stews include shchi, borsch, ukha, solyanka and okroshka. Smetana (a heavy sour cream) is often added to soups and salads. Pirozhki, blini and syrniki are native types of pancakes. Chicken Kiev, pelmeni and shashlyk are popular meat dishes, the last two being of Tatar and Caucasus origin respectively. Other meat dishes include stuffed cabbage rolls "(golubtsy)" usually filled with meat. Salads include Olivier salad, vinegret and dressed herring.
Russia's large number of ethnic groups have distinctive traditions regarding folk music. Typical ethnic Russian musical instruments are gusli, balalaika, zhaleika, and garmoshka. Folk music had a significant influence on Russian classical composers, and in modern times it is a source of inspiration for a number of popular folk bands, like Melnitsa. Russian folk songs, as well as patriotic Soviet songs, constitute the bulk of the repertoire of the world-renowned Red Army choir and other popular ensembles.
Russians have many traditions, including the washing in banya, a hot steam bath somewhat similar to sauna. Old Russian folklore takes its roots in the pagan Slavic religion. Many Russian fairy tales and epic bylinas were adapted for animation films, or for feature movies by the prominent directors like Aleksandr Ptushko ("Ilya Muromets", "Sadko") and Aleksandr Rou ("Morozko", "Vasilisa the Beautiful"). Russian poets, including Pyotr Yershov and Leonid Filatov, made a number of well-known poetical interpretations of the classical fairy tales, and in some cases, like that of Alexander Pushkin, also created fully original fairy tale poems of great popularity.
Architecture.
Since the Christianization of Kievan Rus' for several ages Russian architecture was influenced predominantly by the Byzantine architecture. Apart from fortifications (kremlins), the main stone buildings of ancient Rus' were Orthodox churches with their many domes, often gilded or brightly painted.
Aristotle Fioravanti and other Italian architects brought Renaissance trends into Russia since the late 15th century, while the 16th century saw the development of unique tent-like churches culminating in Saint Basil's Cathedral. By that time the onion dome design was also fully developed. In the 17th century, the "fiery style" of ornamentation flourished in Moscow and Yaroslavl, gradually paving the way for the Naryshkin baroque of the 1690s. After the reforms of Peter the Great the change of architectural styles in Russia generally followed that in the Western Europe.
The 18th-century taste for rococo architecture led to the ornate works of Bartolomeo Rastrelli and his followers. The reigns of Catherine the Great and her grandson Alexander I saw the flourishing of Neoclassical architecture, most notably in the capital city of Saint Petersburg. The second half of the 19th century was dominated by the Neo-Byzantine and Russian Revival styles. Prevalent styles of the 20th century were the Art Nouveau, Constructivism, and the Stalin Empire style.
In 1955, a new Soviet leader, Nikita Khrushchev, condemned the "excesses" of the former academic architecture, and the late Soviet era was dominated by plain functionalism in architecture. This helped somewhat to resolve the housing problem, but created a large quantity of buildings of low architectural quality, much in contrast with the previous bright styles. The situation improved in the recent two decades. Many temples demolished in Soviet times were rebuilt, and this process continues along with the restoration of various historical buildings destroyed in World War II. A total of 23,000 Orthodox churches have been rebuilt between 1991 and 2010, which effectively quadrapled the number of operating churches in Russia.
Visual arts.
Early Russian painting is represented in icons and vibrant frescos, the two genres inherited from Byzantium. As Moscow rose to power, Theophanes the Greek, Dionisius and Andrei Rublev became vital names associated with a distinctly Russian art.
The Russian Academy of Arts was created in 1757 and gave Russian artists an international role and status. Ivan Argunov, Dmitry Levitzky, Vladimir Borovikovsky and other 18th century academicians mostly focused on portrait painting. In the early 19th century, when neoclassicism and romantism flourished, mythological and Biblical themes inspired many prominent paintings, notably by Karl Briullov and Alexander Ivanov.
In the mid-19th century the "Peredvizhniki" ("Wanderers") group of artists broke with the Academy and initiated a school of art liberated from academic restrictions. These were mostly realist painters who captured Russian identity in landscapes of wide rivers, forests, and birch clearings, as well as vigorous genre scenes and robust portraits of their contemporaries. Some artists focused on depicting dramatic moments in Russian history, while others turned to social criticism, showing the conditions of the poor and caricaturing authority; critical realism flourished under the reign of Alexander II. Leading realists include Ivan Shishkin, Arkhip Kuindzhi, Ivan Kramskoi, Vasily Polenov, Isaac Levitan, Vasily Surikov, Viktor Vasnetsov, Ilya Repin, and Boris Kustodiev.
The turn of the 20th century saw the rise of symbolist painting, represented by Mikhail Vrubel, Kuzma Petrov-Vodkin, and Nicholas Roerich.
The Russian avant-garde was a large, influential wave of modernist art that flourished in Russia from approximately 1890 to 1930. The term covers many separate, but inextricably related art movements that occurred at the time, namely neo-primitivism, suprematism, constructivism, rayonism, and Russian Futurism. Notable artists from this era include El Lissitzky, Kazimir Malevich, Wassily Kandinsky, and Marc Chagall. Since the 1930s the revolutionary ideas of the avant-garde clashed with the newly emerged conservative direction of socialist realism.
Soviet art produced works that were furiously patriotic and anti-fascist during and after the Great Patriotic War. Multiple war memorials, marked by a great restrained solemnity, were built throughout the country. Soviet artists often combined innovation with socialist realism, notably the sculptors Vera Mukhina, Yevgeny Vuchetich and Ernst Neizvestny.
Music and dance.
Music in 19th century Russia was defined by the tension between classical composer Mikhail Glinka along with other members of The Mighty Handful, who embraced Russian national identity and added religious and folk elements to their compositions, and the Russian Musical Society led by composers Anton and Nikolay Rubinsteins, which was musically conservative. The later tradition of Pyotr Ilyich Tchaikovsky, one of the greatest composers of the Romantic era, was continued into the 20th century by Sergei Rachmaninoff. World-renowned composers of the 20th century include Alexander Scriabin, Igor Stravinsky, Sergei Prokofiev, Dmitri Shostakovich and Alfred Schnittke.
Russian conservatories have turned out generations of famous soloists. Among the best known are violinists Jascha Heifetz, David Oistrakh, Leonid Kogan, Gidon Kremer, and Maxim Vengerov; cellists Mstislav Rostropovich, Natalia Gutman; pianists Vladimir Horowitz, Sviatoslav Richter, Emil Gilels, Vladimir Sofronitsky and Evgeny Kissin; and vocalists Fyodor Shalyapin, Mark Reizen, Elena Obraztsova, Tamara Sinyavskaya, Nina Dorliak, Galina Vishnevskaya, Anna Netrebko and Dmitry Hvorostovsky.
During the early 20th century, Russian ballet dancers Anna Pavlova and Vaslav Nijinsky rose to fame, and impresario Sergei Diaghilev and his Ballets Russes' travels abroad profoundly influenced the development of dance worldwide. Soviet ballet preserved the perfected 19th century traditions, and the Soviet Union's choreography schools produced many internationally famous stars, including Galina Ulanova, Maya Plisetskaya, Rudolf Nureyev, and Mikhail Baryshnikov. The Bolshoi Ballet in Moscow and the Mariinsky Ballet in St Petersburg remain famous throughout the world.
Modern Russian rock music takes its roots both in the Western rock and roll and heavy metal, and in traditions of the Russian bards of the Soviet era, such as Vladimir Vysotsky and Bulat Okudzhava. Popular Russian rock groups include Mashina Vremeni, DDT, Aquarium, Alisa, Kino, Kipelov, Nautilus Pompilius, Aria, Grazhdanskaya Oborona, Splean and Korol i Shut. Russian pop music developed from what was known in the Soviet times as "estrada" into full-fledged industry, with some performers gaining wide international recognition, such as t.A.T.u., Nu Virgos and Vitas.
Literature and philosophy.
In the 18th century, during the era of Russian Enlightenment, the development of Russian literature was boosted by the works of Mikhail Lomonosov and Denis Fonvizin. By the early 19th century a modern native tradition had emerged, producing some of the greatest writers in Russian history. This period, known also as the Golden Age of Russian Poetry, began with Alexander Pushkin, who is considered the founder of the modern Russian literary language and often described as the "Russian Shakespeare". It continued into the 19th century with the poetry of Mikhail Lermontov and Nikolay Nekrasov, dramas of Alexander Ostrovsky and Anton Chekhov, and the prose of Nikolai Gogol and Ivan Turgenev. Leo Tolstoy and Fyodor Dostoyevsky have been described by literary critics as the greatest novelists of all time.
By the 1880s, the age of the great novelists was over, and short fiction and poetry became the dominant genres. The next several decades became known as the Silver Age of Russian Poetry, when the previously dominant literary realism was replaced by symbolism. Leading authors of this era include such poets as Valery Bryusov, Vyacheslav Ivanov, Alexander Blok, Nikolay Gumilev and Anna Akhmatova, and novelists Leonid Andreyev, Ivan Bunin, and Maxim Gorky.
Russian philosophy blossomed in the 19th century, when it was defined initially by the opposition of Westernizers, advocating Western political and economical models, and Slavophiles, insisting on developing Russia as a unique civilization. The latter group includes Nikolai Danilevsky and Konstantin Leontiev, the founders of eurasianism. In its further development Russian philosophy was always marked by a deep connection to literature and interest in creativity, society, politics and nationalism; Russian cosmism and religious philosophy were other major areas. Notable philosophers of the late 19th and the early 20th centuries include Vladimir Solovyev, Sergei Bulgakov, and Vladimir Vernadsky.
Following the Russian Revolution of 1917 many prominent writers and philosophers left the country, including Bunin, Vladimir Nabokov and Nikolay Berdyayev, while a new generation of talented authors joined together in an effort to create a distinctive working-class culture appropriate for the new Soviet state. In the 1930s censorship over literature was tightened in line with the policy of socialist realism. In the late 1950s restrictions on literature were eased, and by the 1970s and 1980s, writers were increasingly ignoring official guidelines. Leading authors of the Soviet era include novelists Yevgeny Zamyatin, Ilf and Petrov, Mikhail Bulgakov and Mikhail Sholokhov, and poets Vladimir Mayakovsky, Yevgeny Yevtushenko, and Andrey Voznesensky.
The Soviet Union was also a major producer of science fiction, written by authors like Arkady and Boris Strugatsky, Kir Bulychov, Alexander Belayev and Ivan Yefremov. Traditions of Russian science fiction and fantasy are continued today by numerous writers.
Cinema, animation and media.
Russian and later Soviet cinema was a hotbed of invention in the period immediately following 1917, resulting in world-renowned films such as "The Battleship Potemkin" by Sergei Eisenstein. Eisenstein was a student of filmmaker and theorist Lev Kuleshov, who developed the Soviet montage theory of film editing at the world's first film school, the All-Union Institute of Cinematography. Dziga Vertov, whose "kino-glaz" ("film-eye") theory—that the camera, like the human eye, is best used to explore real life—had a huge impact on the development of documentary film making and cinema realism. The subsequent state policy of socialist realism somewhat limited creativity; however, many Soviet films in this style were artistically successful, including "Chapaev", "The Cranes Are Flying", and "Ballad of a Soldier".
The 1960s and 1970s saw a greater variety of artistic styles in Soviet cinema. Eldar Ryazanov's and Leonid Gaidai's comedies of that time were immensely popular, with many of the catch phrases still in use today. In 1961–68 Sergey Bondarchuk directed an Oscar-winning film adaptation of Leo Tolstoy's epic "War and Peace", which was the most expensive film made in the Soviet Union. In 1969, Vladimir Motyl's "White Sun of the Desert" was released, a very popular film in a genre of ostern; the film is traditionally watched by cosmonauts before any trip into space.
Russian animation dates back to late Russian Empire times. During the Soviet era, Soyuzmultfilm studio was the largest animation producer. Soviet animators developed a great variety of pioneering techniques and aesthetic styles, with prominent directors including Ivan Ivanov-Vano, Fyodor Khitruk and Aleksandr Tatarsky. Many Soviet cartoon heroes such as the Russian-style Winnie-the-Pooh, cute little Cheburashka, Wolf and Hare from "Nu, Pogodi!", are iconic images in Russia and many surrounding countries.
The late 1980s and 1990s were a period of crisis in Russian cinema and animation. Although Russian filmmakers became free to express themselves, state subsidies were drastically reduced, resulting in fewer films produced. The early years of the 21st century have brought increased viewership and subsequent prosperity to the industry on the back of the economic revival. Production levels are already higher than in Britain and Germany. Russia's total box-office revenue in 2007 was $565 million, up 37% from the previous year. In 2002 the "Russian Ark" became the first feature film ever to be shot in a single take. The traditions of Soviet animation were developed recently by such directors as Aleksandr Petrov and studios like Melnitsa Animation.
Russia was among the first countries to introduce radio and television. While there were few channels in the Soviet time, in the past two decades many new state and privately owned radio stations and TV channels have appeared. In 2005 a state-run English language Russia Today TV started broadcasting, and its Arabic version Rusiya Al-Yaum was launched in 2007.
Sports.
Combining the total medals of the Soviet Union and Russia, the country is second among all nations by number of gold medals both at the Summer Olympics and at the Winter Olympics. Soviet and later Russian athletes have always been in the top three for the number of gold medals collected at the Summer Olympics. Soviet gymnasts, track-and-field athletes, weight lifters, wrestlers, boxers, fencers, shooters, cross country skiers, biathletes, speed skaters and figure skaters were consistently among the best in the world, along with Soviet basketball, handball, volleyball and ice hockey players. The 1980 Summer Olympics were held in Moscow while the 2014 Winter Olympics were hosted in Sochi.
Although ice hockey was only introduced during the Soviet era, the national team managed to win gold at almost all the Olympics and World Championships they contested. Russian players Valery Kharlamov, Sergei Makarov, Vyacheslav Fetisov and Vladislav Tretiak hold four of six positions in the IIHF "Team of the Century". Russia has not won the Olympic ice hockey tournament since the Unified Team won gold in 1992. Recently Russia won the 2008, 2009, 2012 and the 2014 IIHF World Championships. Russia dominated the 2012 tournament, winning all of its ten matches—the first time any team had done so since the Soviet Union in 1989.
The Kontinental Hockey League (KHL) was founded in 2008 as a successor to the Russian Superleague. It is seen as a rival to the National Hockey League (NHL), is ranked the top hockey league in Europe , and the second-best in the world. It is an international professional ice hockey league in Eurasia and consists of 28 teams, of which 21 are based in Russia and 7 more are located in Latvia, Kazakhstan, Belarus, Finland, Slovakia, and Croatia.
Bandy, also known as Russian hockey, is another traditionally popular ice sport. The Soviet Union won all the Bandy World Championships for men between 1957–79 and some thereafter too. After the dissolution of the Soviet Union, Russia has continuously been one of the most successful teams, winning many world championships.
Association football is one of the most popular sports in modern Russia. The Soviet national team became the first ever European Champions by winning Euro 1960. Appearing in four FIFA World Cups from 1958 to 1970, Lev Yashin is regarded to be one of the greatest goalkeepers in the history of football, and was chosen on the FIFA World Cup Dream Team. The Soviet national team reached the final of Euro 1988. In 1956 and 1988, the Soviet Union won gold at the Olympic football tournament. Russian clubs CSKA Moscow and Zenit St Petersburg won the UEFA Cup in 2005 and 2008 respectively. The Russian national football team reached the semi-finals of Euro 2008, losing only to the eventual champions Spain. Russia will host the 2018 FIFA World Cup, with 11 host cities located in the European part of the country and in the Ural region.
In 2007, the Russian national basketball team won the European Basketball Championship. Russian basketball club PBC CSKA Moscow is one of the top teams in Europe, winning the Euroleague in 2006 and 2008.
Larisa Latynina, who currently holds the record for the most gold Olympic medals won by a woman (and held the record for most Olympic medals won per person from 1964 until 2012 when swimmer Michael Phelps replaced her record), established the USSR as the dominant force in gymnastics for many years. Today, Russia is the leading nation in rhythmic gymnastics with Yevgeniya Kanayeva. Russian synchronized swimming is the best in the world, with almost all gold medals at Olympics and World Championships having been swept by Russians in recent decades. Figure skating is another popular sport in Russia, especially pair skating and ice dancing. With the exception of 2010 a Soviet or Russian pair has won gold at every Winter Olympics since 1964.
Since the end of the Soviet era, tennis has grown in popularity and Russia has produced a number of famous players, including Maria Sharapova, the world's highest paid female athlete. In martial arts, Russia produced the sport Sambo and renowned fighters, like Fedor Emelianenko. Chess is a widely popular pastime in Russia; from 1927, Russian grandmasters have held the world chess championship almost continuously.
The 2014 Winter Olympics were held in Sochi in the south of Russia. Russia won the largest number of medals among the participating nations with 13 gold, 11 silver, and 9 bronze medals for a total of 33 medals. Commentators evaluated the Games as having been an overall success.
Formula One is also becoming increasingly popular in Russia. In 2010 Vitaly Petrov became the first Russian to drive in Formula One. There had only been two Russian Grands Prix (in 1913 and 1914), but the Russian Grand Prix returned as part of the Formula One season in 2014, as part of a six-year deal.
National holidays and symbols.
There are seven public holidays in Russia, except those always celebrated on Sunday. Russian New Year traditions resemble those of the Western Christmas, with New Year Trees and gifts, and Ded Moroz (Father Frost) playing the same role as Santa Claus. Orthodox Christmas falls on 7 January, because the Russian Orthodox Church still follows the Julian calendar, and all Orthodox holidays are 13 days after Western ones. Two other major Christian holidays are Easter and Trinity Sunday. Kurban Bayram and Uraza Bayram are celebrated by Russian Muslims.
Further Russian public holidays include Defender of the Fatherland Day (23 February), which honors Russian men, especially those serving in the army; International Women's Day (8 March), which combines the traditions of Mother's Day and Valentine's Day; Spring and Labor Day (1 May); Victory Day; Russia Day (12 June); and Unity Day (4 November), commemorating the popular uprising which expelled the Polish occupation force from Moscow in 1612.
Victory Day is the second most popular holiday in Russia; it commemorates the victory over Nazism in the Great Patriotic War. A huge military parade, hosted by the President of Russia, is annually organised in Moscow on Red Square. Similar parades take place in all major Russian cities and cities with the status "Hero city" or "City of Military Glory".
Popular non-public holidays include Old New Year (the New Year according to the Julian Calendar on 14 January), Tatiana Day (students holiday on 25 January), Maslenitsa (a pre-Christian spring holiday a week before the Great Lent), Cosmonautics Day (in tribute to the first human trip into space), Ivan Kupala Day (another pre-Christian holiday on 7 July) and Peter and Fevronia Day (which takes place on 8 July and is the Russian analogue of Valentine's Day, focusing, however, on family love and fidelity).
State symbols of Russia include the Byzantine double-headed eagle, combined with St. George of Moscow in the Russian coat of arms. The Russian flag dates from the late Tsardom of Russia period and has been widely used since the time of the Russian Empire. The Russian anthem shares its music with the Soviet Anthem, though not the lyrics. The imperial motto "God is with us" and the Soviet motto "Proletarians of all countries, unite!" are now obsolete and no new motto has replaced them. The hammer and sickle and the full Soviet coat of arms are still widely seen in Russian cities as a part of old architectural decorations. The Soviet Red Stars are also encountered, often on military equipment and war memorials. The Red Banner continues to be honored, especially the Banner of Victory of 1945.
The Matryoshka doll is a recognizable symbol of Russia, and the towers of Moscow Kremlin and Saint Basil's Cathedral in Moscow are Russia's main architectural icons. Cheburashka is a mascot of the Russian national Olympic team. St. Mary, St. Nicholas, St. Andrew, St. George, St. Alexander Nevsky, St. Sergius of Radonezh and St. Seraphim of Sarov are Russia's patron saints. Chamomile is the national flower, while birch is the national tree. The Russian bear is an animal symbol and a national personification of Russia, though this image has a Western origin and Russians themselves have accepted it only fairly recently. The native Russian national personification is Mother Russia.
Tourism.
Tourism in Russia has seen rapid growth since the late Soviet period, first domestic tourism and then international tourism, fueled by the rich cultural heritage and great natural variety of the country. Major tourist routes in Russia include a journey around the Golden Ring of ancient cities, cruises on the big rivers like the Volga, and long journeys on the famous Trans-Siberian Railway. In 2013, Russia was visited by 28.4 million tourists; it is the ninth most visited country in the world and the seventh most visited in Europe. The number of Western visitors dropped in 2014.
The most visited destinations in Russia are Moscow and Saint Petersburg, the current and former capitals of the country. Recognized as World Cities, they feature such world-renowned museums as the Tretyakov Gallery and the Hermitage, famous theaters like Bolshoi and Mariinsky, ornate churches like Saint Basil's Cathedral, Cathedral of Christ the Saviour, Saint Isaac's Cathedral and Church of the Savior on Blood, impressive fortifications like the Kremlin and Peter and Paul Fortress, beautiful squares and streets like Red Square, Palace Square, Tverskaya Street and Nevsky Prospect. Rich palaces and parks are found in the former imperial residences in suburbs of Moscow (Kolomenskoye, Tsaritsyno) and St Petersburg (Peterhof, Strelna, Oranienbaum, Gatchina, Pavlovsk and Tsarskoye Selo). Moscow displays Soviet architecture at its best, along with modern skyscrapers, while St Petersburg, nicknamed "Venice of the North", boasts of its classical architecture, many rivers, channels and bridges.
Kazan, the capital of Tatarstan, shows a mix of Christian Russian and Muslim Tatar cultures. The city has registered a brand "The Third Capital of Russia", though a number of other major cities compete for this status, including Novosibirsk, Yekaterinburg and Nizhny Novgorod.
The warm subtropical Black Sea coast of Russia is the site for a number of popular sea resorts, like Sochi, the follow-up host of the 2014 Winter Olympics. The mountains of the Northern Caucasus contain popular ski resorts such as Dombay. The most famous natural destination in Russia is Lake Baikal, "the Blue Eye of Siberia". This unique lake, the oldest and deepest in the world, has crystal-clear waters and is surrounded by taiga-covered mountains. Other popular natural destinations include Kamchatka with its volcanoes and geysers, Karelia with its lakes and granite rocks, the snowy Altai Mountains, and the wild steppes of Tyva.

</doc>
<doc id="25400" url="https://en.wikipedia.org/wiki?curid=25400" title="Rational choice theory">
Rational choice theory

Rational choice theory, also known as choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior.   • Amartya Sen (2008). "rational behaviour," "The New Palgrave Dictionary of Economics", 2nd Edition. Abstract.</ref> The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory therefore focuses on the determinants of the individual choices (methodological individualism).
Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option they prefer. These preferences are assumed to be complete (the person can always say which of two alternatives they consider preferable or that neither is preferred to the other) and transitive (if option A is preferred over option B and option B is preferred over option C, then A is preferred over C). The rational agent is assumed to take account of available information, probabilities of events, and potential costs and benefits in determining preferences, and to act consistently in choosing the self-determined best choice of action.
Rationality is widely used as an assumption of the behavior of individuals in microeconomic models and analyses and appears in almost all economics textbook treatments of human decision-making. It is also used in political science, sociology, and philosophy. A particular version of rationality is instrumental rationality, which involves seeking the most cost-effective means to achieve a specific goal without reflecting on the worthiness of that goal. Gary Becker was an early proponent of applying rational actor models more widely. Becker won the 1992 Nobel Memorial Prize in Economic Sciences for his studies of discrimination, crime, and human capital.
Definition and scope.
The concept of rationality used in rational choice theory is different from the colloquial and most philosophical use of the word. Colloquially, "rational" behaviour typically means "sensible", "predictable", or "in a thoughtful, clear-headed manner." Rational choice theory uses a narrower definition of rationality. At its most basic level, behavior is rational if it is goal-oriented, reflective (evaluative), and consistent (across time and different choice situations). This contrasts with behavior that is random, impulsive, conditioned, or adopted by (unevaluative) imitation.
Early neoclassical economists writing about rational choice, including William Stanley Jevons, assumed that agents make consumption choices so as to maximize their happiness, or utility. Contemporary theory bases rational choice on a set of choice axioms that need to be satisfied, and typically does not specify where the goal (preferences, desires) comes from. It mandates just a consistent ranking of the alternatives. Individuals choose the best action according to their personal preferences and the constraints facing them. E.g., there is nothing irrational in preferring fish to meat the first time, but there is something irrational in preferring fish to meat in one instant and preferring meat to fish in another, without anything else having changed.
Rational choice theorists do not claim that the theory describes the choice "process", but rather that it predicts the outcome and pattern of choices. 
An assumption often added to the rational choice paradigm is that individual preferences are self-interested, in which case the individual can be referred to as a homo oeconomicus. Such an individual acts "as if" balancing costs against benefits to arrive at action that maximizes personal advantage. Proponents of such models, particularly those associated with the Chicago school of economics, do not claim that a model's assumptions are an accurate description of reality, only that they help formulate clear and falsifiable hypotheses. In this view, the only way to judge the success of a hypothesis is empirical tests. To use an example from Milton Friedman, if a theory that says that the behavior of the leaves of a tree is explained by their rationality passes the empirical test, it is seen as successful.
Without specifying the individual's goal or preferences it may not be possible to empirically test, or falsify, the rationality assumption. However, the predictions made by a specific version of the theory are testable. In recent years, the most prevalent version of rational choice theory, expected utility theory, has been challenged by the experimental results of behavioral economics. Economists are learning from other fields, such as psychology, and are enriching their theories of choice in order to get a more accurate view of human decision-making. For example, the behavioral economist and experimental psychologist Daniel Kahneman won the Nobel Memorial Prize in Economic Sciences in 2002 for his work in this field.
Rational choice theory has become increasingly employed in social sciences other than economics, such as sociology, evolutionary theory and political science in recent decades. It has had far-reaching impacts on the study of political science, especially in fields like the study of interest groups, elections, behaviour in legislatures, coalitions, and bureaucracy. In these fields, the use of the rational choice paradigm to explain broad social phenomena is the subject of active controversy.
Actions, assumptions, and individual preferences.
The premise of rational choice theory as a social science methodology is that the aggregate behavior in society reflects the sum of the choices made by individuals. Each individual, in turn, makes their choice based on their own preferences and the constraints (or choice set) they face.
At the individual level, rational choice theory stipulates that the agent chooses the action (or outcome) they most prefer. In the case where actions (or outcomes) can be evaluated in terms of costs and benefits, a rational individual chooses the action (or outcome) that provides the maximum net benefit, i.e., the maximum benefit minus cost.
The theory applies to more general settings than those identified by costs and benefit. In general, rational decision making entails choosing among all available alternatives the alternative that the individual most prefers. The "alternatives" can be a set of actions ("what to do?") or a set of objects ("what to choose/buy"). In the case of actions, what the individual really cares about are the outcomes that results from each possible action. Actions, in this case, are only an instrument for obtaining a particular outcome.
Formal statement.
The available alternatives are often expressed as a set of objects, for example a set of "j" exhaustive and exclusive actions:
For example, if a person can choose to vote for either Roger or Sara or to abstain, their set of possible alternatives is:
The theory makes two technical assumptions about individuals' preferences over alternatives:
Together these two assumptions imply that given a set of exhaustive and exclusive actions to choose from, an individual can "rank" the elements of this set in terms of his preferences in an internally consistent way (the ranking constitutes a partial ordering), and the set has at least one maximal element.
The preference between two alternatives can be:
Research that took off in the 1980s sought to develop models which drop these assumptions and argue that such behaviour could still be rational, Anand (1993). This work, often conducted by economic theorists and analytical philosophers, suggests ultimately that the assumptions or axioms above are not completely general and might at best be regarded as approximations.
Additional assumptions.
Alternative theories of human action include such components as Amos Tversky and Daniel Kahneman's prospect theory, which reflects the empirical finding that, contrary to standard preferences assumed under neoclassical economics, individuals attach extra value to items that they already own compared to similar items owned by others. Under standard preferences, the amount that an individual is willing to pay for an item (such as a drinking mug) is assumed to equal the amount he or she is willing to be paid in order to part with it. In experiments, the latter price is sometimes significantly higher than the former (but see Plott and Zeiler 2005, Plott and Zeiler 2007 and Klass and Zeiler, 2013 ). Tversky and Kahneman do not characterize loss aversion as irrational. Behavioral economics includes a large number of other amendments to its picture of human behavior that go against neoclassical assumptions.
Utility maximization.
Often preferences are described by their utility function or "payoff function". This is an ordinal number an individual assigns over the available actions, such as:
The individual's preferences are then expressed as the relation between these ordinal assignments. For example, if an individual prefers the candidate Sara over Roger over abstaining, their preferences would have the relation:
A preference relation that as above satisfies completeness, transitivity, and, in addition, continuity, can be equivalently represented by a utility function.
Criticism.
Both the assumptions and the behavioral predictions of rational choice theory have sparked criticism from various camps. As mentioned above, some economists have developed models of bounded rationality, which hope to be more psychologically plausible without completely abandoning the idea that reason underlies decision-making processes. Other economists have developed more theories of human decision-making that allow for the roles of uncertainty, institutions, and determination of individual tastes by their socioeconomic environment (cf. Fernandez-Huerga, 2008).
Martin Hollis and Edward J. Nell's 1975 book offers both a philosophical critique of neo-classical economics and an innovation in the field of economic methodology. Further they outlined an alternative vision to neo-classicism based on a rationalist theory of knowledge. Within neo-classicism, the authors addressed consumer behaviour (in the form of indifference curves and simple versions of revealed preference theory) and marginalist producer behaviour in both product and factor markets. Both are based on rational optimizing behaviour. They consider imperfect as well as perfect markets since neo-classical thinking embraces many market varieties and disposes of a whole system for their classification. However, the authors believe that the issues arising from basic maximizing models have extensive implications for econometric methodology (Hollis and Nell, 1975, p. 2). In particular it is this class of models – rational behavior as maximizing behaviour – which provide support for specification and identification. And this, they argue, is where the flaw is to be found. Hollis and Nell (1975) argued that positivism (broadly conceived) has provided neo-classicism with important support, which they then show to be unfounded. They base their critique of neo-classicism not only on their critique of positivism but also on the alternative they propose, rationalism. Indeed, they argue that rationality is central to neo-classical economics – as rational choice – and that this conception of rationality is misused. Demands are made of it that it cannot fulfill.
In their 1994 work, "Pathologies of Rational Choice Theory", Donald P. Green and Ian Shapiro argue that the empirical outputs of rational choice theory have been limited. They contend that much of the applicable literature, at least in political science, was done with weak statistical methods and that when corrected many of the empirical outcomes no longer hold. When taken in this perspective, rational choice theory has provided very little to the overall understanding of political interaction - and is an amount certainly disproportionately weak relative to its appearance in the literature. Yet, they concede that cutting edge research, by scholars well-versed in the general scholarship of their fields (such as work on the U.S. Congress by Keith Krehbiel, Gary Cox, and Mat McCubbins) has generated valuable scientific progress.
Duncan K. Foley (2003, p. 1) has also provided an important criticism of the concept of "rationality" and its role in economics. He argued that“Rationality” has played a central role in shaping and establishing the hegemony of contemporary mainstream economics. As the specific claims of robust neoclassicism fade into the history of economic thought, an orientation toward situating explanations of economic phenomena in relation to rationality has increasingly become the touchstone by which mainstream economists identify themselves and recognize each other. This is not so much a question of adherence to any particular conception of rationality, but of taking rationality of individual behavior as the unquestioned starting point of economic analysis.
Foley (2003, p. 9) went on to argue thatThe concept of rationality, to use Hegelian language, represents the relations of modern capitalist society one-sidedly. The burden of rational-actor theory is the assertion that ‘naturally’ constituted individuals facing existential conflicts over scarce resources would rationally impose on themselves the institutional structures of modern capitalist society, or something approximating them. But this way of looking at matters systematically neglects the ways in which modern capitalist society and its social relations in fact constitute the ‘rational’, calculating individual. The well-known limitations of rational-actor theory, its static quality, its logical antinomies, its vulnerability to arguments of infinite regress, its failure to develop a progressive concrete research program, can all be traced to this starting-point.
Schram and Caterino (2006) contains a fundamental methodological criticism of rational choice theory for promoting the view that the natural science model is the only appropriate methodology in social science and that political science should follow this model, with its emphasis on quantification and mathematization. Schram and Caterino argue instead for methodological pluralism. The same argument is made by William E. Connolly, who in his work Neuropolitics shows that advances in neuroscience further illuminate some of the problematic practices of rational choice theory.
More recently Edward J. Nell and Karim Errouaki (2011, Ch. 1) argued that:The DNA of neoclassical economics is defective. Neither the induction problem nor the problems of methodological individualism can be solved within the framework of neoclassical assumptions. The neoclassical approach is to call on rational economic man to solve both. Economic relationships that reflect rational choice should be ‘projectible’. But that attributes a deductive power to ‘rational’ that it cannot have consistently with positivist (or even pragmatist) assumptions (which require deductions to be simply analytic). To make rational calculations projectible, the agents may be assumed to have idealized abilities, especially foresight; but then the induction problem is out of reach because the agents of the world do not resemble those of the model. The agents of the model can be abstract, but they cannot be endowed with powers actual agents could not have. This also undermines methodological individualism; if behaviour cannot be reliably predicted on the basis of the ‘rational choices of agents’, a social order cannot reliably follow from the choices of agents.
Furthermore, Pierre Bourdieu fiercely opposed rational choice theory as grounded in a misunderstanding of how social agents operate. Bourdieu argued that social agents do not continuously calculate according to explicit rational and economic criteria. According to Bourdieu, social agents operate according to an implicit practical logic—a practical sense—and bodily dispositions. Social agents act according to their "feel for the game" (the "feel" being, roughly, habitus, and the "game" being the field).
Other social scientists, inspired in part by Bourdieu's thinking have expressed concern about the inappropriate use of economic metaphors in other contexts, suggesting that this may have political implications. The argument they make is that by treating everything as a kind of "economy" they make a particular vision of the way an economy works seem more natural. Thus, they suggest, rational choice is as much ideological as it is scientific, which does not in and of itself negate its scientific utility.
An evolutionary psychology perspective is that many of the seeming contradictions and biases regarding rational choice can be explained as being rational in the context of maximizing biological fitness in the ancestral environment but not necessarily in the current one. Thus, when living at subsistence level where a reduction of resources may have meant death it may have been rational to place a greater value on losses than on gains. Proponents argue it may also explain differences between groups.
Benefits.
The rational choice approach allows preferences to be represented as real-valued utility functions. Economic decision making then becomes a problem of maximizing this utility function, subject to constraints (e.g. a budget). This has many advantages. It provides a compact theory that makes empirical predictions with a relatively sparse model - just a description of the agent's objectives and constraints. Furthermore, optimization theory is a well-developed field of mathematics. These two factors make rational choice models tractable compared to other approaches to choice. Most importantly, this approach is strikingly general. It has been used to analyze not only personal and household choices about
traditional economic matters like consumption and savings, but also choices about education, marriage, child-bearing, migration, crime and so on, as well as business decisions about output, investment, hiring, entry, exit, etc. with varying degrees of success.
Despite the empirical shortcomings of rational choice theory, the flexibility and tractability of rational choice models (and the lack of equally powerful alternatives) lead to them still being widely used.

</doc>
