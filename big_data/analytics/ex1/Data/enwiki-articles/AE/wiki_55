<doc id="27698" url="https://en.wikipedia.org/wiki?curid=27698" title="Sanskrit">
Sanskrit

Sanskrit (; Sanskrit: ' or ', originally "", "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a "lingua franca" in Greater India. It is a standardised dialect of Old Indo-Aryan, originating as Vedic Sanskrit and tracing its linguistic ancestry back to Proto-Indo-Iranian and Proto-Indo-European. Today it is listed as one of the 22 scheduled languages of India and is an official language of the state of Uttarakhand. As one of the oldest Indo-European languages for which substantial written documentation exists, Sanskrit holds a prominent position in Indo-European studies.
The body of Sanskrit literature encompasses a rich tradition of poetry and drama as well as scientific, technical, philosophical and religious texts. Sanskrit continues to be widely used as a ceremonial language in Hindu religious rituals and Buddhist practice in the form of hymns and chants. Spoken Sanskrit has been revived in some villages with traditional institutions, and there are attempts to enhance its popularisation.
Name.
The Sanskrit verbal adjective ' may be translated as "put together, constructed, well or completely formed; refined, adorned, highly elaborated". It is derived from the root word ' "to put together, compose, arrange, prepare"
As a term for "refined or elaborated speech" the adjective appears only in Epic and Classical Sanskrit in the "Manusmṛti" and the "Mahabharata". The language referred to as ' "the cultured language" has by definition always been a "sacred" and "sophisticated" language, used for religious and learned discourse in ancient India, in contrast to the language spoken by the people, ' "natural, artless, normal, ordinary".
Variants.
The pre-Classical form of Sanskrit is known as Vedic Sanskrit, with the language of the Rigveda being the oldest and most archaic stage preserved, dating back to as early as the early second millennium BCE. This qualifies Rigvedic Sanskrit as one of the oldest attestations of any Indo-Iranian language, and one of the earliest members of the Indo-European languages, which include English and most European languages.
"Classical Sanskrit" is the standard register as laid out in the grammar of , around the fourth century BCE. Its position in the cultures of Greater India is akin to that of Latin and Ancient Greek in Europe and it has significantly influenced most modern languages of the Indian subcontinent, particularly in India, Bangladesh, Pakistan, Sri Lanka and Nepal.
Vedic Sanskrit.
Sanskrit, as defined by , evolved out of the earlier Vedic form. The present form of Vedic Sanskrit can be traced back to as early as the second millennium BCE (for Rig-vedic). Scholars often distinguish Vedic Sanskrit and Classical or "Pāṇinian" Sanskrit as separate dialects. Though they are quite similar, they differ in a number of essential points of phonology, vocabulary, grammar and syntax. Vedic Sanskrit is the language of the Vedas, a large collection of hymns, incantations (Samhitas) and theological and religio-philosophical discussions in the Brahmanas and Upanishads. Modern linguists consider the metrical hymns of the Rigveda Samhita to be the earliest, composed by many authors over several centuries of oral tradition. The end of the Vedic period is marked by the composition of the Upanishads, which form the concluding part of the traditional Vedic corpus; however, the early Sutras are Vedic, too, both in language and content.
Classical Sanskrit.
For nearly 2000 years, Sanskrit was the language of a cultural order that exerted influence across South Asia, Inner Asia, Southeast Asia, and to a certain extent East Asia. A significant form of post-Vedic Sanskrit is found in the Sanskrit of Indian epic poetry—the "Ramayana" and "Mahabharata". The deviations from in the epics are generally considered to be on account of interference from Prakrits, or innovations, and not because they are pre-Paninian. Traditional Sanskrit scholars call such deviations "ārṣa" (आर्ष), meaning 'of the ṛṣis', the traditional title for the ancient authors. In some contexts, there are also more "prakritisms" (borrowings from common speech) than in Classical Sanskrit proper. Buddhist Hybrid Sanskrit is a literary language heavily influenced by the Middle Indo-Aryan languages, based on early Buddhist Prakrit texts which subsequently assimilated to the Classical Sanskrit standard in varying degrees.
There were four principal dialects of classical Sanskrit: ' (Northwestern, also called Northern or Western), ' (lit., middle country), ' (Eastern) and ' (Southern, arose in the Classical period). The predecessors of the first three dialects are attested in Vedic "Brāhmaṇas", of which the first one was regarded as the purest (").
Contemporary usage.
As a spoken language.
In the 2001 census of India, 14,135 people reported Sanskrit as their native language. Since the 1990s, movements to spread spoken Sanskrit have been increasing. Organisations like "Samskrita Bharati" conduct Speak Sanskrit workshops to popularise the language.
Indian newspapers have published reports about several villages, where, as a result of recent revival attempts, large parts of the population, including children, are learning Sanskrit and are even using it to some extent in everyday communication:
According to the 2011 national census of Nepal, 1,669 people use Sanskrit as their native language.
In official use.
In India, Sanskrit is among the 14 original languages of the Eighth Schedule to the Constitution. The state of Uttarakhand in India has ruled Sanskrit as its second official language. In October 2012 social activist Hemant Goswami filed a writ petition in the Punjab and Haryana High Court for declaring Sanskrit as a 'minority' language.
Contemporary literature and patronage.
More than 3000 Sanskrit works have been composed since India's independence in 1947. Much of this work has been judged of high quality, in comparison to both classical Sanskrit literature and modern literature in other Indian languages.
The Sahitya Akademi has given an award for the best creative work in Sanskrit every year since 1967. In 2009, Satya Vrat Shastri became the first Sanskrit author to win the Jnanpith Award, India's highest literary award.
In music.
Sanskrit is used extensively in the Carnatic and Hindustani branches of classical music. Kirtanas, bhajans, stotras, and shlokas of Sanskrit are popular throughout India. The samaveda uses musical notations in several of its recessions.
In Mainland China, musicians such as Sa Dingding have written pop songs in Sanskrit.
In mass media.
Over 90 weeklies, fortnightlies and quarterlies are published in Sanskrit. Sudharma, a daily newspaper in Sanskrit, has been published out of Mysore, India, since 1970, while Sanskrit Vartman Patram and Vishwasya Vrittantam started in Gujarat during the last five years.
Since 1974, there has been a short daily news broadcast on state-run All India Radio. These broadcasts are also made available on the internet on AIR's website. Sanskrit news is broadcast on TV and on the internet through the DD National channel at 6:55 AM IST.
In liturgy.
Sanskrit is the sacred language of various Hindu, Buddhist, and Jain traditions. It is used during worship in Hindu temples throughout the world. In Newar Buddhism, it is used in all monasteries, while Mahayana and Tibetan Buddhist religious texts and sutras are in Sanskrit as well as vernacular languages. Jain texts are written in Sanskrit, including the Tattvartha sutra, Ratnakaranda śrāvakācāra , the Bhaktamara Stotra and the Agamas.
It is also popular amongst the many practitioners of yoga in the West, who find the language helpful for understanding texts such as the Yoga Sutras of Patanjali.
Symbolic usage.
In Nepal, India and Indonesia, Sanskrit phrases are widely used as mottoes for various national, educational and social organisations:
Many of India's and Nepal's scientific and administrative terms are named in Sanskrit. The Indian guided missile program that was commenced in 1983 by the Defence Research and Development Organisation has named the five missiles (ballistic and others) that it developed Prithvi, Agni, Akash, Nag and the Trishul missile system. India's first modern fighter aircraft is named HAL Tejas.
Historical usage.
Origin and development.
Sanskrit is a member of the Indo-Iranian subfamily of the Indo-European family of languages. Its closest ancient relatives are the Iranian languages Avestan and Old Persian.
In order to explain the common features shared by Sanskrit and other Indo-European languages, many scholars have proposed the Indo-Aryan migration theory, asserting that the original speakers of what became Sanskrit arrived in what is now India and Pakistan from the north-west some time during the early second millennium BCE. Evidence for such a theory includes the close relationship between the Indo-Iranian tongues and the Baltic and Slavic languages, vocabulary exchange with the non-Indo-European Uralic languages, and the nature of the attested Indo-European words for flora and fauna.
The earliest attested Sanskrit texts are religious texts of the Rigveda, from the mid-to-late second millennium BCE. No written records from such an early period survive, if they ever existed. However, scholars are confident that the oral transmission of the texts is reliable: they were ceremonial literature whose correct pronunciation was considered crucial to its religious efficacy.
From the Rigveda until the time of Pāṇini (fourth century BCE) the development of the early Vedic language can be observed in other Vedic texts: the Samaveda, Yajurveda, Atharvaveda, Brahmanas, and Upanishads. During this time, the prestige of the language, its use for sacred purposes, and the importance attached to its correct enunciation all served as powerful conservative forces resisting the normal processes of linguistic change. However, there is a clear, five-level linguistic development of Vedic from the Rigveda to the language of the Upanishads and the earliest sutras such as the Baudhayana sutras.
Standardisation by Panini.
The oldest surviving Sanskrit grammar is Pāṇini's "" ("Eight-Chapter Grammar"). It is essentially a prescriptive grammar, i.e., an authority that defines Sanskrit, although it contains descriptive parts, mostly to account for some Vedic forms that had become rare in 's time. Classical Sanskrit became fixed with the grammar of Pāṇini (roughly 500 BCE), and remains in use as a learned language through the present day.
Coexistence with vernacular languages.
Sanskrit linguist Madhav Deshpande says that when the term "Sanskrit" arose it was not thought of as a specific language set apart from other languages, but rather as a particularly refined or perfected manner of speaking. Knowledge of Sanskrit was a marker of social class and educational attainment in ancient India, and the language was taught mainly to members of the higher castes through the close analysis of Vyākaraṇins such as and Patanjali, who exhorted proper Sanskrit at all times, especially during ritual. Sanskrit, as the learned language of Ancient India, thus existed alongside the vernacular Prakrits, which were Middle Indo-Aryan languages. However, linguistic change led to an eventual loss of mutual intelligibility.
Many Sanskrit dramas also indicate that the language coexisted with Prakrits, spoken by multilingual speakers with a more extensive education. Sanskrit speakers were almost always multilingual. In the medieval era, Sanskrit continued to be spoken and written, particularly by learned Brahmins for scholarly communication. This was a thin layer of Indian society, but covered a wide geography. Centres like Varanasi, Paithan, Pune and Kanchipuram had a strong presence of teaching and debating institutions, and high classical Sanskrit was maintained until British times.
Decline.
There are a number of sociolinguistic studies of spoken Sanskrit which strongly suggest that oral use of modern Sanskrit is limited, having ceased development sometime in the past.
Sheldon Pollock argues that "most observers would agree that, in some crucial way, Sanskrit is dead". Pollock has further argued that, while Sanskrit continued to be used in literary cultures in India, it was never adapted to express the changing forms of subjectivity and sociality as embodied and conceptualised in the modern age. Instead, it was reduced to "reinscription and restatements" of ideas already explored, and any creativity was restricted to hymns and verses. A notable exception are the military references of Nīlakaṇṭha Caturdhara's 17th-century commentary on the "Mahābhārata".
Hatcher argues that modern works continue to be produced in Sanskrit, while according to Hanneder,
Hanneder has also argued that modern works in Sanskrit are either ignored or their "modernity" contested.
When the British imposed a Western-style education system in India in the 19th century, knowledge of Sanskrit and ancient literature continued to flourish as the study of Sanskrit changed from a more traditional style into a form of analytical and comparative scholarship mirroring that of Europe.
Public education and popularisation.
Adult and continuing education.
Attempts at reviving the Sanskrit language have been undertaken in the Republic of India since its foundation in 1947 (it was included in the 14 original languages of the Eighth Schedule to the Constitution).
Samskrita Bharati is an organisation working for Sanskrit revival. The "All-India Sanskrit Festival" (since 2002) holds composition contests. The 1991 Indian census reported 49,736 fluent speakers of Sanskrit. Sanskrit learning programmes also feature on the lists of most AIR broadcasting centres. The Mattur village in central Karnataka claims to have native speakers of Sanskrit among its population. Inhabitants of all castes learn Sanskrit starting in childhood and converse in the language. Even the local Muslims converse in Sanskrit. Historically, the village was given by king Krishnadevaraya of the Vijayanagara Empire to Vedic scholars and their families, while people in his kingdom spoke Kannada and Telugu. Another effort concentrates on preserving and passing along the oral tradition of the Vedas, is one such organisation based out of Hyderabad that has been digitising the Vedas by recording recitations of Vedic Pandits.
School curricula.
The Central Board of Secondary Education of India (CBSE), along with several other state education boards, has made Sanskrit an alternative option to the state's own official language as a second or third language choice in the schools it governs. In such schools, learning Sanskrit is an option for grades 5 to 8 (Classes V to VIII). This is true of most schools affiliated with the Indian Certificate of Secondary Education (ICSE) board, especially in states where the official language is Hindi. Sanskrit is also taught in traditional gurukulas throughout India.
In the West.
St James Junior School in London, England, offers Sanskrit as part of the curriculum. In the United States, since September 2009, high school students have been able to receive credits as Independent Study or toward Foreign Language requirements by studying Sanskrit, as part of the "SAFL: Samskritam as a Foreign Language" program coordinated by Samskrita Bharati. In Australia, the Sydney private boys' high school Sydney Grammar School offers Sanskrit from years 7 through to 12, including for the Higher School Certificate.
Universities.
A list of Sanskrit universities is given below in chronological order of establishment:
Many universities throughout the world train and employ Sanskrit scholars, either within a separate Sanskrit department or as part of a broader focus area, such as South Asian studies or Linguistics. For example, Delhi university has about 400 Sanskrit students, about half of which are in post-graduate programmes.
European scholarship.
European scholarship in Sanskrit, begun by Heinrich Roth (1620–1668) and Johann Ernst Hanxleden (1681–1731), is considered responsible for the discovery of an Indo-European language family by Sir William Jones (1746–1794). This research played an important role in the development of Western philology, or historical linguistics.
Sir William Jones was one of the most influential philologists of his time. He told The Asiatic Society in Calcutta on 2 February 1786:
The Sanskrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and in the forms of grammar, than could have been produced by accident; so strong, indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists.
British attitudes.
Orientalist scholars of the 18th century like Sir William Jones marked a wave of enthusiasm for Indian culture and for Sanskrit. According to Thomas Trautmann, after this period of "Indomania", a certain hostility to Sanskrit and to Indian culture in general began to assert itself in early 19th century Britain, manifested by a neglect of Sanskrit in British academia. This was the beginning of a general push in favor of the idea that India should be culturally, religiously and linguistically assimilated to Britain as far as possible. Trautmann considers two separate and logically opposite sources for the growing hostility: one was "British Indophobia", which he calls essentially a developmentalist, progressivist, liberal, and non-racial-essentialist critique of Hindu civilisation as an aid for the improvement of India along European lines; the other was scientific racism, a theory of the English "common-sense view" that Indians constituted a "separate, inferior and unimprovable race".
Phonology.
Classical Sanskrit distinguishes about 36 phonemes; the presence of allophony leads the writing systems to generally distinguish 48 phones, or sounds. The sounds are traditionally listed in the order vowels ("Ac"), diphthongs ("Hal"), anusvara and visarga, plosives (Sparśa), nasals, and finally the liquids and fricatives, written in the International Alphabet of Sanskrit Transliteration (IAST) as follows:
Writing system.
Sanskrit originated in an oral society, and the oral tradition was maintained through the development of early classical Sanskrit literature. Writing was not introduced to India until after Sanskrit had evolved into the Prakrits; when it was written, the choice of writing system was influenced by the regional scripts of the scribes. Therefore, Sanskrit has no native script of its own. As such, virtually all the major writing systems of South Asia have been used for the production of Sanskrit manuscripts.
The earliest known inscriptions in Sanskrit date to the mid second century AD. They are in the Brahmi script, which was originally used for Prakrit, not Sanskrit. It has been described as a paradox that the first evidence of written Sanskrit occurs centuries later than that of the Prakrit languages which are its linguistic descendants. In northern India, there are Brāhmī inscriptions dating from the third century BCE onwards, the oldest appearing on the famous Prakrit pillar inscriptions of king Ashoka. The earliest South Indian inscriptions in Tamil Brahmi, written in early Tamil, belong to the same period. When Sanskrit was written down, it was first used for texts of an administrative, literary or scientific nature. The sacred hymns and verse were preserved orally, and were set down in writing "reluctantly" (according to one commentator), and at a comparatively late date.
Brahmi evolved into a multiplicity of Brahmic scripts, many of which were used to write Sanskrit. Roughly contemporary with the Brahmi, Kharosthi was used in the northwest of the subcontinent. Sometime between the fourth and eighth centuries, the Gupta script, derived from Brahmi, became prevalent. Around the eighth century, the Śāradā script evolved out of the Gupta script. The latter was displaced in its turn by Devanagari in the 11th or 12th century, with intermediary stages such as the Siddhaṃ script. In East India, the Bengali alphabet, and, later, the Odia alphabet, were used.
In the south, where Dravidian languages predominate, scripts used for Sanskrit include the Tamil, Kannada, Telugu, the Malayalam and Grantha alphabets.
Romanisation.
Since the late 18th century, Sanskrit has been transliterated using the Latin alphabet. The system most commonly used today is the IAST (International Alphabet of Sanskrit Transliteration), which has been the academic standard since 1888. ASCII-based transliteration schemes have also evolved because of difficulties representing Sanskrit characters in computer systems. These include Harvard-Kyoto and ITRANS, a transliteration scheme that is used widely on the Internet, especially in Usenet and in email, for considerations of speed of entry as well as rendering issues. With the wide availability of Unicode-aware web browsers, IAST has become common online. It is also possible to type using an alphanumeric keyboard and transliterate to Devanagari using software like Mac OS X's international support.
European scholars in the 19th century generally preferred Devanagari for the transcription and reproduction of whole texts and lengthy excerpts. However, references to individual words and names in texts composed in European Languages were usually represented with Roman transliteration. From the 20th century onwards, because of production costs, textual editions edited by Western scholars have mostly been in Romanised transliteration.
Grammar.
The Sanskrit grammatical tradition, Vyākaraṇa, one of the six Vedangas, began in the late Vedic period and culminated in the "Aṣṭādhyāyī" of Pāṇini, which consists of 3990 sutras (ca. fifth century BCE). About a century after Pāṇini (around 400 BCE), Kātyāyana composed "Vārtika"s on the Pāṇini sũtras. Patanjali, who lived three centuries after Pāṇini, wrote the "Mahābhāṣya", the "Great Commentary" on the "Aṣṭādhyāyī" and "Vārtika"s. Because of these three ancient Vyākaraṇins (grammarians), this grammar is called "Trimuni Vyākarana". To understand the meaning of the sutras, Jayaditya and Vāmana wrote a commentary, the "Kāsikā", in 600 CE. Pāṇinian grammar is based on 14 Shiva sutras (aphorisms), where the whole "mātrika" (alphabet) is abbreviated. This abbreviation is called the "Pratyāhara".
Sanskrit verbs are categorized into ten classes, which can be conjugated to form the present, imperfect, imperative, optative, perfect, aorist, future, and conditional moods and tenses. Before Classical Sanskrit, older forms also included a subjunctive mood. Each conjugational ending conveys person, number, and voice.
Nouns are highly inflected, including three grammatical genders, three numbers, and eight cases. Nominal compounds are common, and can include over 10 word stems.
Word order is free, though there is a strong tendency toward subject–object–verb, the original system of Vedic prose.
Influence on other languages.
Indic languages.
Sanskrit has greatly influenced the languages of India that grew from its vocabulary and grammatical base; for instance, Hindi is a "Sanskritised register" of the Khariboli dialect. All modern Indo-Aryan languages, as well as Munda and Dravidian languages, have borrowed many words either directly from Sanskrit ("tatsama" words), or indirectly via middle Indo-Aryan languages ("tadbhava" words). Words originating in Sanskrit are estimated at roughly fifty percent of the vocabulary of modern Indo-Aryan languages, as well as the literary forms of Malayalam and Kannada. Literary texts in Telugu are lexically Sanskrit or Sanskritised to an enormous extent, perhaps seventy percent or more.
Interaction with other languages.
Sanskrit has also influenced Sino-Tibetan languages through the spread of Buddhist texts in translation. Buddhism was spread to China by Mahayana missionaries sent by Ashoka, mostly through translations of Buddhist Hybrid Sanskrit. Many terms were transliterated directly and added to the Chinese vocabulary. Chinese words like 剎那 "chànà" (Devanagari: क्षण "" 'instantaneous period') were borrowed from Sanskrit. Many Sanskrit texts survive only in Tibetan collections of commentaries to the Buddhist teachings, the Tengyur.
In Southeast Asia, languages such as Thai and Lao contain many loanwords from Sanskrit, as do Khmer. For example, in Thai, Ravana, the emperor of Lanka, is called "Thosakanth", a derivation of his Sanskrit name "Dāśakaṇṭha" "having ten necks".
Many Sanskrit loanwords are also found in Austronesian languages, such as Javanese, particularly the older form in which nearly half the vocabulary is borrowed. Other Austronesian languages, such as traditional Malay and modern Indonesian, also derive much of their vocabulary from Sanskrit, albeit to a lesser extent, with a larger proportion derived from Arabic. Similarly, Philippine languages such as Tagalog have some Sanskrit loanwords, although more are derived from Spanish. A Sanskrit loanword encountered in many Southeast Asian languages is the word "bhāṣā", or spoken language, which is used to refer to the names of many languages. English also has words of Sanskrit origin.
In popular culture.
"Satyagraha", an opera by Philip Glass, uses texts from the "Bhagavad Gita", sung in Sanskrit. The closing credits of "The Matrix Revolutions" has a prayer from the "Brihadaranyaka Upanishad". The song "Cyber-raga" from Madonna's album "Music" includes Sanskrit chants, and "Shanti/Ashtangi" from her 1998 album "Ray of Light", which won a Grammy, is the ashtanga vinyasa yoga chant. The lyrics include the mantra Om shanti. Composer John Williams featured choirs singing in Sanskrit for "Indiana Jones and the Temple of Doom" and in "". The theme song of "Battlestar Galactica 2004" is the Gayatri Mantra, taken from the Rigveda. The lyrics of "The Child In Us" by Enigma also contains Sanskrit verses.. A joke in the 1994 comedy film PCU involved a minor character at an American university that was majoring in Sanskrit.

</doc>
<doc id="27699" url="https://en.wikipedia.org/wiki?curid=27699" title="Sign language">
Sign language

A sign language (also signed language) is a language which chiefly uses manual communication to convey meaning, as opposed to acoustically conveyed sound patterns. This can involve simultaneously combining hand shapes, orientation and movement of the hands, arms or body, and facial expressions to fluidly express a speaker's thoughts. Sign languages share many similarities with spoken languages (sometimes called "oral languages", which depend primarily on sound), which is why linguists consider both to be natural languages, but there are also some significant differences between signed and spoken languages. They should not be confused with body language, which is a kind of non-linguistic communication.
Wherever communities of deaf people exist, sign languages have been developed. Signing is not only used by the deaf, it is also used by people who can hear, but cannot physically speak. While they use space for grammar in a way that spoken languages do not, sign languages show the same linguistic properties and use the same language faculty as do spoken languages. Hundreds of sign languages are in use around the world and are at the cores of local deaf cultures. Some sign languages have obtained some form of legal recognition, while others have no status at all.
A common misconception is that all sign languages are the same worldwide or that sign language is international. Aside from the pidgin International Sign, each country generally has its own, native sign language, and some have more than one, though sign languages may share similarities to each other, whether in the same country or another one.
It is not clear how many sign languages there are. The 2013 edition of Ethnologue lists 137 sign languages.
History.
Groups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's "Cratylus", where Socrates says: "If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?"
Until the 19th century, most of what we know about historical sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate transfer of words from an oral language to a sign language, rather than documentation the language itself.
In 1620, Juan Pablo Bonet published (‘Reduction of letters and art for teaching mute people to speak’) in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.
In Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by deaf people. In 1648, John Bulwer described "Master Babington", a deaf man proficient in the use of a manual alphabet, "contryved on the joynts of his fingers", whose wife could converse with him easily, even in the dark through the use of tactile signing.
In 1680, George Dalgarno published "Didascalocophus, or, The deaf and dumb mans tutor", in which he presented his own method of deaf education, including an "arthrological" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.
The vowels of this alphabet have survived in the contemporary alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with "Digiti Lingua", a pamphlet by an anonymous author who was himself unable to speak.
and Hay, A. and Lee, R. "A Pictorial History of the evolution of the British Manual Alphabet" (British Deaf History Society Publications: Middlsex, 2004)</ref> He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.
Charles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described codes for both English and Latin.
By 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities (or at least in classrooms) in former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the USA.
Frenchman Charles-Michel de l'Épée published his manual alphabet in the 18th century, which has survived basically unchanged in France and North America until the present time. In 1755, Abbé de l'Épée founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.
Sign languages generally do not have any linguistic relation to the spoken languages of the lands in which they arise. The correlation between sign and spoken languages is complex and varies depending on the country more than the spoken language. For example, the US, Canada, UK, Australia and New Zealand all have English as their dominant language, but American Sign Language (ASL), used in the US and most parts of Canada, is derived from French Sign Language whereas the other three countries sign dialects of British, Australian and New Zealand Sign Language. Similarly, the sign languages of Spain and Mexico are very different, despite Spanish being the national language in each country, and the sign language used in Bolivia is based on ASL rather than any sign language that is used in a Spanish-speaking country. Variations also arise within a 'national' sign language which don't necessarily correspond to dialect differences in the national spoken language; rather, they can usually be correlated to the geographic location of residential schools for the deaf.
International Sign, formerly known as Gestuno, is used mainly at international Deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. While recent studies claim that International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.
Linguistics.
In linguistic terms, sign languages are as rich and complex as any spoken language, despite the common misconception that they are not "real languages". Professional linguists have studied many sign languages and found that they exhibit the fundamental properties that exist in all languages.
Sign languages are not mime—in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, present but suppressed in spoken languages, to be more fully expressed. This does not mean that sign languages are a visual rendition of a spoken language. They have complex grammars of their own, and can be used to discuss any topic, from the simple and concrete to the lofty and abstract.
Sign languages, like spoken languages, organize elementary, meaningless units (once called cheremes in the case of sign languages, by analogy to the phonemes of spoken languages) into meaningful semantic units. This is often called duality of patterning. As in spoken languages, these meaningless units are represented as (combinations of) features, although often also crude distinctions are made in terms of Handshape (or Handform), Orientation, Location (or Place of Articulation), Movement, and Non-manual expression. More generally, both sign and spoken languages share the characteristics that linguists have found in all natural human languages, such as transitoriness, semanticity, arbitrariness, productivity, and cultural transmission.
Common linguistic features of many sign languages are the occurrence of classifiers, a high degree of inflection by means of changes of movement, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body. Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally considered to be highly iconic, as these complex constructions "function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information". It needs to be noted that the term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms.
Today, linguists study sign languages as true languages, part of the field of linguistics. However, the category "Sign languages" was not added to the "Linguistic Bibliography / Bibliographie Linguistique" until the 1988 volume, when it appeared with 39 entries.
Relationships with spoken languages.
A common misconception is that sign languages are somehow dependent on spoken languages: that they are spoken language expressed in signs, or that they were invented by hearing people. Hearing teachers in deaf schools, such as Charles-Michel de l'Épée or Thomas Hopkins Gallaudet, are often incorrectly referred to as "inventors" of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, deaf people, who may have little or no knowledge of any spoken language.
As a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how and how much they borrow from spoken languages. In many sign languages, a manual alphabet (fingerspelling) may be used in signed communication to borrow a word from a spoken language, by spelling out the letters. This is most commonly used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.
On the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of Britain and America share the same spoken language. The grammars of sign languages do not usually resemble that of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.
Similarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.
Spatial grammar and simultaneity.
Sign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.
One way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.
The large focus on the possibility of simultaneity in sign languages in contrast to spoken languages is sometimes exaggerated, though. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only. Further, sign languages, just like spoken languages, depend on linear sequencing of signs to form sentences; the greater use of simultaneity is mostly seen in the morphology (internal structure of individual signs).
Non-manual signs.
Sign languages convey much of their prosody through non-manual signs. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions.
In ASL (American Sign Language), some signs have required facial components that distinguish them from other signs. An example of this sort of lexical distinction is the sign translated 'not yet', which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features it would be interpreted as 'late'.
Grammatical structure that is shown through non-manual signs includes questions, negation, relative clauses, boundaries between sentences, and the argument structure of some verbs. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.
Some adjectival and adverbial information is conveyed through non-manual signs, but what these signs are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means 'carelessly,' but a similar sign in BSL means 'boring' or 'unpleasant.'
Discourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.
Iconicity.
The first studies on iconicity in ASL were published in the late 1970s, and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of the language. Though they recognized that certain aspects of the language seemed iconic, they considered this to be merely extralinguistic, a property which did not influence the language. Frishberg (1975) wrote a very influential paper addressing the relationship between arbitrariness and iconicity in ASL. She concluded that though originally present in many signs, iconicity is degraded over time through the application of grammatical processes. In other words, over time, the natural processes of regularization in the language obscures any iconically motivated features of the sign.
Some researchers have suggested that the properties of ASL give it a clear advantage in terms of learning and memory. Psychologist Roger Brown was one of the first to document this benefit. In his study, Brown found that when children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than when they were taught signs that had little or no iconic properties.
The pioneers of sign language linguistics were yoked with the task of trying to prove that ASL was a real language and not merely a collection of gestures or "English on the hands." One of the prevailing beliefs at this time was that 'real languages' must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages.
The cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user's mental representation ("construal" in Cognitive Grammar). It is defined as a fully grammatical and central aspect of a sign language rather than periphery phenomena.
The cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for ASK has parts of its form that are iconic ("movement away from the mouth" means "something coming from the mouth"), and parts that are arbitrary (the handshape, and the orientation).
Many signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three way correspondence. The abstract target meaning is "learning." The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as "double mapped."
Classification.
Although sign languages have emerged naturally in deaf communities alongside or among spoken languages, they are unrelated to spoken languages and have different grammatical structures at their core.
Sign languages may be classified by how they arise.
In non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.
A village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably Martha's Vineyard Sign Language of the US, but there are also numerous village languages scattered throughout Africa, Asia, and America.
Deaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.
Both contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular oral language. Hearing people may also develop sign to communicate with speakers of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.
Language contact and creolization is common in the development of sign languages, making clear family classifications difficult – it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. One author has speculated that Adamorobe Sign Language, a village sign language of Ghana, may be related to the "gestural trade jargon used in the markets throughout West Africa", in vocabulary and areal features including prosody and phonetics.
The only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.
In his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called "stimulus diffusion". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.
Typology.
Linguistic typology (going back on Edward Sapir) is based on word structure and distinguishes morphological classes such as agglutinating/concatenating, inflectional, polysynthetic, incorporating, and isolating ones.
Sign languages vary in word-order typology as there are different word orders in different languages. For example, Austrian Sign Language, Japanese Sign Language and Indo-Pakistani Sign Language are Subject-object-verb while ASL is Subject-verb-object. Influence from the surrounding spoken languages is not improbable.
Sign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.
Brentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that via one syllable (i.e. one word, one sign) several morphemes can be expressed, like subject and object of a verb determine the direction of the verb's movement (inflection).
Another aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.
Acquisition.
Children who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language.
The acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh- question word) is learned, the non-manual aspects are attached to the word but don’t have the flexibility associated with adult use. At a certain point the non-manual features are dropped and the word is produced with no facial expression. After a few months the non-manuals reappear, this time being used the way adult signers would use them.
Written forms.
Sign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.
Several ways to represent sign languages in written form have been developed.
So far, there is no formal acceptance of any of these writing systems for any sign language, or even any consensus on the matter. None are widely used.
Sign perception.
For a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.
In society.
Deaf communities and deaf culture.
Deaf communities are very widespread in the world, and the cultures within them are very rich. Sometimes they do not even intersect with the culture of the hearing population because of the communication difficulties caused by the impediments for hard-of-hearing people to perceive aurally conveyed information.
Legal recognition.
Some sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for the disabled, but as the communication medium of language communities.
Telecommunications.
One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair – two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.
The Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with "off-the-shelf" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.
Interpretation.
In order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.
The interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) to spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) to spoken English in the U.K., and American Sign Language (ASL) to spoken English in the USA and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.
Remote interpreting.
Interpreters may be physically present with both parties to the conversation but, since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In video remote interpreting (VRI), the two clients (a sign language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.
However, VRI cannot be used for situations in which all parties are speaking via telephone alone. With video relay service (VRS), the sign language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.
Interpretation on television.
Sign language is sometimes provided for television programmes. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner. Typically for press conferences such as those given by the Mayor of New York City, the signer appears to stage left or right of the public official to allow both the speaker and signer to be in frame at the same time.
Paddy Ladd initiated deaf programming on British television in the 1980s and is credited with getting sign language on television and enabling deaf children to be educated in sign.
In traditional analogue broadcasting, many programmes are repeated, often in the early hours of the morning, with the signer present rather than have them appear at the main broadcast time. This is due to the distraction they cause to those not wishing to see the signer. On the BBC, many programmes that broadcast late at night or early in the morning are signed. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.
Legal requirements covering sign language on television vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.
Use of sign languages in hearing communities.
On occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community. Famous examples of this include Martha's Vineyard Sign Language in the USA, Kata Kolok in a village in Bali, Adamorobe Sign Language in Ghana and Yucatec Maya sign language in Mexico. In such communities deaf people are not socially disadvantaged.
Many Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.
A pidgin sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language). It was used to communicate among tribes with different spoken languages. There are especially users today among the Crow, Cheyenne, and Arapaho. Unlike other sign languages developed by hearing people, it shares the spatial grammar of deaf sign languages.
In the 1500s, a Spanish expeditionary, Cabeza de Vaca, observed natives in the western part of modern-day Florida using sign language, and in the mid-16th century Coronado mentioned that communication with the Tonkawa using signs was possible without a translator. Whether or not these gesture systems reached the stage at which they could properly be called languages is still up for debate. There are estimates indicating that as many as 2% of Native Americans are seriously or completely deaf, a rate more than twice the national average.
Signs may also be used for manual communication in noisy or secret situations, such as hunting.
"Baby sign language" with hearing children.
It has become popular for hearing parents to teach signs (from ASL or some other sign language) to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs can be a beneficial option for better communication. Babies can usually produce signs before they can speak. This decreases the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.
This is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.
Home sign.
Sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, an informal system of signs will naturally develop, unless repressed by the parents. The term for these mini-languages is home sign (sometimes homesign or kitchen sign).
Home sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signals to facilitate the meeting of his or her communication needs. Although this kind of system is grossly inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language, it is a common occurrence. No type of Home Sign is recognized as an official language.
Gestural theory of human language origins.
The gestural theory states that vocal human language developed from a gestural sign language. An important question for gestural theory is what caused the shift to vocalization.
Sign languages and language endangerment.
As with any spoken language, sign languages are also vulnerable to becoming endangered. For example, a sign language used by a small community may be endangered and even abandoned as users shift to a sign language used by a larger community. One example of an endangered sign language is Hawai'i Sign Language, which is almost extinct except for a few elderly signers. Methods are being developed to assess the language vitality of sign languages.
Primate use.
There have been several notable examples of scientists teaching non-human primates basic signs in order to communicate with humans, but the degree to which these basic signs relate to human sign language and the ability of the animals in question to actually communicate is a matter of substantial controversy and dispute. Notable examples include:
External links.
"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages."

</doc>
<doc id="27701" url="https://en.wikipedia.org/wiki?curid=27701" title="String (computer science)">
String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally understood as a data type and is often implemented as an array of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. A string may also denote more general arrays or other sequence (or list) data types and structures.
Depending on programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.
In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.
Formal theory.
Let Σ be a non-empty finite set of symbols (alternatively called characters), called the "alphabet". No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then "01011" is a string over Σ.
The "length" of a string "s" is the number of symbols in "s" (the length of the sequence) and can be any non-negative integer; it is often denoted as |"s"|. The "empty string" is the unique string over Σ of length 0, and is denoted "ε" or "λ".
The set of all strings over Σ of length "n" is denoted Σ"n". For example, if Σ = {0, 1}, then Σ2 = {00, 01, 10, 11}. Note that Σ0 = {ε} for any alphabet Σ.
The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ*. In terms of Σ"n",
For example, if Σ = {0, 1}, then Σ* = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}. Although the set Σ* itself is countably infinite, each element of Σ* is a string of finite length.
A set of strings over Σ (i.e. any subset of Σ*) is called a "formal language" over Σ. For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.
Concatenation and substrings.
"Concatenation" is an important binary operation on Σ*. For any two strings "s" and "t" in Σ*, their concatenation is defined as the sequence of symbols in "s" followed by the sequence of characters in "t", and is denoted "st". For example, if Σ = {a, b, ..., z}, "s" = bear, and "t" = hug, then "st" = bearhug and "ts" = hugbear.
String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string "s", ε"s" = "s"ε = "s". Therefore, the set Σ* and the concatenation operation form a monoid, the free monoid generated by Σ. In addition, the length function defines a monoid homomorphism from Σ* to the non-negative integers (that is, a function formula_2, such that formula_3).
A string "s" is said to be a "substring" or "factor" of "t" if there exist (possibly empty) strings "u" and "v" such that "t" = "usv". The relation "is a substring of" defines a partial order on Σ*, the least element of which is the empty string.
Prefixes and suffixes.
A string "s" is said to be a prefix of "t" if there exists a string "u" such that "t" = "su". If "u" is nonempty, "s" is said to be a "proper" prefix of "t". Symmetrically, a string "s" is said to be a suffix of "t" if there exists a string "u" such that "t" = "us". If "u" is nonempty, "s" is said to be a "proper" suffix of "t". Suffixes and prefixes are substrings of "t". Both the relations "is a prefix of" and "is a suffix of" are prefix orders.
Rotations.
A string "s" = "uv" is said to be a rotation of "t" if "t" = "vu". For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01.
Reversal.
The reverse of a string is a string with the same symbols but in reverse order. For example, if "s" = abc (where a, b, and c are symbols of the alphabet), then the reverse of "s" is cba. A string that is the reverse of itself (e.g., "s" = madam) is called a palindrome, which also includes the empty string and all strings of length 1.
Lexicographical ordering.
It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ* called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ* includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.
See Shortlex for an alternative string ordering that preserves well-foundedness.
String operations.
A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.
Topology.
Strings admit the following interpretation as nodes on a graph:
The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the "p"-adic numbers and some constructions of the Cantor set, and yields the same topology.
Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.
String datatypes.
A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a "literal" or "string literal".
String length.
Although formal strings can have an arbitrary (but finite) length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: "fixed-length strings", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and "variable-length strings", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time. Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – by the number of bits available to a pointer, and by the size of available computer memory. The string length can be stored as a separate integer (which may put an artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero. See also "Null-terminated" below.
Character encoding.
String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC.
Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not "self-synchronizing", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string (these problems were much less with EUC as any ASCII character did synchronize the encoding).
Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the "characters", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make "code points" fixed-sized, but these are not "characters" due to composing codes).
Implementations.
Some languages like C++ implement strings as templates that can be used with any datatype, but this is the exception, not the rule.
Some languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed "mutable" strings. In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed "immutable" strings.
Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.
Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.
Representations.
Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.
The term "byte string" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.
Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.
Null-terminated.
The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an "n"-character string takes "n" + 1 space (1 for the terminator), and is thus an implicit data structure.
In terminated strings, the terminating code is not an allowable character in any string. Strings with "length" field do not have this limitation and can also store arbitrary binary data.
An example of a "null-terminated string" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:
The length of the string in the above example, "codice_1", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called "ASCIZ strings", after the original assembly language directive used to declare them.)
Using a special byte for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. '$' was used by many assembler systems, ':' used by CDC systems (this character had a value of zero), and the ZX80 used '"' since this was the string delimiter in its BASIC language.
Somewhat similar, "data processing" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.
Length-prefixed.
The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value; a convention used in many Pascal dialects, as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the "length" field covers the address space, strings are limited only by the available memory.
If the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking "n" + "k" space, where "k" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
If the length is not bounded, encoding a length "n" takes log("n") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length "n" in log("n") + "n" space.
In the latter case, the length-prefix field itself doesn't have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.
Here is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:
Strings as records.
Many languages, including object-oriented ones, implement strings as records in a structure like:
Although this implementation is hidden, and accessed through member functions. The "text" will be a dynamically allocated memory area, that might be expanded if needed. See also string (C++).
Other representations.
Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.
Both of these limitations can be overcome by clever programming.
It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.
While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.
The core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.
While that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure—a gap buffer, a linked list of lines, a piece table, or a rope—which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.
Security concerns.
The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.
String data is frequently obtained from user-input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user-input can cause a program to be vulnerable to code injection attacks.
Text file strings.
In computer readable text files, for example programming language source files or configuration files, strings can be represented. The NUL byte is normally not used as terminator since that does not correspond to the ASCII text standard, and the length is usually not stored, since the file should be human editable without bugs.
Two common representations are:
Non-text strings.
While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.
C programmers draw a sharp distinction between a "string", aka a "string of characters", which by definition is always null terminated, vs. a "byte string" or "pseudo string" which may be stored in the same array but is often not null terminated.
Using C string handling functions on such a "byte string" often seems to work, but later leads to security problems.
String processing algorithms.
There are many algorithms for processing strings, each with various trade-offs. Some categories of algorithms include:
Advanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite state machines.
The name "stringology" was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.
Character string-oriented languages and utilities.
Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:
Many Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.
Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.
Recent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.
Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.
Character string functions.
String functions are used to manipulate a string or change or edit the contents of a string. They also are used to query information about a string. They are usually used within the context of a computer programming language.
The most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_2 or codice_3. For example, codice_4 would return 11.
String buffers.
In some programming languages, a string buffer is an alternative to a string. It has the ability to be altered through adding or appending, whereas a String is normally fixed or immutable.
In Java.
Theory.
Java's standard way to handle text is to use its codice_5 class. Any given codice_6 in Java is an immutable object, which means its state cannot be changed. A codice_6 has an array of characters. Whenever a codice_6 must be manipulated, any changes require the creation of a new codice_6 (which, in turn, involves the creation of a new array of characters, and copying of the original array). This happens even if the original codice_6's value or intermediate codice_6s used for the manipulation are not kept.
Java provides an alternate class for string manipulation, called . A codice_12, like a codice_6, has an array to hold characters. It, however, is mutable (its state can be altered). Its array of characters is not necessarily completely filled (as opposed to a String, whose array is always the exact required length for its contents). Thus, it has the capability to add, remove, or change its state without creating a new object (and without the creation of a new array, and array copying). The exception to this is when its array is no longer of suitable length to hold its content. In this case, it is required to create a new array, and copy the contents.
For these reasons, Java would handle an expression like
like this:
Implications.
Generally, a codice_12 is more efficient than a String in string handling. However, this is not necessarily the case, since a StringBuffer will be required to recreate its character array when it runs out of space. Theoretically, this is possible to happen the same number of times as a new String would be required, although this is unlikely (and the programmer can provide length hints to prevent this). Either way, the effect is not noticeable in modern desktop computers.
As well, the shortcomings of arrays are inherent in a codice_12. In order to insert or remove characters at arbitrary positions, whole sections of arrays must be moved.
The method by which a codice_12 is attractive in an environment with low processing power takes this ability by using too much memory, which is likely also at a premium in this environment. This point, however, is trivial, considering the space required for creating many instances of Strings in order to process them. As well, the StringBuffer can be optimized to "waste" as little memory as possible.
The class, introduced in J2SE 5.0, differs from codice_12 in that it is unsynchronized. When only a single thread at a time will access the object, using a codice_18 processes more efficiently than using a codice_12.
codice_12 and codice_18 are included in the package.
In .NET.
Microsoft's .NET Framework has a codice_18 class in its Base Class Library.

</doc>
<doc id="27706" url="https://en.wikipedia.org/wiki?curid=27706" title="Satanism">
Satanism

Satanism is a group of ideological and philosophical beliefs based on the character of Satan. Although the public practice of Satanism began with the founding of the Church of Satan in 1966, historical precedents exist: a group called the Ophite Cultus Satanas was founded in Ohio by Herbert Arthur Sloane in 1948.
Satanist groups that appeared after the 1960s are widely diverse, but two major trends are theistic Satanism and atheistic Satanism. Theistic Satanists venerate Satan as a supernatural deity, viewing him not as omnipotent but rather as a patriarch. In contrast, atheistic Satanists regard Satan as merely a symbol of certain human traits.
There are signs that Satanistic beliefs have become more socially tolerated. Satanism is now allowed in the Royal Navy of the British Armed Forces, despite opposition from Christians, and in 2005, the Supreme Court of the United States debated over protecting the religious rights of prison inmates after a lawsuit challenging the issue was filed to them.
Contemporary Satanism is mainly an American phenomenon, the ideas spreading with the effects of globalization and the Internet. The Internet promotes awareness of other Satanists, and is also the main battleground for the definitions of Satanism today. Satanism started to reach Eastern Europe in the 1990s, in time with the fall of the Soviet Union, and most noticeably in Poland and Lithuania, predominantly Roman Catholic countries. It was estimated that there were 50,000 Satanists in 1990. There may now be as many as 100,000 Satanists in the world.
Definition.
In their study of Satanism, the religious studies scholars Asbjorn Dyrendal, James R. Lewis, and Jesper Aa. Petersen stated that the term "Satanism" "has a history of being a designation made by people against those whom they dislike; it is a term used for "othering"."
Elsewhere, Petersen noted that "Satanism as something others do is very different from Satanism as a self-designation".
Eugene Gallagher noted that as commonly used, "Satanism" was usually "a polemical, not a descriptive term".
Accusations of Satanism.
Historically, some people or groups have been specifically described as worshiping Satan or the Devil, or of being devoted to the work of Satan. The widespread preponderance of these groups in European cultures is in part connected with the importance and meaning of Satan within Christianity.
Islam.
The Yazidis, a minority religion of the Middle East who worship Melek Taus, are often referred to as Satan worshippers by some Muslims. Due to this, they have been targeted for conversion and extermination by the Islamic State of Iraq and the Levant.
Satanic Ritual Abuse hysteria.
In his history of Satanism, Gareth Medway labelled the Satanic Ritual Abuse hysteria "a rerun of the old witch-hunts".
Artistic Satanism.
Literary Satanism.
European Enlightenment, some works, such as "Paradise Lost", were taken up by Romantics like Byron and described as presenting the biblical figure of Satan as an allegory representing a crisis of faith, individualism, free will, wisdom and enlightenment. Those works actually featuring Satan as a heroic character are fewer in number but do exist. George Bernard Shaw and Mark Twain (cf. "Letters from the Earth") included such characterizations in their works long before religious Satanists took up the pen. From then on, Satan and Satanism started to gain a new meaning outside of Christianity.
Popular music.
Black metal has often been connected with Satanism, in part for the lyrical content of several bands and their frequent use of imagery often tied to left hand path beliefs (such as the inverted pentagram). More often than not musicians associating themselves with black metal say they do not believe in legitimate Satanic ideology and often profess to being atheists, agnostics, or religious skeptics. In some instances, followers of right hand path religions use Satanic references for entertainment purposes and shock value. Most of black metal's "first wave" bands only used Satanism for shock value; one of the few exceptions is Mercyful Fate singer King Diamond, who follows LaVeyan Satanism and whom Michael Moynihan calls "one of the only performers of the '80s Satanic Metal who was more than just a poseur using a devilish image for shock value". One early precursor to Satanic metal was the 1969 rock album "Witchcraft Destroys Minds & Reaps Souls", which contained numerous references to Satanism that reappeared in later Satanic rock music.
Glen Benton, vocalist and bassist of the band Deicide, once openly claimed to be a practitioner of theistic Satanism, and has spoken publicly to profess staunch anti-Christian sentiment. The controversial Dissection frontman Jon Nödtveidt openly spoke about his "chaos-gnostic" satanic beliefs, being a member of the Misanthropic Luciferian Order, and called his band "the sonic propaganda unit of the MLO". Norwegian black metal artists such as Euronymous from Mayhem and Infernus from Gorgoroth have also identified themselves as Satanists and actively promoted their beliefs. Numerous church burnings that covered parts of Norway in the early 1990s were also attributed to youths involved in the black metal movement, which included people promoting theistic Satanic beliefs and strong anti-LaVeyan attitudes. However, the legitimacy of such actions as Satanic endeavors, rather than simply rebellious actions done for publicity, is something that has been doubted by even some of those who contribute to the genre.
Religious Satanism.
Rather than being one single form of religious Satanism, there are instead multiple different religious Satanisms, each with different ideas about what being a Satanist entails. Dyrendal, Lewis, and Petersen believed that it was not a single movement, but rather a milieu. They believed that there was a family resemblance that united all of the varying groups in this milieu, and that most of them were self religions. They argued that there were a set of features that were common to the groups in this Satanic milieu: these were the positive use of the term "Satanist" as a designation, an emphasis on individualism, a genealogy that connects them to other Satanic groups, a transgressive and antinomian stance, a self-perception as an elite, and an embrace of values such as pride, self-reliance, and productive non-conformity.
Dyrendal, Lewis, and Petersen argued that the groups within the Satanic milieu could be divided into three groups: reactive Satanists, rationalist Satanists, and esoteric Satanists. They saw reactive Satanism as encompassing "popular Satanism, inverted Christianity, and symbolic rebellion" and noted that it situates itself in opposition to society while at the same time conforming to society's perspective of evil. Rationalist Satanism is used to describe the trend in the Satanic milieu which is atheistic, sceptical, materialistic, and epicurean. Esoteric Satanism instead applied to those forms which are theistic and draw upon ideas from other forms of Western esotericism, Modern Paganism, Buddhism, and Hinduism.
Early forms.
Satanic rhetoric and elements featured in the Third Term of the Trinity, an esoteric group founded in Paris, France in 1935 by the Russian occultist Maria de Naglowska.
Palladists.
Palladists are an alleged theistic Satanist society or member of that society. The name Palladian comes from Pallas and refers to the Greco-Roman goddess of wisdom and learning.
Our Lady of Endor Coven.
Our Lady of Endor Coven, also known as Ophite Cultus Satanas (originally spelled "Sathanas"), was a satanic cult founded in 1948 by Herbert Arthur Sloane in Toledo, Ohio. The group was heavily influenced by gnosticism (especially that found in the contemporary book by Hans Jonas, "The Gnostic Religion"), and worshiped Satanas, their name for Satan ("Cultus Satanas" is a Latin version of Cult of Satan). Satanas (or Satan) was defined in gnostic terms as the Serpent in the Garden of Eden who revealed the knowledge of the true God to Eve. That it called itself "Ophite" is a reference to the ancient gnostic sect of the Ophites, who were said to worship the serpent.
Theistic Satanism.
Theistic Satanism (also known as traditional Satanism, Spiritual Satanism or Devil worship) is a form of Satanism with the primary belief that Satan is an actual deity or force to revere or worship. Other characteristics of theistic Satanism may include a belief in magic, which is manipulated through ritual, although that is not a defining criterion, and theistic Satanists may focus solely on devotion.
LaVeyan Satanism.
LaVeyan Satanism was founded in 1966 by Anton LaVey through the establishment of the Church of Satan. Its central text, "The Satanic Bible", was published in 1969. The fundamentals of the religion's creed are synthesized in "The Nine Satanic Statements", "The Nine Satanic Sins", and "The Eleven Satanic Rules of the Earth". Contrary to popular belief, LaVeyan Satanism does not involve the worship of deities. It is an atheistic philosophy that asserts the individual as his or her own god. Adherents instead see the character of Satan as an archetype of pride, carnality and enlightenment. Adherents to the philosophy have described Satanism as a non-spiritual religion of the flesh, or "...the world's first religion".
Luciferianism.
Luciferianism can be understood best as a belief system or intellectual creed that venerates the essential and inherent characteristics that are affixed and commonly given to Lucifer. Luciferianism is often identified as an auxiliary creed or movement of Satanism, due to the common identification of Lucifer with Satan. Some Luciferians accept this identification and/or consider Lucifer as the "light bearer" and illuminated aspect of Satan, giving them the name of Satanists and the right to bear the title. Others reject it, giving the argument that Lucifer is a more positive and easy-going ideal than Satan. They are inspired by the ancient myths of Egypt, Rome and Greece, Gnosticism and traditional Western occultism.
The Church of Satan.
The Church of Satan was established at the Black House in San Francisco, California, on Walpurgisnacht, April 30, 1966, by Anton Szandor LaVey, who was the church's High Priest until his death in 1997. In 2001, Peter H. Gilmore was appointed to the position of high priest, and the church's headquarters were moved to Hell's Kitchen, Manhattan, New York City. The Church is dedicated to the religion of LaVeyan Satanism as codified in "The Satanic Bible". The church rejects the legitimacy of any other organizations who claim to be Satanists.
First Satanic Church.
After LaVey's death in 1997, the Church of Satan was taken over by a new administration and its headquarters was moved to New York. LaVey's daughter, the High Priestess Karla LaVey, felt this to be a disservice to her father's legacy. The First Satanic Church was re-founded on October 31, 1999 by Karla LaVey to carry on the legacy of her father. She continues to run it out of San Francisco, California.
Temple of Set.
The Temple of Set is an initiatory occult society claiming to be the world's leading left-hand path religious organization. It was established in 1975 by Michael A. Aquino and certain members of the priesthood of the Church of Satan, who left because of administrative and philosophical disagreements. ToS deliberately self-differentiates from CoS in several ways, most significantly in theology and sociology. The philosophy of the Temple of Set may be summed up as "enlightened individualism" — enhancement and improvement of oneself by personal education, experiment and initiation. This process is necessarily different and distinctive for each individual. The members do not agree on whether Set is "real" or not, and they're not expected to.
Setianism, in theory, is similar to theistic Satanism. The principle deity of Setianism is the ancient Egyptian god Set, or Seth, the god of adversary. Set supposedly is the Dark Lord behind the Hebrew entity Satan. Set, as the first principle of consciousness, is emulated by Setians, who symbolize the concept of individual, subjective intelligence distinct from the natural order as the "Black Flame". (Some people who are not members of the Temple of Set find spiritual inspiration in the Egyptian god Set, and may share some beliefs with the organization. The belief system in general is referred to as Setianism.)
Members of the Temple of Set are mostly male, between the ages of twenty and fifty.
Order of Nine Angles.
The authors Per Faxneld and Jesper Petersen write that the Order of Nine Angles (ONA, O9A) "represent a dangerous and extreme form of Satanism". The ONA first attracted public attention during the 1980s and 1990s after being mentioned in books detailing fascist Satanism. They were initially formed in the United Kingdom and are presently organized around clandestine cells (which it calls "traditional nexions") and around what it calls "sinister tribes".
The Satanic Temple.
The Satanic Temple is an American political activist organization based in New York. The organization actively participates in public affairs that have manifested in several public political actions and efforts at lobbying, with a focus on the separation of church and state and using satire against Christian groups that it believes interfere with personal freedom.
The Satanic Temple does not believe in a supernatural Satan, as they believe that this encourages superstition that will keep them from being "malleable to the best current scientific understandings of the material world". The Temple uses the literary Satan as metaphor to construct a cultural narrative which promotes pragmatic skepticism, rational reciprocity, personal autonomy, and curiosity. Satan is thus used as a symbol representing "the eternal rebel" against arbitrary authority and social norms.

</doc>
<doc id="27707" url="https://en.wikipedia.org/wiki?curid=27707" title="Socialist law">
Socialist law

Socialist law or Soviet law denotes a general type of legal system which has been used in communist and formerly communist states. It is based on the civil law system, with major modifications and additions from Marxist-Leninist ideology. There is controversy as to whether socialist law ever constituted a separate legal system or not. If so, prior to the end of the Cold War, "socialist law" would be ranked among the major legal systems of the world.
While civil law systems have traditionally put great pains in defining the notion of private property, how it may be acquired, transferred, or lost, socialist law systems provide for most property to be owned by the state or by agricultural co-operatives, and having special courts and laws for state enterprises.
Many scholars argue that socialist law was not a separate legal classification. Although the command economy approach of the communist states meant that most types of property could not be owned, the Soviet Union always had a civil code, courts that interpreted this civil code, and a civil law approach to legal reasoning (thus, both legal process and legal reasoning were largely analogous to the French or German civil code system). Legal systems in all socialist states preserved formal criteria of the Romano-Germanic civil law; for this reason, law theorists in post-socialist states usually consider the Socialist law as a particular case of the Romano-Germanic civil law. Cases of development of common law into Socialist law are unknown because of incompatibility of basic principles of these two systems (common law presumes influential rule-making role of courts while courts in socialist states play a dependent role).
Soviet legal theory.
Soviet law displayed many special characteristics that derived from the socialist nature of the Soviet state and reflected Marxist-Leninist ideology. Vladimir Lenin accepted the Marxist conception of the law and the state as instruments of coercion in the hands of the bourgeoisie and postulated the creation of popular, informal tribunals to administer revolutionary justice. One of the main theoreticians of Soviet socialist legality in this early phase was Pēteris Stučka.
Alongside this utopian trend was one more critical of the concept of "proletarian justice", represented by Evgeny Pashukanis. A dictatorial trend developed that advocated the use of law and legal institutions to suppress all opposition to the regime. This trend reached its zenith under Joseph Stalin with the ascendancy of Andrey Vyshinsky, when the administration of justice was carried out mainly by the security police in special tribunals.
During the de-Stalinization of the Nikita Khrushchev era, a new trend developed, based on socialist legality, that stressed the need to protect the procedural and statutory rights of citizens, while still calling for obedience to the state. New legal codes, introduced in 1960, were part of the effort to establish legal norms in administering laws. Although socialist legality remained in force after 1960, the dictatorial and utopian trends continued to influence the legal process. Persecution of political and religious dissenters continued, but at the same time there was a tendency to decriminalize lesser offenses by handing them over to people's courts and administrative agencies and dealing with them by education rather than by incarceration.
By late 1986, the Mikhail Gorbachev era was stressing anew the importance of individual rights in relation to the state and criticizing those who violated procedural law in implementing Soviet justice. This signaled a resurgence of socialist legality as the dominant trend. It should be noted, however, that socialist legality itself still lacked features associated with Western jurisprudence.
Characteristic traits.
Socialist law is similar to common law or civil law but with a greatly increased public law sector and decreased private law sector.
A specific institution characteristic to Socialist law was the so-called burlaw court (or, verbally, "court of comrades", Russian товарищеский суд) which decided on minor offences.
Chinese Socialist law.
Among the remaining communist governments, some (most notably the People's Republic of China) have added extensive modifications to their legal systems. In general, this is a result of their market-oriented economic changes. However, some communist influence can still be seen. For example, in Chinese real estate law there is no unified concept of real property; the state owns all land but often not the structures that sit on that land. A rather complex "ad hoc" system of use rights to land property has developed, and these use rights are the things being officially traded (rather than the property itself). In some cases (for example in the case of urban residential property), the system results in something that resembles real property transactions in other legal systems.
In other cases, the Chinese system results in something quite different. For example, it is a common misconception that reforms under Deng Xiaoping resulted in the privatization of agricultural land and a creation of a land tenure system similar to those found in Western countries. In actuality, the village committee owns the land and contracts the right to use this land to individual farmers who may use the land to make money from agriculture. Hence the rights that are normally unified in Western economies are split up between the individual farmer and the village committee.
This has a number of consequences. One of them is that, because the farmer does not have an absolute right to transfer the land, he cannot borrow against his use rights. On the other hand, there is some insurance against risk in the system, in that the farmer can return his land to the village committee if he wants to stop farming and start some other sort of business. Then, if this business does not work, he can get a new contract with the village committee and return to farming. The fact that the land is redistributable by the village committee also ensures that no one is left landless; this creates a form of social welfare.
There have been a number of proposals to reform this system and they have tended to be in the direction of fully privatizing rural land for the alleged purpose of increasing efficiency. These proposals have usually not received any significant support, largely because of the popularity of the current system among the farmers themselves. There is little risk that the village committee will attempt to impose a bad contract on the farmers, since this would reduce the amount of money the village committee receives. At the same time, the farmer has some flexibility to decide to leave farming for other ventures and to return at a later time.

</doc>
<doc id="27709" url="https://en.wikipedia.org/wiki?curid=27709" title="Semiconductor">
Semiconductor

codice_1
Semiconductors are crystalline or amorphous solids with distinct electrical characteristics. They are of high resistance - higher than typical resistance materials, but still of much lower resistance than insulators. Their resistance decreases as their temperature increases, which is behavior opposite to that of a metal. Finally, their conducting properties may be altered in useful ways by the deliberate introduction of impurities ("doping") into the crystal structure, which lowers its resistance but also permits the creation of semiconductor junctions between differently-doped regions of the extrinsic semiconductor crystal. The behavior of charge carriers at these junctions is the basis of diodes, transistors and all modern electronics.
Semiconductor devices can display a range of useful properties such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by controlled addition of impurities, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.
The modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of electrons and holes (collectively known as "charge carriers") in a crystal lattice. Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called "p-type", and when it contains mostly free electrons it is known as "n-type". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behavior.
Although some pure elements and many compounds display semiconductor properties, silicon, germanium, and compounds of gallium are the most widely used in electronic devices. Elements near the so-called "metalloid staircase", where the metalloids are located on the periodic table, are usually used as semiconductors.
Some of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. The first practical application of semiconductors in electronics was the 1904 development of the Cat's-whisker detector, a primitive semiconductor diode widely used in early radio receivers. Developments in quantum physics in turn allowed the development of the transistor in 1947 and the integrated circuit in 1958.
Materials.
A large number of elements and compounds have semiconducting properties, including: 
Most common semiconducting materials are crystalline solids, but amorphous and liquid semiconductors are also known. These include hydrogenated amorphous silicon and mixtures of arsenic, selenium and tellurium in a variety of proportions. These compounds share with better known semiconductors the properties of intermediate conductivity and a rapid variation of conductivity with temperature, as well as occasional negative resistance. Such disordered materials lack the rigid crystalline structure of conventional semiconductors such as silicon. They are generally used in thin film structures, which do not require material of higher electronic quality, being relatively insensitive to impurities and radiation damage.
Preparation of semiconductor materials.
Almost all of today’s technology involves the use of semiconductors, with the most important aspect being the integrated circuit (IC). Some examples of devices that contain integrated circuits includes laptops, scanners, cell-phones, etc. Semiconductors for ICs are mass-produced. To create an ideal semiconducting material, chemical purity is paramount. Any small imperfection can have a drastic effect on how the semiconducting material behaves due to the scale at which the materials are used.
A high degree of crystalline perfection is also required, since faults in crystal structure (such as dislocations, twins, and stacking faults) interfere with the semiconducting properties of the material. Crystalline faults are a major cause of defective semiconductor devices. The larger the crystal, the more difficult it is to achieve the necessary perfection. Current mass production processes use crystal ingots between 100 and 300 mm (4 and 12 in) in diameter which are grown as cylinders and sliced into wafers.
There is a combination of processes that is used to prepare semiconducting materials for ICs. One process is called thermal oxidation, which forms silicon dioxide on the surface of the silicon. This is used as a gate insulator and field oxide. Other processes are called photomasks and photolithography. This process is what creates the patterns on the circuity in the integrated circuit. Ultraviolet light is used along with a photoresist layer to create a chemical change that generates the patterns for the circuit.
Etching is the next process that is required. The part of the silicon that was not covered by the photoresist layer from the previous step can now be etched. The main process typically used today is called plasma etching. Plasma etching usually involves an etch gas pumped in a low-pressure chamber to create plasma. A common etch gas is chlorofluorocarbon, or more commonly known Freon. A high radio-frequency voltage between the cathode and anode is what creates the plasma in the chamber. The silicon wafer is located on the cathode, which causes it to be hit by the positively charged ions that are released from the plasma. The end result is silicon that is etched anisotropically.
The last process is called diffusion. This is the process that gives the semiconducting material its desired semiconducting properties. It is also known as doping. The process introduces an impure atom to the system, which creates the p-n junction. In order to get the impure atoms embedded in the silicon wafer, the wafer is first put in a 1100 degree Celsius chamber. The atoms are injected in and eventually diffuse with the silicon. After the process is completed and the silicon has reached room temperature, the doping process is done and the semiconducting material is ready to be used in an integrated circuit.
Physics of semiconductors.
Energy bands and electrical conduction.
Semiconductors are defined by their unique electric conductive behavior, somewhere between that of a metal and an insulator.
The differences between these materials can be understood in terms of the quantum states for electrons, each of which may contain zero or one electron (by the Pauli exclusion principle). These states are associated with the electronic band structure of the material.
Electrical conductivity arises due to the presence of electrons in states that are delocalized (extending through the material), however in order to transport electrons a state must be "partially filled", containing an electron only part of the time. If the state is always occupied with an electron, then it is inert, blocking the passage of other electrons via that state.
The energies of these quantum states are critical, since a state is partially filled only if its energy is near the Fermi level (see Fermi–Dirac statistics).
High conductivity in a material comes from it having many partially filled states and much state delocalization.
Metals are good electrical conductors and have many partially filled states with energies near their Fermi level.
Insulators, by contrast, have few partially filled states, their Fermi levels sit within band gaps with few energy states to occupy.
Importantly, an insulator can be made to conduct by increasing its temperature: heating provides energy to promote some electrons across the band gap, inducing partially filled states in both the band of states beneath the band gap (valence band) and the band of states above the band gap (conduction band).
An (intrinsic) semiconductor has a band gap that is smaller than that of an insulator and at room temperature significant numbers of electrons can be excited to cross the band gap.
A pure semiconductor, however, is not very useful, as it is neither a very good insulator nor a very good conductor.
However, one important feature of semiconductors (and some insulators, known as "semi-insulators") is that their conductivity can be increased and controlled by doping with impurities and gating with electric fields. Doping and gating move either the conduction or valence band much closer to the Fermi level, and greatly increase the number of partially filled states.
Some wider-band gap semiconductor materials are sometimes referred to as semi-insulators. When undoped, these have electrical conductivity nearer to that of electrical insulators, however they can be doped (making them as useful as semiconductors). Semi-insulators find niche applications in micro-electronics, such as substrates for HEMT. An example of a common semi-insulator is gallium arsenide. Some materials, such as titanium dioxide, can even be used as insulating materials for some applications, while being treated as wide-gap semiconductors for other applications.
Charge carriers (electrons and holes).
The partial filling of the states at the bottom of the conduction band can be understood as adding electrons to that band.
The electrons do not stay indefinitely (due to the natural thermal recombination) but they can move around for some time.
The actual concentration of electrons is typically very dilute, and so (unlike in metals) it is possible to think of the electrons in the conduction band of a semiconductor as a sort of classical ideal gas, where the electrons fly around freely without being subject to the Pauli exclusion principle. In most semiconductors the conduction bands have a parabolic dispersion relation, and so these electrons respond to forces (electric field, magnetic field, etc.) much like they would in a vacuum, though with a different effective mass.
Because the electrons behave like an ideal gas, one may also think about conduction in very simplistic terms such as the Drude model, and introduce concepts such as electron mobility.
For partial filling at the top of the valence band, it is helpful to introduce the concept of an electron hole.
Although the electrons in the valence band are always moving around, a completely full valence band is inert, not conducting any current.
If an electron is taken out of the valence band, then the trajectory that the electron would normally have taken is now missing its charge.
For the purposes of electric current, this combination of the full valence band, minus the electron, can be converted into a picture of a completely empty band containing a positively charged particle that moves in the same way as the electron.
Combined with the "negative" effective mass of the electrons at the top of the valence band, we arrive at a picture of a positively charged particle that responds to electric and magnetic fields just as a normal positively charged particle would do in vacuum, again with some positive effective mass.
This particle is called a hole, and the collection of holes in the valence band can again be understood in simple classical terms (as with the electrons in the conduction band).
Carrier generation and recombination.
When ionizing radiation strikes a semiconductor, it may excite an electron out of its energy level and consequently leave a hole. This process is known as "electron–hole pair generation". Electron-hole pairs are constantly generated from thermal energy as well, in the absence of any external energy source.
Electron-hole pairs are also apt to recombine. Conservation of energy demands that these recombination events, in which an electron loses an amount of energy larger than the band gap, be accompanied by the emission of thermal energy (in the form of phonons) or radiation (in the form of photons).
In some states, the generation and recombination of electron–hole pairs are in equipoise. The number of electron-hole pairs in the steady state at a given temperature is determined by quantum statistical mechanics. The precise quantum mechanical mechanisms of generation and recombination are governed by conservation of energy and conservation of momentum.
As the probability that electrons and holes meet together is proportional to the product of their amounts, the product is in steady state nearly constant at a given temperature, providing that there is no significant electric field (which might "flush" carriers of both types, or move them from neighbour regions containing more of them to meet together) or externally driven pair generation. The product is a function of the temperature, as the probability of getting enough thermal energy to produce a pair increases with temperature, being approximately exp(−"E""G"/"kT"), where "k" is Boltzmann's constant, "T" is absolute temperature and "E""G" is band gap.
The probability of meeting is increased by carrier traps—impurities or dislocations which can trap an electron or hole and hold it until a pair is completed. Such carrier traps are sometimes purposely added to reduce the time needed to reach the steady state.
Doping.
The conductivity of semiconductors may easily be modified by introducing impurities into their crystal lattice. The process of adding controlled impurities to a semiconductor is known as "doping". The amount of impurity, or dopant, added to an "intrinsic" (pure) semiconductor varies its level of conductivity. Doped semiconductors are referred to as "extrinsic". By adding impurity to the pure semiconductors, the electrical conductivity may be varied by factors of thousands or millions.
A 1 cm3 specimen of a metal or semiconductor has of the order of 1022 atoms. In a metal, every atom donates at least one free electron for conduction, thus 1 cm3 of metal contains on the order of 1022 free electrons, whereas a 1 cm3 sample of pure germanium at 20 °C contains about atoms, but only free electrons and holes. The addition of 0.001% of arsenic (an impurity) donates an extra 1017 free electrons in the same volume and the electrical conductivity is increased by a factor of 10,000.
The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with "donor" impurities are called "n-type", while those doped with "acceptor" impurities are known as "p-type". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.
For example, the pure semiconductor silicon has four valence electrons which bond each silicon atom to its neighbors. In silicon, the most common dopants are "group III" and "group V" elements. Group III elements all contain three valence electrons, causing them to function as acceptors when used to dope silicon. When an acceptor atom replaces a silicon atom in the crystal, a vacant state ( an electron "hole") is created, which can move around the lattice and functions as a charge carrier. Group V elements have five valence electrons, which allows them to act as a donor; substitution of these atoms for silicon creates an extra free electron. Therefore, a silicon crystal doped with boron creates a p-type semiconductor whereas one doped with phosphorus results in an n-type material.
During manufacture, dopants can be diffused into the semiconductor body by contact with gaseous compounds of the desired element, or ion implantation can be used to accurately position the doped regions.
Early history of semiconductors.
The history of the understanding of semiconductors begins with experiments on the electrical properties of materials. The properties of negative temperature coefficient of resistance, rectification, and light-sensitivity were observed starting in the early 19th century.
In 1833, Michael Faraday reported that the resistance of specimens of silver sulfide decreases when they are heated. This is contrary to the behavior of metallic substances such as copper. In 1839, A. E. Becquerel reported observation of a voltage between a solid and a liquid electrolyte when struck by light, the photovoltaic effect. In 1873 Willoughby Smith observed that selenium resistors exhibit decreasing resistance when light falls on them. In 1874 Karl Ferdinand Braun observed conduction and rectification in metallic sulphides, although this effect had been discovered much earlier by M.A. Rosenschold writing for the Annalen der Physik und Chemie in 1835, and Arthur Schuster found that a copper oxide layer on wires has rectification properties that ceases when the wires are cleaned. Adams and Day observed the photovoltaic effect in selenium in 1876.
A unified explanation of these phenomena required a theory of solid-state physics which developed greatly in the first half of the 20th Century. In 1878 Edwin Herbert Hall demonstrated the deflection of flowing charge carriers by an applied magnetic field, the Hall effect. The discovery of the electron by J.J. Thomson in 1897 prompted theories of electron-based conduction in solids. Karl Baedeker, by observing a Hall effect with the reverse sign to that in metals, theorized that copper iodide had positive charge carriers. Johan Koenigsberger classified solid materials as metals, insulators and "variable conductors" in 1914, although his student Josef Weiss introduced term "Halbleiter" (semiconductor) in modern meaning in PhD thesis already in 1910. Felix Bloch published a theory of the movement of electrons through atomic lattices in 1928. In 1930, B. Gudden stated that conductivity in semiconductors was due to minor concentrations of impurities. By 1931, the band theory of conduction had been established by Alan Herries Wilson and the concept of band gaps had been developed. Walter H. Schottky and Nevill Francis Mott developed models of the potential barrier and of the characteristics of a metal-semiconductor junction. By 1938, Boris Davydov had developed a theory of the copper-oxide rectifier, identifying the effect of the p–n junction and the importance of minority carriers and surface states.
Agreement between theoretical predictions (based on developing quantum mechanics) and experimental results was sometimes poor. This was later explained by John Bardeen as due to the extreme "structure sensitive" behavior of semiconductors, whose properties change dramatically based on tiny amounts of impurities. Commercially pure materials of the 1920s containing varying proportions of trace contaminants produced differing experimental results. This spurred the development of improved material refining techniques, culminating in modern semiconductor refineries producing materials with parts-per-trillion purity.
Devices using semiconductors were at first constructed based on empirical knowledge, before semiconductor theory provided a guide to construction of more capable and reliable devices.
Alexander Graham Bell used the light-sensitive property of selenium to transmit sound over a beam of light in 1880. A working solar cell, of low efficiency, was constructed by Charles Fritts in 1883 using a metal plate coated with selenium and a thin layer of gold; the device became commercially useful in photographic light meters in the 1930s. Point-contact microwave detector rectifiers made of lead sulfide were used by Jagadish Chandra Bose in 1904; the cat's-whisker detector using natural galena or other materials became a common device in the development of radio. However, it was somewhat unpredictable in operation and required manual adjustment for best performance. In 1906 H.J. Round observed light emission when electric current passed through silicon carbide crystals, the principle behind the light emitting diode. Oleg Losev observed similar light emission in 1922 but at the time the effect had no practical use. Power rectifiers, using copper oxide and selenium, were developed in the 1920s and became commercially important as an alternative to vacuum tube rectifiers.
In the years preceding World War II, infra-red detection and communications devices prompted research into lead-sulfide and lead-selenide materials. These devices were used for detecting ships and aircraft, for infrared rangefinders, and for voice communication systems. The point-contact crystal detector became vital for microwave radio systems, since available vacuum tube devices could not serve as detectors above about 4000 MHz; advanced radar systems relied on the fast response of crystal detectors. Considerable research and development of silicon materials occurred during the war to develop detectors of consistent quality.
Detector and power rectifiers could not amplify a signal. Many efforts were made to develop a solid-state amplifier, but these were unsuccessful because of limited theoretical understanding of semiconductor materials. In 1922 Oleg Losev developed two-terminal, negative resistance amplifiers for radio; however, he perished in the Siege of Leningrad. In 1926 Julius Edgar Lilienfeld patented a device resembling a modern field-effect transistor, but it was not practical. R. Hilsch and R. W. Pohl in 1938 demonstrated a solid-state amplifier using a structure resembling the control grid of a vacuum tube; although the device displayed power gain, it had a cut-off frequency of one cycle per second, too low for any practical applications, but an effective application of the available theory. At Bell Labs, William Shockley and A. Holden started investigating solid-state amplifiers in 1938. The first p–n junction in silicon was observed by Russell Ohl about 1941, when a specimen was found to be light-sensitive, with a sharp boundary between p-type impurity at one end and n-type at the other. A slice cut from the specimen at the p–n boundary developed a voltage when exposed to light.
In France, during the war, Herbert Mataré had observed amplification between adjacent point contacts on a germanium base. After the war, Mataré's group announced their "Transistron" amplifier only shortly after Bell Labs announced the "transistor".

</doc>
<doc id="27711" url="https://en.wikipedia.org/wiki?curid=27711" title="Starch">
Starch

Starch or amylum is a carbohydrate consisting of a large number of glucose units joined by glycosidic bonds. This polysaccharide is produced by most green plants as an energy store. It is the most common carbohydrate in human diets and is contained in large amounts in staple foods such as potatoes, wheat, maize (corn), rice, and cassava.
Pure starch is a white, tasteless and odorless powder that is insoluble in cold water or alcohol. It consists of two types of molecules: the linear and helical amylose and the branched amylopectin.
Depending on the plant, starch generally contains 20 to 25% amylose and 75 to 80% amylopectin by weight. Glycogen, the glucose store of animals, is a more branched version of amylopectin.
Starch is processed to produce many of the sugars in processed foods. Dissolving starch in warm water gives wheatpaste, which can be used as a thickening, stiffening or gluing agent. The biggest industrial non-food use of starch is as adhesive in the papermaking process. Starch can be applied to parts of some garments before ironing, to stiffen them.
Etymology.
The word "starch" is from a Germanic root with the meanings "strong, stiff, strengthen, stiffen". Modern German "Stärke" (starch) is related.
"Amylum" for starch is from the Greek αμυλον, "amylon" which means "not ground at a mill". The root amyl is used in biochemistry for several compounds related to starch.
History.
Starch grains from the rhizomes of "Typha" (cattails, bullrushes) as flour have been identified from grinding stones in Europe dating back to 30,000 years ago. Starch grains from sorghum were found on grind stones in caves in Ngalue, Mozambique dating up to 100,000 years ago.
Pure extracted wheat starch paste was used in Ancient Egypt possibly to glue papyrus. The extraction of starch is first described in the "Natural History" of Pliny the Elder around AD 77–79. Romans used it also in cosmetic creams, to powder the hair and to thicken sauces. Persians and Indians used it to make dishes similar to gothumai wheat halva. Rice starch as surface treatment of paper has been used in paper production in China, from 700 AD onwards.
In addition to starchy plants consumed directly, 66 million tonnes of starch were being produced per year world-wide by 2008. In the EU this was around 8.5 million tonnes, with around
40% being used for industrial applications and 60% for food uses, most of the latter as glucose syrups.
Energy store of plants.
Most green plants use starch as their energy store. An exception is the family Asteraceae (asters, daisies and sunflowers), where starch is replaced by the fructan inulin.
In photosynthesis, plants use light energy to produce glucose from carbon dioxide. The glucose is stored mainly in the form of starch granules, in amyloplasts. Toward the end of the growing season, starch accumulates in twigs of trees near the buds. Fruit, seeds, rhizomes, and tubers store starch to prepare for the next growing season.
Glucose is soluble in water, hydrophilic, binds with water and then takes up much space and is osmotically active; glucose in the form of starch, on the other hand, is not soluble, therefore osmotically inactive and can be stored much more compactly.
Glucose molecules are bound in starch by the easily hydrolyzed alpha bonds. The same type of bond is found in the animal reserve polysaccharide glycogen. This is in contrast to many structural polysaccharides such as chitin, cellulose and peptidoglycan, which are bound by beta bonds and are much more resistant to hydrolysis.
Biosynthesis.
Plants produce starch by first converting glucose 1-phosphate to ADP-glucose using the enzyme glucose-1-phosphate adenylyltransferase. This step requires energy in the form of ATP. The enzyme starch synthase then adds the ADP-glucose via a 1,4-alpha glycosidic bond to a growing chain of glucose residues, liberating ADP and creating amylose. Starch branching enzyme introduces 1,6-alpha glycosidic bonds between these chains, creating the branched amylopectin. The starch debranching enzyme isoamylase removes some of these branches. Several isoforms of these enzymes exist, leading to a highly complex synthesis process.
Glycogen and amylopectin have the same structure, but the former has about one branch point per ten 1,4-alpha bonds, compared to about one branch point per thirty 1,4-alpha bonds in amylopectin. Amylopectin is synthesized from ADP-glucose while mammals and fungi synthesize glycogen from UDP-glucose; for most cases, bacteria synthesize glycogen from ADP-glucose (analogous to starch).
In addition to starch synthesis in plants, starch can be synthesized from non-food starch mediated by an enzyme cocktail. In this cell-free biosystem, beta-1,4-glycosidic bond-linked cellulose is partially hydrolyzed to cellobiose.Cellobiose phosphorylase cleaves to glucose 1-phosphate and glucose; the other enzyme—potato alpha-glucan phosphorylase can add a glucose unit from glucose 1-phosphorylase to the non-ruducing ends of starch. In it, phosphate is internally recycled. The other product, glucose, can be assimilated by a yeast. This cell-free bioprocessing does not need any costly chemical and energy input, can be conducted in aqueous solution, and does not have sugar losses.
Degradation.
Starch is synthesized in plant leaves during the day, in order to serve as an energy source at night. Starch is stored as granulates. These insoluble highly branched chains have to be phosphorylated in order to be accessible for degrading enzymes. The enzyme glucan, water dikinase (GWD) phosphorylates at the C-6 position of a glucose molecule, close to the chains 1,6-alpha branching bonds. A second enzyme, phosphoglucan, water dikinase (PWD) phosphorylates the glucose molecule at the C-3 position. A loss of these enzymes, for example a loss of the GWD, leads to a starch excess (sex) phenotype. Because starch cannot be phosphorylated, it accumulates in the plastid.
After the phosphorylation, the first degrading enzyme, beta-amylase (BAM) is able to attack the glucose chain at its non-reducing end. Maltose is released as the main product of starch degradation. If the glucose chain consists of three or less molecules, BAM cannot release maltose. A second enzyme, disproportionating enzyme-1 (DPE1), combines two maltotriose molecules. From this chain, a glucose molecule is released. Now, BAM can release another maltose molecule from the remaining chain. This cycle repeats until starch is degraded completely. If BAM comes close to the phosphorylated branching point of the glucose chain, it can no longer release maltose. In order for the phosphorylated chain to be degraded, the enzyme isoamylase (ISA) is required.
The products of starch degradation are to the major part maltose and to a less extensive part glucose. These molecules are now exported from the plastid to the cytosol. Maltose is exported via the maltose transporter. If this transporter is mutated (MEX1-mutant), maltose accumulates in the plastid. Glucose is exported via the plastidic glucose translocator (pGlcT). Now, these two sugars act as a precursor for sucrose synthesis. Sucrose can the be used in the oxidative pentose phosphate pathway in the mitochondria, in order to generate ATP at night.
Properties.
Structure.
While amylose was traditionally thought to be completely unbranched, it is now known that some of its molecules contain a few branch points. Although in absolute mass only about one quarter of the starch granules in plants consist of amylose, there are about 150 times more amylose molecules than amylopectin molecules. Amylose is a much smaller molecule than amylopectin.
Starch molecules arrange themselves in the plant in semi-crystalline granules. Each plant species has a unique starch granular size: rice starch is relatively small (about 2μm) while potato starches have larger granules (up to 100μm).
Starch becomes soluble in water when heated. The granules swell and burst, the semi-crystalline structure is lost and the smaller amylose molecules start leaching out of the granule, forming a network that holds water and increasing the mixture's viscosity. This process is called starch gelatinization. During cooking, the starch becomes a paste and increases further in viscosity. During cooling or prolonged storage of the paste, the semi-crystalline structure partially recovers and the starch paste thickens, expelling water. This is mainly caused by retrogradation of the amylose. This process is responsible for the hardening of bread or staling, and for the water layer on top of a starch gel (syneresis).
Some cultivated plant varieties have pure amylopectin starch without amylose, known as "waxy starches". The most used is waxy maize, others are glutinous rice and waxy potato starch. Waxy starches have less retrogradation, resulting in a more stable paste. High amylose starch, amylomaize, is cultivated for the use of its gel strength and for use as a resistant starch (a starch that resists digestion) in food products.
Synthetic amylose made from cellulose has a well-controlled degree of polymerization. Therefore, it can be used as a potential drug deliver carrier.
Certain starches, when mixed with water, will produce a non-newtonian fluid sometimes nicknamed "oobleck".
Hydrolysis.
The enzymes that break down or hydrolyze starch into the constituent sugars are known as amylases.
Alpha-amylases are found in plants and in animals. Human saliva is rich in amylase, and the pancreas also secretes the enzyme. Individuals from populations with a high-starch diet tend to have more amylase genes than those with low-starch diets;
Beta-amylase cuts starch into maltose units. This process is important in the digestion of starch and is also used in brewing, where amylase from the skin of seed grains is responsible for converting starch to maltose (Malting, Mashing).
Dextrinization.
If starch is subjected to dry heat, it breaks down to form dextrins, also called "pyrodextrins" in this context. This break down process is known as dextrinization. (Pyro)dextrins are mainly yellow to brown in color and dextrinization is partially responsible for the browning of toasted bread.
Chemical tests.
A triiodide (I3−) solution formed by mixing iodine and iodide (usually from potassium iodide) is used to test for starch; a dark blue color indicates the presence of starch. The details of this reaction are not yet fully known, but it is thought that the iodine (I3− and I5− ions) fit inside the coils of amylose, the charge transfers between the iodine and the starch, and the energy level spacings in the resulting complex correspond to the absorption spectrum in the visible light region. The strength of the resulting blue color depends on the amount of amylose present. Waxy starches with little or no amylose present will color red.
Starch indicator solution consisting of water, starch and iodide is often used in redox titrations: in the presence of an oxidizing agent the solution turns blue, in the presence of reducing agent the blue color disappears because triiodide (I3−) ions break up into three iodide ions, disassembling the starch-iodine complex.
A 0.3% w/w solution is the standard concentration for a starch indicator. It is made by adding 3 grams of soluble starch to 1 liter of heated water; the solution is cooled before use (starch-iodine complex becomes unstable at temperatures above 35 °C).
Each species of plant has a unique type of starch granules in granular size, shape and crystallization pattern. Under the microscope, starch grains stained with iodine illuminated from behind with polarized light show a distinctive Maltese cross effect (also known as extinction cross and birefringence).
Food.
Starch is the most common carbohydrate in the human diet and is contained in many staple foods. The major sources of starch intake worldwide are the cereals (rice, wheat, and maize) and the root vegetables (potatoes and cassava). Many other starchy foods are grown, some only in specific climates, including acorns, arrowroot, arracacha, bananas, barley, breadfruit, buckwheat, canna, colacasia, katakuri, kudzu, malanga, millet, oats, oca, polynesian arrowroot, sago, sorghum, sweet potatoes, rye, taro, chestnuts, water chestnuts and yams, and many kinds of beans, such as favas, lentils, mung beans, peas, and chickpeas.
Widely used prepared foods containing starch are bread, pancakes, cereals, noodles, pasta, porridge and tortilla.
Digestive enzymes have problems digesting crystalline structures. Raw starch will digest poorly in the duodenum and small intestine, while bacterial degradation will take place mainly in the colon. When starch is cooked, the digestibility is increased. 
Starch gelatinization during cake baking can be impaired by sugar competing for water, preventing gelatinization and improving texture.
Historically, people consumed large amounts of uncooked and unprocessed starch-containing plants, which contained high amounts of resistant starch. Microbes within in the large intestine fermented the starch, produced short-chain fatty acids, which were used as energy as well as maintenance and growth of the microbes. As foods became more processed, they were more easily digested and released more glucose in the small intestine - less starch reached the large intestine and more energy was absorbed by the human body. This shift in energy delivery may be one of the contributing factors to the development of metabolic disorders of modern life, including obesity and diabetes. 
Starch industry.
The starch industry extracts and refines starches from seeds, roots and tubers, by wet grinding, washing, sieving and drying. Today, the main commercial refined starches are cornstarch, tapioca, wheat, rice and potato starch. To a lesser extent, sources include rice, sweet potato, sago and mung bean. Historically, Florida arrowroot was also commercialized. To this day, starch is extracted from more than 50 types of plants.
Untreated starch requires heat to thicken or gelatinize. When a starch is pre-cooked, it can then be used to thicken instantly in cold water. This is referred to as a pregelatinized starch.
Starch sugars.
Starch can be hydrolyzed into simpler carbohydrates by acids, various enzymes, or a combination of the two. The resulting fragments are known as dextrins. The extent of conversion is typically quantified by dextrose equivalent (DE), which is roughly the fraction of the glycosidic bonds in starch that have been broken.
These starch sugars are by far the most common starch based food ingredient and are used as sweetener in many drinks and foods. They include:
Modified starches.
A modified starch is a starch that has been chemically modified to allow the starch to function properly under conditions frequently encountered during processing or storage, such as high heat, high shear, low pH, freeze/thaw and cooling.
The modified food starches are E coded according to the International Numbering System for Food Additives (INS):
INS 1400, 1401, 1402, 1403 and 1405 are in the EU food ingredients without an E-number. Typical modified starches for technical applications are cationic starches, hydroxyethyl starch and carboxymethylated starches.
Use as food additive.
As an additive for food processing, food starches are typically used as thickeners and stabilizers in foods such as puddings, custards, soups, sauces, gravies, pie fillings, and salad dressings, and to make noodles and pastas.
Gummed sweets such as jelly beans and wine gums are not manufactured using a mold in the conventional sense. A tray is filled with native starch and leveled. A positive mold is then pressed into the starch leaving an impression of 1,000 or so jelly beans. The jelly mix is then poured into the impressions and put into a stove to set. This method greatly reduces the number of molds that must be manufactured.
Use in pharmaceutical industry.
In the pharmaceutical industry, starch is also used as an excipient, as tablet disintegrant or as binder.
Resistant starch.
Resistant starch is starch that escapes digestion in the small intestine of healthy individuals.
High amylose starch from corn has a higher gelatinization temperature than other types of starch and retains its resistant starch content through baking, mild extrusion and other food processing techniques. It is used as an insoluble dietary fiber in processed foods such as bread, pasta, cookies, crackers, pretzels and other low moisture foods. It is also utilized as a dietary supplement for its health benefits. Published studies have shown that Type 2 resistant corn helps to improve insulin sensitivity, increases satiety and improves markers of colonic function.
It has been suggested that resistant starch contributes to the health benefits of intact whole grains.
Industrial applications.
Papermaking.
Papermaking is the largest non-food application for starches globally, consuming millions of metric tons annually. In a typical sheet of copy paper for instance, the starch content may be as high as 8%. Both chemically modified and unmodified starches are used in papermaking. In the wet part of the papermaking process, generally called the "wet-end", the starches used are cationic and have a positive charge bound to the starch polymer. These starch derivatives associate with the anionic or negatively charged paper fibers / cellulose and inorganic fillers. Cationic starches together with other retention and internal sizing agents help to give the necessary strength properties to the paper web formed in the papermaking process (wet strength), and to provide strength to the final paper sheet (dry strength).
In the dry end of the papermaking process, the paper web is rewetted with a starch based solution. The process is called surface sizing. Starches used have been chemically, or enzymatically depolymerized at the paper mill or by the starch industry (oxidized starch). The size - starch solutions are applied to the paper web by means of various mechanical presses (size presses). Together with surface sizing agents the surface starches impart additional strength to the paper web and additionally provide water hold out or "size" for superior printing properties. Starch is also used in paper coatings as one of the binders for the coating formulations which include a mixture of pigments, binders and thickeners. Coated paper has improved smoothness, hardness, whiteness and gloss and thus improves printing characteristics.
Corrugated board adhesives.
Corrugated board adhesives are the next largest application of non-food starches globally. Starch glues are mostly based on unmodified native starches, plus some additive such as borax and caustic soda. Part of the starch is gelatinized to carry the slurry of uncooked starches and prevent sedimentation. This opaque glue is called a SteinHall adhesive. The glue is applied on tips of the fluting. The fluted paper is pressed to paper called liner. This is then dried under high heat, which causes the rest of the uncooked starch in glue to swell/gelatinize. This gelatinizing makes the glue a fast and strong adhesive for corrugated board production.
Clothing starch.
Clothing or laundry starch is a liquid that is prepared by mixing a vegetable starch in water (earlier preparations also had to be boiled), and is used in the laundering of clothes. Starch was widely used in Europe in the 16th and 17th centuries to stiffen the wide collars and ruffs of fine linen which surrounded the necks of the well-to-do. During the 19th century and early 20th century, it was stylish to stiffen the collars and sleeves of men's shirts and the ruffles of girls' petticoats by applying starch to them as the clean clothes were being ironed. Aside from the smooth, crisp edges it gave to clothing, it served practical purposes as well. Dirt and sweat from a person's neck and wrists would stick to the starch rather than to the fibers of the clothing, and would easily wash away along with the starch. After each laundering, the starch would be reapplied. Today, the product is sold in aerosol cans for home use.
Other.
Another large non-food starch application is in the construction industry, where starch is used in the gypsum wall board manufacturing process. Chemically modified or unmodified starches are added to the stucco containing primarily gypsum. Top and bottom heavyweight sheets of paper are applied to the formulation, and the process is allowed to heat and cure to form the eventual rigid wall board. The starches act as a glue for the cured gypsum rock with the paper covering, and also provide rigidity to the board.
Starch is used in the manufacture of various adhesives or glues for book-binding, wallpaper adhesives, paper sack production, tube winding, gummed paper, envelope adhesives, school glues and bottle labeling. Starch derivatives, such as yellow dextrins, can be modified by addition of some chemicals to form a hard glue for paper work; some of those forms use borax or soda ash, which are mixed with the starch solution at to create a very good adhesive. Sodium silicate can be added to reinforce these formula.
Occupational safety and health.
The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for starch exposure in the workplace as 15 mg/m3 total exposure and 5 mg/m3 respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 10 mg/m3 total exposure and 5 mg/m3 respiratory exposure over an 8-hour workday.

</doc>
<doc id="27712" url="https://en.wikipedia.org/wiki?curid=27712" title="Sugar">
Sugar

Sugar is the generalized name for sweet, short-chain, soluble carbohydrates, many of which are used in food. They are composed of carbon, hydrogen, and oxygen. There are various types of sugar derived from different sources. Simple sugars are called monosaccharides and include glucose (also known as dextrose), fructose, and galactose. The table or granulated sugar most customarily used as food is sucrose, a disaccharide. (In the body, sucrose hydrolyses into fructose and glucose.) Other disaccharides include maltose and lactose. Longer chains of sugars are called oligosaccharides. Chemically-different substances may also have a sweet taste, but are not classified as sugars. Some are used as lower-calorie food substitutes for sugar described as artificial sweeteners.
Sugars are found in the tissues of most plants, but are present in sufficient concentrations for efficient extraction only in sugarcane and sugar beet. Sugarcane refers to any of several species of giant grass in the genus "Saccharum" that have been cultivated in tropical climates in South Asia and Southeast Asia since ancient times. A great expansion in its production took place in the 18th century with the establishment of sugar plantations in the West Indies and Americas. This was the first time that sugar became available to the common people, who had previously had to rely on honey to sweeten foods. Sugar beet, a cultivated variety of "Beta vulgaris", is grown as a root crop in cooler climates and became a major source of sugar in the 19th century when methods for extracting the sugar became available. Sugar production and trade have changed the course of human history in many ways, influencing the formation of colonies, the perpetuation of slavery, the transition to indentured labour, the migration of peoples, wars between sugar-trade–controlling nations in the 19th century, and the ethnic composition and political structure of the New World.
The world produced about 168 million tonnes of sugar in 2011. The average person consumes about of sugar each year (33.1 kg in industrialised countries), equivalent to over 260 food calories per person, per day.
Since the latter part of the twentieth century, it has been questioned whether a diet high in sugars, especially refined sugars, is good for human health. Sugar has been linked to obesity, and suspected of, or fully implicated as a cause in the occurrence of diabetes, cardiovascular disease, dementia, macular degeneration, and tooth decay. Numerous studies have been undertaken to try to clarify the position, but with varying results, mainly because of the difficulty of finding populations for use as controls that do not consume or are largely free of any sugar consumption.
Etymology.
The etymology reflects the spread of the commodity. The English word "sugar" originates from the Sanskrit "śarkarā", via Persian "shakkar". It most probably came to England by way of Italian merchants. The contemporary Italian word is "zucchero", whereas the Spanish and Portuguese words, "azúcar" and "açúcar" respectively, have kept a trace of the Arabic definite article. The Old French word is "zuchre" – contemporary French "sucre". The earliest Greek word attested is σάκχαρις ("sákkʰaris"). A satisfactory "pedigree" explaining the spread of the word has yet to be done. The English word "jaggery", a coarse brown sugar made from date palm sap or sugarcane juice, has a similar etymological origin; Portuguese "xagara" or "jagara", from the Sanskrit "śarkarā".
History.
Ancient times and Middle Ages.
Sugar has been produced in the Indian subcontinent since ancient times. It was not plentiful or cheap in early times and honey was more often used for sweetening in most parts of the world. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical South Asia and Southeast Asia. Different species seem to have originated from different locations with "Saccharum barberi" originating in India and "S. edule" and "S. officinarum" coming from New Guinea. One of the earliest historical references to sugarcane is in Chinese manuscripts dating back to 8th century BC that state that the use of sugarcane originated in India.
Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport. Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century AD.In the local Indian language, these crystals were called "khanda" (Devanagari:खण्ड,), which is the source of the word "candy".
Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar on the various trade routes they travelled. Buddhist monks, as they travelled around, brought sugar crystallization methods to China. During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China then established its first sugarcane plantations in the seventh century. Chinese documents confirm at least two missions to India, initiated in 647 AD, to obtain technology for sugar refining. In South Asia, the Middle East and China, sugar became a staple of cooking and desserts.
The triumphant progress of Alexander the Great was halted on the banks of the Indus River by the refusal of his troops to go further east. They saw people in the Indian subcontinent growing sugarcane and making "granulated, salt-like sweet powder", locally called "Sharkara" (Devanagari:शर्करा,), Latin "saccharum", Greek ζάκχαρι (zakkhari). On their return journey, the Macedonian soldiers carried the "honey-bearing reeds" home with them. Sugarcane remained a little-known crop in Europe for over a millennium, sugar a rare commodity, and traders in sugar wealthy.
Crusaders brought sugar home with them to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe, where it supplemented honey, which had previously been the only available sweetener.
Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind". In the 15th century, Venice was the chief sugar refining and distribution centre in Europe.
Modern history.
In August 1492, Christopher Columbus stopped at La Gomera in the Canary Islands, for wine and water, intending to stay only four days. He became romantically involved with the governor of the island, Beatriz de Bobadilla y Ossorio, and stayed a month. When he finally sailed, she gave him cuttings of sugarcane, which became the first to reach the New World.
The first sugar cane harvest was conducted in Hispaniola in 1501, and many sugar mills had been constructed in Cuba and Jamaica by the 1520s. The Portuguese took sugar cane to Brazil. By 1540, there were 800 cane sugar mills in Santa Catarina Island and another 2,000 on the north coast of Brazil, Demarara, and Surinam.
Sugar was a luxury in Europe until the 18th century, when it became more widely available. It then became popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. It drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and sugar manufacturing could thrive. The demand for cheap labor to perform the hard work involved in its cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa). After slavery was abolished, there was high demand for indentured laborers from South Asia (in particular India). Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. The modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.
Sugar also led to some industrialization of areas where sugar cane was grown. For example, Lieutenant J. Paterson, of the Bengal establishment, persuaded the British Government that sugar cane could be cultivated in British India with many advantages and at less expense than in the West Indies; as a result, sugar factories were established in Bihar in eastern India.
During the Napoleonic Wars, sugar beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880, the sugar beet was the main source of sugar in Europe. It was cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.
Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called Sugar nips. In later years, granulated sugar was more usually sold in bags.
Sugar cubes were produced in the nineteenth century. The first inventor of a process to make sugar in cube form was the Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar cube production after being granted a five-year patent for the process on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.
Chemistry.
Scientifically, "sugar" loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars," the most important being glucose. Almost all sugars have the formula (n is between 3 and 7). Glucose has the molecular formula . The names of typical sugars end with -"ose", as in "glucose" and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water () per bond.
Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.
Natural polymers of sugars.
Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose () or (as in cane and beet) sucrose (). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectin for cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy. Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut. DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula and ribose the formula .
Flammability.
Because sugars burn easily when exposed to flame, the handling of sugars risks dust explosion. The 2008 Georgia sugar refinery explosion, which killed 14 persons and injured 40, and destroyed most of the refinery, was caused by the ignition of sugar dust.
Types of sugar.
Monosaccharides.
Fructose, galactose, and glucose are all simple sugars, monosaccharides, with the general formula C6H12O6. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.
Disaccharides.
Lactose, maltose, and sucrose are all compound sugars, disaccharides, with the general formula C12H22O11. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.
Sources.
The sugar contents of common fruits and vegetables are presented in Table 1.
All data with a unit of g (gram) are based on 100 g of a food item.
The fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.
Production.
Sugar beet.
Sugar beet ("Beta vulgaris") is a biennial plant in the Family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in a clamp in the field for some weeks before being transported to the processing plant. Here the crop is washed and sliced and the sugar extracted by diffusion. Milk of lime is added to the raw juice and carbonatated in a number of stages in order to purify it. Water is evaporated by boiling the syrup under a vacuum. The syrup is then cooled and seeded with sugar crystals. The white sugar that crystallizes out can be separated in a centrifuge and dried. It requires no further refining.
Sugarcane.
Sugarcane ("Saccharum spp.") is a perennial grass in the family Poaceae. It is cultivated in tropical and sub-tropical regions for the sucrose that is found in its stems. It requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's great growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant. Here, it is either milled and the juice extracted with water or extracted by diffusion. The juice is then clarified with lime and heated to kill enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed by evaporation in vacuum containers. The resulting supersaturated solution is seeded with sugar crystals and the sugar crystallizes out and is separated from the fluid and dried. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are or can be bleached by sulfur dioxide or can be treated in a carbonatation process to produce a whiter product. About of irrigation water is needed for every one kilogram of sugar produced.
Refining.
Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses. Raw sugar is a sucrose which is synthesized from sugarcane or sugar beet and cannot immediately be consumed before going through the refining process to produce refined sugar or white sugar.
The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of colour is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.
The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.
Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500). The level of purity associated with the colors of sugar, expressed by standard number ICUMSA (International Commission for Uniform Methods of sugar Analysis), the smaller ICUMSA numbers indicate that higher purity of sugar.
Producing countries.
The five largest producers of sugar in 2011 were Brazil, India, the European Union, China and Thailand. In the same year, the largest exporter of sugar was Brazil, distantly followed by Thailand, Australia and India. The largest importers were the European Union, United States and Indonesia. At present, Brazil has the highest per capita consumption of sugar, followed by Australia, Thailand, and the European Union.
Consumption.
In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugarcane and beet provided more kilocalories per capita per day on average than other food groups. According to the FAO, an average of of sugar, equivalent to over 260 food calories per day, was consumed annually per person of all ages in the world in 1999. Even with rising human populations, sugar consumption is expected to increase to per person per year by 2015.
Data collected in multiple nationwide surveys between 1999 and 2008 show that the intake of added sugars has declined by 24 percent with declines occurring in all age, ethnic and income groups.
The per capita consumption of refined sugar in the United States has varied between in the last 40 years. In 2008, American per capita total consumption of sugar and sweeteners, exclusive of artificial sweeteners, equalled per year. This consisted of pounds of refined sugar and pounds of corn-derived sweeteners per person.
Health effects.
Some studies involving the health impact of sugars are effectively inconclusive. The FAO meta studies and WHO studies have shown directly contrasting impacts of sugar in refined and unrefined forms and since most studies do not use a population that do not consume any "free sugars" at all, the baseline is effectively flawed. Hence, there are articles such as "Consumer Reports on Health" that stated in 2008, "Some of the supposed dietary dangers of sugar have been overblown. Many studies have debunked the idea that it causes hyperactivity, for example".
Addiction.
Sugar addiction is the term for the relationship between sugar and the various aspects of food addiction including "bingeing, withdrawal, craving and cross-sensitization". Some scientists assert that consumption of sweets or sugar could have a heroin addiction-like effect.
Alzheimer's disease.
Claims have been made of a sugar–Alzheimer's disease connection, but debate continues over whether cognitive decline is attributable to dietary fructose or to overall energy intake.
Blood glucose levels.
It used to be believed that sugar raised blood glucose levels more quickly than did starch because of its simpler chemical structure. However, it turned out that white bread or French fries have the same effect on blood sugar as pure glucose, while fructose, although a simple carbohydrate, has a minimal effect on blood sugar. As a result, as far as blood sugar is concerned, carbohydrates are classified according to their glycemic index, a system for measuring how quickly a food that is eaten raises blood sugar levels, and glycemic load, which takes into account both the glycemic index and the amount of carbohydrate in the food. This has led to carbohydrate counting, a method used by diabetics for planning their meals.
Cardiovascular disease.
Studies in animals have suggested that chronic consumption of refined sugars can contribute to metabolic and cardiovascular dysfunction. Some experts have suggested that refined fructose is more damaging than refined glucose in terms of cardiovascular risk. Cardiac performance has been shown to be impaired by switching from a carbohydrate diet including fiber to a high-carbohydrate diet.
Switching from saturated fatty acids to carbohydrates with high glycemic index values shows a statistically-significant increase in the risk of myocardial infarction. Other studies have shown that the risk of developing coronary heart disease is decreased by adopting a diet high in polyunsaturated fatty acids but low in sugar, whereas a low-fat, high-carbohydrate diet brings no reduction. This suggests that consuming a diet with a high glycemic load typical of the "junk food" diet is strongly associated with an increased risk of developing coronary heart disease.
The consumption of added sugars has been positively associated with multiple measures known to increase cardiovascular disease risk amongst adolescents as well as adults.
Studies are suggesting that the impact of refined carbohydrates or high glycemic load carbohydrates are more significant than the impact of saturated fatty acids on cardiovascular disease.
A high dietary intake of sugar (in this case, sucrose or disaccharide) can substantially increase the risk of heart and vascular diseases. According to a Swedish study of undertaken by Lund University and Malmö University College, sugar was associated with higher levels of bad blood lipids, causing a high level of small and medium low-density lipoprotein (LDL) and reduced high-density lipoprotein (HDL). In contrast, the amount of fat eaten did not affect the level of blood fats. Incidentally quantities of alcohol and protein were linked to an increase in the good HDL blood fat.
Hyperactivity.
There is a common notion that sugar leads to hyperactivity, in particular in children, but studies and meta-studies tend to disprove this. Some articles and studies do refer to the increasing evidence supporting the links between refined sugar and hyperactivity. The WHO FAO meta-study suggests that such inconclusive results are to be expected when some studies do not effectively segregate or control for free sugars as opposed to sugars still in their natural form (entirely unrefined) while others do. One study followed thirty-five 5-to-7-year-old boys who were reported by their mothers to be behaviorally "sugar-sensitive." They were randomly assigned to experimental and control groups. In the experimental group, mothers were told that their children were fed sugar, and, in the control group, mothers were told that their children received a placebo. In fact, all children received the placebo, but mothers in the sugar expectancy condition rated their children as significantly more hyperactive. This result suggests that the real effect of sugar is that it increases worrying among parents with preconceived notions.
Obesity and diabetes.
Controlled trials have now shown unequivocally that consumption of sugar-sweetened beverages increases body weight and body fat, and that replacement of sugar by artificial sweeteners reduces weight.
Studies on the link between sugars and diabetes are inconclusive, with some suggesting that eating excessive amounts of sugar does not increase the risk of diabetes, although the extra calories from consuming large amounts of sugar can lead to obesity, which may itself increase the risk of developing this metabolic disease. 
Other studies show correlation between refined sugar (free sugar) consumption and the onset of diabetes, and negative correlation with the consumption of fiber. These included a 2010 meta-analysis of eleven studies involving 310,819 participants and 15,043 cases of type 2 diabetes. 
This found that "SSBs (sugar-sweetened beverages) may increase the risk of metabolic syndrome and type 2 diabetes not only through obesity but also by increasing dietary glycemic load, leading to insulin resistance, β-cell dysfunction, and inflammation". As an overview to consumption related to chronic disease and obesity, the World Health Organization's independent meta-studies specifically distinguish free sugars ("all monosaccharides and disaccharides added to foods by the manufacturer, cook or consumer, plus sugars naturally present in honey, syrups and fruit juices") from sugars occurring naturally in food. The reports prior to 2000 set the limits for free sugars at a maximum of 10% of carbohydrate intake, measured by energy, rather than mass, and since 2002 have aimed for a level across the entire population of less than 10%. The consultation committee recognized that this goal is "controversial. However, the Consultation considered that the studies showing no effect of free sugars on excess weight have limitations".
Tooth decay.
In regard to contributions to tooth decay, the role of free sugars is also recommended to be below an absolute maximum of 10% of energy intake, with a minimum of zero. There is "convincing evidence from human intervention studies, epidemiological studies, animal studies and experimental studies, for an association between the amount and frequency of free sugars intake and dental caries" while other sugars (complex carbohydrate) consumption is normally associated with a lower rate of dental caries. Lower rates of tooth decay have been seen in individuals with hereditary fructose intolerance.
Also, studies have shown that the consumption of sugar and starch have different impacts on oral health with the ingestion of starchy foods and fresh fruit being associated with low levels of dental caries.
Recommended dietary intake.
The World Health Organization (WHO) recommends that both adults and children reduce the intake of free sugars to less than 10% of total energy intake. A reduction to below 5% of total energy intake brings additional health benefits, especially in what regards dental caries. These recommendations were based on the totality of available evidence reviewed regarding the relationship between free sugars intake and body weight and dental caries.
Free sugars include monosaccharides and disaccharides added to foods and beverages by the manufacturer, cook or consumer, and sugars naturally present in honey, syrups, fruit juices and fruit juice concentrates.
Measurements.
Various culinary sugars have different densities due to differences in particle size and inclusion of moisture.
Domino Sugar gives the following weight to volume conversions (in United States customary units):
The "Engineering Resources – Bulk Density Chart" published in "Powder and Bulk" gives different values for the bulk densities:

</doc>
<doc id="27715" url="https://en.wikipedia.org/wiki?curid=27715" title="Saint Louis">
Saint Louis

Saint Louis, Saint-Louis or St. Louis may refer to:

</doc>
<doc id="27717" url="https://en.wikipedia.org/wiki?curid=27717" title="Salma Hayek">
Salma Hayek

Salma Hayek Pinault (born September 2, 1966) is a Mexican-American film actress, producer and former model. She began her career in Mexico starring in the telenovela "Teresa" and starred in the film "El Callejón de los Milagros" ("Miracle Alley") for which she was nominated for an Ariel Award. In 1991 Hayek moved to Hollywood and came to prominence with roles in movies such as "Desperado" (1995), "Dogma" (1999), and "Wild Wild West" (1999).
Her breakthrough role was in the 2002 film "Frida" as Mexican painter Frida Kahlo for which she was nominated in the category of Best Actress for an Academy Award, BAFTA Award, Screen Actors Guild Award, and Golden Globe Award. This movie received widespread attention and was a critical and commercial success. She won a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special in 2004 for "The Maldonado Miracle" and received an Emmy Award nomination for Outstanding Guest Actress in a Comedy Series in 2007 after guest-starring in the ABC television comedy-drama "Ugly Betty." She also guest-starred on the NBC comedy series "30 Rock" from 2009 to 2013.
Hayek's recent films include "Grown Ups" (2010), "Puss in Boots" (2011), "Grown Ups 2" (2013), and "Tale of Tales" (2015).
Early life.
Hayek was born Salma Hayek Jiménez in Coatzacoalcos, Veracruz, Mexico. Her younger brother, Sami (born 1972), is a furniture designer. Her mother, Diana Jiménez Medina, is an opera singer and talent scout. Her father, Sami Hayek, is an oil company executive and owner of an industrial-equipment firm, who once ran for mayor of Coatzacoalcos. Her father is of Lebanese descent, with his family being from the city Baabdat, Lebanon, a city Salma and her father visited in 2015 to promote her movie "Kahlil Gibran's The Prophet". Her mother is of Mexican/Spanish descent, with her grandmother and maternal great-grandparents being from Spain. Her first given name, Salma, is Arabic for "peace" or "calm". Raised in a wealthy, devout Roman Catholic family, she was sent to the Academy of the Sacred Heart in Grand Coteau, Louisiana USA, at the age of twelve. As a teen, she was diagnosed with dyslexia. She attended university in Mexico City, where she studied International Relations at the Universidad Iberoamericana.
Career.
Mexico.
At the age of 23, Hayek landed the title role in "Teresa" (1989), a successful Mexican telenovela that made her a star in Mexico. In 1994, Hayek starred in the film "El Callejón de los Milagros" ("Miracle Alley"), which has won more awards than any other movie in the history of Mexican cinema. For her performance, Hayek was nominated for an Ariel Award.
Early Hollywood acting work.
Hayek moved to Los Angeles, California, in 1991 to study acting under Stella Adler. She had limited fluency in English, and dyslexia. Robert Rodriguez, and his producer and then-wife, Elizabeth Avellan, soon gave Hayek a starring role opposite Antonio Banderas in 1995's "Desperado". She followed her role in "Desperado" with a brief role as a vampire queen in "From Dusk till Dawn", in which she performed a table-top snake dance.
Hayek had a starring role opposite Matthew Perry in the 1997 romantic comedy "Fools Rush In". In 1999 she co-starred in Will Smith's big-budget "Wild Wild West", and played a supporting role in Kevin Smith's "Dogma". In 2000 Hayek had an uncredited acting part opposite Benicio del Toro in "Traffic". In 2003, she reprised her role from "Desperado" by appearing in "Once Upon a Time in Mexico", the final film of the "Mariachi Trilogy".
Director, producer and actress.
Around 2000, Hayek founded film production company Ventanarosa, through which she produces film and television projects. Her first feature as a producer was 1999's "El Coronel No Tiene Quien Le Escriba", Mexico's official selection for submission for Best Foreign Film at the Oscars.
"Frida", co-produced by Hayek, was released in 2002. Starring Hayek as Frida Kahlo, and Alfred Molina as her unfaithful husband, Diego Rivera, the film was directed by Julie Taymor and featured an entourage of stars in supporting and minor roles (Valeria Golino, Ashley Judd, Edward Norton, Geoffrey Rush) and cameos (Antonio Banderas). She earned a Best Actress Academy Award nomination for her performance.
"In the Time of the Butterflies" is a 2001 feature film based on the Julia Álvarez book of the same name, covering the lives of the Mirabal sisters. In the movie, Salma Hayek plays one of the sisters, Minerva, and Edward James Olmos plays the Dominican dictator Rafael Leónidas Trujillo whom the sisters opposed.
In 2003, Hayek produced and directed "The Maldonado Miracle", a Showtime movie based on the book of the same name, winning her a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special. In December 2005, she directed a music video for Prince, titled "Te Amo Corazon" ("I love you, sweetheart") that featured Mía Maestro.
Hayek was an executive producer of "Ugly Betty", a television series that aired around the world from 2006 to 2010. Hayek adapted the series for American television with Ben Silverman, who acquired the rights and scripts from the Colombian telenovela "Yo Soy Betty La Fea" in 2001. Originally intended as a half-hour sitcom for NBC in 2004, the project would later be picked up by ABC for the 2006–2007 season with Silvio Horta also producing. Hayek guest-starred on "Ugly Betty" as Sofia Reyes, a magazine editor. She also had a cameo playing an actress in the telenovela within the show. The show won a Golden Globe Award for Best Comedy Series in 2007. Hayek's performance as Sofia resulted in a nomination for Outstanding Guest Actress in a Comedy Series at the 59th Primetime Emmy Awards.
In April 2007, Hayek finalized negotiations with MGM to become the CEO of her own Latin-themed film production company, Ventanarosa. The following month, she signed a two-year deal with ABC for Ventanarosa to develop projects for the network.
Hayek played the wife of Adam Sandler's character in the buddy comedy "Grown Ups", which also co-starred Chris Rock and Kevin James. At his insistence, Hayek co-starred with Antonio Banderas in the "Shrek" spin-off film "Puss in Boots" as the voice of the character Kitty Softpaws, who serves as Puss's female counterpart and love interest. In 2012, Hayek directed Jada Pinkett Smith in the music video "Nada se compara." She reprised her role in "Grown Ups 2", which was released in July 2013.
Singing credits.
Hayek has been credited as a song performer on her own in three movies, and as a duet partner in a fourth.
Promotional work.
Hayek has been a spokeswoman for Avon cosmetics since February 2004. She was a spokeswoman for Revlon in 1998. In 2001, she modeled for Chopard and was featured in 2006 Campari adverts, photographed by Mario Testino. On April 3, 2009, she helped introduce La Doña, a watch by Cartier inspired by fellow Mexican actress María Félix.
Hayek has worked with the Procter & Gamble Company and UNICEF to promote the funding (through disposable diaper sales) of vaccines against maternal and neonatal tetanus. She is a global spokesperson for the Pampers/UNICEF "partnership" 1 Pack = 1 Vaccine to help raise awareness of the program. This "partnership" involves Procter & Gamble donating the cost of one tetanus vaccination (approximately 24 cents) for every pack of Pampers sold. (Pampers diapers cost approximately 25 cents each, or about US $1,000 per child per year in the US market).
In 2011, Hayek launched her own line of cosmetics, skincare, and haircare products called Nuance by Salma Hayek, to be sold at CVS stores in North America. She was inspired to create a cosmetic line from her grandmother, who used to make her own facial care products.
Hayek was also featured in a series of Spanish language commercials for Lincoln cars.
In art.
In spring 2006, the Blue Star Contemporary Art Center in San Antonio, Texas displayed sixteen portrait paintings by muralist George Yepes and filmmaker Robert Rodriguez of Hayek as Aztec goddess Itzpapalotl.
Personal life.
Hayek is a naturalized United States citizen. She studied at Ramtha's School of Enlightenment and is a practitioner of yoga. Hayek, who was raised Catholic, said she is not very devout anymore and does not believe in the institution , but still believes in Jesus Christ and God.
On March 9, 2007, Hayek confirmed her engagement to French billionaire and Kering CEO, François-Henri Pinault, as well as her pregnancy. She gave birth to daughter, Valentina Paloma Pinault, in September 2007 at Cedars-Sinai Medical Center in Los Angeles, California. They were married on Valentine's Day 2009 in Paris. 
On April 25, 2009, they had a second ceremony in Venice.
In July 2011, Hayek's husband was named in a paternity case. According to reports, Pinault is the father of supermodel Linda Evangelista's son, Augustin James, who was born in October 2006. He denied all allegations, although he later reached a settlement with Evangelista.
Activism.
Hayek's charitable work includes increasing awareness on violence against women and discrimination against immigrants. On July 19, 2005, Hayek testified before the U.S. Senate Committee on the Judiciary supporting reauthorizing the Violence Against Women Act. In February 2006, she donated $25,000 to a Coatzacoalcos, Mexico, shelter for battered women and another $50,000 to Monterrey based anti-domestic violence groups. Hayek is a board member of V-Day, the charity founded by playwright Eve Ensler. Nonetheless, Hayek has stated that she is not a feminist.
Hayek also advocates breastfeeding. During a UNICEF fact-finding trip to Sierra Leone, she breastfed a hungry week-old baby whose mother could not produce milk. She said she did it to reduce the stigma associated with breastfeeding and to encourage infant nutrition.
In 2010 Hayek's humanitarian work earned her a nomination for the VH1 Do Something Awards. In 2013 Hayek launched with Beyoncé Knowles and Frida Giannini a Gucci campaign, "Chime For Change", that aims to spread female empowerment.
For International Women's Day 2014 Hayek is one of the artist signatories of Amnesty International's letter to UK Prime Minister David Cameron campaigning for women's rights in Afghanistan. Following his visit to Lebanon in 2015, she criticised the discrimination against women there.
Honors and awards.
In July 2007, "The Hollywood Reporter" ranked Hayek fourth in their inaugural Latino Power 50, a list of the most powerful members of the Hollywood Latino community. That same month, a poll found Hayek to be the "sexiest celebrity" out of a field of 3,000 celebrities (male and female); according to the poll, "65 percent of the U.S. population would use the term 'sexy' to describe her". In 2008, she was awarded the Women in Film Lucy Award in recognition of her excellence and innovation in her creative works that have enhanced the perception of women through the medium of television In December of that year, "Entertainment Weekly" ranked Hayek number 17 in their list of the "25 Smartest People in TV."

</doc>
<doc id="27718" url="https://en.wikipedia.org/wiki?curid=27718" title="Super Bowl">
Super Bowl

The Super Bowl is the annual championship game of the National Football League (NFL), the highest level of professional American football in the world. The game culminates a season that begins in the late summer of the previous calendar year. Normally, Roman numerals are used to identify each game, rather than the year in which it is held. For example, Super Bowl I was played on January 15, 1967, following the 1966 regular season. The single exception to this rule is Super Bowl 50, which was played on February 7, 2016, following the 2015 regular season. The next game, Super Bowl LI, scheduled for February 5, 2017, will follow the 2016 regular season.
The game was created as part of a merger agreement between the NFL and its then-rival league, the American Football League (AFL). It was agreed that the two leagues' champion teams would play in the AFL–NFL World Championship Game until the merger was to officially begin in 1970. After the merger, each league was redesignated as a "conference", and the game has since been played between the conference champions to determine the NFL's league champion. Currently, the National Football Conference (NFC) leads the league with 26 wins to 24 wins for the American Football Conference (AFC). The Pittsburgh Steelers have the most Super Bowl victories with six.
The day on which the Super Bowl is played, now considered by some an unofficial American national holiday, is called "Super Bowl Sunday". It is the second-largest day for U.S. food consumption, after Thanksgiving Day. In addition, the Super Bowl has frequently been the most-watched American television broadcast of the year; the four most-watched broadcasts in U.S. television history are Super Bowls. In 2015, Super Bowl XLIX became the most-watched American television program in history with an average audience of 114.4 million viewers, the fifth time in six years the game had set a record, starting with the 2010 Super Bowl, which itself had taken over the number-one spot held for 27 years by the final episode of "M*A*S*H". The Super Bowl is also among the most-watched sporting events in the world, almost all audiences being North American, and is second to soccer's UEFA Champions League final as the most watched "annual" sporting event worldwide.
The NFL restricts the use of its "Super Bowl" trademark; it is frequently called the Big Game or other generic terms by non-sponsoring corporations. Because of the high viewership, commercial airtime during the Super Bowl broadcast is the most expensive of the year, leading to companies regularly developing their most expensive advertisements for this broadcast. As a result, watching and discussing the broadcast's commercials has become a significant aspect of the event. In addition, popular singers and musicians including Michael Jackson, Madonna, Prince, Beyoncé, The Rolling Stones, The Who, and Whitney Houston have performed during the event's pre-game and halftime ceremonies.
Origin.
For four decades after its 1920 inception, the NFL successfully fended off several rival leagues. However, in 1960, it encountered its most serious competitor when the American Football League (AFL) was formed. The AFL vied heavily with the NFL for both players and fans, but by the middle of the decade the strain of competition led to serious merger talks between the two leagues. Prior to the 1966 season, the NFL and AFL reached a merger agreement that was to take effect for the 1970 season. As part of the merger, the champions of the two leagues agreed to meet in a world championship game for professional American football until the merger was effected.
A bowl game is a post-season college football game. The original "bowl game" was the Rose Bowl Game in Pasadena, California, which was first played in 1902 as the "Tournament East-West football game" as part of the Pasadena Tournament of Roses and moved to the new Rose Bowl Stadium in 1923. The stadium got its name from the fact that the game played there was part of the Tournament of Roses and that it was shaped like a bowl, much like the Yale Bowl in New Haven, Connecticut; the Tournament of Roses football game itself eventually came to be known as the Rose Bowl Game. Exploiting the Rose Bowl Game's popularity, post-season college football contests were created for Miami (the Orange Bowl), New Orleans (the Sugar Bowl), and El Paso, Texas (the Sun Bowl) in 1935, and for Dallas (the Cotton Bowl) in 1937. By the time the first Super Bowl was played, the term "bowl" for any major American football game was well established.
Lamar Hunt, owner of the AFL's Kansas City Chiefs, first used the term "Super Bowl" to refer to the NFL-AFL championship game in the merger meetings. Hunt later said the name was likely in his head because his children had been playing with a Super Ball toy; a vintage example of the ball is on display at the Pro Football Hall of Fame in Canton, Ohio. In a July 25, 1966, letter to NFL commissioner Pete Rozelle, Hunt wrote, "I have kiddingly called it the 'Super Bowl,' which obviously can be improved upon."
The leagues' owners chose the name "AFL-NFL Championship Game", but in July 1966 the "Kansas City Star" quoted Hunt in discussing "the Super Bowl — that's my term for the championship game between the two leagues", and the media immediately began using the term. Although the league stated in 1967 that "not many people like it", asking for suggestions and considering alternatives such as 'Merger Bowl' and 'The Game', the Associated Press reported that 'Super Bowl' "grew and grew and grew-until it reached the point that there was Super Week, Super Sunday, Super Teams, Super Players, ad infinitum". 'Super Bowl' became official beginning with the third annual game. Roman numerals were first affixed for the fifth edition, in January 1971.
After the NFL's Green Bay Packers won the first two Super Bowls, some team owners feared for the future of the merger. At the time, many doubted the competitiveness of AFL teams compared with their NFL counterparts, though that perception changed when the AFL's New York Jets defeated the NFL's Baltimore Colts in Super Bowl III in Miami. One year later, the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV in New Orleans, which was the final AFL-NFL World Championship Game played before the merger. Beginning with the 1970 season, the NFL realigned into two conferences; the former AFL teams plus three NFL teams (the Colts, Pittsburgh Steelers, and Cleveland Browns) would constitute the American Football Conference (AFC), while the remaining NFL clubs would form the National Football Conference (NFC). The champions of the two conferences would play each other in the Super Bowl.
The winning team receives the Vince Lombardi Trophy, named after the coach of the Green Bay Packers, who won the first two Super Bowl games and three of the five preceding NFL championships in 1961, 1962, and 1965. Following Lombardi's death in September 1970, the trophy was named the Vince Lombardi Trophy, and was the first awarded as such to the Baltimore Colts following their win in Super Bowl V in Miami.
Date.
The Super Bowl is currently played on the first Sunday in February. This is due to the NFL current schedule which consists of the opening weekend of the season being held immediately after Labor Day (the first Monday in September), the 17-week regular season (where teams each play 16 games and have one bye), the first three rounds of the playoffs, and the Super Bowl two weeks after the two Conference Championship Games. This schedule has been in effect since Super Bowl XXXVIII in February 2004. The date of the Super Bowl can thus be determined from the date of the preceding Labor Day. For example, Labor Day 2015 was September 7: therefore the next Super Bowl is scheduled exactly five months later on February 7, 2016.
Originally, the game took place in early to mid-January. For Super Bowl I there was only one round of playoffs: the pre-merger NFL and AFL Championship Games. The addition of two playoff rounds (first in 1967 and then in 1978), an increase in regular season games from 14 to 16 (1978), and the establishment of one bye-week per team (1990) have caused the Super Bowl to be played later. Partially offsetting these season-lengthening effects, simultaneous with the addition of two regular season games in 1978, the season was started earlier. Prior to 1978 the season started as late as September 21. Now, since Labor Day is always the first Monday of September, September 13 is the latest possible date for the first full Sunday set of games (Since 2002, the regular season has started with the Kickoff Game on the Thursday after Labor Day).
Game history.
The Pittsburgh Steelers have won six Super Bowls, the most of any team; the Dallas Cowboys and San Francisco 49ers have five victories each, while the Green Bay Packers, New York Giants and New England Patriots have four Super Bowl championships. Thirteen other NFL franchises have won at least one Super Bowl. Nine teams have appeared in Super Bowl games without a win. The Minnesota Vikings were the first team to have appeared a record four times without a win. The Buffalo Bills played in a record four Super Bowls in a row, and lost every one. Four teams (the Cleveland Browns, Detroit Lions, Jacksonville Jaguars, and Houston Texans) have never appeared in a Super Bowl. The Browns and Lions both won NFL Championships prior to the Super Bowl's creation, while the Jaguars (1995) and Texans (2002) are both recent NFL expansion teams. The Minnesota Vikings won the last NFL Championship before the merger, but lost to the AFL champion Kansas City Chiefs in Super Bowl IV.
1960s: Early history.
The Green Bay Packers won the first two Super Bowls, defeating the Kansas City Chiefs and Oakland Raiders following the 1966 and 1967 seasons, respectively. The Packers were led by quarterback Bart Starr, who was named the Most Valuable Player (MVP) for both games. These two championships, coupled with the Packers' NFL championships in , , and , amount to the most successful stretch in NFL History; five championships in seven years.
In Super Bowl III, the AFL's New York Jets defeated the eighteen-point favorite Baltimore Colts of the NFL, 16–7. The Jets were led by quarterback Joe Namath (who had famously guaranteed a Jets win prior to the game) and former Colts head coach Weeb Ewbank, and their victory proved that the AFL was the NFL's competitive equal. This was reinforced the following year, when the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV.
1970s: Dominant franchises.
After the AFL–NFL merger was completed in 1970, three franchises – the Dallas Cowboys, Miami Dolphins, and Pittsburgh Steelers – would go on to dominate the 1970s, winning a combined eight Super Bowls in the decade.
The Baltimore Colts, now a member of the AFC, would start the decade by defeating the Cowboys in Super Bowl V, a game which is notable as being the only Super Bowl to date in which a player from the losing team won the Super Bowl MVP (Cowboys' linebacker Chuck Howley). Beginning with this Super Bowl, all Super Bowls have served as the NFL's league championship game.
The Cowboys, coming back from a loss the previous season, won Super Bowl VI over the Dolphins. However, this would be the Dolphins' final loss in over a year, as the next year, the Dolphins would go 14–0 in the regular season and eventually win all of their playoff games, capped off with a 14-7 victory in Super Bowl VII, becoming the first and only team to finish an entire perfect regular and post season. The Dolphins would repeat as league champions by winning Super Bowl VIII a year later.
In the late 1970s, the Steelers became the first NFL dynasty of the post-merger era by winning four Super Bowls (IX, X, XIII, and XIV) in six years. They were led by head coach Chuck Noll, the play of offensive stars Terry Bradshaw, Franco Harris, Lynn Swann, John Stallworth, and Mike Webster, and their dominant "Steel Curtain" defense, led by "Mean" Joe Greene, L.C. Greenwood, Ernie Holmes, Mel Blount, Jack Ham, and Jack Lambert. The coaches and administrators also were part of the dynasty's greatness as evidenced by the team's "final pieces" being part of the famous 1974 draft. The selections in that class have been considered the best by any pro franchise ever, as Pittsburgh selected four future Hall of Famers, the most for any team in any sport in a single draft. The Steelers were the first team to win three and then four Super Bowls and appeared in six AFC Championship Games during the decade, making the playoffs in eight straight seasons. Nine players and three coaches and administrators on the team have been inducted into the Pro Football Hall of Fame. Pittsburgh still remains the only team to win back-to-back Super Bowls twice and four Super Bowls in a six-year period.
The Steelers' dynasty was interrupted only by the Cowboys winning their second Super Bowl of the decade and the Oakland Raiders' Super Bowl XI win.
1980s and 1990s: The NFC's winning streak.
In the 1980s and 1990s, the tables turned for the AFC, as the NFC dominated the Super Bowls of the new decade and most of those of the 1990s. The NFC won 16 of the 20 Super Bowls during these two decades, including 13 straight from Super Bowl XIX to Super Bowl XXXI.
The most successful team of the 1980s was the San Francisco 49ers, which featured the West Coast offense of Hall of Fame head coach Bill Walsh. This offense was led by three-time Super Bowl MVP and Hall of Fame quarterback Joe Montana, Super Bowl MVP and Hall of Fame wide receiver Jerry Rice, and tight end Brent Jones. Under their leadership, the 49ers won four Super Bowls in the decade (XVI, XIX, XXIII, and XXIV) and made nine playoff appearances between 1981 and 1990, including eight division championships, becoming the second dynasty of the post-merger NFL.
The 1980s also produced the 1985 Chicago Bears, who posted an 18–1 record under head coach Mike Ditka; colorful quarterback Jim McMahon; and Hall of Fame running back Walter Payton. Their team won Super Bowl XX in dominating fashion. The Washington Redskins and New York Giants were also top teams of this period; the Redskins won Super Bowls XVII, XXII, and XXVI. The Giants claimed Super Bowls XXI and XXV. As in the 1970s, the Oakland Raiders were the only team to interrupt the Super Bowl dominance of other teams; they won Super Bowls XV and XVIII (the latter as the Los Angeles Raiders).
Following several seasons with poor records in the 1980s, the Dallas Cowboys rose back to prominence in the 1990s. During this decade, the Cowboys made post-season appearances every year except for the seasons of 1990 and 1997. From 1992 to 1996, the Cowboys won their division championship each year. In this same period, the Buffalo Bills had made their mark reaching the Super Bowl for a record four consecutive years, only to lose all four. After Super Bowl championships by division rivals New York (1990) and Washington (1991), the Cowboys won three of the next four Super Bowls (XXVII, XXVIII, and XXX) led by quarterback Troy Aikman, running back Emmitt Smith, and wide receiver Michael Irvin. All three of these players went to the Hall of Fame. The Cowboys' streak was interrupted by the 49ers, who won their league-leading fifth title overall with Super Bowl XXIX in dominating fashion under Super Bowl MVP and Hall of Fame quarterback Steve Young, Hall of Fame wide receiver Jerry Rice, and Hall of Fame cornerback Deion Sanders; however, the Cowboys' victory in Super Bowl XXX the next year also gave them five titles overall and they did so with Deion Sanders after he won the Super Bowl the previous year with the San Francisco 49ers. The NFC's winning streak was continued by the Green Bay Packers who, under quarterback Brett Favre, won Super Bowl XXXI, their first championship since Super Bowl II in the late 1960s.
1997–2009: AFC resurgence.
Super Bowl XXXII saw quarterback John Elway and running back Terrell Davis lead the Denver Broncos to an upset victory over the defending champion Packers, snapping the NFC's 13 year winning streak. The following year, the Broncos defeated the Atlanta Falcons in Super Bowl XXXIII, Elway's fifth Super Bowl appearance, his second NFL championship, and his final NFL game. The back-to-back victories heralded a change in momentum in which AFC teams would win 10 out of 13 Super Bowls. In the years between 2001 and 2011, three teams – the Patriots, Steelers, and Colts – accounted for ten of the AFC Super Bowl appearances, with those same teams often meeting each other earlier in the playoffs. In contrast, the NFC saw a different representative in the Super Bowl every season from 2001 through 2010.
The year following the Denver Broncos' second victory, however, a surprising St. Louis Rams led by undrafted quarterback Kurt Warner would close out the 1990s in a wild battle against the Tennessee Titans in Super Bowl XXXIV. The tense game came down to the final play in which Tennessee had the opportunity to tie the game and send it to overtime. The Titans nearly pulled it off, but the tackle of receiver Kevin Dyson by linebacker Mike Jones kept the ball out of the end zone by a matter of inches. In 2007, ESPN would rank "The Tackle" as the 2nd greatest moment in Super Bowl history.
Super Bowl XXXV was played by the AFC's Baltimore Ravens and the NFC's New York Giants. The Ravens defeated the Giants by the score of 34–7. The game was played on January 28, 2001, at Raymond James Stadium in Tampa, Florida.
The New England Patriots became the dominant team throughout the early 2000s, winning the championship three out of four years early in the decade. They would become only the second team in the history of the NFL to do so (after the 1990s Dallas Cowboys). In Super Bowl XXXVI, first-year starting quarterback Tom Brady led his team to a 20–17 upset victory over the St. Louis Rams. Brady would go on to win the MVP award for this game. The Patriots also won Super Bowls XXXVIII and XXXIX defeating the Carolina Panthers and the Philadelphia Eagles respectively. This four-year stretch of Patriot dominance was interrupted by the Tampa Bay Buccaneers' 48-21 Super Bowl XXXVII victory over the Oakland Raiders.
The Pittsburgh Steelers and Indianapolis Colts continued the era of AFC dominance by winning Super Bowls XL and XLI in 2005-06 and 2006–07, respectively defeating the Seattle Seahawks and Chicago Bears.
In the 2007 season, the Patriots became the second team in NFL history to have a perfect regular season record, after the 1972 Miami Dolphins, and the first to finish 16–0. They easily marched through the AFC playoffs and were heavy favorites in Super Bowl XLII. However, they lost that game to Eli Manning and the New York Giants 17–14, leaving the Patriots' 2007 record at 18-1.
The following season, the Steelers logged their record sixth Super Bowl title (XLIII) in a 27-23, final-minute victory against the Arizona Cardinals.
2010–present: The NFC re-emerges.
The 2010s have seen a return to dominance by NFC teams. Between 2010 and 2016, four of the seven Super Bowl winners hailed from the NFC.
The Giants won another title after the 2011 season, again defeating the Patriots in Super Bowl XLVI. Prior to that Super Bowl victory, the New Orleans Saints won their first (XLIV) by defeating the Indianapolis Colts in February 2010, and the Green Bay Packers won their fourth Super Bowl (XLV) and record thirteenth NFL championship overall by defeating the Pittsburgh Steelers in February 2011.
The Baltimore Ravens snapped the NFC's three-game winning streak by winning Super Bowl XLVII in a 34-31 nail-biter over the San Francisco 49ers.
Super Bowl XLVIII, played at New Jersey's MetLife Stadium in February 2014, was the first Super Bowl held outdoors in a cold weather environment. The Seattle Seahawks won their first NFL title with a 43-8 defeat of the Denver Broncos, in a highly touted matchup that pitted Seattle's top-ranked defense against a Peyton-Manning-led Denver offense that had broken the NFL's single-season scoring record.
In Super Bowl XLIX, the New England Patriots, the AFC champions, beat the NFC and defending Super Bowl champions, the Seattle Seahawks.
In Super Bowl 50, the Denver Broncos, led by the league's top-ranked defense, defeated the Carolina Panthers, who had the league's top-ranked offense, in what became the final game of quarterback Peyton Manning's career.
The Super Bowls of the 2000s and early 2010s are notable for the performances (and the pedigrees) of several of the participating quarterbacks. During that era, Tom Brady (six Super Bowl appearances, four wins), Ben Roethlisberger (three appearances, two wins), Peyton Manning (four appearances, two wins), Eli Manning (two appearances, two wins), Kurt Warner (three appearances, one win), Drew Brees (one appearance, one win), Aaron Rodgers (one appearance, one win), Joe Flacco (one appearance, one win), and Russell Wilson (two appearances, one win) have all added Super Bowl championships to their lists of individual accomplishments.
Television coverage and ratings.
The Super Bowl is one of the most watched annual sporting events in the world. The only other annual events that gather more viewers are the UEFA Champions League final, and "El Clásico" in Spain. For many years, the Super Bowl has possessed a large US and global television viewership, and it is often the most watched United States originating television program of the year. The game tends to have high Nielsen television ratings, which is usually around a 40 rating and 60 share. This means that on average, more than 100 million people from the United States alone are tuned into the Super Bowl at any given moment.
In press releases preceding each year's event, the NFL typically claims that that year's Super Bowl will have a potential worldwide audience of around one billion people in over 200 countries. This figure refers to the number of people "able" to watch the game, not the number of people "actually" watching. However the statements have been frequently misinterpreted in various media as referring to the latter figure, leading to a common misperception about the game's actual global audience. The New York-based media research firm Initiative measured the global audience for the 2005 Super Bowl at 93 million people, with 98 percent of that figure being viewers in North America, which meant roughly 2 million people outside North America watched the Super Bowl that year.
2015's Super Bowl XLIX holds the record for total number of U.S. viewers, with a final number of 114.4 million, making the game the most-viewed television broadcast of any kind in American history. The halftime show was the most watched ever with 118.5 million viewers tuning in, and an all-time high of 168 million viewers in the United States had watched several portions of the Super Bowl 2015 broadcast. The game set a record for total viewers for the fifth time in six years.
The highest-rated game according to Nielsen was Super Bowl XVI in 1982, which was watched in 49.1 percent of households (73 share), or 40,020,000 households at the time. Ratings for that game, a San Francisco victory over Cincinnati, may have been aided by a large blizzard that had affected much of the northeastern United States on game day, leaving residents to stay at home more than usual. Super Bowl XVI still ranks fourth on Nielsen's list of top-rated programs of all time, and three other Super Bowls, XII, XVII, and XX, made the top ten.
Famous commercial campaigns include the Budweiser "Bud Bowl" campaign, the 1984 introduction of Apple's MacIntosh computer, and the 1999 and 2000 dot-com ads. As the television ratings of the Super Bowl have steadily increased over the years, prices have also increased every year, with advertisers paying as much as $3.5 million for a thirty-second spot during Super Bowl XLVI in 2012. A segment of the audience tunes into the Super Bowl solely to view commercials. In 2010, Nielsen reported that 51 percent of Super Bowl viewers tune in for the commercials. The Super Bowl halftime show has spawned another set of alternative entertainment such as the Lingerie Bowl, the Beer Bottle Bowl, and others.
Since 1991, the Super Bowl has begun between 6:19 and 6:40 PM EST so that most of the game is played during the primetime hours on the East Coast.
Super Bowl on TV.
Note: Years listed are the year the game was actually played "(will be played)" rather than what NFL season it is considered to have been.
<br>
Super Bowls I–VI were blacked out in the television markets of the host cities, due to league restrictions then in place.
Lead-out programming.
The Super Bowl provides an extremely strong lead-in to programming following it on the same channel, the effects of which can last for several hours. For instance, in discussing the ratings of a local TV station, Buffalo television critic Alan Pergament noted on the coattails from Super Bowl XLVII, which aired on CBS: "A paid program that ran on Channel 4 (WIVB-TV) at 2:30 in the morning had a 1.3 rating. That’s higher than some CW prime time shows get on WNLO-TV, Channel 4’s sister station."
Because of this strong coattail effect, the network that airs the Super Bowl typically takes advantage of the large audience to air an episode of a hit series, or to premiere the pilot of a promising new one in the lead-out slot, which immediately follows the Super Bowl and post-game coverage.
Entertainment.
Early Super Bowls featured a halftime show consisting of marching bands from local colleges or high schools; but as the popularity of the game increased, a trend where popular singers and musicians performed during its pre-game ceremonies and the halftime show, or simply sang the national anthem of the United States, emerged. Unlike regular season or playoff games, thirty minutes are allocated for the Super Bowl halftime. The first halftime show to have featured only one star performer was Super Bowl XXVII in 1993, at which Michael Jackson performed. The NFL specifically went after him to increase viewership and to continue expanding the Super Bowl's realm. Sports bloggers have ranked Jackson's appearance as the No. 1 Super Bowl halftime show since its inception. Another notable performance came during Super Bowl XXXVI in 2002, when U2 performed; during their third song, "Where the Streets Have No Name", the band played under a large projection screen which scrolled through names of the victims of the September 11 attacks.
Whitney Houston's performance of the national anthem at Super Bowl XXV in 1991, during the Gulf War, has been regarded as one of the best renditions of the anthem in history. Her performance was released as a single on February 12, 1991, and appeared on the album "".
The halftime show of Super Bowl XXXVIII in 2004 generated controversy when Justin Timberlake removed a piece of Janet Jackson's top, exposing her right breast with a star-shaped pastie around the nipple. Timberlake and Jackson have maintained that the incident was accidental, calling it a "wardrobe malfunction". The game was airing live on CBS, and MTV had produced the halftime show. Immediately after the moment, the footage jump-cut to a wide-angle shot and went to a commercial break; however, video captures of the moment in detail circulated quickly on the internet. The NFL, embarrassed by the incident, permanently banned MTV from conducting future halftime shows. This also led to the FCC tightening controls on indecency and fining CBS and CBS-owned stations a total of $550,000 for the incident. The fine was later reversed in July 2008. CBS and MTV eventually split into two separate companies in part because of the fiasco, with CBS going under the control of CBS Corporation and MTV falling under the banner of Viacom (although both corporations remain under the ownership of National Amusements). For six years following the incident, all of the performers in Super Bowl halftime shows were artists associated with the classic rock genre of the 1970s and 1980s (including three acts from the British Invasion of the 1960s), with only one act playing the entire halftime show. Paul McCartney (formerly of The Beatles) played Super Bowl XXXIX in 2005, The Rolling Stones played Super Bowl XL in 2006, and The Who played Super Bowl XLIV in 2010. The halftime show returned to a modern act in 2011 with The Black Eyed Peas. But during the halftime show of Super Bowl XLVI in 2012, M.I.A. gave the middle finger during a performance of "Give Me All Your Luvin'" with Madonna, which was caught by TV cameras. An attempt to censor the gesture by blurring the entire screen came late.
Excluding Super Bowl XXXIX, the famous "I'm going to Disney World!" advertising campaign took place at every Super Bowl since Super Bowl XXI, when quarterback Phil Simms from the New York Giants became the first player to say the tagline.
Venue.
As of Super Bowl XLVIII, 27 of 49 Super Bowls have been played in three cities: New Orleans (ten times), the Greater Miami area (ten times), and the Greater Los Angeles area (seven times). No market or region without an NFL franchise has ever hosted a Super Bowl, and the presence of an NFL team in a market or region is now a "de jure" requirement for bidding on the game. The winning market is not, however, required to host the Super Bowl in the same stadium that its NFL team uses, and nine Super Bowls have been held in a stadium other than the one the NFL team in that city was using at the time. Los Angeles's last five Super Bowls were all played at the Rose Bowl.
No team has ever played the Super Bowl in its home stadium. Two teams have played the Super Bowl in their home market: the San Francisco 49ers, who played Super Bowl XIX in Stanford Stadium instead of Candlestick Park; and the Los Angeles Rams, who played Super Bowl XIV in the Rose Bowl instead of the Los Angeles Memorial Coliseum. In both cases, the stadium in which the Super Bowl was held was perceived to be a better stadium for a large, high-profile event than the stadiums the Rams and 49ers were playing in at the time; this situation has not arisen since 1993, in part because the league has traditionally awarded the Super Bowl in modern times to the newest stadiums. Besides those two, the only other Super Bowl venue that was not the home stadium to an NFL team at the time was Rice Stadium in Houston: the Houston Oilers had played there previously, but moved to the Astrodome several years prior to Super Bowl VIII. The Orange Bowl was the only AFL stadium to host a Super Bowl and the only stadium to host consecutive Super Bowls, hosting Super Bowls II and III.
Traditionally, the NFL does not award Super Bowls to stadiums that are located in climates with an expected average daily temperature less than 50 °F (10 °C) on game day unless the field can be completely covered by a fixed or retractable roof. Five Super Bowls have been played in northern cities: two in the Detroit area—Super Bowl XVI at Pontiac Silverdome in Pontiac, Michigan and Super Bowl XL at Ford Field in Detroit, one in Minneapolis—Super Bowl XXVI, one in Indianapolis at Lucas Oil Stadium for Super Bowl XLVI, and one in the New York area—Super Bowl XLVIII at MetLife Stadium. Only MetLife Stadium did not have a roof (be it fixed or retractable) but it was still picked as the host stadium for Super Bowl XLVIII in an apparent waiver of the warm-climate rule. A sixth Super Bowl is planned in a northern city as Minneapolis has been picked to host Super Bowl LII in 2018 in the under-construction U.S. Bank Stadium.
There have been a few instances where the league has rescinded the Super Bowl from cities. Super Bowl XXVII in 1993 was originally awarded to Sun Devil Stadium in Tempe, Arizona, but after Arizona voters elected not to recognize Martin Luther King, Jr. Day as a paid state-employee's holiday in 1990, the NFL moved the game to the Rose Bowl in Pasadena, California. When voters in Arizona opted to create such a legal holiday in 1992, Super Bowl XXX in 1996 was awarded to Tempe. Super Bowl XXXIII was awarded first to Candlestick Park in San Francisco, but when plans to renovate the stadium fell through the game was moved to Pro Player Stadium in greater Miami. Super Bowl XXXVII was awarded to a new stadium not yet built in San Francisco, when that stadium failed to be built, the game was moved to San Diego. Super Bowl XLIV, slated for February 7, 2010, was withdrawn from New York City's proposed West Side Stadium, because the city, state, and proposed tenants New York Jets could not agree on funding. Super Bowl XLIV was then eventually awarded to Sun Life Stadium in Miami Gardens, Florida. And Super Bowl XLIX in 2015 was originally given to Arrowhead Stadium in Kansas City, Missouri, but after two sales taxes failed to pass at the ballot box, and opposition by local business leaders and politicians increased, Kansas City eventually withdrew its request to host the game. Super Bowl XLIX was then eventually awarded to University of Phoenix Stadium in Glendale, Arizona.
In 2011, Texas Attorney General Greg Abbott said, "It's commonly known as the single largest human trafficking incident in the United States." According to Forbes, 10,000 prostitutes were brought to Miami in 2010 for the Super Bowl. "Snopes" research in 2015 determined that the actual number of prostitutes involved in a typical Super Bowl weekend is less than 100, not statistically higher than any other time of the year, and that the notion of mass increases in human trafficking around the Super Bowl was a politician's myth.
Selection process.
The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game. Cities place bids to host a Super Bowl and are evaluated in terms of stadium renovation and their ability to host. In 2014, a document listing the specific requirements of Super Bowl hosts was leaked, giving a clear list of what was required for a Super Bowl host. Much of the cost of the Super Bowl is to be assumed by the host community, although some costs are enumerated within the requirements to be assumed by the NFL. Some of the host requirements include:
The NFL owners meet to make a selection on the site, usually three years prior to the event. In 2007, NFL commissioner Roger Goodell suggested that a Super Bowl might be played in London, England, perhaps at Wembley Stadium. The game has never been played in a region that lacks an NFL franchise; seven Super Bowls have been played in Los Angeles, but none since the Los Angeles Raiders and Los Angeles Rams relocated to Oakland and St. Louis respectively in 1995. New Orleans, the site of the 2013 Super Bowl, invested more than $1 billion in infrastructure improvements in the years leading up to the game.
Home team designation.
The designated "home team" alternates between the NFC team in odd-numbered games and the AFC team in even-numbered games. This alternation was initiated with the first Super Bowl, when the Green Bay Packers were the designated home team. Regardless of being the home or away team of record, each team has their team wordmark painted in one of the end zones. Designated away teams have won 29 of 50 Super Bowls to date (58 percent).
Since Super Bowl XIII in January 1979, the home team is given the choice of wearing their colored or white jerseys. Originally, the designated home team was specified to wear their colored jerseys, which resulted in Dallas donning their less familiar dark blue jerseys for Super Bowl V. While most of the home teams in the Super Bowl have chosen to wear their colored jerseys, there have been five exceptions: the Cowboys during Super Bowl XIII and XXVII, the Washington Redskins during Super Bowl XVII, the Pittsburgh Steelers during Super Bowl XL, and the Denver Broncos in Super Bowl 50. The Cowboys, since , and Redskins, with the arrival of coach Joe Gibbs in , have traditionally worn white jerseys at home. Meanwhile, the Steelers, who have always worn their black jerseys at home since the AFL-NFL merger in , opted for the white jerseys after winning three consecutive playoff games on the road, wearing white. The Steelers' decision was compared with the New England Patriots in Super Bowl XX; the Patriots had worn white jerseys at home during the season, but after winning road playoff games against the New York Jets and Miami Dolphins wearing red jerseys, New England opted to switch to red for the Super Bowl as the designated home team. For the Broncos in Super Bowl 50, Denver general manager John Elway simply stated, "We've had Super Bowl success in our white uniforms"; they previously had been in Super Bowls when wearing their orange jerseys. The Broncos' decision is also perceived to be made out of superstition, losing all Super Bowl games with the orange jerseys in terrible fashion. White-shirted teams have won 32 of 50 Super Bowls to date (64 percent).
Host cities/regions.
Fifteen different regions have hosted Super Bowls.
Note: Years listed are the year the game was actually played "(will be played)" rather than what NFL season it is considered to have been.
Host stadiums.
A total of twenty-four different stadiums, five of which no longer exist and one of which does not yet exist, have hosted or are scheduled to host Super Bowls. Years listed in the table below are the years the game was actually played "(will be played)" rather than what NFL season it is considered to have been.
^ Stadium is now demolished.<br>
‡ Miami Gardens became a separate city in 2003. Prior to, the stadium had a Miami address.<br>
† The original Stanford Stadium, which hosted Super Bowl XIX, was demolished and replaced with a new stadium in 2006.<br>
The game has never been played in a region that lacked an NFL franchise. London, England has occasionally been mentioned as a host city for a Super Bowl in the near future. Wembley Stadium has hosted several NFL games as part of the NFL International Series and is specifically designed for large, individual events. NFL Commissioner Roger Goodell has openly discussed the possibility on different occasions. Time zone complications are a significant obstacle to a Super Bowl in London; a typical 6:30 p.m. Eastern Time start would result in the game beginning at 11:30 p.m. local time in London, an unusually late hour to be holding spectator sports (the NFL has never in its history started a game later than 9:15 p.m. local time). As bids have been submitted for all Super Bowls through Super Bowl LIV, the soonest that any stadium outside the NFL's footprint could serve as host would be Super Bowl LV in 2021.
Super Bowl trademark.
The NFL is very active on stopping what it says is unauthorized commercial use of its trademarked terms "NFL", "Super Bowl", and "Super Sunday". As a result, many events and promotions tied to the game, but not sanctioned by the NFL, are asked to refer to it with colloquialisms such as "The Big Game", or other generic descriptions. A radio spot for Planters nuts parodied this, by saying "it would be "super"...to have a "bowl"...of Planters nuts while watching the big game!" and comedian Stephen Colbert began referring to the game in 2014 as the "Superb Owl". In 2015, The NFL filed opposition with the USPTO Trademark Trial and Appeal Board to a trademark application submitted by an Arizona-based nonprofit for "Superb Owl". The NFL claims that the use of the phrase "Super Bowl" implies an NFL affiliation, and on this basis the league asserts broad rights to restrict how the game may be shown publicly; for example, the league says Super Bowl showings are prohibited in churches or at other events that "promote a message", while venues that do not regularly show sporting events cannot show the Super Bowl on any television screen larger than 55 inches. Some critics say the NFL is exaggerating its ownership rights by stating that "any use is prohibited", as this contradicts the broad doctrine of fair use in the United States. Legislation was proposed by Utah Senator Orrin Hatch in 2008 "to provide an exemption from exclusive rights in copyright for certain nonprofit organizations to display live football games", and "for other purposes".
In 2006, the NFL made an attempt to trademark "The Big Game" as well; however, it withdrew the application in 2007 due to growing commercial and public-relations opposition to the move, mostly from Stanford University and the University of California, Berkeley and their fans, as the Stanford Cardinal football and California Golden Bears football teams compete in the "Big Game", which has been played since 1892 (28 years before the formation of the NFL and 75 years before Super Bowl I). Additionally, the Mega Millions lottery game was known as The Big Game from 1996 to 2002.
Use of the phrase "world champions".
Like the other major professional leagues in the United States, the winner of the Super Bowl is usually declared "world champions", a title often mocked by non-Americans. Others feel the title is fitting, since it is the only professional league of its kind.
The practice by the U.S. major leagues of using the "World Champion" moniker originates from the World Series of professional baseball, and it was later used during the first three Super Bowls when they were referred to as AFL-NFL World Championship Games. The phrase is still engraved on the Super Bowl rings.

</doc>
<doc id="27725" url="https://en.wikipedia.org/wiki?curid=27725" title="Surface area">
Surface area

The surface area of a solid object is a measure of the total area that the surface of the object occupies. The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. This definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration. 
A general definition of surface area was sought by Henri Lebesgue and Hermann Minkowski at the turn of the twentieth century. Their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. An important example is the Minkowski content of a surface.
Definition.
While the areas of many simple surfaces have been known since antiquity, a rigorous mathematical "definition" of area requires a great deal of care. 
This should provide a function
which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. The most fundamental property of the surface area is its additivity: "the area of the whole is the sum of the areas of the parts". More rigorously, if a surface "S" is a union of finitely many pieces "S"1, …, "S""r" which do not overlap except at their boundaries, then 
Surface areas of flat polygonal shapes must agree with their geometrically defined area. Since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the group of Euclidean motions. These properties uniquely characterize surface area for a wide class of geometric surfaces called "piecewise smooth". Such surfaces consist of finitely many pieces that can be represented in the parametric form
with a continuously differentiable function formula_4 The area of an individual piece is defined by the formula 
Thus the area of "S""D" is obtained by integrating the length of the normal vector formula_6 to the surface over the appropriate region "D" in the parametric "uv" plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs "z" = "f"("x","y") and surfaces of revolution.
One of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by Hermann Schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area (Known as Schwarz's paradox.)
Various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by Henri Lebesgue and Hermann Minkowski. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of fractals. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. A specific example of such an extension is the Minkowski content of the surface.
Common formulas.
Ratio of surface areas of a sphere and cylinder of the same radius and height.
The below given formulas can be used to show that the surface area of a sphere and cylinder of the same radius and height are in the ratio 2 : 3, as follows.
Let the radius be "r" and the height be "h" (which is 2"r" for the sphere).
formula_7
The discovery of this ratio is credited to Archimedes.
In chemistry.
Surface area is important in chemical kinetics. Increasing the surface area of a substance generally increases the rate of a chemical reaction. For example, iron in a fine powder will combust, while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.
In biology.
The surface area of an organism is important in several considerations, such as regulation of body temperature and digestion. Animals use their teeth to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains microvilli, greatly increasing the area available for absorption. Elephants have large ears, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss. 
The surface area to volume ratio (SA:V) of a cell imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the cell membrane to interstitial spaces or to other cells. Indeed, representing a cell as an idealized sphere of radius "r", the volume and surface area are, respectively, "V" = 4/3 π "r"3; "SA" = 4 π "r"2. The resulting surface area to volume ratio is therefore 3/"r". Thus, if a cell has a radius of 1 μm, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 μm, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.

</doc>
<doc id="27727" url="https://en.wikipedia.org/wiki?curid=27727" title="Solid state">
Solid state

Solid state, or solid matter, is one of the four fundamental states of matter. It may also refer to:

</doc>
<doc id="27730" url="https://en.wikipedia.org/wiki?curid=27730" title="Serbo-Croatian">
Serbo-Croatian

Serbo-Croatian , also called Serbo-Croat , Serbo-Croat-Bosnian (SCB), Bosnian-Croatian-Serbian (BCS), or Bosnian-Croatian-Montenegrin-Serbian (BCMS), is a South Slavic language and the primary language of Serbia, Croatia, Bosnia and Herzegovina, and Montenegro. It is a pluricentric language with four mutually intelligible standard varieties.
South Slavic dialects historically formed a continuum. The turbulent history of the area, particularly due to expansion of the Ottoman Empire, resulted in a patchwork of dialectal and religious differences. Due to population migrations, Shtokavian became the most widespread in the western Balkans, intruding westwards into the area previously occupied by Chakavian and Kajkavian (which further blend into Slovenian in the northwest). Bosniaks, Croats and Serbs differ in religion and were historically often part of different cultural circles, although a large part of the nations have lived side by side under foreign overlords. During that period, the language was referred to under a variety of names, such as "Slavic", "Illyrian", or according to region, "Bosnian", "Serbian" and "Croatian", the latter often in combination with "Slavonian" or "Dalmatian".
Serbo-Croatian was standardized in the mid-19th-century Vienna Literary Agreement by Croatian and Serbian writers and philologists, decades before a Yugoslav state was established. From the very beginning, there were slightly different literary Serbian and Croatian standards, although both were based on the same Shtokavian subdialect, Eastern Herzegovinian. In the 20th century, Serbo-Croatian served as the official language of the Kingdom of Yugoslavia (when it was called "Serbo-Croato-Slovenian"), and later as one of the official languages of the Socialist Federal Republic of Yugoslavia. The breakup of Yugoslavia affected language attitudes, so that social conceptions of the language separated on ethnic and political lines. Since the breakup of Yugoslavia, Bosnian has likewise been established as an official standard in Bosnia and Herzegovina, and there is an ongoing movement to codify a separate Montenegrin standard. Serbo-Croatian thus generally goes by the ethnic names Serbian, Croatian, Bosnian, and sometimes Montenegrin and Bunjevac.
Like other South Slavic languages, Serbo-Croatian has a simple phonology, with the common five-vowel system and twenty-five consonants. Its grammar evolved from Common Slavic, with complex inflection, preserving seven grammatical cases in nouns, pronouns, and adjectives. Verbs exhibit imperfective or perfective aspect, with a moderately complex tense system. Serbo-Croatian is a pro-drop language with flexible word order, subject–verb–object being the default. It can be written in Serbian Cyrillic or Gaj's Latin alphabet, whose thirty letters mutually map one-to-one, and the orthography is highly phonemic in all standards.
Name.
Throughout the history of the South Slavs, the vernacular, literary, and written languages (e.g. Chakavian, Kajkavian, Shtokavian) of the various regions and ethnicities developed and diverged independently. Prior to the 19th century, they were collectively called "Illyric", "Slavic", "Slavonian", "Bosnian", "Dalmatian", "Serbian" or "Croatian". As such, the term "Serbo-Croatian" was first used by Jacob Grimm in 1824, popularized by the Vienna philologist Jernej Kopitar in the following decades, and accepted by Croatian Zagreb grammarians in 1854 and 1859. At that time, Serb and Croat lands were still part of the Ottoman and Austrian Empires. Officially, the language was called variously "Serbo-Croat, Croato-Serbian, Serbian and Croatian, Croatian and Serbian, Serbian or Croatian, Croatian or Serbian." Unofficially, Serbs and Croats typically called the language "Serbian" or "Croatian", respectively, without implying a distinction between the two, and again in independent Bosnia and Herzegovina, "Bosnian", "Croatian", and "Serbian" were considered to be three names of a single official language. Croatian linguist Dalibor Brozović advocated the term "Serbo-Croatian" as late as 1988, claiming that in an analogy with Indo-European, Serbo-Croatian does not only name the two components of the same language, but simply charts the limits of the region in which it is spoken and includes everything between the limits (‘Bosnian’ and ‘Montenegrin’). Today, use of the term "Serbo-Croatian" is controversial due to the prejudice that nation and language must match. It is still used for lack of a succinct alternative, though alternative names have been used, such as "Bosnian/Croatian/Serbian" (BCS), which is often seen in political contexts such as the Hague War Crimes tribunal.
History.
Early development.
Old Church Slavonic was adopted as the language of the liturgy. This language was gradually adapted to non-liturgical purposes and became known as the Croatian version of Old Slavonic. The two variants of the language, liturgical and non-liturgical, continued to be a part of the Glagolitic service as late as the middle of the 19th century. The earliest known Croatian Church Slavonic Glagolitic manuscripts are the "Glagolita Clozianus" and the "Vienna Folia" from the 11th century.
The beginning of written Serbo-Croatian can be traced from the 10th century and on when Serbo-Croatian medieval texts were written in five scripts: Latin, Glagolitic, Early Cyrillic, Bosnian Cyrillic ("bosančica/bosanica"), and Arebica, the last principally by Bosniak nobility. Serbo-Croatian competed with the more established literary languages of Latin and Old Slavonic in the west and Persian and Arabic in the east.
Old Slavonic developed into the Serbo-Croatian variant of Church Slavonic between the 12th and 16th centuries.
Among the earliest attestations of Serbo-Croatian are the Humac tablet, dating from the 10th or 11th century, written in Bosnian Cyrillic and Glagolitic; the Plomin tablet, dating from the same era, written in Glagolitic; the Valun tablet, dated to the 11th century, written in Glagolitic and Latin; and the Inscription of Župa Dubrovačka, a Glagolitic tablet dated to the 11th century.
The Baška tablet from the late 11th century was written in Glagolitic. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk that contains text written mostly in Chakavian in the Croatian script. It is also important in the history of the nation as it mentions Zvonimir, the king of Croatia at the time.
The Charter of Ban Kulin of 1189, written by Ban Kulin of Bosnia, was an early Shtokavian text, written in Bosnian Cyrillic.
The luxurious and ornate representative texts of Serbo-Croatian Church Slavonic belong to the later era, when they coexisted with the Serbo-Croatian vernacular literature. The most notable are the "Missal of Duke Novak" from the Lika region in northwestern Croatia (1368), "Evangel from Reims" (1395, named after the town of its final destination), Hrvoje's Missal from Bosnia and Split in Dalmatia (1404), and the first printed book in Serbo-Croatian, the Glagolitic Missale Romanum Glagolitice (1483).
During the 13th century Serbo-Croatian vernacular texts began to appear, the most important among them being the "Istrian land survey" of 1275 and the "Vinodol Codex" of 1288, both written in the Chakavian dialect.
The Shtokavian dialect literature, based almost exclusively on Chakavian original texts of religious provenance (missals, breviaries, prayer books) appeared almost a century later. The most important purely Shtokavian vernacular text is the Vatican Croatian Prayer Book (c. 1400).
Both the language used in legal texts and that used in Glagolitic literature gradually came under the influence of the vernacular, which considerably affected its phonological, morphological, and lexical systems. From the 14th and the 15th centuries, both secular and religious songs at church festivals were composed in the vernacular.
Writers of early Serbo-Croatian religious poetry ("začinjavci") gradually introduced the vernacular into their works. These "začinjavci" were the forerunners of the rich literary production of the 16th-century literature, which, depending on the area, was Chakavian-, Kajkavian-, or Shtokavian-based. The language of religious poems, translations, miracle and morality plays contributed to the popular character of medieval Serbo-Croatian literature.
One of the earliest dictionaries, also in the Slavic languages as a whole, was the "Bosnian–Turkish Dictionary" of 1631 authored by Muhamed Hevaji Uskufi and was written in the Arebica script.
Modern standardization.
In the mid-19th century, Serbian (led by self-taught writer and folklorist Vuk Stefanović Karadžić) and most Croatian writers and linguists (represented by the Illyrian movement and led by Ljudevit Gaj and Đuro Daničić), proposed the use of the most widespread dialect, Shtokavian, as the base for their common standard language. Karadžić standardised the Serbian Cyrillic alphabet, and Gaj and Daničić standardized the Croatian Latin alphabet, on the basis of vernacular speech phonemes and the principle of phonological spelling. In 1850 Serbian and Croatian writers and linguists signed the Vienna Literary Agreement, declaring their intention to create a unified standard. Thus a complex bi-variant language appeared, which the Serbs officially called "Serbo-Croatian" or "Serbian or Croatian" and the Croats "Croato-Serbian", or "Croatian or Serbian". Yet, in practice, the variants of the conceived common literary language served as different literary variants, chiefly differing in lexical inventory and stylistic devices. The common phrase describing this situation was that Serbo-Croatian or "Croatian or Serbian" was a single language. During the Austro-Hungarian occupation of Bosnia and Herzegovina, the language of all three nations was called "Bosnian" until the death of administrator von Kállay in 1907, at which point the name was changed to "Serbo-Croatian".
With unification of the first the Kingdom of the Serbs, Croats, and Slovenes – the approach of Karadžić and the Illyrians became dominant. The official language was called "Serbo-Croato-Slovenian" ("srpsko-hrvatsko-slovenački") in the 1921 constitution. In 1929, the constitution was suspended, and the country was renamed the Kingdom of Yugoslavia, while the official language of Serbo-Croato-Slovene was reinstated in the 1931 constitution.
In June 1941, the Nazi puppet Independent State of Croatia began to rid the language of "Eastern" (Serbian) words, and shut down Serbian schools.
On January 15, 1944, the Anti-Fascist Council of the People's Liberation of Yugoslavia (AVNOJ) declared Croatian, Serbian, Slovene, and Macedonian to be equal in the entire territory of Yugoslavia. In 1945 the decision to recognize Croatian and Serbian as separate languages was reversed in favor of a single Serbo-Croatian or Croato-Serbian language. In the Communist-dominated second Yugoslavia, ethnic issues eased to an extent, but the matter of language remained blurred and unresolved.
In 1954, major Serbian and Croatian writers, linguists and literary critics, backed by Matica srpska and Matica hrvatska signed the Novi Sad Agreement, which in its first conclusion stated: "Serbs, Croats and Montenegrins share a single language with two equal variants that have developed around Zagreb (western) and Belgrade (eastern)". The agreement insisted on the equal status of Cyrillic and Latin scripts, and of Ekavian and Ijekavian pronunciations. It also specified that "Serbo-Croatian" should be the name of the language in official contexts, while in unofficial use the traditional "Serbian" and "Croatian" were to be retained. Matica hrvatska and Matica srpska were to work together on a dictionary, and a committee of Serbian and Croatian linguists was asked to prepare a "pravopis". During the sixties both books were published simultaneously in Ijekavian Latin in Zagreb and Ekavian Cyrillic in Novi Sad. Yet Croatian linguists claim that it was an act of unitarianism. The evidence supporting this claim is patchy: Croatian linguist Stjepan Babić complained that the television transmission from Belgrade always used the Latin alphabet— which was true, but was not proof of unequal rights, but of frequency of use and prestige. Babić further complained that the Novi Sad Dictionary (1967) listed side by side words from both the Croatian and Serbian variants wherever they differed, which one can view as proof of careful respect for both variants, and not of unitarism. Moreover, Croatian linguists criticized those parts of the Dictionary for being unitaristic that were written by Croatian linguists. And finally, Croatian linguists ignored the fact that the material for the "Pravopisni rječnik" came from the Croatian Philological Society. Regardless of these facts, Croatian intellectuals brought the Declaration on the Status and Name of the Croatian Literary Language in 1967. On occasion of the publication’s 45th anniversary, the Croatian weekly journal Forum published the Declaration again in 2012, accompanied by a critical analysis.
West European scientists judge the Yugoslav language policy as an exemplary one: although three-quarters of the population spoke one language, no single language was official on a federal level. Official languages were declared only at the level of constituent republics and provinces, and very generously: Vojvodina had five (among them Slovak and Romanian, spoken by 0.5 per cent of the population), and Kosovo four (Albanian, Turkish, Romany and Serbo-Croatian). Newspapers, radio and television studios used sixteen languages, fourteen were used as languages of tuition in schools, and nine at universities. Only the Yugoslav Army used Serbo-Croatian as the sole language of command, with all other languages represented in the army’s other activities—however, this is not different from other armies of multilingual states, or in other specific institutions, such as international air traffic control where English is used worldwide. All variants of Serbo-Croatian were used in state administration and republican and federal institutions. Both Serbian and Croatian variants were represented in respectively different grammar books, dictionaries, school textbooks and in books known as pravopis (which detail spelling rules). Serbo-Croatian was a kind of soft standardisation. However, legal equality could not dampen the prestige Serbo-Croatian had: since it was the language of three quarters of the population, it functioned as an unofficial lingua franca. And within Serbo-Croatian, the Serbian variant, with twice as many speakers as the Croatian, enjoyed greater prestige, reinforced by the fact that Slovene and Macedonian speakers preferred it to the Croatian variant because their languages are also Ekavian. This is a common situation in other pluricentric languages, e.g. the variants of German differ according to their prestige, the variants of Portuguese too. Moreover, all languages differ in terms of prestige: "the fact is that languages (in terms of prestige, learnability etc.) are not equal, and the law cannot make them equal".
Demographics.
The total number of persons who declared their native language as either 'Bosnian', 'Croatian', 'Serbian', 'Montenegrin', or 'Serbo-Croatian' in countries of the region is about 16 million.
Serbian is spoken by about 9.5 million mostly in Serbia (6.7m), Bosnia and Herzegovina (1.4m), and Montenegro (0.4m). Serbian minorities are found in the Republic of Macedonia and in Romania. In Serbia, there are about 760,000 second-language speakers of Serbian, including Hungarians in Vojvodina and the 400,000 estimated Roma. Familiarity of Kosovo Albanians with Serbian in Kosovo varies depending on age and education, and exact numbers are not available.
Croatian is spoken by roughly 4.8 million including some 575,000 in Bosnia and Herzegovina. A small Croatian minority lives in Italy known as Molise Croats have somewhat preserved traces of the Croatian language. In Croatia, 170,000 mostly Italians and Hungarians use it as a second language.
Bosnian is spoken by 2.2 million people, chiefly Bosniaks, including about 220,000 in Serbia and Montenegro.
Notion of Montenegrin as a separate standard from Serbian is relatively recent. In the 2003 census, around 150,000 Montenegrins, of the country's 620,000, declared Montenegrin as their native language. That figure is likely to increase since, due to the country's independence and strong institutional backing of Montenegrin language.
Serbo-Croatian is also a second language of many Slovenians and Macedonians, especially those born during the time of Yugoslavia. According to the 2002 Census, Serbo-Croatian and its variants have the largest number of speakers of the minority languages in Slovenia.
Outside the Balkans, there are over 2 million native speakers of the language(s), especially in countries which are frequent targets of immigration, such as Australia, Austria, Brazil, Canada, Chile, Germany, Hungary, Italy, Sweden and the United States.
Grammar.
Serbo-Croatian is a highly inflected language. Traditional grammars list seven cases for nouns and adjectives: nominative, genitive, dative, accusative, vocative, locative, and instrumental, reflecting the original seven cases of Proto-Slavic, and indeed older forms of Serbo-Croatian itself. However, in modern Shtokavian the locative has almost merged into dative (the only difference is based on accent in some cases), and the other cases can be shown declining; namely:
Like most Slavic languages, there are mostly three genders for nouns: masculine, feminine, and neuter, a distinction which is still present even in the plural (unlike Russian and, in part, the Čakavian dialect). They also have two numbers: singular and plural. However, some consider there to be three numbers (paucal or "dual," too), since (still preserved in closely related Slovene) after two ("dva", "dvije"/"dve"), three ("tri") and four ("četiri"), and all numbers ending in them (e.g. twenty-two, ninety-three, one hundred four) the genitive singular is used, and after all other numbers five ("pet") and up, the genitive plural is used. (The number one ["jedan"] is treated as an adjective.) Adjectives are placed in front of the noun they modify and must agree in both case and number with it.
There are seven tenses for verbs: past, present, future, exact future, aorist, imperfect, and plusquamperfect; and three moods: indicative, imperative, and conditional. However, the latter three tenses are typically used only in Shtokavian writing, and the time sequence of the exact future is more commonly formed through an alternative construction.
In addition, like most Slavic languages, the Shtokavian verb also has one of two aspects: perfective or imperfective. Most verbs come in pairs, with the perfective verb being created out of the imperfective by adding a prefix or making a stem change. The imperfective aspect typically indicates that the action is unfinished, in progress, or repetitive; while the perfective aspect typically denotes that the action was completed, instantaneous, or of limited duration. Some Štokavian tenses (namely, aorist and imperfect) favor a particular aspect (but they are rarer or absent in Čakavian and Kajkavian). Actually, aspects "compensate" for the relative lack of tenses, because aspect of the verb determines whether the act is completed or in progress in the referred time.
Phonology.
Vowels.
The Serbo-Croatian vowel system is simple, with only five vowels in Shtokavian. All vowels are monophthongs. The oral vowels are as follows:
The vowels can be short or long, but the phonetic quality doesn't change depending on the length. In a word, vowels can be long in the stressed syllable and the syllables following it, never in the ones preceding it.
Consonants.
The consonant system is more complicated, and its characteristic features are series of affricate and palatal consonants. As in English, voice is phonemic, but aspiration is not.
In consonant clusters all consonants are either voiced or voiceless. All the consonants are voiced (if the last consonant is normally voiced) or voiceless (if the last consonant is normally voiceless). This rule does not apply to approximantsa consonant cluster may contain voiced approximants and voiceless consonants; as well as to foreign words ("Washington" would be transcribed as "VašinGton"), personal names and when consonants are not inside of one syllable.
Pitch accent.
Apart from Slovene, Serbo-Croatian is the only Slavic language with a pitch accent (simple tone) system. This feature is present in some other Indo-European languages, such as Swedish, Norwegian, and Ancient Greek. Neo-Shtokavian Serbo-Croatian, which is used as the basis for standard Bosnian, Croatian, Montenegrin, and Serbian, has four "accents", which involve either a rising or falling tone on either long or short vowels, with optional post-tonic lengths:
The tone stressed vowels can be approximated in English with "set" vs. "setting?" said in isolation for a short tonic "e," or "leave" vs. "leaving?" for a long tonic "i," due to the prosody of final stressed syllables in English.
General accent rules in the standard language:
There are no other rules for accent placement, thus the accent of every word must be learned individually; furthermore, in inflection, accent shifts are common, both in type and position (the so-called "mobile paradigms"). The second rule is not strictly obeyed, especially in borrowed words.
Comparative and historical linguistics offers some clues for memorising the accent position: If one compares many standard Serbo-Croatian words to e.g. cognate Russian words, the accent in the Serbo-Croatian word will be one syllable before the one in the Russian word, with the rising tone. Historically, the rising tone appeared when the place of the accent shifted to the preceding syllable (the so-called "Neoshtokavian retraction"), but the quality of this new accent was different – its melody still "gravitated" towards the original syllable. Most Shtokavian dialects (Neoshtokavian) dialects underwent this shift, but Chakavian, Kajkavian and the Old Shtokavian dialects did not.
Accent diacritics are not used in the ordinary orthography, but only in the linguistic or language-learning literature (e.g. dictionaries, orthography and grammar books). However, there are very few minimal pairs where an error in accent can lead to misunderstanding.
Orthography.
Serbo-Croatian orthography is almost entirely phonetic. Thus, most words should be spelled as they are pronounced. In practice, the writing system does not take into account allophones which occur as a result of interaction between words:
Also, there are some exceptions, mostly applied to foreign words and compounds, that favor morphological/etymological over phonetic spelling:
One systemic exception is that the consonant clusters ds and dš do not change into ts and tš (although "d" tends to be unvoiced in normal speech in such clusters):
Only a few words are intentionally "misspelled", mostly in order to resolve ambiguity:
Writing systems.
Through history, this language has been written in a number of writing systems:
The oldest texts since the 11th century are in Glagolitic, and the oldest preserved text written completely in the Latin alphabet is "Red i zakon sestara reda Svetog Dominika", from 1345. The Arabic alphabet had been used by Bosniaks; Greek writing is out of use there, and Arabic and Glagolitic persisted so far partly in religious liturgies.
Today, it is written in both the Latin and Cyrillic scripts. Serbian and Bosnian variants use both alphabets, while Croatian uses the Latin only.
The Serbian Cyrillic alphabet was revised by Vuk Stefanović Karadžić in the 19th century.
The Croatian Latin alphabet ("Gajica") followed suit shortly afterwards, when Ljudevit Gaj defined it as standard Latin with five extra letters that had diacritics, apparently borrowing much from Czech, but also from Polish, and inventing the unique digraphs "lj", "nj" and "dž". These digraphs are represented as "ļ, ń and ǵ" respectively in the "Rječnik hrvatskog ili srpskog jezika", published by the former Yugoslav Academy of Sciences and Arts in Zagreb. The latter digraphs, however, are unused in the literary standard of the language. All in all, this makes Serbo-Croatian the only Slavic language to officially use both the Latin and Cyrillic scripts, albeit the Latin version is more commonly used.
In both cases, spelling is phonetic and spellings in the two alphabets map to each other one-to-one:
Latin to Cyrillic
Cyrillic to Latin
The digraphs "Lj", "Nj" and "Dž" represent distinct phonemes and are considered to be single letters. In crosswords, they are put into a single square, and in sorting, lj follows l and nj follows n, except in a few words where the individual letters are pronounced separately. For instance, "nadživ(j)eti" "to outlive" is composed of the prefix "nad-" "out, over" and the verb "živ(j)eti" "to live". The Cyrillic alphabet avoids such ambiguity by providing a single letter for each phoneme.
"Đ" used to be commonly written as "Dj" on typewriters, but that practice led to too many ambiguities. It is also used on car license plates. Today "Dj" is often used again in place of "Đ" on the Internet as a replacement due to the lack of installed Serbo-Croat keyboard layouts.
Dialects.
South Slavic historically formed a dialect continuum, i.e. each dialect has some similarities with the neighboring one, and differences grow with distance. However, migrations from the 16th to 18th centuries resulting from the spread of Ottoman Empire on the Balkans have caused large-scale population displacement that broke the dialect continuum into many geographical pockets. Migrations in the 20th century, primarily caused by urbanization and wars, also contributed to the reduction of dialectal differences.
The primary dialects are named after the most common question word for "what": Shtokavian uses the pronoun "što" or "šta", Chakavian uses "ča" or "ca", Kajkavian ("kajkavski"), "kaj" or "kej". In native terminology they are referred to as "nar(j)ečje", which would be equivalent of "group of dialects", whereas their many subdialects are referred to as "dijalekti ""dialects" or "govori ""speeches".
The pluricentric Serbo-Croatian standard language and all four contemporary standard variants are based on the Eastern Herzegovinian subdialect of Neo-Shtokavian. Other dialects are not taught in schools or used by the state media. The Torlakian dialect is often added to the list, though sources usually note that it is a transitional dialect between Shtokavian and the Bulgaro-Macedonian dialects.
The Serbo-Croatian dialects differ not only in the question word they are named after, but also heavily in phonology, accentuation and intonation, case endings and tense system (morphology) and basic vocabulary. In the past, Chakavian and Kajkavian dialects were spoken on a much larger territory, but have been replaced by Štokavian during the period of migrations caused by Ottoman Turkish conquest of the Balkans in the 15th and the 16th centuries. These migrations caused the koinéisation of the Shtokavian dialects, that used to form the West Shtokavian (more closer and transitional towards the neighbouring Chakavian and Kajkavian dialects) and East Shtokavian (transitional towards the Torlakian and the whole Bulgaro-Macedonian area) dialect bundles, and their subsequent spread at the expense of Chakavian and Kajkavian. As a result, Štokavian now covers an area larger than all the other dialects combined, and continues to make its progress in the enclaves where non-literary dialects are still being spoken.
The differences among the dialects can be illustrated on the example of Schleicher's fable. Diacritic signs are used to show the difference in accents and prosody, which are often quite significant, but which are not reflected in the usual orthography.
Division by "jat" reflex.
A basic distinction among the dialects is in the reflex of the long Common Slavic vowel "jat", usually transcribed as *ě. Depending on the reflex, the dialects are divided into Ikavian, Ekavian, and Ijekavian, with the reflects of "jat" being /i/, /e/, and /ije/ or /je/ respectively. The long and short "jat" is reflected as long or short */i/ and /e/ in Ikavian and Ekavian, but Ijekavian dialects introduce a "ije"/"je" alternation to retain a distinction.
Standard Croatian and Bosnian are based on Ijekavian, whereas Serbian uses both Ekavian and Ijekavian forms (Ijekavian for Bosnian Serbs, Ekavian for most of Serbia). Influence of standard language through state media and education has caused non-standard varieties to lose ground to the literary forms.
The jat-reflex rules are not without exception. For example, when short "jat" is preceded by "r", in most Ijekavian dialects developed into /re/ or, occasionally, /ri/. The prefix "prě-" ("trans-, over-") when long became "pre-" in eastern Ijekavian dialects but to "prije-" in western dialects; in Ikavian pronunciation, it also evolved into "pre-" or "prije-" due to potential ambiguity with "pri-" ("approach, come close to"). For verbs that had "-ěti " in their infinitive, the past participle ending "-ěl" evolved into "-io" in Ijekavian Neoštokavian.
The following are some examples:
Present sociolinguistic situation.
Comparison with other pluricentric languages.
Enisa Kafadar argues that there is only one Serbo-Croatian language with several varieties. This has made possible to include all four varieties into a new grammar book. Daniel Bunčić concludes that it is a pluricentric language, with four standard variants spoken in Serbia, Croatia, Montenegro and Bosnia and Herzegovina. The mutual intelligibility between their speakers "exceeds that between the standard variants of English, French, German, or Spanish". Other linguists have argued that the differences between the variants of Serbo-Croatian are less significant than those between the variants of English, German, Dutch, and Hindi-Urdu.
Among pluricentric languages, Serbo-Croatian was the only one with a pluricentric standardisation within one state. The dissolution of Yugoslavia has made Serbo-Croatian even more typical pluricentric language, since the variants of other pluricentric languages are also spoken in different states.
Contemporary names.
The current Serbian constitution of 2006 refers to the official language as "Serbian", while the Montenegrin constitution of 2007 proclaimed "Montenegrin" as the primary official language, but also grants other languages the right of official use.
The International Organization for Standardization (ISO) has specified different Universal Decimal Classification (UDC) numbers for Croatian "(UDC 862," abbreviation hr) and Serbian "(UDC 861", abbreviation sr), while the cover term "Serbo-Croatian" is used to refer to the combination of original signs ("UDC 861/862," abbreviation sh). Furthermore, the "ISO 639" standard designates the Bosnian language with the abbreviations bos and bs.
The International Criminal Tribunal for the former Yugoslavia considers what it calls "BCS" (Bosnian-Croatian-Serbian) to be the main language of all Bosnian, Croatian, and Serbian defendants. The indictments, documents, and verdicts of the ICTY are not written with any regard for consistently following the grammatical prescriptions of any of the three standardsbe they Serbian, Croatian, or Bosnian.
For utilitarian purposes, the Serbo-Croatian language is often called ""Naš jezik"" ("Our language") or ""Naški"" (sic. "Ourish" or "Ourian") by native speakers. This politically correct term is frequently used to describe the Serbo-Croatian language by those who wish to avoid nationalistic and linguistic discussions.
Views of linguists in the former Yugoslavia.
Serbian linguists.
The majority of mainstream Serbian linguists consider Serbian and Croatian to be one language, that is called Serbo-Croatian ("srpskohrvatski") or Croato-Serbian ("hrvatskosrpski"). A minority of Serbian linguists are of the opinion that Serbo-Croatian did exist, but has, in the meantime, dissolved.
Croatian linguists.
The opinion of the majority of Croatian linguists is that there has never been a Serbo-Croatian language, but two different standard languages that overlapped sometime in the course of history. However, Croatian linguist Snježana Kordić has been leading an academic discussion on that issue in the Croatian journal "Književna republika" from 2001 to 2010. In the discussion, she shows that linguistic criteria such as mutual intelligibility, huge overlap in linguistic system, and the same dialectic basis of standard language provide evidence that Croatian, Serbian, Bosnian and Montenegrin are four national variants of the pluricentric Serbo-Croatian language. Igor Mandić states: "During the last ten years, it has been the longest, the most serious and most acrid discussion (…) in 21st-century Croatian culture". Inspired by that discussion, a monograph on language and nationalism has been published.
The views of the majority of Croatian linguists that there is no Serbo-Croatian language, but several different standard languages, have been sharply criticized by German linguist Bernhard Gröschel in his monograph "Serbo-Croatian Between Linguistics and Politics".
A more detailed overview, incorporating arguments from the Croatian philology and contemporary linguistics, would be as follows:
The linguistic debate in this region is more about politics than about linguistics per se.
The topic of language for writers from Dalmatia and Dubrovnik prior to the 19th century made a distinction only between speakers of Italian or Slavic, since those were the two main groups that inhabited Dalmatian city-states at that time. Whether someone spoke Croatian or Serbian was not an important distinction then, as the two languages were not distinguished by most speakers. This has been used as an argument to state that Croatian literature Croatian per se, but also includes Serbian and other languages that are part of Serbo-Croatian, These facts undermine the Croatian language proponents' argument that modern-day Croatian is based on a language called Old Croatian.
However, most intellectuals and writers from Dalmatia who used the Štokavian dialect and practiced the Catholic faith saw themselves as part of a Croatian nation as far back as the mid-16th to 17th centuries, some 300 years before Serbo-Croatian ideology appeared. Their loyalty was first and foremost to Catholic Christendom, but when they professed an ethnic identity, they referred to themselves as "Slovin" and "Illyrian" (a sort of forerunner of Catholic baroque pan-Slavism) and Croatthese 30-odd writers over the span of c. 350 years always saw themselves as Croats first and never as part of a Serbian nation. It should also be noted that, in the pre-national era, Catholic religious orientation did not necessarily equate with Croat ethnic identity in Dalmatia. A Croatian follower of Vuk Karadžić, Ivan Broz, noted that for a Dalmatian to identify oneself as a Serb was seen as foreign as identifying oneself as Macedonian or Greek. Vatroslav Jagić pointed out in 1864:
On the other hand, the opinion of Jagić from 1864 is argued not to have firm grounds. When Jagić says "Croatian", he refers to a few cases referring to the Dubrovnik vernacular as "ilirski" (Illyrian). This was a common name for all Slavic vernaculars in Dalmatian cities among the Roman inhabitants. In the meantime, other written monuments are found that mention "srpski", "lingua serviana" (= Serbian), and some that mention Croatian. By far the most competent Serbian scientist on the Dubrovnik language issue, Milan Rešetar, who was born in Dubrovnik himself, wrote behalf of language characteristics: "The one who thinks that Croatian and Serbian are two separate languages must confess that Dubrovnik always (linguistically) used to be Serbian."
Finally, the former "medieval" texts from Dubrovnik and Montenegro dating before the 16th century were neither true Štokavian nor Serbian, but mostly specific a Jekavian-Čakavian that was nearer to actual Adriatic islanders in Croatia.
Political connotations.
Nationalists have conflicting views about the language(s). The nationalists among the Croats conflictingly claim either that they speak an entirely separate language from Serbs and Bosnians or that these two peoples have, due to the longer lexicographic tradition among Croats, somehow "borrowed" their standard languages from them. Bosniak nationalists claim that both Croats and Serbs have "appropriated" the Bosnian language, since Ljudevit Gaj and Vuk Karadžić preferred the Neoštokavian-Ijekavian dialect, widely spoken in Bosnia and Herzegovina, as the basis for language standardization, whereas the nationalists among the Serbs claim either that any divergence in the language is artificial, or claim that the Štokavian dialect is theirs and the Čakavian Croats'— in more extreme formulations Croats have "taken" or "stolen" their language from the Serbs. 
Proponents of unity among Southern Slavs claim that there is a single language with normal dialectal variations. The term "Serbo-Croatian" (or synonyms) is not officially used in any of the successor countries of former Yugoslavia.
In Serbia, the Serbian language is the official one, while both Serbian and Croatian are official in the province of Vojvodina. A large Bosniak minority is present in the southwest region of Sandžak, but the "official recognition" of Bosnian language is moot. Bosnian is an optional course in 1st and 2nd grade of the elementary school, while it is also in official use in the municipality of Novi Pazar. However, its nomenclature is controversial, as there is incentive that it is referred to as "Bosniak" ("bošnjački") rather than "Bosnian" ("bosanski") (see Bosnian language for details).
Croatian is the official language of Croatia, while Serbian is also official in municipalities with significant Serb population.
In Bosnia and Herzegovina, all three languages are recorded as official but in practice and media, mostly Bosnian and Serbian are applied. Confrontations have on occasion been absurd. The academic Muhamed Filipović, in an interview to Slovenian television, told of a local court in a Croatian district requesting a paid translator to translate from Bosnian to Croatian before the trial could proceed.

</doc>
<doc id="27737" url="https://en.wikipedia.org/wiki?curid=27737" title="Saint Kitts">
Saint Kitts

Saint Kitts, also known more formally as Saint Christopher Island, is an island in the West Indies. The west side of the island borders the Caribbean Sea, and the eastern coast faces the Atlantic Ocean. Saint Kitts and the neighbouring island of Nevis constitute one country: the Federation of Saint Kitts and Nevis.
The island is one of the Leeward Islands in the Lesser Antilles. It is situated about southeast of Miami, Florida. The land area of St. Kitts is about , being approximately long and on average about across.
Saint Kitts has a population of around 45,000, the majority of whom are mainly of African descent. The primary language is English, with a literacy rate of approximately 98%. Residents call themselves Kittitians.
Brimstone Hill Fortress National Park, a UNESCO World Heritage Site, is the largest fortress ever built in the Eastern Caribbean. The island of Saint Kitts is home to the Warner Park Cricket Stadium, which was used to host 2007 Cricket World Cup matches. This made St. Kitts and Nevis the smallest nation to ever host a World Cup event. Saint Kitts is also home to several institutions of higher education, including Ross University School of Veterinary Medicine, Windsor University School of Medicine, and the University of Medicine and Health Sciences.
Geography.
The capital of the two-island nation, and also its largest port, is the town of Basseterre on Saint Kitts. There is a modern facility for handling large cruise ships there. A ring road goes around the perimeter of the island with smaller roads branching off it; the interior of the island is too steep for habitation.
Saint Kitts is away from Sint Eustatius to the north and from Nevis to the south. St. Kitts has three distinct groups of volcanic peaks: the North West or Mount Misery Range; the Middle or Verchilds Range and the South East or Olivees Range. The highest peak is Mount Liamuiga, formerly Mount Misery, a dormant volcano 1,156 m high.
Parish Images.
There are nine parishes on the island of St. Kitts:
Economy.
St. Kitts & Nevis uses the Eastern Caribbean dollar, which maintains a fixed exchange rate of 2.7-to-one with the United States dollar. The US dollar is almost as widely accepted as the Eastern Caribbean dollar.
For hundreds of years, St. Kitts operated as a sugar monoculture, but due to decreasing profitability, the government closed the industry in 2005. Tourism is a major and growing source of income to the island, although the number and density of resorts is less than on many other Caribbean islands. Transportation, non-sugar agriculture, manufacturing and construction are the other growing sectors of the economy.
St. Kitts is dependent on tourism to drive its economy. Tourism has been increasing since 1978. In 2009, there were 587,479 arrivals to Saint Kitts compared to 379,473 in 2007, which represents an increase of just under 40% growth in a two-year period. As tourism grows, the demand for vacation property increases in conjunction.
St. Kitts & Nevis also acquires foreign direct investment from their unique citizenship by investment program, outlined in their Citizenship Act of 1984. Interested parties can acquire citizenship if they pass the government's strict background checks and make an investment into an approved real estate development. Purchasers who pass government due diligence and make a minimum investment of US$400,000, into qualifying government approved real estate, are entitled to apply for citizenship of the Federation of St. Kitts and Nevis. Many projects are approved under the citizenship by investment program, and the main qualifying projects of interest can be found within the Henley Estates market overview .
In addition to this, in hopes of expanding tourism, the country hosts its annual St. Kitts Music Festival.
History.
During the last Ice Age, the sea level was lower and St. Kitts and Nevis were one island along with Sint Eustatius (also known as Statia).
St. Kitts was originally settled by pre-agricultural, pre-ceramic "Archaic people", who migrated south down the archipelago from Florida. In a few hundred years they disappeared, to be replaced by the ceramic-using and agriculturalist Saladoid people around 100 BC, who migrated to St. Kitts north up the archipelago from the banks of the Orinoco River in Venezuela. Around 800 AD, they were replaced by the Igneri people, members of the Arawak group.
Around 1300, the Kalinago, or Carib people arrived on the islands. These war-like people quickly dispersed the Igneri, and forced them northwards to the Greater Antilles. They named Saint Kitts "Liamuiga" meaning "fertile island", and would likely have expanded further north if not for the arrival of Europeans.
A Spanish expedition under Christopher Columbus discovered and claimed the island for Spain in 1493. A short-lived French Huguenot settlement was established at Dieppe Bay in 1538.
The first English colony was established in 1623, followed by a French colony in 1625. The English and French briefly united to massacre the local Kalinago (preempting a Kalinago plan to massacre the Europeans), and then partitioned the island, with the English colonists in the middle and the French on either end. In 1629, a Spanish force sent to clear the islands of foreign settlement seized St. Kitts. The English settlement was rebuilt following the 1630 peace between England and Spain.
The island alternated repeatedly between English (then British) and French control during the 17th and 18th centuries, as one power took the whole island, only to have it switch hands due to treaties or military action. Parts of the island were heavily fortified, as exemplified by the UNESCO World Heritage Site at Brimstone Hill and the now-crumbling Fort Charles.
Since 1783, St. Kitts has been affiliated with the Kingdom of Great Britain, which became the United Kingdom.
Slavery.
The island originally produced tobacco; but it changed to sugar cane in 1640, due to stiff competition from the colony of Virginia. The labour-intensive cultivation of sugar cane was the reason for the large-scale importation of African slaves. The importation began almost immediately upon the arrival of Europeans to the region.
The purchasing of enslaved Africans was outlawed in the British Empire by an Act of Parliament in 1807. Slavery was abolished by an Act of Parliament which became law on 1 August 1834. This emancipation was followed by four years of apprenticeship, put in place to protect the planters from losing their labour force.
August the 1st is now celebrated as a public holiday and is called Emancipation Day. In 1883, St. Kitts, Nevis, and Anguilla were all linked under one presidency, located on St. Kitts, to the dismay of the Nevisians and Anguillans. Anguilla eventually separated out of this arrangement, in 1971, after an armed raid on St. Kitts.
Sugar production continued to dominate the local economy until 2005, when, after 365 years of having a mono-culture, the government closed the sugar industry. This was due to huge losses and European Union plans to greatly cut sugar prices.
Transportation.
Robert L. Bradshaw International Airport serves St. Kitts. British Airways flies in twice a week from London and daily connections from Miami and New York are available.
The Basseterre Ferry Terminal facilitates travel between St. Kitts and sister island Nevis.
The narrow-gauge (30 inches) St Kitts Scenic Railway circles the island and offers passenger service from its headquarters near the airport, although the service is geared more for tourists than as day-to-day transportation for residents. Built between 1912 and 1926 to haul sugar cane from farms to the sugar factory in Basseterre, since 2003 the railway has offered a 3.5 hour, 30-mile circle tour of the island on specially designed double-decker open-air coaches, with 12 miles of the trip being by bus.
Notable residents.
Saint Kitts is or was the residence of:

</doc>
<doc id="27739" url="https://en.wikipedia.org/wiki?curid=27739" title="Shogi">
Shogi

The earliest predecessor of the game, chaturanga, originated in India in the 6th century, and sometime in the 10th to 12th centuries xiangqi (Chinese chess) was brought to Japan where it spawned a number of variants. Shogi in its present form was played as early as the 16th century, while a direct ancestor without the "drop rule" was recorded from 1210 in a historical document "Nichūreki", which is an edited copy of "Shōchūreki" and "Kaichūreki" from the late Heian period (c. 1120).
According to "The Chess Variant Pages" :
Equipment.
Two players, "Sente" (Black; more literally, "person with the first move") and "Gote" (White; "person with the second move"), play on a board composed of rectangles in a grid of 9 "ranks" (rows) by 9 "files" (columns). The rectangles are undifferentiated by marking or color. The board is nearly always rectangular; square boards are uncommon. Pairs of dots mark the players' promotion zones.
Each player has a set of 20 wedge-shaped pieces of slightly different sizes. Except for the kings, opposing pieces are undifferentiated by marking or color. Pieces face "forward" (toward the opponent's side); this shows who controls the piece during play. The pieces from largest (most important) to smallest (least important) are:
Several of these names were chosen to correspond to their rough equivalents in international chess, and not as literal translations of the Japanese names.
Each piece has its name written on its surface in the form of two "kanji" (Chinese characters used in Japanese), usually in black ink. On the reverse side of each piece, other than the king and gold general, are one or two other characters, in amateur sets often in a different color (usually red); this side is turned face up during play to indicate that the piece has been promoted.
The suggestion that the Japanese characters have deterred Western players from learning shogi has led to "Westernized" or "international" pieces which use iconic symbols instead of characters. Most players soon learn to recognize the characters, however, partially because the traditional pieces are already iconic by size, with more powerful pieces being larger. As a result, Westernized pieces have never become popular. Bilingual pieces with both Japanese characters and English captions have been developed.
Following is a table of the pieces with their Japanese representations and English equivalents. The abbreviations are used for game notation and often when referring to the pieces in speech in Japanese.
English speakers sometimes refer to promoted bishops as "horses" and promoted rooks as "dragons", after their Japanese names, and generally use the Japanese term "tokin" for promoted pawns. Silver generals and gold generals are commonly referred to simply as "silvers" and "golds".
The characters inscribed on the reverse sides of the pieces to indicate promotion may be in red ink, and are usually cursive. The characters on the backs of the pieces that promote to gold generals are cursive variants of 'gold', becoming more cursive (more abbreviated) as the value of the original piece decreases. These cursive forms have these equivalents in print: for promoted silver, for promoted knight, for promoted lance, and for promoted pawn (tokin). Another typographic convention has abbreviated versions of the original values, with a reduced number of strokes: for a promoted knight , for a promoted lance , and the as above for a promoted silver, but for "tokin".
Setup and gameplay.
Each player sets up his pieces facing forward (toward his opponent).
Traditionally, even the order of placing the pieces on the board is determined. There are two commonly used orders, "Ohashi" and "Ito". Placement sets pieces with multiples (generals, knights, lances) from left to right in all cases, and follows the order:
One player takes Black and moves first; then players alternate turns. (The terms "Black" and "White" are used to differentiate sides although there is no difference in the color of the pieces.) For each turn a player may either move a piece that is currently on the board (and potentially promote it, capture an opposing piece, or both) or else "drop" a piece that has been previously captured onto an empty square of the board. These options are explained below.
Professional games are timed as in international chess, but professionals are never expected to keep time in their games. Instead a timekeeper is assigned, typically an apprentice professional. Time limits are much longer than in international chess (9 hours a side plus extra time in the prestigious "Meijin" title match), and in addition "byōyomi" (literally "second counting") is employed. This means that when the ordinary time has run out, the player will from that point on have a certain amount of time to complete every move (a "byōyomi" period), typically upwards of one minute. The final ten seconds are counted down, and if the time expires the player to move loses the game immediately. Amateurs often play with electronic clocks that beep out the final ten seconds of a "byōyomi" period, with a prolonged beep for the last five.
An illegal move results in an immediate loss of the game in professional and tournament shogi, even if play continued and the move was discovered later in game. However, if neither opposition nor third party points out, and the opposition later resigned, the resignation stands as the result.
Rules.
Movement.
Most shogi pieces can move only to an adjacent square. A few may move across the board, and one jumps over intervening pieces. Shogi pieces capture the same as they move.
Every piece blocks the movement of all other non-jumping pieces through the square it occupies. If a piece occupies a legal destination for an opposing piece, it may be "captured" by removing it from the board and replacing it with the opposing piece. The capturing piece may not continue beyond that square on that turn.
It is common to keep captured pieces on a wooden stand (or "komadai)" which is traditionally placed so that its bottom left corner aligns with the bottom right corner of the board from the perspective of each player. It is not permissible to hide pieces from full view. This is because captured pieces, which are said to be , have a crucial impact on the course of the game.
The knight "jumps", that is, it passes "over" any intervening piece, whether friend or foe, without an effect on either. It is the only piece to do this.
The lance, bishop, and rook are "ranging" pieces: They can move any number of squares along a straight line limited only by intervening pieces and the edge of the board. If an opposing piece intervenes, it may be captured by removing it from the board and replacing it with the moving piece. If a friendly piece intervenes, the moving piece must stop short of that square; if the friendly piece is adjacent, the moving piece may not move in that direction at all.
All pieces but the knight move either horizontally, vertically, or diagonally. These directions cannot be combined in a single move; one direction must be chosen.
Normally when moving a piece, a player snaps it to the board with the ends of the fingers of the same hand. This makes a sudden sound effect, bringing the piece to the attention of the opponent. This is also true for capturing and dropping pieces. On a traditional "shogi-ban", the pitch of the snap is deeper, delivering a subtler effect.
King.
A king moves one square in any direction, orthogonal or diagonal.
Rook.
A rook moves any number of squares in an orthogonal direction.
Bishop.
A bishop moves any number of squares in a diagonal direction.
Because they cannot move orthogonally, the players' unpromoted bishops can reach only half the squares of the board, unless one is captured and then dropped.
Gold general.
A gold general moves one square orthogonally, or one square diagonally forward, giving it six possible destinations. It cannot move diagonally backwards.
Silver general.
A silver general moves one square diagonally, or one square straight forward, giving it five possible destinations.
Because an unpromoted silver can retreat more easily than a promoted one, it is common to leave a silver unpromoted at the far side of the board (see Promotion).
Knight.
A knight "jumps" at an angle intermediate to orthogonal and diagonal, amounting to one square straight forward plus one square diagonally forward, in a single move. Thus the knight has two possible forward destinations. The knight cannot move to the sides or in a backwards direction.
The knight is the only piece that ignores intervening pieces on the way to its destination. It is not blocked from moving if the square in front of it is occupied, but neither can it capture a piece on that square.
It is often useful to leave a knight unpromoted at the far side of the board. A knight "must" promote, however, if it reaches either of the two furthest ranks (see Promotion).
Lance.
A lance moves any number of squares directly forward. It cannot move backwards or to the sides.
It is often useful to leave a lance unpromoted at the far side of the board . A lance "must" promote, however, if it reaches the furthest rank (see Promotion).
Pawn.
A pawn moves one square straight forward. It cannot retreat. Unlike international chess pawns, shogi pawns capture the same as they move. 
A pawn "must" promote if it arrives at the furthest rank (see Promotion). In practice, however, a pawn is usually promoted whenever possible.
There are two restrictions on where a pawn may be dropped (see Drops).
Promotion.
A player's "promotion zone" consists of the furthest one-third of the board – the three ranks occupied by the opponent's pieces at setup. The zone is typically delineated on shogi boards by two inscribed dots. When a piece is moved, if part of the piece's path lies within the promotion zone (that is, if the piece moves into, out of, or wholly within the zone; but "not" if it is dropped into the zone – see Drops), then the player has the option to "promote" the piece at the end of the turn. Promotion is indicated by turning the piece over after it moves, revealing the character of the promoted piece.
If a pawn or lance is moved to the furthest rank, or a knight is moved to either of the two furthest ranks, that piece "must" promote (otherwise, it would have no legal move on subsequent turns). A silver general is never required to promote, and it is often advantageous to keep a silver general unpromoted. (It is easier, for example, to extract an unpromoted silver from behind enemy lines; whereas a promoted silver, with only one line of retreat, can be easily blocked.)
Promoting a piece changes the way it moves. The various pieces promote as follows:
When captured, a piece loses its promoted status. Otherwise promotion is permanent.
Promoted rook.
A promoted rook ("dragon king", "Ryūō") moves as a rook or as a king, but not as both on the same turn.
Promoted bishop.
A promoted bishop ("dragon horse", "Ryūma") moves as a bishop or as a king, but not as both on the same turn.
Promoted silver.
A promoted silver ("narigin") moves the same as a gold general.
Promoted knight.
A promoted knight ("narikei") moves the same as a gold general.
Promoted lance.
A promoted lance ("narikyō") moves the same as a gold general.
Promoted pawn.
A promoted pawn ("tokin") moves the same as a gold general.
Drops.
Captured pieces are retained "in hand", and can be brought back into play under the capturing player's control. On any turn, instead of moving a piece on the board, a player may select a piece in hand and place it—unpromoted side up and facing the opposing side—on any empty square. The piece is then one of that player's active pieces on the board and can be moved accordingly. This is called "dropping" the piece, or simply, a "drop". A drop counts as a complete move.
A drop cannot capture a piece, nor does dropping within the promotion zone result in immediate promotion. Capture and/or promotion may occur normally, however, on subsequent moves of the piece.
A pawn, knight, or lance may not be dropped on the furthest rank, since those pieces would have no legal moves on subsequent turns. For the same reason, a knight may not be dropped on the penultimate (player's 8th) rank.
There are two additional restrictions when dropping pawns:
It is common for players to swap bishops, which oppose each other across the board, early in the game. This leaves each player with a bishop in hand to be dropped later. The ability for drops in shogi give the game tactical richness and complexity. The fact that no piece ever goes entirely out of play accounts for the rarity of draws.
Winning.
When a player's move threatens to capture the opposing king on the next turn, the move is said to "give check" to the king; the king is said to be "in check". If a player's king is in check, that player's responding move must remove the check if possible; if no such move exists, the checking move is also "checkmate" (tsumi 詰み) and immediately wins the game. The losing player should resign out of courtesy at this point, although in practice this rarely occurs, as players normally resign as soon as a loss is deemed inevitable.
To announce "check!" in Japanese, one says ""ōte!"" (). This is an influence of international chess and is not required, however, even as a courtesy.
In professional and serious amateur games, a player who makes an illegal move loses immediately.
There are two other possible, if uncommon, ways for a game to end: "repetition" ( "sennichite") and "impasse" ( "jishōgi"):
As this impasse generally needs to be agreed on for the rule to be invoked, a player may refuse to do so, on the grounds that the player could gain further material or position before an outcome has to be decided. If that happens, one player may force "jishōgi" upon getting his king and all his pieces protected in the promotion zone.
In professional tournaments the rules typically require drawn games to be replayed with colors (sides) reversed, possibly with reduced time limits. This is rare compared to chess and xiangqi, occurring at a rate of 1–2% even in amateur games. The 1982 "Meijin" title match between Makoto Nakahara and Hifumi Kato was unusual in this regard, with "jishōgi" in the first game (only the fifth draw in the then 40-year history of the tournament), a game which lasted for 223 moves (not counting in pairs of moves), with 114 minutes spent pondering a single move, and "sennichite" in the sixth and eighth games. Thus this best-of-seven match lasted eight games and took over three months to finish; Black did not lose a single game and the eventual victor was Kato at 4–3.
Player rank and handicaps.
Amateur players are ranked from 15 "kyū" to 1 "kyū" and then from 1 "dan" and to 8 "dan". Amateur 8 "dan" was only honorarily given to famous people. While it's now possible to win amateur 8 "dan" by actual strength (winning amateur Ryu-oh 3 times), this has yet to be achieved.
Professional players operate with their own scale, from 6 "kyū" to 3 "dan" for pro-aspiring players and professional 4 "dan" to 9 "dan" for formal professional players. Amateur and professional ranks are offset (with amateur 4 "dan" being equivalent to professional 6 "kyū").
Games between players of disparate strengths are often played with handicaps. In a handicap game, one or more of White's pieces are removed from the setup, and in exchange White plays first. Note that the missing pieces are not available for drops and play no further part in the game. The imbalance created by this method of handicapping is not as strong as it is in international chess because material advantage is not as powerful in shogi.
Common handicaps, in increasing order of severity, include the following:
Other handicaps are also occasionally used, especially when teaching the game to new players. The relationship between handicaps and differences in rank is not universally agreed upon, with several systems in use. The system used by the Japan Shogi Association at its headquarters in Tokyo the is as follows:
If a "jishōgi" occurs in a handicap game, the removed pieces are counted towards White's total.
Notation.
The method used in English-language texts to express shogi moves was established by George Hodges in 1976. It is derived from the algebraic notation used for chess, but differs in several respects. It is not used in Japanese-language texts, as it is no more concise than traditional notation with kanji and two ciphers which was originated in Edo period.
A typical move might be notated P-8f. The first letter represents the piece moved: P for Pawn. (There is also L/lance, N/knight, S/silver, G/gold, B/bishop, R/rook, and K/king.) Promoted pieces are indicated by a + preceding the letter: +P is a tokin (promoted pawn). 
Following the abbreviation for the piece is a symbol for the type of move: - (hyphen) for a simple move, x for a capture, or * (asterisk) for a drop. Next is the square on which the piece lands. This is indicated by a numeral for the file and a lowercase letter for the rank, with 1a being the top right corner (Black's perspective) and 9i being the bottom left corner. This is based on the Japanese convention.
If a move entitles the player to promote, then a + is added to the end if the promotion was taken, or an = if it was declined. For example, Nx7c= indicates a knight capturing on 7c without promoting. 
In cases where the piece is ambiguous, the starting square is added to the letter for the piece. For example, at setup Black has two golds which can move to square 5h (in front of the king). These are distinguished as G6i-5h (from the left) and G4i-5h (from the right).
In Japanese notation, a typical move is indicated like (= P-8f). Pieces are indicated with kanji instead of letters. The piece's kanji follows the pieces board coordinates. Moves by promoted pieces are indicated with the promoted piece's kanji (such as "" = +B, "" = +P, etc.). Captured pieces are not explicitly notated since the capture can be understood in the game's context. Dropped pieces are indicated with "" following the piece's character. This "" indication is optional as drops are often understood in the game's context; however, when there is ambiguity, the "" is obligatory. For the board's coordinates, the file is indicated with a Roman numeral followed by the rank indicated with a Japanese numeral (instead of a letter). For example, square 2c in Western notation is "". There is also a shorthand convention: when a piece moves to the same coordinates as the previous move's piece (as in a capture), the position is simply indicated with instead of the file-rank coordinate numbers. A piece that promotes is indicated with "" following the piece's character, such as "" (N-7c+). If a piece does not promote, this is indicated with "" following the piece's character, such as "" (N-7c=). When there is ambiguity in piece movement, the piece's starting position can be indicated with a "" ('"right") or "" ("left") character or using the piece's origin coordinates as in Western notation. It is also common for the the white and black player to be indicated at the beginning of the notation string with either black and white triangles or shogi-piece-shaped pentagons, such as or . This white/black convention is more common when the moves are not numbered, which is also optional to notate.
Although Japanese notation often doesn't number players' moves, shogi moves are always counted per player's move. This is commonly seen in checkmate problems where a "3 move" checkmate problem would mean a move sequence of black-white-black. This is unlike western chess which counts each pair of moves as one move. In western-style notation for shogi, the move numbering tends to follow western chess notation conventions.
As an example, a one-turn loss bishop exchange ( "ittezon kakukawari") game might proceed and be notated like this:
In handicap games White plays first, so Black's move 1 is replaced by an ellipsis.
Strategy and tactics.
Shogi is similar to chess but has a much larger game tree complexity because of the use of drops. Like chess, however, the game can be divided into the opening, middle game and endgame, each requiring a different strategy. The opening consists of arranging one's defenses and positioning for attack, the mid game consists of attempting to break through the opposing defenses while maintaining one's own, and the end game starts when one side's defenses have been compromised.
History.
From "The Chess Variant Pages":
It is not clear when chess was brought to Japan. The earliest generally accepted mention of shogi is (1058–64) by Fujiwara Akihira. The oldest archaeological evidence is a group of 16 shogi pieces excavated from the grounds of Kōfuku-ji in Nara Prefecture. As it was physically associated with a wooden tablet written on in the sixth year of Tenki (1058), the pieces are thought to date from that period. These simple pieces were cut from a writing plaque in the same five-sided shape as modern pieces, with the names of the pieces written on them.
The dictionary of common folk culture, (c. 1210–21), a collection based on the two works and , describes two forms of shogi, large "(dai)" shogi and small "(shō)" shogi. These are now called Heian shogi (or Heian small shogi) and Heian dai shogi. Heian small shogi is the version on which modern shogi is based, but the "Nichūreki" states that one wins if one's opponent is reduced to a single king, indicating that drops had not yet been introduced. According to Kōji Shimizu, chief researcher at the Archaeological Institute of Kashihara, Nara Prefecture, the names of the Heian shogi pieces keep those of chaturanga (general, elephant, horse, chariot and soldier), and add to them the five treasures of Buddhism (jade, gold, silver, katsura tree, and incense).
Around the 13th century the game of dai shogi developed, created by increasing the number of pieces in Heian shogi, as was sho shogi, which added the rook, bishop, and drunken elephant from dai shogi to Heian shogi. Around the 15th century, the rules of dai shogi were simplified, creating the game of chu shogi in a form close to the modern game. It is thought that the rules of standard shogi were fixed in the 16th century, when the drunken elephant was removed from the set of pieces. There is no clear record of when drops were introduced, however.
In the Edo period, shogi variants were greatly expanded: tenjiku shogi, dai dai shogi, maka dai dai shogi, tai shogi, and taikyoku shogi were all invented. It is thought that these were played to only a very limited extent, however. Both standard shogi and Go were promoted by the Tokugawa shogunate. In 1612, the shogunate passed a law giving endowments to top shogi players (). During the reign of the eighth shogun, Tokugawa Yoshimune, castle shogi tournaments were held once a year on the 17th day of Kannazuki, corresponding to November 17, which is Shogi Day on the modern calendar.
The title of "meijin" became hereditary in the Ōhashi and Itō families until the fall of the shogunate, when it came to be passed by recommendation. Today the title is used for the winner of the Meijin-sen competition, the first modern title match. From around 1899, newspapers began to publish records of shogi matches, and high-ranking players formed alliances with the aim of having their games published. In 1909, the was formed, and in 1924, the was formed. This was an early incarnation of the modern , or JSA, and 1924 is considered by the JSA to be the date it was founded.
In 1935, "meijin" Kinjirō Sekine stepped down, and the rank of meijin came to be awarded to the winner of a . became the first Meijin under this system in 1937. This was the start of the (see titleholder system). After the war other tournaments were promoted to title matches, culminating with the in 1988 for the modern line-up of seven. About 200 professional shogi players compete. Each year, the title holder defends the title against a challenger chosen from knockout or round matches.
After the Second World War, SCAP (occupational government mainly led by US) tried to eliminate all "feudal" factors from Japanese society and shogi was included in the possible list of items to be banned along with Bushido (philosophy of samurai) and other things. The reason for banning shogi for SCAP was its exceptional character as a board game seen in the usage of captured pieces. SCAP insisted that this could lead to the idea of prisoner abuse. But Kozo Masuda, then one of the top professional shogi players, when summoned to the SCAP headquarters for an investigation, criticized such understanding of shogi and insisted that it is not shogi but western chess that potentially contains the idea of prisoner abuse because it just kills the pieces of the opponent while shogi is rather democratic for giving prisoners the chance to get back into the game. Masuda also said that chess contradicts the ideal of gender equality in western society because the king shields itself behind the queen and runs away. Masuda’s assertion is said to have eventually led to the exemption of shogi from the list of items to be banned.
The closest cousin of shogi in the chaturanga family is makruk of Thailand. Not only the similarity in distribution and movements of the pieces but also the names of shogi pieces suggest intimacy between shogi and makruk by its Buddhist symbolism (gold, silver, Cassia and Incense), which is not recognized in Chinese chess at all. In fact, Chinese chess and its East Asian variants are far remoter relatives than makruk. Though some early variants of chaturanga more similar to shogi and makruk are known to have been played in Tang dynasty China, they are thought to have been extinguished in Song dynasty China and in East Asia except in Japan probably owing to the popularity of Chinese chess.
Tournament play.
There are two organizations for shogi professional players in Japan: the JSA, and the , or LPSA. The JSA is the primary organization for men and women's professional shogi while the LPSA is a group of women professionals who broke away from the JSA in 2007 to establish their own independent organization. Both organize tournaments for their members and have reached an agreement to cooperate with each other to promote shogi through events and other activities.
The JSA recognizes two categories of shogi professionals: , and . Sometimes "kishi" are addressed as , a term from Go used to distinguish "kishi" from other classes of players. JSA professional ranks and female professional ranks are not equivalent and each has their own promotion criteria and ranking system. In 2006, the JSA officially granted women "professional status". This is not equivalent, however, to the more traditional way of "gaining professional status", i.e., being promoted from the : leagues of strong amateur players aspiring to become a professional. Rather, it is a separate system especially designed for female professionals. Qualified amateurs, regardless of gender, may apply for the "Shoreikai System" and all those who successfully "graduate" are granted "kishi" status; however, no woman has yet to accomplish this feat (the highest women have reached is Kana Satomi in "Shoreikai 3 "dan" league", currently one step away from "kishi" status), so "kishi" is de facto only used to refer to male shogi professionals.
The JSA is the only body which can organize tournaments for "professionals", e.g., the seven major tournaments in the titleholder system and other professional tournaments. In 1996, Yoshiharu Habu became the only "kishi" to hold all existing seven major titles at the same time. For female professionals, both the JSA and LPSA organize tournaments, either jointly or separately. Tournaments for amateurs may be organized by the JSA and LPSA as well as local clubs, newspapers, private corporations, educational institutions or municipal governments for cities or prefectures under the guidance of the JSA or LPSA.
Since the 1990s, shogi has grown in popularity outside Japan, particularly in the People's Republic of China, and especially in Shanghai. The January 2006 edition of stated that there were 120,000 shogi players in Shanghai. The spread of the game to countries where Chinese characters are not in common use, however, has been slower.
Etiquette.
Shogi players are expected to follow etiquette in addition to rules explicitly described. Commonly accepted etiquette include following:
Shogi piece sets may contain two types of king pieces, (king) and (jewel). In this case, the higher classed player, in either social or genuine shogi player rank, may take the king piece. For example, in titleholder system games, the current titleholder takes the king piece as the higher.
Computer shogi.
Shogi has the highest game complexity of all popular chess variants. Computers have steadily improved in playing shogi since the 1970s. In 2007, champion Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza at the level of two-dan shoreikai.
The JSA prohibits its professionals from playing computers in public without prior permission, with the reason of promoting shogi and monetizing the computer-human events.
On October 12, 2010, after some 35 years of development, a computer finally beat a professional player, when the top ranked female champion Ichiyo Shimizu was beaten by the Akara2010 system in a game lasting just over 6 hours.
On July 24, 2011, computer shogi programs Bonanza and Akara crushed the amateur team of Kosaku and Shinoda in two games. The allotted time for the amateurs was one hour and then three minutes per move. The allotted time for the computer was 25 minutes and then 10 seconds per move.
On April 20, 2013, GPS Shogi defeated 8-dan professional shogi player in a 102-move game which lasted over 8 hours.
The highest rated player on Shogi Club 24 is computer program Bonkras, rated 3335 on December 2, 2011.
Shogi video games.
Hundreds of video games were released exclusively in Japan for several consoles.
In popular culture.
Shogi has been a central plot point in manga and anime "Shion no Ō", manga and television series "81diver" and manga "March Comes in Like a Lion".
In the manga series "Naruto", shogi plays an essential part in Shikamaru Nara's character development. He often plays it with his teacher, Asuma Sarutobi, apparently always beating him. When Asuma is fatally injured in battle, he reminds Shikamaru that the shogi king must always be protected, and draws a parallel between the king in shogi and his yet-unborn daughter, Mirai, whom he wanted Shikamaru to guide. After Asuma's death, Shikamaru realizes that Asuma really meant that the entire next generation of ninja should be protected.
The Japanese culture and lifestyle television show "Begin Japanology" aired on NHK World featured a full episode on shogi in 2009.
References.
Bibliography
External links.
Rules
Online play

</doc>
<doc id="27743" url="https://en.wikipedia.org/wiki?curid=27743" title="Solar energy">
Solar energy

Solar energy is radiant light and heat from the Sun harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture and artificial photosynthesis.
It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on the way they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.
The large magnitude of solar energy available makes it a highly appealing source of electricity. The United Nations Development Programme in its 2000 World Energy Assessment found that the annual potential of solar energy was 1,575–49,837 exajoules (EJ). This is several times larger than the total world energy consumption, which was 559.8 EJ in 2012.
In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly 
import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".
Potential.
The Earth receives 174,000 terawatts (TW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most people around the world live in areas with insolation levels of 150 to 300 watts per square meter or 3.5 to 7.0 kWh/m2 per day.
Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived.
The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,
The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limits the amount of solar energy that we can acquire.
Geography affects solar energy potential because areas that are closer to the equator have a greater amount of solar radiation. However, the use of photovoltaics that can follow the position of the sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation effects the potential of solar energy because during the nighttime there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can affect the potential of solar panels because clouds block incoming light from the sun and reduce the light available for solar cells.
In addition, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is unowned and suitable for solar panels. Roofs have been found to be a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are unowned by businesses where solar plants can be established.
Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than geothermal and tidal, derive their energy from the Sun in a direct or indirect way.
Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies.
In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of 1,575–49,837 EJ per year "(see table below)".
Thermal energy.
Solar thermal technologies can be used for water heating, space heating, space cooling and process heat generation.
Early commercial adaption.
In 1897, Frank Shuman, a U.S. inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water, and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.
Shuman built the world’s first solar thermal power station in Maadi, Egypt, between 1912 and 1913. Shuman’s plant used parabolic troughs to power a engine that pumped more than of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman’s vision and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:
Water heating.
Solar hot water systems use sunlight to heat water. In low geographical latitudes (below 40 degrees) from 60 to 70% of the domestic hot water use with temperatures up to 60 °C can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.
As of 2007, the total installed capacity of solar hot water systems is approximately 154 thermal gigawatt (GWth). China is the world leader in their deployment with 70 GWth installed as of 2006 and a long-term goal of 210 GWth by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada and Australia heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GWth as of 2005.
Heating, cooling and ventilation.
In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.
Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting and shading conditions. When properly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.
A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.
Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.
Cooking.
Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of . Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of and above but require direct light to function properly and must be repositioned to track the Sun.
Process heat.
Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the "right to dry" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to and deliver outlet temperatures of . The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of had been installed worldwide, including an collector in Costa Rica used for drying coffee beans and a collector in Coimbatore, India, used for drying marigolds.
Water treatment.
Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of , could produce up to per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.
Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.
Solar energy may be used in a water stabilisation pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.
Electricity production.
Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect.
Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively.
Commercial CSP plants were first developed in the 1980s. Since 1985 the eventually 354 MW SEGS CSP installation, in the Mojave Desert of California, is the largest solar power plant in the world. Other large CSP plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world’s largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are grid connected using net metering and/or a feed-in tariff. In 2013 solar generated less than 1% of the worlds total grid electricity.
Photovoltaics.
In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceed 20% and the maximum efficiency of research photovoltaics is over 40%.
Concentrated solar power.
Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.
Architecture and urban planning.
Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.
The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans and switchable windows can complement passive design and improve system performance.
Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures are a result of increased absorption of the Solar light by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and plant trees. Using these methods, a hypothetical "cool communities" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1 billion, giving estimated total annual benefits of US$530 million from reduced air-conditioning costs and healthcare savings.
Agriculture and horticulture.
Agriculture and horticulture seek to optimize the capture of solar energy in order to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vinters, who use the energy generated by solar panels to power grape presses.
Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today, and plastic transparent materials have also been used to similar effect in polytunnels and row covers.
Transport.
Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was and by 2007 the winner's average speed had improved to .
The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.
Some vehicles use solar panels for auxiliary power, such as for air conditioning, to keep the interior cool, thus reducing fuel consumption.
In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar powered crossing of the Pacific Ocean, and the "sun21" catamaran made the first solar powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010.
In 1974, the unmanned AstroFlight Sunrise plane made the first solar flight. On 29 April 1979, the "Solar Riser" made the first flight in a solar-powered, fully controlled, man carrying flying machine, reaching an altitude of . In 1980, the "Gossamer Penguin" made the first piloted flights powered solely by photovoltaics. This was quickly followed by the "Solar Challenger" which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the "Pathfinder" (1997) and subsequent designs, culminating in the "Helios" which set the altitude record for a non-rocket-propelled aircraft at in 2001. The "Zephyr", developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2015, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The designed allows the aircraft to remain airborne for 36 hours.
A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.
Fuel production.
Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 the splitting of sea water providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants.
Hydrogen production technologies been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above . This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.
Energy storage methods.
Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.
Phase change materials such as paraffin wax and Glauber's salt are another thermal storage media. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately ). The "Dover House" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity and can deliver heat at temperatures compatible with conventional power systems. The Solar Two used this method of energy storage, allowing it to store in its 68 cubic metres storage tank with an annual storage efficiency of about 99%.
Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems a credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.
Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.
Development, deployment and economics.
Beginning with the surge in coal use which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum.
The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).
Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s and growth rates have averaged 20% per year since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154 GW as of 2007.
The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces: 
The development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.
In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water and concentrated solar power could provide a third of the world’s energy by 2060 if politicians commit to limiting climate change. The energy from the sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. "The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale".
ISO standards.
The International Organization for Standardization has established a number of standards relating to solar energy equipment. For example, ISO 9050 relates to glass in building while ISO 10217 relates to the materials used in solar water heaters.

</doc>
<doc id="27745" url="https://en.wikipedia.org/wiki?curid=27745" title="Standard conditions for temperature and pressure">
Standard conditions for temperature and pressure

Standard conditions for temperature and pressure are standard sets of conditions for experimental measurements to be established to allow comparisons to be made between different sets of data. The most used standards are those of the International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.
In chemistry, IUPAC could have established two standards: 
The STP and the SATP should not be confused with the standard state commonly used in thermodynamic evaluations of the Gibbs energy of a reaction.
NIST uses a temperature of 20 °C (293.15 K, 68 °F) and an absolute pressure of 1 atm (14.696 psi, 101.325 kPa). This standard is also called normal temperature and pressure (abbreviated as NTP).
The International Standard Metric Conditions for natural gas and similar fluids are and 101.325 kPa.
In industry and commerce, standard conditions for temperature and pressure are often necessary to define the standard reference conditions to express the volumes of gases and liquids and related quantities such as the rate of volumetric flow (the volumes of gases vary significantly with temperature and pressure). However, many technical publications (books, journals, advertisements for equipment and machinery) simply state "standard conditions" without specifying them, often leading to confusion and errors. Good practice is to always incorporate the reference conditions of temperature and pressure.
Definitions.
Past use.
In the last five to six decades, professionals and scientists using the metric system of units defined the standard reference conditions of temperature and pressure for expressing gas volumes as being and . During those same years, the most commonly used standard reference conditions for people using the imperial or U.S. customary systems was and 14.696 psi (1 atm) because it was almost universally used by the oil and gas industries worldwide. The above definitions are no longer the most commonly used in either system of units.
Current use.
Many different definitions of standard reference conditions are currently being used by organizations all over the world. The table below lists a few of them, but there are more. Some of these organizations used other standards in the past. For example, IUPAC has, since 1982, defined standard reference conditions as being 0 °C and 100 kPa (1 bar), in contrast to its old standard of 0 °C and 101.325 kPa (1 atm).
Natural gas companies in Europe and South America have adopted 15 °C (59 °F) and 101.325 kPa (14.696 psi) as their standard gas volume reference conditions. Also, the International Organization for Standardization (ISO), the United States Environmental Protection Agency (EPA) and National Institute of Standards and Technology (NIST) each have more than one definition of standard reference conditions in their various standards and regulations.
Notes:
International Standard Atmosphere.
In aeronautics and fluid dynamics the "International Standard Atmosphere" (ISA) is a specification of pressure, temperature, density, and speed of sound at each altitude. The International Standard Atmosphere is representative of atmospheric conditions at mid latitudes. In the USA this information is specified the U.S. Standard Atmosphere which is identical to the "International Standard Atmosphere" at all altitudes up to 65,000 feet above sea level.
Standard laboratory conditions.
Due to the fact that many definitions of standard temperature and pressure differ in temperature significantly from standard laboratory temperatures (e.g., 0 °C vs. ~25 °C), reference is often made to "standard laboratory conditions" (a term deliberately chosen to be different from the term "standard conditions for temperature and pressure", despite its semantic near identity when interpreted literally). However, what is a "standard" laboratory temperature and pressure is inevitably culture-bound, given that different parts of the world differ in climate, altitude and the degree of use of heat/cooling in the workplace. For example, schools in New South Wales, Australia use 25 °C at 100 kPa for standard laboratory conditions.
ASTM International has published Standard ASTM E41- Terminology Relating to Conditioning and hundreds of special conditions for particular materials and test methods. Other standards organizations also have specialized standard test conditions.
Molar volume of a gas.
It is equally as important to indicate the applicable reference conditions of temperature and pressure when stating the molar volume of a gas as it is when expressing a gas volume or volumetric flow rate. Stating the molar volume of a gas without indicating the reference conditions of temperature and pressure has very little meaning and can cause confusion.
The molar volume of gases around STP can be calculated with an accuracy that is usually sufficient by using the ideal gas law. The molar volume of any ideal gas may be calculated at various standard reference conditions as shown below:
Technical literature can be confusing because many authors fail to explain whether they are using the ideal gas constant R, or the specific gas constant Rs. The relationship between the two constants is Rs = R / m, where m is the molecular mass of the gas.
The US Standard Atmosphere (USSA) uses 8.31432 m3·Pa/(mol·K) as the value of "R". However, the USSA,1976 does recognize that this value is not consistent with the values of the Avogadro constant and the Boltzmann constant.

</doc>
<doc id="27750" url="https://en.wikipedia.org/wiki?curid=27750" title="Script kiddie">
Script kiddie

In programming and hacking culture, a script kiddie or skiddie (also known as "skid and" "script bunny", the term "script kitty" is not valid in this context) is an unskilled individual who uses scripts or programs developed by others to attack computer systems and networks and deface websites. It is generally assumed that script kiddies are juveniles who lack the ability to write sophisticated programs or exploits on their own and that their objective is to try to impress their friends or gain credit in computer-enthusiast communities. However, the term does not relate to the actual age of the participant. The term is generally considered to be pejorative.
Characteristics.
In a Carnegie Mellon report prepared for the U.S. Department of Defense in 2005, script kiddies are defined as "The more immature but unfortunately often just as dangerous exploiter of security lapses on the Internet. The typical script kiddy uses existing and frequently well known and easy-to-find techniques and programs or scripts to search for and exploit weaknesses in other computers on the Internet—often randomly and with little regard or perhaps even understanding of the potentially harmful consequences.
Script kiddies have at their disposal a large number of effective, easily downloadable programs capable of breaching computers and networks. Such programs have included remote denial-of-service WinNuke, trojans, Back Orifice, NetBus, Sub7, and ProRat, vulnerability scanner/injector kit Metasploit, and often software intended for legitimate security auditing.
Script kiddies vandalize websites both for the thrill of it and to increase their reputation among their peers. Some more malicious script kiddies have used virus toolkits to create and propagate the Anna Kournikova and Love Bug viruses.
Script kiddies lack, or are only developing, programming skills sufficient to understand the effects and side effects of their actions. As a result, they leave significant traces which lead to their detection, or directly attack companies which have detection and countermeasures already in place, or in recent cases, leave automatic crash reporting turned on.

</doc>
<doc id="27751" url="https://en.wikipedia.org/wiki?curid=27751" title="Scalable Vector Graphics">
Scalable Vector Graphics

Scalable Vector Graphics (SVG) is an XML-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.
SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, but are more often created with drawing software.
All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, and Safari—have at least some degree of SVG rendering support.
Overview.
SVG has been in development since 1999 by a group of companies within the W3C after the competing standards Precision Graphics Markup Language (PGML, developed from Adobe's PostScript) and Vector Markup Language (VML, developed from Microsoft's RTF) were submitted to W3C in 1998. SVG drew on experience from the designs of both those formats.
SVG allows three types of graphic objects: vector graphics, raster graphics, and text. Graphical objects, including PNG and JPEG raster images, can be grouped, styled, transformed, and composited into previously rendered objects. SVG does not directly support z-indices that separate drawing order from document order for overlapping objects, unlike some other vector markup languages like VML. Text can be in any XML namespace suitable to the application, which enhances search ability and accessibility of the SVG graphics. The feature set includes nested transformations, clipping paths, alpha masks, filter effects, template objects, and extensibility.
Since 2001, the SVG specification has been updated to version 1.1.
The SVG Mobile Recommendation introduced two simplified "profiles" of SVG 1.1, SVG Basic and SVG Tiny, meant for devices with reduced computational and display capabilities.
An enhanced version of SVG Tiny, called SVG Tiny 1.2, later became an autonomous Recommendation.
Work is currently in progress on SVG 2, which incorporates several new features in addition to those of SVG 1.1 and SVG Tiny 1.2.
Printing.
Though the SVG Specification primarily focuses on vector graphics markup language, its design includes the basic capabilities of a page description language like Adobe's PDF. It contains provisions for rich graphics, and is compatible with CSS for styling purposes. SVG has the information needed to place each glyph and image in a chosen location on a printed page. (By contrast, XHTML's primary purpose is to communicate content, not presentation, so XHTML specifies objects to be displayed but not where to place them.) A print-specialized subset of SVG (SVG Print, authored by Canon, HP, Adobe and Corel) is a W3C Working Draft.
Scripting and animation.
SVG drawings can be dynamic and interactive. Time-based modifications to the elements can be described in SMIL, or can be programmed in a scripting language (e.g. ECMAScript or JavaScript). The W3C explicitly recommends SMIL as the standard for animation in SVG. However SMIL was deprecated in Google Chrome as of Aug 2015.
A rich set of event handlers such as "onmouseover" and "onclick" can be assigned to any SVG graphical object.
Compression.
SVG images, being XML, contain many repeated fragments of text, so they are well suited for lossless data compression algorithms. When an SVG image has been compressed with the industry standard gzip algorithm, it is referred to as an "SVGZ" image and uses the corresponding codice_1 filename extension. Conforming SVG 1.1 viewers will display compressed images. An SVGZ file is typically 20 to 50 percent of the original size. W3C provides SVGZ files to test for conformance.
Development history.
SVG was developed by the W3C SVG Working Group starting in 1998, after Macromedia and Microsoft introduced VML whereas Adobe Systems and Sun Microsystems submitted a competing format known as PGML. The working group was chaired by Chris Lilley of the W3C.
The MPEG-4 Part 20 standard - "Lightweight Application Scene Representation (LASeR) and Simple Aggregation Format (SAF)" is based on SVG Tiny. It was developed by MPEG (ISO/IEC JTC1/SC29/WG11) and published as ISO/IEC 14496-20:2006. SVG capabilities are enhanced in MPEG-4 Part 20 with key features for mobile services, such as dynamic updates, binary encoding, state-of-art font representation. SVG was also accommodated in MPEG-4 Part 11, in the Extensible MPEG-4 Textual (XMT) format - a textual representation of the MPEG-4 multimedia content using XML.
Mobile profiles.
Because of industry demand, two mobile profiles were introduced with SVG 1.1: "SVG Tiny" (SVGT) and "SVG Basic" (SVGB). These are subsets of the full SVG standard, mainly intended for user agents with limited capabilities. In particular, SVG Tiny was defined for highly restricted mobile devices such as cellphones; it doesn't support styling or scripting. SVG Basic was defined for higher-level mobile devices, such as PDAs.
In 2003, the 3GPP, an international telecommunications standards group, adopted SVG Tiny as the mandatory vector graphics media format for next-generation phones. SVGT is the required vector graphics format and support of SVGB is optional for Multimedia Messaging Service (MMS) and Packet-switched Streaming Service. It was later added as required format for vector graphics in 3GPP IP Multimedia Subsystem (IMS).
Neither mobile profile includes support for the full DOM, while only SVG Basic has optional support for scripting, but because they are fully compatible subsets of the full standard, most SVG graphics can still be rendered by devices which only support the mobile profiles.
SVGT 1.2 adds a microDOM (μDOM), styling and scripting.
Functionality.
The SVG 1.1 specification defines 14 functional areas or feature sets:
An SVG document can define components including shapes, gradients etc., and use them repeatedly. SVG images can also contain raster graphics, such as PNG and JPEG images, and further SVG images.
Example.
This code will produce the shapes shown in the image (excluding the grid):
SVG on the web.
The use of SVG on the web was limited by the lack of support in older versions of Internet Explorer (IE). Many web sites that serve SVG images, such as Wikipedia, also provide the images in a raster format, either automatically by HTTP content negotiation or by allowing the user directly to choose the file.
Google announced on 31 August 2010 that it had started to index SVG content on the web, whether it is in standalone files or embedded in HTML, and that users would begin to see such content listed among their search results.
It was announced on 8 December 2010 that Google Image Search would also begin indexing SVG files. On 28 January 2011, it was discovered that Google was allowing Image Search results to be restricted exclusively to SVG files. This feature was announced officially on 11 February 2011.
Native browser support.
Konqueror was the first browser to support SVG in release version 3.2 in February 2004. As of 2011, all major desktop browsers, and many minor ones, have some level of SVG support. Other browsers' implementations are not yet complete; see comparison of layout engines for further details.
Some earlier versions of Firefox (e.g. versions between 1.5 and 3.6), as well as a smattering of other now-outdated web browsers capable of displaying SVG graphics, needed them embedded in codice_18 or codice_19 elements to display them integrated as parts of an HTML webpage instead of using the standard way of integrating images with codice_20. However, SVG images may be included in XHTML pages using XML namespaces.
Tim Berners-Lee, the inventor of the World Wide Web, has been critical of (earlier versions of) Internet Explorer for its failure to support SVG.
There are several advantages to native and full support: plugins are not needed, SVG can be freely mixed with other content in a single document, and rendering and scripting become considerably more reliable.
Plug-in browser support.
Internet Explorer, up to and including IE8, was the only major browser not to provide native SVG support. IE8 and older require a plug-in to render SVG content. There are a number of plug-ins available to assist, including:
On 5 January 2010, a senior manager of the Internet Explorer team at Microsoft announced on his official blog that Microsoft had just requested to join the SVG Working Group of the W3C in order to "take part in ensuring future versions of the SVG spec will meet the needs of developers and end users," although no plans for SVG support in Internet Explorer were mentioned at that time. Internet Explorer 9 beta supported a basic SVG feature set based on the SVG 1.1 W3C recommendation. Functionality has been implemented for most of the SVG document structure, interactivity through scripting and styling inline and through CSS. The presentation elements, attributes and DOM interfaces that have been implemented include basic shapes, colors, filling, gradients, patterns, paths and text.
Mobile support.
SVG Tiny (SVGT) 1.1 and 1.2 are mobile profiles for SVG. SVGT 1.2 includes some features not found in SVG 1.1, including non-scaling strokes, which are supported by some SVG 1.1 implementations, such as Opera, Firefox and WebKit. As shared code bases between desktop and mobile browsers increased, the use of SVG 1.1 over SVGT 1.2 also increased.
Support for SVG may be limited to SVGT on older or more limited smart phones, or may be primarily limited by their respective operating system. Adobe Flash Lite has optionally supported SVG Tiny since version 1.1. At the SVG Open 2005 conference, Sun demonstrated a mobile implementation of SVG Tiny 1.1 for the Connected Limited Device Configuration (CLDC) platform.
Mobiles that use Opera Mobile, as well as the iPhone's built in browser, also include SVG support. However, even though it used the WebKit engine, the Android built-in browser did not support SVG prior to v3.0 (Honeycomb). Prior to v3.0, Firefox Mobile 4.0b2 (beta) for Android was the first browser running under Android to support SVG by default.
The level of SVG Tiny support available varies from mobile to mobile, depending on the SVG engine installed. Many newer mobile products support additional features beyond SVG Tiny 1.1, like gradient and opacity; this is sometimes referred as "SVGT 1.1+", though there is no such standard.
Rim's BlackBerry has built-in support for SVG Tiny 1.1 since version 5.0. Support continues for WebKit-based BlackBerry Torch browser in OS 6 and 7.
Nokia's S60 platform has built-in support for SVG. For example, icons are generally rendered using the platform's SVG engine. Nokia has also led the JSR 226: Scalable 2D Vector Graphics API expert group that defines Java ME API for SVG presentation and manipulation. This API has been implemented in S60 Platform 3rd Edition Feature Pack 1 and onward. Some Series 40 phones also support SVG (such as Nokia 6280).
Most Sony Ericsson phones beginning with K700 (by release date) support SVG Tiny 1.1. Phones beginning with K750 also support such features as opacity and gradients. Phones with Sony Ericsson Java Platform-8 have support for JSR 226.
Windows Phone has supported SVG since version 7.5
SVG is also supported on various mobile devices from Motorola, Samsung, LG, and Siemens mobile/BenQ-Siemens. eSVG, an SVG rendering library mainly written for embedded devices, is available on some mobile platforms.
OpenVG is an API designed for hardware-accelerated 2D vector graphics. Its primary platforms are handheld devices, mobile phones, gaming or media consoles, and consumer electronic devices including operating systems with Gallium3D based graphics drivers.
Online SVG converters.
This is an incomplete list of web applications that can convert SVG files to raster image formats (this process is known as rasterization), or raster images to SVG (this process is known as image tracing or vectorization) - without the need of installing a desktop software or browser plug-in.
Application support.
SVG images can be produced by the use of a vector graphics editor, such as Inkscape, Adobe Illustrator, Adobe Flash Professional or CorelDRAW, and rendered to common raster image formats such as PNG using the same software. Inkscape uses a (built-in) potrace to import raster image formats.
Software can be programmed to render SVG images by using a library such as librsvg used by GNOME since 2000, or Batik. SVG images can also be rendered to any desired popular image format by using the free software command-line utility ImageMagick (which also uses librsvg under the hood).
Other uses for SVG include embedding for use in word processing (e.g. with LibreOffice) and desktop publishing (e.g. Scribus), plotting graphs (e.g. gnuplot), and importing paths (e.g. for use in GIMP or Blender). The Uniform Type Identifier for SVG used by Apple is public.svg-image and conforms to public.image and public.xml.

</doc>
<doc id="27752" url="https://en.wikipedia.org/wiki?curid=27752" title="Spectroscopy">
Spectroscopy

Spectroscopy is the study of the interaction between matter and electromagnetic radiation. Historically, spectroscopy originated through the study of visible light dispersed according to its wavelength, by a prism. Later the concept was expanded greatly to include any interaction with radiative energy as a function of its wavelength or frequency. Spectroscopic data is often represented by a spectrum, a plot of the response of interest as a function of wavelength or frequency.
Introduction.
Spectroscopy and spectrography are terms used to refer to the measurement of radiation intensity as a function of wavelength and are often used to describe experimental spectroscopic methods. Spectral measurement devices are referred to as spectrometers, spectrophotometers, spectrographs or spectral analyzers.
Daily observations of color can be related to spectroscopy. Neon lighting is a direct application of atomic spectroscopy. Neon and other noble gases have characteristic emission frequencies (colors). Neon lamps use collision of electrons with the gas to excite these emissions. Inks, dyes and paints include chemical compounds selected for their spectral characteristics in order to generate specific colors and hues. A commonly encountered molecular spectrum is that of nitrogen dioxide. Gaseous nitrogen dioxide has a characteristic red absorption feature, and this gives air polluted with nitrogen dioxide a reddish brown color. Rayleigh scattering is a spectroscopic scattering phenomenon that accounts for the color of the sky.
Spectroscopic studies were central to the development of quantum mechanics and included Max Planck's explanation of blackbody radiation, Albert Einstein's explanation of the photoelectric effect and Niels Bohr's explanation of atomic structure and spectra. Spectroscopy is used in physical and analytical chemistry because atoms and molecules have unique spectra. As a result, these spectra can be used to detect, identify and quantify information about the atoms and molecules. Spectroscopy is also used in astronomy and remote sensing on earth. Most research telescopes have spectrographs. The measured spectra are used to determine the chemical composition and physical properties of astronomical objects (such as their temperature and velocity).
Theory.
One of the central concepts in spectroscopy is a resonance and its corresponding resonant frequency. Resonances were first characterized in mechanical systems such as pendulums. Mechanical systems that vibrate or oscillate will experience large amplitude oscillations when they are driven at their resonant frequency. A plot of amplitude vs. excitation frequency will have a peak centered at the resonance frequency. This plot is one type of spectrum, with the peak often referred to as a spectral line, and most spectral lines have a similar appearance.
In quantum mechanical systems, the analogous resonance is a coupling of two quantum mechanical stationary states of one system, such as an atom, via an oscillatory source of energy such as a photon. The coupling of the two states is strongest when the energy of the source matches the energy difference between the two states. The energy formula_1 of a photon is related to its frequency formula_2 by formula_3 where formula_4 is Planck's constant, and so a spectrum of the system response vs. photon frequency will peak at the resonant frequency or energy. Particles such as electrons and neutrons have a comparable relationship, the de Broglie relations, between their kinetic energy and their wavelength and frequency and therefore can also excite resonant interactions.
Spectra of atoms and molecules often consist of a series of spectral lines, each one representing a resonance between two different quantum states. The explanation of these series, and the spectral patterns associated with them, were one of the experimental enigmas that drove the development and acceptance of quantum mechanics. The hydrogen spectral series in particular was first successfully explained by the Rutherford-Bohr quantum model of the hydrogen atom. In some cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough. Named series of lines include the principal, sharp, diffuse and fundamental series.
Classification of methods.
Spectroscopy is a sufficiently broad field that many sub-disciplines exist, each with numerous implementations of specific spectroscopic techniques. The various implementations and techniques can be classified in several ways.
Type of radiative energy.
Types of spectroscopy are distinguished by the type of radiative energy involved in the interaction. In many applications, the spectrum is determined by measuring changes in the intensity or frequency of this energy. The types of radiative energy studied include:
Nature of the interaction.
Types of spectroscopy can also be distinguished by the nature of the interaction between the energy and the material. These interactions include:
Type of material.
Spectroscopic studies are designed so that the radiant energy interacts with specific types of matter.
Atoms.
Atomic spectroscopy was the first application of spectroscopy developed. Atomic absorption spectroscopy (AAS) and atomic emission spectroscopy (AES) involve visible and ultraviolet light. These absorptions and emissions, often referred to as atomic spectral lines, are due to electronic transitions of outer shell electrons as they rise and fall from one electron orbit to another. Atoms also have distinct x-ray spectra that are attributable to the excitation of inner shell electrons to excited states.
Atoms of different elements have distinct spectra and therefore atomic spectroscopy allows for the identification and quantitation of a sample's elemental composition. Robert Bunsen and Gustav Kirchhoff discovered new elements by observing their emission spectra. Atomic absorption lines are observed in the solar spectrum and referred to as Fraunhofer lines after their discoverer. A comprehensive explanation of the hydrogen spectrum was an early success of quantum mechanics and explained the Lamb shift observed in the hydrogen spectrum led to the development of quantum electrodynamics.
Modern implementations of atomic spectroscopy for studying visible and ultraviolet transitions include flame emission spectroscopy, inductively coupled plasma atomic emission spectroscopy, glow discharge spectroscopy, microwave induced plasma spectroscopy, and spark or arc emission spectroscopy. Techniques for studying x-ray spectra include X-ray spectroscopy and X-ray fluorescence (XRF).
Molecules.
The combination of atoms into molecules leads to the creation of unique types of energetic states and therefore unique spectra of the transitions between these states. Molecular spectra can be obtained due to electron spin states (electron paramagnetic resonance), molecular rotations, molecular vibration and electronic states. Rotations are collective motions of the atomic nuclei and typically lead to spectra in the microwave and millimeter-wave spectral regions; rotational spectroscopy and microwave spectroscopy are synonymous. Vibrations are relative motions of the atomic nuclei and are studied by both infrared and Raman spectroscopy. Electronic excitations are studied using visible and ultraviolet spectroscopy as well as fluorescence spectroscopy.
Studies in molecular spectroscopy led to the development of the first maser and contributed to the subsequent development of the laser.
Crystals and extended materials.
The combination of atoms or molecules into crystals or other extended forms leads to the creation of additional energetic states. These states are numerous and therefore have a high density of states. This high density often makes the spectra weaker and less distinct, i.e., broader. For instance, blackbody radiation is due to the thermal motions of atoms and molecules within a material. Acoustic and mechanical responses are due to collective motions as well.
Pure crystals, though, can have distinct spectral transitions, and the crystal arrangement also has an effect on the observed molecular spectra. The regular lattice structure of crystals also scatters x-rays, electrons or neutrons allowing for crystallographic studies.
Nuclei.
Nuclei also have distinct energy states that are widely separated and lead to gamma ray spectra. Distinct nuclear spin states can have their energy separated by a magnetic field, and this allows for NMR spectroscopy.
Other types.
Other types of spectroscopy are distinguished by specific applications or implementations:
History.
The history of spectroscopy began with Isaac Newton's optics experiments (1666–1672). Newton applied the word "spectrum" to describe the rainbow of colors that combine to form white light and that are revealed when the white light is passed through a prism. During the early 1800s, Joseph von Fraunhofer made experimental advances with dispersive spectrometers that enabled spectroscopy to become a more precise and quantitative scientific technique. Since then, spectroscopy has played and continues to play a significant role in chemistry, physics and astronomy.

</doc>
<doc id="27753" url="https://en.wikipedia.org/wiki?curid=27753" title="List of science fiction themes">
List of science fiction themes

The following is a list of recurring themes in science fiction.

</doc>
<doc id="27760" url="https://en.wikipedia.org/wiki?curid=27760" title="Statute of Anne">
Statute of Anne

The Statute of Anne, also known as the Copyright Act 1709 (cited either as 8 Ann. c. 21 or as 8 Ann. c. 19), is an act of the Parliament of Great Britain passed in 1710, which was the first statute to provide for copyright regulated by the government and courts, rather than by private parties.
Prior to the statute's enactment in 1710, copying restrictions were authorized by the Licensing of the Press Act 1662. These restrictions were enforced by the Stationers' Company, a guild of printers given the exclusive power to print—and the responsibility to censor—literary works. The censorship administered under the Licensing Act led to public protest; as the act had to be renewed at two-year intervals, authors and others sought to prevent its reauthorisation. In 1694, Parliament refused to renew the Licensing Act, ending the Stationers' monopoly and press restrictions.
Over the next 10 years the Stationers repeatedly advocated bills to re-authorize the old licensing system, but Parliament declined to enact them. Faced with this failure, the Stationers decided to emphasise the benefits of licensing to authors rather than publishers, and the Stationers succeeded in getting Parliament to consider a new bill. This bill, which after substantial amendments was granted Royal Assent on 5 April 1710, became known as the Statute of Anne due to its passage during the reign of Queen Anne. The new law prescribed a copyright term of 14 years, with a provision for renewal for a similar term, during which only the author and the printers they chose to license their works to could publish the author's creations. Following this, the work's copyright would expire, with the material falling into the public domain. Despite a period of instability known as the Battle of the Booksellers when the initial copyright terms under the Statute began to expire, the Statute of Anne remained in force until the Copyright Act 1842 replaced it.
The statute is considered a "watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Under the statute, copyright was for the first time vested in authors rather than publishers; it also included provisions for the public interest, such as a legal deposit scheme. The Statute was an influence on copyright law in several other nations, including the United States, and even in the 21st century is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law".
Background.
With the introduction of the printing press to England by William Caxton in 1476, printed works became both more common and more economically important. As early as 1483, Richard III recognised the value of literary works by specifically exempting them from the government's protectionist legislation. Over the next fifty years, the government moved further towards economic regulation, abolishing the provision with the Printers and Binders Act 1534, which also banned the import of foreign works and empowered the Lord Chancellor to set maximum pricing for English books. This was followed by increasing degrees of censorship. A further proclamation of 1538, aiming to stop the spread of Lutheran doctrine, saw Henry VIII note that "sondry contentious and sinyster opiniones, have by wrong teachynge and naughtye bokes increaced and growen within this his realme of England", and declare that all authors and printers must allow the Privy Council or their agents to read and censor books before publication.
Stationers' Company.
This censorship peaked on 4 May 1557, when Mary I issued a royal warrant formally incorporating the Stationers' Company. The old method of censorship had been limited by the Second Statute of Repeal, and with Mary's increasing unpopularity the existing system was unable to cope with the number of critical works being printed. Instead, the royal warrant devolved this power to the Company. This was done by decreeing that only the Company's publishers could print and distribute books. Their Wardens were given the power to enter any printing premises, destroy illegal works and imprison anyone found manufacturing them. In this way the government "harnessed the self interest of the publishers to the yoke of royal incentive", guaranteeing that the Company would follow the rules due to the economic monopoly it gave their members. With the abolition of the Star Chamber and Court of High Commission by the Long Parliament, the legal basis for this warrant was removed, but the Long Parliament chose to replace it with the Licensing Act 1662. This provided that the Company would retain their original powers, and imposed additional restrictions on printing; King's Messengers were permitted to enter any home or business in search of illegal presses. The legislation required renewal every two years, and was regularly reapproved.
This was not "copyright" as is normally understood; although there was a monopoly on the right to copy, this was available to publishers, not authors, and did not exist by default; it only applied to books which had been accepted and published by the Company. A member of the Company would register the book, and would then have a perpetual copyright over its printing, copying and publication, which could be leased, transferred to others or given to heirs upon the member's death. The only exception to this was that, if a book was out of print for more than 6 months and the publisher ignored a warning to make it available, the copyright would be released and other publishers would be permitted to copy it. Authors themselves were not particularly respected until the 18th century, and were not permitted to be members of the Company, playing no role in the development or use of its licenses despite the Company's sovereign authority to decide what was published. There is evidence that some authors were recognised by the Company itself to have the right to copy and the right to alter their works; these authors were uniformly the writers of uneconomical books who were underwriting their publication.
The Company's monopoly, censorship and failure to protect authors made the system highly unpopular; John Milton wrote "Areopagitica" as a result of his experiences with the Company, accusing Parliament of being deceived by "the fraud of some old patentees and monopolisers in the trade of bookselling". He was not the first writer to criticise the system, with John Locke writing a formal memorandum to the MP Edward Clarke in 1693 while the Licensing Act was being renewed, complaining that the existing system restricted the free exchange of ideas and education while providing an unfair monopoly for Company members. Academic Mark Rose attributes the efforts of Milton to promote the "bourgeois public sphere", along with the Glorious Revolution's alterations to the political system and the rise of public coffee houses, as the source of growing public unhappiness with the system. At the same time, this was a period in which clearly defined political parties were taking shape, and with the promise of regular elections, an environment where the public were of increasing importance to the political process. The result was a "developing public sphere provided the context that enabled the collapse of traditional press controls".
Lapse of the Licensing Act.
The result of this environment was the lapse of the Licensing Act. In November 1694, a committee was appointed by the Commons to see what laws were "lately expired and expiring fit to be revived and continued". The Committee reported in January 1695, and suggested the renewal of the Licensing Act; this was included in the "Continuation Bill", but rejected by the House of Commons on 11 February. When it reached the House of Lords, the Lords re-included the Licensing Act, and returned the bill to the Commons. In response, a second committee was appointed - this one to produce a report indicating why the Commons disagreed with the inclusion of the Licensing Act, and chaired by Edward Clarke. This committee soon reported to the Commons, and Clarke was ordered to carry a message to the Lords requesting a conference over the Act. On 18 April 1695, Clarke met with representatives of the Lords, and they agreed to allow the Continuation Bill to pass without the renewal of the Licensing Act. With this, "the Lords' decision heralded an end to a relationship that had developed throughout the sixteenth and seventeenth centuries between the State and the Company of Stationers", ending both nascent publishers' copyright and the existing system of censorship.
John Locke's close relationship with Clarke, along with the respect he commanded, is seen by academics as what led to this decision. Locke had spent the early 1690s campaigning against the statute, considering it "ridiculous" that the works of dead authors were held perpetually in copyright. In letters to Clarke he wrote of the absurdity of the existing system, complaining primarily about the unfairness of it to authors, and "he parallels between Locke's commentary and those reasons presented by the Commons to the Lords for refusing to renew the 1662 Act are striking". He was assisted by a number of independent printers and booksellers, who opposed the monopolistic aspects of the Act, and introduced a petition in February 1693 that the Act prevented them from conducting their business. The "developing public sphere", along with the harm the existing system had caused to both major political parties, is also seen as a factor.
The failure to renew the Licensing Act led to confusion and both positive and negative outcomes; while the government no longer played a part in censoring publications, and the monopoly of the Company over printing was broken, there was uncertainty as to whether or not copyright was a binding legal concept without the legislation. Economic chaos also resulted; with the Company now unable to enforce any monopoly, provincial towns began establishing printing presses, producing cheaper books than the London booksellers. The absence of the censorship provisions also opened Britain up as a market for internationally printed books, which were similarly cheaper than those British printers could produce.
Attempts at replacement.
The rejection of the existing system was not done with universal approval, and there were ultimately twelve unsuccessful attempts to replace it. The first was introduced to the House of Commons on 11 February 1695. A committee, again led by Clarke, was to write a "Bill for the Better Regulating of Printing and the Printing Presses". This bill was essentially a copy of the Licensing Act, but with a narrower jurisdiction; only books covering religion, history, the affairs of the state or the law would require official authorisation. Four days after its introduction, the Stationers' held an emergency meeting to agree to petition the Commons - this was because the bill did not contain any reference to books as property, eliminating their monopoly on copying. Clarke also had issues with the provisions, and the debate went on until the end of the Parliamentary session, with the bill failing to pass.
With the end of the Parliamentary session came the first general election under the Triennial Act 1694, which required the Monarch to dissolve Parliament every 3 years, causing a general election. This led to the "golden age" of the English electorate, and allowed for the forming of two major political parties - the Whigs and Tories. At the same time, with the failure to renew the Licensing Act, a political press developed. While the Act had been in force only one official newspaper existed; the "London Gazette", published by the government. After its demise, a string of newspapers sprang into being, including the "Flying Post", the "Evening Post" and the "Daily Courant". Newspapers had a strong bias towards particular parties, with the "Courant" and the "Flying Post" supporting the Whigs and the "Evening Post" in favour of the Tories, leading to politicians from both parties realising the importance of an efficient propaganda machine in influencing the electorate. This added a new dimension to the Commons' decision to reject two new renewals of the Licensing Act in the new Parliamentary session.
Authors, as well as Stationers, then joined the demand for a new system of licensing. Jonathan Swift was a strong advocate for licensing, and Daniel Defoe wrote on 8 November 1705 that with the absence of licensing, "One Man Studies Seven Year, to bring a finish'd Peice into the World, and a Pyrate Printer, Reprints his Copy immediately, and Sells it for a quarter of the Price ... these things call for an Act of Parliament". Seeing this, the Company took the opportunity to experiment with a change to their approach and argument. Instead of lobbying because of the impact the absence of legislation was having on their trade, they lobbied on behalf of the authors, but seeking the same things. The first indication of this change in approach comes from the 1706 pamphlet by John How, a stationer, titled "Reasons humbly Offer'd for a Bill for the Encouragement of Learning and the Improvement of Printing". This argued for a return to licensing, not with reference to the printers, but because without something to protect authors and guarantee them an income, "Learned men will be wholly discouraged from Propagating the most useful Parts of Knowledge and Literature". Using these new tactics and the support of authors, the Company petitioned Parliament again in both 1707 and 1709 to introduce a bill providing for copyright.
Act.
Passage.
Although both bills failed, they led to media pressure that was exacerbated by both Defoe and How. Defoe's "A Review", published on 3 December 1709 and demanding "a Law in the present Parliament ... for the Encouragement of Learning, Arts, and Industry, by securing the Property of Books to the Authors or Editors of them", was followed by How's "Some Thoughts on the Present State of Printing and Bookselling", which hoped that Parliament "might think fit to secure Property in Books by a Law". This was followed by another review by Defoe on 6 December, in which he even went so far as to provide a draft text for the bill. On 12 December, the Stationers submitted yet another petition asking for legislation on the issue, and the House of Commons gave three MPs – Spencer Compton, Craven Peyton and Edward Wortley – permission to form a drafting committee. On 11 January 1710, Wortley introduced this bill, titling it "A Bill for the Encouragement of Learning and for Securing the Property of Copies of Books to the rightful Owners thereof".
The bill allowed for fines for anyone who imported or traded in unlicensed or foreign books, required every book that would be given copyright protection to be entered into the Stationers' Register, provided a legal deposit system centred around the King's Library, the University of Oxford and the University of Cambridge, but said nothing about limiting the term of copyright. It also specified that books were property; an emphasis on the idea that authors deserved copyright simply due to their efforts. The Stationers were enthusiastic, urging Parliament to pass the bill, and it received its second reading on 9 February. A Committee of the Whole met to amend it on 21 February, with further alterations made when it was passed back to the House of Commons on 25 February. Alterations during this period included minor changes, such as extending the legal deposit system to cover Sion College and the Faculty of Advocates, but also major ones, including the introduction of a limit on the length of time for which copyright would be granted.
Linguistic amendments were also included; the line in the preamble emphasising that authors possessed books as they would any other piece of property was dropped, and the bill moved from something designed "for Securing the Property of Copies of Books to the rightful Owners thereof" to a bill "for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of such Copies". Another amendment allowed anyone to own and trade in copies of books, undermining the Stationers. Other changes were made when the bill went to the House of Lords, and it was finally returned to the Commons on 5 April. The aims of the resulting statute are debated; Ronan Deazley suggests that the intent was to balance the rights of the author, publisher and public in such a way as to ensure the maximum dissemination of works, while other academics argue that the bill was intended to protect the Company's monopoly or, conversely, to weaken it. Oren Bracha, writing in the "Berkeley Technology Law Journal", says that when considering which of these options are correct, "the most probable answer all of them". Whatever the motivations, the bill was passed on 5 April 1710, and is commonly known simply as the Statute of Anne due its passage during the reign of Queen Anne.
Text.
Consisting of 11 sections, the Statute of Anne is formally titled "An Act for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of Copies, during the Times therein mentioned". The preamble for the Statute indicates the purpose of the legislation - to bring order to the book trade - saying: 
The Statute then moved on to stating the nature of copyright. The right granted was the right to copy; to have sole control over the printing and reprinting of books, with no provision to benefit the owner of this right after the sale. This right, previously held by the Stationers' Company's members, would automatically be given to the author as soon as it was published, although they had the ability to license these rights to another person. The copyright could be gained through two stages; first, the registration of the book's publication with the Company, to prevent unintentional infringement, and second, the deposit of copies of the book at the Stationers' Company, the royal library and various universities. One restriction on copyright was a "cumbersome system" designed to prohibit unreasonably high prices for books, which limited how much authors could charge for copies. There was also a prohibition on importing foreign works, with exceptions made for Latin and Greek classics.
Once registration had been completed and the deposits were made, the author was granted an exclusive right to control the copying of the book. Penalties for infringing this right were severe, with all infringing copies to be destroyed and large fines to be paid to both the copyright holder and the government; there was only a three-month statute of limitations on bringing a case, however. This exclusive right's length was dependent on when the book had been published. If it was published after 10 April 1710, the length of copyright was 14 years; if published before that date, 21 years. An author who survived until the copyright expired would be granted an additional 14-year term, and when that ran out, the works would enter the public domain. Copyright under the Statute applied to Scotland and England, as well as Ireland when that country joined the union in 1800.
Aftermath.
Impact.
The passage of the Statute was initially much welcomed, ushering in "stability to an insecure book trade" while providing for a "pragmatic bargain" between the rights of the author, publisher and public intended to boost public learning and the availability of knowledge. The clause requiring book deposits, however, was not seen as a success. If the books were not deposited, the penalties would be severe, with a fine of £5. The number of deposits required, however, meant that it was a substantial burden; a print run might only be of 250 copies, and if they were particularly expensive to print, it could be cheaper to ignore the law. Some booksellers argued that the deposit provision only applied to registered books, and so deliberately avoided registration just to be able to minimise their liability. This was further undermined by the ruling in "Beckford v Hood", where the Court of King's Bench confirmed that, even without registration, copyright could be enforced against infringers.
Another failure, identified by Bracha, is not found in what the Statute covered, but in what it did not. The Statute did not provide any means for identifying authors, did not identify what constituted authored works, and covered only "books", even while discussing "property" as a whole. Moreover, the right provided was merely that of "making and selling ... exact reprints. To a large extent, the new regime was the old stationer's privilege, except it was universalised, capped in time, and formally conferred upon authors rather than publishers". The impact of the Statute on authors was also minimal. Previously, publishers would have bought the original manuscript from writers for a lump sum; with the passage of the Statute, they simply did the same thing, but with the manuscript's copyright as well. The remaining economic power of the Company also allowed them to pressure booksellers and distributors into continuing their past arrangements, meaning that even theoretically "public domain" works were, in practise, still treated as copyrighted.
Battle of the Booksellers.
When the copyrights granted to works published before the Statute began to expire in 1731, the Stationers' Company and their publishers again began to fight to preserve the status quo. Their first port of call was Parliament, where they lobbied for new legislation to extend the length of copyright, and when this failed, they turned to the courts. Their principal argument was that copyright had not been created by the Statute of Anne; it existed beforehand, in the common law, and was perpetual. As such, even though the Statute provided for a limited term, all works remained in copyright under the common law regardless of when statutory copyright expired. Starting in 1743, this began a thirty-year campaign known as the "Battle of the Booksellers". They first tried going to the Court of Chancery and applying for injunctions prohibiting other publishers from printing their works, and this was initially successful. A series of legal setbacks over the next few years, however, left the law ambiguous.
The first major action taken to clarify the situation was "Millar v Taylor". Andrew Millar, a British publisher, purchased the rights to James Thomson's "The Seasons" in 1729, and when the copyright term expired, a competing publisher named Robert Taylor began issuing his own reprints of the work. Millar sued, and went to the Court of King's Bench to obtain an injunction and advocate perpetual copyright at common law. The jury found that the facts submitted by Millar were accurate, and asked the judges to clarify whether common law copyright existed. The first arguments were delivered on 30 June 1767, with John Dunning representing Millar and Edward Thurlow representing Taylor. A second set of arguments were submitted for Millar by William Blackstone on 7 June, and judgment was given on 20 April 1769. The final decision, written by Lord Mansfield and endorsed by Aston and Willes JJ, confirmed that there existed copyright at common law that turned "upon Principles before and independent" of the Statute of Anne, something justified because it was right "that an Author should reap the pecuniary Profits of his own Ingenuity and Labour". In other words, regardless of the Statute, there existed a perpetual copyright under the common law. Yates J dissented, on the grounds that the focus on the author obscured the impact this decision would have on "the rest of mankind", which he felt would be to create a virtual monopoly, something that would have a detrimental impact on the public and should certainly not be considered "an encouragement of the propagation of learning".
Although this decision was a boon to the Stationers, it was short-lived. Following "Millar", the right to print "The Seasons" was sold to a coalition of publishers including Thomas Becket. Two Scottish printers, Alexander and John Donaldson, began publishing an unlicensed edition, and Becket successfully obtained an injunction to stop them. This decision was appealed in "Donaldson v Beckett", and eventually went to the House of Lords. After consulting with the judges of the King's Bench, Common Pleas and Exchequer of Pleas, the Lords concluded that there was no copyright at common law - certainly not perpetual copyright - and as such, that the term permitted by the Statute of Anne was the maximum length of legal protection for publishers and authors alike.
Expansion and repeal.
Until its repeal, most extensions to copyright law were based around provisions found in the Statute of Anne. The one successful bill from the lobbying in the 1730s, which came into force on 29 September 1739, extended the provision prohibiting the import of foreign books to also prohibit the import of books that, while originally published in Britain, were being reprinted in foreign nations and then shipped to England and Wales. This was intended to stop the influx of cheap books from Ireland, and also repealed the price restrictions in the Statute of Anne. Another alteration was over the legal deposit provisions of the Statute, which many booksellers found unfair. Despite an initial period of compliance, the principle of donating copies of books to certain libraries lapsed, partly due to the unwieldiness of the statute's provisions and partly because of a lack of cooperation by the publishers. In 1775 Lord North, who was Chancellor of the University of Oxford, succeeded in passing a bill that reiterated the legal deposit provisions and granted the universities perpetual copyright on their works.
Another range of extensions came in relation to what could be copyrighted. The Statute only referred to books, and being an Act of Parliament, it was necessary to pass further legislation to include various other types of intellectual property. The Engraving Copyright Act 1734 extended copyright to cover engravings, statutes in 1789 and 1792 involved cloth, sculptures were copyrighted in 1814 and the performance of plays and music were covered by copyright in 1833 and 1842 respectively. The length of copyright was also altered; the Copyright Act 1814 set a copyright term of either 28 years, or the natural life of the author if this was longer. Despite these expansions, some still felt copyright was not a strong enough regime. In 1837, Thomas Noon Talfourd introduced a bill into Parliament to expand the scope of copyright. A friend of many men of letters, Talfourd aimed to provide adequate rewards for authors and artists. He campaigned for copyright to exist for the life of the author, with an additional 60 years after that. He also proposed that existing statutes be codified under the bill, so that the case law that had arisen around the Statute of Anne was clarified.
Talfourd's proposals led to opposition, and he reintroduced modified versions of them year on year. Printers, publishers and booksellers were concerned about the cost implications for original works, and for reprinting works that had fallen out of copyright. Many within Parliament argued that the bill failed to take into account the public interest, including Lord Macaulay, who succeeded in defeating one of Talfourd's bills in 1841. The Copyright Act 1842 passed, but "fell far short of Talfourd's dream of a uniform, consistent, codified law of copyright". It extended copyright to life plus seven years, and, as part of the codification clauses, repealed the Statute of Anne.
Significance.
The Statute of Anne is traditionally seen as "a historic moment in the development of copyright", and the first statute in the world to provide for copyright. Craig Joyce and Lyman Ray Patterson, writing in the "Emory Law Journal", call this a "too simple understanding ignores the statute's source", arguing that it is at best a derivative of the Licensing Act. Even considering this, however, the Statute of Anne was "the watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Patterson, writing separately, does note the differences between the Licensing Act and the Statute of Anne; the question of censorship was, by 1710, out of the question, and in that regard the Statute is distinct, not providing for censorship.
It also marked the first time that copyright had been vested primarily in the author, rather than the publisher, and also the first time that the injurious treatment of authors by publishers was recognised; regardless of what authors signed away, the second 14-year term of copyright would automatically return to them. Even in the 21st century, the Statute of Anne is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law". In "IceTV v Nine Network", for example, the High Court of Australia noted that the title of the Statute "echoed explicitly the emphasis on the practical or utilitarian importance that certain seventeenth-century philosophers attached to knowledge and its encouragement in the scheme of human progress". Despite "widely recognised flaws", the Act became a model copyright statute, both within the United Kingdom and internationally. Christophe Geiger notes that it is "a difficult, almost impossible task" to analyse the relationship between the Statute of Anne and early French copyright law, both because it is difficult to make a direct connection, and because the ongoing debate over both has led to radically different interpretations of each nation's law.
Similarly, Belgium took no direct influence from the Statute or English copyright theory, but Joris Deene of the University of Ghent identifies an indirect influence "at two levels"; the criteria for what constitutes copyrightable material, which comes from the work of English theorists such as Locke and Edward Young, and the underlying justification of copyright law. In Belgium, this justification is both that copyright serves the public interest, and that copyright is a "private right" that serves the interests of individual authors. Both theories were taken into account in "Donaldson v Beckett", as well as in the drafting of the Statute of Anne, and Deene infers that they subsequently had an impact on the Belgian debates over their first copyright statute. In the United States, the Copyright Clause of the United States Constitution and the first Federal copyright statute, the Copyright Act of 1790, both draw on the Statute of Anne. The 1790 Act contains provisions for a 14-year term of copyright and sections that provide for authors who published their works before 1790, both of which mirror the protection offered by the Statute 80 years previously.

</doc>
<doc id="27761" url="https://en.wikipedia.org/wiki?curid=27761" title="School choice">
School choice

School choice is a term or label given to a wide array of programs offering students and their families alternatives to publicly provided schools, to which students are generally assigned by the location of their family residence. In the United States, the most common—both by number of programs and by number of participating students—school choice programs are scholarship tax credit programs, which allow individuals or corporations to receive tax credits toward their state taxes in exchange for donations made to non-profit organizations that grant private school scholarships. In other cases, a similar subsidy may be provided by the state through a school voucher program. Other school choice options include open enrollment laws (which allow students to attend public schools outside of the district in which the students live), charter schools, magnet schools, virtual schools, homeschooling, education savings accounts (ESAs), and individual tax credits or deductions for educational expenses.
Forms.
Scholarship Tax Credits.
States with scholarship tax credit programs grant individuals and/or businesses a credit, whether full or partial, toward their taxes for donations made to scholarship granting organizations (also called school tuition organizations). SGOs/STOs use the donations to create scholarships that are then given to help pay for the cost of tuition for students. These scholarships allow students to attend private schools or out-of-district public schools that would otherwise be prohibitively expensive for many families. These programs currently exist in fourteen states: Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia in the United States.
Vouchers.
In a traditional public education system, schools receive funding from the state on a per student basis. Under a voucher system, eligible "students" receive state funding ("vouchers") which can be spent at whatever eligible private schools the parents choose for their children. The two most common voucher designs are universal vouchers and means-tested vouchers. Means-tested vouchers are directed towards low-income families and constitute the bulk of voucher plans in the United States.
Charter schools.
Charter schools are independent public schools which are exempt from many of the state and local regulations which govern most public schools. These exemptions grant charter schools some autonomy and flexibility with decision-making, such as teacher union contracts, hiring, and curriculum. In return, charter schools are subject to stricter accountability on spending and academic performance. The majority of states (and the District of Columbia) have charter school laws, though they vary in how charter schools are approved. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy High School, opened in St. Paul, Minnesota in 1992.
22-26% of Dayton, Ohio children are in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%), and Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of cyber charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like all charter schools, cyber charters are public schools, but they are free from some of the rules and regulations that conventional public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology, or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, some (but not all) magnet schools require a test to get in.
Home schooling.
"Home education" or "home schooling" is instruction in a child's home, or provided primarily by a parent, or under direct parental control. Informal home education has always taken place, and formal instruction in the home has at times also been very popular. As public education grew in popularity during the 1900s, however, the number of people educated at home using a planned curriculum dropped. In the last 20 years, in contrast, the number of children being formally educated at home has grown tremendously, in particular in the United States. The laws relevant to home education differ throughout the country. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the federal government, about 1.1 million children were home educated in 2003.
Education Savings Accounts.
Education Savings Accounts (ESAs) are somewhat similar to vouchers: a percentage of the funds that the state would otherwise spend to educate a student in a public school are instead given to the student's family to spend on private school tuition. However, ESAs give parents additional flexibility to customize their children's educations. For example, in addition to private school tuition, ESA funds may be used for private tutoring or online learning. Alternatively, ESA funds may be saved to pay for future higher education costs. Currently, there are ESA programs in two states: Arizona ("Empowerment Savings Accounts") and Florida (the "Personal Learning Scholarship Account Program").
Tax Credit/Deduction For Educational Expenses.
Certain states allow parents to claim a tax credit or deduction as a means to provide relief for certain educational expenses. These can include private school tuition, textbooks, school supplies and equipment, tutoring, and transportation. Currently, Alabama, Illinois, Indiana, Iowa, Louisiana, Minnesota, and Wisconsin have such programs.
Debate.
Support.
The goal of school choice programs is to give parents more control over their child's education and to allow parents to pursue the most appropriate learning environments for children. For example, school choice may enable parents to choose a school that provides religious instruction, stronger discipline, better foundational skills (including reading, writing, mathematics, and science), everyday skills (from handling money to farming), or other desirable foci.
Supporters of voucher models of school choice argue that choice creates competition between schools for students. Schools that fail to attract students can be closed. Advocates of school choice argue that this competition for students (and the dollars that come with them) create a catalyst for schools to create innovative programs, become more responsive to parental demands, and to increase student achievement. Caroline Hoxby suggests that this competition increases the productivity of a school. Hoxby describes a productive school as being one that produces high achievements in its student for each dollar that is spends. Others suggest that this competition gives parents more power to influence their child's school in the school marketplace. Parents and students become the consumers and schools must work to attract new students with new programs. Parents also have the ability to punish schools that they judge to be inferior by leaving the 'bad' school for a better, more highly ranked school. Parents look for schools that will advocate for the needs of their child and if the school does not meet the needs required for that child, parents have the choice to find a school that will be more suitable
Another argument in favor of school choice is based on cost-effectiveness. Studies undertaken by the Cato Institute and other libertarian and conservative thinktanks conclude that privately run education both costs less and produces superior outcomes compared to public education.
Others argue that since children from impoverished families almost exclusively attend D or F ranked public schools, school choice programs would give parents the power to opt their children out of poorly-performing schools assigned by zip code and seek better education elsewhere. Supporters say this would level the playing field by broadening opportunities for low-income students—particularly minorities—to attend high-quality schools that would otherwise be accessible only to higher-income families.
The Organisation Internationale pour le Droit à l'Education et la Liberté d'Enseignement (OIDEL), an international non-profit organization for the development of freedom of education, maintains that the right to education is a fundamental human right which cannot exist without the presence of State benefits and the protection of individual liberties. According to the organization, freedom of education notably implies the freedom for parents to choose a school for their children without discrimination on the basis of finances. To advance freedom of education, OIDEL promotes a greater parity between public and private schooling systems.
Opposition.
School choice measures are criticized as profiteering in an under-regulated environment. Charter authorization organizations have non-profit status; and contract with related for-profit entities with public funding. Some reports indicate that the New Markets Tax Credit allows double returns on charter school related investments. Reports indicate that charters create organizational arms that profit by charging high rent, and that while the facilities are used as schools, there are no property taxes. Other reports indicate bankers, hedge fund types and private equity investors gathered in New York to hear about opportunities at Capital Roundtable’s conference on “private equity investing in for-profit education companies” which involve the collection of an individual's property taxes. Walton Foundation has also held charter school investment conferences featuring Standard & Poor's, Piper Jaffray, Bank of America, and Wells Capital Management.
Public school entities are chiefly concerned that these school choice measures are taking funding away from public schools and therefore depleting their already strained resources. Other opponents of certain school choice policies (particularly vouchers) have cited the Establishment Clause and individual state Blaine amendments, which forbid, to one degree or another, the use of direct government aid to religiously affiliated entities. This is of particular concern in the voucher debate because voucher dollars are often spent at parochial schools.
Some school choice measures are criticized by public school entities, organizations opposed to church-state entanglement, and self-identified liberal advocacy groups. Known plaintiffs who have filed suit to challenge the constitutionality of state sponsored school choice laws are as follows: School Boards Associations, Public School Districts, Federations for Teachers, Associations of School Business Officials, Education Associations/Associations of Educators (unions for public school teachers), the American Civil Liberties Union, Freedom From Religion Foundation, and People for the American Way.
International overview and major institutional options.
Finland.
The basic compulsory educational system in Finland is the nine-year comprehensive school (Finnish "peruskoulu", Swedish "grundskola", "basic school"), for which school attendance is mandatory (homeschooling is allowed, but extremely rare). There are no so-called "gifted" programs. The more able children are expected to help those who are slower to catch on.
France.
The French government subsidizes most private primary and secondary schools, including those affiliated with religious denominations, under contracts stipulating that education must follow the same curriculum as public schools and that schools cannot discriminate on grounds of religion or force pupils to attend religion classes.
This system of "école libre" (Free Schooling) is mostly used not for religious reasons, but for practical reasons (private schools may offer more services, such as after-class tutoring) as well as the desire of parents living in disenfranchised areas to send their children away from the local schools, where they perceive that the youth are too prone to delinquency or have too many difficulties keeping up with schooling requirements that the educational content is bound to suffer. The threatened repealing of that status in the 1980s triggered mass street demonstrations in favor of the status. 
Sweden.
Sweden reformed its school system in 1992. Its system of school choice is one of the freest in the world, allowing students to use public funds for the publicly or privately run school of their choice, including religious and for-profit schools. Fifteen years after the reform, private school enrolment had increased from 1% to 10% of the student population.
Canada.
Ontario is the only large province in Canada with limited school choice funding, Catholic, Secular and one Protestant school receive funding and are open to all students. In 2003, following an international human rights ruling, the provincial Conservative government gradually introduced a tax credit over 5 years, (when it would have been fully implemented it would have been worth up to 50% of tuition to a maximum of $3,500 at any independent school in Ontario) in order to meet the human rights norms and expand funded choice to all interested parents. However, the tax credit was retroactively canceled by the subsequent Liberal government when it had been only been in place for two years to the $1,000 point. Currently there are over 900 independent schools in Ontario. The only school choice program available to non-rich parents who wish to send their children to an independent school is a privately funded program called Children First, a program of The Fraser Institute.
Chile.
In Chile, there is an extensive voucher system in which the state pays private and municipal schools directly, based on average attendance (90% of the country students utilize such a system). The result has been a steady increase in the number and recruitment of private schools that show consistently better results in standardized testing than municipal schools. The reduction of students in municipal schools has gone from 78% of all students in 1981, to 57% in 1990, and to less than 50% in 2005.
Regarding vouchers in Chile, researchers have found that when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant. There is also greater variation within each subsector than between the two systems.
United States.
A variety of forms of school choice exist in the United States.
Scholarship Tax Credits.
Scholarship tax credit programs currently exist in Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia.
Arizona has a well-known and fast-growing tax credit program. In the Arizona Individual Private School Tuition Tax Credit Program, in accordance with A.R.S. §43-1089 and §1089.03, individuals can claim up to $1,053 and couples filing joint returns can claim up to $2106 (for 2014, amounts are indexed annually). Nearly 24,000 children received scholarships in the 2011-2012 school year. Since the program has started in 1998, over 77,500 taxpayers have participated in the program, providing over $500 million in scholarship money for children at private schools across the state.
The Arizona program was challenged in court in "ACSTO v Winn" by a group of state taxpayers on the grounds that the tax credit violated the First Amendment because the tuition grants could go to students who attend private schools with religious affiliations. The suit was initially brought against the state until the Arizona Christian School Tuition Organization (ACSTO), one of the largest School Tuition Organizations in the state, voluntarily stepped in to represent the defense with the help of the Alliance Defending Freedom (formerly Alliance Defense Fund). Typically, taxpayers are not allowed to bring suit against the government regarding how taxes are spent because injury would be purely speculative. In addition, insomuch as a donation to a School Tuition Organization is still a charitable act, just like any donation to a charity, there would be no standing unless all charitable deduction programs nationwide were brought under scrutiny. The Court ruled 5-4 to let the tax credit program stand. In April 2011, a Fairleigh Dickinson University PublicMind poll found that a majority of American voters (60%) felt that the tax credits support school choice for parents whereas 26% felt as it the tax credits support religion.
In Iowa, the Educational Opportunities Act was signed into law in 2006, creating a pool of tax credits for eligible donors to student tuition organizations (STOs). At first, these tax caps were $5 million but in 2007, Governor Chet Culver increased the total amount to $7.5 million. The Iowa Alliance for Choice in Education (Iowa ACE) oversees the STOs and advocates for school choice in Iowa.
Greater Opportunities for Access to Learning (GOAL) is the Georgia program which offers a state income tax credit to donors of scholarships to private schools. Representative David Casas was responsible for passing the Georgia version of the school choice legislation.
Vouchers.
Vouchers currently exist in Wisconsin, Ohio, Florida, Indiana and, most recently, the District of Columbia and Georgia.
The largest and oldest Voucher program is in Milwaukee. Started in 1990, and expanded in 1995, it currently allows no more than 15% of the district's public school enrollment to use vouchers. As of 2005 over 14,000 students use vouchers and they are nearing the 15% cap.
School vouchers are legally controversial in some states. In 2014 a lawsuit sought to challenge the legality of the Florida voucher program.
In the U.S., the legal and moral precedents for vouchers may have been set by the G.I. bill, which includes a voucher program for university-level education of veterans. The G.I. bill permits veterans to take their educational benefits at religious schools, an extremely divisive issue when applied to primary and secondary schools.
In "Zelman v. Simmons-Harris", 536 U.S. 639 (2002), the Supreme Court of the United States held that school vouchers could be used to pay for education in sectarian schools without violating the Establishment Clause of the First Amendment. As a result, states are basically free to enact voucher programs that provide funding for any school of the parent's choosing.
The Supreme Court has not decided, however, whether states can provide vouchers for secular schools only, excluding sectarian schools. Proponents of funding for parochial schools argue that such an exclusion would violate the free exercise clause. However, in "Locke v. Davey", 540 U.S. 712 (2004), the Court held that states could exclude majors in "devotional theology" from an otherwise generally available college scholarship. The Court has not indicated, however, whether this holding extends to the public school context, and it may well be limited to the context of individuals training to enter the ministry.
Charter schools.
The majority of states (and the District of Columbia) have charter school laws. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy, opened in St. Paul, Minnesota in 1992.
Dayton, Ohio has between 22–26% of all children in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%) and the State of Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of Cyber Charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like charter schools, they are public schools, but free of many of the rules and regulations that public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, the students must test into the school.
Home schooling.
The laws relevant to homeschooling differ between US states. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the Federal Government, about 1.1 million children were Home Educated in 2003.
College.
The United States has school choice at the university level. College students can get subsidized tuition by attending "any" public college or university within their state of residence. Furthermore, the U.S. federal government provides tuition assistance for both public and private colleges via the G.I. Bill and federally guaranteed student loans.

</doc>
<doc id="27762" url="https://en.wikipedia.org/wiki?curid=27762" title="Star Frontiers">
Star Frontiers

Star Frontiers is a science fiction role-playing game produced by TSR beginning in 1982. The game offered a space-opera action-adventure setting.
Setting.
"Star Frontiers" takes place near the center of a spiral galaxy (the setting does not specify whether the galaxy is our own Milky Way). 
A previously undiscovered quirk of the laws of physics allows starships to jump to "The Void", a hyperspatial realm that greatly shortens the travel times between inhabited worlds, once they reach 1% of the speed of light (3,000 km/s).
The basic game setting was an area known as "The Frontier Sector" where four sentient races (Dralasite, Humans, Vrusk, and Yazirian) had met and formed the United Planetary Federation (UPF). The original homeworlds of the Dralasites, Humans, and Vrusk were never detailed in the setting and it is possible that they no longer existed. A large number of the star systems shown on the map of the Frontier sector in the basic rulebook were unexplored and undetailed, allowing the Gamemaster (called the "referee" in the game) to put whatever they wished there.
Players could take on any number of possible roles in the setting but the default was to act as hired agents of the Pan Galactic corporation in exploring the Frontier and fighting the aggressive incursions of the alien and mysterious worm-like race known as the Sathar. Most published modules for the game followed these themes.
Sapient races.
These races were altered heavily and reused in TSR's "Spelljammer", and were later loosely republished for "d20 Future" by Wizards of the Coast.
Game mechanics.
The game was a percentile-based system and used only 10-sided dice (d10). Characters had attributes rated from 1-100 (usually in the 25-75 range) which could be rolled against for raw-attribute actions such as lifting items or getting out of the way of falling rocks. There were eight attributes that were paired together (and shared the same rating to begin with)—Strength/Stamina, Dexterity/Reaction Speed, Intuition/Logic, and Personality/Leadership.
Characters also each had a Primary Skill Area (PSA—Military, Technological, or Biosocial) which allowed them to buy skills that fell into their PSA at a discount. Skills were rated from 1–6 and usually consisted of a set of subskills that gave a chance for accomplishing a particular action as a base percentage plus a 10% bonus for each skill level the character had in the skill. Weapon skills were based on the character's relevant attribute (Dexterity or Strength) but other skills had a base chance of success independent of the character's attributes. Many of the technological skills were penalized by the complexity of the robot, security system, or computer the character was attempting to manipulate (also rated from 1 to 6).
Characters were usually quite durable in combat—it would take several hits from normal weapons to kill an average character. Medical technology was also advanced—characters could recover quickly from wounds with appropriate medical attention and a dead character could be "frozen" and revived later.
Vehicle and robot rules were included in the "Alpha Dawn" basic set. A beneficial feature of the game was its seamless integration of personal, vehicle and aerial combat simulation. The "Knight Hawks" rules expansion set included detailed rules for starships.
The basic set also included a short "bestiary" of creatures native to the world of Volturnus (the setting for the introductory module included with the basic boxed set), along with rules for creating new creatures.
Character advancement consisted of spending experience points on improving skills and attributes.
Products.
The basic boxed set was renamed "Alpha Dawn" after the expansions began publication. It included two ten-sided dice, a large set of cardboard counters, and a folding map with a futuristic city on one side and various wilderness areas on the other for use with the included adventure, SF-0: "Crash on Volturnus".
A second boxed set called "Knight Hawks" followed shortly. It provided rules for using starships in the setting and also a set of wargame rules for fighting space battles between the UPF and Sathar. Included were counters for starships, two-ten sided dice, a large folding map with open space on one side and on the other a space station and starship (for use with the included adventure), and the adventure SFKH-0: "Warriors of White Light". This set was designed by Douglas Niles (who also designed the D&D wargame "Battlesystem", released two years later).
Adventures printed separately for the game included two more adventures set on Volturnus (SF-1: "Volturnus, Planet of Mystery" and SF-2: "Starspawn of Volturnus" continuing the adventure included in the basic set), SF-3: "Sundown on Starmist", SF-4: "Mission to Alcazzar", SF-5: "Bugs in the System" and SF-6: "Dark Side of the Moon". The last two modules (SF-5 and SF-6) were written by authors from TSR's UK division, and are distinctly different from the others in the series in tone and production style.
Adventures using the "Knight Hawks" rules included SFKH-1: "Dramune Run" and a trilogy set "Beyond the Frontier" in which the players learn more about the Sathar and foil their latest plot (SFKH-2: "Mutiny on the Eleanor Moraes", SFKH-3: "Face of the Enemy", and SFKH-4: "The War Machine").
Two modules also re-created the plot and setting of the movies ' and '.
A late addition to the line was "Zebulon's Guide to Frontier Space" which introduced several additional races and radical changes to the game's mechanics. Of the three planned volumes of the Guide, only the first was ever published (in 1985), leaving the game in an uncomfortable, half-overhauled state. Gamers were given little to no practical advice on how to convert their existing characters to the new rules, and TSR never published any further products using the "Zebulon's" concepts.
Current products.
Wizards of the Coast published many of the races originally found in "Star Frontiers" in their "d20 Future" supplement for d20 Modern.
Current versions of the original game are fanworks. In addition to the "Star frontiers" Redux and "Knight Hawks" Vector Rules, a high-quality fan E-zine named "Star Frontiersman" is being published on a regular basis. A multiplayer flight simulator version of Knight Hawks Vector is being developed for Orbiter Space Simulator program, and a virtual tabletop version is also in the works. Play-by-forum post and OpenRPG games are currently being played with new ones starting all the time in the Starfrontiers.org forum.
A version of the setting called "Star Law", which uses the d20 system rules was published as an alternate campaign setting in the d20 Future book. It uses the species names of Vrusk, Dralasite, Sathar, and Yazirian, but is not actually the "Star Frontiers" setting.

</doc>
<doc id="27763" url="https://en.wikipedia.org/wiki?curid=27763" title="Structuralism">
Structuralism

In sociology, anthropology and linguistics, structuralism is the methodology that elements of human culture must be understood in terms of their relationship to a larger, overarching system or structure. It works to uncover the structures that underlie all the things that humans do, think, perceive, and feel. Alternatively, as summarized by philosopher Simon Blackburn, structuralism is "the belief that phenomena of human life are not intelligible except through their interrelations. These relations constitute a structure, and behind local variations in the surface phenomena there are constant laws of abstract culture".
Structuralism in Europe developed in the early 1900s, in the structural linguistics of Ferdinand de Saussure the subsequent Prague, Moscow and Copenhagen schools of linguistics. In the late 1950s and early '60s, when structural linguistics was facing serious challenges from the likes of Noam Chomsky and thus fading in importance, an array of scholars in the humanities borrowed Saussure's concepts for use in their respective fields of study. French anthropologist Claude Lévi-Strauss was arguably the first such scholar, sparking a widespread interest in structuralism.
The structuralist mode of reasoning has been applied in a diverse range of fields, including anthropology, sociology, psychology, literary criticism, economics and architecture. The most prominent thinkers associated with structuralism include Lévi-Strauss, linguist Roman Jakobson, and psychoanalyst Jacques Lacan. As an intellectual movement, structuralism was initially presumed to be the heir apparent to existentialism. However, by the late 1960s, many of structuralism's basic tenets came under attack from a new wave of predominantly French intellectuals such as the philosopher and historian Michel Foucault, the philosopher and social commentator Jacques Derrida, the Marxist philosopher Louis Althusser, and the literary critic Roland Barthes. Though elements of their work necessarily relate to structuralism and are informed by it, these theorists have generally been referred to as post-structuralists. In the 1970s, structuralism was criticised for its rigidity and ahistoricism. Despite this, many of structuralism's proponents, such as Lacan, continue to assert an influence on continental philosophy and many of the fundamental assumptions of some of structuralism's post-structuralist critics are a continuation of structuralism.
Overview.
The term "structuralism" is a belated term that describes a particular philosophical/literary movement or moment. The term appeared in the works of French anthropologist Claude Lévi-Strauss and gave rise, in France, to the "structuralist movement." Influencing the thinking of writers such as Louis Althusser, the psychoanalyst Jacques Lacan, as well as the structural Marxism of Nicos Poulantzas, most of whom disavowed themselves as being a part of this movement.
The origins of structuralism connect with the work of Ferdinand de Saussure on linguistics, along with the linguistics of the Prague and Moscow schools. In brief, de Saussure's structural linguistics propounded three related concepts.
Proponents of structuralism would argue that a specific domain of culture may be understood by means of a structure—modelled on language—that is distinct both from the organizations of reality and those of ideas or the imagination—the "third order". In Lacan's psychoanalytic theory, for example, the structural order of "the Symbolic" is distinguished both from "the Real" and "the Imaginary"; similarly, in Althusser's Marxist theory, the structural order of the capitalist mode of production is distinct both from the actual, real agents involved in its relations and from the ideological forms in which those relations are understood.
Blending Freud and de Saussure, the French (post)structuralist Jacques Lacan applied structuralism to psychoanalysis and, in a different way, Jean Piaget applied structuralism to the study of psychology. But Jean Piaget, who would better define himself as constructivist, considers structuralism as "a method and not a doctrine" because for him "there exists no structure without a construction, abstract or genetic".
Although the French theorist Louis Althusser is often associated with a brand of structural social analysis which helped give rise to "structural Marxism", such association was contested by Althusser himself in the Italian foreword to the second edition of "Reading Capital". In this foreword Althusser states the following: 
"Despite the precautions we took to distinguish ourselves from the 'structuralist' ideology ..., despite the decisive intervention of categories foreign to 'structuralism' ..., the terminology we employed was too close in many respects to the 'structuralist' terminology not to give rise to an ambiguity. With a very few exceptions ... our interpretation of Marx has generally been recognized and judged, in homage to the current fashion, as 'structuralist'... We believe that despite the terminological ambiguity, the profound tendency of our texts was not attached to the 'structuralist' ideology."
In a later development, feminist theorist Alison Assiter enumerated four ideas that she says are common to the various forms of structuralism. First, that a structure determines the position of each element of a whole. Second, that every system has a structure. Third, structural laws deal with co-existence rather than change. Fourth, structures are the "real things" that lie beneath the surface or the appearance of meaning.
Structuralism in linguistics.
In Ferdinand de Saussure's "Course in General Linguistics" (written by Saussure's colleagues after his death and based on student notes), the analysis focuses not on the "use" of language (called ""parole"", or speech), but rather on the underlying system of language (called ""langue""). This approach examines how the elements of language relate to each other in the present, synchronically rather than diachronically. Saussure argued that linguistic signs were composed of two parts:
This was quite different from previous approaches that focused on the relationship between words and the things in the world that they designate. Other key notions in structural linguistics include paradigm, syntagm, and value (though these notions were not fully developed in Saussure's thought). A structural "idealism" is a class of linguistic units (lexemes, morphemes or even constructions) that are possible in a certain position in a given linguistic environment (such as a given sentence), which is called the "syntagm". The different functional role of each of these members of the paradigm is called "value" ("valeur" in French).
Saussure's "Course" influenced many linguists between World War I and World War II. In the United States, for instance, Leonard Bloomfield developed his own version of structural linguistics, as did Louis Hjelmslev in Denmark and Alf Sommerfelt in Norway. In France Antoine Meillet and Émile Benveniste continued Saussure's project, and members of the Prague school of linguistics such as Roman Jakobson and Nikolai Trubetzkoy conducted research that would be greatly influential. However, by the 1950s Saussure's linguistic concepts were under heavy criticism and were soon largely abandoned by practicing linguists: 
"Saussure's views are not held, so far as I know, by modern linguists, only by literary critics and the occasional philosopher. adherence to Saussure has elicited wrong film and literary theory on a grand scale. One can find dozens of books of literary theory bogged down in signifiers and signifieds, but only a handful that refer to Chomsky."
The clearest and most important example of Prague school structuralism lies in phonemics. Rather than simply compiling a list of which sounds occur in a language, the Prague school sought to examine how they were related. They determined that the inventory of sounds in a language could be analyzed in terms of a series of contrasts. Thus in English the sounds /p/ and /b/ represent distinct phonemes because there are cases (minimal pairs) where the contrast between the two is the only difference between two distinct words (e.g. 'pat' and 'bat'). Analyzing sounds in terms of contrastive features also opens up comparative scope—it makes clear, for instance, that the difficulty Japanese speakers have differentiating /r/ and /l/ in English is because these sounds are not contrastive in Japanese. Phonology would become the paradigmatic basis for structuralism in a number of different fields.
Structuralism in anthropology.
According to structural theory in anthropology and social anthropology, meaning is produced and reproduced within a culture through various practices, phenomena and activities that serve as systems of signification. A structuralist approach may study activities as diverse as food-preparation and serving rituals, religious rites, games, literary and non-literary texts, and other forms of entertainment to discover the deep structures by which meaning is produced and reproduced within the culture. For example, Lévi-Strauss analyzed in the 1950s cultural phenomena including mythology, kinship (the alliance theory and the incest taboo), and food preparation. In addition to these studies, he produced more linguistically focused writings in which he applied Saussure's distinction between "langue" and "parole" in his search for the fundamental structures of the human mind, arguing that the structures that form the "deep grammar" of society originate in the mind and operate in people unconsciously. Lévi-Strauss took inspiration from mathematics.
Another concept used in structural anthropology came from the Prague school of linguistics, where Roman Jakobson and others analyzed sounds based on the presence or absence of certain features (such as voiceless vs. voiced). Lévi-Strauss included this in his conceptualization of the universal structures of the mind, which he held to operate based on pairs of binary oppositions such as hot-cold, male-female, culture-nature, cooked-raw, or marriageable vs. tabooed women.
A third influence came from Marcel Mauss (1872–1950), who had written on gift-exchange systems. Based on Mauss, for instance, Lévi-Strauss argued that kinship systems are based on the exchange of women between groups (a position known as 'alliance theory') as opposed to the 'descent'-based theory described by Edward Evans-Pritchard and Meyer Fortes. While replacing Marcel Mauss at his "Ecole Pratique des Hautes Etudes" chair, Lévi-Strauss' writing became widely popular in the 1960s and 1970s and gave rise to the term "structuralism" itself.
In Britain, authors such as Rodney Needham and Edmund Leach were highly influenced by structuralism. Authors such as Maurice Godelier and Emmanuel Terray combined Marxism with structural anthropology in France. In the United States, authors such as Marshall Sahlins and James Boon built on structuralism to provide their own analysis of human society. Structural anthropology fell out of favour in the early 1980s for a number of reasons. D'Andrade suggests that this was because it made unverifiable assumptions about the universal structures of the human mind. Authors such as Eric Wolf argued that political economy and colonialism should be at the forefront of anthropology. More generally, criticisms of structuralism by Pierre Bourdieu led to a concern with how cultural and social structures were changed by human agency and practice, a trend which Sherry Ortner has referred to as 'practice theory'.
Some anthropological theorists, however, while finding considerable fault with Lévi-Strauss's version of structuralism, did not turn away from a fundamental structural basis for human culture. The Biogenetic Structuralism group for instance argued that some kind of structural foundation for culture must exist because all humans inherit the same system of brain structures. They proposed a kind of neuroanthropology which would lay the foundations for a more complete scientific account of cultural similarity and variation by requiring an integration of cultural anthropology and neuroscience—a program that theorists such as Victor Turner also embraced.
Structuralism in literary theory and criticism.
In literary theory, structuralist criticism relates literary texts to a larger structure, which may be a particular genre, a range of intertextual connections, a model of a universal narrative structure, or a system of recurrent patterns or motifs. Structuralism argues that there must be a structure in every text, which explains why it is easier for experienced readers than for non-experienced readers to interpret a text. Hence, everything that is written seems to be governed by specific rules, or a "grammar of literature", that one learns in educational institutions and that are to be unmasked.
A potential problem of structuralist interpretation is that it can be highly reductive, as scholar Catherine Belsey puts it: "the structuralist danger of collapsing all difference." An example of such a reading might be if a student concludes the authors of "West Side Story" did not write anything "really" new, because their work has the same structure as Shakespeare's "Romeo and Juliet". In both texts a girl and a boy fall in love (a "formula" with a symbolic operator between them would be "Boy + Girl") despite the fact that they belong to two groups that hate each other ("Boy's Group - Girl's Group" or "Opposing forces") and conflict is resolved by their death. Structuralist readings focus on how the structures of the single text resolve inherent narrative tensions. If a structuralist reading focuses on multiple texts, there must be some way in which those texts unify themselves into a coherent system. The versatility of structuralism is such that a literary critic could make the same claim about a story of two "friendly" families ("Boy's Family + Girl's Family") that arrange a marriage between their children despite the fact that the children hate each other ("Boy - Girl") and then the children commit suicide to escape the arranged marriage; the justification is that the second story's structure is an 'inversion' of the first story's structure: the relationship between the values of love and the two pairs of parties involved have been reversed.
Structuralistic literary criticism argues that the "literary banter of a text" can lie only in new structure, rather than in the specifics of character development and voice in which that structure is expressed. Literary structuralism often follows the lead of Vladimir Propp, Algirdas Julien Greimas, and Claude Lévi-Strauss in seeking out basic deep elements in stories, myths, and more recently, anecdotes, which are combined in various ways to produce the many versions of the ur-story or ur-myth.
There is considerable similarity between structural literary theory and Northrop Frye's archetypal criticism, which is also indebted to the anthropological study of myths. Some critics have also tried to apply the theory to individual works, but the effort to find unique structures in individual literary works runs counter to the structuralist program and has an affinity with New Criticism.
History and background.
Throughout the 1940s and 1950s, existentialism, such as that propounded by Jean-Paul Sartre, was the dominant European intellectual movement. Structuralism rose to prominence in France in the wake of existentialism, particularly in the 1960s. The initial popularity of structuralism in France led to its spread across the globe.
Structuralism rejected the concept of human freedom and choice and focused instead on the way that human experience and thus, behavior, is determined by various structures. The most important initial work on this score was Claude Lévi-Strauss's 1949 volume "The Elementary Structures of Kinship". Lévi-Strauss had known Jakobson during their time together at the New School in New York during WWII and was influenced by both Jakobson's structuralism as well as the American anthropological tradition. In "Elementary Structures" he examined kinship systems from a structural point of view and demonstrated how apparently different social organizations were in fact different permutations of a few basic kinship structures. In the late 1950s he published "Structural Anthropology", a collection of essays outlining his program for structuralism.
By the early 1960s structuralism as a movement was coming into its own and some believed that it offered a single unified approach to human life that would embrace all disciplines. Roland Barthes and Jacques Derrida focused on how structuralism could be applied to literature.
The so-called "Gang of Four" of structuralism was Lévi-Strauss, Lacan, Barthes, and Foucault.
Interpretations and general criticisms.
Structuralism is less popular today than other approaches, such as post-structuralism and deconstruction. Structuralism has often been criticized for being ahistorical and for favoring deterministic structural forces over the ability of people to act. As the political turbulence of the 1960s and 1970s (and particularly the student uprisings of May 1968) began affecting academia, issues of power and political struggle moved to the center of people's attention.
In the 1980s, deconstruction—and its emphasis on the fundamental ambiguity of language rather than its crystalline logical structure—became popular. By the end of the century structuralism was seen as an historically important school of thought, but the movements that it spawned, rather than structuralism itself, commanded attention.
Several social thinkers and academics have strongly criticized structuralism or even dismissed it "in toto". The French hermeneutic philosopher Paul Ricœur (1969) criticized Lévi-Strauss for constantly overstepping the limits of validity of the structuralist approach, ending up in what Ricoeur described as "a Kantianism without a transcendental subject". Anthropologist Adam Kuper (1973) argued that "'Structuralism' came to have something of the momentum of a millennial movement and some of its adherents felt that they formed a secret society of the seeing in a world of the blind. Conversion was not just a matter of accepting a new paradigm. It was, almost, a question of salvation." Philip Noel Pettit (1975) called for an abandoning of "the positivist dream which Lévi-Strauss dreamed for semiology" arguing that semiology is not to be placed among the natural sciences. Cornelius Castoriadis (1975) criticized structuralism as failing to explain symbolic mediation in the social world; he viewed structuralism as a variation on the "logicist" theme, and he argued that, contrary to what structuralists advocate, language—and symbolic systems in general—cannot be reduced to logical organizations on the basis of the binary logic of oppositions. Critical theorist Jürgen Habermas (1985) accused structuralists, such as Foucault, of being positivists; he remarked that while Foucault is not an ordinary positivist, he nevertheless paradoxically uses the tools of science to criticize science (see "Performative contradiction" and "Foucault–Habermas debate"). Sociologist Anthony Giddens (1993) is another notable critic; while Giddens draws on a range of structuralist themes in his theorizing, he dismisses the structuralist view that the reproduction of social systems is merely "a mechanical outcome".

</doc>
<doc id="27764" url="https://en.wikipedia.org/wiki?curid=27764" title="Systems engineering">
Systems engineering

Systems engineering is an interdisciplinary field of engineering that focuses on how to design and manage complex engineering systems over their life cycles. Issues such as requirements engineering, reliability, logistics, "coordination" of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, control engineering, software engineering, organizational studies, and project management. Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identify the most probable or highest impact failures that can occur - systems engineering involves finding elegant solutions to these problems.
History.
The term "systems engineering" can be traced back to Bell Telephone Laboratories in the 1940s. The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. Military, to apply the discipline.
When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly. The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in the better comprehension and the design and development control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.
In 1990, a professional society for systems engineering, the "National Council on Systems Engineering" (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995. Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.
Concept.
Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.
Origins and traditional scope.
The traditional scope of engineering embraces the conception, design, development, production and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. "Systems engineering", in this sense of the term, refers to the distinctive set of concepts, methodologies, organizational structures (and so on) that have been developed to meet the challenges of engineering effective functional systems of unprecedented size and complexity within time, budget, and other constraints. The Apollo program is a leading example of a systems engineering project.
Evolution to broader scope.
The use of the term "systems engineer" has evolved over time to embrace a wider, more holistic concept of "systems" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy, and the term continues to apply to both the narrower and broader scope.
Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical system, such as space craft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' "can be read in its general sense; you can engineer a meeting or a political agreement."
Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK) has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.
Holistic view.
Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into
Within Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes "assessing available information", "defining effectiveness measures", to "create a behavior model", "create a structure model", "perform trade-off analysis", and "create sequential build & test plan".
Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.
Interdisciplinary field.
System development often requires contribution from diverse technical disciplines. By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.
This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.
Managing complexity.
The need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.
The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:
Taking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.
Scope.
One way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems. Keeping this in mind, the principles of systems engineering – holism, emergent behavior, boundary, et al. – can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels. Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.
An analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15-20% of the total project effort. At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits. However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.
Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.
Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)—all currently being explored, evaluated, and developed to support the engineering decision process.
Education.
Education in systems engineering is often seen as an extension to the regular engineering courses, reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, automotive engineering, electrical engineering, mechanical engineering, industrial engineering)—plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs in systems engineering are rare. Typically, systems engineering is offered at the graduate level in combination with interdisciplinary study.
INCOSE maintains a continuously updated Directory of Systems Engineering Academic Programs worldwide. As of 2009, there are about 80 institutions in United States that offer 165 undergraduate and graduate programs in systems engineering. Education in systems engineering can be taken as "Systems-centric" or "Domain-centric".
Both of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.
Systems engineering topics.
Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.
System.
There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:
The systems engineering process.
Depending on their application, tools are used for various stages of the systems engineering process:
Using models.
Models play important and diverse roles in systems engineering. A model can be defined in several
ways, including:
Together, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.
The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation. Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.
Modeling formalisms and graphical representations.
Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements. Common graphical representations include:
A graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.
Once the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.
Other tools.
Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.
Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.
Related fields and sub-fields.
Many related fields may be considered tightly coupled to systems engineering. These areas have contributed to the development of systems engineering as a distinct entity.

</doc>
<doc id="27765" url="https://en.wikipedia.org/wiki?curid=27765" title="September 4">
September 4


</doc>
<doc id="27766" url="https://en.wikipedia.org/wiki?curid=27766" title="Sam &amp; Max">
Sam &amp; Max

Sam & Max is a media franchise focusing on the fictional characters of Sam and Max, the Freelance Police. The characters, who occupy a universe that parodies American popular culture, were created by Steve Purcell in his youth, and later debuted in a 1987 comic book series. The characters have since been the subject of a graphic adventure video game developed by LucasArts, a produced for Fox in cooperation with Nelvana Limited, and a series of episodic adventure games developed by Telltale Games. In addition, a variety of machinima and a webcomic have been produced for the series.
The characters are a pair of anthropomorphic, vigilante private investigators based in a dilapidated office block in New York City. Sam is a calculative six-foot dog wearing a suit and a fedora, while Max is a short and aggressive "hyperkinetic rabbity thing". Both enjoy solving problems and cases as maniacally as possible, often with complete disregard for the law. Driving a seemingly indestructible black-and-white 1960 DeSoto Adventurer, the pair travel to many contemporary and historical locations to fight crime, including the Moon, Ancient Egypt, the White House and the Philippines, as well as several fictitious locations.
The series has been very successful despite its relatively limited amount of media, and has gathered a significant fan base. However, the franchise did not gain more widespread recognition until after the 1993 release of LucasArts' "Sam & Max Hit the Road", which cultivated interest in Purcell's original comics. "Sam & Max Hit the Road" is regarded as an exceptional adventure game and an iconic classic of computer gaming in the 1990s. Subsequent video games and the television series have also fared well with both critics and fans; critics consider the episodic video games to be the first successful application of the episodic distribution model.
Overview.
Creation.
The idea of "Sam & Max" originated with Steve Purcell's younger brother, Dave, who invented the concept of a comic about a detective team consisting of a dog and a rabbit in his youth. Dave would often leave the comics around the house, so Steve, in a case of sibling rivalry, often finished the incomplete stories in parodies of their original form, deliberately making the characters mix up each other's names, over-explain things, shoot at each other and mock the way in which they had been drawn, as "kind of a parody of the way a kid talks when he's writing comics". Over time, this developed from Steve merely mocking his brother's work to him creating his own full stories with the characters. Ultimately, in the late 1970s, Dave Purcell gave Steve the rights to the characters, signing them over in a contract on Steve's birthday and allowing him to develop the characters in his own way. In 1980, Purcell began to produce "Sam & Max" comic strips for the weekly newsletter of the California College of Arts and Crafts. Whilst the visual appearance of the characters had not yet been fully developed, the stories were similar in style to those that would follow when Purcell was offered by "Fish Police" author Steven Moncuse the chance to publish his work properly in 1987. 
Many aspects of the "Sam & Max" comics were influenced by Purcell's own experiences. Rats and cockroaches are common throughout the franchise, the former inspired by Purcell's pet rat. In another example, Sam and Max are occasionally shown playing a game called "fizzball", in which the object of the exercise is to hit a can of beer in mid-air with a solid axe handle. Purcell had previously invented the game with his friends, including fellow comic book writers Art Adams and Mike Mignola.
Characters.
Sam is a laid-back but enthusiastic, brown-coated anthropomorphic "canine shamus". He wears either a gray or blue suit with a matching fedora, to make people more cooperative when conversing with a six-foot talking dog. A warped sense of justice makes Sam the more passionate of the pair for their police work, only held back from taking his job seriously by Max. Nevertheless, he enjoys the mannerisms and dress that come with their line of work. Sam possesses near encyclopedic amounts of knowledge, particularly on obscure topics, and is prone to long-winded sentences filled with elaborate terminology. Although he is always keen to display this information—regardless of its accuracy—Sam can be capable of total ignorance towards more practical matters; for instance, despite his regard for his DeSoto Adventurer, he is severely negligent with the car's maintenance. Sam still retains various doglike qualities: he is excitable, enthusiastic but also susceptible to emotions of embarrassment and guilt. Nevertheless, Sam is "not above sticking his head out the car window and letting his tongue flap in the breeze". Sam rarely loses his temper, and is able to react to panic-inducing situations with extreme calm. When he does get angry, Sam tends to react in a violent, uncharacteristically savage manner, in which case it is usually Max that calms him down and prevents him from acting upon his anger. Sam usually is armed with an oversized .44 revolver.
Max is an anthropomorphic "hyperkinetic, three-foot rabbity thing" with white fur, but prefers being called a lagomorph. Max retains few characteristics consistent with a rabbit, with permanently rigid ears set in an excited posture and a huge jaw normally stuck in a crazed grin. Unhinged, uninhibited and near psychotic, Max enjoys violence and tends to prefer the aggressive way of solving problems, seeing the world as little more than a vessel for his "pinball-like stream of consciousness". This creates a seeming disregard for self-preservation; Max will revel in dangerous situations with little impression that he understands the risks he faces. As a result, Max is usually enthusiastic to engage in any activity, including being used by Sam as a cable cutter or an impromptu bludgeon. Despite this, Max possesses a sharp mind and an observational nature, and enjoys interpreting new experiences in as unpredictable manner as possible. However, Max has a distaste for long stories and occasionally loses focus during lengthy scenes of plot exposition; by his own admission, Max possesses a particularly short attention span. Despite his seemingly heartless personality, he believes strongly in protecting Sam. However, Max can still act violently towards his friend, stating that when he dies he will take Sam with him. Moreover, Max is extremely possessive of Sam and their status as partners and best friends. Max traditionally carries a Luger pistol, but as he wears no clothes, other characters often make comments as to where Max keeps it on his person. Purcell considers Max to be representative of pure id, the uncoordinated instinctual trends of the human psyche.
Media.
Comic books.
Sam and Max debuted in the 1987 comic book series "Sam & Max: Freelance Police", published by Fishwrap Productions, also the publisher of "Fish Police". The first comic, "Monkeys Violating the Heavenly Temple", was Steve Purcell's first full story. The comic came about after Purcell agreed to create a full "Sam & Max" story for publication alongside Steve Moncuse's "Fish Police" series. "Monkeys Violating the Heavenly Temple" established many of the key features in the series; the main story of the comic saw the Freelance Police journey to the Philippines to stop a volcano god cult. "Night of the Gilded Heron-Shark" and "Night of the Cringing Wildebeest" accompanied the main story, focusing on a stand-off with a group of gangsters in Sam and Max's office and an investigation into a carnival refreshment booth respectively.
Over the subsequent years, several other comics were published, often by different publishers, including Comico Comics and Epic Comics. "Fair Wind to Java" was originally published in 1988 as a Munden's Bar story in the pages of First Comics' "Grimjack", featuring the Freelance Police fighting pyramid-building aliens in Ancient Egypt, and was followed in 1989 by "On the Road", a three chapter story showing what Sam and Max do on vacation. In 1990, a Christmas themed story, "The Damned Don't Dance" was released. 1992 saw the release of a further two comics; "Bad Day On The Moon" took the Freelance Police to deal with a roach infestation bothering giant rats on the Moon, and was later adapted as a story for the animated TV series, whilst "Beast From The Cereal Aisle" focused on the duo conducting an exorcism at the local supermarket. Two more comics were produced in 1997, "The Kids Take Over" and "Belly Of The Beast". The former has Sam and Max wake up from cryogenic sleep to discover that the entire world is now ruled by children while the latter sees the Freelance Police confronting a vampire abducting children at Halloween.
Purcell joined LucasArts in 1988 as an artist and game designer, where he was approached about contributing to LucasArts' new quarterly newsletter, "The Adventurer", a publication designed to inform customers about upcoming LucasArts games and company news. From its debut issue in 1990 to 1996, Purcell created twelve comic strips for the newsletter. The strips portrayed a variety of stories, from similar plots as in the comic books to parodies of LucasArts games such as "Monkey Island" and "Full Throttle" and the Lucasfilm franchises "Star Wars" and "Indiana Jones".
In 1995, all of the comics and "The Adventurer" strips published to that date were released in a compilation, "Sam & Max: Surfin' the Highway". Published by Marlowe & Company, the 154 page book was updated and republished in 1996. This original version of "Surfin' the Highway" went out of print in 1997, becoming a high priced collectors item sold through services such as eBay. In 2007, a 197-page twenty-year anniversary edition, containing all printed comics and strips as well as a variety of other artwork, was co-designed by Steve Purcell and Jake Rodkin and published by Telltale Games. This second publication received an Eisner Award nomination for "Best Graphic Album – Reprint" in 2009.
In December 2005, Purcell started a "Sam & Max" webcomic, hosted on the website of Telltale Games. Entitled "The Big Sleep", the webcomic began with Sam and Max bursting out of their graves at Kilpeck Church in England, symbolizing the Freelance Police's return after nearly a decade. In the twelve page story, Max has to save Sam after earwigs start a colony in Sam's brain. The webcomic concluded in April 2007, and was later awarded the Eisner Award for "Best Digital Comic" of 2007.
Video games.
Following LucasArts' employment of Purcell in 1988, the characters of Sam and Max appeared in internal testing material for new SCUMM engine programmers; Purcell created animated versions of the characters and an office backdrop for the programmers to practice on. In 1992, LucasArts offered Purcell the chance to create a video game out of the characters, out of a wish to use new characters after the success of its two other main adventure titles, "Monkey Island" and "Maniac Mansion", and after a positive reaction from fans to the "Sam & Max" comic strips featured in LucasArts' "The Adventurer" newsletter. Consequently, development on a graphic adventure game, "Sam & Max Hit the Road", began shortly after. Based on the SCUMM engine and designed by Sean Clark, Michael Stemmle, Steve Purcell and his future wife Collette Michaud, the game was partially based on the 1989 comic "On The Road", and featured the Freelance Police travelling across America in search of an escaped bigfoot. Sam was voiced in the game by comedian Bill Farmer, while actor Nick Jameson voiced Max. "Sam & Max Hit the Road" was originally released for DOS in November 1993. Soon after "Sam & Max Hit the Road", another "Sam & Max" game using SCUMM entered planning under Purcell and Dave Grossman, but was abandoned. In a later interview Grossman described this sequel's highlight as "a giant spaceship shaped like Max's head".
In September 2001 development began on a new project, "Sam & Max Plunge Through Space". The game was to be an Xbox exclusive title, developed by Infinite Machine, a small company consisting of a number of former LucasArts employees. The story of the game was developed by Purcell and fellow designer Chuck Jordan and involved the Freelance Police travelling the galaxy to find a stolen Statue of Liberty. However, Infinite Machine went bankrupt within a year, partially due to the failure of their first game, "New Legends", and the project was abandoned.
At the 2002 Electronic Entertainment Expo convention, nearly a decade after the release of "Sam & Max Hit the Road", LucasArts announced the production of a PC sequel, entitled "Sam & Max: Freelance Police". "Freelance Police", like "Hit the Road", was to be a point-and-click graphic adventure game, utilising a new 3D game engine. Development of "Freelance Police" was led by Michael Stemmle. Steve Purcell contributed to the project by writing the story and producing concept art. Farmer and Jameson were also set to reprise their voice acting roles. In March 2004, however, quite far into the game's development, "Sam & Max: Freelance Police" was abruptly cancelled by LucasArts, citing "current market place realities and underlying economic considerations" in a short press release. The fan reaction to the cancellation was strong; a petition of 32,000 signatures stating the disappointment of fans was later presented to LucasArts.
After LucasArts' license with Steve Purcell expired in 2005, the "Sam & Max" franchise moved to Telltale Games, a company of former LucasArts employees who had worked on a number of LucasArts adventure games, including on the development of "Freelance Police". Under Telltale Games, a new episodic series of "Sam & Max" video games was announced. Like both "Sam & Max Hit the Road" and "Freelance Police", "Sam & Max Save the World" was in a point-and-click graphic adventure game format. The game utilized a new 3D game engine, different from the one used in "Freelance Police". The first season ran for six episodes, each with a self-contained storyline but with an overall story arc involving hypnotism running through the series. The first episode was released on GameTap in October 2006, with episodes following regularly until April 2007. Sam is voiced by David Nowlin, while Max is voiced by William Kasten in all episodes except the first one, where Andrew Chaikin voices the character. In addition, Telltale Games produced fifteen machinima shorts to accompany the main episodes. These shorts were released in groups of three in between the release of each episode, showing the activities of the Freelance Police in between each story.
A second season of episodic video games developed by Telltale Games was announced in July 2007. "Sam & Max Beyond Time and Space" followed the same overall format as "Save the World", with each episode having a contained storyline with an overarching storyline involving laundering of the souls of the dead. As with "Save the World", episodes were originally published on GameTap before being made available for general release. The season consisted of five episodes and ran from November 2007 to April 2008. Nowlin and Kasten both returned to reprise their voice roles. In addition to the main games, a twenty-minute machinima video was produced, taking the form of a "Sam & Max" Christmas special. 
A third game entitled "Sam & Max: The Devil's Playhouse" was confirmed in May 2008 for release in 2009; the title was later pushed back to 2010, with concept art emerging after Telltale's completion of "Tales of Monkey Island". The season again ran for five episodes, released monthly from April to August 2010. "The Devil's Playhouse" followed a structure similar to "'Tales of Monkey Island", with each episode forming a part of an on-going narrative, involving psychic powers and forces that would use them for world domination. A two-minute Flash cartoon also accompanied the game, dealing with the origin story of General Skun-ka'pe, one of the game's antagonists. Max also appears in Telltale's 2010 casual game "Poker Night at the Inventory" alongside Tycho Brahe from "Penny Arcade", the Heavy from "Team Fortress 2" and Strong Bad from "Homestar Runner". Sam and Max (now voiced by Dave Boat) also appear in the game's sequel alongside Claptrap from "Borderlands", Brock Samson from "The Venture Bros.", Ash Williams from "Evil Dead" and GLaDOS from "Portal".
Television series.
"Sam & Max" were adapted into a cartoon series for Fox in 1997. Produced by Canadian studio Nelvana, the series ran for 24 episodes. Each episode was approximately ten minutes, and were often aired in pairs. Broadcast on Fox Kids in the United States, YTV in Canada, and Channel 4 in the United Kingdom, the first episode was aired on October 4, 1997; the series concluded on April 25, 1998. As opposed to the more adult humor in the rest of the series, "The Adventures of Sam & Max: Freelance Police" was aimed more at children, even though some humor in it was often directed at adults. As such, the violence inherent in the franchise is toned down, including removing Sam and Max's guns, and the characters do not use the moderate profanity that they use in their other appearances. As in most "Sam & Max" stories, the series revolves around the Freelance Police accepting missions from their mysterious superior, the commissioner, and embarking on cases to a large variety of implausible locations. Sam is voiced by Harvey Atkin, while Max is voiced by Robert Tinkler. The series performed well and was considered a success, and in 1998 received the Gemini Award for "Best Animated Program or Series". Despite the series' success, a second series was never commissioned. In June 2007, it was reported that Shout! Factory were preparing a DVD release of the series. In October 2007, as part of their marketing for "Sam & Max Save the World", GameTap hosted the series on their website. The DVD release of the series was later published in March 2008.
Music.
The "Sam & Max" franchise features a variety of soundtracks that accompany its video game products. This music is mostly grounded in film noir jazz, incorporating various other styles at certain points, such as Dixieland, waltz and mariachi, usually to support the cartoon nature of the series. The first "Sam & Max" game, "Sam & Max Hit the Road", was one of the first games to feature a fully scored music soundtrack, written by LucasArts' composers Clint Bajakian, Michael Land and Peter McConnell. The music was incorporated into the game using Land and McConnell's iMUSE engine, which allowed for audio to be synchronized with the visuals. Although the full soundtrack was never released, audio renders of four of the game's MIDI tracks were included on the CD version of the game.
For "Sam & Max Save the World", "Beyond Time and Space", and "The Devil's Playhouse", Telltale Games contracted composer Jared Emerson-Johnson, a musician whose previous work included composition and sound editing for LucasArts, to write the scores. The soundtracks for the first two games were released in two disc sets after the release of the games themselves; the "Season One Soundtrack" was published in July 2007, whilst the "Season Two Soundtrack" was released in September 2008. Emerson-Johnson's scores use live performances as opposed to synthesized music often used elsewhere in the video games industry. Critics reacted positively to Emerson-Johnson's scores, IGN described Emerson-Johnson's work as a "breath of fresh air", while 1UP.com praised his work as "top-caliber" and Music4Games stated that the "whimsical nature of classical jazz approach is well suited to the "Sam & Max" universe, which approaches American popular culture with a level of irreverence". Purcell later commented that Emerson-Johnson had seamlessly blended a "huge palette of genres and styles", whilst in September 2008, Brendan Q. Ferguson, one of the lead designers on "Save the World" and "Beyond Time and Space", stated that he believed that it was Emerson-Johnson's scores that created the vital atmosphere in the games, noting that prior to the implementation of the soundtracks, playing the games was an "unrelenting horror".
Cultural impact and reception.
The "Sam & Max" franchise has been highly successful critically, and is considered an iconic and influential aspect of the video game industry in the 1990s and the adventure game genre. In 2007, Steve Purcell wrote that he was somewhat surprised at the success of his creation, noting that the series had gained a large fan gathering despite the small size of the franchise. As the series contains only a small amount of comics, video games and a short TV series, Purcell commented that there was "certainly not enough material to build that relentless traction of an endlessly renewed sitcom or a syndicated comic that has existed since the Korean Conflict". The comics were well received by critics, many praising the humor and style of the stories and characters. However, later commentators have noted that the comic book series did not gain much popularity or recognition until after the release of "Sam & Max Hit the Road" in 1993; the later episodic video games are seen to have revived interest in the comics again, resulting in the creation of the webcomic "The Big Sleep" and publication of an anniversary edition of "Surfin' The Highway".
Upon its release in 1993, "Sam & Max Hit the Road" was met with near universal acclaim. Critics praised the title for its humor, voice acting, graphics, music and gameplay. It has since come to be regarded as a classic graphic adventure game, one of the most critically successful projects by LucasArts to date. "Sam & Max Hit the Road" is regularly featured in lists of top games, and was nominated for the 1994 Annie Award for "Best Animated CD-ROM", although the award instead went to LucasArts' "". The abrupt cancellation of the sequel to "Sam & Max Hit the Road" in 2004 garnered substantial criticism of LucasArts. In addition to a petition of 32,000 signatures objecting to the termination of development on "Sam & Max: Freelance Police", both Steve Purcell and the media were critical of LucasArts' decision. Purcell stated that he failed to understand quite why the game was cancelled, as he believed the development of the game was proceeding without hindrance, while the media put forward the view that LucasArts was moving to consolidate its position with low business risk "Star Wars" video games instead of pursuing the adventure games that had brought them success in earlier years. The cancellation of "Freelance Police" is often cited as the culmination in a perceived decline in the overall adventure game genre, and LucasArts later dismissed many of the designers involved with developing their adventure games, effectively ending their adventure game era.
Although "Sam & Max Save the World" did not receive the critical acclaim that "Sam & Max Hit the Road" acquired, it still received a favorable response from critics across its release in 2006 and 2007. Critics praised the game's humor, graphics and gameplay, although concerns were voiced over the low difficulty of the puzzles and the effectiveness of the story. "Save the World" is considered by journalists in the video game industry to be the first successful application of episodic gaming, as Telltale Games had managed to release a steady stream content with only small time gaps. Previous attempts by Valve Software with the "Half-Life" series, Ritual Entertainment with "SiN Episodes" and Telltale Games themselves with "" were for a variety of reasons not considered successful implementations of the distribution model. "Beyond Time and Space" was considered similar to "Save the World" and reviewers equally praised and faulted the game on this, although overall "Beyond Time and Space" received a good reception from critics.
The success of the franchise has spawned a selection of merchandise, including posters and prints, items of clothing and sketchbooks of Purcell's work during various stages of the series' development. Collectable statues of the characters have also been created. However, despite references in Purcell's sketchbooks and demand from both fans and journalists alike, plush toys of the characters have not been produced.

</doc>
<doc id="27767" url="https://en.wikipedia.org/wiki?curid=27767" title="Standard-definition television">
Standard-definition television

Standard-definition television (SDTV) is a television system that uses a resolution that is not considered to be either high-definition television (720p, 1080i, 1080p, 1440p, 4K UHDTV, and 8K UHD) or enhanced-definition television (EDTV 480p). The two common SDTV signal types are 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems; and 480i based on the American National Television System Committee NTSC system. Standard definition television also provides a picture quality similar to VHS (Video Home System). SDTV and high definition television (HDTV) are the two categories of display formats for digital television (DTV) transmissions, which are becoming the standard.
In North America, digital SDTV is broadcast in the same aspect ratio as NTSC signals with widescreen content being center cut. However, in other parts of the world that used the PAL or SECAM color systems, standard-definition television is now usually shown with a aspect ratio, with the transition occurring between the mid-1990s and mid-2000s. Older programs with a 4:3 aspect ratio are shown in the US as 4:3 with non-ATSC countries preferring to reduce the horizontal resolution by anamorphically scaling a pillarboxed image.
Standards that support digital SDTV broadcast include DVB, ATSC, and ISDB. The last two were originally developed for HDTV, but are more often used for their ability to deliver multiple SD video and audio streams via multiplexing, than for using the entire bitstream for one HD channel.
For SMPTE 259M-C compliance, a SDTV broadcast image is scaled to 720 pixels wide (with only 704 center pixels containing the image with 16 pixels reserved for horizontal blanking, a number of broadcasters incorrectly fill the whole 720 frame) for every 480 NTSC (or 576 PAL) lines of the image with the amount of non-proportional line scaling dependent on either the display or pixel aspect ratio. The display ratio for broadcast widescreen is commonly 16:9 (with a pixel ratio of 40:33 for anamorphic), the display ratio for a traditional or letterboxed broadcast is 4:3 (with a pixel aspect ratio of 10:11).
An SDTV image outside the constraints of the SMPTE standards requires no non-proportional scaling with 640 pixels (as defined by the adopted IBM VGA standard) for every line of the image. The display and pixel aspect ratio is generally not required with the line height defining the aspect. For widescreen 16:9, 360 lines define a widescreen image and for traditional 4:3, 480 lines define an image.
SDTV refresh rates can be 24, 25, 30, 50 or 60 frames per second with a possible rate multiplier of 1000/1001 for NTSC timing accuracy. 50 and 60 rates are generally frame doubled versions of 25 and 30 rates for jitter issues when using non-interlaced lines.
Digital SDTV in 4:3 aspect ratio has the same appearance as regular analog TV (NTSC, PAL, SECAM) without the ghosting, snowy images and white noise. However, if the reception has interference or is poor, where the error correction can not compensate one will encounter various other artifacts such as image freezing, stuttering or dropouts from missing intra-frames or blockiness from missing macroblocks. The audio encoding is the last to suffer loss due to the lower bandwidth requirements.
Pixel aspect ratio.
Television signals are transmitted in digitally encoded form, and the lines are scaled to fit SMPTE SDI bandwidth requirements, as opposed to unrestricted uses such as when lines are rendered or overlaid to a modern computer monitor and modern SMPTE implementations of HDTV. The table below summarizes pixel aspect ratios for the scaling of various kinds of SDTV video lines. Note that the actual image (be it 4:3 or 16:9) is always contained in the center 704 horizontal pixels of the digital frame, regardless of how many horizontal pixels (704 or 720) are used. In case of digital video line having 720 horizontal pixels, only the center 704 pixels contain actual 4:3 or 16:9 image, and the 8 pixel wide stripes from either side are called nominal analogue blanking for horizontal blanking and should be discarded before displaying the image. Nominal analogue blanking should not be confused with overscan, as overscan areas are part of the actual 4:3 or 16:9 image.
The pixel aspect ratio is always the same for corresponding 720 and 704 pixel resolutions because the center part of a 720 pixels wide image is equal to the corresponding 704 pixels wide image.

</doc>
