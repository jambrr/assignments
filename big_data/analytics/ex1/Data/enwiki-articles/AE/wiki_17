<doc id="25530" url="https://en.wikipedia.org/wiki?curid=25530" title="Robert Rodriguez">
Robert Rodriguez

Robert Anthony Rodríguez (born June 20, 1968) is an American filmmaker, screenwriter, and musician. He shoots and produces many of his films in Mexico and his home state, Texas.
Rodriguez directed the 1992 action film "El Mariachi", which was a commercial success after grossing $2 million against a budget of $7,000. The film spawned two sequels known collectively as the "Mexico Trilogy": "Desperado" and "Once Upon a Time in Mexico". He directed "From Dusk till Dawn" in 1996 and developed its (2014–present). Rodriguez co-directed the 2005 neo-noir crime thriller anthology "Sin City" (adapted from the graphic novel of the same name) and the 2014 sequel, "". Rodriguez also directed the "Spy Kids" films, "The Faculty", as well as "The Adventures of Sharkboy and Lavagirl in 3-D", "Planet Terror", and "Machete".
He is a friend and frequent collaborator of filmmaker Quentin Tarantino, who founded the production company A Band Apart, which Rodriguez was a member of. In December 2013, Rodriguez launched his own cable television channel, El Rey.
Early life.
Rodríguez was born in San Antonio, Texas, the son of Mexican-American parents Rebecca (née Villegas), a nurse, and Cecilio G. Rodríguez, a salesman. He began his interest in film at age eleven, when his father bought one of the first VCRs, which came with a camera.
While attending St. Anthony High School Seminary in San Antonio, Rodríguez was commissioned to videotape the school's football games. According to his sister, he was fired soon afterward as he had shot footage in a cinematic style, getting shots of parents' reactions and the ball traveling through the air instead of shooting the whole play. In high school he met Carlos Gallardo; they both shot films on video throughout high school and college.
Rodriguez went to the College of Communication at the University of Texas at Austin, where he also developed a love of cartooning. Not having grades high enough to be accepted into the school's film program, he created a daily comic strip entitled "Los Hooligans." Many of the characters were based on his siblings – in particular, one of his sisters, Maricarmen. The comic ran for three years in the student newspaper "The Daily Texan", while Rodríguez continued to make short films.
Rodríguez shot action and horror short films on video, and edited on two VCRs. In the fall of 1990, his entry in a local film contest earned him a spot in the university's film program. There he made the award-winning 16 mm short "Bedhead" (1991). The film chronicles the amusing misadventures of a young girl whose older brother sports an incredibly tangled mess of hair which she detests. Even at this early stage, Rodríguez's trademark style began to emerge: quick cuts, intense zooms, and fast camera movements deployed with a sense of humor.
"Bedhead" (1991) was recognized for excellence in the Black Maria Film Festival. It was selected by Film/Video Curator Sally Berger, for the Black Maria 20th-anniversary retrospective at MoMA in 2006.
Career.
Early career.
The short film "Bedhead" attracted enough attention to encourage him to seriously attempt a career as a filmmaker. He went on to shoot the action flick "El Mariachi" (1992) in Spanish; he shot it for around $7,000 with money raised by his friend Carlos Gallardo and from payments for his own participation in medical testing studies. Rodriquez won the Audience Award for this film at the Sundance Film Festival in 1993. Intended for the Spanish-language low-budget home-video market, the film was "cleaned up" by Columbia Pictures with post-production work costing several hundred thousand dollars before it was distributed in the United States. Its promotion still advertised it as "the movie made for $7,000". Rodríguez described his experiences making the film in his book "Rebel Without a Crew" (1995).
Mainstream success.
"Desperado", was a sequel to "El Mariachi" that starred Antonio Banderas and introduced Salma Hayek to American audiences. Rodríguez went on to collaborate with Quentin Tarantino on the vampire thriller "From Dusk till Dawn" (also both co-producing its two sequels), and he is currently writing, directing, and producing the for his own cable network, El Rey. Rodriguez has also worked with Kevin Williamson, on the horror film "The Faculty".
In 2001, Rodríguez enjoyed his first Hollywood hit with "Spy Kids", which went on to become a movie franchise. A third "mariachi" film also appeared in late 2003, "Once Upon a Time in Mexico", which completed the Mexico Trilogy (also called the Mariachi Trilogy). He operates a production company called Troublemaker Studios, formerly Los Hooligans Productions.
Rodríguez co-directed "Sin City" (2005), an adaptation of the Frank Miller "Sin City" comic books; Quentin Tarantino guest-directed a scene. During production in 2004, Rodríguez insisted that Miller would be credited as co-director, because he considered the visual style of Miller's comic art to be just as important as his own in the film. However, the Directors Guild of America would not allow it, citing that only "legitimate teams", "e.g.", the Wachowskis, could share the director's credit. Rodríguez chose to resign from the DGA, stating, "It was easier for me to quietly resign before shooting because otherwise I'd be forced to make compromises I was unwilling to make or set a precedent that might hurt the guild later on." By resigning from the DGA, Rodríguez was forced to relinquish his director's seat on the film "John Carter of Mars" for Paramount Pictures. Rodríguez had already signed on and had been announced as director of that film, planning to begin filming soon after completing "Sin City".
"Sin City" was a critical hit in 2005 as well as a box office success, particularly for a hyperviolent comic book adaptation that did not have name recognition comparable to the "X-Men" or "Spider-Man". He has stated that he is interested in eventually adapting all of Miller's "Sin City" comic books.
Rodríguez released "The Adventures of Sharkboy and Lavagirl in 3-D" in 2005, a superhero-kid movie intended for the same younger audiences as his "Spy Kids" series. "Sharkboy and Lavagirl" was based on a story conceived by Rodríguez's 7-year-old son, Racer, who was given credit for the screenplay. The film was not a major success, grossing just $39 million at the box office.
Rodríguez wrote and directed the film "Planet Terror" as part of the double-bill release "Grindhouse" (2007). Quentin Tarantino directed "Grindhouse"'s other film.
He also has a series of "Ten Minute Film School" segments on several of his DVD releases, showing aspiring filmmakers how to make good, profitable movies using inexpensive tactics. Starting with the "Once Upon a Time in Mexico" DVD, Rodríguez began creating a series called, "Ten Minute Cooking School" where he revealed his recipe for "Puerco Pibil" (based on Cochinita pibil, an old dish from Yucatán), the same food Johnny Depp's character, "Agent Sands" ate in the film. The popularity of this series led to the inclusion of another "Cooking School" on the two-disc version of the "Sin City" DVD where Rodríguez teaches the viewer how to make "Sin City Breakfast Tacos", a dish (made for his cast and crew during late-night shoots and editing sessions) utilizing his grandmother's tortilla recipe and different egg mixes for the filling. He had initially planned to release a third "Cooking School" with the DVD release of "Planet Terror" but then announced on the "Film School" segment of the DVD that he would put it on the "Grindhouse" DVD set instead. The Cooking School, titled "Texas Barbecue...from the GRAVE!", is a dish based on the "secret barbecue recipe" of JT Hague, Jeff Fahey's character in the film.
Rodríguez is a strong supporter of digital filmmaking, having been introduced to the practice by director George Lucas, who personally invited Rodríguez to use the digital cameras at Lucas' headquarters. He was presented with the Extraordinary Contribution to Filmmaking Award at the 2010 Austin Film Festival.
"Predators".
On April 23, 2009, it was announced that Rodríguez would produce a new Predator sequel, entitled "Predators". This film's script was based on early drafts he had written after seeing the original. Rodriguez's ideas included a planet-sized game preserve and various creatures used by the Predators to hunt a group of abducted yet skilled humans. Opening to mostly positive reviews, the film fared reasonably well at the box office.
"Machete".
"Machete" is a feature film directed by Rodríguez and released in September 2010. It is an expansion of a fake trailer Rodriguez directed for the 2007 film "Grindhouse". It starred Danny Trejo as the title character. Trejo, Rodriguez' 2nd cousin, has worked with him in some of his other movies such as "Desperado", "From Dusk till Dawn", "Once Upon a Time in Mexico" and "Spy Kids", where Trejo's character was also known as Machete. Although originally announced to be released direct-to-DVD as an extra on the "Planet Terror" DVD, the film was produced as a theatrical release.
According to Rodríguez, the origins of the film go back to "Desperado". He says, "When I met Danny, I said, 'This guy should be like the Mexican Jean-Claude Van Damme or Charles Bronson, putting out a movie every year and his name should be Machete.' So I decided to do that way back when, never got around to it until finally now. So now, of course, I want to keep going and do a feature." In an interview with "Rolling Stone" magazine, Rodriguez said that he wrote the screenplay back in 1993 when he cast Trejo in "Desperado". "So I wrote him this idea of a federale from Mexico who gets hired to do hatchet jobs in the U.S. I had heard sometimes FBI or DEA have a really tough job that they don't want to get their own agents killed on, they'll hire an agent from Mexico to come do the job for $25,000. I thought, "That's "Machete". He would come and do a really dangerous job for a lot of money to him but for everyone else over here it's peanuts." But I never got around to making it."
Rodríguez hoped to film "Machete" at the same time as "Sin City 2". Additionally, during Comic-Con International 2008, he took the time to speak about Machete, including such topics as: status, possible sequels after the release of Machete, and production priorities. It was also revealed that he has regularly pulled sequences from it for his other productions including Once Upon a Time in Mexico. "Machete" was released in theaters September 3, 2010 in the U.S.A.
On May 5, 2010, Robert Rodríguez responded to Arizona's controversial immigration law by releasing an "illegal" trailer on Ain't It Cool News. The fake trailer combined elements of the "Machete" trailer that appeared in "Grindhouse" with footage from the actual film, and implied that the film would be about Machete leading a revolt against anti-immigration politicians and border vigilantes. Several movie websites, including Internet Movie Database, reported that it was the official teaser for the film. However, Rodriguez later revealed the trailer to be a joke, explaining "it was Cinco de Mayo and I had too much tequila."
Unproduced projects and upcoming films.
Since 1998, he has owned the film rights to Mike Allred's off-beat comic "Madman". The two have hinted at the project being close to beginning on several occasions without anything coming of it. However, other projects have been completed first (Allred was instrumental in connecting Rodríguez with Frank Miller, leading to the production of "Sin City"). In 2004, Allred, while promoting his comic book, "The Golden Plates", announced that a screenplay by George Huang was near completion. In March 2006, it was announced that production on "Sin City 2" would be postponed. Allred announced at the 2006 WonderCon that production would likely commence on "Madman the Movie" in 2006. Huang is actually friends with Rodriguez, who advised him to pursue filmmaking as a career when Rodriguez landed a deal with Columbia Pictures where Huang was an employee.
In May 2007 it was announced that Rodríguez had signed on to direct a remake of "Barbarella" for a 2008 release. At the 2007 Comic-Con convention, actress Rosario Dawson announced that because of "Barbarella", production of "Sin City 2" would be put on hold. She also announced that she would be playing an amazon in the Barbarella film. As of June 2008, plans to remake the film Barbarella with Rose McGowan as the lead have been delayed; the actress and director are instead remaking the film "Red Sonja".
In May 2008 Rodríguez is said to be shopping around a prison drama television series called "Woman in Chains!", with Rose McGowan being a possibility for a lead role.
As of May 2009, Rodríguez plans to produce a live-action remake of "Fire and Ice", a 1983 film collaboration between painter Frank Frazetta and animator Ralph Bakshi. The deal was closed shortly after Frazetta's death.
In 2011, Rodríguez announced at Comic-Con that he had purchased the film rights to "Heavy Metal" and planned to develop a new animated film at the new Quick Draw Studios.
In late October 2015, Rodriguez appeared on the RoosterTeeth Podcast, in which he discussed with the cast about all the work that goes into film's pre and post production. They also discussed some of the projects Rodriguez had worked on in "Stage 5" in Austin, Texas.
In October 2015, it was reported that Rodriguez will direct Battle Angel Alita with James Cameron and Jon Landau producing.
In November it was announced Rodriguez was directing the film "100 Years", which would be released in 2115.
Personal life.
Rodríguez announced in April 2006 that he and his wife Elizabeth Avellán, with whom he had five children, had separated after 16 years of marriage. Avellán has continued to produce most of his films since the split-up, so their professional relationship has continued.
He reportedly had a "dalliance" with actress Rose McGowan during the shooting of "Grindhouse". In October 2007, "Elle Magazine" revealed that Rodríguez cast McGowan as the title role in his remake of "Barbarella". After some reports of their breaking up and being together again, they split up in October 2009.
In October 2010, he walked Alexa Vega down the aisle at her wedding to producer Sean Covel.
In March 2014, Rodriguez showed his collection of Frank Frazetta original paintings in Austin, Texas, during the SXSW festival.
The "one-man film crew" and "Mariachi-style".
Rodríguez not only has the credits of producing, directing and writing his films, he also frequently serves as editor, director of photography, camera operator, steadicam operator, composer, production designer, visual effects supervisor, and sound editor on his films. This has earned him the nickname of "the one-man film crew". He abbreviates his numerous roles in his film credits; "Once Upon a Time in Mexico", for instance, is "shot, chopped, and scored by Robert Rodriguez", and "Sin City" is "shot and cut by Robert Rodriguez".
He calls his style of making movies "Mariachi-style" (in reference to his first feature film "El Mariachi") in which (according to the back cover of his book "Rebel Without a Crew")
"Creativity, not money, is used to solve problems."
In his book "The DV Rebel's Guide", Stu Maschwitz coined the term "Robert Rodriguez list", i.e. the filmmaker compiling a list of things they have access to like cool cars, apartments, horses, samurai swords and so on, and then writing the screenplay based on that list.
Rodriguez wrote a blurb for the book that stated:
"I'd been wanting to write a book for the new breed of digital filmmakers, but now I don't have to. My pal and fellow movie maker Stu Maschwitz has compressed years of experience into this thorough guide. Don't make a movie without reading this book!" 

</doc>
<doc id="25531" url="https://en.wikipedia.org/wiki?curid=25531" title="Romantic comedy film">
Romantic comedy film

Romantic comedy films are films with light-hearted, humorous plotlines, centered on romantic ideals such as that trulovee is able to surmount most obstacles. One dictionary definition is "a funny movie, play, or television program about a love story that ends happily". Another definition states that its "primary distinguishing feature is a love plot in which two sympathetic and well-matched lovers are united or reconciled".
Romantic comedy films are a certain genre of comedy films as well as of romance films, and may also have elements of screwball comedies and stoner comedies. Some television series can also be classified as romantic comedies.
In a typical romantic comedy the two lovers tend to be young, likeable, and apparently meant for each other, yet they are kept apart by some complicating circumstance (e.g., class differences, parental interference; a previous girlfriend or boyfriend) until, surmounting all obstacles, they are finally wed. A wedding-bells, fairy-tale-style happy ending is practically mandatory.
Description.
The basic plot of a romantic comedy is that two characters, usually a man and a woman, meet, part ways due to an argument or other obstacle, then ultimately reunite. Sometimes the two leads meet and become involved initially, then must confront challenges to their union. Sometimes they are hesitant to become romantically involved because they believe that they do not like each other, because one of them already has a partner, or because of social pressures. However, the screenwriters leave clues that suggest that the characters are, in fact, attracted to each other and that they would be a good love match. The protagonists often separate or seek time apart to sort out their feelings or deal with the external obstacles to their being together.
While the two protagonists are separated, one or both of them usually realizes that they are ideal for each other, or that they are in love with each other. Then, after one of the two makes some spectacular effort (sometimes called the "grand gesture") to find the other person and declare their love, or through an astonishing coincidental encounter, the two meet again. Then, perhaps with some comic friction or awkwardness, they declare their love for each other and the film ends happily. The couple does not, however, have to marry, or live together "happily ever after". The ending of a romantic comedy is meant to affirm the primary importance of the love relationship in its protagonists' lives, even if they physically separate in the end (e.g. "Shakespeare in Love", "Roman Holiday").
There are many variations on this basic plotline. Sometimes, instead of the two lead characters ending up in each other's arms, another love match will be made between one of the principal characters and a secondary character (e.g., "My Best Friend's Wedding" and "My Super Ex-Girlfriend"). Alternatively, the film may be a rumination on the impossibility of love, as in Woody Allen's film "Annie Hall." The basic format of a romantic comedy film can be found in much earlier sources, such as Shakespeare plays like "Much Ado About Nothing" and "A Midsummer Night's Dream".
Some comedy films, such as "Knocked Up", combine themes of romantic comedies and stoner comedies, creating a subgenre that appeals to both men and women. Often known as "bromance", such films usually use sexual elements which bring the two characters together. Films in this genre include "American Pie 2" and even "Wedding Crashers".
Evolution and subgenres.
Romantic comedies have begun to spread out of their conventional and traditional structure into other territory. This territory explores more subgenres and more complex topics. These films still follow the typical plot of "a light and humorous movie, play, etc., whose central plot is a happy love story" but with more complexity. These are a few ways romantic comedies are adding more subtlety and complexity into the genre.
Extreme circumstances.
Some romantic comedies have adopted extreme or strange circumstances for the main characters, as in "Warm Bodies" where the protagonist is a zombie who falls in love with a human girl after eating her boyfriend. Another strange set of circumstances is in "Zack and Miri Make a Porno" where the two protagonists are building a relationship while trying to make a porno together. Both these films take the typical story-arch and then utilize circumstances to add originality.
Flipping conventions.
Other romantic comedies flip the standard conventions of the romantic comedy genre. In films like "500 Days of Summer" the two main interests do not end up together, leaving the protagonist somewhat distraught. While other films like "Adam" have the two main interests end up separated but still content and pursuing other goals and love interests.
Serious elements.
Other remakes of romantic comedies involve similar elements but explore more adult themes such as marriage, responsibility or even disability. Two films by Judd Apatow such as "This is 40" or "Knocked Up" deal with these before mentioned issues. "This is 40" chronicles the mid life crisis of a couple entering their 40's and "Knocked Up" addresses unintended pregnancy and the ensuing assuming of responsibility. While "Silver Linings Playbook" deals with mental illness and unrequited love that is never resolved.
All of these go against the stereotype of what romantic comedy has become as a genre. Yet the genre of romantic comedy is simply a structure and all of these elements do not negate the fact that these films are still romantic comedies.
Contrived romantic encounters: the "meet cute".
One of the conventions of romantic comedy films is the funny parts and contrived encounter of two potential romantic partners in unusual or comic circumstances, which film critics such as Roger Ebert or the Associated Press' Christy Lemire have called a "meet-cute" situation. During a "meet-cute", scriptwriters often create a humorous sense of awkwardness between the two potential partners by depicting an initial clash of personalities or beliefs, an embarrassing situation, or by introducing a comical misunderstanding or mistaken identity situation. Sometimes the term is used without a hyphen (a "meet cute"), or as a verb ("to meet cute").
Roger Ebert describes the "concept of a Meet Cute" as "when boy meets girl in a cute way." As an example, he cites "The Meet Cute in "Lost and Found" has Jackson and Segal running their cars into each other in Switzerland. Once recovered, they Meet Cute again when they run into each other while on skis. Eventually... they fall in love."
In many romantic comedies, the potential couple comprises polar opposites, two people of different temperaments, situations, social statuses, or all three ("It Happened One Night"), who would not meet or talk under normal circumstances, and the meet cute's contrived situation provides the opportunity for these two people to meet.
Use of "meet cute" situations.
Certain movies are entirely driven by the meet-cute situation, and contrived circumstances throw the couple together for much of the screenplay. However, movies in which the contrived situation is the main feature, such as "Some Like It Hot", rather than the romance being the main feature, are not considered "meet-cutes".
The use of the meet-cute is less marked in television series and novels, because these formats have more time to establish and develop romantic relationships. In situation comedies, relationships are static and meet-cute is not necessary, though flashbacks may recall one ("The Dick Van Dyke Show", "Mad About You") and lighter fare may require contrived romantic meetings.
The heyday of "meet cute" in films was during the Great Depression in the 1930s; screwball comedy films made a heavy use of contrived romantic "meet cutes", perhaps because the more rigid class consciousness and class divisions of this period made cross-social class romances into tantalizing fantasies.
While film critic Roger Ebert has popularized the term "meet cute" in his reviews of romantic comedies, the term is mostly used in the film and screenwriting industries, where it provides a convenient shorthand for screenwriters who are doing a very compressed pitch to a film production company.
History.
The "Oxford Dictionary of Literary Terms" defines romantic comedy as "a general term for comedies that deal mainly with the follies and misunderstandings of young lovers, in a light‐hearted and happily concluded manner which usually avoids serious satire". This reference states that the "best‐known examples are Shakespeare's comedies of the late 1590s, "A Midsummer Night's Dream", "Twelfth Night", and "As You Like It" being the most purely romantic, while "Much Ado About Nothing" approaches the comedy of manners and "The Merchant of Venice" is closer to tragicomedy."
Comedies since ancient Greece have often incorporated sexual or social elements.
It was not until the creation of romantic love in the western European medieval period, though, that "romance" came to refer to "romantic love" situations, rather than the heroic adventures of medieval Romance. These adventures, however, often revolved about a knight's feats on behalf of a lady, and so the modern themes of love were quickly woven into them, as in Chrétien de Troyes's "Lancelot, the Knight of the Cart".
Shakespearean comedy and Restoration comedy remain influential. The creation of huge economic social strata in the Gilded Age, combined with the heightened openness about sex after the Victorian era and the celebration of Sigmund Freud's theories, and the birth of the film industry in the early twentieth century, gave birth to the screwball comedy. As class consciousness declined and World War II unified various social orders, the savage screwball comedies of the twenties and thirties, proceeding through Rock Hudson–Doris Day-style comedies, gave way to more innocuous comedies. This style faded in the 1960s, and the genre lay mostly dormant until the more sexually charged "When Harry Met Sally" had a successful box office run in 1989, paving the way for a rebirth for the Hollywood romantic comedy in the mid-1990s.
The French film industry went in a completely different direction, with less inhibitions about sex. Virginia Woolf, tired of stories that ended in 'happily ever after' at the beginning of a serious relationship, called "Middlemarch" by George Eliot, with its portrayal of a difficult marriage, "one of the few English novels written for grown-up people."
Effects.
On Society Today.
With the increase of Romantic Comedy movies, there has been an apparent change in the way society views romance. Researchers are asking whether the romance projected in romantic comedies are preventing true love in real life. The increase in use of technology has also lead the society to spend a great amount of time engaging in mediated reality and less time with each other. Even though researchers have only started to explore the impact of romantic comedy films on human romance, the few studies conducted have already shown correlation between romantic comedies and the love delusion.
The Illusion of Love.
In the past, love has not always been the real reason of people coming together. In some cultures, arranged marriages were common to keep the caste systems or to join kingdoms. Today, love is the root of all romance, and it is over-emphasized through these films. It tells viewers that love conquers all and will ultimately bring never-ending happiness, which is rarely affected by any conflict. When people do not experience the romance portrayed in these movies, the often wonder what they are doing wrong. Although people should be able to tell between an overly romanticized love and realistic love, they are often caught up in constantly trying to echo the stories they see on screen. While most of us know that the idea of a perfect relationship is unrealistic, a few of us are still more influenced by media portrayals than we realize.
Conducted Research.
A study was conducted at Heriot Watt University in Edinburgh to understand this phenomenon. They studied 40 top box office films released between 1995 and 2005 to establish common themes. Then they asked hundreds of people to complete a questionnaire to describe their beliefs and expectations in romantic relationships. Researchers found that people who enjoyed movies such as "You’ve Got Mail, The Wedding Planner" and "While You Were Sleeping," often fail to communicate with their partners effectively. They also believe that if someone is meant to be with you, then they should know your needs without you telling them. Although this study is just one of a handful, it shows a correlation of how peoples expectations are distorted through watching romantic comedies.

</doc>
<doc id="25532" url="https://en.wikipedia.org/wiki?curid=25532" title="Renaissance">
Renaissance

The Renaissance (, ) is a period in Europe, from the 14th to the 17th century, considered the bridge between the Middle Ages and modern history. It started as a cultural movement in Italy in the Late Medieval period and later spread to the rest of Europe, marking the beginning of the Early Modern Age.
The Renaissance's intellectual basis was its own invented version of humanism, derived from the rediscovery of classical Greek philosophy, such as that of Protagoras, who said, that "Man is the measure of all things." This new thinking became manifest in art, architecture, politics, science and literature. Early examples were the development of "perspective" in oil painting and the recycled knowledge of how to make concrete. Although the invention of metal movable type sped the dissemination of ideas from the later 15th century, the changes of the Renaissance were not uniformly experienced across Europe.
As a cultural movement, it encompassed innovative flowering of Latin and vernacular literatures, beginning with the 14th century resurgence of learning based on classical sources, which contemporaries credited to Petrarch; the development of linear perspective and other techniques of rendering a more natural reality in painting; and gradual but widespread educational reform. In politics, the Renaissance contributed to the development of the customs and conventions of diplomacy, and in science to an increased reliance on observation and inductive reasoning. Although the Renaissance saw revolutions in many intellectual pursuits, as well as social and political upheaval, it is perhaps best known for its artistic developments and the contributions of such polymaths as Leonardo da Vinci and Michelangelo, who inspired the term "Renaissance man".
There is a consensus that the Renaissance began in Florence, in the 14th century. Various theories have been proposed to account for its origins and characteristics, focusing on a variety of factors including the social and civic peculiarities of Florence at the time; its political structure; the patronage of its dominant family, the Medici; and the migration of Greek scholars and texts to Italy following the Fall of Constantinople at the hands of the Ottoman Turks. Other major centres were northern Italian city-states such as Venice, Genoa, Milan; Bologna; and finally Rome during the Renaissance Papacy.
The Renaissance has a long and complex historiography, and, in line with general scepticism of discrete periodizations, there has been much debate among historians reacting to the 19th-century glorification of the "Renaissance" and individual culture heroes as "Renaissance men", questioning the usefulness of "Renaissance" as a term and as a historical delineation. The art historian Erwin Panofsky observed of this resistance to the concept of "Renaissance":
It is perhaps no accident that the factuality of the Italian Renaissance has been most vigorously questioned by those who are not obliged to take a professional interest in the aesthetic aspects of civilization—historians of economic and social developments, political and religious situations, and, most particularly, natural science—but only exceptionally by students of literature and hardly ever by historians of Art.
Some observers have called into question whether the Renaissance was a cultural "advance" from the Middle Ages, instead seeing it as a period of pessimism and nostalgia for classical antiquity, while social and economic historians, especially of the "longue durée", have instead focused on the continuity between the two eras which are linked, as Panofsky observed, "by a thousand ties". 
The word "Renaissance", literally meaning "Rebirth" in French, first appears in English in the 1830s. The word occurs in Jules Michelet's 1855 work, "Histoire de France". The word "Renaissance" has also been extended to other historical and cultural movements, such as the Carolingian Renaissance and the Renaissance of the 12th century.
Overview.
Renaissance humanists such as Poggio Bracciolini sought out in Europe's monastic libraries the Latin literary, historical, and oratorical texts of Antiquity, while the Fall of Constantinople (1453) generated a wave of émigré Greek scholars bringing precious manuscripts in ancient Greek, many of which had fallen into obscurity in the West. It is in their new focus on literary and historical texts that Renaissance scholars differed so markedly from the medieval scholars of the Renaissance of the 12th century, who had focused on studying Greek and Arabic works of natural sciences, philosophy and mathematics, rather than on such cultural texts.
In the revival of neo-Platonism Renaissance humanists did not reject Christianity; quite the contrary, many of the Renaissance's greatest works were devoted to it, and the Church patronized many works of Renaissance art. However, a subtle shift took place in the way that intellectuals approached religion that was reflected in many other areas of cultural life. In addition, many Greek Christian works, including the Greek New Testament, were brought back from Byzantium to Western Europe and engaged Western scholars for the first time since late antiquity. This new engagement with Greek Christian works, and particularly the return to the original Greek of the New Testament promoted by humanists Lorenzo Valla and Erasmus, would help pave the way for the Protestant Reformation.
Well after the first artistic return to classicism had been exemplified in the sculpture of Nicola Pisano, Florentine painters led by Masaccio strove to portray the human form realistically, developing techniques to render perspective and light more naturally. Political philosophers, most famously Niccolò Machiavelli, sought to describe political life as it really was, that is to understand it rationally. A critical contribution to Italian Renaissance humanism Giovanni Pico della Mirandola wrote the famous text ""De hominis dignitate"" (Oration on the Dignity of Man, 1486), which consists of a series of theses on philosophy, natural thought, faith and magic defended against any opponent on the grounds of reason. In addition to studying classical Latin and Greek, Renaissance authors also began increasingly to use vernacular languages; combined with the introduction of printing, this would allow many more people access to books, especially the Bible.
In all, the Renaissance could be viewed as an attempt by intellectuals to study and improve the secular and worldly, both through the revival of ideas from antiquity, and through novel approaches to thought. Some scholars, such as Rodney Stark, play down the Renaissance in favor of the earlier innovations of the Italian city-states in the High Middle Ages, which married responsive government, Christianity and the birth of capitalism. This analysis argues that, whereas the great European states (France and Spain) were absolutist monarchies, and others were under direct Church control, the independent city republics of Italy took over the principles of capitalism invented on monastic estates and set off a vast unprecedented commercial revolution which preceded and financed the Renaissance.
Origins.
Many argue that the ideas that characterized the Renaissance had their origin in late 13th century Florence, in particular with the writings of Dante Alighieri (1265–1321) and Francesco Petrarca (1304–1374), as well as the paintings of Giotto di Bondone (1267–1337). Some writers date the Renaissance quite precisely; one proposed starting point is 1401, when the rival geniuses Lorenzo Ghiberti and Filippo Brunelleschi competed for the contract to build the bronze doors for the Baptistery of the Florence Cathedral (Ghiberti won). Others see more general competition between artists and polymaths such as Brunelleschi, Ghiberti, Donatello, and Masaccio for artistic commissions as sparking the creativity of the Renaissance. Yet it remains much debated why the Renaissance began in Italy, and why it began when it did. Accordingly, several theories have been put forward to explain its origins.
During the Renaissance, money and art went hand in hand. Artists depended totally on patrons while the patrons needed money to foster artistic talent. Wealth was brought to Italy in the 14th, 15th, and 16th centuries by expanding trade into Asia and Europe. Silver mining in Tyrol increased the flow of money. Luxuries from the Eastern world, brought home during the Crusades, increased the prosperity of Genoa and Venice.
Jules Michelet defined the 16th-century Renaissance in France as a period in Europe's cultural history that represented a break from the Middle Ages, creating a modern understanding of humanity and its place in the world.
Latin and Greek phases of Renaissance humanism.
In stark contrast to the High Middle Ages, when Latin scholars focused almost entirely on studying Greek and Arabic works of natural science, philosophy and mathematics, Renaissance scholars were most interested in recovering and studying Latin and Greek literary, historical, and oratorical texts. Broadly speaking, this began in the 14th century with a Latin phase, when Renaissance scholars such as Petrarch, Coluccio Salutati (1331–1406), Niccolò de' Niccoli (1364–1437) and Poggio Bracciolini (1380–1459 AD) scoured the libraries of Europe in search of works by such Latin authors as Cicero, Lucretius, Livy and Seneca. By the early 15th century, the bulk of such Latin literature had been recovered; the Greek phase of Renaissance humanism was now under way, as Western European scholars turned to recovering ancient Greek literary, historical, oratorical and theological texts.
Unlike the case of those Latin texts, which had been preserved and studied in Western Europe since late antiquity, the study of ancient Greek texts was very limited in medieval Western Europe. Ancient Greek works on science, maths and philosophy had been studied since the High Middle Ages in Western Europe and in the medieval Islamic world (normally in translation), but Greek literary, oratorical and historical works (such as Homer, the Greek dramatists, Demosthenes and Thucydides and so forth), were not studied in either the Latin or medieval Islamic worlds; in the Middle Ages these sorts of texts were only studied by Byzantine scholars. One of the greatest achievements of Renaissance scholars was to bring this entire class of Greek cultural works back into Western Europe for the first time since late antiquity. This movement to reintegrate the regular study of Greek literary, historical, oratorical and theological texts back into the Western European curriculum is usually dated to Coluccio Salutati's 1396 invitation to the Byzantine diplomat and scholar Manuel Chrysoloras(c.1355–1415) to Florence to teach Greek. This legacy was continued by a number of expatriate Greek scholars, from Basilios Bessarion to Leo Allatius.
Social and political structures in Italy.
The unique political structures of late Middle Ages Italy have led some to theorize that its unusual social climate allowed the emergence of a rare cultural efflorescence. Italy did not exist as a political entity in the early modern period. Instead, it was divided into smaller city states and territories: the Kingdom of Naples controlled the south, the Republic of Florence and the Papal States at the center, the Milanese and the Genoese to the north and west respectively, and the Venetians to the east. Fifteenth-century Italy was one of the most urbanised areas in Europe. Many of its cities stood among the ruins of ancient Roman buildings; it seems likely that the classical nature of the Renaissance was linked to its origin in the Roman Empire's heartland.
Historian and political philosopher Quentin Skinner points out that Otto of Freising (c. 1114–1158), a German bishop visiting north Italy during the 12th century, noticed a widespread new form of political and social organization, observing that Italy appeared to have exited from Feudalism so that its society was based on merchants and commerce. Linked to this was anti-monarchical thinking, represented in the famous early Renaissance fresco cycle Allegory of Good and Bad Government in Siena by Ambrogio Lorenzetti (painted 1338–1340) whose strong message is about the virtues of fairness, justice, republicanism and good administration. Holding both Church and Empire at bay, these city republics were devoted to notions of liberty. Skinner reports that there were many defences of liberty such as Matteo Palmieri's (1406–1475) celebration of Florentine genius not only in art, sculpture and architecture, but "the remarkable efflorescence of moral, social and political philosophy that occurred in Florence at the same time".
Even cities and states beyond central Italy, such as the Republic of Florence at this time, were also notable for their merchant Republics, especially the Republic of Venice. Although in practice these were oligarchical, and bore little resemblance to a modern democracy, they did have democratic features and were responsive states, with forms of participation in governance and belief in liberty. The relative political freedom they afforded was conducive to academic and artistic advancement. Likewise, the position of Italian cities such as Venice as great trading centres made them intellectual crossroads. Merchants brought with them ideas from far corners of the globe, particularly the Levant. Venice was Europe's gateway to trade with the East, and a producer of fine glass, while Florence was a capital of textiles. The wealth such business brought to Italy meant large public and private artistic projects could be commissioned and individuals had more leisure time for study.
Black Death/Plague.
One theory that has been advanced is that the devastation caused by the Black Death in Florence, which hit Europe between 1348 and 1350, resulted in a shift in the world view of people in 14th-century Italy. Italy was particularly badly hit by the plague, and it has been speculated that the resulting familiarity with death caused thinkers to dwell more on their lives on Earth, rather than on spirituality and the afterlife. It has also been argued that the Black Death prompted a new wave of piety, manifested in the sponsorship of religious works of art. However, this does not fully explain why the Renaissance occurred specifically in Italy in the 14th century. The Black Death was a pandemic that affected all of Europe in the ways described, not only Italy. The Renaissance's emergence in Italy was most likely the result of the complex interaction of the above factors.
The plague was carried by fleas on sailing vessels returning from the ports of Asia, spreading quickly due to lack of proper sanitation: the population of England, then about 4.2 million, lost 1.4 million people to the bubonic plague. Florence's population was nearly halved in the year 1347. As a result of the decimation in the populace the value of the working class increased, and commoners came to enjoy more freedom. To answer the increased need for labor, workers traveled in search of the most favorable position economically.
The demographic decline due to the plague had some economic consequences: the prices of food dropped and land values declined by 30 to 40% in most parts of Europe between 1350 and 1400. Landholders faced a great loss but for ordinary men and women, it was a windfall. The survivors of the plague found not only that the prices of food were cheaper but also found that lands were more abundant, and that most of them inherited property from their dead relatives.
The spread of disease was significantly more rampant in areas of poverty. Epidemics ravaged cities, particularly children. Plagues were easily spread by lice, unsanitary drinking water, armies, or by poor sanitation. Children were hit the hardest because many diseases such as typhus and syphilis target the immune system and left young children without a fighting chance. Children in city dwellings were more affected by the spread of disease than the children of the wealthy.
The Black Death caused greater upheaval to Florence's social and political structure than later epidemics. Despite a significant number of deaths among members of the ruling classes, the government of Florence continued to function during this period. Formal meetings of elected representatives were suspended during the height of the epidemic due to the chaotic conditions in the city, but a small group of officials was appointed to conduct the affairs of the city, which ensured continuity of government.
Cultural conditions in Florence.
It has long been a matter of debate why the Renaissance began in Florence, and not elsewhere in Italy. Scholars have noted several features unique to Florentine cultural life which may have caused such a cultural movement. Many have emphasized the role played by the Medici, a banking family and later ducal ruling house, in patronizing and stimulating the arts. Lorenzo de' Medici (1449–1492) was the catalyst for an enormous amount of arts patronage, encouraging his countrymen to commission works from Florence's leading artists, including Leonardo da Vinci, Sandro Botticelli, and Michelangelo Buonarroti. Works by Neri di Bicci, Sandro Botticelli, Leonardo da Vinci and Filippino Lippi had been commissioned additionally by the convent di San Donato agli Scopeti of the Augustinians order in Florence.
The Renaissance was certainly underway before Lorenzo came to power; indeed, before the Medici family itself achieved hegemony in Florentine society. Some historians have postulated that Florence was the birthplace of the Renaissance as a result of luck, i.e. because "Great Men" were born there by chance. Leonardo da Vinci, Botticelli and Michelangelo were all born in Tuscany. Arguing that such chance seems improbable, other historians have contended that these "Great Men" were only able to rise to prominence because of the prevailing cultural conditions at the time.
Characteristics.
Humanism.
In some ways humanism was not a philosophy but a method of learning. In contrast to the medieval scholastic mode, which focused on resolving contradictions between authors, humanists would study ancient texts in the original, and appraise them through a combination of reasoning and empirical evidence. Humanist education was based on the programme of 'Studia Humanitatis', that being the study of five humanities: poetry, grammar, history, moral philosophy and rhetoric. Although historians have sometimes struggled to define humanism precisely, most have settled on "a middle of the road definition... the movement to recover, interpret, and assimilate the language, literature, learning and values of ancient Greece and Rome". Above all, humanists asserted "the genius of man ... the unique and extraordinary ability of the human mind".
Humanist scholars shaped the intellectual landscape throughout the early modern period. Political philosophers such as Niccolò Machiavelli and Thomas More revived the ideas of Greek and Roman thinkers, and applied them in critiques of contemporary government. Pico della Mirandola wrote what is often considered the "manifesto" of the Renaissance, a vibrant defence of thinking, the Oration on the Dignity of Man. Matteo Palmieri (1406–1475), another humanist, is most known for his work "Della vita civile" ("On Civic Life"; printed 1528) which advocated civic humanism, and his influence in refining the Tuscan vernacular to the same level as Latin. Palmieri's written works drawn on Roman philosophers and theorists, especially Cicero, who, like Palmieri, lived an active public life as a citizen and official, as well as a theorist and philosopher and also Quintilian. Perhaps the most succinct expression of his perspective on humanism is in a 1465 poetic work "La città di vita", but an earlier work "Della vita civile" (On Civic Life) is more wide-ranging. Composed as a series of dialogues set in a country house in the Mugello countryside outside Florence during the plague of 1430, Palmieri expounds on the qualities of the ideal citizen. The dialogues include ideas about how children develop mentally and physically, how citizens can conduct themselves morally, how citizens and states can ensure probity in public life, and an important debate on the difference between that which is pragmatically useful and that which is honest.
The humanists believed that it is important to transcend to the afterlife with a perfect mind and body. This transcending belief can be done with education. The purpose of humanism was to create a universal man whose person combined intellectual and physical excellence and who was capable of functioning honorably in virtually any situation. This ideology was referred to as the "uomo universale", an ancient Greco-Roman ideal. The education during Renaissance was mainly composed of ancient literature and history. It was thought that the classics provided moral instruction and an intensive understanding of human behavior.
Art.
The Renaissance marks the period of European history at the close of the Middle Ages and the rise of the Modern world.
It represents a cultural rebirth from the 14th through the middle of the 17th centuries.
Early Renaissance, mostly in Italy, bridges the art period during the fifteenth century, between the Middle Ages and the High Renaissance in Italy. It is generally known that Renaissance matured in Northern Europe later, in the 16th century. One of the distinguishing features of Renaissance art was its development of highly realistic linear perspective. Giotto di Bondone (1267–1337) is credited with first treating a painting as a window into space, but it was not until the demonstrations of architect Filippo Brunelleschi (1377–1446) and the subsequent writings of Leon Battista Alberti (1404–1472) that perspective was formalized as an artistic technique.
The development of perspective was part of a wider trend towards realism in the arts. To that end, painters also developed other techniques, studying light, shadow, and, famously in the case of Leonardo da Vinci, human anatomy. Underlying these changes in artistic method, was a renewed desire to depict the beauty of nature, and to unravel the axioms of aesthetics, with the works of Leonardo, Michelangelo and Raphael representing artistic pinnacles that were to be much imitated by other artists. Other notable artists include Sandro Botticelli, working for the Medici in Florence, Donatello another Florentine and Titian in Venice, among others.
Concurrently, in the Netherlands, a particularly vibrant artistic culture developed, the work of Hugo van der Goes and Jan van Eyck having particular influence on the development of painting in Italy, both technically with the introduction of oil paint and canvas, and stylistically in terms of naturalism in representation. (see "Renaissance in the Netherlands"). Later, the work of Pieter Brueghel the Elder would inspire artists to depict themes of everyday life.
In architecture, Filippo Brunelleschi, the most inventive and gifted designer of all time, was foremost in studying the remains of ancient classical buildings, and with rediscovered knowledge from the 1st-century writer Vitruvius and the flourishing discipline of mathematics, formulated the Renaissance style which emulated and improved on classical forms. Brunelleschi's major feat of engineering was the building of the dome of Florence Cathedral. The first building to demonstrate this is claimed to be the church of St. Andrew built by Alberti in Mantua. The outstanding architectural work of the High Renaissance was the rebuilding of St. Peter's Basilica, combining the skills of Bramante, Michelangelo, Raphael, Sangallo and Maderno.
The Roman orders types of columns are used: Tuscan, Doric, Ionic, Corinthian and Composite. These can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. During the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Filippo Brunelleschi.
Arches, semi-circular or (in the Mannerist style) segmental, are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental. Renaissance vaults do not have ribs. They are semi-circular or segmental and on a square plan, unlike the Gothic vault which is frequently rectangular.
The Renaissance artists were not pagans although they admired antiquity and they also kept some ideas and symbols of the medieval past. Nicola Pisano (c. 1220–c. 1278) imitated classical forms by portraying scenes from the Bible. Pisano's "Annunciation" from the Baptistry at Pisa, demonstrates that classical models influenced Italian art before the Renaissance took root as a literary movement 
Science.
The rediscovery of ancient texts and the invention of printing democratized learning and allowed a faster propagation of ideas. In the first period of Italian Renaissance, humanists favoured the study of humanities over natural philosophy or applied mathematics. And their reverence for classical sources further enshrined the Aristotelian and Ptolemaic views of the universe.
Even though, around 1450, the writings of Nicholas Cusanus were anticipating Copernicus' heliocentric world-view, it was made in a philosophical fashion. Science and art were very much intermingled in the early Renaissance, with polymath artists such as Leonardo da Vinci making observational drawings of anatomy and nature. He set up controlled experiments in water flow, medical dissection, and systematic study of movement and aerodynamics; he devised principles of research method that led to Fritjof Capra classifying him as "father of modern science".
In 1492 the discovery of the New World by Christopher Columbus challenged the classical world-view, as the works of Ptolemy (geography) and Galen (medicine) were found not always to match everyday observations: a suitable environment was created to question scientific doctrine. As the Protestant Reformation and Counter-Reformation clashed, the Northern Renaissance showed a decisive shift in focus from Aristotelean natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine). The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements.
In the end of the 15th century, Luca Pacioli published the first work on bookkeeping, making him the founder of accounting.
Some have seen this as a "scientific revolution", heralding the beginning of the modern age, others as an acceleration of a continuous process stretching from the ancient world to the present day. Regardless, there is general agreement that the Renaissance saw significant changes in the way the universe was viewed and the methods sought to explain natural phenomena. Traditionally held to have begun in 1543, when were first printed the books "De humani corporis fabrica" ("On the Workings of the Human Body") by Andreas Vesalius, which gave a new confidence to the role of dissection, observation, and mechanistic view of anatomy, and also "De Revolutionibus", by Copernicus. The famous thesis of Copernicus's book was that the Earth moved around the Sun. Significant scientific advances were made during this time by Galileo Galilei, Tycho Brahe and Johannes Kepler.
One important development was not any specific discovery, but rather the further development of the "process" for discovery, the scientific method. It focused on empirical evidence, the importance of mathematics, and discarded Aristotelian science. Early and influential proponents of these ideas included Copernicus, Galileo, and Francis Bacon.
The new scientific method led to great contributions in the fields of astronomy, physics, biology, and anatomy.
Music.
From this changing society emerged a common, unifying musical language, in particular the polyphonic style of the Franco-Flemish school. The development of printing made distribution of music possible on a wide scale. Demand for music as entertainment and as an activity for educated amateurs increased with the emergence of a bourgeois class. Dissemination of chansons, motets, and masses throughout Europe coincided with the unification of polyphonic practice into the fluid style which culminated in the second half of the sixteenth century in the work of composers such as Palestrina, Lassus, Victoria and William Byrd.
Religion.
The new ideals of humanism, although more secular in some aspects, developed against a Christian backdrop, especially in the Northern Renaissance. Much, if not most, of the new art was commissioned by or in dedication to the Church. However, the Renaissance had a profound effect on contemporary theology, particularly in the way people perceived the relationship between man and God. Many of the period's foremost theologians were followers of the humanist method, including Erasmus, Zwingli, Thomas More, Martin Luther, and John Calvin.
The Renaissance began in times of religious turmoil. The late Middle Ages saw a period of political intrigue surrounding the Papacy, culminating in the Western Schism, in which three men simultaneously claimed to be true Bishop of Rome. While the schism was resolved by the Council of Constance (1414), the 15th century saw a resulting reform movement known as Conciliarism, which sought to limit the pope's power. Although the papacy eventually emerged supreme in ecclesiastical matters by the Fifth Council of the Lateran (1511), it was dogged by continued accusations of corruption, most famously in the person of Pope Alexander VI, who was accused variously of simony, nepotism and fathering four children (most of whom were married off, presumably for the consolidation of power) while a cardinal.
Churchmen such as Erasmus and Luther proposed reform to the Church, often based on humanist textual criticism of the New Testament. It was Luther who in October 1517 published the 95 Theses, challenging papal authority and criticizing its perceived corruption, particularly with regard to instances of sold indulgences. The 95 Theses led to the Reformation, a break with the Roman Catholic Church that previously claimed hegemony in Western Europe. Humanism and the Renaissance therefore played a direct role in sparking the Reformation, as well as in many other contemporaneous religious debates and conflicts.
In an era following the sack of Rome in 1527 and prevalent with uncertainties in the Catholic Church following the Protestant Reformation, Pope Paul III came to the papal throne (1534–1549), to whom Nicolaus Copernicus dedicated "De revolutionibus orbium coelestium" (On the Revolutions of the Celestial Spheres) and who became the grandfather of Alessandro Farnese (cardinal), who had paintings by Titian, Michelangelo, and Raphael, and an important collection of drawings and who commissioned the masterpiece of Giulio Clovio, arguably the last major illuminated manuscript, the "Farnese Hours".
Self-awareness.
By the 15th century, writers, artists, and architects in Italy were well aware of the transformations that were taking place and were using phrases such as, "modi antichi" (in the antique manner) or "alle romana et alla antica" (in the manner of the Romans and the ancients) to describe their work. In the 1330s Petrarch referred to pre-Christian times as "antiqua" (ancient) and to the Christian period as "nova" (new). From Petrarch's Italian perspective, this new period (which included his own time) was an age of national eclipse.
Leonardo Bruni was the first to use tripartite periodization in his "History of the Florentine People" (1442). Bruni's first two periods were based on those of Petrarch, but he added a third period because he believed that Italy was no longer in a state of decline. Flavio Biondo used a similar framework in "Decades of History from the Deterioration of the Roman Empire" (1439–1453).
Humanist historians argued that contemporary scholarship restored direct links to the classical period, thus bypassing the Medieval period, which they then named for the first time the "Middle Ages". The term first appears in Latin in 1469 as "media tempestas" (middle times). The term "la rinascita" (rebirth) first appeared, however, in its broad sense in Giorgio Vasari's "Lives of the Artists", 1550, revised 1568). Vasari divides the age into three phases: the first phase contains Cimabue, Giotto, and Arnolfo di Cambio; the second phase contains Masaccio, Brunelleschi, and Donatello; the third centers on Leonardo da Vinci and culminates with Michelangelo. It was not just the growing awareness of classical antiquity that drove this development, according to Vasari, but also the growing desire to study and imitate nature.
Spread.
In the 15th century, the Renaissance spread with great speed from its birthplace in Florence, first to the rest of Italy, and soon to the rest of Europe. The invention of the printing press by German printer Johannes Gutenberg allowed the rapid transmission of these new ideas. As it spread, its ideas diversified and changed, being adapted to local culture. In the 20th century, scholars began to break the Renaissance into regional and national movements.
Northern Europe.
The Renaissance as it occurred in Northern Europe has been termed the "Northern Renaissance". While Renaissance ideas were moving north from Italy, there was a simultaneous southward spread of some areas of innovation, particularly in music. The music of the 15th century Burgundian School defined the beginning of the Renaissance in that art and the polyphony of the Netherlanders, as it moved with the musicians themselves into Italy, formed the core of what was the first true international style in music since the standardization of Gregorian Chant in the 9th century. The culmination of the Netherlandish school was in the music of the Italian composer, Palestrina. At the end of the 16th century Italy again became a center of musical innovation, with the development of the polychoral style of the Venetian School, which spread northward into Germany around 1600.
The paintings of the Italian Renaissance differed from those of the Northern Renaissance. Italian Renaissance artists were among the first to paint secular scenes, breaking away from the purely religious art of medieval painters. At first, Northern Renaissance artists remained focused on religious subjects, such as the contemporary religious upheaval portrayed by Albrecht Dürer. Later on, the works of Pieter Bruegel influenced artists to paint scenes of daily life rather than religious or classical themes. It was also during the Northern Renaissance that Flemish brothers Hubert and Jan van Eyck perfected the oil painting technique, which enabled artists to produce strong colors on a hard surface that could survive for centuries. A feature of the Northern Renaissance was its use of the vernacular in place of Latin or Greek, which allowed greater freedom of expression. This movement had started in Italy with the decisive influence of Dante Alighieri on the development of vernacular languages; in fact the focus on writing in Italian has neglected a major source of Florentine ideas expressed in Latin. The spread of the technology of the German invention of movable type printing boosted the Renaissance, in Northern Europe as elsewhere; with Venice becoming a world center of printing.
England.
[[File:Shakespeare.jpg|thumb|upright|William Shakespeare|"What a piece of work is a man, how noble in reason, how infinite in faculties, in form and moving
how express and admirable, in action how like an angel, in apprehension
how like a god!" — from William Shakespeare's" Hamlet".]]
In England, the sixteenth century marked the beginning of the English Renaissance with the work of writers William Shakespeare, Christopher Marlowe, Edmund Spenser, Sir Thomas More, Francis Bacon, Sir Philip Sidney, as well as great artists, architects (such as Inigo Jones who introduced Italianate architecture to England), and composers such as Thomas Tallis, John Taverner, and William Byrd.
France.
The word "Renaissance" is borrowed from the French language, where it means "re-birth". It was first used in the eighteenth century and was later popularized by French historian Jules Michelet (1798–1874), in his 1855 work, "Histoire de France" (History of France).
In 1495 the Italian Renaissance arrived in France, imported by King Charles VIII after his invasion of Italy. A factor that promoted the spread of secularism was the Church's inability to offer assistance against the Black Death. Francis I imported Italian art and artists, including Leonardo da Vinci, and built ornate palaces at great expense. Writers such as François Rabelais, Pierre de Ronsard, Joachim du Bellay and Michel de Montaigne, painters such as Jean Clouet and musicians such as Jean Mouton also borrowed from the spirit of the Renaissance.
In 1533, a fourteen-year-old Caterina de' Medici (1519–1589), born in Florence to Lorenzo II de' Medici and Madeleine de la Tour d'Auvergne, married Henry, second son of King Francis I and Queen Claude. Though she became famous and infamous for her role in France's religious wars, she made a direct contribution in bringing arts, sciences and music (including the origins of ballet) to the French court from her native Florence.
Germany.
In the second half of the 15th century, the spirit of the age spread to Germany and the Low Countries, where the development of the printing press (ca. 1450) and early Renaissance artists such as the painters Jan van Eyck (1395–1441) and Hieronymus Bosch (1450–1516) and the composers Johannes Ockeghem (1410–1497), Jacob Obrecht (1457–1505) and Josquin des Prez (1455–1521), predated the influence from Italy. In the early Protestant areas of the country humanism became closely linked to the turmoil of the Protestant Reformation, and the art and writing of the German Renaissance frequently reflected this dispute.
However, the gothic style and medieval scholastic philosophy remained exclusively until the turn of the 16th century. Emperor Maximilian I of Habsburg (Ruling 1493–1519) was the first truly Renaissance monarch of the Holy Roman Empire, later known as "Holy Roman Empire of the German Nation" (Imperial Diet of Cologne, 1512).
Hungary.
After Italy, Hungary was the first European country where the renaissance appeared. The Renaissance style came directly from Italy during the Quattrocento to Hungary first in the Central European region, thanks to the development of early Hungarian-Italian relationships – not only in dynastic connections, but also in cultural, humanistic and commercial relations – growing in strength from the 14th century. The relationship between Hungarian and Italian Gothic styles was a second reason – exaggerated breakthrough of walls is avoided, preferring clean and light structures. Large-scale building schemes provided ample and long term work for the artists, for example, the building of the Friss (New) Castle in Buda, the castles of Visegrád, Tata and Várpalota. In Sigismund's court there were patrons such as Pipo Spano, a descendant of the Scolari family of Florence, who invited Manetto Ammanatini and Masolino da Pannicale to Hungary.
The new Italian trend combined with existing national traditions to create a particular local Renaissance art. Acceptance of Renaissance art was furthered by the continuous arrival of humanist thought in the country. Many young Hungarians studying at Italian universities came closer to the Florentine humanist center, so a direct connection with Florence evolved. The growing number of Italian traders moving to Hungary, specially to Buda, helped this process. New thoughts were carried by the humanist prelates, among them Vitéz János, archbishop of Esztergom, one of the founders of Hungarian humanism. During the long reign of emperor Sigismund of Luxemburg the Royal Castle of Buda became probably the largest Gothic palace of the late Middle Ages. King Matthias Corvinus (r. 1458–1490) rebuilt the palace in early Renaissance style and further expanded it.
After the marriage in 1476 of King Matthias to Beatrice of Naples, Buda became one of the most important artistic centres of the Renaissance north of the Alps. The most important humanists living in Matthias' court were Antonio Bonfini and the famous Hungarian poet Janus Pannonius. András Hess set up a printing press in Buda in 1472. Matthias Corvinus's library, the "Bibliotheca Corviniana", was Europe's greatest collections of secular books: historical chronicles, philosophic and scientific works in the 15th century. His library was second only in size to the Vatican Library. (However, the Vatican Library mainly contained Bibles and religious materials.)
In 1489, Bartolomeo della Fonte of Florence wrote that Lorenzo de' Medici founded his own Greek-Latin library encouraged by the example of the Hungarian king. Corvinus's library is part of UNESCO World Heritage.
Other important figures of Hungarian Renaissance: Bálint Balassi (poet), Sebestyén Tinódi Lantos (poet), Bálint Bakfark (composer and lutenist)
Netherlands.
Culture in the Netherlands at the end of the 15th century was influenced by the Italian Renaissance, through trade via Bruges which made Flanders wealthy. Its nobles commissioned artists who became known across Europe. In science, the anatomist Andreas Vesalius led the way; in cartography, Gerardus Mercator's map assisted explorers and navigators. In art, Dutch and Flemish Renaissance painting went from the strange work of Hieronymus Bosch to the everyday life of Pieter Brueghel the Elder.
Poland.
An early Italian humanist who came to Poland in the mid-15th century was Filippo Buonaccorsi. Many Italian artists came to Poland with Bona Sforza of Milan, when she married King Sigismund I the Old in 1518. This was supported by temporarily strengthened monarchies in both areas, as well as by newly established universities. The Polish Renaissance lasted from the late 15th to the late 16th century and is widely considered to have been the Golden Age of Polish culture. Ruled by the Jagiellon dynasty, the Kingdom of Poland (from 1569 known as the Polish-Lithuanian Commonwealth) actively participated in the broad European Renaissance. The multi-national Polish state experienced a substantial period of cultural growth thanks in part to a century without major wars – aside from conflicts in the sparsely populated eastern and southern borderlands. The Reformation spread peacefully throughout the country (giving rise to the Polish Brethren), while living conditions improved, cities grew, and exports of agricultural products enriched the population, especially the nobility ("szlachta") who gained dominance in the new political system of Golden Liberty.
The Polish Renaissance architecture has three periods of development.
Portugal.
Although Italian Renaissance had a modest impact in Portuguese arts, Portugal was influential in broadening the European worldview, stimulating humanist inquiry. Renaissance arrived through the influence of wealthy Italian and Flemish merchants who invested in the profitable commerce overseas. As the pioneer headquarters of European exploration, Lisbon flourished in the late 15th century, attracting experts who made several breakthroughs in mathematics, astronomy and naval technology including Pedro Nunes, João de Castro, Abraham Zacuto and Martin Behaim. Cartographers Pedro Reinel, Lopo Homem, Estêvão Gomes and Diogo Ribeiro made crucial advances to help mapping the world. Apothecary Tomé Pires and physicians Garcia de Orta and Cristóvão da Costa collected and published works on plants and medicines, soon translated by Flemish pioneer botanist Carolus Clusius.
In architecture, the huge profits of the spice trade financed a sumptuous composite style in the first decades of the 16th century, the Manueline, incorporating maritime elements. The main painters being Nuno Gonçalves, Gregório Lopes and Vasco Fernandes. In music, Pedro de Escobar and Duarte Lobo, and four songbooks, including the Cancioneiro de Elvas. In literature, Sá de Miranda introduced Italian forms of verse, Bernardim Ribeiro developed pastoral romance; Gil Vicente plays fused it with popular culture, reporting the changing times, and Luís de Camões inscribed the Portuguese feats overseas in the epic poem Os Lusíadas. Travel literature specially flourished: João de Barros, Castanheda, António Galvão, Gaspar Correia, Duarte Barbosa, Fernão Mendes Pinto, among others, described new lands and were translated and spread with the new printing press. After joining the Portuguese exploration of Brazil in 1500, Amerigo Vespucci coined the term New World, in his letters to Lorenzo di Pierfrancesco de' Medici.
The intense international exchange produced several cosmopolitan humanist scholars: Francisco de Holanda, André de Resende and Damião de Góis, a friend of Erasmus who wrote with rare independence on the reign of King Manuel I; Diogo and André de Gouveia, who made relevant teaching reforms via France. Foreign news and products in the Portuguese factory in Antwerp attracted the interest of Thomas More and Dürer to the wider world. There, profits and know-how helped nurture the Dutch Renaissance and Golden Age, especially after the arrival of the wealthy cultured Jewish community expelled from Portugal.
Russia.
Renaissance trends from Italy and Central Europe influenced Russia in many ways, though this influence was rather limited due to the large distances between Russia and the main European cultural centers, on one hand, and the strong adherence of Russians to their Orthodox traditions and Byzantine legacy, on the other hand.
Prince Ivan III introduced Renaissance architecture to Russia by inviting a number of architects from Italy, who brought new construction techniques and some Renaissance style elements with them, while in general following the traditional designs of the Russian architecture. In 1475 the Bolognese architect Aristotele Fioravanti came to rebuild the Cathedral of the Dormition in the Moscow Kremlin, damaged in an earthquake. Fioravanti was given the 12th-century Vladimir Cathedral as a model, and produced a design combining traditional Russian style with a Renaissance sense of spaciousness, proportion and symmetry.
In 1485 Ivan III commissioned the building of a royal Terem Palace within the Kremlin, with Aloisio da Milano being the architect of the first three floors. Aloisio da Milano, as well as the other Italian architects, also greatly contributed to the construction of the Kremlin walls and towers. The small banqueting hall of the Russian Tsars, called the Palace of Facets because of its facetted upper story, is the work of two Italians, Marco Ruffo and Pietro Solario, and shows a more Italian style. In 1505, an Italian known in Russia as Aleviz Novyi or Aleviz Fryazin arrived in Moscow. He may have been the Venetian sculptor, Alevisio Lamberti da Montagne. He built 12 churches for Ivan III, including the Cathedral of the Archangel, a building remarkable for the successful blending of Russian tradition, Orthodox requirements and Renaissance style. It is believed that the Cathedral of the Metropolitan Peter in Vysokopetrovsky Monastery, another work of Aleviz Novyi, later served as an inspiration for the so-called "octagon-on-tetragon" architectural form in the Moscow Baroque of the late 17th century.
Between the early 16th and the late 17th centuries, however, an original tradition of stone tented roof architecture had been developed in Russia. It was quite unique and different from the contemporary Renaissance architecture elsewhere in Europe, though some researches call that style 'Russian Gothic' and compare it with the European Gothic architecture of the earlier period. The Italians, with their advanced technology, may have influenced the invention of the stone tented roof (the wooden tents were known in Russia and Europe long before). According to one hypothesis, an Italian architect called Petrok Maly may have been an author of the Ascension Church in Kolomenskoye, one of the earliest and most prominent tented roof churches.
By the 17th century the influence of Renaissance painting resulted in Russian icons becoming slightly more realistic, while still following most of the old icon painting canons, as seen in the works of Bogdan Saltanov, Simon Ushakov, Gury Nikitin, Karp Zolotaryov and other Russian artists of the era. Gradually the new type of secular portrait painting appeared, called "parsúna" (from "persona" – person), which was transitional style between abstract iconographics and real paintings.
In the mid 16th-century Russians adopted printing from Central Europe, with Ivan Fyodorov being the first known Russian printer. In the 17th century printing became widespread, and woodcuts became especially popular. That led to the development of a special form of folk art known as lubok printing, which persisted in Russia well into the 19th century.
A number of technologies from the European Renaissance period were adopted by Russia rather early, and subsequently perfected to become a part of a strong domestic tradition. Mostly these were military technologies, such as cannon casting adopted by at least the 15th century. The Tsar Cannon, which is the world's largest bombard by caliber, is a masterpiece of Russian cannon making. It was cast in 1586 by Andrey Chokhov, and is notable for its rich, decorative relief. Another technology, that according to one hypothesis originally was brought from Europe by the Italians, resulted in the development of vodka, the national beverage of Russia. As early as 1386 the Genoese ambassadors brought the first aqua vitae ("the living water") to Moscow and presented it to Grand Duke Dmitry Donskoy. The Genoese likely got this beverage with the help of the alchemists of Provence, who used an Arab-invented distillation apparatus to convert grape must into alcohol. A Moscovite monk called Isidore used this technology to produce the first original Russian vodka c. 1430.
Spain.
The Renaissance arrived in the Iberian peninsula through the Mediterranean possessions of the Aragonese Crown and the city of Valencia. Indeed, many of the early Spanish Renaissance writers come from the Kingdom of Aragon, including Ausiàs March and Joanot Martorell. In the Kingdom of Castile, the early Renaissance was heavily influenced by the Italian humanism, starting with writers and poets starting with the Marquis of Santillana, who introduced the new Italian poetry to Spain in the early 15th century. Other writers, such as Jorge Manrique, Fernando de Rojas, Juan del Encina, Juan Boscán Almogáver and Garcilaso de la Vega, kept a close resemblance to the Italian canon. Miguel de Cervantes's masterpiece Don Quixote is credited as the first Western novel. Renaissance humanism flourished in the early 16th century, with influential writers such as philosopher Juan Luis Vives, grammarian Antonio de Nebrija or natural historian Pedro de Mexía.
Later Spanish Renaissance tended towards religious themes and mysticism, with poets such as fray Luis de León, Teresa of Ávila and John of the Cross, and treated issues related to the exploration of the New World, with chroniclers and writers such as Inca Garcilaso de la Vega or Bartolomé de las Casas, giving rise to a body of work, now known as Spanish Renaissance literature. The late Renaissance in Spain also saw the rise of artists such as El Greco, and composers such as Tomás Luis de Victoria and Antonio de Cabezón.
Historiography.
Conception.
The Italian artist and critic Giorgio Vasari (1511–1574) first used the term "rinascita" retrospectively in his book "The Lives of the Artists" (published 1550). In the book Vasari attempted to define what he described as a break with the barbarities of gothic art: the arts (he held) had fallen into decay with the collapse of the Roman Empire and only the Tuscan artists, beginning with Cimabue (1240–1301) and Giotto (1267–1337) began to reverse this decline in the arts. Vasari saw antique art as central to the rebirth of Italian art.
However, only in the 19th century did the French word "Renaissance" achieve popularity in describing the self-conscious cultural movement based on revival of Roman models that began in the late-13th century. French historian Jules Michelet (1798–1874) defined "The Renaissance" in his 1855 work, "Histoire de France" as an entire historical period, whereas previously it had been used in a more limited sense. For Michelet, the Renaissance was more a development in science than in art and culture. He asserted that it spanned the period from Columbus to Copernicus to Galileo; that is, from the end of the 15th century to the middle of the 17th century. Moreover, Michelet distinguished between what he called, "the bizarre and monstrous" quality of the Middle Ages and the democratic values that he, as a vocal Republican, chose to see in its character. A French nationalist, Michelet also sought to claim the Renaissance as a French movement.
The Swiss historian Jacob Burckhardt (1818–1897) in his "The Civilization of the Renaissance in Italy" (1860), by contrast, defined the Renaissance as the period between Giotto and Michelangelo in Italy, that is, the 14th to mid-16th centuries. He saw in the Renaissance the emergence of the modern spirit of individuality, which the Middle Ages had stifled. His book was widely read and became influential in the development of the modern interpretation of the Italian Renaissance. However, Buckhardt has been accused of setting forth a linear Whiggish view of history in seeing the Renaissance as the origin of the modern world.
More recently, some historians have been much less keen to define the Renaissance as a historical age, or even as a coherent cultural movement. The historian Randolph Starn, of the University of California Berkeley, stated in 1998:
Debates about progress.
There is debate about the extent to which the Renaissance improved on the culture of the Middle Ages. Both Michelet and Burckhardt were keen to describe the progress made in the Renaissance towards the modern age. Burckhardt likened the change to a veil being removed from man's eyes, allowing him to see clearly.
On the other hand, many historians now point out that most of the negative social factors popularly associated with the medieval period – poverty, warfare, religious and political persecution, for example – seem to have worsened in this era which saw the rise of Machiavellian politics, the Wars of Religion, the corrupt Borgia Popes, and the intensified witch-hunts of the 16th century. Many people who lived during the Renaissance did not view it as the "golden age" imagined by certain 19th-century authors, but were concerned by these social maladies. Significantly, though, the artists, writers, and patrons involved in the cultural movements in question believed they were living in a new era that was a clean break from the Middle Ages. Some Marxist historians prefer to describe the Renaissance in material terms, holding the view that the changes in art, literature, and philosophy were part of a general economic trend from feudalism towards capitalism, resulting in a bourgeois class with leisure time to devote to the arts.
Johan Huizinga (1872–1945) acknowledged the existence of the Renaissance but questioned whether it was a positive change. In his book "The Waning of the Middle Ages", he argued that the Renaissance was a period of decline from the High Middle Ages, destroying much that was important. The Latin language, for instance, had evolved greatly from the classical period and was still a living language used in the church and elsewhere. The Renaissance obsession with classical purity halted its further evolution and saw Latin revert to its classical form. Robert S. Lopez has contended that it was a period of deep economic recession. Meanwhile, George Sarton and Lynn Thorndike have both argued that scientific progress was perhaps less original than has traditionally been supposed. Finally, Joan Kelly argued that the Renaissance led to greater gender dichotomy, lessening the agency women had had during the Middle Ages.
Some historians have begun to consider the word "Renaissance" to be unnecessarily loaded, implying an unambiguously positive rebirth from the supposedly more primitive "Dark Ages", the Middle Ages. Most historians now prefer to use the term "early modern" for this period, a more neutral designation that highlights the period as a transitional one between the Middle Ages and the modern era. Others such as Roger Osborne have come to consider the Italian Renaissance as a repository of the myths and ideals of western history in general, and instead of rebirth of ancient ideas as a period of great innovation.
Other Renaissances.
The term "Renaissance" has also been used to define periods outside of the 15th and 16th centuries. Charles H. Haskins (1870–1937), for example, made a case for a Renaissance of the 12th century. Other historians have argued for a Carolingian Renaissance in the 8th and 9th centuries, and still later for an Ottonian Renaissance in the 10th century. Other periods of cultural rebirth have also been termed "renaissances", such as the Bengal Renaissance, Tamil Renaissance, Nepal Bhasa renaissance, al-Nahda or the Harlem Renaissance.

</doc>
<doc id="25533" url="https://en.wikipedia.org/wiki?curid=25533" title="Rheged">
Rheged

Rheged () is described in poetic sources as one of the kingdoms of the "Hen Ogledd" ("Old North"), the Brittonic-speaking region of what is now Northern England and southern Scotland, in the Early Middle Ages. Its borders are not described in the poems, but some modern scholars have suggested that it included what is now Cumbria in North West England and possibly extended into Lancashire and Scotland. In the historical sources Rheged is intimately associated with the king Urien Rheged and his family. Its inhabitants spoke Cumbric, a Brittonic dialect closely related to Old Welsh.
Location.
The name Rheged appears regularly as an epithet of a certain Urien in a number of early Welsh poems and royal genealogies. His victories over the Anglian chieftains of Bernicia in the second half of the 6th century are recorded by Nennius and celebrated by the bard Taliesin, who calls him "Ruler of Rheged". He is thus placed squarely in the North of Britain and perhaps specifically in Westmorland when referred to as "Ruler of Llwyfenydd" (identified with the Lyvennet Valley). Later legend associates Urien with the city of Carlisle (the Roman Luguvalium), only twenty-five miles away; Higham suggests that Rheged was "broadly conterminous with the earlier "Civitas Carvetiorum", the Roman administrative unit based on Carlisle". Although it is possible that Rheged was merely a stronghold, it was not uncommon for sub-Roman monarchs to use their kingdom's name as an epithet. It is generally accepted, therefore, that Rheged was a kingdom covering a large part of modern Cumbria.
Place-name evidence from Dunragit (possibly "Fort of Rheged") suggests that, at least in one period of its history, Rheged extended into Dumfries and Galloway. More problematic interpretations suggest that it could also have reached as far south as Rochdale in Greater Manchester, recorded in the Domesday Book as "Recedham". The River Roch on which Rochdale stands was recorded in the 13th century as "Rached" or "Rachet". These place-names may (apparently) incorporate the element 'Rheged' precisely because they lay on or near its borders. Certainly Urien's kingdom stretched eastward at one time, as he was also "Ruler of Catraeth" (Catterick in North Yorkshire).
Kings of Rheged.
The traditional royal genealogy of Urien and his successors traces their ancestry back to Coel Hen (alias King Cole), who is considered by many to be a mythical figure; if he has some historicity, he may have ruled a considerable part of the North in the early 5th century. All of those listed below may have ruled in Rheged, but only three of their number can be verified from external sources:
Southern Rheged.
A second royal genealogy exists for a line, perhaps of kings, descended from Cynfarch Oer's brother: Elidir Lydanwyn. According to "Bonedd Gwŷr y Gogledd" Elidir's son, Llywarch Hen, was a ruler in North Britain in the 6th century. He was driven from his territory by princely in-fighting after Urien's death and was perhaps in old age associated with Powys. However, it is possible, because of internal inconsistencies, that the poetry connected to Powys was associated with Llywarch's name at a later, probably 9th century, date. Llywarch is referred to in some poems as king of South Rheged, and in others as king of Argoed, suggesting that the two regions were the same. Searching for Llywarch's kingdom has led some historians to propose that Rheged may have been divided between sons, resulting in northern and southern successor states. The connections of the family of Llywarch and Urien with Powys has suggested to some, on grounds of proximity, that the area of modern Lancashire may have been their original home.
End of Rheged.
After Bernicia united with Deira to become the kingdom of Northumbria, Rheged was annexed by Northumbria, some time before AD 730. There was a royal marriage between Prince (later King) Oswiu of Northumbria and the Rhegedian princess Riemmelth, granddaughter of Rum (Rhun), probably in 638, so it is probable that it was a peaceful takeover, both kingdoms being inherited by the same man.
After Rheged was incorporated into Northumbria, the old Cumbric language was gradually replaced by Old English, Cumbric surviving only in remote upland communities. In the 10th century, after the power of Northumbria was destroyed by Viking incursions and settlement, large areas west of the Pennines fell without warfare under the control of the British Kingdom of Strathclyde, with Leeds recorded as being on the border between the Britons and the Norse Kingdom of York. This may have represented the political assertion of lingering British culture in the region. The area of Cumbria remained under the control of Strathclyde until the early 11th century when Strathclyde itself was absorbed into the Scottish kingdom. The name of the people, whose modern Welsh form is "Cymry" has, however, survived in the name of Cumberland and now Cumbria; it probably derives from an old Celtic word *"Kombroges" meaning "fellow countrymen".
Rheged Centre.
The name Rheged has been adopted by the Rheged Centre close to Penrith in Cumbria. The centre has a number of retail outlets and cafés with a Cumbrian theme, as well as the largest turf roof in Europe and a giant cinema screen. A special film was commissioned for the opening of the centre in 2000 but this film is no longer being shown. (Rheged: The Lost Kingdom - http://www.imdb.com/title/tt0248362/ )

</doc>
<doc id="25534" url="https://en.wikipedia.org/wiki?curid=25534" title="Romanian language">
Romanian language

Romanian (obsolete spellings Rumanian, Roumanian; autonym: "română", "limba română" , "the Romanian language", or "românește", lit. "in Romanian") is a Romance language spoken by around 24 million people as a native language, primarily in Romania and Moldova, and by another 4 million people as a second language. It has official status in Romania, the Republic of Moldova, the unrecognized state of Transnistria, the Autonomous Province of Vojvodina in Serbia, and the autonomous monastic state of Mount Athos in Greece. It is one of the official languages of the European Union and the Latin Union.
Romanian is a part of the Balkan-Romance group that evolved from several dialects of Vulgar Latin separated from the Western Romance during the 5th-8th centuries. To distinguish it within that group in comparative linguistics it is called "Daco-Romanian" as opposed to its closest relatives, Aromanian, Megleno-Romanian and Istro-Romanian, respectively. 
During Soviet times—and to some extent even today—Romanian was called "Moldovan" in the Republic of Moldova, although the Constitutional Court ruled in 2013 that "the official language of the republic is Romanian".
Romanian speakers are scattered across many other countries, notably Australia, Italy, Spain, Ukraine, Bulgaria, the United States, Canada, Brazil, Mexico, Argentina, Greece, Turkey, Israel, Russia, Portugal, the United Kingdom, Cyprus, France and Germany.
History.
Prehistory.
Eastern Romance languages, like the other branches of Romance languages, descend from Vulgar Latin, adopted in Dacia by a process of Romanization during early centuries AD.
The influence of the military in Dacia is due to the distribution of the military units in this bridgehead of the Roman Empire's defense (two legiones, 12 alae, 41 cohortes and 13 numeri), contrary, e.g., to that of the Rhenish army, which was concentrated at the Germanic limes and so left little influence on the local spoken Gallo-Latin. The identification of numerous words of military (Dacian-)Roman usage – 52 semantic specific changes and inherited military Latin words with their classical meanings – is at the heart of the hypothesis that the Romanian language is the continuation of the military Latin spoken in the north-eastern frontier region of the Roman Empire. These vestiges of military usage are unique to Romanian in its language family. Thus, Romanian is scientifically very interesting from a linguistic and historical viewpoint, since Romance languages did not prevail in the other frontier regions of the Roman Empire in Europe, Asia and Africa. Also, the conservation in Romanian of these numerous vestiges of Latin military slang (sermo castrensis) – such as (“to waylay”), (“helmet”), (“emperor”), (“to encircle with pressure”), (“to venture”), (“to make thin a tree for its collapse on the invaders”), (“made thin a tree”), “fiancé” (< Lat. “soldiers”, metonymy), (“to advance”), (“to kill”), “sense” a.s.o. (< Lat. “beak at prow of Roman warship”), “village” (< Lat. “trench for defence”, metonymy), “plain” (< Lat. “plane place for camping”, metonymy), (“to subject”), “veranda” (< Lat. “tent out of agglomerated fortress”, metonymy), “homeland” (< Lat. “earth” ˃ Arom. "țară" “earth”) a.s.o. and their absence in Aromanian (Balkan Romanian dialect spoken in peaceful area) – indicates the continuity of the Latinophons in northern Danubian region, this despite dire and constant defensive wars with Germanic, Turanian and Slavic populations who entered and eventually settled there. This linguistic evidence challenges the Roeslerian theory. The vestiges from sermo castrensis particularize the Romanian language in the neolatin area, together with its isolated history.
The Roman Empire withdrew from Dacia in AD 271-5, leaving it to the Goths.
The history of Eastern Romance between the 3rd century and the development of Proto-Romanian by the 10th century, when the area came under the influence of the Byzantine Empire, is unknown. It is a matter of debate whether Proto-Romanian developed among Romanized people who were left behind in Dacia by the Roman withdrawal or among Latin-speakers in the Balkans south of the Danube.
During the Middle Ages, Romanian became influenced by the Slavic languages and to some degree by Greek. Romanian remains unattested throughout the Middle Ages, and only enters the historical record in the early 16th century.
Early history.
The use of the denomination "Romanian" ("română") for "our beautiful language" ("limba noastră cea frumoasă") and use of the demonym "Romanians" ("Români") for speakers of this language predates the foundation of the modern Romanian state. Although the followers of the former Romanian voievodships used to designate themselves as "Ardeleni" (or "Ungureni"), "Moldoveni" or "Munteni", the name of "rumână" or "rumâniască" for the Romanian language itself is attested earlier, during the 16th century, by various foreign travellers into the Carpathian Romance-speaking space, as well as in other historical documents written in Romanian at that time such as Cronicile Țării Moldovei ("The Chronicles of the land of Moldova") by Grigore Ureche.
In 1534, Tranquillo Andronico notes: ""Valachi nunc se Romanos vocant"" ("se denumesc Români" - "are calling themselves Romanians"). Francesco della Valle writes in 1532 that Romanians ""se denumesc Români în limba lor"" ("are calling themselves Romanians in their own language"), and he subsequently quotes the expression: ""Știi Românește?"" (Do you know Romanian?).
After travelling through Wallachia, Moldavia and Transylvania Ferrante Capecci accounts in 1575 that the indigenous population of these regions call themselves “românești” (""romanesci"").
Pierre Lescalopier writes in 1574 that those who live in Moldavia, Wallachia and the vast part of Transylvania, "“se consideră adevărați urmași ai romanilor și-și numesc limba “românește”, adică romana”" ("they consider themselves as the descendants of the Romans and they name their language Romanian").
The Transylvanian Saxon Johann Lebel writes in 1542 that "«Vlachi» se numeau între ei «Romuini»" and the Polish chronicler Stanislaw Orzechowski (Orichovius) notes in 1554 that "în limba lor «walachii» se numesc «romini »" ("In their language the Wallachians call themselves Romanians").
The Croatian Ante Verančić precises in 1570 that "«Vlahii» din Transilvania, Moldova și Țara Românească se desemnează ca «romani»" and the Transylvanian Hungarian Martinus Szent-Ivany in 1699 quotes the following: "«Si noi sentem Rumeni»" ("Și noi suntem români" - "We are Romanians as well") and "«Noi sentem di sange Rumena»" ("Noi suntem de sânge român" - "We are of Romanian blood").
In Palia de la Orăștie (1581) stands written "«.[...] că văzum cum toate limbile au și înfluresc întru cuvintele slăvite a lui Dumnezeu numai noi românii pre limbă nu avem. Pentru aceia cu mare muncă scoasem de limba jidovească si grecească si srâbească pre limba românească 5 cărți ale lui Moisi prorocul si patru cărți și le dăruim voo frați rumâni și le-au scris în cheltuială multă... și le-au dăruit voo fraților români... și le-au scris voo fraților români»" and in Letopisețul Țării Moldovei written by the Moldavian chronicler Grigore Ureche we can read: "«În Țara Ardialului nu lăcuiesc numai unguri, ce și sași peste seamă de mulți și români peste tot locul...»".
Nevertheless, the oldest extant document written in Romanian remains Neacșu's letter (1521) and was written using Cyrillic letters (which remained in use up until the late 19th century). There are no records of any other documents written in Romanian from before 1521.
The language remains poorly attested during the Early Modern period.
Miron Costin, in his "De neamul moldovenilor" (1687), while noting that Moldavians, Wallachians, and the Romanians living in the Hungarian Country have the same origin, says that although people of Moldavia call themselves "Moldavians", they name their language "Romanian" ("românește") instead of "Moldavian" ("moldovenește").
Dimitrie Cantemir, in his "Descriptio Moldaviae" (Berlin, 1714), points out that the inhabitants of Moldavia, Wallachia and Transylvania spoke the same language. He notes, however, some differences in accent and vocabulary.
Cantemir's work provides one of the earliest histories of the language, in which he notes, like Ureche before him, the evolution from Latin and notices the Greek and Polish borrowings. Additionally, he introduces the idea that some words must have had Dacian roots. Cantemir also notes that while the idea of a Latin origin of the language was prevalent in his time, other scholars considered it to have derived from Italian.
Modern history.
The first Romanian grammar was published in Vienna in 1780.
Following the annexation of Bessarabia by Russia (after 1812), Moldavian was established as an official language in the governmental institutions of Bessarabia, used along with Russian,
The publishing works established by Archbishop Gavril Bănulescu-Bodoni were able to produce books and liturgical works in Moldavian between 1815–1820.
The linguistic situation in Bessarabia from 1812 to 1918 was the gradual development of bilingualism. Russian continued to develop as the official language of privilege, whereas Romanian remained the principal vernacular.
The period from 1905 to 1917 was one of increasing linguistic conflict, with the re-awakening of Romanian national consciousness. In 1905 and 1906, the Bessarabian "zemstva" asked for the re-introduction of Romanian in schools as a "compulsory language", and the "liberty to teach in the mother language (Romanian language)". At the same time, Romanian-language newspapers and journals began to appear, such as "Basarabia" (1906), "Viața Basarabiei" (1907), "Moldovanul" (1907), "Luminătorul" (1908), "Cuvînt moldovenesc" (1913), "Glasul Basarabiei" (1913). From 1913, the synod permitted that "the churches in Bessarabia use the Romanian language".-->
Romanian finally became the official language with the Constitution of 1923.
Historical grammar.
Romanian has preserved a part of the Latin declension, but whereas Latin had six cases, from a morphological viewpoint, Romanian has only three: the nominative-accusative, the genitive-dative, and marginally the vocative. Romanian nouns also preserve the neuter gender, although instead of functioning as a separate gender with its own forms in adjectives, the Romanian neuter became a mixture of masculine and feminine. The verb morphology of Romanian has shown the same move towards a compound perfect and future tense as the other Romance languages. Compared with the other Romance languages, during its evolution, Romanian simplified the original Latin tense system in extreme ways, in particular the absence of sequence of tenses.
Geographic distribution.
Romanian is spoken mostly in Central and the Balkan region of Southern Europe, although speakers of the language can be found all over the world, mostly due to emigration of Romanian nationals and the return of immigrants to Romania back to their original countries. Romanian speakers account for 0.5% of the world's population, and 4% of the Romance-speaking population of the world.
Romanian is the single official and national language in Romania and Moldova, although it shares the official status at regional level with other languages in the Moldovan autonomies of Gagauzia and Transnistria. Romanian is also an official language of the Autonomous Province of Vojvodina in Serbia along with five other languages. Romanian minorities are encountered in Serbia (Timok Valley), Ukraine (Chernivtsi and Odessa oblasts), and Hungary (Gyula). Large immigrant communities are found in Italy, Spain, France, and Portugal.
In 1995, the largest Romanian-speaking community in the Middle East was found in Israel, where Romanian was spoken by 5% of the population. Romanian is also spoken as a second language by people from Arabic-speaking countries who have studied in Romania. It is estimated that almost half a million Middle Eastern Arabs studied in Romania during the 1980s. Small Romanian-speaking communities are to be found in Kazakhstan and Russia. Romanian is also spoken within communities of Romanian and Moldovan immigrants in the United States, Canada and Australia, although they do not make up a large homogeneous community state-wide.
Legal status.
In Romania.
According to the Constitution of Romania of 1991, as revised in 2003, Romanian is the official language of the Republic.
Romania mandates the use of Romanian in official government publications, public education and legal contracts. Advertisements as well as other public messages must bear a translation of foreign words, while trade signs and logos shall be written predominantly in Romanian.
The Romanian Language Institute ("Institutul Limbii Române"), established by the Ministry of Education of Romania, promotes Romanian and supports people willing to study the language, working together with the Ministry of Foreign Affairs' Department for Romanians Abroad.
In Moldova.
Romanian is the official language of the Republic of Moldova. The 1991 Declaration of Independence names the official language Romanian. The Constitution of Moldova names the state language of the country Moldovan. In December 2013, a decision of the Constitutional Court of Moldova ruled that the Declaration of Independence takes precedence over the Constitution and the state language should be called Romanian.
Scholars agree that Moldovan and Romanian are the same language, with the glottonym "Moldovan" used in certain political contexts. It has been the sole official language since the adoption of the Law on State Language of the Moldavian SSR in 1989. This law mandates the use of Moldovan in all the political, economical, cultural and social spheres, as well as asserting the existence of a "linguistic Moldo-Romanian identity". It is also used in schools, mass media, education and in the colloquial speech and writing. Outside the political arena the language is most often called "Romanian". In the breakaway territory of Transnistria, it is co-official with Ukrainian and Russian.
In the 2004 census, out of the 3,383,332 people living in Moldova, 16.5% (558,508) stated Romanian as their native language, whereas 60% stated Moldovan. While 40% of all urban Romanian/Moldovan speakers identified their native tongue as Romanian, in the countryside under 12% of Romanian/Moldovan speakers indicated Romanian as their native language. However, the group of experts from the international census observation Mission to the Republic of Moldova concluded that the items in the questionnaire dealing with nationality and language proved to be the most sensitive ones, particularly with reference to the recording of responses to these questions as being "Moldovan" or "Romanian", and therefore it concluded that special care would need to be taken in interpreting them.
In Vojvodina, Serbia.
[[File:Romanian and Vlach language in Serbia.png|thumb|left|180px|Romanian language in entire Serbia "(see also Romanians of Serbia)", census 2002
The Constitution of the Republic of Serbia determines that in the regions of the Republic of Serbia inhabited by national minorities, their own languages and scripts shall be officially used as well, in the manner established by law.
The Statute of the Autonomous Province of Vojvodina determines that, together with the Serbo-Croat language and the Cyrillic script, and the Latin script as stipulated by the law, the Hungarian, Slovak, Romanian and Rusyn languages and their scripts, as well as languages and scripts of other nationalities, shall simultaneously be officially used in the work of the bodies of the Autonomous Province of Vojvodina, in the manner established by the law. The bodies of the Autonomous Province of Vojvodina are: the Assembly, the Executive Council and the Provincial administrative bodies.
The Romanian language and script are officially used in eight municipalities: Alibunar, Biserica Albă (), Zitiște (Žitište), Zrenianin (Zrenjanin), Kovăcița (Kovačica), Cuvin (Kovin), Plandiște (Plandište) and Sečanj. In the municipality of Vârșeț (Vršac), Romanian is official only in the villages of Voivodinț (Vojvodinci), Marcovăț (Markovac), Straja (Straža), Jamu Mic (Mali Žam), Srediștea Mică (Malo Središte), Mesici (Mesić), Jablanka, Sălcița (Salčica), Râtișor (Ritiševo), Oreșaț (Orašac) and Coștei (Kuštilj).
In the 2002 Census, the last carried out in Serbia, 1.5% of Vojvodinians stated Romanian as their native language.
Regional language status in Ukraine.
In parts of Ukraine where Romanians constitute a significant share of the local population (districts in Chernivtsi, Odessa and Zakarpattia oblasts) Romanian is taught in schools as a primary language and there are Romanian-language newspapers, TV, and radio broadcasting.
The University of Chernivtsi in western Ukraine trains teachers for Romanian schools in the fields of Romanian philology, mathematics and physics.
In Hertsa Raion of Ukraine as well as in other villages of Chernivtsi Oblast and Zakarpattia Oblast, Romanian has been declared a "regional language" alongside Ukrainian as per the 2012 legislation on languages in Ukraine.
In other countries and organizations.
Romanian is an official or administrative language in various communities and organisations, such as the Latin Union and the European Union. Romanian is also one of the five languages in which religious services are performed in the autonomous monastic state of Mount Athos, spoken in the monk communities of Prodromos and Lacu.
As a second and foreign language.
Romanian is taught in some areas that have Romanian minority communities, such as Vojvodina in Serbia, Bulgaria, Ukraine and Hungary. The Romanian Cultural Institute (ICR) has since 1992 organised summer courses in Romanian for language teachers. There are also non-Romanians who study Romanian as a foreign language, for example the Nicolae Bălcescu High-school in Gyula, Hungary.
Romanian is taught as a foreign language in tertiary institutions, mostly in European countries such as Germany, France and Italy, and the Netherlands, as well as in the United States. Overall, it is taught as a foreign language in 43 countries around the world.
[[File:Knowledge Romanian Eastern EU.png|thumb|300px|left|Romanian as second and/or foreign language in Eastern Europe
Punctuation and capitalization.
Uses of punctuation peculiar to Romanian are:
Spelling issues between Romania and Moldova.
Prior to 2010, there existed a minor spelling difference between standard forms of Romanian language used in Romania and the variant (also called Moldovan) used in the Republic of Moldova—the Academy of Sciences of Moldova did not switch to the new spelling rules introduced by the Romanian Academy in 1993. In 2000, the Moldovan Academy recommended adopting the spelling rules used in Romania, and in 2010 the Academy launched a schedule for the transition to the new rules that was completed in 2011 (regarding publications) and is currently under implementation in the educational system (due to be completed within two school years). However as of 2015 most Moldovan commercial websites maintain the 'old' spelling.
Sample text.
English text:
Romanian – highlighted words were "directly" derived from Latin:
Contemporary Romanian – highlighted words are French or Italian loanwords:
Romanian, excluding French and Italian loanwords – highlighted words are Slavic loanwords:
Romanian, excluding loanwords and having almost the same meaning:

</doc>
<doc id="25536" url="https://en.wikipedia.org/wiki?curid=25536" title="Republic">
Republic

A republic (from ) is a sovereign state or country which is organised with a form of government in which power resides in elected individuals representing the citizen body and government leaders exercise power according to the rule of law. In modern times, the definition of a republic is commonly limited to a government which excludes a monarch. Currently, 147 of the world's 206 sovereign states use the word "republic" as part of their official names; not all of these are republics in the sense of having elected governments, nor do all nations with elected governments use the word "republic" in their names.
Both modern and ancient republics vary widely in their ideology and composition. In the classical and medieval period of Europe, many states were fashioned on the Roman Republic, which referred to the governance of the city of Rome, between it having kings and emperors. The Italian medieval and Renaissance political tradition, today referred to as "civic humanism", is sometimes considered to derive directly from Roman republicans such as Sallust and Tacitus. However, Greek-influenced Roman authors, such as Polybius and Cicero, sometimes also used the term as a translation for the Greek "politeia" which could mean regime generally, but could also be applied to certain specific types of regime which did not exactly correspond to that of the Roman Republic. Republics were not equated with classical democracies such as Athens, but had a democratic aspect.
Republics became more common in the Western world starting in the late 18th century, eventually displacing absolute monarchy as the most common form of government in Europe. In modern republics the executive is legitimized both by a constitution and by popular suffrage. Montesquieu included in his work "The Spirit of the Laws" both democracies, where all the people have a share in rule, and aristocracies or oligarchies, where only some of the people rule, as republican forms of government.
Most often a republic is a single sovereign state, but there are also sub-sovereign state entities that are referred to as republics, or which have governments that are described as 'republican' in nature. For instance, Article IV of the United States Constitution "guarantee to every State in this Union a Republican form of Government". In contrast, the Soviet Union was constitutionally described as a "federal multinational state", composed of 15 republics, two of which – Ukraine and Belarus – had their own seats at the United Nations.
Etymology.
The term originates as the Latin translation of Greek word "politeia". Cicero, among other Latin writers, translated "politeia" as "res publica" and it was in turn translated by Renaissance scholars as "republic" (or similar terms in various western European languages).
The term "politeia" can be translated as "form of government", "polity", or "regime", and is therefore not always a word for a specific type of regime as the modern word republic is. (One of Plato's major works on political science was titled "Politeia" and in English it is thus known as "The Republic". However, apart from the title, in modern translations of "The Republic", alternative translations of "politeia" are also used.) However, in Book III of his "Politics" (1279a), Aristotle was apparently the first classical writer to state that the term "politeia" can be used to refer more specifically to one type of "politeia": "When the citizens at large govern for the public good, it is called by the name common to all governments ("to koinon onoma pasōn tōn politeiōn"), government ("politeia")". And also amongst classical Latin, the term "republic" can be used in a general way to refer to any regime, or in a specific way to refer to governments which work for the public good.
In medieval Northern Italy, a number of city states had commune or signoria based governments. In the late Middle Ages, writers, such as Giovanni Villani, began writing about the nature of these states and the differences from other types of regime. They used terms such as "libertas populi", a free people, to describe the states. The terminology changed in the 15th century as the renewed interest in the writings of Ancient Rome caused writers to prefer using classical terminology. To describe non-monarchical states writers, most importantly Leonardo Bruni, adopted the Latin phrase "res publica".
While Bruni and Machiavelli used the term to describe the states of Northern Italy, which were not monarchies, the term "res publica" has a set of interrelated meanings in the original Latin. The term can quite literally be translated as "public matter". It was most often used by Roman writers to refer to the state and government, even during the period of the Roman Empire.
In subsequent centuries, the English word "commonwealth" came to be used as a translation of "res publica", and its use in English was comparable to how the Romans used the term "res publica". Notably, during The Protectorate of Oliver Cromwell the word "commonwealth" was the most common term to call the new monarchless state, but the word "republic" was also in common use.
Presently, the term "republic" commonly means a system of government which derives its power from the people rather than from another basis, such as heredity or divine right.
History.
While the philosophical terminology developed in classical Greece and Rome, as already noted by Aristotle there was already a long history of city states with a wide variety of constitutions, not only in Greece but also in the Middle East. After the classical period, during the Middle Ages, many free cities developed again, such as Venice.
Classical republics.
The modern type of "republic" itself is different from any type of state found in the classical world. Nevertheless, there are a number of states of the classical era that are today still called republics. This includes ancient Athens, Sparta and the Roman Republic. While the structure and governance of these states was very different from that of any modern republic, there is debate about the extent to which classical, medieval, and modern republics form a historical continuum. J. G. A. Pocock has argued that a distinct republican tradition stretches from the classical world to the present. Other scholars disagree. Paul Rahe, for instance, argues that the classical republics had a form of government with few links to those in any modern country.
The political philosophy of the classical republics have in any case had an influence on republican thought throughout the subsequent centuries. Philosophers and politicians advocating for republics, such as Machiavelli, Montesquieu, Adams, and Madison, relied heavily on classical Greek and Roman sources which described various types of regimes.
Aristotle's "Politics" discusses various forms of government. One form Aristotle named "politeia", which consisted of a mixture of the other forms. He argued that this was one of the ideal forms of government. Polybius expanded on many of these ideas, again focusing on the idea of mixed government. The most important Roman work in this tradition is Cicero's "De re publica".
Over time, the classical republics were either conquered by empires or became ones themselves. Most of the Greek republics were annexed to the Macedonian Empire of Alexander. The Roman Republic expanded dramatically conquering the other states of the Mediterranean that could be considered republics, such as Carthage. The Roman Republic itself then became the Roman Empire.
Other ancient republics.
The term "republic" is not commonly used to refer to pre-classical city states, especially if outside Europe and the area which was under Graeco-Roman influence. However some early states outside Europe had governments that are sometimes today considered similar to republics.
In the ancient Near East, a number of cities of the Eastern Mediterranean achieved collective rule. Arwad has been cited as one of the earliest known examples of a republic, in which the people, rather than a monarch, are described as sovereign. The Israelite confederation of the era before the United Monarchy has also been considered a type of republic. In Africa the Axum Empire was organized as a confederation ruled similarly to a royal republic. Similarly the Igbo nation of what is now Nigeria.
The ancient Indian subcontinent had a number of early republics known as Mahajanapadas. Mahajanapadas consisted of sixteen oligarchic republics that existed during the sixth centuries BCE to fourth centuries BCE. Some Indian scholars, such as K.P. Jayaswal, have argued that a number of states in ancient India had republican forms of government. There are no surviving constitutions or works of political philosophy from this period in Indian history, but surviving religious texts do refer to a number of states having "Gaṇa sangha", or council-based, as opposed to monarchical, governments.
Icelandic Commonwealth.
The Icelandic Commonwealth was established in 930 AD by refugees from Norway who had fled the unification of that country under King Harald Fairhair. The Commonwealth consisted of a number of clans run by chieftains, and the Althing was a combination of parliament and supreme court where disputes appealed from lower courts were settled, laws were decided, and decisions of national importance were taken. One such example was the Christianisation of Iceland in 1000, where the Althing decreed, in order to prevent an invasion, that all Icelanders must be baptized, and forbade celebration of pagan rituals. Contrary to most states, the Icelandic Commonwealth had no official leader.
In the early 13th century, the Age of the Sturlungs, the Commonwealth began to suffer from long conflicts between warring clans. This, combined with pressure from the Norwegian king Haakon IV for the Icelanders to re-join the Norwegian "family", led the Icelandic chieftains to accept Haakon IV as king by the signing of the "Gamli sáttmáli" ("Old Covenant") in 1262. This effectively brought the Commonwealth to an end. The Althing, however, is still Iceland's parliament, almost 800 years later.
Mercantile republics.
In Europe new republics appeared in the late Middle Ages when a number of small states embraced republican systems of government. These were generally small, but wealthy, trading states, like the Italian city-states and the Hanseatic League, in which the merchant class had risen to prominence. Knud Haakonssen has noted that, by the Renaissance, Europe was divided with those states controlled by a landed elite being monarchies and those controlled by a commercial elite being republics.
Across Europe a wealthy merchant class developed in the important trading cities. Despite their wealth they had little power in the feudal system dominated by the rural land owners, and across Europe began to advocate for their own privileges and powers. The more centralized states, such as France and England, granted limited city charters.
In the more loosely governed Holy Roman Empire, 51 of the largest towns became free imperial cities. While still under the dominion of the Holy Roman Emperor most power was held locally and many adopted republican forms of government. The same rights to imperial immediacy were secured by the major trading cities of Switzerland. The towns and villages of alpine Switzerland had, courtesy of geography, also been largely excluded from central control. Unlike Italy and Germany, much of the rural area was thus not controlled by feudal barons, but by independent farmers who also used communal forms of government. When the Habsburgs tried to reassert control over the region both rural farmers and town merchants joined the rebellion. The Swiss were victorious, and the Swiss Confederacy was proclaimed, and Switzerland has retained a republican form of government to the present.
Italy was the most densely populated area of Europe, and also one with the weakest central government. Many of the towns thus gained considerable independence and adopted commune forms of government. Completely free of feudal control, the Italian city-states expanded, gaining control of the rural hinterland. The two most powerful were the Republic of Venice and its rival the Republic of Genoa. Each were large trading ports, and further expanded by using naval power to control large parts of the Mediterranean. It was in Italy that an ideology advocating for republics first developed. Writers such as Bartholomew of Lucca, Brunetto Latini, Marsilius of Padua, and Leonardo Bruni saw the medieval city-states as heirs to the legacy of Greece and Rome.
Two Russian cities with powerful merchant class—Novgorod and Pskov—also adopted republican forms of government in 12th and 13th centuries, respectively, which ended when the republics were conquered by Muscovy/Russia at the end 15th – beginning of 16th century.
The dominant form of government for these early republics was control by a limited council of elite patricians. In those areas that held elections, property qualifications or guild membership limited both who could vote and who could run. In many states no direct elections were held and council members were hereditary or appointed by the existing council. This left the great majority of the population without political power, and riots and revolts by the lower classes were common. The late Middle Ages saw more than 200 such risings in the towns of the Holy Roman Empire. Similar revolts occurred in Italy, notably the Ciompi Revolt in Florence.
Calvinist republics.
While the classical writers had been the primary ideological source for the republics of Italy, in Northern Europe, the Protestant Reformation would be used as justification for establishing new republics. Most important was Calvinist theology, which developed in the Swiss Confederacy, one of the largest and most powerful of the medieval republics. John Calvin did not call for the abolition of monarchy, but he advanced the doctrine that the faithful had the right to overthrow irreligious monarchs. Advocacy for republics appeared in the writings of the Huguenots during the French Wars of Religion.
Calvinism played an important role in the republican revolts in England and the Netherlands. Like the city-states of Italy and the Hanseatic League, both were important trading centres, with a large merchant class prospering from the trade with the New World. Large parts of the population of both areas also embraced Calvinism. During the Dutch Revolt (beginning in 1566), the Dutch Republic emerged from rejection of Spanish Habsburg rule. However, the country did not adopt the republican form of government immediately: in the formal declaration of independence (Act of Abjuration, 1581), the throne of king Philip was only declared vacant, and the Dutch magistrates asked the Duke of Anjou, queen Elizabeth of England and prince William of Orange, one after another, to replace Philip. It took until 1588 before the Estates (the "Staten", the representative assembly at the time) decided to vest the sovereignty of the country in themselves.
In 1641 the English Civil War began. Spearheaded by the Puritans and funded by the merchants of London, the revolt was a success, and King Charles I was executed. In England James Harrington, Algernon Sidney, and John Milton became some of the first writers to argue for rejecting monarchy and embracing a republican form of government. The English Commonwealth was short lived, and the monarchy soon restored. The Dutch Republic continued in name until 1795, but by the mid-18th century the stadtholder had become a "de facto" monarch. Calvinists were also some of the earliest settlers of the British and Dutch colonies of North America.
Liberal republics.
Along with these initial republican revolts, early modern Europe also saw a great increase in monarchial power. The era of absolute monarchy replaced the limited and decentralized monarchies that had existed in most of the Middle Ages. It also saw a reaction against the total control of the monarch as a series of writers created the ideology known as liberalism.
Most of these Enlightenment thinkers were far more interested in ideas of constitutional monarchy than in republics. The Cromwell regime had discredited republicanism, and most thinkers felt that republics ended in either anarchy or tyranny. Thus philosophers like Voltaire opposed absolutism while at the same time being strongly pro-monarchy.
Jean-Jacques Rousseau and Montesquieu praised republics, and looked on the city-states of Greece as a model. However, both also felt that a nation-state like France, with 20 million people, would be impossible to govern as a republic. Rousseau admired the republican experiment in Corsica (1755-1769) and described his ideal political structure of small self-governing communes. Montesquieu felt that a city-state should ideally be a republic, but maintained that a limited monarchy was better suited to a large nation.
The American Revolution began as a rejection only of the authority of the British Parliament over the colonies, not of the monarchy. The failure of the British monarch to protect the colonies from what they considered the infringement of their rights to representative government, the monarch's branding of those requesting redress as traitors, and his support for sending combat troops to demonstrate authority resulted in widespread perception of the British monarchy as tyrannical. With the United States Declaration of Independence the leaders of the revolt firmly rejected the monarchy and embraced republicanism. The leaders of the revolution were well versed in the writings of the French liberal thinkers, and also in history of the classical republics. John Adams had notably written a book on republics throughout history. In addition, the widely distributed and popularly read-aloud tract "Common Sense", by Thomas Paine, succinctly and eloquently laid out the case for republican ideals and independence to the larger public. The Constitution of the United States, ratified in 1789, created a relatively strong federal republic to replace the relatively weak confederation under the first attempt at a national government with the Articles of Confederation and Perpetual Union ratified in 1783. The first ten amendments to the Constitution, called the United States Bill of Rights, guaranteed certain natural rights fundamental to republican ideals that justified the Revolution.
The French Revolution was also not republican at its outset. Only after the Flight to Varennes removed most of the remaining sympathy for the king was a republic declared and Louis XVI sent to the guillotine. The stunning success of France in the French Revolutionary Wars saw republics spread by force of arms across much of Europe as a series of client republics were set up across the continent. The rise of Napoleon saw the end of the French First Republic and her Sister Republics, each replaced by 'popular monarchies'. Throughout the Napoleonic period, the victors extinguished many of the oldest republics on the continent, including the Republic of Venice, the Republic of Genoa, and the Dutch Republic. They were eventually transformed into monarchies or absorbed into neighbouring monarchies.
Outside Europe another group of republics was created as the Napoleonic Wars allowed the states of Latin America to gain their independence. Liberal ideology had only a limited impact on these new republics. The main impetus was the local European descended Creole population in conflict with the Peninsulares - governors sent from overseas. The majority of the population in most of Latin America was of either African or Amerindian descent, and the Creole elite had little interest in giving these groups power and broad-based popular sovereignty. Simón Bolívar, both the main instigator of the revolts and one of its most important theorists, was sympathetic to liberal ideals but felt that Latin America lacked the social cohesion for such a system to function and advocated autocracy as necessary.
In Mexico this autocracy briefly took the form of a monarchy in the First Mexican Empire. Due to the Peninsular War, the Portuguese court was relocated to Brazil in 1808. Brazil gained independence as a monarchy on September 7, 1822, and the Empire of Brazil lasted until 1889. In the other states various forms of autocratic republic existed until most were liberalized at the end of the 20th century.
The French Second Republic was created in 1848, but abolished by Napoleon III who proclaimed himself Emperor in 1852. The French Third Republic was established in 1870, when a civil revolutionary committee refused to accept Napoleon III's surrender during the Franco-Prussian War. Spain briefly became the First Spanish Republic in 1873–4, but the monarchy was soon restored. By the start of the 20th century France, Switzerland and San Marino remained the only republics in Europe. This changed when, after the 1908 Lisbon Regicide, the 5 October 1910 revolution established the Portuguese Republic. This would encourage new republics during and in the aftermath of World War I, when several of the largest European empires collapsed: the German Empire, Austro-Hungarian Empire, Russian Empire, and Ottoman Empire were all replaced by republics. New states gained independence during this turmoil, and many of these, such as Ireland, Poland, Finland and Czechoslovakia, chose republican forms of government. Following Greece's defeat in the Greco-Turkish War (1919–22), the monarchy was briefly replaced by the Second Hellenic Republic (1924–1935). In 1931, the proclamation of the Second Spanish Republic (1931–1939) resulted in the Spanish Civil War that would be the prelude of World War II.
Republican ideas were spreading, especially in Asia. The United States began to have considerable influence in East Asia in the later part of the 19th century, with Protestant missionaries playing a central role. The liberal and republican writers of the west also exerted influence. These combined with native Confucian inspired political philosophy that had long argued that the populace had the right to reject unjust government that had lost the Mandate of Heaven.
Two short-lived republics were proclaimed in East Asia, the Republic of Formosa and the First Philippine Republic. China had seen considerable anti-Qing sentiment, and a number of protest movements developed calling for constitutional monarchy. The most important leader of these efforts was Sun Yat-sen, whose Three Principles of the People combined American, European, and Chinese ideas. The Republic of China was proclaimed on January 1, 1912.
Decolonization.
In the years following World War II, most of the remaining European colonies gained their independence, and most became republics. The two largest colonial powers were France and the United Kingdom. Republican France encouraged the establishment of republics in its former colonies. the United Kingdom attempted to follow the model it had for its earlier settler colonies of creating independent Commonwealth realms still linked under the same monarchy. While most of the settler colonies and the smaller states of the Caribbean retained this system, it was rejected by the newly independent countries in Africa and Asia, which revised their constitutions and became republics.
Britain followed a different model in the Middle East; it installed local monarchies in several colonies and mandates including Iraq, Jordan, Kuwait, Bahrain, Oman, Yemen and Libya. In subsequent decades revolutions and coups overthrew a number of monarchs and installed republics. Several monarchies remain, and the Middle East is the only part of the world where several large states are ruled by monarchs with almost complete political control.
Socialist republics.
In the wake of the First World War, the Russian monarchy fell during the Russian Revolution. The Russian Provisional Government was established in its place on the lines of a liberal republic, but this was overthrown by the Bolsheviks who went on to establish the Union of Soviet Socialist Republics. This was the first republic established under Marxist-Leninist ideology. Communism was wholly opposed to monarchy, and became an important element of many republican movements during the 20th century. The Russian Revolution spread into Mongolia, and overthrew its theocratic monarchy in 1924. In the aftermath of the Second World War the communists gradually gained control of Romania, Bulgaria, Yugoslavia, Hungary and Albania, ensuring that the states were reestablished as socialist republics rather than monarchies.
Communism also intermingled with other ideologies. It was embraced by many national liberation movements during decolonization. In Vietnam, communist republicans pushed aside the Nguyễn Dynasty, and monarchies in neighbouring Laos and Cambodia were overthrown by communist movements in the 1970s. Arab socialism contributed to a series of revolts and coups that saw the monarchies of Egypt, Iraq, Libya, and Yemen ousted. In Africa Marxist-Leninism and African socialism led to the end of monarchy and the proclamation of republics in states such as Burundi and Ethiopia.
Islamic republics.
Islamic political philosophy has a long history of opposition to absolute monarchy, notably in the work of Al-Farabi. Sharia law took precedence over the will of the ruler, and electing rulers by means of the Shura was an important doctrine. While the early caliphate maintained the principles of an elected ruler, later states became hereditary or military dictatorships though many maintained some pretense of a consultative shura.
None of these states are typically referred to as republics. The current usage of republic in Muslim countries is borrowed from the western meaning, adopted into the language in the late 19th century. The 20th century saw republicanism become an important idea in much of the Middle East, as monarchies were removed in many states of the region. Iraq became a secular state. Some nations, such as Indonesia and Azerbaijan, began as secular. In Iran, the 1979 revolution overthrew the monarchy and created an Islamic republic based on the ideas of Islamic democracy.
Head of state.
Structure.
With no monarch, most modern republics use the title president for the head of state. Originally used to refer to the presiding officer of a committee or governing body in Great Britain the usage was also applied to political leaders, including the leaders of some of the Thirteen Colonies (originally Virginia in 1608); in full, the "President of the Council." The first republic to adopt the title was the United States of America. Keeping its usage as the head of a committee the President of the Continental Congress was the leader of the original congress. When the new constitution was written the title of President of the United States was conferred on the head of the new executive branch.
If the head of state of a republic is also the head of government, this is called a presidential system. There are a number of forms of presidential government. A full-presidential system has a president with substantial authority and a central political role.
In other states the legislature is dominant and the presidential role is almost purely ceremonial and apolitical, such as in Germany and India. These states are parliamentary republics and operate similarly to constitutional monarchies with parliamentary systems where the power of the monarch is also greatly circumscribed. In parliamentary systems the head of government, most often titled prime minister, exercises the most real political power. Semi-presidential systems have a president as an active head of state, but also have a head of government with important powers.
The rules for appointing the president and the leader of the government, in some republics permit the appointment of a president and a prime minister who have opposing political convictions: in France, when the members of the ruling cabinet and the president come from opposing political factions, this situation is called cohabitation.
In some countries, like Switzerland and San Marino, the head of state is not a single person but a committee (council) of several persons holding that office. The Roman Republic had two consuls, elected for a one year-term by the "comitia centuriata", consisting of all adult, freeborn males who could prove citizenship.
Elections.
In liberal democracies presidents are elected, either directly by the people or indirectly by a parliament or council. Typically in presidential and semi-presidential systems the president is directly elected by the people, or is indirectly elected as done in the United States. In that country the president is officially elected by an electoral college, chosen by the States, all of which do so by direct election of the electors. The indirect election of the president through the electoral college conforms to the concept of republic as one with a system of indirect election. In the opinion of some, direct election confers legitimacy upon the president and gives the office much of its political power. However, this concept of legitimacy differs from that expressed in the United States Constitution which established the legitimacy of the United States president as resulting from the signing of the Constitution by nine states. The idea that direct election is required for legitimacy also contradicts the spirit of the Great Compromise, whose actual result was manifest in the clause that provides voters in smaller states with slightly more representation in presidential selection than those in large states.
In states with a parliamentary system the president is usually elected by the parliament. This indirect election subordinates the president to the parliament, and also gives the president limited legitimacy and turns most presidential powers into reserve powers that can only be exercised under rare circumstance. There are exceptions where elected presidents have only ceremonial powers, such as in Ireland.
Ambiguities.
The distinction between a republic and a monarchy is not always clear. The constitutional monarchies of the former British Empire and Western Europe today have almost all real political power vested in the elected representatives, with the monarchs only holding either theoretical powers, no powers or rarely used reserve powers. Real legitimacy for political decisions comes from the elected representatives and is derived from the will of the people. While hereditary monarchies remain in place, political power is derived from the people as in a republic. These states are thus sometimes referred to as crowned republics.
Terms such as "liberal republic" are also used to describe all of the modern liberal democracies.
There are also self-proclaimed republics that act similarly to monarchies with absolute power vested in the leader and passed down from father to son. North Korea and Syria are two notable examples where a son has inherited political control. Neither of these states are officially monarchies. There is no constitutional requirement that power be passed down within one family, but it has occurred in practice.
There are also elective monarchies where ultimate power is vested in a monarch, but the monarch is chosen by some manner of election. A current example of such a state is Malaysia where the Yang di-Pertuan Agong is elected every five years by the Conference of Rulers composed of the nine hereditary rulers of the Malay states, and the Vatican City-State, where the pope is selected by cardinal-electors, currently all cardinals under a specific age. While rare today, elective monarchs were common in the past. The Holy Roman Empire is an important example, where each new emperor was chosen by a group of electors. Islamic states also rarely employed primogeniture, instead relying on various forms of election to choose a monarch's successor.
The Polish–Lithuanian Commonwealth had an elective monarchy, with a wide suffrage of some 500,000 nobles. The system, known as the Golden Liberty, had developed as a method for powerful landowners to control the crown. The proponents of this system looked to classical examples, and the writings of the Italian Renaissance, and called their elective monarchy a "rzeczpospolita", based on "res publica."
Sub-national republics.
In general being a republic also implies sovereignty as for the state to be ruled by the people it cannot be controlled by a foreign power. There are important exceptions to this, for example, republics in the Soviet Union were member states which had to meet three criteria to be named republics:
It is sometimes argued that the former Soviet Union was also a supra-national republic, based on the claim that the member states were different nations.
Socialist Federative Republic of Yugoslavia (and earlier names) was a federal entity composed of six republics (Socialist Republic of Bosnia and Herzegovina, Croatia, Macedonia, Montenegro, Serbia, and Slovenia). Each republic had its parliament, government, institute of citizenship, constitution, etc... but certain functions were delegated to the federation (army, monetary matters). Each republic also had a right of self-determination according to the conclusions of the second session of the AVNOJ and according to the federal constitution.
States of the United States are required, like the federal government, to be republican in form, with final authority resting with the people. This was required because the states were intended to create and enforce most domestic laws, with the exception of areas delegated to the federal government and prohibited to the states. The founding fathers of the country intended most domestic laws to be handled by the states. Requiring the states to be a republic in form was seen as protecting the citizens' rights and preventing a state from becoming a dictatorship or monarchy, and reflected unwillingness on the part of the original 13 states (all independent republics) to unite with other states that were not republics. Additionally, this requirement ensured that only other republics could join the union.
In the example of the United States, the original 13 British colonies became independent states after the American Revolution, each having a republican form of government. These independent states initially formed a loose confederation called the United States and then later formed the current United States by ratifying the current U.S. Constitution, creating a union of sovereign states with the union or federal government also being a republic. Any state joining the union later was also required to be a republic.
Other meanings.
Political philosophy.
The term "republic" originated from the writers of the Renaissance as a descriptive term for states that were not monarchies. These writers, such as Machiavelli, also wrote important prescriptive works describing how such governments should function. These ideas of how a government and society should be structured is the basis for an ideology known as classical republicanism or civic humanism. This ideology is based on the Roman Republic and the city states of Ancient Greece and focuses on ideals such as civic virtue, rule of law, and mixed government.
This understanding of a republic as a distinct form of government from a liberal democracy is one of the main theses of the Cambridge School of historical analysis. This grew out of the work of J. G. A. Pocock who in 1975 argued that a series of scholars had expressed a consistent set of republican ideals. These writers included Machiavelli, Milton, Montesquieu, and the founders of the United States of America.
Pocock argued that this was an ideology with a history and principles distinct from liberalism. These ideas were embraced by a number of different writers, including Quentin Skinner, Philip Pettit and Cass Sunstein. These subsequent writers have further explored the history of the idea, and also outlined how a modern republic should function.
United States.
A distinct set of definitions for the word "republic" evolved in the United States. In common parlance, a republic is a state that does not practice direct democracy but rather has a government indirectly controlled by the people. This understanding of the term was originally developed by James Madison, and notably employed in Federalist Paper No. 10. This meaning was widely adopted early in the history of the United States, including in Noah Webster's dictionary of 1828. It was a novel meaning to the term; representative democracy was not an idea mentioned by Machiavelli and did not exist in the classical republics. Also, there is evidence that contemporaries of Madison considered the meaning of the word to reflect the definition found elsewhere, as is the case with a quotation of Benjamin Franklin taken from the notes of James McHenry where the question is put forth, "a Republic or a Monarchy?"
The term "republic" does not appear in the Declaration of Independence, but does appear in Article IV of the Constitution which "guarantee to every State in this Union a Republican form of Government." What exactly the writers of the constitution felt this should mean is uncertain. The Supreme Court, in "Luther v. Borden" (1849), declared that the definition of "republic" was a "political question" in which it would not intervene. In two later cases, it did establish a basic definition. In "United States v. Cruikshank" (1875), the court ruled that the "equal rights of citizens" were inherent to the idea of a republic.
However, the term "republic" is not synonymous with the republican form. The republican form is defined as one in which the powers of sovereignty are vested in the people and are exercised by the people, either directly, or through representatives chosen by the people, to whom those powers are specially delegated. In re Duncan, 139 U.S. 449, 11 S.Ct. 573, 35 L.Ed. 219; Minor v. Happersett, 88 U.S. (21 Wall.) 162, 22 L.Ed. 627.
Beyond these basic definitions the word "republic" has a number of other connotations. W. Paul Adams observes that "republic" is most often used in the United States as a synonym for state or government, but with more positive connotations than either of those terms. Republicanism is often referred to as the founding ideology of the United States. Traditionally scholars believed this American republicanism was a derivation of the classical liberal ideologies of John Locke and others developed in Europe.
A political philosophy of republicanism that formed during the Renaissance period, and initiated by Machiavelli, was thought to have had little impact on the founders of the United States. In the 1960s and 1970s a revisionist school led by the likes of Bernard Bailyn began to argue that republicanism was just as or even more important than liberalism in the creation of the United States. This issue is still much disputed and scholars like Isaac Kramnick completely reject this view.
Further reading.
• Thomas Corwin, Senate Speech Against the Mexican War-Congressional Globe 1847 

</doc>
<doc id="25538" url="https://en.wikipedia.org/wiki?curid=25538" title="Robyn">
Robyn

Robin Miriam Carlsson (born 12 June 1979), known as Robyn, is a Swedish soprano vocalist. Robyn became known in 1997 for the worldwide dance-pop hit "Do You Know (What It Takes)" from her debut album, "Robyn Is Here". The popularity of her UK number-one "With Every Heartbeat" and her 2005 album, "Robyn", brought her international success. Australian Dance Albums Chart Belgian Albums Chart [61, Canadian Albums Chart European Top 100 Albums [46, Irish Albums Chart Swiss Albums Chart [64, UK Albums Chart US Billboard 200 [50, In June 2010 Robyn released the first album of a trilogy, "Body Talk Pt. 1" (her first album since "Robyn"), which reached number one. Its lead single, "Dancing on My Own", was released several weeks before the album and was nominated for Best Dance Recording at the 2010 Grammy Awards. "Body Talk Pt. 2" was released on 6 September and debuted at number one on the Swedish chart. The trilogy's final album, "Body Talk", was released on 22 November with "Indestructible" its lead single; "Call Your Girlfriend" was the album's second single. In May 2015 Robyn announced a new music project, La Bagatelle Magique.
Career.
2005–08: Konichiwa Records and "Robyn".
The decade-long relationship between Robyn and Jive Records ended in 2004. When the label reacted negatively to "Who's That Girl?s electropop sound, she decided to release music on her own. In early 2005 the singer announced that she would found her own record label, Konichiwa Records.
The second single from the UK release, "With Every Heartbeat", was released in late July 2007 and reached number one on the UK singles chart. Robyn appeared on Jo Whiley's BBC Radio 1 show, "Live Lounge", and her follow-up singles "Handle Me", "Be Mine!", "Who's That Girl?" and "Dream On" were top 30 hits. In Australia, "Robyn" reached the top 10 of the iTunes Store album chart. Robyn completed a short US tour to promote "Robyn", and supported Madonna's Sticky & Sweet Tour at some European dates in 2008.
2009–present: "Body Talk", "Do It Again" and "Love Is Free".
In January 2009, Robyn won a Swedish Grammis Award for Best Live Act 2008. She released the first album of the "Body Talk" trilogy, "Body Talk Pt. 1", on 14 June 2010 in the Nordic countries on EMI and on 15 June in the US on Interscope Records. The album was preceded by the single "Dancing on My Own" on 1 June 2010. It was Robyn's first number-one hit in Sweden and her fourth top-ten hit in the UK and the US, peaking at number eight on the UK Singles Chart and number three on "Billboard"'s Hot Dance Club Songs chart. In July 2010 she sang a minimalist, electro cover version of Alicia Keys' "Try Sleeping with a Broken Heart" in a live performance on iheartradio. Robyn made the All Hearts Tour in July–August 2010 with American singer Kelis to promote the "Body Talk" albums, and a four-date UK tour at the end of October.
On 6 September 2010, "Body Talk Pt. 2" was released in the UK. It was preceded by the lead single, a dance version of "Hang with Me" from "Body Talk Pt. 1", on 5 September. The album includes a duet with American rapper Snoop Dogg, "U Should Know Better". Robyn performed "Dancing on My Own" with deadmau5 at the 2010 MTV Video Music Awards on 12 September. In a BBC "Newsbeat" interview, Robyn explained her decision to release three albums in one year: "It was just something I felt like I needed to do. I just never thought about selling records or not, making this decision. I just did it for myself. It's a way of, for me, to stay inspired and to be able to do the things I like to do". She said she would not do it again: "When you do 16 or 13 songs in one go, you kind of empty yourself, and it takes a while to fill back up and have new things to talk about, so I think it's good for everyone".
Robyn announced the release of the single, "Indestructible", on 13 October 2010; an acoustic version appeared on "Body Talk Pt. 2". The song was released on 17 November in Scandinavia and 22 November in the UK. Co-written by Klas Åhlund, it was described as a "pulsating full power version takes every ounce of that emotion and wraps it up in another exceptional disco-pop record worthy of any dance-floor or passion-laden sing-a-long." Robyn planned to collaborate with Swedish producer Max Martin on the song, "Time Machine"; Martin produced Robyn's US hits "Do You Know (What It Takes)" and "Show Me Love", both of which peaked in the top 10 of the "Billboard" 100 in 1997. Combined, the "Body Talk" albums have sold 91,000 copies in the US.
In 2010 Robyn guest-starred on "War at the Roses", an episode of "Gossip Girl", in which she performed the acoustic version of "Hang With Me". "Dancing on My Own" was also featured at the end of the episode. In November, she said she would return to the studio in January 2011 with enough material to release a new album later that year. Robyn supported Coldplay on their 2012 tour as the opening act in Dallas, Houston, Tampa, Miami, Atlanta, Charlotte, Philadelphia and Washington, D.C. In mid-2013, Robyn appeared with Paul Rudd and Sean Combs in "Go Kindergarten" on the Lonely Island's "The Wack Album".
On 21 and 22 June 2013, Robyn posted two videos of the Snoop Dogg collaboration: "U Should Know Better" and "Behind The Scenes", and a game, Mixory.
That year she received the Stockholm KTH Royal Institute of Technology Great Prize for "artistic contributions and embrace of technology", a prize of 1.2 million Swedish kronor (£117,197), which she planned to donate to a cause of her choice.
In 2014 Robyn announced the Do It Again Tour with Röyksopp and that she would record with the band for a collaborative mini-album, "Do It Again". The album was planned for release on 26 May through Don Triumph, Wall of Sound and Cooking Vinyl. Robyn sings on Neneh Cherry's "Out of the Black" from her album, "Blank Project".
Personal life.
Robyn started dating Olof Inger in 2003, and they were engaged until 2011.
She has since become engaged to videographer Max Vitali, referring to him in a 2013 interview with "Collection of Style" magazine as her fiancé: "We became friends when we made the video for 'Be Mine', and now we work together a lot. He made all the videos for the last album ["Body Talk"]." Robyn has two younger siblings, Jac and Effie. 

</doc>
<doc id="25540" url="https://en.wikipedia.org/wiki?curid=25540" title="Request for Comments">
Request for Comments

A Request for Comments (RFC) is a type of publication from the Internet Engineering Task Force (IETF) and the Internet Society, the principal technical development and standards-setting bodies for the Internet.
An RFC is authored by engineers and computer scientists in the form of a memorandum describing methods, behaviors, research, or innovations applicable to the working of the Internet and Internet-connected systems. It is submitted either for peer review or simply to convey new concepts, information, or (occasionally) engineering humor. The IETF adopts some of the proposals published as RFCs as Internet Standards.
Request for Comments documents were invented by Steve Crocker in 1969 to help record unofficial notes on the development of ARPANET. RFCs have since become official documents of Internet specifications, communications protocols, procedures, and events.
History.
The inception of the RFC format occurred in 1969 as part of the seminal ARPANET project. Today, it is the official publication channel for the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), and — to some extent — the global community of computer network researchers in general.
The authors of the first RFCs typewrote their work and circulated hard copies among the ARPA researchers. Unlike the modern RFCs, many of the early RFCs were actual requests for comments and were titled as such to avoid sounding too declarative and to encourage discussion. The RFC leaves questions open and is written in a less formal style. This less formal style is now typical of Internet Draft documents, the precursor step before being approved as an RFC.
In December 1969, researchers began distributing new RFCs via the newly operational ARPANET. RFC 1, entitled "Host Software", was written by Steve Crocker of the University of California, Los Angeles (UCLA), and published on April 7, 1969. Although written by Steve Crocker, the RFC emerged from an early working group discussion between Steve Crocker, Steve Carr and Jeff Rulifson.
In RFC 3, which first defined the RFC series, Crocker started attributing the RFC series to the Network Working Group. Rather than being a formal committee, it was a loose association of researchers interested in the ARPANET project. In effect, it included anyone who wanted to join the meetings and discussions about the project.
Many of the subsequent RFCs of the 1970s also came from UCLA, because UCLA was one of the first Interface Message Processors (IMPs) on ARPANET.
The Augmentation Research Center (ARC) at Stanford Research Institute, directed by Douglas Engelbart, was another of the four first ARPANET nodes and the source of early RFCs.
The ARC became the first network information center (InterNIC), which was managed by Elizabeth J. Feinler to distribute the RFCs along with other network information. From 1969 until 1998, Jon Postel served as the RFC editor. On his death in 1998, his obituary was published as RFC 2468.
Following the expiration of the original ARPANET contract with the U.S. federal government, the Internet Society, acting on behalf of the IETF, contracted with the Networking Division of the University of Southern California (USC) Information Sciences Institute (ISI) to assume the editorship and publishing responsibilities under the direction of the IAB. 
Sandy Ginoza joined USC/ISI in 1999 to work on RFC editing, and Alice Hagens in 2005.
Bob Braden took over the role of RFC project lead, while Joyce K. Reynolds continued to be part of the team until October 13, 2006.
In July 2007, "streams" of RFCs were defined, so that the editing duties could be divided. IETF documents came from IETF working groups or submissions sponsored by an IETF area director from the Internet Engineering Steering Group. The IAB can publish its own documents. A research stream of documents comes from the Internet Research Task Force (IRTF), and an independent stream from other outside sources.
A new model was proposed in 2008, refined, and published in August 2009, splitting the task into several roles,
including the RFC Series Advisory Group (RSAG). 
(The model was updated in 2012 
The streams were also refined in December 2009, with standards defined for their style.
In January 2010 the RFC editor function was moved to a contractor, Association Management Solutions, with Glenn Kowack serving as interim series editor.
In late 2011, Heather Flanagan was hired as the permanent RFC Series Editor.
Also at that time, an RFC Series Oversight Committee (RSOC) was created.
Production and evolution.
The RFC Editor assigns each RFC a serial number. Once assigned a number and published, an RFC is never rescinded or modified; if the document requires amendments, the authors publish a revised document. Therefore, some RFCs supersede others; the superseded RFCs are said to be "deprecated", "obsolete", or "obsoleted by" the superseding RFC. Together, the serialized RFCs compose a continuous historical record of the evolution of Internet standards and practices. The RFC process is documented in RFC 2026 ("The Internet Standards Process, Revision 3") 
The RFC production process differs from the standardization process of formal standards organizations such as ISO. Internet technology experts may submit an Internet Draft without support from an external institution. Standards-track RFCs are published with approval from the IETF, and are usually produced by experts participating in working groups, which first publish an Internet Draft. This approach facilitates initial rounds of peer review before documents mature into RFCs.
The RFC tradition of pragmatic, experience-driven, after-the-fact standards authorship accomplished by individuals or small working groups can have important advantages over the more formal, committee-driven process typical of ISO and national standards bodies.
Most RFCs use a common set of terms such as "MUST" and "NOT RECOMMENDED" (as defined by RFC 2119), Augmented Backus–Naur Form (ABNF) (RFC 5234) as a meta-language, and simple text-based formatting, in order to keep the RFCs consistent and easy to understand.
Sub-series.
The RFC series contains three sub-series for IETF RFCs:
Streams.
There are four streams of RFCs: (1) IETF, (2) IRTF, (3) IAB, and (4) "independent submission". Only the IETF creates BCPs and RFCs on standards track. An "independent submission" is checked by the IESG for conflicts with IETF work; the quality is assessed by an "independent submission editorial board". In other words, IRTF and "independent " RFCs are supposed to contain relevant info or experiments for the Internet at large not in conflict with IETF work; compare RFC 4846, RFC 5742, and RFC 5744.
Obtaining RFCs.
The official source for RFCs on the World Wide Web is the [//www.rfc-editor.org/rfc.html RFC Editor]. Almost any published RFC can be retrieved via a URL of the form http://www.rfc-editor.org/rfc/rfc5000.txt, shown for RFC 5000.
Every RFC is submitted as plain ASCII text and is published in that form, but may also be available in other formats. However, .
For easy access to the metadata of an RFC, including abstract, keywords, author(s), publication date, errata, status, and especially later updates, the RFC Editor site offers a search form with many features. A redirection sets some efficient parameters, example: codice_1
The official International Standard Serial Number (ISSN) of the RFC series is 2070-1721.
Status.
Not all RFCs are standards. Each RFC is assigned a designation with regard to status within the Internet standardization process. This status is one of the following: "Informational", "Experimental", "Best Current Practice", "Standards Track", or "Historic". 
Each RFC is static; if the document is changed, it is submitted again and assigned a new RFC number. 
"Standards Track".
Standards-track documents are further divided into "Proposed Standard", "Draft Standard", and "Internet Standard" documents. 
Only the IETF, represented by the Internet Engineering Steering Group (IESG), can approve standards-track RFCs.
If an RFC becomes an Internet Standard (STD), it is assigned an STD number but retains its RFC number. The definitive list of Internet Standards is itself an Internet Standard, STD 1: "Internet Official Protocol Standards".
When an Internet Standard is updated, its STD number stays the same, now referring to a new RFC or set of RFCs. A given Internet Standard, STD "n", may be RFCs "x" and "y" at a given time, but later the same standard may be updated to be RFC "z" instead. For example, in 2007 RFC 3700 was an Internet Standard—STD 1—and in May 2008 it was replaced with RFC 5000, so RFC 3700 changed to "Historic", RFC 5000 became an Internet Standard, and STD 1 is RFC 5000. When STD 1 is updated again, it will simply refer to a newer RFC that will have completed the standards track, but it will still be STD 1. 
(Best Current Practices work in a similar fashion; BCP "n" refers to a certain RFC or set of RFCs, but which RFC or RFCs may change over time).
"Informational".
An "informational" RFC can be nearly anything from April 1 jokes to widely recognized essential RFCs like Domain Name System Structure and Delegation (RFC 1591). Some informational RFCs formed the <abbr title="For Your Information">FYI</abbr> sub-series.
"Experimental".
An "experimental" RFC can be an IETF document or an individual submission to the 'RFC Editor'. A draft is designated experimental if it is unclear the proposal will work as intended or unclear if the proposal will be widely adopted. Experimental RFCs may be promoted to standards track if it becomes popular and works well.
"Best Current Practice".
The Best Current Practice subseries collects administrative documents and other texts which are considered as official rules and not only "informational", but which do not affect "over the wire data". The border between standards track and BCP is often unclear. If a document only affects the Internet Standards Process, like BCP 9, or IETF administration, it is clearly a BCP. If it only defines rules and regulations for Internet Assigned Numbers Authority (IANA) registries it is less clear; most of these documents are BCPs, but some are on the standards track.
The BCP series also covers technical recommendations for how to practice Internet standards; for instance the recommendation to use source filtering to make DoS attacks more difficult (RFC 2827: ""Network Ingress Filtering: Defeating Denial of Service Attacks which employ IP Source Address Spoofing"") is [//tools.ietf.org/html/bcp38 BCP 38].
"Historic".
An "historic" RFC is one that the technology defined by the RFC is no longer recommended for use, which differs from "Obsoletes" header in a replacement RFC. For example, RFC 821 (SMTP) itself is obsoleted by various newer RFCs, but SMTP itself is still "current technology", so it is not in "Historic" status. On the other hand, BGP version 4 is universally used for as the only protocol on exchanging routes between autonomous systems, earlier BGP versions (e.g. RFC 1267) are made historic and no longer used.
"Unknown".
Status "unknown" is used for some very old RFCs, where it is unclear which status the document would get if it were published today. Some of these RFCs would not be published at all today; an early RFC was often just that: a simple request for comments, not intended to specify a protocol, administrative procedure, or anything else for which the RFC series is used today.

</doc>
<doc id="25596" url="https://en.wikipedia.org/wiki?curid=25596" title="Ragga">
Ragga

Raggamuffin music, usually abbreviated as ragga, is a subgenre of dancehall music and reggae, in which the instrumentation primarily consists of electronic music. Similar to hip hop, sampling often serves a prominent role in raggamuffin music.
In the mid-1980s, French Antilles Kassav, the first in the Caribbean to use MIDI technology, took Caribbean music to another level by recording in a digital format. Wayne Smith's "Under Mi Sleng Teng" was produced by King Jammy in 1985 on a Casio MT-40 synthesizer and is generally recognized as the seminal ragga song. "Sleng Teng" boosted Jammy's popularity immensely, and other producers quickly released their own versions of the riddim, accompanied by dozens of different vocalists.
Ragga is now mainly used as a synonym for dancehall reggae or for describing dancehall with a deejay chatting rather than singjaying or singing on top of the riddim.
Origins.
Ragga originated in Jamaica during the 1980s, at the same time that electronic dance music's popularity was increasing globally. One of the reasons for ragga's swift propagation is that it is generally easier and less expensive to produce than reggae performed on traditional musical instruments. Ragga evolved first in Jamaica, and later in Europe, North America, and Africa, eventually spreading to Japan, India, and the rest of the world. Ragga heavily influenced early jungle music, and also spawned the syncretistic bhangragga style when fused with bhangra. In the 1990s, ragga and breakcore music fused, creating a style known as raggacore.
The term "raggamuffin" is an intentional misspelling of "ragamuffin", a word that entered the Jamaican Patois lexicon after the British Empire colonized Jamaica in the 17th century. Despite the British colonialists' pejorative application of the term, Jamaican youth appropriated it as an ingroup designation. The term "raggamuffin music" describes the music of Jamaica's "ghetto dwellers".
Ragga and hip hop music.
In the late 1980s, influential Jamaican rapper Daddy Freddy's pioneering efforts in fusing ragga with hip hop music earned him international acclaim while helping to publicize and popularize ragga. In 1987, Daddy Freddy and Asher D's "Ragamuffin Hip-Hop" became the first multinational single to feature the word "ragga" in its title. In 1992, Canadian hip hop group Rascalz released their debut album under the name Ragga Muffin Rascals. As ragga matured, an increasing number of dancehall artists began to appropriate stylistic elements of hip hop music, while ragga music, in turn, influenced more and more hip hop artists, most notably KRS-One, the Boot Camp Clik, Das EFX, Busta Rhymes, as well as some artists with ragga-influenced styles, like early Common, Main Source, Ill Al Scratch, Fu-Schnickens, and Redman. Artists like Mad Lion grew in popularity during this early 90's trend, exemplified by his crossing from reggae to hip-hop culture.
Some ragga artists believe that the assimilation of hip hop sensibilities is crucial to the international marketability of dancehall music. Indeed, the appeal to the contemporary rhythm and blues and hip hop music audiences in the English-speaking world contributed substantially to the multinational commercial success of such dancehall artists as:

</doc>
<doc id="25597" url="https://en.wikipedia.org/wiki?curid=25597" title="Religious conversion">
Religious conversion

Religious conversion is the adoption of a set of beliefs identified with one particular religious denomination to the exclusion of others. Thus "religious conversion" would describe the abandoning of adherence to one denomination and affiliating with another. This might be from one to another denomination within the same religion, for example, Christian Baptist to Methodist or Catholic, Muslim Shi'a to Sunni. In some cases, religious conversion "marks a transformation of religious identity and is symbolized by special rituals".
People convert to a different religion for various reasons, including: active conversion by free choice due to a change in beliefs, secondary conversion, deathbed conversion, conversion for convenience and marital conversion, and forced conversion such as conversion by violence or charity.
Conversion or reaffiliation for convenience is an insincere act, sometimes for relatively trivial reasons such as a parent converting to enable a child to be admitted to a good school associated with a religion, or a person adopting a religion more in keeping with the social class they aspire to. When people marry one spouse may convert to the religion of the other.
Forced conversion is adoption of a different religion under duress. The convert may secretly retain the previous beliefs and continue, covertly, with the practices of the original religion, while outwardly maintaining the forms of the new religion. Over generations a family forced against their will to convert may wholeheartedly adopt the new religion.
"Proselytism" is the act of attempting to convert by persuasion another individual from a different religion or belief system. (See proselyte).
Apostate is a term used by members of a religion or denomination to refer to someone who has left that religion or denomination.
Abrahamic religions.
Judaism.
Procedure.
Jewish law has a number of requirements of potential converts. They should desire conversion to Judaism for its own sake, and for no other motives. A male convert needs to undergo a ritual circumcision conducted according to Jewish law (if already circumcised, a needle is used to draw a symbolic drop of blood while the appropriate blessings are said), and the convert must commit to observe Jewish law. A convert must join the Jewish community and reject any previous religious affiliation. Ritual immersion in a small pool of water known as a "mikvah" is required.
History.
The Greek word "proselyte" means a convert to Judaism. It is known that some Chinese, Khazars, Edomites, and Ethiopians, as well as many Arabs, particularly in Yemen, were converts. As late as the 6th century the Eastern Roman empire and Caliph Umar ibn Khattab were issuing decrees against conversion to Judaism, implying that this was still occurring. The Quran tells hagiographical stories about Mohammad dealing with Jewish Arab tribes ("Banu"), Muhammad expelling them from their home in conquered Medina in 625, and the execution of Jewish prisoners by his own hands.
Christianity.
Within Christianity 'conversion' refers variously to three different phenomena: a person becoming Christian who was previously not Christian; a Christian moving from one Christian denomination to another; a particular spiritual development, sometimes called the second conversion, or the convertion of the baptised.
Conversion to Christianity is the religious conversion of a previously non-Christian person to some form of Christianity. Some Christian sects require full conversion for new members regardless of any history in other Christian sects, or from certain other sects. The exact requirements vary between different churches and denominations. All Christian sects hold that baptism is a necessary ritual, but the practice differs. Jesus Christ was baptized at the beginning of his ministry, and prior to that event, John the Baptist had been baptizing Jewish believers as a sign of repentance, though not of conversion per se. Baptism is usually understood as an outward symbol of an inward change. Prior to that Awakening at Antioch (Acts ch.8 ff; see also Acts 2:38), all converts to Christianity were ethnic and religious Jews. Following the Awakening, Pagans and infidels were required to undergo Christian baptism to be ultimately accepted in the Kingdom of God. Christian Baptism has some parallels with Jewish Immersion by Mikvah.
Converting to Catholicism involves religious education followed by initial participation in the sacraments, which are baptism, confession, penance, and communion. In general, conversion to Christian Faith primarily involves repentance for sin and a decision to live a life that is holy and acceptable to God through faith in the atoning death and resurrection of Jesus Christ. Most other sects require a period of indoctrination prior to acceptance. All of this is essentially done through a voluntary exercise of the will of the individual concerned. True conversion to Christianity is thus a personal, internal matter and cannot be forced.
Christians consider that conversion requires internalization of the new belief system. It implies a new reference point for the convert's self-identity, and is a matter of belief and social structure—of both faith and affiliation. This typically entails the sincere avowal of a new belief system, but may also present itself in other ways, such as adoption into an identity group or spiritual lineage.
Baptism.
Catholics, and Orthodox denominations encourage infant baptism before children are aware of their status. In Roman Catholicism and certain high church forms of Protestantism, baptized children are expected to participate in confirmation classes as pre-teens. In Eastern Orthodoxy, the equivalent of confirmation, chrismation, is administered to all converts, adult and infant alike, immediately after baptism.
Methods of baptism include immersion, sprinkling (aspersion) and pouring (affusion). Baptism received by adults or younger people who have reached the age of accountability where they can make a personal religious decision is referred to as believer's baptism among conservative or evangelical Protestant groups.
It is intended as a public statement of a person's prior decision to become a Christian. Some Christian groups such as Catholics, Churches of Christ, and Christadelphians believe baptism is essential to salvation.
Accepting Christ and renouncing sin.
According to Christianity, a convert renounces sin as worthless and treasures instead the supreme worth of Christ in Jesus' sacrificial death and resurrection.
Christian conversion is a “deeply personal” matter. It entails changes in thinking, priorities and commitments: “a whole new direction in one's life”.
Because conversion is a change in values that embraces God and rejects sin, it includes a personal commitment to a life of holiness as described by Paul of Tarsus and exemplified by Jesus. In some Protestant traditions, this is called "accepting Christ as one's Savior and following him as Lord." In another variation, the 1910 Catholic Dictionary defines "conversion" as "One who turns or changes from a state of sin to repentance, from a lax to a more earnest and serious way of life, from unbelief to faith, from heresy to the true faith." The Eastern Orthodox understanding of conversion is illustrated in the rite of baptism, in which the convert faces west while publicly renouncing and symbolically spitting upon Satan, and then turns to the east to worship Christ "as king and God".
Responsibilities.
In the New Testament, Jesus commanded his disciples in the Great Commission to "go and make disciples of all nations" (, ). Evangelization—sharing the Gospel message or "Good News" in deed and word, is an expectation of Christians.
Reaffiliation.
Transferring from one Christian denomination to another may consist of a relatively simple transfer of membership, especially if moving from one Trinitarian denomination to another, and if the person has received water baptism in the name of the Trinity. If not, then the person may be required to be baptized or rebaptized before acceptance by the new church. Some denominations, such as those in the Anabaptist tradition, require previously baptized Christians to be re-baptized. The Eastern Orthodox Church treats a transfer from another denomination of Christianity to Orthodoxy (conceived of as the one true Church) as a category of conversion and repentance, though re-baptism is not always required.
The process of conversion to Christianity varies somewhat among Christian denominations. Most Protestants believe in conversion by "faith" to attain salvation. According to this understanding, a person professes faith in Jesus Christ as God, their Lord and savior. Repentance for sin and a holy living are expected of those professing faith in Jesus Christ. While an individual may make such a decision privately, usually it entails being baptized and becoming a member of a denomination or church. In these traditions, a person is considered to become a Christian by publicly acknowledging the foundational Christian doctrines that Jesus Christ died, was buried, and was resurrected for the remission of sins.
Comparison between Protestants.
This table summarizes three Protestant beliefs.
Latter Day Saint movement.
Much of the theology of Latter Day Saint baptism was established during the early Latter Day Saint movement founded by Joseph Smith. According to this theology, baptism must be by immersion, for the remission of sins (meaning that through baptism, past sins are forgiven), and occurs after one has shown faith and repentance. Mormon baptism does not purport to remit any sins other than personal ones, as adherents do not believe in original sin. Latter Day Saints baptisms also occur only after an "age of accountability" which is defined as the age of eight years. The theology thus rejects infant baptism.
In addition, Latter Day Saint theology requires that baptism may only be performed with one who has been called and ordained by God with priesthood authority. Because the churches of the Latter Day Saint movement operate under a lay priesthood, children raised in a Mormon family are usually baptized by a father or close male friend or family member who has achieved the office of priest, which is conferred upon worthy male members at least 16 years old in the LDS Church.
Baptism is seen as symbolic both of Jesus' death, burial and resurrection and is also symbolic of the baptized individual putting off of the natural or sinful man and becoming spiritually reborn as a disciple of Jesus.
Membership into a Latter Day Saint church is granted only by baptism whether or not a person has been raised in the church. Latter Day Saint churches do not recognize baptisms of other faiths as valid because they believe baptisms must be performed under the church's unique authority. Thus, all who come into one of the Latter Day Saint faiths as converts are baptized, even if they have previously received baptism in another faith.
When performing a Baptism, Latter Day Saints say the following prayer before performing the ordinance:
Baptisms inside and outside the temples are usually done in a baptistry, although they can be performed in any body of water in which the person may be completely immersed. The person administering the baptism must recite the prayer exactly, and immerse every part, limb, hair and clothing of the person being baptized. If there are any mistakes, or if any part of the person being baptized is not fully immersed, the baptism must be redone. In addition to the baptizer, two priesthood holders witness the baptism to ensure that it is performed properly.
Following baptism, Latter Day Saints receive the Gift of the Holy Ghost by the laying on of hands of a Melchizedek Priesthood holder.
Latter Day Saints hold that one may be baptized after death through the vicarious act of a living individual, and holders of the Melchezidek Priesthood practice baptism for the dead as a missionary ritual. This doctrine answers the question of the righteous non-believer and the unevangelized by providing a post-mortem means of repentance and salvation.
Islam.
There are five pillars, or foundations, of Islam but the primary, and most important is to believe that there is only one God and creator, referred to as Allah (the word for the name of God in Arabic) and that the Islamic prophet, Muhammad, is His final messenger. The time of a person's conversion is counted from the moment they sincerely make this "declaration of faith", called the shahadah.
Islam teaches that everyone is Muslim at birth because every child that is born has a natural inclination to goodness and to worship the one true God alone, but the parents or society can cause them to deviate from the straight path. When someone accepts Islam, they are considered to revert to the original condition. While conversion "to" Islam is among its most supported tenets, conversion "from" Islam to another religion is considered to be the sin of apostasy. There is minority of non believers of Islam who spread rumors about apostasy in Islam that in Islam apostasy leads to death but in Islam it is allowed to change your religion on your own will but it is also true that in some cases apostasy leads to death only when someone coverts themselves from Islam to any other religion and propagates against Islam. The holy book of Qur'an states (Al-Ma'idah 5:32) that "whoever kills a soul unless for a soul or for corruption in the land - it is as if he had slain mankind entirely. And whoever saves one - it is as if he had saved mankind entirely". In Islam, circumcision is a "Sunnah" custom not mentioned in the Qur'an. The majority clerical opinion holds that circumcision is not a condition for entering Islam. The Shafi`i and Hanbali schools regard it as obligatory, while the Maliki and Hanafi schools regard it as only recommended. However, it is not a precondition for the acceptance of a person's Islamic practices, nor does is choice to forgo circumcision considered a sin. It is not one of the Five Pillars of Islam or the Six Fundamentals of Belief.
According to Abul A'la Maududi, people should accept Islam through their own free choice, not through compulsion. Even though Maududi believes in the superiority of Islam, he insists that if persons elect not to embrace Islam, Muslims must avoid placing political or social pressure on them to convert.
Bahá'í Faith.
In sharing their faith with others, Bahá'ís are cautioned to "obtain a hearing" – meaning to make sure the person they are proposing to teach is open to hearing what they have to say. "Bahá'í pioneers", rather than attempting to supplant the cultural underpinnings of the people in their adopted communities, are encouraged to integrate into the society and apply Bahá'í principles in living and working with their neighbors.
Bahá'ís recognize the divine origins of all revealed religion, and believe that these religions occurred sequentially as part of a Divine plan (see Progressive revelation), with each new revelation superseding and fulfilling that of its predecessors. Bahá'ís regard their own faith as the most recent (but not the last), and believe its teachings – which are centered around the principle of the oneness of humanity – are most suited to meeting the needs of a global community.
In most countries conversion is a simple matter of filling out a card stating a declaration of belief. This includes acknowledgement of Bahá'u'llah – the Founder of the Faith – as the Messenger of God for this age, awareness and acceptance of His teachings, and intention to be obedient to the institutions and laws He established.
Conversion to the Bahá'í Faith carries with it an explicit belief in the common foundation of all revealed religion, a commitment to the unity of mankind, and active service to the community at large, especially in areas that will foster unity and concord. Since the Bahá'í Faith has no clergy, converts to this Faith are encouraged to be active in all aspects of community life. Even a recent convert may be elected to serve on a Local Spiritual Assembly – the guiding Bahá'í institution at the community level.
Indian religions.
Hinduism.
Since 1800 CE, religious conversion from and to Hinduism has been a controversial subject within Hinduism. Some have suggested that the concept of missionary conversion, either way, is contrary to the precepts of Hinduism. Religious leaders of some of Hinduism sects such as Brahmo Samaj have seen Hinduism as a non-missionary religion yet welcomed new members, while other leaders of Hinduism's diverse schools have stated that with the arrival of missionary Islam and Christianity in India, this "there is no such thing as proselytism in Hinduism" view must be re-examined.
Hinduism is a diverse system of thought with beliefs spanning monotheism, polytheism, panentheism, pantheism, pandeism, monism, and atheism among others. Hinduism has no traditional ecclesiastical order, no centralized religious authorities, no universally accepted governing body, no prophet(s), no binding holy book nor any mandatory prayer attendance requirements. Hinduism has been described as a way of life. In its diffuse and open structure, numerous schools and sects of Hinduism have developed and spun off in India with help from its ascetic scholars, since the Vedic age. The six Astika and two Nastika schools of Hindu philosophy, in its history, did not develop a missionary or proselytization methodology, and they co-existed with each other. Most Hindu sub-schools and sects do not actively seek converts. Individuals have had a choice to enter, leave or change their god(s), spiritual convictions, accept or discard any rituals and practices, and pursue spiritual knowledge and liberation (moksha) in different ways. However, various schools of Hinduism do have some core common beliefs, such as the belief that all living beings have Atman (soul), a belief in karma theory, spirituality, ahimsa (non-violence) as the greatest dharma or virtue, and others.
Religious conversion to Hinduism has a long history outside India. Merchants and traders of India, particularly from Indian peninsula, carried their religious ideas, which led to religious conversions to Hinduism in Indonesia, Vietnam, Cambodia and Burma. Some sects of Hindus, particularly of the Bhakti schools began seeking or accepting converts in early to mid 20th century. For example, Arya Samaj, Saiva Siddhanta Church, BAPS, and the International Society for Krishna Consciousness accept those who have a desire to follow their sects of Hinduism, and each has their own religious conversion procedure.
In recent decades, mainstream Hinduism schools have attempted to systematize ways to accept religious converts, with an increase in inter-religious mixed marriages. The steps involved in becoming a Hindu have variously included a period where the interested person gets an informal "ardha-Hindu" name and studies ancient literature on spiritual path and practices (English translations of Upanishads, Agamas, Epics, ethics in Sutras, festivals, yoga). If after a period of study, the individual still wants to convert, a "Namakarana Samskaras" ceremony is held, where the individual adopts a traditional Hindu name. The initiation ceremony may also include "Yajna" (i.e., fire ritual with Sanskrit hymns) under guidance of a local Hindu priest. Some of these places are "mathas" and "asramas" (hermitage, monastery), where one or more "gurus" (spiritual guide) conduct the conversion and offer spiritual discussions. Some schools encourage the new convert to learn and participate in community activities such as festivals (Diwali etc.), read and discuss ancient literature, learn and engage in rites of passages (ceremonies of birth, first feeding, first learning day, age of majority, wedding, cremation and others).
Sikhism.
Sikhism is not known to openly proselytize, but accepts converts.
Jainism.
Jainism accepts anyone who wants to embrace the religion. There is no specific ritual for becoming a Jain. One does not need to ask any authorities for admission. One becomes a Jain on one's own by observing the five vows ("vratas") The five main vows as mentioned in the ancient Jain texts like Tattvarthasutra are:
Following the five vows is the main requirement in Jainism. All other aspects such as visiting temples are secondary. Jain monks and nuns are required to observe these five vows strictly.
Buddhism.
Persons newly adhering to Buddhism traditionally "take Refuge" (express faith in the Three Jewels—Buddha, Dharma, and Sangha) before a monk, nun, or similar representative. But cultural or secular Buddhists often hold multiple religious identities, combining the religion with some East Asian religions in different countries and ethnics, such as:
Throughout the timeline of Buddhism, conversions of entire countries and regions to Buddhism were frequent, as Buddhism spread throughout Asia. For example, in the 11th century in Burma, king Anoratha converted his entire country to Theravada Buddhism. At the end of the 12th century, Jayavarman VII set the stage for conversion of the Khmer people to Theravada Buddhism. Mass conversions of areas and communities to Buddhism occur up to the present day, for example, in the Dalit Buddhist movement in India there have been organized mass conversions.
Exceptions to encouraging conversion may occur in some Buddhist movements. In Tibetan Buddhism, for example, the current Dalai Lama discourages active attempts to win converts.
Other religions and sects.
In the second half of the 20th century, the rapid growth of new religious movements (NRMs) led some psychologists and other scholars to propose that these groups were using "brainwashing" or "mind control" techniques to gain converts. This theory was publicized by the popular news media but disputed by other scholars, including some sociologists of religion.
In the 1960s sociologist John Lofland lived with Unification Church missionary Young Oon Kim and a small group of American church members in California and studied their activities in trying to promote their beliefs and win converts to their church. Lofland noted that most of their efforts were ineffective and that most of the people who joined did so because of personal relationships with other members, often family relationships. Lofland published his findings in 1964 as a doctoral thesis entitled "The World Savers: A Field Study of Cult Processes", and in 1966 in book form by Prentice-Hall as "". It is considered to be one of the most important and widely cited studies of the process of religious conversion, and one of the first modern sociological studies of a new religious movement.
The Church of Scientology attempts to gain converts by offering "free stress tests". It has also used the celebrity status of some of its members (most famously the American actor Tom Cruise) to attract converts. The Church of Scientology requires that all converts sign a legal waiver which covers their relationship with the Church of Scientology before engaging in Scientology services.
Research in the United States and the Netherlands has shown a positive correlation between areas lacking mainstream churches and the percentage of people who are a member of a new religious movement. This applies also for the presence of New Age centres.
On the other end of the scale are religions that do not accept any converts, or do so very rarely. Often these are relatively small, close-knit minority religions that are ethnically based such as the Yazidis, Druze, and Mandaeans. Zoroastrianism classically does not accept converts, but this issue has become controversial in the 20th century due to the rapid decline in membership. Chinese traditional religion lacks clear criteria for membership, and hence for conversion. The Shakers and some Indian eunuch brotherhoods do not allow procreation, so that every member is a convert.
International law.
The United Nations Universal Declaration of Human Rights defines religious conversion as a human right: "Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief" (Article 18). Despite this UN-declared human right, some groups forbid or restrict religious conversion (see below).
Based on the declaration the United Nations Commission on Human Rights (UNCHR) drafted the International Covenant on Civil and Political Rights, a legally binding treaty. It states that "Everyone shall have the right to freedom of thought, conscience and religion. This right shall include freedom to have or to adopt a religion or belief of his choice" (Article 18.1). "No one shall be subject to coercion which would impair his freedom to have or to adopt a religion or belief of his choice" (Article 18.2).
The UNCHR issued a General Comment on this Article in 1993: "The Committee observes that the freedom to 'have or to adopt' a religion or belief necessarily entails the freedom to choose a religion or belief, "including the right to replace one's current religion or belief with another" or to adopt atheistic views [...] Article 18.2 bars coercion that would impair the right to have or adopt a religion or belief, including the use of threat of physical force or penal sanctions to compel believers or non-believers to adhere to their religious beliefs and congregations, to recant their religion or belief "or to convert"." (CCPR/C/21/Rev.1/Add.4, General Comment No. 22.; emphasis added)
Some countries distinguish voluntary, motivated conversion from organized proselytism, attempting to restrict the latter. The boundary between them is not easily defined: what one person considers legitimate evangelizing, or witness-bearing, another may consider intrusive and improper. Illustrating the problems that can arise from such subjective viewpoints is this extract from an article by Dr. C. Davis, published in Cleveland State University's "Journal of Law and Health": "According to the Union of American Hebrew Congregations, Jews for Jesus and Hebrew Christians constitute two of the most dangerous cults, and its members are appropriate candidates for deprogramming. Anti-cult evangelicals ... protest that 'aggressiveness and proselytizing ... are basic to authentic Christianity,' and that Jews for Jesus and Campus Crusade for Christ are not to be labeled as cults. Furthermore, certain Hassidic groups who physically attacked a meeting of the Hebrew Christian 'cult' have themselves been labeled a 'cult' and equated with the followers of Reverend Moon, by none other than the President of the Central Conference of American Rabbis."
Since the collapse of the former Soviet Union the Russian Orthodox Church has enjoyed a revival. However, it takes exception to what it considers illegitimate proselytizing by the Roman Catholic Church, the Salvation Army, Jehovah's Witnesses, and other religious movements in what it refers to as its "canonical territory".
Greece has a long history of conflict, mostly with Jehovah's Witnesses, but also with some Pentecostals, over its laws on proselytism. This situation stems from a law passed in the 1930s by the dictator Ioannis Metaxas. A Jehovah's Witness, Minos Kokkinakis, won the equivalent of $14,400 in damages from the Greek state after being arrested for trying to preach his faith from door to door. In another case, "Larissis v. Greece", a member of the Pentecostal church also won a case in the European Court of Human Rights.
Some Islamic countries with Islamic law outlaw and carry strict sentences for proselytizing. Several Islamic countries under Islamic law—Saudi Arabia, Yemen, Afghanistan, Pakistan, Egypt, Iran, and Maldives—outlaw apostasy and carry imprisonment or the death penalty for those leaving Islam and those enticing Muslims to leave Islam. Also, induced religious conversions in the Indian states Orissa has resulted in communal riots.

</doc>
<doc id="25599" url="https://en.wikipedia.org/wiki?curid=25599" title="Rubidium">
Rubidium

Rubidium is a chemical element with symbol Rb and atomic number 37. Rubidium is a soft, silvery-white metallic element of the alkali metal group, with an atomic mass of 85.4678. Elemental rubidium is highly reactive, with properties similar to those of other alkali metals, such as very rapid oxidation in air. Natural rubidium is a mix of two isotopes: 85Rb, the only stable one, constitutes 72% of it. The remaining 28% is the slightly radioactive 87Rb with a half-life of 49 billion years—more than three times longer than the estimated age of the universe.
German chemists Robert Bunsen and Gustav Kirchhoff discovered rubidium in 1861 by the newly developed method of flame spectroscopy.
Rubidium's compounds have various chemical and electronic applications. Rubidium metal is easily vaporized and has a convenient spectral absorption range, making it a frequent target for laser manipulation of atoms.
Rubidium is not known to be necessary for any living organisms. However, rubidium ions are handled by living organisms in a manner similar to potassium ions, being actively taken up by plants and by animal cells due to their identical charge.
Characteristics.
Rubidium is a very soft, ductile, silvery-white metal. It is the second most electropositive of the non-radioactive alkali metals and melts at a temperature of . Similar to other alkali metals, rubidium metal reacts violently with water, forms amalgams with mercury and alloys with gold, iron, caesium, sodium, and potassium, but not lithium (even though rubidium and lithium are in the same group). As with potassium (which is slightly less reactive) and caesium (which is slightly more reactive), rubidium's reaction with water is usually vigorous enough to ignite the hydrogen gas it liberates. Rubidium has also been reported to ignite spontaneously in air. Rubidium has a very low ionization energy of only 406 kJ/mol. Rubidium and potassium show a very similar purple color in the flame test, which makes spectroscopy methods necessary to distinguish the two elements.+, this cation, once formed, is very stable, and is normally unreactive toward further oxidative or reductive chemical reactions.-->
Compounds.
Rubidium chloride (RbCl) is probably the most used rubidium compound; it is used in biochemistry to induce cells to take up DNA (which is not a unique feature, several chlorides are used in this manner) and as a biomarker since it is readily taken up to replace potassium, and occurs in only small quantities in living organisms. Other common rubidium compounds are the corrosive rubidium hydroxide (RbOH), the starting material for most rubidium-based chemical processes; rubidium carbonate (Rb2CO3), which is used in some optical glasses, and rubidium copper sulfate, Rb2SO4·CuSO4·6H2O. Rubidium silver iodide (RbAg4I5) has the highest room temperature conductivity of any known ionic crystal, a property that is being exploited in thin film batteries and other applications.
Rubidium has a number of oxides, including rubidium monoxide (Rb2O), Rb6O and Rb9O2, which form if rubidium metal is exposed to air; rubidium in excess oxygen gives the superoxide RbO2. Rubidium forms salts with halides, making rubidium fluoride, rubidium chloride, rubidium bromide, and rubidium iodide.
Isotopes.
Although rubidium is monoisotopic, naturally occurring rubidium is composed of two isotopes: the stable 85Rb (72.2%) and the radioactive 87Rb (27.8%). Natural rubidium is radioactive with specific activity of about 670 Bq/g, enough to significantly expose a photographic film in 110 days. Aside from 85Rb and 87Rb, another 24 synthetically produced isotopes of rubidium are known, with half-lives of under 3 months; most of these are highly radioactive and have few uses.
Rubidium-87 has a half-life of  years, which is more than three times the age of the universe of  years, making it a primordial nuclide. It readily substitutes for potassium in minerals, and is therefore fairly widespread. Rb has been used extensively in dating rocks; 87Rb beta decays to stable 87Sr. During fractional crystallization, Sr tends to become concentrated in plagioclase, leaving Rb in the liquid phase. Hence, the Rb/Sr ratio in residual magma may increase over time, resulting in rocks with elevated Rb/Sr ratios due to progressing differentiation. The highest ratios (10 or more) occur in pegmatites. If the initial amount of Sr is known or can be extrapolated, then the age can be determined by measurement of the Rb and Sr concentrations and of the 87Sr/86Sr ratio. The dates indicate the true age of the minerals only if the rocks have not been subsequently altered (see rubidium-strontium dating).
Rubidium-82, one of the element's non-natural isotopes, is produced by electron-capture decay of strontium-82 with a half-life of 25.36 days. The subsequent decay of rubidium-82 with a half-life of 76 seconds to stable krypton-82 happens by positron emission.
Occurrence.
Rubidium is the twenty-third most abundant element in the Earth's crust, roughly as abundant as zinc and rather more common than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, and zinnwaldite, which contain up to 1% of its oxide. Lepidolite contains between 0.3% and 3.5% rubidium, and is the commercial source of the element. Some potassium minerals and potassium chlorides also contain the element in commercially significant amounts.
Seawater contains an average of 125 µg/L of rubidium compared to the much higher value for potassium of 408 mg/L and the much lower value of 0.3 µg/L for caesium.
Because of its large ionic radius, rubidium is one of the "incompatible elements." During magma crystallization, rubidium is concentrated together with its heavier analogue caesium in the liquid phase and crystallizes last. Therefore, the largest deposits of rubidium and caesium are zone pegmatite ore bodies formed by this enrichment process. Because rubidium substitutes for potassium in the crystallization of magma, the enrichment is far less effective than in the case of caesium. Zone pegmatite ore bodies containing mineable quantities of caesium as pollucite or the lithium minerals lepidolite are also a source for rubidium as a by-product.
Two notable sources of rubidium are the rich deposits of pollucite at Bernic Lake, Manitoba, Canada, and the rubicline ((Rb,K)AlSi3O8) found as impurities in pollucite on the Italian island of Elba, with a rubidium content of 17.5%. Both of these deposits are also sources of caesium.
Production.
Although rubidium is more abundant in Earth's crust than caesium, the limited applications and the lack of a mineral rich in rubidium limits the production of rubidium compounds to 2 to 4 tonnes per year. Several methods are available for separating potassium, rubidium, and caesium. The fractional crystallization of a rubidium and caesium alum (Cs,Rb)Al(SO4)2·12H2O yields after 30 subsequent steps pure rubidium alum. Two other methods are reported, the chlorostannate process and the ferrocyanide process.
For several years in the 1950s and 1960s, a by-product of potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium, with the rest being potassium and a small fraction of caesium. Today the largest producers of caesium, such as the Tanco Mine, Manitoba, Canada, produce rubidium as a by-product from pollucite.
History.
Rubidium was discovered in 1861 by Robert Bunsen and Gustav Kirchhoff, in Heidelberg, Germany, in the mineral lepidolite through the use of a spectroscope. Because of the bright red lines in its emission spectrum, they chose a name derived from the Latin word "rubidus", meaning "deep red".
Rubidium is present as a minor component in lepidolite. Kirchhoff and Bunsen processed 150 kg of a lepidolite containing only 0.24% rubidium oxide (Rb2O). Both potassium and rubidium form insoluble salts with chloroplatinic acid, but these salts show a slight difference in solubility in hot water. Therefore, the less-soluble rubidium hexachloroplatinate (Rb2PtCl6) could be obtained by fractional crystallization. After reduction of the hexachloroplatinate with hydrogen, this process yielded 0.51 grams of rubidium chloride for further studies. The first large scale isolation of caesium and rubidium compounds, performed from 44,000 liters of mineral water by Bunsen and Kirchhoff, yielded, besides 7.3 grams of caesium chloride, also 9.2 grams of rubidium chloride. Rubidium was the second element, shortly after caesium, to be discovered spectroscopically, only one year after the invention of the spectroscope by Bunsen and Kirchhoff.
The two scientists used the rubidium chloride thus obtained to estimate the atomic weight of the new element as 85.36 (the currently accepted value is 85.47). They tried to generate elemental rubidium by electrolysis of molten rubidium chloride, but instead of a metal, they obtained a blue homogeneous substance which "neither under the naked eye nor under the microscope showed the slightest trace of metallic substance." They assigned it as a subchloride (); however, the product was probably a colloidal mixture of the metal and rubidium chloride. In a second attempt to produce metallic rubidium, Bunsen was able to reduce rubidium by heating charred rubidium tartrate. Although the distilled rubidium was pyrophoric, it was possible to determine the density and the melting point of rubidium. The quality of the research done in the 1860s can be appraised by the fact that their determined density differs less than 0.1 g/cm3 and the melting point by less than 1 °C from the presently accepted values.
The slight radioactivity of rubidium was discovered in 1908 but before the theory of isotopes was established in the 1910s and the low activity due to the long half-life of above 1010 years made interpretation complicated. The now proven decay of 87Rb to stable 87Sr through beta decay was still under discussion in the late 1940s.
Rubidium had minimal industrial value before the 1920s. Since then, the most important use of rubidium has been in research and development, primarily in chemical and electronic applications. In 1995, rubidium-87 was used to produce a Bose–Einstein condensate, for which the discoverers, Eric Allin Cornell, Carl Edwin Wieman and Wolfgang Ketterle, won the 2001 Nobel Prize in Physics.
Applications.
Rubidium compounds are sometimes used in fireworks to give them a purple color. Rubidium has also been considered for use in a thermoelectric generator using the magnetohydrodynamic principle, where rubidium ions are formed by heat at high temperature and passed through a magnetic field. These conduct electricity and act like an armature of a generator thereby generating an electric current. Rubidium, particularly vaporized 87Rb, is one of the most commonly used atomic species employed for laser cooling and Bose–Einstein condensation. Its desirable features for this application include the ready availability of inexpensive diode laser light at the relevant wavelength, and the moderate temperatures required to obtain substantial vapor pressures.
Rubidium has been used for polarizing 3He, producing volumes of magnetized 3He gas, with the nuclear spins aligned toward a particular direction in space, rather than randomly.
Rubidium vapor is optically pumped by a laser and the polarized Rb polarizes 3He through the hyperfine interaction.
Such spin-polarized 3He cells are becoming popular for neutron polarization measurements and for producing polarized neutron beams for other purposes.
The resonant element in atomic clocks utilizes the hyperfine structure of rubidium's energy levels, making rubidium useful for high-precision timing, and is used as the main component of secondary frequency references (rubidium oscillators) to maintain frequency accuracy in cell site transmitters and other electronic transmitting, networking, and test equipment. These rubidium standards are often used with GPS to produce a "primary frequency standard" that has greater accuracy and is less expensive than caesium standards. Such rubidium standards are often mass-produced for the telecommunication industry.
Other potential or current uses of rubidium include a working fluid in vapor turbines, as a getter in vacuum tubes, and as a photocell component. Rubidium is also used as an ingredient in special types of glass, in the production of superoxide by burning in oxygen, in the study of potassium ion channels in biology, and as the vapor to make atomic magnetometers. In particular, 87Rb is currently being used, with other alkali metals, in the development of spin-exchange relaxation-free (SERF) magnetometers.
Rubidium-82 is used for positron emission tomography. Rubidium is very similar to potassium and therefore tissue with high potassium content will also accumulate the radioactive rubidium. One of the main uses is in myocardial perfusion imaging. The very short half-life of 76 seconds makes it necessary to produce the rubidium-82 from decay of strontium-82 close to the patient. As a result of changes in the blood brain barrier in brain tumors, rubidium collects more in brain tumors than normal brain tissue, allowing the use of radioisotope rubidium-82 in nuclear medicine to locate and image brain tumors.
Rubidium was tested for the influence on manic depression and depression. Dialysis patients suffering from depression show a depletion in rubidium and therefore a supplementation may help during depression. In some tests the rubidium was administered as rubidium chloride with up to 720 mg per day for 60 days.
Precautions and biological effects.
Rubidium reacts violently with water and can cause fires. To ensure safety and purity, this metal is usually kept under a dry mineral oil or sealed in glass ampoules in an inert atmosphere. Rubidium forms peroxides on exposure even to small amount of air diffusing into oil, and is thus subject to similar peroxide precautions as storage of metallic potassium.
Rubidium, like sodium and potassium, almost always has +1 oxidation state when dissolved in water, including its presence in all biological systems. The human body tends to treat Rb+ ions as if they were potassium ions, and therefore concentrates rubidium in the body's intracellular fluid (i.e., inside cells). The ions are not particularly toxic; a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons. The biological half-life of rubidium in humans was measured as 31–46 days. Although a partial substitution of potassium by rubidium is possible, rats with more than 50% of their potassium substituted in the muscle tissue died.

</doc>
<doc id="25600" url="https://en.wikipedia.org/wiki?curid=25600" title="Ruthenium">
Ruthenium

Ruthenium is a chemical element with symbol Ru and atomic number 44. It is a rare transition metal belonging to the platinum group of the periodic table. Like the other metals of the platinum group, ruthenium is inert to most other chemicals. The Baltic German scientist Karl Ernst Claus discovered the element in 1844, and named it after his homeland, the Russian Empire (one of Russia's Latin names is Ruthenia). Ruthenium usually occurs as a minor component of platinum ores; annual production is about 20 tonnes. Most ruthenium produced is used for wear-resistant electrical contacts and the production of thick-film resistors. A minor application of ruthenium is its use in some platinum alloys, and as a catalyst.
Characteristics.
Physical properties.
A polyvalent hard white metal, ruthenium is a member of the platinum group and is in group 8 of the periodic table:
However, it has an atypical configuration in its outermost electron shells: whereas all other group 8 elements have 2 electrons in the outermost shell, in ruthenium, one of those is transferred to a lower shell. This effect can be observed in the neighboring metals niobium (41), rhodium (45), and palladium (46).
Ruthenium has four crystal modifications and does not tarnish unless subject to high temperatures. Ruthenium dissolves in fused alkalis, is not attacked by acids but is attacked by halogens at high temperatures. Small amounts of ruthenium can increase the hardness of platinum and palladium. The corrosion resistance of titanium is increased markedly by the addition of a small amount of ruthenium. The metal can be plated either by electroplating or by thermal decomposition methods. A ruthenium-molybdenum alloy is known to be superconductive at temperatures below 10.6 K.
Isotopes.
Naturally occurring ruthenium is composed of seven stable isotopes. Additionally, 34 radioactive isotopes have been discovered. Of these radioisotopes, the most stable are 106Ru with a half-life of 373.59 days, 103Ru with a half-life of 39.26 days and 97Ru with a half-life of 2.9 days.
Fifteen other radioisotopes have been characterized with atomic weights ranging from 89.93 u (90Ru) to 114.928 u (115Ru). Most of these have half-lives that are less than five minutes except 95Ru (half-life: 1.643 hours) and 105Ru (half-life: 4.44 hours).
The primary decay mode before the most abundant isotope, 102Ru, is electron capture and the primary mode after is beta emission. The primary decay product before 102Ru is technetium and the primary decay product after is rhodium.
Occurrence.
Ruthenium is exceedingly rare, only the 74th most abundant element in Earth's crust. This element is generally found in ores with the other platinum group metals in the Ural Mountains and in North and South America. Small but commercially important quantities are also found in pentlandite extracted from Sudbury, Ontario, Canada, and in pyroxenite deposits in South Africa. The native form of ruthenium is a very rare mineral (Ir replaces part of Ru in its structure).
Production.
Mining.
Roughly 12 tonnes of ruthenium is mined each year with world reserves estimated as 5,000 tonnes. The composition of the mined platinum group metal (PGM) mixtures varies in a wide range depending on the geochemical formation. For example, the PGMs mined in South Africa contain on average 11% ruthenium while the PGMs mined in the former USSR contain only 2% based on research dating from 1992.
Ruthenium, like the other platinum group metals, is obtained commercially as a by-product from nickel and copper mining and processing as well as by the processing of platinum group metal ores. During electrorefining of copper and nickel, noble metals such as silver, gold and the platinum group metals settle to the bottom of the cell as "anode mud", which forms the starting point for their extraction. To separate the metals, they must first be brought into solution. Several methods are available depending on the separation process and the composition of the mixture; two representative methods are fusion with sodium peroxide followed by dissolution in aqua regia, and dissolution in a mixture of chlorine with hydrochloric acid. Osmium, ruthenium, rhodium and iridium can be separated from platinum and gold and base metals by their insolubility in aqua regia, leaving a solid residue. Rhodium can be separated from the residue by treatment with molten sodium bisulfate. The insoluble residue, containing Ru, Os and Ir is treated with sodium oxide, in which Ir is insoluble, producing water-soluble Ru and Os salts. After oxidation to the volatile oxides, is separated from by precipitation of (NH4)3RuCl6 with ammonium chloride or by distillation or extraction with organic solvents of the volatile osmium tetroxide. Hydrogen is used to reduce ammonium ruthenium chloride yielding a powder. The first method to precipitate the ruthenium with ammonium chloride is similar to the procedure that Smithson Tennant and William Hyde Wollaston used for their separation. Several methods are suitable for industrial scale production. In either case, the product is reduced using hydrogen, yielding the metal as a powder or sponge metal that can be treated using powder metallurgy techniques or by argon-arc welding.
From used nuclear fuels.
Fission products of uranium-235 contain significant amounts of ruthenium and the lighter platinum group metals and therefore used nuclear fuel might be a possible source of ruthenium. The complicated extraction is expensive and the radioactive isotopes of ruthenium that are present would make storage for several half-lives of the decaying isotopes necessary. This makes this source of ruthenium unattractive and no large-scale extraction has been started.
Chemical compounds.
The oxidation states of ruthenium range from 0 to +8, and −2. The properties of ruthenium and osmium compounds are often similar. The +2, +3, and +4 states are the most common. The most prevalent precursor is ruthenium trichloride, a red solid that is poorly defined chemically but versatile synthetically.
Oxides.
Ruthenium can be oxidized to ruthenium(IV) oxide (RuO2, oxidation state +4) which can in turn be oxidized by sodium metaperiodate to ruthenium tetroxide, RuO4, a strong oxidizing agent with structure and properties analogous to osmium tetroxide. Like osmium tetroxide, ruthenium tetroxide is a potent fixative and stain for electron microscopy of organic materials, and is mostly used to reveal the structure of polymer samples. Dipotassium ruthenate (K2RuO4, +6), and potassium perruthenate (KRuO4, +7) are also known.
Coordination and organometallic complexes.
Ruthenium forms a variety of coordination complexes. Examples are the many pentammine derivatives [Ru(NH3)5L]n+ which often exist in both Ru(II) and Ru(III). Derivatives of bipyridine and terpyridine are numerous, best known being the luminescent tris(bipyridine)ruthenium(II) chloride.
Ruthenium forms a wide range compounds with carbon-ruthenium bonds. Ruthenocene is analogous to ferrocene structurally, but exhibits distinctive redox properties. A large number of complexes of carbon monoxide are known, the parent being triruthenium dodecacarbonyl. The analogue of iron pentacarbonyl, ruthenium pentacarbonyl is unstable at ambient conditions. Ruthenium trichloride carbonylates (reacts with carbon monoxide) to give mono- and diruthenium(II) carbonyls from which many derivatives have been prepared such as RuHCl(CO)(PPh3)3 and Ru(CO)2(PPh3)3 (Roper's complex). Heating solutions of ruthenium trichloride in alcohols with triphenylphosphine gives tris(triphenylphosphine)ruthenium dichloride (RuCl2(PPh3)3), which converts to the hydride complex chlorohydridotris(triphenylphosphine)ruthenium(II) (RuHCl(PPh3)3).
In the area of fine chemical synthesis, Grubbs' catalyst is used for alkene metathesis.
Ruthenides.
Metal ruthenides (Ru2−) are very rare, but are commonly found in superconductor applications, especially with regard to lanthanide metals such as cerium ruthenide (CeRu2).
History.
Though naturally occurring platinum alloys containing all six platinum-group metals were used for a long time by pre-Columbian Americans and known as a material to European chemists from the mid-16th century, it took until the mid-18th century for platinum to be identified as a pure element. The discovery that natural platinum contained palladium, rhodium, osmium and iridium occurred in the first decade of the 19th century. Platinum in alluvial sands of Russian rivers gave access to raw material for use in plates and medals and for the minting of ruble coins, starting in 1828. Residues of platinum production for minting were available in the Russian Empire, and therefore most of the research on them was done in Eastern Europe.
It is possible that the Polish chemist Jędrzej Śniadecki isolated element 44 (which he called "vestium" after the asteroid Vesta discovered shortly before) from South American platinum ores in 1807. He published an announcement of his discovery in 1808. His work was never confirmed, however, and he later withdrew his claim of discovery.
Jöns Berzelius and Gottfried Osann nearly discovered ruthenium in 1827. They examined residues that were left after dissolving crude platinum from the Ural Mountains in aqua regia. Berzelius did not find any unusual metals, but Osann thought he found three new metals, which he called pluranium, ruthenium and polinium. This discrepancy led to a long-standing controversy between Berzelius and Osann about the composition of the residues. As Osann was not able to repeat his isolation of ruthenium, he eventually relinquished his claims. The name "ruthenium" was chosen by Osann because the analysed samples stemmed from the Ural Mountains in Russia. The name itself derives from Ruthenia, the Latin word for Rus', a historical area that included present-day western Russia, Ukraine, Belarus, and parts of Slovakia and Poland.
In 1844, the Russian scientist of Baltic German descent Karl Ernst Claus showed that the compounds prepared by Gottfried Osann contained small amounts of ruthenium, which Claus had discovered the same year. Claus isolated ruthenium from the platinum residues of the rouble production while he was working in Kazan University, Kazan. Claus showed that ruthenium oxide contained a new metal and obtained 6 grams of ruthenium from the part of crude platinum that is insoluble in aqua regia. Choosing the name for the new element, Claus stated: "I named the new body, in honour of my Motherland, ruthenium. I had every right to call it by this name because Mr. Osann relinquished his ruthenium and the word does not yet exist in chemistry."
Applications.
Because of its ability to harden platinum and palladium, ruthenium is used in platinum and palladium alloys to make wear-resistant electrical contacts. In this application, only thin-plated films are used to achieve the necessary wear-resistance. Because of its lower cost and similar properties compared to rhodium, the use as plating material for electric contacts is one of the major applications. The thin coatings are either applied by electroplating or sputtering.
Ruthenium dioxide and lead and bismuth ruthenates are used in thick-film chip resistors. These two electronic applications account for 50% of the ruthenium consumption.
Only a few ruthenium alloys are used other than those with other platinum group metals. Ruthenium is often used in small quantities in those alloys to improve some of their properties. The beneficial effect on the corrosion resistance of titanium alloys led to the development of a special alloy containing 0.1% ruthenium. Ruthenium is also used in some advanced high-temperature single-crystal superalloys, with applications including the turbine blades in jet engines. Several nickel based superalloy compositions are described in the literature. Among them are EPM-102 (with 3% Ru) and TMS-162 (with 6% Ru), as well as TMS-138 and TMS-174. both containing 6% rhenium. Fountain pen nibs are frequently tipped with alloys containing ruthenium. From 1944 onward, the famous Parker 51 fountain pen was fitted with the "RU" nib, a 14K gold nib tipped with 96.2% ruthenium and 3.8% iridium.
Ruthenium is a component of mixed-metal oxide (MMO) anodes used for cathodic protection of underground and submerged structures, and for electrolytic cells for chemical processes such as generating chlorine from salt water. The fluorescence of some ruthenium complexes is quenched by oxygen, which has led to their use as optode sensors for oxygen. Ruthenium red, [(NH3)5Ru-O-Ru(NH3)4-O-Ru(NH3)5]6+, is a biological stain used to stain polyanionic molecules such as pectin and nucleic acids for light microscopy and electron microscopy. The beta-decaying isotope 106 of ruthenium is used in radiotherapy of eye tumors, mainly malignant melanomas of the uvea. Ruthenium-centered complexes are being researched for possible anticancer properties. Compared with platinum complexes, those of ruthenium show greater resistance to hydrolysis and more selective action on tumors. NAMI-A and KP1019 are two drugs undergoing clinical evaluation against metastatic tumors and colon cancers.
Ruthenium tetroxide is used to expose latent fingerprints by turning to the brown/black ruthenium dioxide when in contact with fatty oils or fats contained in sebaceous contaminants of the print.
Catalysis.
Ruthenium is a versatile catalyst. Hydrogen sulfide can be split by light by using an aqueous suspension of CdS particles loaded with ruthenium dioxide. This may be useful in the removal of H2S in oil refineries and other industrial processing facilities. Organometallic ruthenium carbene and alkylidene complexes have been found to be highly efficient catalysts for olefin metathesis, a process with important applications in organic and pharmaceutical chemistry. Ruthenium-promoted cobalt catalysts are used in Fischer-Tropsch synthesis.
Solar energy conversion.
Some ruthenium complexes absorb light throughout the visible spectrum and are being actively researched in various, potential, solar energy technologies. For example, Ruthenium-based compounds have been used for light absorption in dye-sensitized solar cells, a promising new low-cost solar cell system.
Data storage.
Chemical vapor deposition of ruthenium is used as a method to produce thin films of pure ruthenium on substrates. These films show promising properties for the use in microchips and for the giant magnetoresistive read element for hard disk drives. Ruthenium was also suggested as a possible material for microelectronics because its use is compatible with semiconductor processing techniques.
Exotic materials.
Many ruthenium-based oxides show very unusual properties, such as a quantum critical point behavior, exotic superconductivity, and high-temperature ferromagnetism.
4) is highly volatile, as is ruthenium trioxide (RuO3). By oxidizing ruthenium (for example with an oxygen plasma) into the volatile oxides, ruthenium can be easily patterned. The properties of the common ruthenium oxides make ruthenium a metal compatible with the semiconductor processing techniques needed to manufacture microelectronics.
To continue miniaturization of microelectronics, new materials are needed as dimensions change. There are three main applications for thin ruthenium films in microelectronics. The first is using thin films of ruthenium as electrodes on both sides of tantalum pentoxide (Ta2O5) or barium strontium titanate ((Ba, Sr)TiO3, also known as BST) in the next generation of three-dimensional dynamic random access memories (DRAMs). Ruthenium thin-film electrodes could also be deposited on top of lead zirconate titanate (Pb(ZrxTi1-x)O3, also known as PZT) in another kind of RAM, ferroelectric random access memory (FRAM). Platinum has been used as the electrodes in RAMs in laboratory settings, but it is difficult to pattern. Ruthenium is chemically similar to platinum, preserving the function of the RAMs, but in contrast to Pt patterns easily. The second is using thin ruthenium films as metal gates in p-doped metal-oxide-semiconductor field effect transistors (p-MOSFETs). When replacing silicide gates with metal gates in MOSFETs, a key property of the metal is its work function. The work function needs to match the surrounding materials. For p-MOSFETs, the ruthenium work function is the best materials property match with surrounding materials such as HfO2, HfSiOx, HfNOx, and HfSiNOx, to achieve the desired electrical properties. The third large-scale application for ruthenium films is as a combination adhesion promoter and electroplating seed layer between TaN and Cu in the copper dual damascene process. Copper can be directly electroplated onto ruthenium, in contrast to tantalum nitride. Copper also adheres poorly to TaN, but well to Ru. By depositing a layer of ruthenium on the TaN barrier layer, copper adhesion would be improved and deposition of a copper seed layer would not be necessary.
There are also other suggested uses. In 1990, IBM scientists discovered that a thin layer of ruthenium atoms created a strong anti-parallel coupling between adjacent ferromagnetic layers, stronger than any other nonmagnetic spacer-layer element. Such a ruthenium layer was used in the first giant magnetoresistive read element for hard disk drives. In 2001, IBM announced a three-atom-thick layer of the element ruthenium, informally referred to as "pixie dust", which would allow a quadrupling of the data density of current hard disk drive media.
Cultural references.
Ruthenium plays an important role in the novel "Arctic Drift" by Clive Cussler. Ruthenium is found to be a catalyst in a very important new industrial process, but very little of the metal is known to remain on Earth, and the search for a new deposit in Canada is a vital part of the plot.

</doc>
<doc id="25601" url="https://en.wikipedia.org/wiki?curid=25601" title="Rhodium">
Rhodium

Rhodium is a chemical element with symbol Rh and atomic number 45. It is a rare, silvery-white, hard, and chemically inert transition metal. It is a member of the platinum group. It has only one naturally occurring isotope, 103Rh. Naturally occurring rhodium is usually found as the free metal, alloyed with similar metals, and rarely as a chemical compound in minerals such as bowieite and rhodplumsite. It is one of the rarest and most valuable precious metals.
Rhodium is a noble metal, resistant to corrosion, found in platinum- or nickel ores together with the other members of the platinum group metals. It was discovered in 1803 by William Hyde Wollaston in one such ore, and named for the rose color of one of its chlorine compounds, produced after it reacted with the powerful acid mixture aqua regia.
The element's major use (approximately 80% of world rhodium production) is as one of the catalysts in the three-way catalytic converters in automobiles. Because rhodium metal is inert against corrosion and most aggressive chemicals, and because of its rarity, rhodium is usually alloyed with platinum or palladium and applied in high-temperature and corrosion-resistive coatings. White gold is often plated with a thin rhodium layer to improve its appearance while sterling silver is often rhodium-plated for tarnish resistance.
Rhodium detectors are used in nuclear reactors to measure the neutron flux level.
History.
Rhodium (Greek "rhodon" (ῥόδον) meaning "rose") was discovered in 1803 by William Hyde Wollaston, soon after his discovery of palladium. He used crude platinum ore presumably obtained from South America. His procedure involved dissolving the ore in aqua regia and neutralizing the acid with sodium hydroxide (NaOH). He then precipitated the platinum as ammonium chloroplatinate by adding ammonium chloride (). Most other metals like copper, lead, palladium and rhodium were precipitated with zinc. Diluted nitric acid dissolved all but palladium and rhodium, which were dissolved in aqua regia, and the rhodium was precipitated by the addition of sodium chloride as . After being washed with ethanol, the rose-red precipitate was reacted with zinc, which displaced the rhodium in the ionic compound and thereby released the rhodium as free metal.
After the discovery, the rare element had only minor applications; for example, by the turn of the century, rhodium-containing thermocouples were used to measure temperatures up to 1800 °C. The first major application was electroplating for decorative uses and as corrosion-resistant coating. The introduction of the three-way catalytic converter by Volvo in 1976 increased the demand for rhodium. The previous catalytic converters used platinum or palladium, while the three-way catalytic converter used rhodium to reduce the amount of NOx in the exhaust.
Characteristics.
Rhodium is a hard, silvery, durable metal that has a high reflectance. Rhodium metal does not normally form an oxide, even when heated. Oxygen is absorbed from the atmosphere only at the melting point of rhodium, but is released on solidification. Rhodium has both a higher melting point and lower density than platinum. It is not attacked by most acids: it is completely insoluble in nitric acid and dissolves slightly in aqua regia.
Chemical properties.
Rhodium belongs to group 9 of the periodic table but has an atypical configuration in its outermost electron shells compared to the rest of the members. This can also be observed in the neighborhood of niobium (41), ruthenium (44), and palladium (46).
The common oxidation state of rhodium is +3, but oxidation states from +0 to +6 are also observed.
Unlike ruthenium and osmium, rhodium forms no volatile oxygen compounds. The known stable oxides include , , , , and . Halogen compounds are known in nearly the full range of possible oxidation states. Rhodium(III) chloride, rhodium(IV) fluoride, rhodium(V) fluoride and rhodium(VI) fluoride are some examples. The lower oxidation states are only stable if ligands are present.
The best-known rhodium-halogen compound is the Wilkinson's catalyst chlorotris(triphenylphosphine)rhodium(I). This catalyst is used, for example, in the hydroformylation or hydrogenation of alkenes.
Isotopes.
Naturally occurring rhodium is composed of only one isotope, 103Rh. The most stable radioisotopes are 101Rh with a half-life of 3.3 years, 102Rh with a half-life of 207 days, 102mRh with a half-life of 2.9 years, and 99Rh with a half-life of 16.1 days. Twenty other radioisotopes have been characterized with atomic weights ranging from 92.926 u (93Rh) to 116.925 u (117Rh). Most of these have half-lives shorter than an hour, except 100Rh (half-life: 20.8 hours) and 105Rh (half-life: 35.36 hours). There are also numerous meta states, the most stable being 102mRh (0.141 MeV) with a half-life of about 2.9 years and 101mRh (0.157 MeV) with a half-life of 4.34 days (see isotopes of rhodium).
The primary decay mode before the only stable isotope, 103Rh, is electron capture and the primary mode after is beta emission. The primary decay product before 103Rh is ruthenium and the primary product after is palladium.
Occurrence.
Rhodium is one of the rarest elements in the Earth's crust, of which it comprises an estimated 0.0002 parts per million (2 × 10−10). Its rarity affects its price, and thus its usage in commercial applications.
Mining and price.
The industrial extraction of rhodium is complex, as the metal occurs in ores mixed with other metals such as palladium, silver, platinum, and gold. It is found in platinum ores and extracted as a white inert metal which is very difficult to fuse. Principal sources are located in South Africa; in river sands of the Ural Mountains; and in North America, including the copper-nickel sulfide mining area of the Sudbury, Ontario, region. Although the quantity at Sudbury is very small, the large amount of processed nickel ore makes rhodium recovery cost-effective.
The main exporter of rhodium is South Africa (approximately 80% in 2010) followed by Russia. The annual world production of this element is 30 tonnes and there are very few rhodium-bearing minerals. The price of rhodium is historically highly variable. In 2007, rhodium cost approximately eight times more than gold, 450 times more than silver, and 27,250 times more than copper by weight. In 2008, the price briefly rose above $10,000 per ounce ($350,000 per kilogram). The economic slowdown of the 3rd quarter of 2008 pushed rhodium prices sharply back below $1,000 per ounce ($35,000 per kilogram); they rebounded to $2,750 by early 2010 ($97,000 per kilogram) (over twice the gold price), but in late 2013, the prices were a bit lower than $1000.
Political and financial problems led to very low oil prices due to oversupply, causing most metals to also become much cheaper. The economies of China, India and other emerging countries slowed down in 2014 and 2015. In 2014 alone 23,722,890 motor vehicles were produced in China, excluding motorbikes. This resulted in a rhodium price of 740.00 US-$ per Troy ounce (31.1 grams) in late November 2015.
Used nuclear fuels.
Rhodium is a fission product of uranium-235; therefore, each kilogram of fission products contains significant amounts of the lighter platinum group metals including rhodium. Used nuclear fuel might be a possible source for rhodium. However, the extraction is complex and expensive, and the also present radioactive isotopes of rhodium would require a storage for several half-lives of the longest-lived decaying isotope (i.e. about 10 years) to reduce the radioactivity. This makes this source of rhodium unattractive and no large-scale extraction has been attempted.
Applications.
The primary use of this element is in automobiles as a catalytic converter, which changes harmful unburned hydrocarbons, carbon monoxide, and nitrogen oxide emissions from the engine into less noxious gases. Of 30,000 kg of rhodium consumed worldwide in 2012, some 24,300 kg (81%) went into and 8,060 kg recovered from this application. About 964 kg of rhodium was used in the glass industry, mostly for production of fiberglass and flat-panel glass, and 2,520 kg in the chemical industry.
Catalyst.
In 2012, 81% of the world production of rhodium was consumed to produce three-way catalytic converters. Rhodium shows some advantages over the other platinum metals in the reduction of nitrogen oxides to nitrogen and oxygen:
Rhodium-based catalysts are used in a number of industrial processes; notably, in the automobile catalytic converters and for catalytic carbonylation of methanol to produce acetic acid by the Monsanto process. It is also used to catalyze addition of hydrosilanes to molecular double bonds, a process important in manufacture of certain silicone rubbers. Rhodium catalysts are also used to reduce benzene to cyclohexane.
The complex of a rhodium ion with BINAP gives a widely used chiral catalyst for chiral synthesis, as in the synthesis of menthol.
Ornamental uses.
Rhodium finds use in jewelry and for decorations. It is electroplated on white gold and platinum to give it a reflective white surface at time of sale, after which the thin layer wears away with use. This is known as rhodium flashing in the jewelry business. It may also be used in coating sterling silver to protect against tarnish, which is silver sulfide (Ag2S) produced from the atmospheric hydrogen sulfide (H2S). Solid (pure) rhodium jewelry is very rare because the metal has both a high melting point and poor malleability, making such jewelry very hard to fabricate — rather than due to its high price. Additionally, its typically higher cost assures that most of its jewelry usage is in the form of tiny amounts of powder, commonly called rhodium sponge, dissolved into electroplating solutions.
Rhodium has also been used for honors or to signify elite status, when more commonly used metals such as silver, gold or platinum were deemed insufficient. In 1979 the "Guinness Book of World Records" gave Paul McCartney a rhodium-plated disc for being history's all-time best-selling songwriter and recording artist.
Other uses.
Rhodium is used as an alloying agent for hardening and improving the corrosion resistance of platinum and palladium. These alloys are used in furnace windings, bushings for glass fiber production, thermocouple elements, electrodes for aircraft spark plugs, and laboratory crucibles. Other uses include:
Precautions.
Being a noble metal, pure rhodium is inert. However, chemical complexes of rhodium can be reactive. Median lethal dose (LD50) for rats is 198 mg of rhodium chloride () per kilogram of body weight. Like the other noble metals, all of which are too inert to occur as chemical compounds in nature, rhodium has not been found to play any biological role. If used in elemental form rather than as compounds, the metal is harmless.
People can be exposed to rhodium in the workplace by inhalation. The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for rhodium exposure in the workplace as 0.1 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m3 over an 8-hour workday. At levels of 100 mg/m3, rhodium is immediately dangerous to life and health. For soluble compounds, the PEL and REL are both 0.001 mg/m3.

</doc>
<doc id="25602" url="https://en.wikipedia.org/wiki?curid=25602" title="Radium">
Radium

Radium is a chemical element with symbol Ra and atomic number 88. It is the sixth element in group 2 of the periodic table, also known as the alkaline earth metals. Pure radium is almost colorless, but it readily combines with nitrogen (rather than oxygen) on exposure to air, forming a black surface layer of radium nitride (Ra3N2). All isotopes of radium are highly radioactive, with the most stable isotope being radium-226, which has a half-life of 1600 years and decays into radon gas (specifically the isotope radon-222). When radium decays, ionizing radiation is a product, which can excite fluorescent chemicals and cause radioluminescence.
Radium, in the form of radium chloride, was discovered by Marie and Pierre Curie in 1898. They extracted the radium compound from uraninite and published the discovery at the French Academy of Sciences five days later. Radium was isolated in its metallic state by Marie Curie and André-Louis Debierne through the electrolysis of radium chloride in 1910.
In nature, radium is found in uranium and (to a lesser extent) thorium ores in trace amounts as small as a seventh of a gram per ton of uraninite. Radium is not necessary for living organisms, and adverse health effects are likely when it is incorporated into biochemical processes because of its radioactivity and chemical reactivity. Currently, other than its use in nuclear medicine, radium has no commercial applications; formerly, it was used as a radioactive source for radioluminescent devices and also in radioactive quackery for its supposed curative powers. Today, these former applications are no longer in vogue because radium's toxicity has since become known, and less dangerous isotopes are used instead in radioluminescent devices.
Characteristics.
Radium is the heaviest known alkaline earth metal and is the only radioactive member of its group. Its physical and chemical properties most closely resemble its lighter congener barium.
Physical.
Pure radium is a volatile silvery-white metal. Its color rapidly vanishes in air, yielding a black layer of radium nitride (Ra3N2). Its melting point is either or , and its boiling point is . Both of these values are slightly lower than those of barium, confirming periodic trends down the group 2 elements. Like barium, radium crystallizes in the body-centered cubic structure at standard temperature and pressure: the radium–radium bond distance is 514.8 picometers. Radium has a density of 5.5 g/cm3, higher than that of barium, again confirming periodic trends; the radium-barium density ratio is comparable to the radium-barium atomic mass ratio, due to the two elements' similar crystal structures.
Chemical.
Radium, like barium, is a highly reactive metal and always exhibits its group oxidation state of +2. It forms the colorless Ra2+ cation in aqueous solution, which is highly basic and does not form complexes readily. Most radium compounds are therefore simple ionic compounds, though participation from the 6s and 6p electrons (in addition to the valence 7s electrons) is expected due to relativistic effects and would enhance the covalent character of radium compounds such as RaF2 and RaAt2. Solid radium compounds are white as radium ions provide no specific coloring, but they gradually turn yellow and then dark over time due to self-radiolysis from radium's alpha decay. Insoluble radium compounds coprecipitate with all barium, most strontium, and most lead compounds.
Isotopes.
Radium has 33 known isotopes, with mass numbers from 202 to 234: all of them are radioactive. Four of these – 223Ra (half-life 11.4 days), 224Ra (3.64 days), 226Ra (1600 years), and 228Ra (5.75 years) – occur naturally in the decay chains of primordial thorium-232, uranium-235, and uranium-238 (223Ra from uranium-235, 226Ra from uranium-238, and the other two from thorium-232). These isotopes nevertheless still have half-lives too short to be primordial radionuclides and only exist in nature from these decay chains. Together with the artificial 225Ra (15 d), these are the five most stable isotopes of radium. All other known radium isotopes have half-lives under two hours, and the majority have half-lives under a minute. At least 12 nuclear isomers have been reported; the most stable of them is radium-205m, with a half-life of between 130 and 230 milliseconds, which is still shorter than thirty-four ground-state radium isotopes.
In the early history of the study of radioactivity, the different natural isotopes of radium were given different names. In this scheme, 223Ra was named actinium X (AcX), 224Ra thorium X (ThX), 226Ra radium (Ra), and 228Ra mesothorium 1 (MsTh1). When it was realized that all of these are isotopes of radium, many of these names fell out of use, and "radium" came to refer to all isotopes, not just 226Ra. Some of radium-226's decay products received historical names including "radium", ranging from radium A to radium G.
226Ra is the most stable isotope of radium and is the last isotope in the (4"n" + 2) decay chain of uranium-238 with a half-life of over a millennium. Its immediate decay product is the dense radioactive noble gas radon, which is responsible for much of the danger of environmental radium. It is 2.7 million times more radioactive than the same molar amount of natural uranium (mostly uranium-238), due to its proportionally shorter half-life.
A sample of radium metal maintains itself at a higher temperature than its surroundings because of the radiation it emits – alpha particles, beta particles, and gamma rays. More specifically, natural radium (which is mostly 226Ra) emits mostly alpha particles, but other steps in its decay chain (the uranium or radium series) emit alpha or beta particles, and almost all particle emissions are accompanied by gamma rays.
History.
Discovery.
Radium was discovered by Marie Sklodowska-Curie and her husband Pierre Curie on 21 December 1898, in a uraninite sample. While studying the mineral earlier, the Curies removed uranium from it and found that the remaining material was still radioactive. They separated out an element similar to bismuth from pitchblende in July 1898, that turned out to be polonium. They then separated out a radioactive mixture consisting mostly of two components: compounds of barium, which gave a brilliant green flame color, and unknown radioactive compounds which gave carmine spectral lines that had never been documented before. The Curies found the radioactive compounds to be very similar to the barium compounds, except that they were more insoluble. This made it possible for the Curies to separate out the radioactive compounds and discover a new element in them. The Curies announced their discovery to the French Academy of Sciences on 26 December 1898. The naming of radium dates to about 1899, from the French word "radium", formed in Modern Latin from "radius" ("ray"): this was in recognition of radium's power of emitting energy in the form of rays.
Subsequent developments.
In 1910, radium was isolated as a pure metal by Marie Curie and André-Louis Debierne through the electrolysis of a pure radium chloride (RaCl2) solution using a mercury cathode, producing a radium–mercury amalgam. This amalgam was then heated in an atmosphere of hydrogen gas to remove the mercury, leaving pure radium metal. The same year, E. Eoler isolated radium by thermal decomposition of its azide, Ra(N3)2. Radium metal was first industrially produced in the beginning of the 20th century by Biraco, a subsidiary company of Union Minière du Haut Katanga (UMHK) in its Olen plant in Belgium.
The common historical unit for radioactivity, the curie, is based on the radioactivity of 226Ra.
Occurrence.
All isotopes of radium have half-lives much shorter than the age of the Earth, so that any primordial radium would have decayed long ago. Radium nevertheless still occurs in the environment, as the isotopes 223Ra, 224Ra, 226Ra, and 228Ra are part of the decay chains of natural thorium and uranium isotopes. Of these four isotopes, the most long-lived is 226Ra (half-life 1600 years), a decay product of natural uranium. Because of its relative longevity, 226Ra is the most common isotope of the element. Thus, radium is found in tiny quantities in the uranium ore uraninite and various other uranium minerals, and in even tinier quantities in thorium minerals. One ton of pitchblende typically yields about one seventh of a gram of radium. One kilogram of the Earth's crust contains about 900 picograms of radium, and one liter of sea water contains about 89 femtograms of radium.
Extraction.
In the first extraction of radium Curie used the residues after extraction of uranium from pitchblende. The uranium had been extracted by dissolution in sulphuric acid leaving radium sulphate, which is similar to barium sulphate but even less soluble in the residues. The residues also contained rather substantial amounts of barium sulphate which thus acted as a carrier for the radium sulphate. The first steps of the radium extraction process involved boiling with sodium hydroxide followed by hydrochloric acid treatment to remove as much as possible of other compounds. The remaining residue was then treated with sodium carbonate to convert the barium sulphate into barium carbonate carrying the radium, thus making it soluble in hydrochloric acid. After dissolution the barium and radium are reprecipitated as sulphates and this was repeated one or few times, for further purification of the mixed sulphate. Some impurities, that form insoluble sulphides, were removed by treating the chloride solution with hydrogen sulphide followed by filtering. When the mixed sulphate were pure enough they were once more converted to mixed chloride and barium and radium were separated by fractional crystallisation while monitoring the progress using a spectroscope (radium gives characteristic red lines in contrast to the green barium lines), and the electroscope. A similar process was still used for industrial radium extraction in 1940, but mixed bromides were then used for the fractionation. If the barium content of the uranium ore is not high enough it is easy to add some to carry the radium. These processes were applied to high grade uranium ores but may not work well with low grade ores.
Production.
Uranium had no large scale application in the late 19th century and therefore no large uranium mines existed. In the beginning the only larger source for uranium ore was the silver mines in Joachimsthal, Austria-Hungary (now Jáchymov, Czech Republic). The uranium ore was only a by-product of the mining activities. After the isolation of radium by Marie and Pierre Curie from uranium ore from Joachimsthal several scientists started to isolate radium in small quantities. Later small companies purchased mine tailings from Joachimsthal mines and started isolating radium. In 1904 the Austrian government nationalised the mines and stopped exporting raw ore. For some time the radium availability was low.
The formation of an Austrian monopoly and the strong urge of other countries to have access to radium led to a worldwide search for uranium ores. The United States took over as leading producer in the early 1910s. The Carnotite sands in Colorado provide some of the element, but richer ores are found in the Congo and the area of the Great Bear Lake and the Great Slave Lake of northwestern Canada. Neither of the deposits is mined for radium but the uranium content makes mining profitable.
The amounts of radium produced were and are always relatively small; for example, in 1918, 13.6 g of radium were produced in the United States. In 1954, the total worldwide supply of purified radium amounted to about 5 pounds (2.3 kg), and it is still in this range today, while the annual production of pure radium compounds is only about 100 g in total today. The chief radium-producing countries are Belgium, Canada, the Czech Republic, Slovakia, the United Kingdom, and the post-Soviet states.
Compounds.
Radium oxide (RaO) has not been characterized, despite oxides being common compounds for the other alkaline earth metals. Radium hydroxide (Ra(OH)2) is the most readily soluble among the alkaline earth hydroxides and is a stronger base than its barium congener, barium hydroxide. It is also more soluble than actinium hydroxide and thorium hydroxide: these three adjacent hydroxides may be separated by precipitating them with ammonia.
Radium chloride (RaCl2) is a colorless, luminous compound. It becomes yellow after some time due to self-damage by the alpha radiation given off by radium when it decays. Small amounts of barium impurities give the compound a rose color. It is soluble in water, though less so than barium chloride, and its solubility decreases with increasing concentration of hydrochloric acid. Crystallization from aqueous solution gives the dihydrate RaCl2·2H2O, isomorphous with its barium analog.
Radium bromide (RaBr2) is also a colorless, luminous compound. In water, it is more soluble than radium chloride. Like radium chloride, crystallization from aqueous solution gives the dihydrate RaBr2·2H2O, isomorphous with its barium analog. The ionizing radiation emitted by radium bromide excites nitrogen molecules in the air, making it glow. The alpha particles of radium quickly gain two electrons to become neutral helium, with builds up inside and weakens radium bromide crystals. This effect sometimes causes the crystals to break or even explode.
Radium nitrate (Ra(NO3)2) is a white compound that can be made by dissolving radium carbonate in nitric acid. As the concentration of nitric acid increases, the solubility of radium nitrate decreases, an important property for the chemical purification of radium.
Radium forms much the same insoluble salts as its lighter congener barium: it forms the insoluble sulfate (RaSO4, the most insoluble known sulfate), chromate (RaCrO4), carbonate (RaCO3), iodate (Ra(IO3)2), tetrafluoroberyllate (RaBeF4), and nitrate (Ra(NO3)2). With the exception of the carbonate, all of these are less soluble in water than the corresponding barium salts. Additionally, radium phosphate, oxalate, and sulfite are probably also insoluble, as they coprecipitate with the corresponding insoluble barium salts. The great insolubility of radium sulfate (at 20 °C, only 2.1 mg will dissolve in 1 kg of water) means that it is one of the less biologically dangerous radium compounds.
Applications.
Some of the few practical uses of radium are derived from its radioactive properties. More recently discovered radioisotopes, such as cobalt-60 and caesium-137, are replacing radium in even these limited uses because several of these isotopes are more powerful emitters, safer to handle, and available in more concentrated form.
Historical.
Luminescent paint.
Radium was formerly used in self-luminous paints for watches, nuclear panels, aircraft switches, clocks, and instrument dials. A typical self-luminous watch that uses radium paint contains around 1 microgram of radium. In the mid-1920s, a lawsuit was filed against the United States Radium Corporation by five dying "Radium Girl" dial painters who had painted radium-based luminous paint on the dials of watches and clocks. The dial painters routinely licked their brushes to give them a fine point, thereby ingesting radium. Their exposure to radium caused serious health effects which included sores, anemia, and bone cancer. This is because radium is treated as calcium by the body, and deposited in the bones, where radioactivity degrades marrow and can mutate bone cells.
During the litigation, it was determined that the company's scientists and management had taken considerable precautions to protect themselves from the effects of radiation, yet had not seen fit to protect their employees. Additionally, for several years the companies had attempted to cover up the effects and avoid liability by insisting that the Radium Girls were instead suffering from syphilis. This complete disregard for employee welfare had a significant impact on the formulation of occupational disease labor law.
As a result of the lawsuit, the adverse effects of radioactivity became widely known, and radium-dial painters were instructed in proper safety precautions and provided with protective gear. In particular, dial painters no longer licked paint brushes to shape them (which caused some ingestion of radium salts). Radium was still used in dials as late as the 1960s, but there were no further injuries to dial painters. This highlighted that the harm to the Radium Girls could easily have been avoided.
From the 1960s the use of radium paint was discontinued. In many cases luminous dials were implemented with non-radioactive fluorescent materials excited by light; such devices glow in the dark after exposure to light, but the glow fades. Where long-lasting self-luminosity in darkness was required, safer radioactive promethium-147 (half-life 2.6 years) or tritium (half-life 12 years) paint was used; both continue to be used today. These had the added advantage of not degrading the phosphor over time, unlike radium. Tritium emits very low-energy beta radiation (even lower-energy than the beta radiation emitted by promethium) which cannot penetrate the skin, rather than the penetrating gamma radiation of radium and is regarded as safer.
Clocks, watches, and instruments dating from the first half of the 20th century, often in military applications, may have been painted with radioactive luminous paint. They are usually no longer luminous; however, this is not due to radioactive decay of the radium (which has a half-life of 1600 years) but to the fluorescence of the zinc sulfide fluorescent medium being worn out by the radiation from the radium. The appearance of an often thick layer of green or yellowish brown paint in devices from this period suggests a radioactive hazard. The radiation dose from an intact device is relatively low and usually not an acute risk; but the paint is dangerous if released and inhaled or ingested.
Commercial use.
Radium was once an additive in products such as toothpaste, hair creams, and even food items due to its supposed curative powers. Such products soon fell out of vogue and were prohibited by authorities in many countries after it was discovered they could have serious adverse health effects. (See, for instance, "Radithor" or "Revigator" types of "Radium water" or "Standard Radium Solution for Drinking".) Spas featuring radium-rich water are still occasionally touted as beneficial, such as those in Misasa, Tottori, Japan. In the U.S., nasal radium irradiation was also administered to children to prevent middle-ear problems or enlarged tonsils from the late 1940s through the early 1970s.
Medical use.
Radium (usually in the form of radium chloride or radium bromide) was used in medicine to produce radon gas which in turn was used as a cancer treatment; for example, several of these radon sources were used in Canada in the 1920s and 1930s. However, many treatments that were used in the early 1900s are not used anymore because of the harmful effects radium bromide exposure caused. Some examples are anaemia, cancer, and genetic mutations.
Howard Atwood Kelly, one of the founding physicians of Johns Hopkins Hospital, was a major pioneer in the medical use of radium to treat cancer. His first patient was his own aunt in 1904, who died shortly after surgery. Kelly was known to use excessive amounts of radium to treat various cancers and tumors. As a result, some of his patients died from high amounts of radium exposure. His method of radium application was inserting a radium capsule near the affected area then sewing the radium "points" directly to the tumor. This was the same method used to treat Henrietta Lacks, the host of the original HeLa cells, for cervical cancer. Currently, safer and more available radioisotopes are usually used instead.
Current.
The isotope 223Ra (under the trade name Xofigo) was approved by the United States Food and Drug Administration in 2013 for use in medicine as a cancer treatment of bone metastasis. The main indication of treatment with Xofigo is the therapy of bony metastases from castration-resistant prostate cancer due to the favourable characteristics of this alpha-emitter radiopharmaceutical.
Radium is still used today as a radiation source in some industrial radiography devices to check for flawed metallic parts, similarly to X-ray imaging. When mixed with beryllium, radium acts as a neutron source. Radium-beryllium neutron sources are still sometimes used even today, but other materials such as polonium are now more common: about 1500 polonium-beryllium neutron sources, with an individual activity of , have been used annually in Russia.
Precautions.
Radium is highly radioactive and its immediate daughter, radon gas, is also radioactive. When ingested, 80% of the ingested radium leaves the body through the feces, while the other 20% goes into the bloodstream, mostly accumulating in the bones. Exposure to radium, internal or external, can cause cancer and other disorders, because radium and radon emit alpha and gamma rays upon their decay, which kill and mutate cells. At the time of the Manhattan Project in 1944, the "tolerance dose" for workers was set at 0.1 microgram of ingested radium.
Some of the biological effects of radium were apparent from the start. The first case of so-called "radium-dermatitis" was reported in 1900, only 2 years after the element's discovery. The French physicist Antoine Becquerel carried a small ampoule of radium in his waistcoat pocket for 6 hours and reported that his skin became ulcerated. Pierre and Marie Curie were so intrigued by radiation that they sacrificed their own health to learn more about it. Pierre Curie attached a tube filled with radium to his arm for ten hours. Oddly enough, he was delighted to see that a skin lesion had appeared. This was a major breakthrough for science. It posed a question: if radium was capable of destroying healthy tissue, could it also attack cancerous tissue? Handling of radium has been blamed for Marie Curie's death due to aplastic anemia. However, most of radium's danger comes from its daughter radon: being a gas, it can enter the body far more readily than can its parent radium.

</doc>
<doc id="25603" url="https://en.wikipedia.org/wiki?curid=25603" title="Rhenium">
Rhenium

Rhenium is a chemical element with symbol Re and atomic number 75. It is a silvery-white, heavy, third-row transition metal in group 7 of the periodic table. With an "estimated" average concentration of 1 part per billion (ppb), rhenium is one of the rarest elements in the Earth's crust. The free element has the third-highest melting point and highest boiling point of any element, at . Rhenium resembles manganese and technetium chemically and is mainly obtained as a by-product of the extraction and refinement of molybdenum and copper ores. Rhenium shows in its compounds a wide variety of oxidation states ranging from −1 to +7.
Discovered in 1925, rhenium was the last stable element to be discovered. It was named after the river Rhine in Europe.
Nickel-based superalloys of rhenium are used in the combustion chambers, turbine blades, and exhaust nozzles of jet engines. These alloys contain up to 6% rhenium, making jet engine construction the largest single use for the element, with the chemical industry's catalytic uses being next-most important. Because of the low availability relative to demand, rhenium is among the most expensive of metals, with an average price of approximately US$2,750 per kilogram (US$85.53 per troy ounce) ; it is also of critical strategic military importance, for its use in high performance military jet and rocket engines.
History.
Rhenium ( meaning: "Rhine") was the last-discovered of the elements that have a stable isotope (other new elements discovered in nature since then, such as francium, are radioactive). The existence of a yet-undiscovered element at this position in the periodic table had been first predicted by Dmitri Mendeleev. Other calculated information was obtained by Henry Moseley in 1914. It is generally considered to have been discovered by Walter Noddack, Ida Tacke, and Otto Berg in Germany. In 1925 they reported that they had detected the element in platinum ore and in the mineral columbite. They also found rhenium in gadolinite and molybdenite. In 1928 they were able to extract 1 g of the element by processing 660 kg of molybdenite. It was estimated in 1968 that 75% of the rhenium metal in the United States was used for research and the development of refractory metal alloys. It took several years from that point before the superalloys became widely used.
In 1908, Japanese chemist Masataka Ogawa announced that he had discovered the 43rd element and named it "nipponium" (Np) after Japan ("Nippon" in Japanese). However, later analysis indicated the presence of rhenium (element 75), not element 43. The symbol Np was later used for the element neptunium.
Characteristics.
Rhenium is a silvery-white metal with one of the highest melting points of all elements, exceeded by only tungsten and carbon. It also has the highest boiling point of all elements. It is also one of the densest, exceeded only by platinum, iridium and osmium. Rhenium has a hexagonal close-packed crystal structure, with lattice parameters "a" = 276.1 pm and "c" = 445.6 pm.
Its usual commercial form is a powder, but this element can be consolidated by pressing and sintering in a vacuum or hydrogen atmosphere. This procedure yields a compact solid having a density above 90% of the density of the metal. When annealed this metal is very ductile and can be bent, coiled, or rolled. Rhenium-molybdenum alloys are superconductive at 10 K; tungsten-rhenium alloys are also superconductive around 4–8 K, depending on the alloy. Rhenium metal superconducts at 1.697 ± 0.006 K.
In bulk form and at room temperature and atmospheric pressure, the element resists alkalis, sulfuric acid, hydrochloric acid, dilute (but not concentrated) nitric acid, and aqua regia.
Isotopes.
Rhenium has one stable isotope, rhenium-185, which nevertheless occurs in minority abundance, a situation found only in two other elements (indium and tellurium). Naturally occurring rhenium is only 37.4% 185Re, and 62.6% 187Re, which is unstable but has a very long half-life (≈1010 years). This lifetime can be greatly affected by the charge state of rhenium atom. The beta decay of 187Re is used for rhenium-osmium dating of ores. The available energy for this beta decay (2.6 keV) is one of the lowest known among all radionuclides. The isotope rhenium-186m is notable as being one of the longest lived metastable isotopes with a half-life of around 200,000 years. There are twenty-five other recognized radioactive isotopes of rhenium.
Compounds.
Rhenium compounds are known for all the oxidation states between −3 and +7 except −2. The oxidation states +7, +6, +4, and +2 are the most common. Rhenium is most available commercially as salts of perrhenate, including sodium and ammonium perrhenates. These are white, water-soluble compounds.
Halides and oxyhalides.
The most common rhenium chlorides are ReCl6, ReCl5, ReCl4, and ReCl3. The structures of these compounds often feature extensive Re-Re bonding, which is characteristic of this metal in oxidation states lower than VII. Salts of [Re2Cl8]2− feature a quadruple metal-metal bond. Although the highest rhenium chloride features Re(VI), fluorine gives the d0 Re(VII) derivative rhenium heptafluoride. Bromides and iodides of rhenium are also well known.
Like tungsten and molybdenum, with which it shares chemical similarities, rhenium forms a variety of oxyhalides. The oxychlorides are most common, and include ReOCl4, ReOCl3.
Oxides and sulfides.
The most common oxide is the volatile colourless Re2O7. Rhenium trioxide ReO3 adopts a perovskite-like structure. Other oxides include Re2O5, ReO2, and Re2O3. The sulfides are ReS2 and Re2S7. Perrhenate salts can be converted to tetrathioperrhenate by the action of ammonium hydrosulfide.
Other compounds.
Rhenium diboride (ReB2) is a hard compound having the hardness similar to that of tungsten carbide, silicon carbide, titanium diboride or zirconium diboride.
Organorhenium compounds.
Dirhenium decacarbonyl is the most common entry to organorhenium chemistry. Its reduction with sodium amalgam gives Na[Re(CO)5] with rhenium in the formal oxidation state −1. Dirhenium decacarbonyl can be oxidised with bromine to bromopentacarbonylrhenium(I):
Reduction of this pentacarbonyl with zinc and acetic acid gives pentacarbonylhydridorhenium:
Methylrhenium trioxide ("MTO"), CH3ReO3 is a volatile, colourless solid has been used as a catalyst in some laboratory experiments. It can be prepared by many routes, a typical method is the reaction of Re2O7 and tetramethyltin:
Analogous alkyl and aryl derivatives are known. MTO catalyses for the oxidations with hydrogen peroxide. Terminal alkynes yield the corresponding acid or ester, internal alkynes yield diketones, and alkenes give epoxides. MTO also catalyses the conversion of aldehydes and diazoalkanes into an alkene.
Nonahydridorhenate.
A distinctive derivative of rhenium is nonahydridorhenate, originally thought to be the "rhenide" anion, Re−, but actually containing the anion in which the oxidation state of rhenium is +7.
Occurrence.
Rhenium is one of the rarest elements in Earth's crust with an average concentration of 1 ppb; other sources quote the number of 0.5 ppb making it the 77th most abundant element in Earth's crust. Rhenium is probably not found free in nature (its possible natural occurrence is uncertain), but occurs in amounts up to 0.2% in the mineral molybdenite (which is primarily molybdenum disulfide), the major commercial source, although single molybdenite samples with up to 1.88% have been found. Chile has the world's largest rhenium reserves, part of the copper ore deposits, and was the leading producer as of 2005. It was only recently that the first rhenium mineral was found and described (in 1994), a rhenium sulfide mineral (ReS2) condensing from a fumarole on Russia's Kudriavy volcano, Iturup island, in the Kuril Islands. Kudryavy discharges up to 20–60 kg rhenium per year mostly in the form of rhenium disulfide. Named rheniite, this rare mineral commands high prices among collectors. 
Production.
Commercial rhenium is extracted from molybdenum roaster-flue gas obtained from copper-sulfide ores. Some molybdenum ores contain 0.001% to 0.2% rhenium. Rhenium(VII) oxide and perrhenic acid readily dissolve in water; they are leached from flue dusts and gasses and extracted by precipitating with potassium or ammonium chloride as the perrhenate salts, and purified by recrystallization. Total world production is between 40 and 50 tons/year; the main producers are in Chile, the United States, Peru, and Poland. Recycling of used Pt-Re catalyst and special alloys allow the recovery of another 10 tons per year. Prices for the metal rose rapidly in early 2008, from $1000–$2000 per kg in 2003–2006 to over $10,000 in February 2008. The metal form is prepared by reducing ammonium perrhenate with hydrogen at high temperatures:
Applications.
Rhenium is added to high-temperature superalloys that are used to make jet engine parts, using 70% of the worldwide rhenium production. Another major application is in platinum–rhenium catalysts, which are primarily used in making lead-free, high-octane gasoline.
Alloys.
The nickel-based superalloys have improved creep strength with the addition of rhenium. The alloys normally contain 3% or 6% of rhenium. Second-generation alloys contain 3%; these alloys were used in the engines for the F-15 and F-16, whereas the newer single-crystal third-generation alloys contain 6% of rhenium; they are used in the F-22 and F-35 engines. Rhenium is also used in the superalloys, such as CMSX-4 (2nd gen) and CMSX-10 (3rd gen) that are used in industrial gas turbine engines like the GE 7FA. Rhenium can cause superalloys to become microstructurally unstable, forming undesirable TCP (topologically close packed) phases. In 4th- and 5th-generation superalloys, ruthenium is used to avoid this effect. Among others the new superalloys are EPM-102 (with 3% Ru) and TMS-162 (with 6% Ru), as well as TMS-138 and TMS-174.
For 2006, the consumption is given as 28% for General Electric, 28% Rolls-Royce plc and 12% Pratt & Whitney, all for superalloys, whereas the use for catalysts only accounts for 14% and the remaining applications use 18%. In 2006, 77% of the rhenium consumption in the United States was in alloys. The rising demand for military jet engines and the constant supply made it necessary to develop superalloys with a lower rhenium content. For example, the newer CFM International CFM56 high-pressure turbine (HPT) blades will use Rene N515 with a rhenium content of 1.5% instead of Rene N5 with 3%.
Rhenium improves the properties of tungsten. Tungsten-rhenium alloys are more ductile at low temperature, allowing them to be more easily machined. The high-temperature stability is also improved. The effect increases with the rhenium concentration, and therefore tungsten alloys are produced with up to 27% of Re, which is the solubility limit. One application for the tungsten-rhenium alloys is X-ray sources. The high melting point of both compounds, together with the high atomic mass, makes them stable against the prolonged electron impact. Rhenium tungsten alloys are also applied as thermocouples to measure temperatures up to 2200 °C.
The high temperature stability, low vapor pressure, good wear resistance and ability to withstand arc corrosion of rhenium are useful in self-cleaning electrical contacts. In particular, the discharge occurring during the switching oxidizes the contacts. However, rhenium oxide Re2O7 has poor stability (sublimes at ~360 °C) and therefore is removed during the discharge.
Rhenium has a high melting point and a low vapor pressure similar to tantalum and tungsten. Therefore, rhenium filaments exhibit a higher stability if the filament is operated not in vacuum, but in oxygen-containing atmosphere. Those filaments are widely used in mass spectrometers, in ion gauges and in photoflash lamps in photography.
Catalysts.
Rhenium in the form of rhenium-platinum alloy is used as catalyst for catalytic reforming, which is a chemical process to convert petroleum refinery naphthas with low octane ratings into high-octane liquid products. Worldwide, 30% of catalysts used for this process contain rhenium. The olefin metathesis is the other reaction for which rhenium is used as catalyst. Normally Re2O7 on alumina is used for this process. Rhenium catalysts are very resistant to chemical poisoning from nitrogen, sulfur and phosphorus, and so are used in certain kinds of hydrogenation reactions.
Other uses.
The isotopes 188Re and 186Re are radioactive and are used for treatment of liver cancer. They both have similar penetration depth in tissue (5 mm for 186Re and 11 mm for 188Re), but 186Re has advantage of longer lifetime (90 hours vs. 17 hours).
188Re is also being used experimentally in a novel treatment of pancreatic cancer where it is delivered by means of the bacterium "Listeria monocytogenes".
Related by periodic trends, rhenium has a similar chemistry to that of technetium; work done to label rhenium onto target compounds can often be translated to technetium. This is useful for radiopharmacy, where it is difficult to work with technetium – especially the 99m isotope used in medicine – due to its expense and short half-life.
Precautions.
Very little is known about the toxicity of rhenium and its compounds because they are used in very small amounts. Soluble salts, such as the rhenium halides or perrhenates, could be hazardous due to elements other than rhenium or due to rhenium itself. Only a few compounds of rhenium have been tested for their acute toxicity; two examples are potassium perrhenate and rhenium trichloride, which were injected as a solution into rats. The perrhenate had an LD50 value of 2800 mg/kg after seven days (this is very low toxicity, similar to that of table salt) and the rhenium trichloride showed LD50 of 280 mg/kg.

</doc>
<doc id="25604" url="https://en.wikipedia.org/wiki?curid=25604" title="Radon">
Radon

Radon is a chemical element with symbol Rn and atomic number 86. It is a radioactive, colorless, odorless, tasteless noble gas, occurring naturally as a decay product of radium. Its most stable isotope, 222Rn, has a half-life of 3.8 days. Radon is one of the densest substances that remains a gas under normal conditions. It is also the only gas under normal conditions that only has radioactive isotopes, and is considered a health hazard due to its radioactivity. Intense radioactivity has also hindered chemical studies of radon and only a few compounds are known.
Radon is formed as one intermediate step in the normal radioactive decay chains through which thorium and uranium slowly decay into lead. Thorium and uranium are the two most common radioactive elements on earth; they have been around since the earth was formed. Their naturally occurring isotopes have very long half-lives, on the order of billions of years. Thorium and uranium, their decay product radium, and its decay product radon, will therefore continue to occur for tens of millions of years at almost the same concentrations as they do now. As radon itself decays, it produces new radioactive elements called radon progeny (formerly called daughters) or decay products. Unlike the gaseous radon itself, radon progeny are solids and stick to surfaces, such as dust particles in the air. If such contaminated dust is inhaled, these particles can stick to the airways of the lung and increase the risk of developing lung cancer.
Unlike all the other intermediate elements in the aforementioned decay chains, radon is gaseous and easily inhaled. Thus, naturally-occurring radon is responsible for the majority of the public exposure to ionizing radiation. It is often the single largest contributor to an individual's background radiation dose, and is the most variable from location to location. Despite its short lifetime, some radon gas from natural sources can accumulate to far higher than normal concentrations in buildings, especially in low areas such as basements and crawl spaces due to its density. It can also occur in water where the water comes from a ground source -e.g. in some spring waters and hot springs.
Epidemiological studies have shown a clear link between breathing high concentrations of radon and incidence of lung cancer. Thus, radon is considered a significant contaminant that affects indoor air quality worldwide. According to the United States Environmental Protection Agency, radon is the second most frequent cause of lung cancer, after cigarette smoking, causing 21,000 lung cancer deaths per year in the United States. About 2,900 of these deaths occur among people who have never smoked. While radon is the second most frequent cause of lung cancer, it is the number one cause among non-smokers, according to EPA estimates.
Characteristics.
Physical properties.
Radon is a colorless, odorless, and tasteless gas and therefore not detectable by human senses alone. At standard temperature and pressure, radon forms a monatomic gas with a density of 9.73 kg/m3, about 8 times the density of the Earth's atmosphere at sea level, 1.217 kg/m3. Radon is one of the densest gases at room temperature and is the densest of the noble gases. Although colorless at standard temperature and pressure, when cooled below its freezing point of , radon emits a brilliant radioluminescence that turns from yellow to orange-red as the temperature lowers. Upon condensation, radon glows because of the intense radiation it produces. Radon is sparingly soluble in water, but more soluble than lighter noble gases. Radon is appreciably more soluble in organic liquids than in water.
Chemical properties.
Being a noble gas, radon is chemically not very reactive. However, the 3.8-day half-life of radon-222 makes it useful in physical sciences as a natural tracer.
Radon is a member of the zero-valence elements that are called noble gases. It is inert to most common chemical reactions, such as combustion, because the outer valence shell contains eight electrons. This produces a stable, minimum energy configuration in which the outer electrons are tightly bound. 1037 kJ/mol is required to extract one electron from its shells (also known as the first ionization energy). In accordance with periodic trends, radon has a lower electronegativity than the element one period before it, xenon, and is therefore more reactive. Early studies concluded that the stability of radon hydrate should be of the same order as that of the hydrates of chlorine () or sulfur dioxide (), and significantly higher than the stability of the hydrate of hydrogen sulfide ().
Because of its cost and radioactivity, experimental chemical research is seldom performed with radon, and as a result there are very few reported compounds of radon, all either fluorides or oxides. Radon can be oxidized by powerful oxidizing agents such as fluorine, thus forming radon difluoride. It decomposes back to elements at a temperature of above 250 °C. It has a low volatility and was thought to be . Because of the short half-life of radon and the radioactivity of its compounds, it has not been possible to study the compound in any detail. Theoretical studies on this molecule predict that it should have a Rn–F bond distance of 2.08 Å, and that the compound is thermodynamically more stable and less volatile than its lighter counterpart . The octahedral molecule was predicted to have an even lower enthalpy of formation than the difluoride. The higher fluorides RnF4 and RnF6 have been claimed, and are calculated to be stable, but it is doubtful whether they have yet been synthesized. The + ion is believed to form by the following reaction:
Radon oxides are among the few other reported compounds of radon; only the trioxide has been confirmed. Radon carbonyl RnCO has been predicted to be stable and to have a linear molecular geometry. The molecules and RnXe were found to be significantly stabilized by spin-orbit coupling. Radon caged inside a fullerene has been proposed as a drug for tumors. Despite the existence of Xe(VIII), no Rn(VIII) compounds have been claimed to exist; RnF8 should be highly unstable chemically (XeF8 is thermodynamically unstable). It is predicted that the most stable Rn(VIII) compound would be barium perradate (Ba2RnO6), analogous to barium perxenate. The instability of Rn(VIII) is due to the relativistic stabilization of the 6s shell, also known as the inert pair effect.
Isotopes.
Radon has no stable isotopes. Thirty-six radioactive isotopes have been characterized, with atomic masses ranging from 193 to 228. The most stable isotope is 222Rn, which is a decay product of 226Ra, a decay product of 238U. A trace amount of the (highly unstable) isotope 218Rn is also among the daughters of 222Rn.
Three other radon isotopes have a half-life of over an hour: 211Rn, 210Rn and 224Rn. The 220Rn isotope is a natural decay product of the most stable thorium isotope (232Th), and is commonly referred to as thoron. It has a half-life of 55.6 seconds and also emits alpha radiation. Similarly, 219Rn is derived from the most stable isotope of actinium (227Ac)—named "actinon"—and is an alpha emitter with a half-life of 3.96 seconds. No radon isotopes occur significantly in the neptunium (237Np) decay series, though a trace amount of the (extremely unstable) isotope 217Rn is produced.
Progenies.
222Rn belongs to the radium and uranium-238 decay chain, and has a half-life of 3.8235 days. Its four first products (excluding marginal decay schemes) are very short-lived, meaning that the corresponding disintegrations are indicative of the initial radon distribution. Its decay goes through the following sequence:
The radon equilibrium factor is the ratio between the activity of all short-period radon progenies (which are responsible for most of radon's biological effects), and the activity that would be at equilibrium with the radon parent.
If a closed volume is constantly supplied with radon, the concentration of short-lived isotopes will increase until an equilibrium is reached where the rate of decay of each decay product will equal that of the radon itself. The equilibrium factor is 1 when both activities are equal, meaning that the decay products have stayed close to the radon parent long enough for the equilibrium to be reached, within a couple of hours. Under these conditions each additional pCi/L of radon will increase exposure, by 0.01 WL (Working Level -a measure of radioactivity commonly used in mining. A detailed explanation of WL is given in Concentration Units). These conditions are not always met; in many homes, the equilibrium fraction is typically 40%; that is, there will be 0.004 WL of progeny for each pCi/L of radon in air. 210Pb takes much longer (decades) to come in equilibrium with radon, but, if the environment permits accumulation of dust over extended periods of time, 210Pb and its decay products may contribute to overall radiation levels as well.
Because of their electrostatic charge, radon progenies adhere to surfaces or dust particles, whereas gaseous radon does not. Attachment removes them from the air, usually causing the equilibrium factor in the atmosphere to be less than one. The equilibrium factor is also lowered by air circulation or air filtration devices, and is increased by airborne dust particles, including cigarette smoke. In high concentrations, airborne radon isotopes contribute significantly to human health risk. The equilibrium factor found in epidemiological studies is 0.4.
History and etymology.
Radon was the fifth radioactive element to be discovered, in 1900 by Friedrich Ernst Dorn, after uranium, thorium, radium and polonium. In 1900 Dorn reported some experiments in which he noticed that radium compounds emanate a radioactive gas he named "Radium Emanation" ("Ra Em"). Before that, in 1899, Pierre and Marie Curie observed that the gas emitted by radium remained radioactive for a month. Later that year, Robert B. Owens and Ernest Rutherford, at McGill University in Montreal, noticed variations when trying to measure radiation from thorium oxide. Rutherford noticed that the compounds of thorium continuously emit a radioactive gas that retains the radioactive powers for several minutes, and called this gas "emanation" (from Latin "emanare"—to elapse and "emanatio"—expiration), and later "Thorium Emanation" ("Th Em"). In 1901, he demonstrated that the emanations are radioactive, but credited the Curies for the discovery of the element. In 1903, similar emanations were observed from actinium by André-Louis Debierne and were called "Actinium Emanation" ("Ac Em").
Several names were suggested for these three gases: "exradio", "exthorio", and "exactinio" in 1904; "radon", "thoron", and "akton" in 1918; "radeon", "thoreon", and "actineon" in 1919, and eventually "radon", "thoron", and "actinon" in 1920. The likeness of the spectra of these three gases with those of argon, krypton, and xenon, and their observed chemical inertia led Sir William Ramsay to suggest in 1904 that the "emanations" might contain a new element of the noble gas family.
In 1910, Sir William Ramsay and Robert Whytlaw-Gray isolated radon, determined its density, and determined that it was the heaviest known gas. They wrote that "L'expression de l'émanation du radium est fort incommode", (the expression 'radium emanation' is very awkward) and suggested the new name niton (Nt) (from the Latin "nitens" meaning "shining") to emphasize the radioluminescence property, and in 1912 it was accepted by the International Commission for Atomic Weights. In 1923, the International Committee for Chemical Elements and International Union of Pure and Applied Chemistry (IUPAC) chose among the names radon (Rn), thoron (Tn), and actinon (An). Later, when isotopes were numbered instead of named, the element took the name of the most stable isotope, "radon", while Tn was renamed 220Rn and An was renamed 219Rn. As late as the 1960s, the element was also referred to simply as "emanation". The first synthesized compound of radon, radon fluoride, was obtained in 1962.
The danger of high exposure to radon in mines, where exposures reaching 1,000,000 Bq/m3 can be found, has long been known. In 1530, Paracelsus described a wasting disease of miners, the "mala metallorum", and Georg Agricola recommended ventilation in mines to avoid this mountain sickness ("Bergsucht"). In 1879, this condition was identified as lung cancer by Herting and Hesse in their investigation of miners from Schneeberg, Germany. The first major studies with radon and health occurred in the context of uranium mining in the Joachimsthal region of Bohemia. In the US, studies and mitigation only followed decades of health effects on uranium miners of the Southwestern United States employed during the early Cold War; standards were not implemented until 1971.
The presence of radon in indoor air was documented as early as 1950. Beginning in the 1970s research was initiated to address sources of indoor radon, determinants of concentration, health effects, and approaches to mitigation. In the United States, the problem of indoor radon received widespread publicity and intensified investigation after a widely publicized incident in 1984. During routine monitoring at a Pennsylvania nuclear power plant, a worker was found to be contaminated with radioactivity. A high contamination of radon in his home was subsequently identified as responsible for the contamination.
Occurrence.
Concentration units.
All discussions of radon concentrations in the environment refer to 222Rn. While the average rate of production of 220Rn (from the thorium decay series) is about the same as 222Rn, the amount of 220Rn in the environment is much less than that of 222Rn because of the short half-life of 220Rn (55 seconds, versus 3.8 days respectively).
Radon concentration in the atmosphere is usually measured in becquerel per cubic meter (Bq/m3), the SI derived unit. Another unit of measurement common in the US is picocuries per liter (pCi/L); 1 pCi/L=37 Bq/m3. Typical domestic exposures average about 48 Bq/m3 indoors, though this varies widely, and 15 Bq/m3 outdoors.
In the mining industry, the exposure is traditionally measured in "working level" (WL), and the cumulative exposure in "working level month" (WLM); 1 WL equals any combination of short-lived 222Rn progeny (218Po, 214Pb, 214Bi, and 214Po) in 1 liter of air that releases 1.3 × 105 MeV of potential alpha energy; one WL is equivalent to 2.08 × 10−5 joules per cubic meter of air (J/m3). The SI unit of cumulative exposure is expressed in joule-hours per cubic meter (J·h/m3). One WLM is equivalent to 3.6 × 10−3 J·h/m3. An exposure to 1 WL for 1 working month (170 hours) equals 1 WLM cumulative exposure. A cumulative exposure of 1 WLM is roughly equivalent to living one year in an atmosphere with a radon concentration of 230 Bq/m3.
Radon (222Rn), decays to 210Pb and other radioisotopes. The levels of 210Pb can be measured. The rate of deposition of this radioisotope is weather-dependent.
Radon concentrations found in natural environments are much too low to be detected by chemical means. A 1000 Bq/m3 (relatively high) concentration corresponds to 0.17 picogram per cubic meter. The average concentration of radon in the atmosphere is about 6 atoms of radon for each molecule in the air, or about 150 atoms in each ml of air. The radon activity of the entire Earth's atmosphere originates from only a few tens of grams of radon, consistently replaced by decay of larger amounts of radium and uranium.
Natural.
Radon is produced by the radioactive decay of radium-226, which is found in uranium ores, phosphate rock, shales, igneous and metamorphic rocks such as granite, gneiss, and schist, and to a lesser degree, in common rocks such as limestone. Every square mile of surface soil, to a depth of 6 inches (2.6 km2 to a depth of 15 cm), contains approximately 1 gram of radium, which releases radon in small amounts to the atmosphere. On a global scale, it is estimated that 2,400 million curies (90 TBq) of radon are released from soil annually.
Radon concentration varies widely from place to place. In the open air, it ranges from 1 to 100 Bq/m3, even less (0.1 Bq/m3) above the ocean. In caves or aerated mines, or ill-aerated houses, its concentration climbs to 20–2,000 Bq/m3.
Radon concentration can be much higher in mining contexts. Ventilation regulations instruct to maintain radon concentration in uranium mines under the "working level", with 95th percentile levels ranging up to nearly 3 WL (546 pCi 222Rn per liter of air; 20.2 kBq/m3, measured from 1976 to 1985).
The concentration in the air at the (unventilated) Gastein Healing Gallery averages 43 kBq/m3 (1.2 nCi/L) with maximal value of 160 kBq/m3 (4.3 nCi/L).
Radon mostly appears with the decay chain of the radium and uranium series (222Rn), and marginally with the thorium series (220Rn). The element emanates naturally from the ground, and some building materials, all over the world, wherever traces of uranium or thorium can be found, and particularly in regions with soils containing granite or shale, which have a higher concentration of uranium. Not all granitic regions are prone to high emissions of radon. Being a rare gas, it usually migrates freely through faults and fragmented soils, and may accumulate in caves or water. Owing to its very short half-life (four days for 222Rn), radon concentration decreases very quickly when the distance from the production area increases. Radon concentration varies greatly with season and atmospheric conditions. For instance, it has been shown to accumulate in the air if there is a meteorological inversion and little wind.
High concentrations of radon can be found in some spring waters and hot springs. The towns of Boulder, Montana; Misasa; Bad Kreuznach, Germany; and the country of Japan have radium-rich springs that emit radon. To be classified as a radon mineral water, radon concentration must be above 2 nCi/L (74 kBq/m3). The activity of radon mineral water reaches 2,000 kBq/m3 in Merano and 4,000 kBq/m3 in Lurisia (Italy).
Natural radon concentrations in the Earth's atmosphere are so low that radon-rich water in contact with the atmosphere will continually lose radon by volatilization. Hence, ground water has a higher concentration of 222Rn than surface water, because radon is continuously produced by radioactive decay of 226Ra present in rocks. Likewise, the saturated zone of a soil frequently has a higher radon content than the unsaturated zone because of diffusional losses to the atmosphere.
In 1971, Apollo 15 passed 110 km (68 mi) above the Aristarchus plateau on the Moon, and detected a significant rise in alpha particles thought to be caused by the decay of 222Rn. The presence of 222Rn has been inferred later from data obtained from the Lunar Prospector alpha particle spectrometer.
Radon is found in some petroleum. Because radon has a similar pressure and temperature curve to propane, and oil refineries separate petrochemicals based on their boiling points, the piping carrying freshly separated propane in oil refineries can become radioactive because of decaying radon and its products.
Residues from the petroleum and natural gas industry often contain radium and its daughters. The sulfate scale from an oil well can be radium rich, while the water, oil, and gas from a well often contains radon. Radon decays to form solid radioisotopes that form coatings on the inside of pipework.
Accumulation in buildings.
High concentrations of radon in homes were discovered by chance in 1985 after the stringent radiation testing conducted at a nuclear power plant entrance revealed that Stanley Watras, an engineer entering the plant, was contaminated by radioactive substances. Typical domestic exposures are of approximately 100 Bq/m3 (2.7 pCi/L) indoors. Some level of radon will be found in all buildings. Radon mostly enters a building directly from the soil through the lowest level in the building that is in contact with the ground. High levels of radon in the water supply can also increase indoor radon air levels. Typical entry points of radon into buildings are cracks in solid foundations, construction joints, cracks in walls, gaps in suspended floors, gaps around service pipes, cavities inside walls, and the water supply. Radon concentrations in the same location may differ by a factor of two over a period of 1 hour. Also, the concentration in one room of a building may be significantly different from the concentration in an adjoining room.
The distribution of radon concentrations will generally change from room to room, and the readings are averaged according to regulatory protocols. Indoor radon concentration is usually assumed to follow a lognormal distribution on a given territory. Thus, the geometric mean is generally used for estimating the "average" radon concentration in an area.
The mean concentration ranges from less than 10 Bq/m3 to over 100 Bq/m3 in some European countries. Typical geometric standard deviations found in studies range between 2 and 3, meaning (given the 68–95–99.7 rule) that the radon concentration is expected to be more than a hundred times the mean concentration for 2 to 3% of the cases.
The highest average radon concentrations in the United States are found in Iowa and in the Appalachian Mountain areas in southeastern Pennsylvania. Some of the highest readings ever have been recorded in the Irish town of Mallow, County Cork, prompting local fears regarding lung cancer. Iowa has the highest average radon concentrations in the United States due to significant glaciation that ground the granitic rocks from the Canadian Shield and deposited it as soils making up the rich Iowa farmland. Many cities within the state, such as Iowa City, have passed requirements for radon-resistant construction in new homes.
In a few locations, uranium tailings have been used for landfills and were subsequently built on, resulting in possible increased exposure to radon.
Since radon is a colorless, odorless gas the only way to know how much is present in the air or water is to perform tests. In the United States radon test kits are available to the public at retail stores, such as hardware stores, for home use and testing is available through licensed professionals, who are often home inspectors. Efforts to reduce indoor radon levels are called radon mitigation. In the U.S. the Environmental Protection Agency recommends all houses be tested for radon.
Industrial production.
Radon is obtained as a by-product of uraniferous ores processing after transferring into 1% solutions of hydrochloric or hydrobromic acids. The gas mixture extracted from the solutions contains , , He, Rn, , and hydrocarbons. The mixture is purified by passing it over copper at 720 °C to remove the and the , and then KOH and Phosphorus pentoxide are used to remove the acids and moisture by sorption. Radon is condensed by liquid nitrogen and purified from residue gases by sublimation.
Radon commercialization is regulated, but it is available in small quantities for the calibration of 222Rn measurement systems, at a price of almost $6,000 per milliliter of radium solution (which only contains about 15 picograms of actual radon at a given moment).
Radon is produced by a solution of radium-226 (half-life of 1600 years). Radium-226 decays by alpha-particle emission, producing radon that collects over samples of radium-226 at a rate of about 1 mm3/day per gram of radium; equilibrium is quickly achieved and radon is produced in a steady flow, with an activity equal to that of the radium (50 Bq). Gaseous 222Rn (half-life of about four days) escapes from the capsule through diffusion.
Applications.
Medical.
An early-20th-century form of quackery was the treatment of maladies in a radiotorium. It was a small, sealed room for patients to be exposed to radon for its "medicinal effects". The carcinogenic nature of radon due to its ionizing radiation became apparent later on. Radon's molecule-damaging radioactivity has been used to kill cancerous cells, but it does not increase the health of healthy cells. The ionizing radiation causes the formation of free radicals, which results in genetic and other cell damage, resulting in increased rates of illness, including cancer.
Exposure to radon, a process known as radiation hormesis, has been suggested to mitigate auto-immune diseases such as arthritis. As a result, in the late 20th century and early 21st century, "health mines" established in Basin, Montana attracted people seeking relief from health problems such as arthritis through limited exposure to radioactive mine water and radon. The practice is discouraged because of the well-documented ill effects of high-doses of radiation on the body.
Radioactive water baths have been applied since 1906 in Jáchymov, Czech Republic, but even before radon discovery they were used in Bad Gastein, Austria. Radium-rich springs are also used in traditional Japanese onsen in Misasa, Tottori Prefecture. Drinking therapy is applied in Bad Brambach, Germany. Inhalation therapy is carried out in Gasteiner-Heilstollen, Austria, in Świeradów-Zdrój, Czerniawa-Zdrój, Kowary, Lądek Zdrój, Poland, in Harghita Băi, Romania, and in Boulder, United States. In the US and Europe there are several "radon spas", where people sit for minutes or hours in a high-radon atmosphere in the belief that low doses of radiation will invigorate or energize them.
Radon has been produced commercially for use in radiation therapy, but for the most part has been replaced by radionuclides made in accelerators and nuclear reactors. Radon has been used in implantable seeds, made of gold or glass, primarily used to treat cancers.
The gold seeds were produced by filling a long tube with radon pumped from a radium source, the tube being then divided into short sections by crimping and cutting. The gold layer keeps the radon within, and filters out the alpha and beta radiations, while allowing the gamma rays to escape (which kill the diseased tissue). The activities might range from 0.05 to 5 millicuries per seed (2 to 200 MBq). The gamma rays are produced by radon and the first short-lived elements of its decay chain (218Po, 214Pb, 214Bi, 214Po).
Radon and its first decay products being very short-lived, the seed is left in place. After 12 half-lives (43 days), radon radioactivity is at 1/2000 of its original level. At this stage, the predominant residual activity originates from the radon decay product 210Pb, whose half-life (22.3 years) is 2000 times that of radon (and whose activity is thus 1/2000 of radon's), and its descendants 210Bi and 210Po.
In the early part of the 20th century in the US, gold contaminated with 210Pb entered the jewelry industry. This was from gold seeds that had held 222Rn that had been melted down after the radon had decayed.
Scientific.
Radon emanation from the soil varies with soil type and with surface uranium content, so outdoor radon concentrations can be used to track air masses to a limited degree. This fact has been put to use by some atmospheric scientists. Because of radon's rapid loss to air and comparatively rapid decay, radon is used in hydrologic research that studies the interaction between ground water and streams. Any significant concentration of radon in a stream is a good indicator that there are local inputs of ground water.
Radon soil-concentration has been used in an experimental way to map buried close-subsurface geological faults because concentrations are generally higher over the faults. Similarly, it has found some limited use in prospecting for geothermal gradients.
Some researchers have investigated changes in groundwater radon concentrations for earthquake prediction. Radon has a half-life of approximately 3.8 days, which means that it can be found only shortly after it has been produced in the radioactive decay chain. For this reason, it has been hypothesized that increases in radon concentration is due to the generation of new cracks underground, which would allow increased ground water circulation, flushing out radon. The generation of new cracks might not unreasonably be assumed to precede major earthquakes. In the 1970s and 1980s, scientific measurements of radon emissions near faults found that earthquakes often occurred with no radon signal, and radon was often detected with no earthquake to follow. It was then dismissed by many as an unreliable indicator. As of 2009, it was under investigation as a possible precursor by NASA.
Radon is a known pollutant emitted from geothermal power stations because it is present in the material pumped from deep underground. It disperses rapidly, and no radiological hazard has been demonstrated in various investigations. In addition, typical systems re-inject the material deep underground rather that releasing it at the surface, so its environmental impact is minimal.
In the 1940s and '50s, radon was used for industrial radiography, Other X-ray sources, which became available after World War II, quickly replaced radon for this application, as they were lower in cost and had less hazard of alpha radiation.
Health risks.
In mines.
Radon-222 decay products have been classified by the International Agency for Research on Cancer as being carcinogenic to humans, and as a gas that can be inhaled, lung cancer is a particular concern for people exposed to elevated levels of radon for sustained periods. During the 1940s and '50s, when safety standards requiring expensive ventilation in mines were not widely implemented, radon exposure was linked to lung cancer among non-smoking miners of uranium and other hard rock materials in what is now the Czech Republic, and later among miners from the Southwestern United States and South Australia. Despite these hazards being known in the early 1950s, this occupational hazard remained poorly managed in many mines until the 1970s. During this period, several entrepreneurs opened former uranium mines in the USA to the general public and advertised alleged health benefits from breathing radon gas underground. Health benefits claimed including pain, sinus, asthma and arthritis relief but these were proven to be false.
Since that time, ventilation and other measures have been used to reduce radon levels in most affected mines that continue to operate. In recent years, the average annual exposure of uranium miners has fallen to levels similar to the concentrations inhaled in some homes. This has reduced the risk of occupationally induced cancer from radon, although health issues may persist for those who are currently employed in affected mines and for those who have been employed in them in the past. As the relative risk for miners has decreased, so has the ability to detect excess risks among that population.
In addition to lung cancer, researchers have theorized a possible increased risk of leukemia due to radon exposure. Empirical support from studies of the general population is inconsistent, and a study of uranium miners found a correlation between radon exposure and chronic lymphocytic leukemia.
Miners (as well as milling and ore transportation workers) who worked in the uranium industry in the United States between the 1940s and 1971 may be eligible for compensation under the Radiation Exposure Compensation Act (RECA). Surviving relatives may also apply in cases where the formerly employed person is deceased.
Domestic-level exposure.
Radon exposure (mostly radon progeny) has been linked to lung cancer in numerous case-control studies performed in the United States, Europe and China. There are approximately 21,000 deaths per year in the US due to radon-induced lung cancers. One of the most comprehensive radon studies performed in the United States by Dr. R. William Field and colleagues found a 50% increased lung cancer risk even at the protracted exposures at the EPA's action level of 4 pCi/L. North American and European Pooled analyses further support these findings.
Most models of residential radon exposure are based on studies of miners, and direct estimates of the risks posed to homeowners would be more desirable. Because of the difficulties of measuring the risk of radon relative to smoking, models of their effect have often made use of them.
Radon has been considered the second leading cause of lung cancer and leading environmental cause of cancer mortality by the United States Environmental Protection Agency. Others have reached similar conclusions for the United Kingdom and France. Radon exposure in homes and offices may arise from certain subsurface rock formations, and also from certain building materials (e.g., some granites). The greatest risk of radon exposure arises in buildings that are airtight, insufficiently ventilated, and have foundation leaks that allow air from the soil into basements and dwelling rooms.
Action and reference level.
WHO presented in 2009 a recommended reference level (the national reference level), 100 Bq/m3, for radon in dwellings. The recommendation also says that where this is not possible, 300 Bq/m3 should be selected as the highest level. A national reference level should not be a limit, but should represent the maximum acceptable annual average radon concentration in a dwelling.
The actionable concentration of radon in a home varies depending on the organization doing the recommendation, for example, the United States Environmental Protection Agency encourages that action be taken at concentrations as low as 74 Bq/m3 (2 pCi/L), and the European Union recommends action be taken when concentrations reach 400 Bq/m3 (11 pCi/L) for old houses and 200 Bq/m3 (5 pCi/L) for new ones. On 8 July 2010 the UK's Health Protection Agency issued new advice setting a "Target Level" of 100 Bq/m3 whilst retaining an "Action Level" of 200 Bq/m3. The same levels (as UK) apply to Norway from 2010; in all new housings preventative measures should be taken against radon accumulation.
Relationship to smoking.
Results from epidemiological studies indicate that the risk of lung cancer increases with exposure to residential radon. A well-known example of source of error is smoking. In addition, smoking is the most important risk factor for lung cancer. In the West, tobacco smoke is estimated to cause about 90% of all lung cancers. There is a tendency for other hypothetical lung cancer risks to drown in the risk of smoking. Results from epidemiological studies must always be interpreted with caution.
According to the EPA, the risk of lung cancer for smokers is significant due to synergistic effects of radon and smoking. For this population about 62 people in a total of 1,000 will die of lung cancer compared to 7 people in a total of 1,000 for people who have never smoked. It can not be excluded that the risk of non-smokers should be primarily explained by a combination effect of radon and passive smoking (see below).
Radon, like other known or suspected external risk factors for lung cancer, is a threat for smokers and former smokers. This was demonstrated by the European pooling study. A commentary to the pooling study stated: "it is not appropriate to talk simply of a risk from radon in homes. The risk is from smoking, compounded by a synergistic effect of radon for smokers. Without smoking, the effect seems to be so small as to be insignificant."
According to the European pooling study, there is a difference in risk from radon between histological types. Small cell lung carcinoma, which practically only affects smokers have high risk from radon. For other histological types such as adenocarcinoma, the type that primarily affects never smokers, the risk from radon appears to be lower.
A study of radiation from post mastectomy radiotherapy shows that the simple models previously used to assess the combined and separate risks from radiation and smoking need to be developed. This is also supported by new discussion about the calculation method, LNT, which routinely has been used.
Relationship to passive smoking.
An important question is if also passive smoking can cause a similar synergy effect with residential radon. This has been insufficiently studied. The basic data for the European pooling study makes it impossible to exclude that such synergy effect is an explanation for the (very limited) increase in the risk from radon that was stated for non-smokers.
A study from 2001, which included 436 cases (never smokers who had lung cancer), and a control group (1649 never smokers) showed that exposure to radon increased the risk of lung cancer in never smokers. But the group that had been exposed to passive smoking at home appeared to bear the entire risk increase, while those who were not exposed to passive smoking did not show any increased risk with increasing radon level.
In drinking water.
The effects of radon if ingested are similarly unknown, although studies have found that its biological half-life ranges from 30–70 minutes, with 90 percent removal at 100 minutes. In 1999 National Research Council investigated the issue of radon in drinking water. The risks associated with ingestion was considered almost negligible. Water from underground sources may contain significant amounts of radon depending on the surrounding rock and soil conditions, whereas surface sources generally do not.
As well as being ingested through drinking water, radon is also released from water when temperature is increased, pressure is decreased and when water is aerated. Optimum conditions for radon release and exposure occur during showering. Water with a radon concentration of 104 pCi/L can increase the indoor airborne radon concentration by 1 pCi/L under normal conditions.
Testing and mitigation.
There are relatively simple tests for radon gas. In some countries these tests are methodically done in areas of known systematic hazards. Radon detection devices are commercially available. The short-term radon test devices used for screening purposes are inexpensive, in some cases free. There are very important protocols for taking short-term radon tests and it is imperative that they be strictly followed. The kit includes a collector that the user hangs in the lowest habitable floor of the house for 2 to 7 days. The user then sends the collector to a laboratory for analysis. Long term kits, taking collections for up to one year, are also available. An open-land test kit can test radon emissions from the land before construction begins.
Radon levels fluctuate naturally, due to factors like transient weather conditions, so an initial test might not be an accurate assessment of a home's average radon level. Radon levels are at a maximum during the coolest part of the day when pressure differentials are greatest. Therefore, a high result (over 4 pCi/L) justifies repeating the test before undertaking more expensive abatement projects. Measurements between 4 and 10 pCi/L warrant a long term radon test. Measurements over 10 pCi/L warrant only another short term test so that abatement measures are not unduly delayed. Purchasers of real estate are advised to delay or decline a purchase if the seller has not successfully abated radon to 4 pCi/L or less.
Because the half-life of radon is only 3.8 days, removing or isolating the source will greatly reduce the hazard within a few weeks. Another method of reducing radon levels is to modify the building's ventilation. Generally, the indoor radon concentrations increase as ventilation rates decrease. In a well ventilated place, the radon concentration tends to align with outdoor values (typically 10 Bq/m3, ranging from 1 to 100 Bq/m3).
The four principal ways of reducing the amount of radon accumulating in a house are:
According to the EPA the method to reduce radon "...primarily used is a vent pipe system and fan, which pulls radon from beneath the house and vents it to the outside", which is also called sub-slab depressurization, active soil depressurization, or soil suction. Generally indoor radon can be mitigated by sub-slab depressurization and exhausting such radon-laden air to the outdoors, away from windows and other building openings. "EPA generally recommends methods which prevent the entry of radon. Soil suction, for example, prevents radon from entering your home by drawing the radon from below the home and venting it through a pipe, or pipes, to the air above the home where it is quickly diluted" and "EPA does not recommend the use of sealing alone to reduce radon because, by itself, sealing has not been shown to lower radon levels significantly or consistently".
Positive-pressure ventilation systems can be combined with a heat exchanger to recover energy in the process of exchanging air with the outside, and simply exhausting basement air to the outside is not necessarily a viable solution as this can actually draw radon gas "into" a dwelling. Homes built on a crawl space may benefit from a radon collector installed under a "radon barrier" (a sheet of plastic that covers the crawl space).
For crawlspaces, the EPA states "An effective method to reduce radon levels in crawlspace homes involves covering the earth floor with a high-density plastic sheet. A vent pipe and fan are used to draw the radon from under the sheet and vent it to the outdoors. This form of soil suction is called submembrane suction, and when properly applied is the most effective way to reduce radon levels in crawlspace homes."

</doc>
<doc id="25606" url="https://en.wikipedia.org/wiki?curid=25606" title="Rocca (Italian-American rapper)">
Rocca (Italian-American rapper)

Rocca is an Italian American rapper who was born in California. His first official album entitled "Sexy Smooth" was released in January 1993 and sold many copies in the United States as well as many other countries. Rocca has 3 music videos to his credentials and wrote/rapped/starred in the "James Bond Spoof - Secret Agent OO Soul - with Billy Dee Williams". Rocca feels that there was an obligation to the youth of the world to try to promote positivity.

</doc>
<doc id="25607" url="https://en.wikipedia.org/wiki?curid=25607" title="Ruby (disambiguation)">
Ruby (disambiguation)

A ruby is a red gemstone.
Ruby may also refer to:

</doc>
<doc id="25609" url="https://en.wikipedia.org/wiki?curid=25609" title="Royal Institute of Technology">
Royal Institute of Technology

KTH Royal Institute of Technology (KTH, ) is a university in Stockholm, Sweden. KTH was founded in 1827 as Sweden's first polytechnic and is one of Scandinavia's largest (by some definitions) institutions of higher education in technology. KTH accounts for one-third of Sweden's technical research and engineering education capacity at university level. KTH offers programmes leading to a Master of Architecture, Master of Science in Engineering, Bachelor of Science in Engineering, Bachelor of Science, Master of Science, licentiate or doctoral degree. The university also offers a technical preparatory programme for non-scientists and further education.
There are a total of just over 14,000 full-year equivalent undergraduate students, more than 1,700 active postgraduate students and 4,600 full-time-equivalent employees. KTH is one of the leading technical universities in Europe and highly respected worldwide, especially in the domains of technology and natural sciences.
Campuses.
Main Campus.
The main campus buildings at Valhallavägen in Östermalm, by architect Erik Lallerstedt, were completed in 1917. The buildings and surroundings were decorated by prominent early 20th-century Swedish artists such as Carl Milles, Axel Törneman, Georg Pauli, Tore Strindberg and Ivar Johnsson. The older buildings on the campus were renovated heavily in 1994. While the original campus was large for its time, KTH very soon outgrew it, and the campus was expanded with new buildings. Today, KTH institutions and faculties are distributed across several campuses in Stockholm County, located in Flemingsberg, Haninge, Kista and Södertälje, beyond the ones in Östermalm.
Kista.
The Kista Campus, in Kista, Stockholm, houses the ICT department of the university. With companies such as Ericsson, Volvo, IBM, Tele2, TietoEnator, Microsoft, Intel and Oracle as neighbors, the cooperation between industry and KTH is widely known. Stockholm University’s computer science programmes are also located in Kista.
Flemingsberg.
School of Technology and Health has a part of its activities in Flemingsberg. At KTH Flemingsberg the school offers courses in Medical Engineering and conducts research within the subject.
KTH's activities in Flemingsberg started in 2002. Since 2003, the school offers a Bachelor of Education in Medical Engineering, in collaboration with the Karolinska Institutet. In autumn 2008, a master of science in Medical Engineering started. located here are undergraduate studies, most research departments, and the research center: Center for Technology in Medicine and Health (CTMH), which collaborates with Karolinska Institutet and Stockholm County Council (SLL) to contribute to the development and growth of research in engineering, medicine and health.
Flemingsberg is an area of high academic "density" and one of northern Europe's most important areas within biotechnology – both terms of research and industrial activities. Here are also Södertörn University and the Karolinska Institutet with over 10 000 students and Novum Research Center, where 1000 people are involved in research.
Flemingsberg is an area of strong growth. To meet the need for student housing more apartments are planned.
Haninge.
In Haninge, students from two schools at KTH receive education – the School of Architecture and the Built Environment, ABE, and the School of Technology and Health, STH. Here, students study at undergraduate and master level in fields such as building engineering (ABE), computer engineering, electrical engineering and foundation training. The School of Technology and Health also has a research centre i Haninge – Centre for Health and Building.
Just south of metropolitan Stockholm you will discover a modern campus with reputation for its nice campus area, safe atmosphere and the feeling of togetherness among staff and students.
KTH Haninge holds a large number of courses. For example, the international master programme Architectural Lighting Design is located here, with students from over 20 different countries.
KTH Haninge has a meeting point where the diverse worlds of culture, scientific research and business come together to experience and examine the significance of light and lighting in our daily lives. This is called the Lighting Laboratory.
The campus itself is located near the commuter train, which takes you to Stockholm in just 20 minutes. It is also close to a beautiful archipelago and enjoy the great outdoors in Tyresta National Park.
Södertälje.
KTH Södertälje is Södertälje’s university college campus. KTH Södertälje is a moderately large area with close and natural contacts between teachers and students.
KTH Södertälje’s education is being constantly developed via a close co-operation with the town’s business community and in particular major Södertälje companies such as Scania and AstraZeneca. Among other topics, university engineers in electronics and mechanical engineering are educated here. Also, KTH offer various Master's programmes in logistics, project management, and product development.
Since autumn 2007, the design studio Go Deep KTH at KTH Södertälje has been able to offer product development work via surveys, concept, visualisation and development through design. The idea with Go Deep KTH is to create realistic cooperation projects with companies, public administrations and institutions together with researchers in technology, medicine and health.
At KTH Södertälje, each year Tegelnatten is arranged, a competition where students in teams solve a 24 hour project assignment. The project assignment for design night 2008 was to provide proposals and ideas as to how future games and competitions will be designed to attract and entice the future generations.
Södertälje offers KTH's students who will be studying at KTH Södertälje a guaranteed apartment in Södertälje. Students who live and study in Södertälje also receive a computer via cooperation with the municipality which they are able to use during the time they spend as a student.
History.
KTH was founded in 1827 under the name Technological Institute ("Teknologiska institutet"), following the establishment of polytechnical schools in many European countries the early years of the 19th century, often based on the model of École Polytechnique in Paris in 1794.
KTH's earliest Swedish predecessor was the "Laboratorium mechanicum", a collection of mechanical models for teaching created in 1697 by Christopher Polhem, who is considered to be the father of mechanics in Sweden. The models were used intermittently for teaching practical mechanics by different masters until the School of Mechanics ("Mekaniska skolan") was founded in 1798. This is the year from which there has been continuous teaching of technology in Sweden. The activities of the School of Mechanics was taken over by KTH when it was founded.
The institute had one professor of chemistry and one of physics, and one class in mechanical engineering and one in chemical engineering. During the first years, however, teaching was at a very elementary level, and more aimed at craftsmanship rather than engineering as such. The institute was also plagued by conflicts between the faculty and the founder and head of the institute, Gustaf Magnus Schwartz, who was responsible for the artisanal focus of the institute. A government committee was appointed in 1844 to solve the issues, which led to removing Schwartz in 1845. Instead, Joachim Åkerman, the head of the School of Mining in Falun and a former professor of chemistry at KTH, took over. He led a full reorganisation of the institute in 1846–1848, after which he returned to his post in Falun. An entrance test and a minimum age of 16 for students was introduced, which led to creating proper engineering training at the institute. In 1851, the course was extended from two years to three.
In the late 1850s, the institute entered a time of expansion. In 1863, it received its own purpose-built buildings on Drottninggatan. In 1867, its regulations were again overhauled, to state explicitly that the institute should provide scientific training to its students. In 1869, the School of Mining in Falun was moved to Stockholm and merged with the institute. In 1871, the institute took over the civil engineering course formerly arranged by the Higher Artillery College in Marieberg.
In 1877, the name was changed into the current one, which changed KTH's status from Institute ("institut") to College ("högskola"), and some courses were extended from three years to four. Architecture was also added to the curriculum.
In 1915, the degree titles conferred by KTH received legal protection. In the late 19th century, it had become common to use the title "civilingenjör" (literally "civil engineer") for most KTH-trained engineers, and not just those who studied building and construction-related subjects. The only exception was the mining engineers, which called themselves "bergsingenjör" ("mountain engineer"). For a while, the title "civilingenjör" was equal to "KTH graduate" but in 1937, Chalmers in Gothenburg became the second Swedish engineering college which were allowed to confirm these titles.
In 1917, the first buildings of KTH's new campus on Valhallavägen were completed, and still constitute its main campus.
Although the engineering education of the late 19th and early 20th century were scientifically founded, up until the early 20th century, research as such was not seen as a central activity of an Institute of Technology. Those engineering graduates which went on to academic research had to earn their doctorates, typically in physics or chemistry, at a regular university. In 1927, KTH was finally granted the right to confer its own doctorates, under the designation "Teknologie doktor" (Doctor of Technology), and the first five doctors were created in 1929.
In 1984 the "civilingenjör" course at all Swedish universities was extended from four years to 4.5. From 1989, the shorter training in technology arranged by the municipal polytechnical schools in Sweden was gradually extended and moved into the university system, from 1989 as two-year courses and from 1995 alternatively as three-year courses. For KTH, this meant that additional campuses around the Stockholm area were added.
In present-day KTH continue to be Sweden's largest, oldest, and most international technical university. The university provides one-third of Sweden's research and engineering education. In 2012, there were a total of 14,000 undergraduate students, 1,700 postgraduate students, and 4,600 members of staff at the university.
R1 nuclear reactor.
After the American deployment of nuclear weapons at the end of World War II, the Swedish military leadership recognized the need for nuclear weapons to be thoroughly investigated and researched to provide Sweden with the knowledge to defend itself from a nuclear attack. With the mission to "make something with neutrons", the Swedish team, with scientists like Rolf Maximilian Sievert, set out to research the subject and eventually build a nuclear reactor for testing.
After a few years of basic research, they started building a 300 kW (later expanded to 1 MW) reactor, named "Reaktor 1" ("R1"), in a reactor hall 25 meters under the surface right underneath KTH. Today this might seem ill-considered, since approximately 40,000 people lived within a 1 km radius. It was risky, but was deemed tolerable since the reactor was an important research tool for scientists at the Royal Swedish Academy of Engineering Sciences ("Ingenjörsvetenskapsakademien").
At 18:59 on 13 July 1954, the reactor reached critical mass and sustained Sweden's first nuclear reaction. R1 was to be the main site for almost all Swedish nuclear research until 1970 when the reactor was finally decommissioned, mostly due to the increased awareness of the risks associated with operating a reactor in a densely populated area of Stockholm. The reactor hall remains an amusement to many as once it was next door to what used to be Sweden's first nuclear reactor. Close to the reactor hall is the restaurant "Q".
Organization.
From 2005 KTH is organized into nine schools each consisting of a number of departments:
Quality of education.
In 2007, by government initiative, the Swedish National Agency for Higher Education employed an international expert committee to find and award the top five highest quality education areas among all universities and colleges in Sweden. The Royal Institute of Technology received one such "Centre of Excellent Quality in Higher Education" (in Vehicle Engineering). It is the only higher education institution in the Stockholm/Uppsala region to receive an award. In 2009, KTH was the only institution among all Sweden's universities to be awarded Centre of Excellent Quality in Higher Education (in computer science). In 2014/2015 the university was ranked 110th in the world by QS World University Rankings, and it was ranked 33rd in the world for engineering and technology (making it the highest ranked institution in Scandinavia) and 74th in the natural sciences. The 2013-14 Times Higher Education World University Rankings ranked KTH 117th in the world. KTH is ranked as the 34th best university worldwide in the field of engineering and technology. It is also 13th among the world’s technical universities, and 6th among Europe’s.
Notable alumni.
Many prominent students have graduated from KTH, including;

</doc>
<doc id="25610" url="https://en.wikipedia.org/wiki?curid=25610" title="Rescuing Prometheus">
Rescuing Prometheus

Rescuing Prometheus: Four Monumental Projects That Changed the Modern World is a book by Thomas P. Hughes. The book uses four extremely large engineering projects of the late 20th century as examples to explore how the limits of modern system engineering are stressed by real life projects. It also traces the development of the management of large technical system development.
The book documents four massively-cooperative projects:

</doc>
<doc id="25611" url="https://en.wikipedia.org/wiki?curid=25611" title="Riddarfjärden">
Riddarfjärden

Riddarfjärden, literally "The Knight Firth", is a bay of Lake Mälaren in central Stockholm. Stockholm was founded in 1252 on an island in the stream where Lake Mälaren (from the west) drains into the Baltic Sea (to the east). The island is today called Stadsholmen and constitutes Stockholm's Old Town. It is surrounded by land to the north (Norrmalm) and south (Södermalm), and by water to the west (Riddarfjärden) and east (Stockholms ström).
The panorama picture featured in this article was taken from the heights of Södermalm, west of Stadsholmen, looking down on Riddarfjärden. Left to right are viewable:

</doc>
<doc id="25612" url="https://en.wikipedia.org/wiki?curid=25612" title="Random access">
Random access

In computer science, random access (more precisely and more generally called direct access) is the ability to access an item of data at any given coordinates in a population of addressable elements. As a rule, the assumption is that each element can be accessed roughly as easily and efficiently as any other, no matter how many elements may be in the set, nor how many coordinates may be available for addressing the data. For example, data might be stored notionally in a single sequence like a row, in two dimensions like rows and columns on a surface, or in multiple dimensions. However, given all the coordinates, a program can access each record about as quickly and easily as any other, and in particular, access it in time to be of value to the user. In this sense the choice of data item is arbitrary in the sense that no matter which item is sought, all that is needed to find it, is its address, that is to say, the coordinates at which it is located, such as its row and column (or its track and record number on a magnetic drum). At first the term "random access" was used because the process had to be capable of finding records no matter in which sequence they were required. However, soon the term "direct access" gained favour because one could directly retrieve a record, no matter what its position might be. The operative attribute however is that the device can access any required record immediately on demand. The opposite is sequential access, where a remote element takes longer time to access.[http://technet.microsoft.com/en-us/library/cc938619.aspx]
A typical illustration of this distinction is to compare an ancient scroll (sequential; all material prior to the data needed must be unrolled) and the book (direct: can be immediately flipped open to any arbitrary page). A more modern example is a cassette tape (sequential — one must fast forward through earlier songs to get to later ones) and a CD (direct access — one can skip to the track wanted, knowing that it would be the one retrieved).
In data structures, direct access implies the ability to access any entry in a list in constant time (independent of its position in the list and of list's size). Very few data structures can guarantee this, other than arrays (and related structures like dynamic arrays). Direct access is required, or at least valuable, in many algorithms such as binary search, integer sorting or certain versions of sieve of Eratosthenes.
Other data structures, such as linked lists, sacrifice direct access to permit efficient inserts, deletes, or reordering of data. Self-balancing binary search trees may provide an acceptable compromise, where access time is not equal for all members of a collection, but the maximum time to retrieve a given member grows only logarithmically with its size.

</doc>
<doc id="25613" url="https://en.wikipedia.org/wiki?curid=25613" title="Racism">
Racism

Racism is a product of the complex interaction in a given society of a race-based worldview with prejudice, stereotyping, and discrimination. Racism can be present in social actions, practices, or political systems (e.g., apartheid) that support the expression of prejudice or aversion in discriminatory practices. The ideology underlying racist practices often includes the idea that humans can be subdivided into distinct groups that are different in their social behavior and innate capacities and that can be ranked as inferior or superior. Racist ideology can become manifest in many aspects of social life. Associated social actions may include xenophobia, otherness, segregation, hierarchical ranking, supremacism, and related social phenomena.
While race and ethnicity are considered to be separate in contemporary social science, the two terms have a long history of equivalence in popular usage and older social science literature. "Ethnicity" is often used in a sense close to one traditionally attributed to "race": the division of human groups based on qualities assumed to be essential or innate to the group (e.g. shared ancestry or shared behavior).
"Racism" and "racial discrimination" are often used to describe discrimination on an ethnic or cultural basis, independent of whether these differences are described as racial. According to an United Nations convention, there is no distinction between the terms "racial" and "ethnic" discrimination. The UN convention further concludes that superiority based on racial differentiation is scientifically false, morally condemnable, socially unjust and dangerous, and there is no justification for racial discrimination, anywhere, in theory or in practice.
Today, the use of the term "racism" does not easily fall under a single definition. It is usually found in, but usage is not limited to, law, the social and behavioral sciences, humanities, and popular culture.
Etymology, definition and usage.
In the 19th century, many scientists subscribed to the belief that the human population can be divided into races. The term "racism" is a noun describing the state of being racist, i.e., subscribing to the belief that the human population can be classified according to race. The origin of the root word "race" is far from clear. Linguists generally agree that it came to the English language from Middle French, but there is no such agreement on how it came into Latin-based languages, generally. A recent proposal is that it derives from the Arabic "ra's", which means "head, beginning, origin" or the Hebrew "rosh", which has a similar meaning. Early race theorists generally held that some races were inferior to others and that differential treatment of races was consequently justified. These early theories guided pseudo-scientific research assumptions; the collective endeavors to adequately define and form hypotheses about racial differences are generally termed scientific racism.
Today, most biologists, anthropologists, and sociologists reject a taxonomy of races in favor of more specific and/or empirically verifiable criteria, such as geography, ethnicity or a history of endogamy. To date, there is little evidence in human genome research indicating that race can be defined in such a way as to be useful in a genetic classification of humans.
An entry in the Oxford English Dictionary (2008) defines racialism simply as "An earlier term than racism, but now largely superseded by it," and cites it in a 1902 quote. The revised Oxford English Dictionary cites the shortened term "racism" in a quote from the following year, 1903. It was first defined by the Oxford English Dictionary as "he theory that distinctive human characteristics and abilities are determined by race", which gives 1936 as the first recorded use. Additionally, the Oxford English Dictionary records "racism" as a synonym of "racialism": "belief in the superiority of a particular race". By the end of World War II, "racism" had acquired the same supremacist connotations formerly associated with "racialism": "racism" now implied racial discrimination, racial supremacism and a harmful intent. (The term "race hatred" had also been used by sociologist Frederick Hertz in the late 1920s.)
As its history indicates, popular use of the word "racism" is relatively recent. The word came into widespread usage in the Western world in the 1930s, when it was used to describe the social and political ideology of Nazism, which saw "race" as a naturally given political unit. It is commonly agreed that racism existed before the coinage of the word, but there is not a wide agreement on a single definition of what racism is and what it is not. Today, some scholars of racism prefer to use the concept in the plural "racisms" to emphasize its many different forms that do not easily fall under a single definition and that different forms have characterized different historical periods and geographical areas. Garner (2009: p. 11) summarizes different existing definitions of racism and identifies three common elements contained in those definitions of racism. First, a historical, hierarchical power relationship between groups; second, a set of ideas (an ideology) about racial differences; and, third, discriminatory actions (practices).
Legal.
Though many countries around the globe have passed laws related to race and discrimination, the first significant international human rights instrument developed by the United Nations (UN) was the Universal Declaration of Human Rights (UDHR). The UDHR was adopted by the United Nations General Assembly in 1948. The UDHR recognizes that if people are to be treated with dignity, they require economic rights, social rights including education, and the rights to cultural and political participation and civil liberty. It further states that everyone is entitled to these rights "without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status."
The UN does not define "racism"; however, it does define "racial discrimination": According to the 1965 UN International Convention on the Elimination of All Forms of Racial Discrimination,
the term "racial discrimination" shall mean any distinction, exclusion, restriction, or preference based on race, colour, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life.
In their 1978 United Nations Educational, Scientific, and Cultural Organization (UNESCO) Declaration on Race and Racial Prejudice (Article 1), the UN states, "All human beings belong to a single species and are descended from a common stock. They are born equal in dignity and rights and all form an integral part of humanity."
The UN definition of racial discrimination does not make any distinction between discrimination based on ethnicity and race, in part because the distinction between the two has been a matter of debate among academics, including anthropologists. Similarly, in British law the phrase "racial group" means "any group of people who are defined by reference to their race, colour, nationality (including citizenship) or ethnic or national origin".
In Norway, the word "race" has been removed from national laws concerning discrimination as the use of the phrase is considered problematic and unethical. The Norwegian Anti-Discrimination Act bans discrimination based on ethnicity, national origin, descent and skin color.
Social and behavioral science.
Sociologists, in general, recognize "race" as a social construct. This means that, though the concepts of race and racism are based in observable biological characteristics, any conclusions drawn about race on the basis of those observations are heavily influenced by cultural ideologies. Racism, as an ideology, exists in a society at both the individual and the institutional level.
While much of the research and work on racism in the last half-century or so has concentrated on "white racism" in the Western world, historical accounts of race-based social practices can be found across the globe. Thus, racism can be broadly defined to encompass individual and group prejudices and acts of discrimination that result in material and cultural advantages conferred on a majority or dominant social group. So-called "white racism" focuses on societies in which white populations are the majority or dominant social group. In studies of these majority white societies, the aggregate of material and cultural advantages is usually termed "white privilege".
Race and race relations are prominent areas of study in sociology and economics. Much of the sociological literature focuses on white racism. Some of the earliest sociological works on racism were penned by sociologist W. E. B. Du Bois, the first African American to earn a doctoral degree from Harvard University. Du Bois wrote, "The problem of the twentieth century is the problem of the color line.” Wellman (1993) defines racism as "culturally sanctioned beliefs, which, regardless of intentions involved, defend the advantages whites have because of the subordinated position of racial minorities". In both sociology and economics, the outcomes of racist actions are often measured by the inequality in income, wealth, net worth, and access to other cultural resources, such as education, between racial groups.
In sociology and social psychology, racial identity and the acquisition of that identity is often used as a variable in racism studies. Racial ideologies and racial identity affect individuals' perception of race and discrimination. Cazenave and Maddern (1999) define racism as "... a highly organized system of 'race'-based group privilege that operates at every level of society and is held together by a sophisticated ideology of color/'race' supremacy. Racial centrality (the extent to which a culture recognizes individuals' racial identity) appears to affect the degree of discrimination African American young adults perceive whereas racial ideology may buffer the detrimental emotional effects of that discrimination. Sellers and Shelton (2003) found that a relationship between racial discrimination and emotional distress was moderated by racial ideology and social beliefs.
Some sociologists also argue that, particularly in the West where racism is often negatively sanctioned in society, racism has changed from being a blatant to a more covert expression of racial prejudice. The "newer" (more hidden and less easily detectable) forms of racism—which can be considered as embedded in social processes and structures—are more difficult to explore as well as challenge. It has been suggested that, while in many countries overt or explicit racism has become increasingly taboo, even in those who display egalitarian explicit attitudes, an implicit or aversive racism is still maintained subconsciously.
This process has been studied extensively in social psychology as implicit associations and implicit attitudes, a component of implicit cognition. Implicit attitudes are evaluations that occur without conscious awareness towards an attitude object or the self. These evaluations are generally either favorable or unfavorable. They come about from various influences in the individual experience. Implicit attitudes are not consciously identified (or are inaccurately identified) traces of past experience that mediate favorable or unfavorable feeling, thought, or action toward social objects. These thoughts, feelings or actions have an influence on behavior of which the individual may not be aware.
Therefore, subconscious racism can influence our visual processing and how our minds work when we are subliminally exposed to faces of different colors. In thinking about crime, for example, social psychologist Jennifer L. Eberhardt (2004) of Stanford University holds that, "...blackness is so associated with crime you're ready to pick out these crime objects." Such exposures influence our mind and can cause subconscious racism in our behavior towards other people or even objects. Thus, racist thoughts and actions can arise from stereotypes and fears of which we are not aware.
Humanities.
Language, linguistics and discourse are active areas of study in the humanities, along with literature and arts. Discourse analysis seeks to reveal the meaning of race and the actions of racists through careful study of the ways these factors of human society are described and discussed in various written and oral works. Van Dijk (1992), for example, examines the different ways in which descriptions of racism and racist actions are depicted by perpetrators of such actions and their victims. He notes that when descriptions of action have negative implications about the majority, and especially of white elites, they are often seen as controversial and such controversial interpretations are typically marked with quotation marks or expressions of distance or doubt. The previously cited book, "The Souls of Black Folk" by W.E.B. Du Bois, represents early African-American literature that describes the author's experience of racism traveling in the South as an African American.
Much American fictional literature has focused on issues of racism and the black "racial experience" in the US, including those written by whites such as "Uncle Tom's Cabin", "To Kill a Mockingbird", and "Imitation of Life", or even the non-fiction "Black Like Me". These books, and others like them, feed into what has been called the "white savior narrative in film", in which the heroes and heroines are white even though the story is about things that happen to black characters. Textual analysis of such writings can contrast sharply with black authors' descriptions of African Americans and their experiences in US society. African American writers have been sometimes portrayed in African-American studies as retreating from racial issues when they write about "whiteness", while others identify this as an African American literary tradition called "the literature of white estrangement", part of a multipronged approach to challenge and dismantle white supremacy in the US.
Popular usage.
Racism can be said to describe a condition in society in which a dominant racial group benefits from the oppression of others, whether they want such benefits or not.
In popular usage, as in some academic usage, little distinction is made between "racism" and "ethnocentrism". Often, the two are listed together as "racial and ethnic" in describing some action or outcome that is associated with prejudice within a majority or dominant group in society. Further, the meaning of the term racism is often conflated with the terms prejudice, bigotry, and discrimination. Racism is a complex concept that can involve each of those, but cannot be equated to nor is it synonymous with these other terms.
Also, the term is often used in relation to what is seen as prejudice within a minority or subjugated group, as in the concept of "reverse racism". Reverse racism describes discriminatory action by members of a minority group against a dominant or formerly dominant racial or other group representative of the majority in a particular society.
Aspects.
The ideology underlying racism can become manifest in many aspects of social life. Such aspects are described in this section, although the list is not exhaustive.
Aversive racism.
Aversive racism is a form of implicit racism in which a person's unconscious negative evaluations of racial or ethnic minorities are realized by a persistent avoidance of interaction with other racial and ethnic groups. As opposed to traditional, overt racism, which is characterized by overt hatred for and explicit discrimination against racial/ethnic minorities, aversive racism is characterized by more complex, ambivalent expressions and attitudes. Aversive racism is similar in implications to the concept of symbolic or modern racism (described below), which is also a form of implicit, unconscious, or covert attitude which results in unconscious forms of discrimination.
The term was coined by Joel Kovel to describe the subtle racial behaviors of any ethnic or racial group who rationalize their aversion to a particular group by appeal to rules or stereotypes. People who behave in an aversively racial way may profess egalitarian beliefs, and will often deny their racially motivated behavior; nevertheless they change their behavior when dealing with a member of a minority group. The motivation for the change is thought to be implicit or subconscious. Experiments have provided empirical support for the existence of aversive racism. Aversive racism has been shown to have potentially serious implications for decision making in employment, in legal decisions and in helping behavior.
Color blindness.
In relation to racism, Color blindness is the disregard of racial characteristics in social interaction, for example in the rejection of affirmative action, as way to address the results of past patterns of discrimination. Critics of this attitude argue that by refusing to attend to racial disparities, racial color blindness in fact unconsciously perpetuates the patterns that produce racial inequality.
Eduardo Bonilla-Silva argues that color blind racism arises from an "abstract liberalism, biologization of culture, naturalization of racial matters, and minimization of racism". Color blind practices are "subtle, institutional, and apparently nonracial" because race is explicitly ignored in decision making. If race is disregarded in predominately white populations, for example, whiteness becomes the normative standard, whereas people of color are othered, and the racism these individuals experience may be minimized or erased. At an individual level, people with "color blind prejudice" reject racist ideology, but also reject systemic policies intended to fix institutional racism.
Cultural.
Cultural racism is a term used to describe and explain new racial ideologies and practices that have emerged since World War II. It can be defined as societal beliefs and customs that promote the assumption that the products of a given culture, including the language and traditions of that culture are superior to those of other cultures. It shares a great deal with xenophobia, which is often characterised by fear of, or aggression toward, members of an outgroup by members of an ingroup.
Cultural racism exists when there is a widespread acceptance of stereotypes concerning different ethnic or population groups. Where racism can be characterised by the belief that one race is inherently superior to another, cultural racism can be characterised by the belief that one culture is inherently superior to another.
Economic.
Historical economic or social disparity is alleged to be a form of discrimination caused by past racism and historical reasons, affecting the present generation through deficits in the formal education and kinds of preparation in previous generations, and through primarily unconscious racist attitudes and actions on members of the general population.
In 2011, Bank of America agreed to pay $335 million to settle a federal government claim that its mortgage division, Countrywide Financial, discriminated against black and Hispanic homebuyers.
During the Spanish colonial period, Spaniards developed a complex caste system based on race, which was used for social control and which also determined a person's importance in society. While many Latin American countries have long since rendered the system officially illegal through legislation, usually at the time of their independence, prejudice based on degrees of perceived racial distance from European ancestry combined with one's socioeconomic status remain, an echo of the colonial caste system.
Institutional.
Institutional racism (also known as structural racism, state racism or systemic racism) is racial discrimination by governments, corporations, religions, or educational institutions or other large organizations with the power to influence the lives of many individuals. Stokely Carmichael is credited for coining the phrase "institutional racism" in the late 1960s. He defined the term as "the collective failure of an organization to provide an appropriate and professional service to people because of their colour, culture or ethnic origin".
Maulana Karenga argued that racism constituted the destruction of culture, language, religion, and human possibility and that the effects of racism were "the morally monstrous destruction of human possibility involved redefining African humanity to the world, poisoning past, present and future relations with others who only know us through this stereotyping and thus damaging the truly human relations among peoples".
Othering.
Othering is the term used by some to describe a system of discrimination whereby the characteristics of a group are used to distinguish them as separate from the norm.
Othering plays a fundamental role in the history and continuation of racism. To objectify a culture as something different, exotic or underdeveloped is to generalize that it is not like 'normal' society. Europe's colonial attitude towards the Orient exemplifies this as it was thought that the East was the opposite of the West; feminine where the West was masculine, weak where the West was strong and traditional where the West was progressive. By making these generalizations and othering the East, Europe was simultaneously defining herself as the norm, further entrenching the gap.
Much of the process of othering relies on imagined difference, or the expectation of difference. Spatial difference can be enough to conclude that "we" are "here" and the "others" are over "there". Imagined differences serve to categorize people into groups and assign them characteristics that suit the imaginer's expectations.
Racial discrimination.
Racial discrimination refers to the separation of people through a process of social division into categories not necessarily related to races for purposes of differential treatment. Racial segregation policies may formalize it, but it is also often exerted without being legalized. Researchers Marianne Bertrand and Sendhil Mullainathan, at the University of Chicago and MIT found in a 2004 study that there was widespread racial discrimination in the workplace. In their study, candidates perceived as having "white-sounding names" were 50% more likely than those whose names were merely perceived as "sounding black" to receive callbacks for interviews. Devah Pager, a sociologist at Princeton University, sent matched pairs of applicants to apply for jobs in Milwaukee and New York City, finding that black applicants received callbacks or job offers at half the rate of equally qualified whites. In contrast, institutions and courts have upheld discrimination against whites when it is done to promote a diverse work or educational environment, even when it was shown to be to the detriment of qualified applicants. The researchers view these results as strong evidence of unconscious biases rooted in the United States' long history of discrimination (e.g., Jim Crow laws, etc.) More than 30 years of field experiment studies have found significant levels of discrimination against non-whites in labor, housing, and product markets in 10 different countries.
Segregationism.
Racial segregation is the separation of humans into socially-constructed racial groups in daily life. It may apply to activities such as eating in a restaurant, drinking from a water fountain, using a bath room, attending school, going to the movies, or in the rental or purchase of a home. Segregation is generally outlawed, but may exist through social norms, even when there is no strong individual preference for it, as suggested by Thomas Schelling's models of segregation and subsequent work.
Supremacism.
Centuries of European colonialism of the Americas, Africa and Asia was often justified by white supremacist attitudes. During the early 20th century, the phrase "The White Man's Burden" was widely used to justify imperialist policy as a noble enterprise.
Symbolic/modern.
Some scholars argue that in the US earlier violent and aggressive forms of racism have evolved into a more subtle form of prejudice in the late 20th century. This new form of racism is sometimes referred to as "modern racism" and characterized by outwardly acting unprejudiced while inwardly maintaining prejudiced attitudes, displaying subtle prejudiced behaviors such as actions informed by attributing qualities to others based on racial stereotypes, and evaluating the same behavior differently based on the race of the person being evaluated. This view is based on studies of prejudice and discriminatory behavior, where some people will act ambivalently towards black people, with positive reactions in certain, more public contexts, but more negative views and expressions in more private contexts. This ambivalence may also be visible for example in hiring decisions where job candidates that are otherwise positively evaluated may be unconsciously disfavored by employers in the final decision because of their race. Some scholars consider modern racism to be characterized by an explicit rejection of stereotypes, combined with resistance to changing structures of discrimination for reasons that are ostensibly non-racial, an ideology that considers opportunity at a purely individual basis denying the relevance of race in determining individual opportunities and the exhibition of indirect forms of micro-aggression toward and/or avoidance of people of other races.
International law and racial discrimination.
In 1919, a proposal to include a racial equality provision in the Covenant of the League of Nations was supported by a majority, but not adopted in the Paris Peace Conference, 1919. In 1943, Japan and its allies declared work for the abolition of racial discrimination to be their aim at the Greater East Asia Conference. Article 1 of the 1945 UN Charter includes "promoting and encouraging respect for human rights and for fundamental freedoms for all without distinction as to race" as UN purpose.
In 1950, UNESCO suggested in "The Race Question" —a statement signed by 21 scholars such as Ashley Montagu, Claude Lévi-Strauss, Gunnar Myrdal, Julian Huxley, etc. — to "drop the term "race" altogether and instead speak of ethnic groups". The statement condemned scientific racism theories that had played a role in the Holocaust. It aimed both at debunking scientific racist theories, by popularizing modern knowledge concerning "the race question," and morally condemned racism as contrary to the philosophy of the Enlightenment and its assumption of equal rights for all. Along with Myrdal's "" (1944), "The Race Question" influenced the 1954 U.S. Supreme Court desegregation decision in "Brown v. Board of Education of Topeka". Also in 1950, the European Convention on Human Rights was adopted, widely used on racial discrimination issues.
The United Nations use the definition of racial discrimination laid out in the "International Convention on the Elimination of All Forms of Racial Discrimination", adopted in 1966:
"... any distinction, exclusion, restriction or preference based on race, color, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life."(Part 1 of Article 1 of the U.N. International Convention on the Elimination of All Forms of Racial Discrimination)
In 2001, the European Union explicitly banned racism, along with many other forms of social discrimination, in the Charter of Fundamental Rights of the European Union, the legal effect of which, if any, would necessarily be limited to Institutions of the European Union: "Article 21 of the charter prohibits discrimination on any ground such as race, color, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, disability, age or sexual orientation and also discrimination on the grounds of nationality."
Ideology.
Racism existed during the 19th century as "scientific racism", which attempted to provide a racial classification of humanity. Johann Blumenbach in 1775, advocating polygenism, divided the world's population into five groups according to skin color (Caucasians, Mongols, etc.). The archetypical form of racism is, perhaps, found with the polygenist Christoph Meiners. He split mankind into two divisions which he labeled the "beautiful White race" and the "ugly Black race". In Meiners's book, "The Outline of History of Mankind," Meiners claimed that a main characteristic of race is either beauty or ugliness. He viewed only the white race as beautiful. He considered ugly races as inferior, immoral and animal-like.
Anders Retzius demonstrated that neither Europeans nor others are one "pure race", but of mixed origins. While discredited, derivations of Blumenbach's taxonomy are still widely used for classification of the population in USA. H. P. Steensby, while strongly emphasizing that all humans today are of mixed origins, in 1907 claimed that the origins of human differences must be traced extraordinarily far back in time, and conjectured that the "purest race" today would be the Australian Aboriginals.
Scientific racism fell strongly out of favor in the early 20th Century, but the origins of fundamental human and societal differences are still researched within academia, in fields such as human genetics including paleogenetics, social anthropology, comparative politics, history of religions, history of ideas, prehistory, history, ethics, and psychiatry. There is widespread rejection of any methodology based on anything similar to Blumenbach's races. It is more unclear to which extent and when ethnic and national stereotypes are accepted.
Although after World War II and the Holocaust, racist ideologies were discredited on ethical, political and scientific grounds, but racism and racial discrimination have remained widespread around the world.
Du Bois observed that it is not so much "race" that we think about, but culture: "... a common history, common laws and religion, similar habits of thought and a conscious striving together for certain ideals of life". Late 19th century nationalists were the first to embrace contemporary discourses on "race", ethnicity, and "survival of the fittest" to shape new nationalist doctrines. Ultimately, race came to represent not only the most important traits of the human body, but was also regarded as decisively shaping the character and personality of the nation. According to this view, culture is the physical manifestation created by ethnic groupings, as such fully determined by racial characteristics. Culture and race became considered intertwined and dependent upon each other, sometimes even to the extent of including nationality or language to the set of definition. Pureness of race tended to be related to rather superficial characteristics that were easily addressed and advertised, such as blondness. Racial qualities tended to be related to nationality and language rather than the actual geographic distribution of racial characteristics. In the case of Nordicism, the denomination "Germanic" was equivalent to superiority of race.
Bolstered by some nationalist and ethnocentric values and achievements of choice, this concept of racial superiority evolved to distinguish from other cultures that were considered inferior or impure. This emphasis on culture corresponds to the modern mainstream definition of racism: "Racism does not originate from the existence of 'races'. It "creates" them through a process of social division into categories: anybody can be racialised, independently of their somatic, cultural, religious differences."
This definition explicitly ignores the biological concept of race, still subject to scientific debate. In the words of David C. Rowe "A racial concept, although sometimes in the guise of another name, will remain in use in biology and in other fields because scientists, as well as lay persons, are fascinated by human diversity, some of which is captured by race."
Racial prejudice became subject to international legislation. For instance, the Declaration on the Elimination of All Forms of Racial Discrimination, adopted by the United Nations General Assembly on November 20, 1963, address racial prejudice explicitly next to discrimination for reasons of race, colour or ethnic origin (Article I).
Racism has been a motivating factor in social discrimination, racial segregation, hate speech and violence (such as pogroms, genocides and ethnic cleansings). Despite the persistence of racial stereotypes in humor and epithets in some everyday language, racial discrimination is illegal in many countries.
Some claim that anti-racism is a political instrument of abuse. In a reversal of values, anti-racism is claimed to be propagated by despots in the service of obscurantism and the suppression of women. Philosopher Pascal Bruckner claimed that "nti-racism in the UN has become the ideology of totalitarian regimes who use it in their own interests."
Ethnic nationalism.
After the Napoleonic Wars, Europe was confronted with the new "nationalities question," leading to reconfigurations of the European map, on which the frontiers between the states had been delimited during the 1648 Peace of Westphalia. Nationalism had made its first appearance with the invention of the "levée en masse" by the French revolutionaries, thus inventing mass conscription in order to be able to defend the newly founded Republic against the "Ancien Régime" order represented by the European monarchies. This led to the French Revolutionary Wars (1792–1802) and then to the Napoleonic conquests, and to the subsequent European-wide debates on the concepts and realities of nations, and in particular of nation-states. The Westphalia Treaty had divided Europe into various empires and kingdoms (Ottoman Empire, Holy Roman Empire, Swedish Empire, Kingdom of France, etc.), and for centuries wars were waged between princes ("Kabinettskriege" in German).
Modern nation-states appeared in the wake of the French Revolution, with the formation of patriotic sentiments for the first time in Spain during the Peninsula War (1808–1813, known in Spain as the Independence War). Despite the restoration of the previous order with the 1815 Congress of Vienna, the "nationalities question" became the main problem of Europe during the Industrial Era, leading in particular to the 1848 Revolutions, the Italian unification completed during the 1871 Franco-Prussian War, which itself culminated in the proclamation of the German Empire in the Hall of Mirrors in the Palace of Versailles, thus achieving the German unification.
Meanwhile, the Ottoman Empire, the "sick man of Europe", was confronted with endless nationalist movements, which, along with the dissolving of the Austrian-Hungarian Empire, would lead to the creation after World War I of the various nation-states of the Balkans, with "national minorities" in their borders.
Ethnic nationalism, which advocated the belief in a hereditary membership of the nation, made its appearance in the historical context surrounding the creation of the modern nation-states.
One of its main influences was the Romantic nationalist movement at the turn of the 19th century, represented by figures such as Johann Herder (1744–1803), Johan Fichte (1762–1814) in the "Addresses to the German Nation" (1808), Friedrich Hegel (1770–1831), or also, in France, Jules Michelet (1798–1874). It was opposed to liberal nationalism, represented by authors such as Ernest Renan (1823–1892), who conceived of the nation as a community, which, instead of being based on the "Volk" ethnic group and on a specific, common language, was founded on the subjective will to live together ("the nation is a daily plebiscite", 1882) or also John Stuart Mill (1806–1873).
Ethnic nationalism blended with scientific racist discourses, as well as with "continental imperialist" (Hannah Arendt, 1951) discourses, for example in the pan-Germanism discourses, which postulated the racial superiority of the German Volk. The Pan-German League ("Alldeutscher Verband"), created in 1891, promoted German imperialism, "racial hygiene" and was opposed to intermarriage with Jews. Another popular current, the "Völkisch movement", was also an important proponent of the German ethnic nationalist discourse, which combined with modern antisemitism. Members of the Völkisch movement, in particular the Thule Society, would participate in the founding of the German Workers' Party (DAP) in Munich in 1918, the predecessor of the NSDAP Nazi party. Pan-Germanism and played a decisive role in the interwar period of the 1920s–1930s.
These currents began to associate the idea of the nation with the biological concept of a "master race" (often the "Aryan race" or "Nordic race") issued from the scientific racist discourse. They conflated nationalities with ethnic groups, called "races", in a radical distinction from previous racial discourses that posited the existence of a "race struggle" inside the nation and the state itself. Furthermore, they believed that political boundaries should mirror these alleged racial and ethnic groups, thus justifying ethnic cleansing in order to achieve "racial purity" and also to achieve ethnic homogeneity in the nation-state.
Such racist discourses, combined with nationalism, were not, however, limited to pan-Germanism. In France, the transition from Republican, liberal nationalism, to ethnic nationalism, which made nationalism a characteristic of far-right movements in France, took place during the Dreyfus Affair at the end of the 19th century. During several years, a nationwide crisis affected French society, concerning the alleged treason of Alfred Dreyfus, a French Jewish military officer. The country polarized itself into two opposite camps, one represented by Émile Zola, who wrote "J'accuse" in defense of Alfred Dreyfus, and the other represented by the nationalist poet, Maurice Barrès (1862–1923), one of the founders of the ethnic nationalist discourse in France. At the same time, Charles Maurras (1868–1952), founder of the monarchist "Action française" movement, theorized the "anti-France," composed of the "four confederate states of Protestants, Jews, Freemasons and foreigners" (his actual word for the latter being the pejorative "métèques"). Indeed, to him the first three were all "internal foreigners", who threatened the ethnic unity of the French people.
History.
In antiquity.
Edith Sanders in 1969 cited the Babylonian Talmud, which divides mankind between the three sons of Noah, stating that "the descendants of Ham are cursed by being black, and depicts Ham as a sinful man and his progeny as degenerates." Although the curse of Ham has been used as an explanation for the origin of dark-skinned people since the 3rd century A.D., David M. Goldenberg (2005) writes that this was based on a theory that different climates and sun exposure affect semen composition and through this the physical composition of descendants. Furthermore, the earliest appearance of dark skin as a punishment for the descendants of Ham directly related to "Black Africans" does not appear until the 9th or 10th century (in the "Pirqei de-Rabbenu ha-Qadosh"). Earlier sources assign the punishment of blackness to Ham himself and make no mention of the people of Kush or their skin being a curse. As well, Goldenberg goes on to explain that the earlier (3rd century) sources understood "dark skin" to include not only sub-Saharan Africa but also: ... the Copts, Fezzan, Zaghawa, "Brbr", Indians, Arabs, the people of Marw, the inhabitants of the islands in the Indian Ocean, even the Chinese, as well as the Ethiopians (Habash), Zanj, Buja, and Nubians. In other words, "the coloured people of the world."
Bernard Lewis has cited the Greek philosopher Aristotle who, in his discussion of slavery, stated that while Greeks are free by nature, 'barbarians' (non-Greeks) are slaves by nature, in that it is in their nature to be more willing to submit to despotic government. Though Aristotle does not specify any particular races, he argues that people from outside Greece are more prone to the burden of slavery than those from Greece. Such proto-racism and ethnocentrism must be looked at within context, because a modern understanding of racism based on hereditary inferiority (modern racism based on: eugenics and scientific racism) was not yet developed and it is unclear whether Aristotle believed the natural inferiority of Barbarians was caused by environment and climate (like many of his contemporaries) or by birth. While Aristotle makes remarks about the most natural slaves being those with strong bodies and slave souls (unfit for rule, unintelligent) which would seem to imply a physical basis for discrimination, he also explicitly states that the right kind of souls and bodies don't always go together, implying that the greatest determinate for inferiority and natural slaves versus natural masters is the soul, not the body. This proto-racism is seen as an important precursor to modern racism by classicist Benjamin Isaac.
Historian Dante A. Puzzo, in his discussion of Aristotle, racism, and the ancient world writes that: Racism rests on two basic assumptions: that a correlation exists between physical characteristics and moral qualities; that mankind is divisible into superior and inferior stocks. Racism, thus defined, is a modern conception, for prior to the XVIth century there was virtually nothing in the life and thought of the West that can be described as racist. To prevent misunderstanding a clear distinction must be made between racism and ethnocentrism ... The Ancient Hebrews, in referring to all who were not Hebrews as Gentiles, were indulging in ethnocentrism, not in racism. ... So it was with the Hellenes who denominated all non-Hellenes——whether the wild Scythians or the Egyptians whom they acknowledged as their mentors in the arts if civilization——Barbarians, the term denoting that which was strange or foreign.
Middle Ages and Renaissance.
In the Middle East and North Africa region, racist opinions were expressed within the works of some of its historians and geographers including Al-Muqaddasi, Al-Jahiz, Al-Masudi, Abu Rayhan Biruni, Nasir al-Din al-Tusi, and Ibn Qutaybah. In the 14th century CE, the Tunisian scholar Ibn Khaldun wrote:
- :"beyond peoples of black West Africa to the south there is no civilization in the proper sense. There are only humans who are closer to dumb animals than to rational beings. They live in thickets and caves, and eat herbs and unprepared grain. They frequently eat each other. They cannot be considered human beings."" ""Therefore, the Negro nations are, as a rule, submissive to slavery, because (Negroes) have little that is (essentially) human and possess attributes that are quite similar to those of dumb animals, as we have stated."
Though the Qur'an expresses no racial prejudice, such prejudices later developed among Arabs for a variety of reasons: their extensive conquests and slave trade; the influence of Aristotelian ideas regarding slavery, which some Muslim philosophers directed towards Zanj (Bantu) and Turkic peoples; and the influence of Judeo-Christian ideas regarding divisions among humankind. In response to such views, the Afro-Arab author Al-Jahiz, himself having a Zanj grandfather, wrote a book entitled "Superiority Of The Blacks To The Whites", and explained why the Zanj were black in terms of environmental determinism in the "On the Zanj" chapter of "The Essays". By the 14th century, a significant number of slaves came from sub-Saharan Africa, leading to the likes of Egyptian historian Al-Abshibi (1388–1446) writing: "It is said that when the slave is sated, he fornicates, when he is hungry, he steals." According to J. Philippe Rushton, Arab relations with blacks whom the Muslims had dealt as slave traders for over 1,000 years could be summed up as follows:
It should be noted that ethnic prejudice among some elite Arabs was not limited to darker-skinned black people, but was also directed towards fairer-skinned "ruddy people" (including Persians, Turks, Caucasians and Europeans), while Arabs referred to themselves as "swarthy people".
However, the Umayyad Caliphate invaded Hispania and founded the civilization of Al-Andalus, where an era of religious tolerance and a Golden age of Jewish culture lasted for six centuries. It was followed by a violent "Reconquista" under the Catholic monarchs Ferdinand V and Isabella I. The Catholic Spaniards then formulated the "Cleanliness of blood" doctrine. It was during this time in history that the Western concept of aristocratic "blue blood" emerged in a highly racialized and implicitly white supremacist context, as author Robert Lacey explains:
It was the Spaniards who gave the world the notion that an aristocrat's blood is not red but blue. The Spanish nobility started taking shape around the ninth century in classic military fashion, occupying land as warriors on horseback. They were to continue the process for more than five hundred years, clawing back sections of the peninsula from its Moorish occupiers, and a nobleman demonstrated his pedigree by holding up his sword arm to display the filigree of blue-blooded veins beneath his pale skin—proof that his birth had not been contaminated by the dark-skinned enemy. Sangre azul, blue blood, was thus a euphemism for being a white man—Spain's own particular reminder that the refined footsteps of the aristocracy through history carry the rather less refined spoor of racism.
Following the expulsion of most Sephardic Jews from the Iberian peninsula, the remaining Jews and Muslims were forced to convert to Roman Catholicism, becoming "New Christians" who were despised and discriminated by the "Old Christians". An Inquisition was carried out by members of the Dominican Order in order to weed out converts that still practiced Judaism and Islam in secret. The system and ideology of the "limpieza de sangre" ostracized Christian converts from society, regardless of their actual degree of sincerity in their faith.
In Portugal, the legal distinction between New and Old Christian was only ended through a legal decree issued by the Marquis of Pombal in 1772, almost three centuries after the implementation of the racist discrimination. The "limpieza de sangre" doctrine was also very common in the colonization of the Americas, where it led to the racial separation of the various peoples in the colonies and created a very intricate list of nomenclature to describe one's precise race and, by consequence, one's place in society. This precise classification was described by Eduardo Galeano in the "Open Veins of Latin America" (1971). It included, among others terms, "mestizo" (50% Spaniard and 50% Native American), "castizo" (75% European and 25% Native American), "Spaniard" (87.5% European and 12.5% Native American), "Mulatto" (50% European and 50% African), "Albarazado" (43.75% Native American, 29.6875% European, and 26.5625% African), etc.
At the end of the Renaissance, the Valladolid debate (1550–1551) concerning the treatment of natives of the "New World" opposed the Dominican friar and Bishop of Chiapas Bartolomé de Las Casas to another Dominican philosopher Juan Ginés de Sepúlveda. The latter argued that "Indians" were natural slaves because they had no souls, and were therefore beneath humanity. Thus, reducing them to slavery or serfdom was in accordance with Catholic theology and natural law. To the contrary, Bartolomé de Las Casas argued that the Amerindians were free men in the natural order and deserved the same treatment as others, according to Catholic theology. It was one of the many controversies concerning racism, slavery and Eurocentrism that would arise in the following centuries.
Although antisemitism has a long European history, related to Christianism (anti-Judaism), racism itself is frequently described as a "modern" phenomenon. In the view of the French philosopher and historian Michel Foucault, the first formulation of racism emerged in the Early Modern period as the "discourse of race struggle", a historical and political discourse, which Foucault opposed to the philosophical and juridical discourse of sovereignty. Foucault thus argued that the first appearance of racism as a social discourse (as opposed to simple xenophobia, which some might argue has existed in all places and times) may be found during the 1688 Glorious Revolution in Great Britain, in Edward Coke or John Lilburne's work.
However, this "discourse of race struggle", as interpreted by Foucault, must be distinguished from the 19th century biological racism, also known as "race science" or "scientific racism". Indeed, this early modern discourse has many points of difference with modern racism. First of all, in this "discourse of race struggle", "race" is not considered a biological notion — which would divide humanity into distinct biological groups — but as a "historical notion". Moreover, this discourse is opposed to the sovereign's discourse: it is used by the bourgeoisie, the people and the aristocracy as a mean of struggle against the monarchy. This discourse, which first appeared in Great Britain, was then carried on in France by people such as Boulainvilliers, Nicolas Fréret, and then, during the 1789 French Revolution, Sieyès, and afterward Augustin Thierry and Cournot. Boulainvilliers, which created the matrix of such racist discourse in medieval France, conceived the "race" as something closer to the sense of "nation", that is, in his times, the "people".
He conceived France as divided between various nations — the unified nation-state is, of course, here an anachronism — which themselves formed different "races". Boulainvilliers opposed the absolute monarchy, who tried to bypass the aristocracy by establishing a direct relationship to the Third Estate. Thus, he created this theory of the French aristocrats as being the descendants of foreign invaders, whom he called the "Franks", while the Third Estate constituted according to him the autochthonous, vanquished Gallo-Romans, who were dominated by the Frankish aristocracy as a consequence of the right of conquest. Early modern racism was opposed to nationalism and the nation-state: the Comte de Montlosier, in exile during the French Revolution, who borrowed Boulainvilliers' discourse on the "Nordic race" as being the French aristocracy that invaded the plebeian "Gauls", thus showed his contempt for the Third Estate calling it "this new people born of slaves ... mixture of all races and of all times".
19th century.
While 19th century racism became closely intertwined with nationalism, leading to the ethnic nationalist discourse that identified the "race" to the "folk", leading to such movements as pan-Germanism, pan-Turkism, pan-Arabism, and pan-Slavism, medieval racism precisely divided the nation into various non-biological "races", which were thought as the consequences of historical conquests and social conflicts. Michel Foucault traced the genealogy of modern racism to this medieval "historical and political discourse of race struggle". According to him, it divided itself in the 19th century according to two rival lines: on one hand, it was incorporated by racists, biologists and eugenicists, who gave it the modern sense of "race" and, even more, transformed this popular discourse into a "state racism" (e.g. Nazism). On the other hand, Marxists also seized this discourse founded on the assumption of a political struggle that provided the real engine of history and continued to act underneath the apparent peace. Thus, Marxists transformed the essentialist notion of "race" into the historical notion of "class struggle", defined by socially structured position: capitalist or proletarian. In "The Will to Knowledge" (1976), Foucault analyzed another opponent of the "race struggle" discourse: Sigmund Freud's psychoanalysis, which opposed the concepts of "blood heredity", prevalent in the 19th century racist discourse.
Authors such as Hannah Arendt, in her 1951 book "The Origins of Totalitarianism", have said that the racist ideology ("popular racism") that developed at the end of the 19th century helped legitimize the imperialist conquests of foreign territories and atrocities that sometimes accompanied them (such as the Herero and Namaqua Genocide of 1904–1907 or the Armenian Genocide of 1915–1917). Rudyard Kipling's poem "The White Man's Burden" (1899) is one of the more famous illustrations of the belief in the inherent superiority of the European culture over the rest of the world, though also it is also thought to be a satirical appraisal of such imperialism. Racist ideology thus helped legitimize the conquest and incorporation of foreign territories into an empire, which were regarded as a humanitarian obligation partially as a result of these racist beliefs.
However, during the 19th century, West European colonial powers were involved in the suppression of the Arab slave trade in Africa, as well as in suppression of the slave trade in West Africa. Some Europeans during the time period objected to injustices that occurred in some colonies and lobbied on behalf of aboriginal peoples. Thus, when the Hottentot Venus was displayed in England in the beginning of the 19th century, the African Association publicly opposed itself to the exhibition. The same year that Kipling published his poem, Joseph Conrad published "Heart of Darkness" (1899), a clear criticism of the Congo Free State owned by Leopold II of Belgium.
Examples of racial theories used include the creation of the Hamitic ethno-linguistic group during the European exploration of Africa. It was then restricted by Karl Friedrich Lepsius (1810–1877) to non-Semitic Afro-Asiatic languages.
The term "Hamite" was applied to different populations within North Africa, mainly comprising Ethiopians, Eritreans, Somalis, Berbers, and the ancient Egyptians. Hamites were regarded as Caucasoid peoples who probably originated in either Arabia or Asia on the basis of their cultural, physical and linguistic similarities with the peoples of those areas. Europeans considered Hamites to be more civilized than Sub-Saharan Africans, and more akin to themselves and Semitic peoples. In the first two-thirds of the 20th century, the Hamitic race was, in fact, considered one of the branches of the Caucasian race, along with the Indo-Europeans, Semites, and the Mediterranean race.
However, the Hamitic peoples themselves were often deemed to have failed as rulers, which was usually ascribed to interbreeding with Negroes. In the mid-20th century, the German scholar Carl Meinhof (1857–1944) claimed that the Bantu race was formed by a merger of Hamitic and Negro races. The Hottentots (Nama or Khoi) were formed by the merger of Hamitic and Bushmen (San) races — both being termed nowadays as Khoisan peoples).
In the United States in the early 19th century, the American Colonization Society was established as the primary vehicle for proposals to return black Americans to greater freedom and equality in Africa. The colonization effort resulted from a mixture of motives with its founder Henry Clay stating; "unconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off". Racism spread throughout the New World in the late 19th century and early 20th century. Whitecapping, which started in Indiana in the late 19th century, soon spread throughout all of North America, causing many African laborers to flee from the land they worked on. In the US during the 1860s, racist posters were used during election campaigns. In one of these racist posters (see above), a black man is depicted lounging idly in the foreground as one white man ploughs his field and another chops wood. Accompanying labels are: "In the sweat of thy face shalt thou eat thy bread," and "The white man must work to keep his children and pay his taxes." The black man wonders, "Whar is de use for me to work as long as dey make dese appropriations." Above in a cloud is an image of the "Freedman's Bureau! Negro Estimate of Freedom!" The bureau is pictured as a large domed building resembling the U.S. Capitol and is inscribed "Freedom and No Work." Its columns and walls are labeled, "Candy," "Rum, Gin, Whiskey," "Sugar Plums," "Indolence," "White Women," "Apathy," "White Sugar," "Idleness," and so on.
On June 5, 1873, Sir Francis Galton, distinguished English explorer and cousin of Charles Darwin, wrote in a letter to The Times:
20th century.
The Nazi party, who seized power in the 1933 German elections and maintained a dictatorship over much of Europe until the End of World War II in the continent, deemed the Germans to be part of an Aryan "master race" ("Herrenvolk"), who therefore had the right to expand their territory and enslave or kill members of other races deemed inferior. 
The racial ideology conceived by the Nazis graded humans on a scale of pure Aryan to non-Aryan, with the latter viewed as subhuman. At the top of the scale of pure Aryans were Germans and other Germanic peoples including the Dutch, Scandinavians, and the English, as well as other peoples such as some northern Italians and the French who were said to have a suitable admixture of Germanic blood. Nazi policies labeled Romani people, ethnic Poles, various Slavic peoples, Serbs, and people of color as inferior non-Aryan subhumans. Jews were at the bottom of the hierarchy, considered inhuman and thus unworthy of life. In accordance with Nazi Racial ideology, approximately 6 million Jews were killed in the Holocaust. 2.5 million ethnic Poles, .5 million ethnic Serbs and 0.22-.5 million Romani were killed by the regime and its collaborators.
The Nazis considered most Slavs to be Non-Aryan "Untermenschen". Slavic nations such as the Ukrainians, Czechs, Slovaks, Bulgarians and Croats who collaborated with Nazi Germany were perceived as ethnically superior to other Slavs, mostly due to pseudoscientific theories about these nations having a considerable admixture of Germanic blood. In the secret plan Generalplan Ost ("Master Plan East") the Nazis resolved to expel, enslave, or extermination most Slavic people to provide "living space" for Germans, however Nazi policy towards Slavs changed during World War II due to manpower shortages which necessitated limited Slavic participation in the Waffen-SS. Significant war crimes were committed against Slavs, particularly Poles, and Soviet POWs had a far higher mortality rate than their American and British counterparts due to deliberate neglect and mistreatment.
Serious race riots in Durban between Indians and Zulus erupted in 1949. Ne Win's rise to power in Burma in 1962 and his relentless persecution of "resident aliens" led to an exodus of some 300,000 Burmese Indians. They migrated to escape racial discrimination and wholesale nationalisation of private enterprise a few years later in 1964. The Zanzibar Revolution of January 12, 1964 put an end to the local Arab dynasty. Thousands of Arabs and Indians in Zanzibar were massacred in riots, and thousands more were detained or fled the island. On 4 August 1972, Idi Amin, President of Uganda, ethnically cleansed Uganda's Asians giving them 90 days to leave the country.
Shortly after world war II the South African National Party took control over the governance in South Africa. Between 1948 and 1994, the Apartheid regime took place. This regime based their ideologies on the racial separation of whites and non- whites including the unequal rights of non-whites. Several protests and violence occurred during the Apartheid in South Africa, the most famous of these include the Sharpeville Massacre in 1960, the Soweto uprising in 1976, the Church Street bombing of 1983 and the Cape Town peace march of 1989.
Contemporary.
During the Congo Civil War (1998–2003), Pygmies were hunted down like game animals and eaten. Both sides of the war regarded them as "subhuman" and some say their flesh can confer magical powers. UN human rights activists reported in 2003 that rebels had carried out acts of cannibalism. Sinafasi Makelo, a representative of Mbuti pygmies, has asked the UN Security Council to recognise cannibalism as a crime against humanity and an act of genocide. A report released by the United Nations Committee on the Elimination of Racial Discrimination condemns Botswana's treatment of the 'Bushmen' as racist. In 2008, the tribunal of the 15-nation Southern African Development Community (SADC) accused Zimbabwean President Robert Mugabe of having a racist attitude towards white people.
The mass demonstrations and riots against African students in Nanjing, China, lasted from December 1988 to January 1989. Bar owners in central Beijing had been forced by the police "not to serve black people or Mongolians" during the 2008 Summer Olympics, as the police associates these ethnic groups with illegal prostitution and drug trafficking. In November 2009, British newspaper "The Guardian" reported that Lou Jing, of mixed Chinese and African parentage, had emerged as the most famous talent show contestant in China and has become the subject of intense debate because of her skin color. Her attention in the media opened serious debates about racism in China and racial prejudice.
In Asia and Latin America, light skin is seen as more attractive. Thus, skin whitening cosmetic products are popular in East Asia and India. Some activists, most prominently at the UN conference at Durban, have asserted that the caste system in India is a form of racial discrimination, although many prominent scholars debunk this viewpoint as "scientifically nonsense", since there are no consistent racial differences between the different castes in India. These activists utilize genetic studies that claim to corroborate their view, although other more detailed studies have challenged these assertions as overtly simplistic Currently, there are approximately 165 million Dalits (formerly known as "untouchables") in India.
Some 70,000 black African Mauritanians were expelled from Mauritania in the late 1980s. In the Sudan, black African captives in the civil war were often enslaved, and female prisoners were often sexually abused. The Darfur conflict has been described by some as a racial matter. In October 2006, Niger announced that it would deport the Arabs living in the Diffa region of eastern Niger to Chad. This population numbered about 150,000. While the Government collected Arabs in preparation for the deportation, two girls died, reportedly after fleeing Government forces, and three women suffered miscarriages.
The Jakarta riots of May 1998 targeted many Chinese Indonesians. The anti-Chinese legislation was in the Indonesian constitution until 1998. Resentment against Chinese workers has led to violent confrontations in Africa and Oceania. Anti-Chinese rioting, involving tens of thousands of people, broke out in Papua New Guinea in May 2009. Indo-Fijians suffered violent attacks after the Fiji coup of 2000. Non-indigenous citizens of Fiji are subject to discrimination. Racial divisions also exist in Guyana, Malaysia, Trinidad and Tobago, Madagascar, or South Africa.
Elements within Israeli society have been accused of . Accusations of racism range from birth control policies, education, and housing discrimination.
One form of racism in the United States was enforced racial segregation which existed until the 1960s when it was outlawed in the Civil Rights Act of 1964. It has been argued that this separation of races continues to exist today "de facto". The causes of segregation vary from lack of access to loans and resources to discrimination in realty.
Ethnic conflicts.
Debates over the origins of racism often suffer from a lack of clarity over the term. Many use the term "racism" to refer to more general phenomena, such as xenophobia and ethnocentrism, although scholars attempt to clearly distinguish those phenomena from racism as an ideology or from scientific racism, which has little to do with ordinary xenophobia. Others conflate recent forms of racism with earlier forms of ethnic and national conflict. In most cases, ethno-national conflict seems to owe itself to conflict over land and strategic resources. In some cases, ethnicity and nationalism were harnessed to rally combatants in wars between great religious empires (for example, the Muslim Turks and the Catholic Austro-Hungarians).
Notions of race and racism have often played central roles in such ethnic conflicts. Throughout history, when an adversary is identified as "other" based on notions of race or ethnicity (in particular when "other" is construed to mean "inferior"), the means employed by the self-presumed "superior" party to appropriate territory, human chattel, or material wealth often have been more ruthless, more brutal, and less constrained by moral or ethical considerations. According to historian Daniel Richter, Pontiac's Rebellion saw the emergence on both sides of the conflict of "the novel idea that all Native people were 'Indians,' that all Euro-Americans were 'Whites,' and that all on one side must unite to destroy the other." Basil Davidson states in his documentary, "Africa: Different but Equal", that racism, in fact, only just recently surfaced—as late as the 19th century, due to the need for a justification for slavery in the Americas.
Historically, racism was a major driving force behind the Transatlantic slave trade. It was also a major force behind racial segregation, especially in the United States in the nineteenth and early twentieth centuries and South Africa under apartheid; 19th and 20th century racism in Western culture is particularly well documented and constitutes a reference point in studies and discourses about racism. Racism has played a role in genocides such as the Holocaust, and the Armenian genocide, and colonial projects like the European colonization of the Americas, Africa, and Asia. Indigenous peoples have been –and are– often subject to racist attitudes. Practices and ideologies of racism are condemned by the United Nations in the Declaration of Human Rights.
The idea of slavery as an "equal-opportunity employer" was denounced with the introduction of Christian theory in the West. Maintaining the belief that Africans were "subhuman" was the only loophole in the then accepted law that "men are created equal" that would allow for the sustenance of the Triangular Trade. New peoples in the Americas, possible slaves, were encountered, fought against, and ultimately subdued, but, then, due to European diseases, their populations drastically decreased. Through both influences, theories about "race" developed, and these helped many to justify the differences in position and treatment of people whom they categorized as belonging to different races (see Eric Wolf's "Europe and the People without History").
In the middle of the 16th century, during the Valladolid controversy, Juan Ginés de Sepúlveda argued that Native Americans were natural slaves because they had no "souls". In Asia, the Chinese and Japanese Empires were both strong colonial powers, with the Chinese establishing colonies and vassal states in much of East Asia throughout their history, and with the Japanese doing the same in the 19th–20th centuries. In both cases, the Asian imperial powers believed that they were ethnically and racially preferenced too.
Academic variants.
Scottish philosopher and economist David Hume said, "I am apt to suspect the Negroes to be naturally inferior to the Whites. There scarcely ever was a civilised nation of that complexion, nor even any individual, eminent either in action or in speculation. No ingenious manufacture among them, no arts, no sciences." German philosopher Immanuel Kant stated: "The yellow Indians do have a meagre talent. The Negroes are far below them, and at the lowest point are a part of the American people."
In the 19th century, the German philosopher, Georg Wilhelm Friedrich Hegel, declared that "Africa is no historical part of the world." Hegel further claimed that blacks had no "sense of personality; their spirit sleeps, remains sunk in itself, makes no advance, and thus parallels the compact, undifferentiated mass of the African continent."
Fewer than 30 years before Nazi Germany instigated World War II, the Austrian, Otto Weininger, claimed: "A genius has perhaps scarcely ever appeared amongst the negroes, and the standard of their morality is almost universally so low that it is beginning to be acknowledged in America that their emancipation was an act of imprudence."
The German conservative, Oswald Spengler, remarked on what he perceived as the culturally degrading influence of Africans in modern Western culture: in "The Hour of Decision" Spengler denounced "the 'happy ending' of an empty existence, the boredom of which has brought to jazz music and Negro dancing to perform the Death March for a great Culture." During the Nazi era, German scientists rearranged academia to support claims of a grand "Aryan" agent behind the splendors of all human civilizations, including India and Ancient Egypt.
Scientific variants.
The modern biological definition of race developed in the 19th century with scientific racist theories. The term "scientific racism" refers to the use of science to justify and support racist beliefs, which goes back to the early 18th century, though it gained most of its influence in the mid-19th century, during the New Imperialism period. Also known as academic racism, such theories first needed to overcome the Church's resistance to positivist accounts of history and its support of monogenism, the concept that all human beings were originated from the same ancestors, in accordance with creationist accounts of history.
These racist theories put forth on scientific hypothesis were combined with unilineal theories of social progress, which postulated the superiority of the European civilization over the rest of the world. Furthermore, they frequently made use of the idea of "survival of the fittest", a term coined by Herbert Spencer in 1864, associated with ideas of competition, which were named social Darwinism in the 1940s. Charles Darwin himself opposed the idea of rigid racial differences in "The Descent of Man" (1871) in which he argued that humans were all of one species, sharing common descent. He recognised racial differences as varieties of humanity, and emphasised the close similarities between people of all races in mental faculties, tastes, dispositions and habits, while still contrasting the culture of the "lowest savages" with European civilization.
At the end of the 19th century, proponents of scientific racism intertwined themselves with eugenics discourses of "degeneration of the race" and "blood heredity." Henceforth, scientific racist discourses could be defined as the combination of polygenism, unilinealism, social Darwinism and eugenism. They found their scientific legitimacy on physical anthropology, anthropometry, craniometry, phrenology, physiognomy, and others now discredited disciplines in order to formulate racist prejudices.
Before being disqualified in the 20th century by the American school of cultural anthropology (Franz Boas, etc.), the British school of social anthropology (Bronisław Malinowski, Alfred Radcliffe-Brown, etc.), the French school of ethnology (Claude Lévi-Strauss, etc.), as well as the discovery of the neo-Darwinian synthesis, such sciences, in particular anthropometry, were used to deduce behaviours and psychological characteristics from outward, physical appearances.
The neo-Darwinian synthesis, first developed in the 1930s, eventually led to a gene-centered view of evolution in the 1960s. According to the Human Genome Project, the most complete mapping of human DNA to date indicates that there is no clear genetic basis to racial groups. While some genes are more common in certain populations, there are no genes that exist in all members of one population and no members of any other.
Heredity and eugenics.
The first theory of eugenics was developed in 1869 by Francis Galton (1822–1911), who used the then popular concept of "degeneration". He applied statistics to study human differences and the alleged "inheritance of intelligence", foreshadowing future uses of "intelligence testing" by the anthropometry school. Such theories were vividly described by the writer Émile Zola (1840–1902), who started publishing in 1871 a twenty-novel cycle, "Les Rougon-Macquart", where he linked heredity to behavior. Thus, Zola described the high-born Rougons as those involved in politics ("Son Excellence Eugène Rougon") and medicine ("Le Docteur Pascal") and the low-born Macquarts as those fatally falling into alcoholism ("L'Assommoir"), prostitution ("Nana"), and homicide ("La Bête humaine").
During the rise of Nazism in Germany, some scientists in Western nations worked to debunk the regime's racial theories. A few argued against racist ideologies and discrimination, even if they believed in the alleged existence of biological races. However, in the fields of anthropology and biology, these were minority positions until the mid-20th century. According to the 1950 UNESCO statement, "The Race Question", an international project to debunk racist theories had been attempted in the mid-1930s. However, this project had been abandoned. Thus, in 1950, UNESCO declared that it had resumed:
"up again, after a lapse of fifteen years, a project that the International Committee on Intellectual Cooperation has wished to carry through but that it had to abandon in deference to the appeasement policy of the pre-war period. The race question had become one of the pivots of Nazi ideology and policy. Masaryk and Beneš took the initiative of calling for a conference to re-establish in the minds and consciences of men everywhere the truth about race ... Nazi propaganda was able to continue its baleful work unopposed by the authority of an international organisation."
The Third Reich's racial policies, its eugenics programs and the extermination of Jews in the Holocaust, as well as Romani people in the Porrajmos (the Romani Holocaust) and others minorities led to a change in opinions about scientific research into race after the war. Changes within scientific disciplines, such as the rise of the Boasian school of anthropology in the United States contributed to this shift. These theories were strongly denounced in the 1950 UNESCO statement, signed by internationally renowned scholars, and titled "The Race Question".
Polygenism and racial typologies.
Works such as Arthur de Gobineau's "An Essay on the Inequality of the Human Races" (1853–1855) may be considered as one of the first theorizations of this new racism, founded on an essentialist notion of race, which opposed the former racial discourse, of Boulainvilliers for example, which saw in races a fundamentally historical reality, which changed over time. Gobineau, thus, attempted to frame racism within the terms of biological differences among humans, giving it the legitimacy of biology. He was one of the first theorists to postulate polygenism, stating that there were, at the origins of the world, various discrete "races."
Gobineau's theories would be expanded, in France, by Georges Vacher de Lapouge (1854–1936)'s typology of races, who published in 1899 "The Aryan and his Social Role", in which he claimed that the white, "Aryan race", "dolichocephalic", was opposed to the "brachycephalic" race, of whom the "Jew" was the archetype. Vacher de Lapouge thus created a hierarchical classification of races, in which he identified the ""Homo europaeus" (Teutonic, Protestant, etc.), the ""Homo alpinus"" (Auvergnat, Turkish, etc.), and finally the ""Homo mediterraneus"" (Neapolitan, Andalus, etc.) He assimilated races and social classes, considering that the French upper class was a representation of the "Homo europaeus", while the lower class represented the "Homo alpinus". Applying Galton's eugenics to his theory of races, Vacher de Lapouge's "selectionism" aimed first at achieving the annihilation of trade unionists, considered to be a "degenerate"; second, creating types of man each destined to one end, in order to prevent any contestation of labour conditions. His "anthroposociology" thus aimed at blocking social conflict by establishing a fixed, hierarchical social order
The same year, William Z. Ripley used identical racial classification in "The Races of Europe" (1899), which would have a great influence in the United States. Other scientific authors include H.S. Chamberlain at the end of the 19th century (a British citizen who naturalized himself as German because of his admiration for the "Aryan race") and Madison Grant, a eugenicist and author of "The Passing of the Great Race" (1916). Madison Grant provided statistics for the Immigration Act of 1924, which severely restricted immigration of Jews, Slavs, and southern Europeans, who were subsequently hindered in seeking to escape Nazi Germany.
Human zoos.
Human zoos (called "People Shows"), were an important means of bolstering "popular racism" by connecting it to scientific racism: they were both objects of public curiosity and of anthropology and anthropometry. Joice Heth, an African American slave, was displayed by P.T. Barnum in 1836, a few years after the exhibition of Saartjie Baartman, the "Hottentot Venus", in England. Such exhibitions became common in the New Imperialism period, and remained so until World War II. Carl Hagenbeck, inventor of the modern zoos, exhibited animals beside humans who were considered "savages".
Congolese pygmy Ota Benga was displayed in 1906 by eugenicist Madison Grant, head of the Bronx Zoo, as an attempt to illustrate the "missing link" between humans and orangutans: thus, racism was tied to Darwinism, creating a social Darwinist ideology that tried to ground itself in Darwin's scientific discoveries. The 1931 Paris Colonial Exhibition displayed Kanaks from New Caledonia. A "Congolese village" was on display as late as 1958 at the Brussels' World Fair.
Evolutionary theories about the origins of racism.
Biologists John Tooby and Leda Cosmides were puzzled by the fact that in the US race is one of the three characteristics most often used in brief descriptions of individuals (the others are age and sex). They reasoned that natural selection would not have favoured the evolution of an instinct for using race as a classification, because for most of human history, humans almost never encountered members of other races. Tooby and Cosmides hypothesized that modern people use race as a proxy (rough-and-ready indicator) for coalition membership, since a better-than-random guess about "which side" another person is on will be helpful if one does not actually know in advance.
Their colleague Robert Kurzban designed an experiment whose results appeared to support this hypothesis. Using the Memory confusion protocol, they presented subjects with pictures of individuals and sentences, allegedly spoken by these individuals, which presented two sides of a debate. The errors that the subjects made in recalling who said what indicated that they sometimes misattributed a statement to a speaker of the same race as the "correct" speaker, although they also sometimes misattributed a statement to a speaker "on the same side" as the "correct" speaker. In a second run of the experiment, the team also distinguished the "sides" in the debate by clothing of similar colors; and in this case the effect of racial similarity in causing mistakes almost vanished, being replaced by the color of their clothing. In other words, the first group of subjects, with no clues from clothing, used race as a visual guide to guessing who was on which side of the debate; the second group of subjects used the clothing color as their main visual clue, and the effect of race became very small.
Some research suggests that ethnocentric thinking may have actually contributed to the development of cooperation. Political scientists Ross Hammond and Robert Axelrod created a computer simulation wherein virtual individuals were randomly assigned one of a variety of skin colors, and then one of a variety of trading strategies: be color-blind, favor those of your own color, or favor those of other colors. They found that the ethnocentric individuals clustered together, then grew until all the non-ethnocentric individuals were wiped out.
In "The Selfish Gene", evolutionary biologist Richard Dawkins writes that "Blood-feuds and inter-clan warfare are easily interpretable in terms of Hamilton's genetic theory." Dawkins writes that racial prejudice, while not evolutionarily adaptive, "could be interpreted as an irrational generalization of a kin-selected tendency to identify with individuals physically resembling oneself, and to be nasty to individuals different in appearance". Simulation-based experiments in evolutionary game theory have attempted to provide an explanation for the selection of ethnocentric-strategy phenotypes.
Research on influencing factors.
Research has examined factors influencing tolerance, in particular ethnic tolerance, prejudice, and trust. Authoritarian personality has been associated with prejudice and intolerance. Education has an inverse association which is stronger in established democracies than in emerging. Different groups are viewed differently and including illegal groups in tolerance surveys may reduce tolerance levels in all countries except the United States. Increased contact with other groups increase tolerance. Increased perception of threat, including from the home land of an ethnic minority, reduces tolerance. Competition over jobs reduces tolerance and occupational segregation reduced ethnic conflicts and ethnic prejudice in studies in the United States and Yugoslavia. Tolerance is increased by democratic stability and a federal system. Increased ethnic heterogeneity increases tolerance up to a point but beyond this tolerance decreases. The negative effect of increased ethnic heterogeneity is stronger when looking at larger areas such as nations compared to smaller areas such as neighborhoods. This may be due to the contact effect being relatively more important at local levels while the threat effect becomes more important in larger areas. One study, published by Carl Bell, revealed that "racist attitudes may be indicative of a narcissistic personality disorder or of a regression to primitive narcissistic functioning secondary to environmental forces."
As state-sponsored activity.
State racism—that is, institutions and practices of a nation-state that are grounded in racist ideology—has played a major role in all instances of settler colonialism, from the United States to Australia. It also played a prominent role in the Nazi German regime and fascist regimes in Europe, and in the first part of Japan's Shōwa period. These governments advocated and implemented policies that were racist, xenophobic and, in the case of Nazism, genocidal. The politics of Zimbabwe promote discrimination against whites, in an effort to ethnically cleanse the country.
The Nuremberg Race Laws of 1935 prohibited sexual relations between any Aryan and Jew, considering it "Rassenschande", "racial pollution". The Nuremberg Laws stripped all Jews, even quarter- and half-Jews (second and first degree "Mischlings"), of their German citizenship. This meant that they had no basic citizens' rights, e.g., the right to vote. In 1936, Jews were banned from all professional jobs, effectively preventing them having any influence in education, politics, higher education and industry. On 15 November 1938, Jewish children were banned from going to normal schools. By April 1939, nearly all Jewish companies had either collapsed under financial pressure and declining profits, or had been persuaded to sell out to the Nazi government. This further reduced their rights as human beings; they were in many ways officially separated from the German populace. Similar laws existed in Bulgaria- The Law for protection of the nation, Hungary, Romania, and Austria.
Legislative state racism is known to have been enforced by the National Party of South Africa during their Apartheid regime between 1948 and 1994. Here a series of Apartheid legislation in South Africa was passed through the legal systems to make it legal for white South Africans to have rights which were superior to those of non-white South Africans. Non-white South Africans were not allowed involvement in any governing matters, including voting; access to quality healthcare; the provision of basic services, including clean water; electricity; as well as access to adequate schooling. Non-white South Africans were also prevented from accessing certain public areas, using certain public transportation and were required to live only in certain designated areas. Non-white South Africans were taxed differently from white South Africans and were required to carry on them at all times additional documentation, which later became known as "dom passes", to certify their non-white South African citizenship. All of these legislative racial laws were abolished through a series of equal human rights laws passed at the end of Apartheid in the early 1990s.
The current constitution of Liberia, as enacted in 1984, is racist in its Article 27, as it does not allow Whites to become Liberian citizens: "only persons who are Negroes or of Negro descent shall qualify by birth or by naturalization to be citizens of Liberia".
Inter-minority variants.
Prejudiced thinking among and between minority groups does occur.
In Europe.
In Britain, tensions between minority groups can be just as strong as those between minorities and the majority population. In Birmingham, there have been long-term divisions between the Black and South Asian communities, which were illustrated in the Handsworth riots and in the smaller 2005 Birmingham riots. In Dewsbury, a Yorkshire town with a relatively high Muslim population, there have been tensions and minor civil disturbances between Kurds and South Asians.
In France, home to Europe's largest population of Muslims (about 6 million) as well as the continent's largest community of Jews (about 450,000), anti-Jewish violence (Kidnapping and torture of Ilan Halimi, Toulouse school shootings, Hypercacher massacre), property destruction, and racist language has been increasing over the last several years. Jewish leaders perceive the Muslim population as intensifying antisemitism in France, mainly among Muslims of Arab or African heritage, but also this antisemitism is perceived as also growing among Caribbean islanders from former colonies.
In North America.
Conflicts between African Americans and Korean Americans (notably in the Los Angeles riots of 1992), by blacks towards Jews (such as the riots in Crown Heights in 1991), between new immigrant groups (such as Latinos), or towards whites.
There has been a long-running racial tension between African Americans and Mexican Americans. There have been several significant riots in California prisons in which Mexican American inmates and African Americans have specifically targeted each other based on racial reasons. There have been reports of racially motivated attacks against African Americans who have moved into neighborhoods occupied mostly by Mexican Americans, and vice versa.
In the late 1920s in California, there was animosity between the Filipinos and the Mexicans and between European Americans and Filipino Americans since they competed for the same jobs. Recently, there has also been an increase in racial violence between African immigrants and Blacks who have already lived in the country for generations.
Over 50 members of the Azusa 13 gang, associated with the Mexican Mafia, were indicted in 2011 for harassing and intimidating African Americans.
Anti-racism.
Anti-racism includes beliefs, actions, movements, and policies which are adopted or developed in order to oppose racism. In general, it promotes an egalitarian society in which people are not discriminated against on the basis of race. Movements such as the Civil Rights Movement and the Anti-Apartheid Movement were examples of anti-racist movements. Nonviolent resistance is sometimes embraced as an element of anti-racist movements, although this was not always the case. Hate crime laws, affirmative action, and bans on racist speech are also examples of government policy which is intended to suppress racism.
International Day for the Elimination of Racial Discrimination.
UNESCO marks March 21 as the yearly International Day for the Elimination of Racial Discrimination, in memory of the events that occurred on March 21, 1960 in Sharpeville, South Africa, where police killed demonstrators protesting against the apartheid regime.

</doc>
<doc id="25614" url="https://en.wikipedia.org/wiki?curid=25614" title="Race (human categorization)">
Race (human categorization)

Race, as a social construct, is a group of people who share similar and distinct physical characteristics. First used to refer to speakers of a common language and then to denote national affiliations, by the 17th century race began to refer to physical (i.e. phenotypical) traits. The term was often used in a general biological taxonomic sense, starting from the 19th century, to denote genetically differentiated human populations defined by phenotype.
Social conceptions and groupings of races vary over time, involving folk taxonomies that define of individuals based on perceived traits. Scientists consider biological essentialism obsolete, and generally discourage racial explanations for collective differentiation in both physical and behavioral traits.
Even though there is a broad scientific agreement that essentialist and typological conceptualizations of race are untenable, scientists around the world continue to conceptualize race in widely differing ways, some of which have essentialist implications. While some researchers sometimes use the concept of race to make distinctions among fuzzy sets of traits, others in the scientific community suggest that the idea of race often is used in a naive or simplistic way, and argue that, among humans, race has no taxonomic significance by pointing out that all living humans belong to the same species, "Homo sapiens", and subspecies, "Homo sapiens sapiens".
Since the second half of the 20th century, the associations of race with the ideologies and theories that grew out of the work of 19th-century anthropologists and physiologists has led to the use of the word "race" itself becoming problematic. Although still used in general contexts, "race" has often been replaced by other words which are less ambiguous and emotionally charged, such as "populations", "people(s)", "ethnic groups", or "communities", depending on context.
Complications and various definitions of the concept.
There is a wide consensus that the racial categories that are common in everyday usage are socially constructed, and that racial groups cannot be biologically defined. Nonetheless, some scholars argue that racial categories correlate with biological traits (e.g. phenotype), and that certain genetic markers have varying frequencies among human populations, some of which correspond more or less to traditional racial groupings. For this reason, there is no current consensus about whether racial categories can be considered to have significance for understanding human genetic variation.
When people define and talk about a particular conception of race, they create a social reality through which social categorization is achieved. In this sense, races are said to be social constructs. These constructs develop within various legal, economic, and sociopolitical contexts, and may be the effect, rather than the cause, of major social situations. While race is understood to be a social construct by many, most scholars agree that race has real material effects in the lives of people through institutionalized practices of preference and discrimination.
Socioeconomic factors, in combination with early but enduring views of race, have led to considerable suffering within disadvantaged racial groups. Racial discrimination often coincides with racist mindsets, whereby the individuals and ideologies of one group come to perceive the members of an outgroup as both racially defined and morally inferior. As a result, racial groups possessing relatively little power often find themselves excluded or oppressed, while hegemonic individuals and institutions are charged with holding racist attitudes. Racism has led to many instances of tragedy, including slavery and genocide.
In some countries, law enforcement uses race to profile suspects. This use of racial categories is frequently criticized for perpetuating an outmoded understanding of human biological variation, and promoting stereotypes. Because in some societies racial groupings correspond closely with patterns of social stratification, for social scientists studying social inequality, race can be a significant variable. As sociological factors, racial categories may in part reflect subjective attributions, self-identities, and social institutions.
Scholars continue to debate the degrees to which racial categories are biologically warranted and socially constructed, as well as the extent to which the realities of race must be acknowledged in order for society to comprehend and address racism adequately. Accordingly, the racial paradigms employed in different disciplines vary in their emphasis on biological reduction as contrasted with societal construction.
In the social sciences, theoretical frameworks such as racial formation theory and critical race theory investigate implications of race as social construction by exploring how the images, ideas and assumptions of race are expressed in everyday life. A large body of scholarship has traced the relationships between the historical, social production of race in legal and criminal language, and their effects on the policing and disproportionate incarceration of certain groups.
Historical origins of racial classification.
Groups of humans have always identified themselves as distinct from neighboring groups, but such differences have not always been understood to be natural, immutable and global. These features are the distinguishing features of how the concept of race is used today. In this way the idea of race as we understand it today came about during the historical process of exploration and conquest which brought Europeans into contact with groups from different continents, and of the ideology of classification and typology found in the natural sciences.
Race and colonialism.
The European concept of "race", along with many of the ideas now associated with the term, arose at the time of the scientific revolution, which introduced and privileged the study of natural kinds, and the age of European imperialism and colonization which established political relations between Europeans and peoples with distinct cultural and political traditions. As Europeans encountered people from different parts of the world, they speculated about the physical, social, and cultural differences among various human groups. The rise of the Atlantic slave trade, which gradually displaced an earlier trade in slaves from throughout the world, created a further incentive to categorize human groups in order to justify the subordination of African slaves. Drawing on Classical sources and upon their own internal interactions—for example, the hostility between the English and Irish powerfully influenced early European thinking about the differences between people—Europeans began to sort themselves and others into groups based on physical appearance, and to attribute to individuals belonging to these groups behaviors and capacities which were claimed to be deeply ingrained. A set of folk beliefs took hold that linked inherited physical differences between groups to inherited intellectual, behavioral, and moral qualities. Similar ideas can be found in other cultures, for example in China, where a concept often translated as "race" was associated with supposed common descent from the Yellow Emperor, and used to stress the unity of ethnic groups in China. Brutal conflicts between ethnic groups have existed throughout history and across the world.
Early taxonomic models.
The first post-Classical published classification of humans into distinct races seems to be François Bernier's "Nouvelle division de la terre par les différents espèces ou races qui l'habitent" ("New division of Earth by the different species or races which inhabit it"), published in 1684. In the 18th century the differences among human groups became a focus of scientific investigation. But the scientific classification of phenotypic variation was frequently coupled with racist ideas about innate predispositions of different groups, always attributing the most desirable features to the White, European race and arranging the other races along a continuum of progressively undesirable attributes. The 1735 classification of Carl Linnaeus, inventor of zoological taxonomy, divided the human race "Homo sapiens" into continental varieties of "europaeus", "asiaticus", "americanus", and "afer", each associated with a different humour: sanguine, melancholic, choleric, and phlegmatic, respectively. "Homo sapiens europaeus" was described as active, acute, and adventurous, whereas "Homo sapiens afer" was said to be crafty, lazy, and careless.
The 1775 treatise "The Natural Varieties of Mankind", by Johann Friedrich Blumenbach proposed five major divisions: the Caucasoid race, Mongoloid race, Ethiopian race (later termed Negroid, and not to be confused with the narrower Ethiopid race), American Indian race, and Malayan race, but he did not propose any hierarchy among the races. Blumenbach also noted the graded transition in appearances from one group to adjacent groups and suggested that "one variety of mankind does so sensibly pass into the other, that you cannot mark out the limits between them".
From the 17th through 19th centuries, the merging of folk beliefs about group differences with scientific explanations of those differences produced what one scholar has called an "ideology of race". According to this ideology, races are primordial, natural, enduring and distinct. It was further argued that some groups may be the result of mixture between formerly distinct populations, but that careful study could distinguish the ancestral races that had combined to produce admixed groups. Subsequent influential classifications by Georges Buffon, Petrus Camper and Christoph Meiners all classified "Negros" as inferior to Europeans. In the United States the racial theories of Thomas Jefferson were influential. He saw Africans as inferior to Whites especially in regards to their intellect, and imbued with unnatural sexual appetites, but described Native Americans as equals to whites.
Race and polygenism.
In the last two decades of the 18th century, the theory of polygenism, the belief that different races had evolved separately in each continent and shared no common ancestor, was advocated in England by historian Edward Long and anatomist Charles White, in Germany by ethnographers Christoph Meiners and Georg Forster, and in France by Julien-Joseph Virey. In the US, Samuel George Morton, Josiah Nott and Louis Agassiz promoted this theory in the mid-nineteenth century. Polygenism was popular and most widespread in the 19th century, culminating in the founding of the Anthropological Society of London (1863) during the period of the American Civil War, in opposition to the Ethnological Society, which had abolitionist sympathies.
Modern debate.
Models of human evolution.
Today, all humans are classified as belonging to the species "Homo sapiens" and sub-species "Homo sapiens sapiens". However, this is not the first species of homininae: the first species of genus "Homo", "Homo habilis", is theorized to have evolved in East Africa at least 2 million years ago, and members of this species populated different parts of Africa in a relatively short time. "Homo erectus" is theorized to have evolved more than 1.8 million years ago, and by 1.5 million years ago had spread throughout Europe and Asia. Virtually all physical anthropologists agree that "Archaic Homo sapiens" (A group including the possible species H. heidelbergensis, H. rhodesiensis and H. neanderthalensis) evolved out of African "Homo erectus" (("sensu lato") or "Homo ergaster").
Today anthropologists increasingly believe that anatomically modern humans ("Homo sapiens sapiens") evolved in North or East Africa from "H. heidelbergensis" and then migrated out of Africa, mixing with and replacing "H. heidelbergensis" and "H. neanderthalensis" populations throughout Europe and Asia, and "H. rhodesiensis" populations in Sub-Saharan Africa (a combination of the Out of Africa and Multiregional models).
Biological classification.
In the early 20th century, many anthropologists accepted and taught the belief that biologically distinct races were isomorphic with distinct linguistic, cultural, and social groups, while popularly applying that belief to the field of eugenics, in conjunction with a practice that is now called scientific racism. After the Nazi eugenics program, racial essentialism lost widespread popularity. Race anthropologists were pressured to acknowledge findings coming from studies of culture and population genetics, and to revise their conclusions about the sources of phenotypic variation. A significant number of modern anthropologists and biologists in the West came to view race as an invalid genetic or biological designation.
The first to challenge the concept of race on empirical grounds were the anthropologists Franz Boas, who provided evidence of phenotypic plasticity due to environmental factors, and Ashley Montagu, who relied on evidence from genetics. E. O. Wilson then challenged the concept from the perspective of general animal systematics, and further rejected the claim that "races" were equivalent to "subspecies".
According to Jonathan Marks,
The term "race" in biology is used with caution because it can be ambiguous. Generally, when it is used it is effectively a synonym of "subspecies". (For animals, the only taxonomic unit below the species level is usually the subspecies; there are narrower infraspecific ranks in botany, and "race" does not correspond directly with any of them.)
Population geneticists have debated whether the concept of "population" can provide a basis for a new conception of race. To do this, a working definition of population must be found. Surprisingly, there is no generally accepted concept of population that biologists use. Although the concept of population is central to ecology, evolutionary biology and conservation biology, most definitions of population rely on qualitative descriptions such as "a group of organisms of the same species occupying a particular space at a particular time". Waples and Gaggiotti identify two broad types of definitions for populations; those that fall into an "ecological paradigm", and those that fall into an "evolutionary paradigm". Examples of such definitions are:
Morphologically differentiated populations.
Traditionally, subspecies are seen as geographically isolated and genetically differentiated populations. That is, "the designation 'subspecies' is used to indicate an objective degree of microevolutionary divergence". One objection to this idea is that it does not specify what degree of differentiation is required. Therefore, any population that is somewhat biologically different could be considered a subspecies, even to the level of a local population. As a result, Templeton has argued that it is necessary to impose a threshold on the level of difference that is required for a population to be designated a subspecies.
This effectively means that populations of organisms must have reached a certain measurable level of difference to be recognised as subspecies.
Dean Amadon proposed in 1949 that subspecies would be defined according to the seventy-five percent rule which means that 75% of a population must lie outside 99% of the range of other populations for a given defining morphological character or a set of characters. The seventy-five percent rule still has defenders but other scholars argue that it should be replaced with ninety or ninety-five percent rule.
In 1978, Sewall Wright suggested that human populations that have long inhabited separated parts of the world should, in general, be considered different subspecies by the usual criterion that most individuals of such populations can be allocated correctly by inspection. Wright argued that it does not require a trained anthropologist to classify an array of Englishmen, West Africans, and Chinese with 100% accuracy by features, skin color, and type of hair despite so much variability within each of these groups that every individual can easily be distinguished from every other. However, it is customary to use the term race rather than subspecies for the major subdivisions of the human species as well as for minor ones.
On the other hand, in practice subspecies are often defined by easily observable physical appearance, but there is not necessarily any evolutionary significance to these observed differences, so this form of classification has become less acceptable to evolutionary biologists. Likewise this typological approach to race is generally regarded as discredited by biologists and anthropologists.
Because of the difficulty in classifying subspecies morphologically, many biologists have found the concept problematic, citing issues such as:
Sesardic argues that when several traits are analyzed at the same time, forensic anthropologists can classify a person's race with an accuracy of close to 100% based on only skeletal remains. This is discussed in a later section.
Ancestrally differentiated populations.
Cladistics is another method of classification. A clade is a taxonomic group of organisms consisting of a single common ancestor and all the descendants of that ancestor. Every creature produced by sexual reproduction has two immediate lineages, one maternal and one paternal. Whereas Carl Linnaeus established a taxonomy of living organisms based on anatomical similarities and differences, cladistics seeks to establish a taxonomy—the phylogenetic tree—based on genetic similarities and differences and tracing the process of acquisition of multiple characteristics by single organisms. Some researchers have tried to clarify the idea of race by equating it to the biological idea of the clade. Often mitochondrial DNA or Y chromosome sequences are used to study ancient human migration paths. These single-locus sources of DNA do not recombine and are inherited from a single parent. Individuals from the various continental groups tend to be more similar to one another than to people from other continents, and tracing either mitochondrial DNA or non-recombinant Y-chromosome DNA explains how people in one place may be largely derived from people in some remote location.
Often taxonomists prefer to use phylogenetic analysis to determine whether a population can be considered a subspecies. Phylogenetic analysis relies on the concept of derived characteristics that are not shared between groups, usually applying to populations that are allopatric (geographically separated) and therefore discretely bounded. This would make a subspecies, evolutionarily speaking, a clade – a group with a common evolutionary ancestor population. The smooth gradation of human genetic variation in general tends to rule out any idea that human population groups can be considered monophyletic (cleanly divided), as there appears to always have been considerable gene flow between human populations. Rachel Caspari (2003) have argued that clades are by definition monophyletic groups (a taxon that includes "all" descendants of a given ancestor) and since no groups currently regarded as races are monophyletic, none of those groups can be clades.
For the anthropologists Lieberman and Jackson (1995), however, there are more profound methodological and conceptual problems with using cladistics to support concepts of race. They claim that "the molecular and biochemical proponents of this model explicitly use racial categories "in their initial grouping of samples"". For example, the large and highly diverse macroethnic groups of East Indians, North Africans, and Europeans are presumptively grouped as Caucasians prior to the analysis of their DNA variation. This is claimed to limit and skew interpretations, obscure other lineage relationships, deemphasize the impact of more immediate clinal environmental factors on genomic diversity, and can cloud our understanding of the true patterns of affinity. They argue that however significant the empirical research, these studies use the term race in conceptually imprecise and careless ways. They suggest that the authors of these studies find support for racial distinctions only because they began by assuming the validity of race. "For empirical reasons we prefer to place emphasis on clinal variation, which recognizes the existence of adaptive human hereditary variation and simultaneously stresses that such variation is not found in packages that can be labeled "races"."
These scientists do not dispute the importance of cladistic research, only its retention of the word race, when reference to populations and clinal gradations are more than adequate to describe the results.
Clines.
One crucial innovation in reconceptualizing genotypic and phenotypic variation was the anthropologist C. Loring Brace's observation that such variations, insofar as it is affected by natural selection, slow migration, or genetic drift, are distributed along geographic gradations or clines. In part this is due to isolation by distance. This point called attention to a problem common to phenotype-based descriptions of races (for example, those based on hair texture and skin color): they ignore a host of other similarities and differences (for example, blood type) that do not correlate highly with the markers for race. Thus, anthropologist Frank Livingstone's conclusion, that since clines cross racial boundaries, "there are no races, only clines".
In a response to Livingstone, Theodore Dobzhansky argued that when talking about race one must be attentive to how the term is being used: "I agree with Dr. Livingstone that if races have to be 'discrete units', then there are no races, and if 'race' is used as an 'explanation' of the human variability, rather than vice versa, then the explanation is invalid." He further argued that one could use the term race if one distinguished between "race differences" and "the race concept". The former refers to any distinction in gene frequencies between populations; the latter is "a matter of judgment". He further observed that even when there is clinal variation, "Race differences are objectively ascertainable biological phenomena ... but it does not follow that racially distinct populations must be given racial (or subspecific) labels." In short, Livingstone and Dobzhansky agree that there are genetic differences among human beings; they also agree that the use of the race concept to classify people, and how the race concept is used, is a matter of social convention. They differ on whether the race concept remains a meaningful and useful social convention.
In 1964, the biologists Paul Ehrlich and Holm pointed out cases where two or more clines are distributed discordantly—for example, melanin is distributed in a decreasing pattern from the equator north and south; frequencies for the haplotype for beta-S hemoglobin, on the other hand, radiate out of specific geographical points in Africa. As the anthropologists Leonard Lieberman and Fatimah Linda Jackson observed, "Discordant patterns of heterogeneity falsify any description of a population as if it were genotypically or even phenotypically homogeneous".
Patterns such as those seen in human physical and genetic variation as described above, have led to the consequence that the number and geographic location of any described races is highly dependent on the importance attributed to, and quantity of, the traits considered. Scientists discovered a skin-lighting mutation that partially accounts for the appearance of Light skin in humans (people who migrated out of Africa northward into what is now Europe) which they estimate occurred 20,000 to 50,000 years ago. The East Asians owe their relatively light skin to different mutations. On the other hand, the greater the number of traits (or alleles) considered, the more subdivisions of humanity are detected, since traits and gene frequencies do not always correspond to the same geographical location. Or as put it:
More recent genetic studies indicate that skin color may change radically over as few as 100 generations, or about 2,500 years, given the influence of the environment.
Genetically differentiated populations.
Another way to look at differences between populations is to measure genetic differences rather than physical differences between groups. The mid-20th-century anthropologist William C. Boyd defined race as: "A population which differs significantly from other populations in regard to the frequency of one or more of the genes it possesses. It is an arbitrary matter which, and how many, gene loci we choose to consider as a significant 'constellation'". Leonard Lieberman and Rodney Kirk have pointed out that "the paramount weakness of this statement is that if one gene can distinguish races then the number of races is as numerous as the number of human couples reproducing." Moreover, the anthropologist Stephen Molnar has suggested that the discordance of clines inevitably results in a multiplication of races that renders the concept itself useless. The Human Genome Project states "People who have lived in the same geographic region for many generations may have some alleles in common, but no allele will be found in all members of one population and in no members of any other."
Fixation index.
The population geneticist Sewall Wright developed one way of measuring genetic differences between populations known as the Fixation index, which is often abbreviated to "F"ST. This statistic is often used in taxonomy to compare differences between any two given populations by measuring the genetic differences among and between populations for individual genes, or for many genes simultaneously. It is often stated that the fixation index for humans is about 0.15. This translates to an estimated 85% of the variation measured in the overall human population is found within individuals of the same population, and about 15% of the variation occurs between populations. These estimates imply that any two individuals from different populations are almost as likely to be more similar to each other than either is to a member of their own group. Richard Lewontin, who affirmed these ratios, thus concluded neither "race" nor "subspecies" were appropriate or useful ways to describe human populations. However, others have noticed that group variation was relatively similar to the variation observed in other mammalian species.
Wright himself believed that values >0.25 represent very great genetic variation and that an "F"ST of 0.15–0.25 represented great variation. However, about 5% of human variation occurs between populations within continents, therefore "F"ST values between continental groups of humans (or races) of as low as 0.1 (or possibly lower) have been found in some studies, suggesting more moderate levels of genetic variation. Graves (1996) has countered that "F"ST should not be used as a marker of subspecies status, as the statistic is used to measure the degree of differentiation between populations, although see also Wright (1978).
In an ongoing debate, some geneticists argue that race is neither a meaningful concept nor a useful heuristic device, and even that genetic differences among groups are biologically meaningless, because more genetic variation exists within such races than among them, and that racial traits overlap without discrete boundaries.
Jeffrey Long and Rick Kittles give a long critique of the application of "F"ST to human populations in their 2003 paper "Human Genetic Diversity and the Nonexistence of Biological Races". They find that the figure of 85% is misleading because it implies that all human populations contain on average 85% of all genetic diversity. They claim that this does not correctly reflect human population history, because it treats all human groups as independent. A more realistic portrayal of the way human groups are related is to understand that some human groups are parental to other groups and that these groups represent paraphyletic groups to their descent groups. For example, under the recent African origin theory the human population in Africa is paraphyletic to all other human groups because it represents the ancestral group from which all non-African populations derive, but more than that, non-African groups only derive from a small non-representative sample of this African population. This means that all non-African groups are more closely related to each other and to some African groups (probably east Africans) than they are to others, and further that the migration out of Africa represented a genetic bottleneck, with much of the diversity that existed in Africa not being carried out of Africa by the emigrating groups. This view produces a version of human population movements that do not result in all human populations being independent; but rather, produces a series of dilutions of diversity the further from Africa any population lives, each founding event representing a genetic subset of its parental population. Long and Kittles find that rather than 85% of human genetic diversity existing in all human populations, about 100% of human diversity exists in a single African population, whereas only about 70% of human genetic diversity exists in a population derived from New Guinea. Long and Kittles argued that this still produces a global human population that is genetically homogeneous compared to other mammalian populations.
Cluster analysis.
In his 2003 paper, "", A. W. F. Edwards argued that rather than using a locus-by-locus analysis of variation to derive taxonomy, it is possible to construct a human classification system based on characteristic genetic patterns, or "clusters" inferred from multilocus genetic data. Geographically based human studies since have shown that such genetic clusters can be derived from analyzing of a large number of loci which can assort individuals sampled into groups analogous to traditional continental racial groups. Joanna Mountain and Neil Risch cautioned that while genetic clusters may one day be shown to correspond to phenotypic variations between groups, such assumptions were premature as the relationship between genes and complex traits remains poorly understood. However, Risch denied such limitations render the analysis useless: "Perhaps just using someone's actual birth year is not a very good way of measuring age. Does that mean we should throw it out? ... Any category you come up with is going to be imperfect, but that doesn't preclude you from using it or the fact that it has utility."
Early human genetic cluster analysis studies were conducted with samples taken from ancestral population groups living at extreme geographic distances from each other. It was thought that such large geographic distances would maximize the genetic variation between the groups sampled in the analysis and thus maximize the probability of finding cluster patterns unique to each group. In light of the historically recent acceleration of human migration (and correspondingly, human gene flow) on a global scale, further studies were conducted to judge the degree to which genetic cluster analysis can pattern ancestrally identified groups as well as geographically separated groups. One such study looked at a large multiethnic population in the United States, and "detected only modest genetic differentiation between different current geographic locales within each race/ethnicity group. Thus, ancient geographic ancestry, which is highly correlated with self-identified race/ethnicity—as opposed to current residence—is the major determinant of genetic structure in the U.S. population." ()
Anthropologists such as C. Loring Brace, the philosophers Jonathan Kaplan and Rasmus Winther, and the geneticist Joseph Graves, have argued that while there it is certainly possible to find biological and genetic variation that corresponds roughly to the groupings normally defined as "continental races", this is true for almost all geographically distinct populations. The cluster structure of the genetic data is therefore dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clustering would be different. Weiss and Fullerton have noted that if one sampled only Icelanders, Mayans and Maoris, three distinct clusters would form and all other populations could be described as being clinally composed of admixtures of Maori, Icelandic and Mayan genetic materials. Kaplan and Winther therefore argue that, seen in this way, both Lewontin and Edwards are right in their arguments. They conclude that while racial groups are characterized by different allele frequencies, this does not mean that racial classification is a natural taxonomy of the human species, because multiple other genetic patterns can be found in human populations that crosscut racial distinctions. Moreover, the genomic data underdetermines whether one wishes to see subdivisions (i.e., splitters) or a continuum (i.e., lumpers). Under Kaplan and Winther's view, racial groupings are objective social constructions (see Mills 1998) that have conventional biological reality only insofar as the categories are chosen and constructed for pragmatic scientific reasons. In earlier work, Winther had identified "diversity partitioning" and "clustering analysis" as two separate methodologies, with distinct questions, assumptions, and protocols. Each is also associated with opposing ontological consequences vis-a-vis the metaphysics of race.
Social constructions.
As anthropologists and other evolutionary scientists have shifted away from the language of race to the term "population" to talk about genetic differences, historians, cultural anthropologists and other social scientists re-conceptualized the term "race" as a cultural category or social construct—a particular way that some people talk about themselves and others.
Many social scientists have replaced the word race with the word "ethnicity" to refer to self-identifying groups based on beliefs concerning shared culture, ancestry and history. Alongside empirical and conceptual problems with "race", following the Second World War, evolutionary and social scientists were acutely aware of how beliefs about race had been used to justify discrimination, apartheid, slavery, and genocide. This questioning gained momentum in the 1960s during the U.S. civil rights movement and the emergence of numerous anti-colonial movements worldwide. They thus came to believe that race itself is a social construct, a concept that was believed to correspond to an objective reality but which was believed in because of its social functions.
Craig Venter and Francis Collins of the National Institute of Health jointly made the announcement of the mapping of the human genome in 2000. Upon examining the data from the genome mapping, Venter realized that although the genetic variation within the human species is on the order of 1–3% (instead of the previously assumed 1%), the types of variations do not support notion of genetically defined races. Venter said, "Race is a social concept. It's not a scientific one. There are no bright lines (that would stand out), if we could compare all the sequenced genomes of everyone on the planet." "When we try to apply science to try to sort out these social differences, it all falls apart."
Stephan Palmié asserted that race "is not a thing but a social relation"; or, in the words of Katya Gibel Mevorach, "a metonym", "a human invention whose criteria for differentiation are neither universal nor fixed but have always been used to manage difference." As such, the use of the term "race" itself must be analyzed. Moreover, they argue that biology will not explain why or how people use the idea of race: History and social relationships will.
Imani Perry, a professor in the Center for African American Studies at Princeton University, has made significant contributions to how we define race in America today. Perry's work focuses on how race is experienced. Perry tells us that race "is produced by social arrangements and political decision making." Perry explains race more in stating, "race is something that happens, rather than something that is. It is dynamic, but it holds no objective truth."
The theory that race is merely a social construct has been challenged by the findings of researchers at the Stanford University School of Medicine, published in the "American Journal of Human Genetics" as "Genetic Structure, Self-Identified Race/Ethnicity, and Confounding in Case-Control Association Studies". One of the researchers, Neil Risch, noted: "we looked at the correlation between genetic structure on microsatellite markers versus self-description, we found 99.9% concordance between the two. We actually had a higher discordance rate between self-reported sex and markers on the X chromosome! So you could argue that sex is also a problematic category. And there are differences between sex and gender; self-identification may not be correlated with biology perfectly. And there is sexism."
Brazil.
Compared to 19th-century United States, 20th-century Brazil was characterized by a perceived relative absence of sharply defined racial groups. According to anthropologist Marvin Harris, this pattern reflects a different history and different social relations.
Basically, race in Brazil was "biologized", but in a way that recognized the difference between ancestry (which determines genotype) and phenotypic differences. There, racial identity was not governed by rigid descent rule, such as the one-drop rule, as it was in the United States. A Brazilian child was never automatically identified with the racial type of one or both parents, nor were there only a very limited number of categories to choose from, to the extent that full siblings can pertain to different racial groups.
Over a dozen racial categories would be recognized in conformity with all the possible combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and not one category stands significantly isolated from the rest. That is, race referred preferentially to appearance, not heredity, and appearance is a poor indication of ancestry, because only a few genes are responsible for someone's skin color and traits: a person who is considered white may have more African ancestry than a person who is considered black, and the reverse can be also true about European ancestry. The complexity of racial classifications in Brazil reflects the extent of miscegenation in Brazilian society, a society that remains highly, but not strictly, stratified along color lines. These socioeconomic factors are also significant to the limits of racial lines, because a minority of "pardos", or brown people, are likely to start declaring themselves white or black if socially upward, and being seen as relatively "whiter" as their perceived social status increases (much as in other regions of Latin America).
Fluidity of racial categories aside, the "biologification" of race in Brazil referred above would match contemporary concepts of race in the United States quite closely, though, if Brazilians are supposed to choose their race as one among, Asian and Indigenous apart, three IBGE's census categories. While assimilated Amerindians and people with very high quantities of Amerindian ancestry are usually grouped as "caboclos", a subgroup of "pardos" which roughly translates as both mestizo and hillbilly, for those of lower quantity of Amerindian descent a higher European genetic contribution is expected to be grouped as a "pardo". In several genetic tests, people with less than 60-65% of European descent and 5-10% of Amerindian descent usually cluster with Afro-Brazilians (as reported by the individuals), or 6.9% of the population, and those with about 45% or more of Subsaharan contribution most times do so (in average, Afro-Brazilian DNA was reported to be about 50% Subsaharan African, 37% European and 13% Amerindian).
If a more consistent report with the genetic groups in the gradation of miscegenation is to be considered (e.g. that would not cluster people with a balanced degree of African and non-African ancestry in the black group instead of the multiracial one, unlike elsewhere in Latin America where people of high quantity of African descent tend to classify themselves as mixed), more people would report themselves as white and "pardo" in Brazil (47.7% and 42.4% of the population as of 2010, respectively), because by research its population is believed to have between 65 and 80% of autosomal European ancestry, in average (also >35% of European mt-DNA and >95% of European Y-DNA).
This is not surprising, though: While the greatest number of slaves imported from Africa were sent to Brazil, totalizing roughly 3.5 million people, they lived in such miserable conditions that male African Y-DNA there is significantly rare due to the lack of resources and time involved with raising of children, so that most African descent originarily came from relations between white masters and female slaves. From the last decades of the Empire until the 1950s, the proportion of the white population increased significantly while Brazil welcomed 5.5 million immigrants between 1821 and 1932, not much behind its neighbor Argentina with 6.4 million, and it received more European immigrants in its colonial history than the United States. Between 1500 and 1760, 700.000 Europeans settled in Brazil, while 530.000 Europeans settled in the United States for the same given time. Thus, the historical construction of race in Brazilian society dealt primarily with gradations between persons of majoritarily European ancestry and little minority groups with otherwise lower quantity therefrom in recent times.
European Union.
According to European Council:
The European Union uses the terms racial origin and ethnic origin synonymously in its documents and according to it "the use of the term 'racial origin' in this directive does not imply an acceptance of such theories". Haney López warns that using "race" as a category within the law tends to legitimize its existence in the popular imagination. In the diverse geographic context of Europe, ethnicity and ethnic origin are arguably more resonant and are less encumbered by the ideological baggage associated with "race". In European context, historical resonance of "race" underscores its problematic nature. In some states, it is strongly associated with laws promulgated by the Nazi and Fascist governments in Europe during the 1930s and 1940s. Indeed, in 1996, the European Parliament adopted a resolution stating that "the term should therefore be avoided in all official texts".
The concept of racial origin relies on the notion that human beings can be separated into biologically distinct "races", an idea generally rejected by the scientific community. Since all human beings belong to the same species, the ECRI (European Commission against Racism and Intolerance) rejects theories based on the existence of different "races". However, in its Recommendation ECRI uses this term in order to ensure that those persons who are generally and erroneously perceived as belonging to "another race" are not excluded from the protection provided for by the legislation. The law claims to reject the existence of "race", yet penalize situations where someone is treated less favourably on this ground.
France.
Since the end of the Second World War, France has become an ethnically diverse country. Today, approximately five percent of the French population is non-European and non-white. This does not approach the number of non-white citizens in the United States (roughly 28–37%, depending on how Latinos are classified; see Demographics of the United States). Nevertheless, it amounts to at least three million people, and has forced the issues of ethnic diversity onto the French policy agenda. France has developed an approach to dealing with ethnic problems that stands in contrast to that of many advanced, industrialized countries. Unlike the United States, Britain, or even the Netherlands, France maintains a "color-blind" model of public policy. This means that it targets virtually no policies directly at racial or ethnic groups. Instead, it uses geographic or class criteria to address issues of social inequalities. It has, however, developed an extensive anti-racist policy repertoire since the early 1970s. Until recently, French policies focused primarily on issues of hate speech—going much further than their American counterparts—and relatively less on issues of discrimination in jobs, housing, and in provision of goods and services.
United States.
In the United States, views of race that see racial groups as defined genetically are common in the biological sciences although controversial, whereas the social constructionist view is dominant in the social sciences.
The immigrants to the Americas came from every region of Europe, Africa, and Asia. They mixed among themselves and with the indigenous inhabitants of the continent. In the United States most people who self-identify as African–American have some European ancestors, while many people who identify as European American have some African or Amerindian ancestors.
Since the early history of the United States, Amerindians, African–Americans, and European Americans have been classified as belonging to different races. Efforts to track mixing between groups led to a proliferation of categories, such as mulatto and octoroon. The criteria for membership in these races diverged in the late 19th century. During Reconstruction, increasing numbers of Americans began to consider anyone with "one drop" of known "Black blood" to be Black, regardless of appearance.3 By the early 20th century, this notion was made statutory in many states.4 Amerindians continue to be defined by a certain percentage of "Indian blood" (called "blood quantum"). To be White one had to have perceived "pure" White ancestry. The one-drop rule or hypodescent rule refers to the convention of defining a person as racially black if he or she has any known African ancestry. This rule meant that those that were mixed race but with some discernible African ancestry were defined as black. The one-drop rule is specific to not only those with African ancestry but to the United States, making it a particularly African-American experience.
The decennial censuses conducted since 1790 in the United States created an incentive to establish racial categories and fit people into these categories.
The term "Hispanic" as an ethnonym emerged in the 20th century with the rise of migration of laborers from the Spanish-speaking countries of Latin America to the United States. Today, the word "Latino" is often used as a synonym for "Hispanic". The definitions of both terms are non-race specific, and include people who consider themselves to be of distinct races (Black, White, Amerindian, Asian, and mixed groups). However, there is a common misconception in the US that Hispanic/Latino is a race or sometimes even that national origins such as Mexican, Cuban, Colombian, Salvadoran, etc. are races. In contrast to "Latino" or "Hispanic", "Anglo" refers to non-Hispanic White Americans or non-Hispanic European Americans, most of whom speak the English language but are not necessarily of English descent.
Views across disciplines over time.
In Poland, the race concept was rejected by 25 percent of anthropologists in 2001, although: "Unlike the U.S. anthropologists, Polish anthropologists tend to regard race as a term without taxonomic value, often as a substitute for population."
Wang, Štrkalj et al. (2003) examined the use of race as a biological concept in research papers published in China's only biological anthropology journal, "Acta Anthropologica Sinica". The study showed that the race concept was widely used among Chinese anthropologists. In a 2007 review paper, Štrkalj suggested that the stark contrast of the racial approach between the United States and China was due to the fact that race is a factor for social cohesion among the ethnically diverse people of China, whereas "race" is a very sensitive issue in America and the racial approach is considered to undermine social cohesion - with the result that in the socio-political context of US academics scientists are encouraged not to use racial categories, whereas in China they are encouraged to use them.
Lieberman et al. in a 2004 study researched the acceptance of race as a concept among anthropologists in the United States, Canada, the Spanish speaking areas, Europe, Russia and China. Rejection of race ranged from high to low, with the highest rejection rate in the United States and Canada, a moderate rejection rate in Europe, and the lowest rejection rate in Russia and China. Methods used in the studies reported included questionnaires and content analysis.
Kaszycka et al. (2009) in 2002–2003 surveyed European anthropologists' opinions toward the biological race concept. Three factors, country of academic education, discipline, and age, were found to be significant in differentiating the replies. Those educated in Western Europe, physical anthropologists, and middle-aged persons rejected race more frequently than those educated in Eastern Europe, people in other branches of science, and those from both younger and older generations." The survey shows that the views on race are sociopolitically (ideologically) influenced and highly dependent on education."
United States.
One result of debates over the meaning and validity of the concept of race is that the current literature across different disciplines regarding human variation lacks consensus, though within some fields, such as some branches of anthropology, there is strong consensus. Some studies use the word race in its early essentialist taxonomic sense. Many others still use the term race, but use it to mean a population, clade, or haplogroup. Others eschew the concept of race altogether, and use the concept of population as a less problematic unit of analysis.
Eduardo Bonilla-Silva, Sociology professor at Duke University, remarks, "I contend that racism is, more than anything else, a matter of group power; it is about a dominant racial group (whites) striving to maintain its systemic advantages and minorities fighting to subvert the racial status quo." The types of practices that take place under this new color-blind racism is subtle, institutionalized, and supposedly not racial. Color-blind racism thrives on the idea that race is no longer an issue in the United States. There are contradictions between the alleged color-blindness of most whites and the persistence of a color-coded system of inequality.
U.S. anthropology.
The concept of biological race has declined significantly in frequency of use in physical anthropology in the United States during the 20th century. A majority of physical anthropologists in the United States have rejected the concept of biological races. Since 1932, an increasing number of college textbooks introducing physical anthropology have rejected race as a valid concept: from 1932 to 1976, only seven out of thirty-two rejected race; from 1975 to 1984, thirteen out of thirty-three rejected race; from 1985 to 1993, thirteen out of nineteen rejected race. According to one academic journal entry, where 78 percent of the articles in the 1931 "Journal of Physical Anthropology" employed these or nearly synonymous terms reflecting a bio-race paradigm, only 36 percent did so in 1965, and just 28 percent did in 1996.
The "Statement on 'Race'" (1998) composed by a select committee of anthropologists and issued by the executive board of the American Anthropological Association as a statement they "believe [...] represents generally the contemporary thinking and scholarly positions of a majority of anthropologists", declares:
A survey, taken in 1985 , asked 1,200 American scientists how many disagree with the following proposition: "There are biological races in the species "Homo sapiens"." The responses were for anthropologists:
The figure for physical anthropologists at PhD granting departments was slightly higher, rising from 41% to 42%, with 50% agreeing. Lieberman's study also showed that more women reject the concept of race than men. This survey, however, did not specify any particular definition of race (although it did clearly specify "biological race" within the "species" "Homo sapiens"); it is difficult to say whether those who supported the statement thought of race in taxonomic or population terms.
The same survey, taken in 1999, showed the following changing results for anthropologists:
However, a line of research conducted by Cartmill (1998) seemed to limit the scope of Lieberman's finding that there was "a significant degree of change in the status of the race concept". Goran Štrkalj has argued that this may be because Lieberman and collaborators had looked at all the members of the American Anthropological Association irrespective of their field of research interest, while Cartmill had looked specifically at biological anthropologists interested in human variation.
According to the 2000 edition of a popular physical anthropology textbook, forensic anthropologists are overwhelmingly in support of the idea of the basic biological reality of human races. Forensic physical anthropologist and professor George W. Gill has said that the idea that race is only skin deep "is simply not true, as any experienced forensic anthropologist will affirm" and "Many morphological features tend to follow geographic boundaries coinciding often with climatic zones. This is not surprising since the selective forces of climate are probably the primary forces of nature that have shaped human races with regard not only to skin color and hair form but also the underlying bony structures of the nose, cheekbones, etc. (For example, more prominent noses humidify air better.)" While he can see good arguments for both sides, the complete denial of the opposing evidence "seems to stem largely from socio-political motivation and not science at all". He also states that many biological anthropologists see races as real yet "not one introductory textbook of physical anthropology even presents that perspective as a possibility. In a case as flagrant as this, we are not dealing with science but rather with blatant, politically motivated censorship".
In partial response to Gill's statement, Professor of Biological Anthropology C. Loring Brace argues that the reason laymen and biological anthropologists can determine the geographic ancestry of an individual can be explained by the fact that biological characteristics are clinally distributed across the planet, and that does not translate into the concept of race. He states: 
"Race" is still sometimes used within forensic anthropology (when analyzing skeletal remains), biomedical research, and race-based medicine. Brace has criticized this, the practice of forensic anthropologists for using the controversial concept "race" out of convention when they in fact should be talking about regional ancestry. He argues that while forensic anthropologists can determine that a skeletal remain comes from a person with ancestors in a specific region of Africa, categorizing that skeletal as being "black" is a socially constructed category that is only meaningful in the particular context of the United States, and which is not itself scientifically valid.
Other fields.
In the same 1985 survey , 16% of the surveyed biologists and 36% of the surveyed developmental psychologists disagreed with the proposition: "There are biological races in the species "Homo sapiens"."
The authors of the study also examined 77 college textbooks in biology and 69 in physical anthropology published between 1932 and 1989. Physical anthropology texts argued that biological races exist until the 1970s, when they began to argue that races do not exist. In contrast, biology textbooks did not undergo such a reversal but many instead dropped their discussion of race altogether. The authors attributed this to biologists trying to avoid discussing the political implications of racial classifications, instead of discussing them, and to the ongoing discussions in biology about the validity of the concept "subspecies". The authors also noted that some widely used textbooks in biology such as Douglas J. Futuyama's 1986 "Evolutionary Biology" had abandoned the race concept, "The concept of race, masking the overwhelming genetic similarity of all peoples and the mosaic patterns of variation that do not correspond to racial divisions, is not only socially dysfunctional but is biologically indefensible as well (pp. 5 18-5 19)." 
A 1994 examination of 32 English sport/exercise science textbooks found that 7 (21.9%) claimed that there are biophysical differences due to race that might explain differences in sports performance, 24 (75%) did not mention nor refute the concept, and 1 (3.12%) expressed caution with the idea.
In February 2001, the editors of "Archives of Pediatrics and Adolescent Medicine" asked "authors to not use race and ethnicity when there is no biological, scientific, or sociological reason for doing so." The editors also stated that "analysis by race and ethnicity has become an analytical knee-jerk reflex." "Nature Genetics" now ask authors to "explain why they make use of particular ethnic groups or populations, and how classification was achieved."
Morning (2008) looked at high school biology textbooks during the 1952-2002 period and initially found a similar pattern with only 35% directly discussing race in the 1983–92 period from initially 92% doing so. However, this has increased somewhat after this to 43%. More indirect and brief discussions of race in the context of medical disorders have increased from none to 93% of textbooks. In general, the material on race has moved from surface traits to genetics and evolutionary history. The study argues that the textbooks' fundamental message about the existence of races has changed little.
Gissis (2008) examined several important American and British journals in genetics, epidemiology and medicine for their content during the 1946-2003 period. He wrote that "Based upon my findings I argue that the category of race only "seemingly" disappeared from scientific discourse after World War II and has had a "fluctuating yet continuous use" during the time span from 1946 to 2003, and has even "become more pronounced from the early 1970s on"".
33 health services researchers from differing geographic regions were interviewed in a 2008 study. The researchers recognized the problems with racial and ethnic variables but the majority still believed these variables were necessary and useful.
A 2010 examination of 18 widely used English anatomy textbooks found that they all represented human biological variation in superficial and outdated ways, many of them making use of the race concept in ways that were current in 1950s anthropology. The authors recommended that anatomical education should describe human anatomical variation in more detail and rely on newer research that demonstrates the inadequacies of simple racial typologies.
Political and practical uses.
Biomedicine.
In the United States, federal government policy promotes the use of racially categorized data to identify and address health disparities between racial or ethnic groups. In clinical settings, race has sometimes been considered in the diagnosis and treatment of medical conditions. Doctors have noted that some medical conditions are more prevalent in certain racial or ethnic groups than in others, without being sure of the cause of those differences. Recent interest in race-based medicine, or race-targeted pharmacogenomics, has been fueled by the proliferation of human genetic data which followed the decoding of the human genome in the first decade of the twenty-first century. There is an active debate among biomedical researchers about the meaning and importance of race in their research. Proponents of the use of racial categories in biomedicine argue that continued use of racial categorizations in biomedical research and clinical practice makes possible the application of new genetic findings, and provides a clue to diagnosis.
Other researchers point out that finding a difference in disease prevalence between two socially defined groups does not necessarily imply genetic causation of the difference. They suggest that medical practices should maintain their focus on the individual rather than an individual's membership to any group. They argue that overemphasizing genetic contributions to health disparities carries various risks such as reinforcing stereotypes, promoting racism or ignoring the contribution of non-genetic factors to health disparities. International epidemiological data show that living conditions rather than race make the biggest difference in health outcomes even for diseases that have "race-specific" treatments. Some studies have found that patients are reluctant to accept racial categorization in medical practice.
Law enforcement.
In an attempt to provide general descriptions that may facilitate the job of law enforcement officers seeking to apprehend suspects, the United States FBI employs the term "race" to summarize the general appearance (skin color, hair texture, eye shape, and other such easily noticed characteristics) of individuals whom they are attempting to apprehend. From the perspective of law enforcement officers, it is generally more important to arrive at a description that will readily suggest the general appearance of an individual than to make a scientifically valid categorization by DNA or other such means. Thus, in addition to assigning a wanted individual to a racial category, such a description will include: height, weight, eye color, scars and other distinguishing characteristics.
Criminal justice agencies in England and Wales use at least two separate racial/ethnic classification systems when reporting crime, as of 2010. One is the system used in the 2001 Census when individuals identify themselves as belonging to a particular ethnic group: W1 (White-British), W2 (White-Irish), W9 (Any other white background); M1 (White and black Caribbean), M2 (White and black African), M3 (White and Asian), M9 (Any other mixed background); A1 (Asian-Indian), A2 (Asian-Pakistani), A3 (Asian-Bangladeshi), A9 (Any other Asian background); B1 (Black Caribbean), B2 (Black African), B3 (Any other black background); O1 (Chinese), O9 (Any other). The other is categories used by the police when they visually identify 
someone as belonging to an ethnic group, e.g. at the time of a stop and search or an arrest: White – North European (IC1), White – South European (IC2), Black (IC3), Asian (IC4), Chinese, Japanese, or South East Asian (IC5), Middle Eastern (IC6), and Unknown (IC0). "IC" stands for "Identification Code;" these items are also referred to as Phoenix classifications. Officers are instructed to "record the response that has been given" even if the person gives an answer which may be incorrect; their own perception of the person's ethnic background is recorded separately. Comparability of the information being recorded by officers was brought into question by the Office for National Statistics (ONS) in September 2007, as part of its Equality Data Review; one problem cited was the number of reports that contained an ethnicity of "Not Stated."
In many countries, such as France, the state is legally banned from maintaining data based on race, which often makes the police issue wanted notices to the public that include labels like "dark skin complexion", etc.
In the United States, the practice of racial profiling has been ruled to be both unconstitutional and a violation of civil rights. There is active debate regarding the cause of a marked correlation between the recorded crimes, punishments meted out, and the country's populations. Many consider "de facto" racial profiling an example of institutional racism in law enforcement. The history of misuse of racial categories to impact adversely one or more groups and/or to offer protection and advantage to another has a clear impact on debate of the legitimate use of known phenotypical or genotypical characteristics tied to the presumed race of both victims and perpetrators by the government.
Mass incarceration in the United States disproportionately impacts African American and Latino communities. Michelle Alexander, author of The New Jim Crow: Mass Incarceration in the Age of Colorblindness (2010), argues that mass incarceration is best understood as not only a system of overcrowded prisons. Mass incarceration is also, "the larger web of laws, rules, policies, and customs that control those labeled criminals both in and out of prison." She defines it further as "a system that locks people not only behind actual bars in actual prisons, but also behind virtual bars and virtual walls", illustrating the second-class citizenship that is imposed on a disproportionate number of people of color, specifically African-Americans. She compares mass incarceration to Jim Crow laws, stating that both work as racial caste systems.
Recent work using DNA cluster analysis to determine race background has been used by some criminal investigators to narrow their search for the identity of both suspects and victims. Proponents of DNA profiling in criminal investigations cite cases where leads based on DNA analysis proved useful, but the practice remains controversial among medical ethicists, defense lawyers and some in law enforcement.
Forensic anthropology.
Similarly, forensic anthropologists draw on highly heritable morphological features of human remains (e.g. cranial measurements) to aid in the identification of the body, including in terms of race. In a 1992 article, anthropologist Norman Sauer noted that anthropologists had generally abandoned the concept of race as a valid representation of human biological diversity, except for forensic anthropologists. He asked, "If races don't exist, why are forensic anthropologists so good at identifying them?" He concluded:
In a different approach, anthropologist C. Loring Brace said:
In association with a NOVA program in 2000 about race, he wrote an essay opposing use of the term.
Commercial determination of ancestry.
New research in molecular genetics, and the marketing of genetic identities through the analysis of one's Y chromosome, mtDNA, or autosomal DNA to the general public in the form of "Personalized Genetic Histories" (PGH) has caused debate.
Typically, a consumer of a commercial PGH service sends in a sample of DNA, which is analyzed by molecular biologists, and receives a report on ancestry. Shriver and Kittles remarked:
They noted that the general public was increasingly interested in such tests despite their lack of knowledge in some cases of what the results represent.
Through these reports, advances in molecular genetics are used to create or confirm stories individuals have about social identities. Abu el-Haj argued that genetic lineages, like older notions of race, suggest some idea of biological relatedness. But, unlike older notions of race, they are not directly connected to claims about human behaviour or character. She said that "postgenomics does seem to be giving race a new lease on life."
Abu el-Haj argues that genomics and the mapping of lineages and clusters liberates "the new racial science from the older one by disentangling ancestry from culture and capacity." As an example, she refers to recent work by Hammer "et al.", which aimed to test the claim that present-day Jews are more closely related to one another than to neighbouring non-Jewish populations. Hammer "et al." found that the degree of genetic similarity among Jews shifted depending on the locus investigated, and suggested that this was the result of natural selection acting on particular loci. They focused on the non-recombining Y-chromosome to "circumvent some of the complications associated with selection".
As another example, she points to work by Thomas "et al.", who sought to distinguish between the Y chromosomes of Jewish priests (Kohanim), (in Judaism, membership in the priesthood is passed on through the father's line) and the Y chromosomes of non-Jews. Abu el-Haj concluded that this new "race science" calls attention to the importance of "ancestry" (narrowly defined, as it does not include all ancestors) in some religions and in popular culture, and people's desire to use science to confirm their claims about ancestry; this "race science", she argues, is fundamentally different from older notions of race that were used to explain differences in human behaviour or social status:
Stephan Palmié has responded to Abu el-Haj's claim that genetic lineages make possible a new, politically, economically, and socially benign notion of race and racial difference by suggesting that efforts to link genetic history and personal identity will inevitably "ground present social arrangements in a time-hallowed past", that is, use biology to explain cultural differences and social inequalities.
One problem with these assignments is admixture. Many people have a highly varied ancestry. For example, in the United States, colonial and early federal history were periods of numerous interracial relationships, both outside and inside slavery. This has resulted in a majority of people who identify as African American having some European ancestors. Similarly, many people who identify as white have some African ancestors. In a survey in a northeastern U.S. university of college students who identified as "white", about 30% were estimated to have up to 10% African ancestry.
On the other hand, there are tests that rely on correlations between allele frequencies; often when allele frequencies correlate, these are called clusters. These sorts of tests use informative alleles called Ancestry-informative marker (AIM). These tests use contemporary people sampled from certain parts of the world as references to determine the likely proportion of ancestry for any given individual.
In a recent Public Broadcasting Service (PBS) programme on the subject of genetic ancestry testing, the academic Henry Louis Gates, who identifies as African American, said that he "wasn't thrilled with the AIM results (it turns out that 50 percent of his ancestors are likely European)." He said there had been family stories of white ancestors, but this was a higher proportion than he expected.
In 2003, Charles Rotimi, of Howard University's National Human Genome Center, argued that "the nature or appearance of genetic clustering (grouping) of people is a function of how populations are sampled, of how criteria for boundaries between clusters are set, and of the level of resolution used." As these decisions may each bias the results, he concluded that people should be very cautious about relating genetic lineages or clusters to their personal sense of identity.
On the other hand, Rosenberg (2005) argued that if enough genetic markers and subjects are analyzed, then the clusters found are consistent. How many genetic markers a commercial service uses likely varies, although new technology has continually allowed increasing numbers to be analyzed. In the end, people usually base their individual identity more on family and personal relationships of community than data.

</doc>
