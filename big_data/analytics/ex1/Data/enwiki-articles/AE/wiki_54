<doc id="27661" url="https://en.wikipedia.org/wiki?curid=27661" title="Source code">
Source code

In computing, source code is any collection of computer instructions (possibly with comments) written using some human-readable computer language, usually as text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by a compiler program into low-level machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, an interpreter can be used to analyze and perform the outcomes of the source code program directly on the fly.
Most application softwares are distributed in a form that includes executable files, but not their source code. If the source code were included, it would be useful to a user, programmer, or system administrator, who may wish to modify the program or to understand how it works.
Aside from its machine-readable forms, source code also appears in books and other media; often in the form of small code snippets, but occasionally complete code bases; a well-known case is the source code of PGP.
Definitions.
The Linux Information Project defines source code as:
Source code (also referred to as source or code) is the version of software as it is originally written (i.e., typed into a computer) by a human in plain text (i.e., human readable alphanumeric characters).
The notion of source code may also be taken more broadly, to include machine code and notations in graphical languages, neither of which are textual in nature. An example from an article presented on the annual IEEE conference and on Source Code Analysis and Manipulation:
For the purpose of clarity ‘source code’ is taken to mean any fully executable description of a software system. It is therefore so construed as to include machine code, very high level languages and executable graphical representations of systems.
Often there are several steps of program translation or minification between the original source code typed by a human and an executable program. While some, like the FSF, argue that an intermediate file "is not real source code and does not count as source code", others find it convenient to refer to each intermediate file as the source code for the next step.
History.
The earliest programs for stored-program computers were entered in binary through the front panel switches of the computer. This first-generation programming language had no distinction between source code and machine code.
When IBM first offered software to work with its machine, the source code was provided at no additional charge. At that time, the cost of developing and supporting software was included in the price of the hardware. For decades, IBM distributed source code with its software product licenses, until 1983.
Most early computer magazines published source code as type-in programs.
Occasionally the entire source code to a large program is published as a hardback book, such as ""Computers & Typesetting, Volume B: TeX: The Program"", ""PGP Source Code and Internals"",
""PC SpeedScript"", and ""uC/OS: The Real-Time Kernel"".
Organization.
The source code which constitutes a program is usually held in one or more text files stored on a computer's hard disk ; usually these files are carefully arranged into a directory tree, known as a source tree. Source code can also be stored in a database (as is common for stored procedures) or elsewhere.
The source code for a particular piece of software may be contained in a single file or many files. Though the practice is uncommon, a program's source code can be written in different programming languages. For example, a program written primarily in the C programming language, might have portions written in assembly language for optimization purposes. It is also possible for some components of a piece of software to be written and compiled separately, in an arbitrary programming language, and later integrated into the software using a technique called library linking. In some languages, such as Java, this can be done at run time (each class is compiled into a separate file that is linked by the interpreter at runtime).
Yet another method is to make the main program an interpreter for a programming language, either designed specifically for the application in question or general-purpose, and then write the bulk of the actual user functionality as macros or other forms of add-ins in this language, an approach taken for example by the GNU Emacs text editor.
The code base of a computer programming project is the larger collection of all the source code of all the computer programs which make up the project. It has become common practice to maintain code bases in version control systems. Moderately complex software customarily requires the compilation or assembly of several, sometimes dozens or even hundreds, of different source code files. In these cases, instructions for compilations, such as a Makefile, are included with the source code. These describe the relationships among the source code files, and contain information about how they are to be compiled.
The revision control system is another tool frequently used by developers for source code maintenance.
Purposes.
Source code is primarily used as input to the process that produces an executable program (i.e., it is compiled or interpreted). It is also used as a method of communicating algorithms between people (e.g., code snippets in books).
Computer programmers often find it helpful to review existing source code to learn about programming techniques. The sharing of source code between developers is frequently cited as a contributing factor to the maturation of their programming skills. Some people consider source code an expressive artistic medium.
Porting software to other computer platforms is usually prohibitively difficult without source code. Without the source code for a particular piece of software, portability is generally computationally expensive. Possible porting options include binary translation and emulation of the original platform.
Decompilation of an executable program can be used to generate source code, either in assembly code or in a high-level language.
Programmers frequently adapt source code from one piece of software to use in other projects, a concept known as software reusability.
Licensing.
Software, and its accompanying source code, typically falls within one of two licensing paradigms: open source and proprietary software.
Generally speaking, software is "open source" if the source code is free to use, distribute, modify and study, and "proprietary" if the source code is kept secret, or is privately owned and restricted. The first software license to be published and to explicitly grant these freedoms was the GNU General Public License in 1989. The GNU GPL was originally intended to be used with the GNU operating system.
For proprietary software, the provisions of the various copyright laws, trade secrecy and patents are used to keep the source code closed. Additionally, many pieces of retail software come with an end-user license agreement (EULA) which typically prohibits decompilation, reverse engineering, analysis, modification, or circumventing of copy protection. Types of source code protection beyond traditional compilation to object code include code encryption, code obfuscation or code morphing.
Legal issues in the United States.
In a 2003 court case in the United States, it was ruled that source code should be considered a constitutionally protected form of free speech. Proponents of free speech argued that because source code conveys information to programmers, is written in a language, and can be used to share humor and other artistic pursuits, it is a protected form of communication.
One of the first court cases regarding the nature of source code as free speech involved University of California mathematics professor Dan Bernstein, who had published on the Internet the source code for an encryption program that he created. At the time, encryption algorithms were classified as munitions by the United States government; exporting encryption to other countries was considered an issue of national security, and had to be approved by the State Department. The Electronic Frontier Foundation sued the U.S. government on Bernstein's behalf; the court ruled that source code was free speech, protected by the First Amendment.
Quality.
The way a program is written can have important consequences for its maintainers. Coding conventions, which stress readability and some language-specific conventions, are aimed at the maintenance of the software source code, which involves debugging and updating. Other priorities, such as the speed of the program's execution, or the ability to compile the program for multiple architectures, often make code readability a less important consideration, since code "quality" generally depends on its "purpose".

</doc>
<doc id="27667" url="https://en.wikipedia.org/wiki?curid=27667" title="Space">
Space

Space is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.
Debates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the "Timaeus" of Plato, or Socrates in his reflections on what the Greeks called "khôra" (i.e. "space"), or in the "Physics" of Aristotle (Book IV, Delta) in the definition of "topos" (i.e. place), or in the later "geometrical conception of place" as "space "qua" extension" in the "Discourse on Place" ("Qawl fi al-Makan") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the "visibility of spatial depth" in his "Essay Towards a New Theory of Vision". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world—they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of "space" in his "Critique of Pure Reason" as being a subjective "pure "a priori" form of intuition".
In the 19th and 20th centuries mathematicians began to examine geometries that are not Euclidean, in which space can be said to be "curved", rather than "flat". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.
Philosophy of space.
Leibniz and Newton.
In the seventeenth century, the philosophy of space and time emerged as a central issue in epistemology and metaphysics. At its heart, Gottfried Leibniz, the German philosopher-mathematician, and Isaac Newton, the English physicist-mathematician, set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: "space is that which results from places taken together". Unoccupied regions are those that "could" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.
Space could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.
Leibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.
Newton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.
Kant.
In the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both "a priori" and "synthetic". According to Kant, knowledge about space is "synthetic", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but imposed by us as part of a framework for organizing experience.
Non-Euclidean geometry.
Euclid's "Elements" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line "L1" and a point "P" not on "L1", there is exactly one straight line "L2" on the plane that passes through the point "P" and is parallel to the straight line "L1". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point "P". Consequently, the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through "P". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.
Gauss and Poincaré.
Although there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.
Henri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.
Einstein.
In 1905, Albert Einstein published a paper on a special theory of relativity, which led to the concept that space and time may be combined into a single construct known as "spacetime". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.
Over the following ten years Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.
Mathematics.
In modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.
Physics.
Many of the laws of physics, such as the various inverse square laws, depend on dimension three.
In physics, our three-dimensional space is viewed as embedded in four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.
Classical mechanics.
Space is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.
Relativity.
Before Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.
In addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).
Furthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted- "curved" -near to gravitationally significant masses.
One consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing.
Cosmology.
Relativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the Cosmic Inflation.
Spatial measurement.
The measurement of "physical space" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.
Currently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.
Geographical space.
Geography is the branch of science concerned with identifying and describing the Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data to create an estimate for unobserved phenomena.
Geographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.
Ownership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.
Public space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.
Abstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.
In psychology.
Psychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.
Other, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.
Several space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).
The understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.

</doc>
<doc id="27669" url="https://en.wikipedia.org/wiki?curid=27669" title="Spanish cuisine">
Spanish cuisine

Spanish cuisine as opposed to other national cuisines in Europe, is heavily influenced by regional cuisines and the particular historical processes that shaped culture and society in those territories. Geography and climate, had great influence on cooking methods and available ingredients, and these particularities are still present in the gastronomy of the various regions that make up the country. Spanish cuisine derives from a complex history, where invasions of the country and conquests of new territories modified traditions and made new ingredients available .
History of Spain.
There is not much information about diet or culinary culture before historical time. Archeological evidence suggests a hunter-gatherer society that slowly evolved into a more sedentary culture around 7th century BC in the South Western area of the country. This culture, known as Tartessos established trading relations with Phoenician and Greek cities, and used them to develop new agricultural techniques and the adoption of new products, such as olive trees and grapes.
Authors such as Strabo, however, write about aboriginal people using nuts and acorns as staple food.
Spain as a territory of the Roman Empire.
The Romans introduced the custom of collecting and eating mushrooms, which is still preserved in many parts of Spain, especially in the north. The Romans along with the Greeks introduced viticulture; it also appears that the extension of the vine along the Mediterranean seems to be due to colonization of the Greeks.
Middle Ages.
The Visigoths introduced brewing. The change came in 711 AD, when Muslim troops composed of Arabs and Berbers crossed the Strait of Gibraltar, invading the Iberian Peninsula. The Muslim conquest brought new ingredients to Spanish cuisine from different parts of the world, such as Persia and India
The cuisine of Al-Andalus included such ingredients as: rice, sorghum, sugar cane, spinach, eggplant, watermelon, lemon, peach, orange and almonds. It is common for modern dishes to possess Berber and Arab roots.
The "New World".
The discovery of America, in 1492, initiated the advent of new culinary elements, such as tomatoes, cucumbers, potatoes, corn, bell peppers, spicy peppers, paprika, vanilla and cocoa or chocolate. The latter caused a furor in the Spanish society in the sixteenth and seventeenth centuries; Spain was where it was first mixed with sugar to remove its natural bitterness. Other ingredients traveled to the Americas, such as rice, grapes, olives and many types of cereals.
Spanish regional variation: typical dishes and meal routines.
La comida, the large midday meal in Spain contains several courses. It spans about two hours from 2:00 pm to 4:00 pm, and is usually followed by Sobremesa, which refers to the tabletalk that Spanish people undertake. Menus are organized according to these courses and include five or six choices in each course. At home, Spanish meals wouldn't be too fancy, and would contain soup or a pasta dish, salad, a meat or a fish dish and a dessert such as fruit or cheese. Green salad with the meat or fish courses.
The following is a list of traditional Spanish meals:
Andalucia.
Andalusian cuisine is twofold: rural and coastal. Of all the Spanish regions, this region uses the most olive oil in its cuisine. The Andalusian dish that has achieved the most international fame is Gazpacho. It is a cold soup (or in an alternative view, a liquid salad) made with five vegetables, bread, vinegar, water, salt and olive oil. Other cold soups include: pulley, Zoque, salmorejo, etc.
Snacks made with olives are common. Meat dishes include: flamenquín, pringá, oxtail stew and Menudo Gitano (also called Andalusian tripe). The hot soups include cat soup (made with bread), dog stew (fish soup with orange juice) and Migas Canas. Fish dishes include: fried fish, cod pavías, and parpandúas. A culinary custom is the typical Andalusian breakfast, considered to be a traditional characteristic of laborers and today extending throughout Spain.
Cured meats include: Serrano Ham and Iberico Ham. Typical drinks in the area include: anise, wine (Malaga, Jerez, Pedro Ximénez, etc..) and sherry brandy.
Aragon.
The Aragonese cuisine has a rural and mountainous origin. The central part of Aragon, the flattest, is the richest in culinary specialties. Being a land of lambs raised on the slopes of the Pyrenees, one of its most famous dishes is roast lamb (asado de ternasco) (with garlic, salt and bacon fat), having the lamb to the shepherd, the heads of lamb and Highlanders asparagus (lamb tails). Pork dishes are also very popular, among them: Magras con tomate, roasted pork leg and Almojábanas de Cerdo. Among the recipes made with bread are: migas de Pastor, migas con chocolate, Regañaos (cakes with sardines or herring) and goguera. The most notable condiment is garlic-oil.
Legumes are very important and the most popular vegetables are borage and thistle. In terms of cured meats, ham from Teruel and Huesca are famous. Among the cheeses Tronchon is notable. Fruit-based cuisine includes the very popular Fruits of Aragon (Spanish: Frutas de Aragón) and Maraschino cherries.
Asturias.
Asturian cuisine has a long and rich history, deeply rooted in Celtic traditions of northern Europe. One of its most famous dishes is the Asturian bean stew, which is the traditional stew of the region, made with white beans, sausages such as chorizo and morcilla and pork. Another well-known recipe is beans with clams, hare and partridge. Also of note are Asturian stew and vigil. Pork-based foods, for example chosco, tripe Asturias and bollos preñaos are popular.
Common meat dishes include: carne gobernada, cachopo and stew. Asturian cheeses are very popular in the rest of Spain. Among them, the most representative is Cabrales Cheese a strong-smelling cheese developed in the regions near the Picos de Europa. This can be enjoyed with the local cider. Notable desserts are frisuelos, rice pudding and carbayones.
Balearic Islands.
The Balearic cuisine has purely Mediterranean characteristics. The islands have been conquered several times throughout their history by the French and the English, which has left some culinary influences. At present are well known: the spicy sausage and rice brut, cheese Mahon, Mahon Gin ("pellofa") and mayonnaise. Among the dishes are tumbet, variat frit and roast suckling pig.
Among the desserts are: Ensaimadas, drum almond, sighs of Manacor.
Basque Country.
The cuisine of the Basque Country is a wide and varied range of ingredients and preparations. The culture of eating is very strong among the inhabitants of this region. Highlights include meat and fish dishes. Among fish, cod is produced in various preparations: bacalao al pil pil, cod Bilbao, etc.. Are also common anchovy, bream, bonito, etc.. Among the most famous dishes is the seafood changurro. Among the meats are: the beef steaks, pork loin with milk, fig leaf quail, marinated goose, etc.
Canary Islands.
The Canary Islands have a unique cuisine due to their geographical location in the Atlantic ocean. The Canary Islands were part of the trading routes to the American Continent, hence creating a melting pot of different culinary traditions. Fish (fresh or salted) and potatoes are among the most common staple foods in the islands. The consumption of cheese, fruits and pork meat also characterizes canarian cuisine. The closeness to Africa influences climate and creates a range of warm temperatures that in modern times have fostered the agriculture of tropical and semitropical crops: bananas, yams, mangoes, avocados and persimmons which are heavily used in canarian cuisine.
The aboriginal people Guanches based their diet on gofio (a type of flour made of different toasted grains), shellfish, and goat and pork products. Gofio is still consumed in the islands and has become part of the traditional cuisine of the islands.
A sauce called mojo (from Portuguese origins) is very common through the islands and has developed different varieties adapted to the main dish where it is being used. Fish dishes usually require a "green mojo" made from coriander or parsley, while roasted meats require a red variety made from chilli peppers that is commonly known as mojo picón.
Stew is a very common kind of fish stew, reminiscent of dishes in other culinary traditions. Some other classic dishes in the Canary Islands include papas arrugadas, almogrote, frangollo, rabbit in "salmorejo sauce" and stewed goat.
Some popular desserts are: truchas (pastries filled with sweet potato or pumpkin), roasted gofio (a gofio-based dough with nuts and honey), príncipe alberto (a mousse-like preparation with almonds, coffee, and chocolate) and quesillo (a variety of flan made with condensed milk)
Winery is common in the islands; however, only Malvasia wine from Lanzarote has gained international recognition.
Cantabria.
A popular Cantabrian dish is "cocido montañés", a rich stew made with beans, cabbage and pork. Seafood is widely used and bonito is present in the typical "sorropotún" or marmite. Recognized quality meats are Tudanca veal and game meat. Cantabrian pastries include "sobaos" and "quesadas pasiegas". Dairy products include Cantabrian cream cheese, smoked cheeses, "picón Bejes-Tresviso" and "quesucos de Liébana". "Orujo" is the Cantabrian pomace brandy. Cider ("sidra") and "chacoli" wine are increasing in popularity.
Cantabria has two wines labelled DOC: Costa de Cantabria and Liébana.
Castile-La Mancha.
In this region, the culinary habits reflect the origin of foods eaten by shepherds and peasants. "Al-Manchara" means, in Arabic, "Dry Land" indicating the arid lands and the quality of its dishes. It is said that the best La Mancha cuisine cookbook is the novel "Don Quixote" by Miguel de Cervantes. Wheat and grains are dominant, used in bread, soups, gazpacho manchego, crumbs, porridge, etc.. One of the most abundant ingredients in Manchego cuisine is garlic, leading to dishes such as: ajoarriero, ajopuerco and garlic marinade.
Some traditional recipes are gazpacho manchego, pisto manchego and migas ruleras. Also popular is morteruelo, a kind of foie gras manchego. Manchego cheese is renowned.
Given the fact that its lands are dry, and thus unable to sustain big amounts of cattle living on grass, an abundance of small animals, such as rabbit, and especially birds (pheasant, quail, partridge, squab) can be found. This has led to game meat being incorporated into traditional dishes, such as Conejo al Ajillo (rabbit in garlic sauce), Perdiz Escabechada (marinated partridge) or Huevos de Codorniz (Quail's eggs).
Castile and León.
In Castile and León characteristic dishes include morcilla, Valladolid (a black pudding made with special spices), "judión de la granja", "sopa de ajo" (garlic soup), "Cochinillo asado" (roast piglet), "lechazo" (roast lamb), "botillo del Bierzo", "hornazo" from Salamanca, "Jamón de Guijuelo" (a cured ham from Guijuelo, Salamanca), "Salchichas de Zaratán" and other sausages, Serrada cheese, Burgos's soft cheese, and Ribera del Duero wines.
Major wines in Castilian-Leonese cuisine include the robust wine of Toro, reds from Ribera del Duero, whites from Rueda, and clarets from Cigales.
Catalonia.
The cuisine of Catalonia is based in a rural culture; it is very extensive and a great culinary wealth. Notably, it was in Catalonia where the first cookbook was written in Spain. It has a triple cuisine: seafood, mountain and interior. Among the most popular dishes include: escudella and tomato bread. Bean tortilla, Coca de recapte, samfaina, farigola soup and snails are famous dishes. Notable sauces are: romesco sauce, aioli, bouillabaisse of Catalan origin and picada.
Cured pork cuisine boasts sausage (white and black) and the salami and pepperoni of Vic. Among the fish dishes are: suquet, stewed cod and black rice. Among the vegetable dishes, the most famous are calçots and the Escalivada (roasted vegetables). Among the desserts are: Catalan cream, carquiñoles, panellets, Kings Tortel, kink and neulas.
La Rioja.
La Rioja is recognized by the use of meats such as pork, and their cold cuts made after the traditional slaughter. The lamb is perhaps the second most popular meat product in this region (Sarmiento chops) and finally, veal is common in mountain areas. The most famous dish is Rioja potatoes and Fritada. Lesser known are: Holy lunch and Ajo huevo (garlic eggs).
Another well-known dish is Rioja stew. Pimientos asados (roasted peppers) is a notable vegetable dish. Rioja wine has designated origin status.
Extremadura.
The cuisine of Extremadura is austere, with dishes prepared by shepherds. It is very similar to the cuisine of Castilla. Extremaduran cuisine is abundant in pork; it is said that the region is one of the best for breeding pigs in Spain, thanks to the acorns that grow in their fields: Iberian pig herds raised in the fields of Montánchez are characterized by dark skin and black, thin legs. This breed of pig is found exclusively in Spain and Portugal. Iberian pork sausages are common, such as pork stews (cocido extremeño).
Another meat dishes is lamb stew. It is also known that lizard is often cooked in Extremadura. Highlights include game meats such as wild boar, partridge, pheasant or venison. Famous cheeses are Torta de la Serena and Torta de casar. Among the desserts are: Leche frita, perrunillas and fritters, as well as many sweets that have their origins in convents.
Galicia.
Galician cuisine is known in Spanish territory because of the emigration of its inhabitants. One of the most noted is Galician soup. Also notable is pork with turnip tops, a popular component of the Galician carnival meal laconadas. Another remarkable recipe is Caldo de castañas (a chestnut broth), which is commonly consumed during winter. Pork products are also popular.
The seafood dishes are very famous and rich in variety. Among these are: the Galician empanada, Galician octopus, scallops, crab and barnacles. Among the many dairy products is Queso de tetilla. Orujo is one of Galicia's alcoholic drinks. Sweets that are famous throughout the Iberian Peninsula are the Tarta de Santiago and Filloas (pancakes made with blood).
Madrid.
Madrid did not gain its own identity in the Court until 1561, when Philip II moved the capital to Madrid. Since then, due to immigration, many of Madrid's culinary dishes have been made from modifications to dishes from other Spanish regions. Madrid, due to the influx of visitors from the nineteenth century onwards, was one of the first cities to introduce the concept of the restaurant, hosting some of the earliest examples.
Notable dairy products are: rice pudding, meringue milk, cheese and curd. Some important fruits and vegetables are Aranjuez strawberries and melons. Madrid is rich in religious confectionery, with sweets such as chocolate con churros and buñuelos.
The nutritional value of the madrilian cuisine was discovered by the American epidemiologist Ancel Keys in the 1950, the Spanish cuisine being later often mentioned by epidemiologists as one of the best examples of the Mediterranean diet.
Murcia.
The cuisine of the region of Murcia has two sides with the influence of Manchego cuisine. The region of Murcia is famous for its varied fruit production. Among the most outstanding dishes are: Murcia tortilla, zarangollo, mojete, eggplants cream, pipirrana, etc.. A typical sauce of this area is the cabañil garlic, used to accompany meat dishes.
Among the culinary preparations are: the michirones (dried beans cooked with bay leaves, hot peppers and garlic). Among the cooked include: the gypsy pot, cooked with balls, mondongo, etc.. Among meat products Murcia find black pudding, which is flavored with oregano, and Murcia cake that is made with ground beef. Among the fish and seafood are: the golden salt, the Mar Menor prawns and octopus baked. Rices are common and among them are: the cauldron, the pavement rice, rice with rabbit and snails, rice scribe, and the widower rice.
Among confectionery products are: the Salteadores(Robbers) and Doe Cake, them are some typical cakes in Murcia gastronomy,they are found in almost all pastry shop in Murcia,are both sweet and savory at the same time.
The desserts are very abundant, among them are: paparajotes Orchard, stufed pastries and various pastries. This region also has wine appellation of origin, as the wine from Jumilla, Bullas wine and wine Yecla.
Navarre.
The gastronomy of Navarra has many similarities with the Basque cuisine. Two of its flag dishes are: Tout Navarre Style and Ajoarriero, although we must not forget the lamb chilindrón or relleno. There are very curious recipes such as the Carlists eggs.
Salted products are common and, between them, include: chorizo de Pamplona, stuffing and sausage. The lamb and beef have, at present, designations of origin. Among the dairy products are: Roncal cheese, the curd or Idiazabal cheese. Among the most typical alcoholic drinks are: the claret and pacharán.
Valencia.
The cuisine of Valencia has two components: the rural (products of the field) and the other coastal, which is seafood. One of the most popular dishes is paella, but there are many other rice dishes, such as Arroz con costra, Arròs negre, fideuá and throw rice, Arroz al horno, and rice with beans and turnips.
Coastal towns supply the region with fish, leading to popular dishes like "all i pebre" typical of the Albufera, or fish stew. Among the desserts are: coffee liqueur, chocolate Alicante, arnadí and horchata. Notably, during Christmas, nougat is made in Alicante and Jijona; also well-known are peladillas (almonds wrapped in a thick layer of caramel).
Other Spanish dishes:
See also.
Derivatives:

</doc>
<doc id="27670" url="https://en.wikipedia.org/wiki?curid=27670" title="Santiago de Compostela">
Santiago de Compostela

Santiago de Compostela,
commonly known as Santiago (, , ), is the capital of the autonomous community of Galicia in northwestern Spain.
The city has its origin in the shrine of Saint James the Great, now the city's cathedral, as destination of the Way of St. James, a leading Catholic pilgrimage route originated in the . In 1985 the city's Old Town was designated a UNESCO World Heritage Site.
Toponym.
"Santiago" is the local Galician evolution of Vulgar Latin "Sanctus Iacobus" "Saint James". According to legend, "Compostela" derives from the Latin "Campus Stellae" (i.e., "field of the star"); it seems unlikely, however, that this phrase could have yielded the modern "Compostela" under normal evolution from Latin to Medieval Galician. Other etymologies derive the name from Latin "compositum", local Vulgar Latin "Composita Tella", meaning "burial ground", or simply from Latin "compositellam", meaning "the well-composed one". Other sites in Galicia share this toponym, akin to "Compostilla" in the province of León.
The city.
The cathedral borders the main plaza of the old and well-preserved city. Legend has it that the remains of the apostle James were brought to Galicia for burial. In 813, according to medieval legend, the light of a bright star guided a shepherd who was watching his flock at night to the burial site in Santiago de Compostela. The shepherd quickly reported his discovery to the bishop of Iria, Bishop Teodomiro. The bishop declared that the remains were those of the apostle James and immediately notified King Alfonso II in Oviedo. To honour St. James, the cathedral was built on the spot where his remains were said to have been found. The legend, which included numerous miraculous events, enabled the Catholic faithful to bolster support for their stronghold in northern Spain during the Christian crusades against the Moors, but also led to the growth and development of the city.
Along the western side of the "Praza do Obradoiro" is the elegant 18th century Pazo de Raxoi, now the city hall. Across the square is the Pazo de Raxoi (Raxoi's Palace), the town hall, and on the right from the cathedral steps is the Hostal dos Reis Católicos, founded in 1492 by the Catholic Monarchs, Isabella of Castille and Ferdinand II of Aragon, as a pilgrims' hospice (now a parador). The Obradoiro façade of the cathedral, the best known, is depicted on the Spanish euro coins of 1 cent, 2 cents, and 5 cents (€0.01, €0.02, and €0.05).
Santiago is the site of the University of Santiago de Compostela, established in the early 16th century. The main campus can be seen best from an alcove in the large municipal park in the centre of the city.
Within the old town there are many narrow winding streets full of historic buildings. The new town all around it has less character though some of the older parts of the new town have some big flats in them.
Santiago de Compostela has a substantial nightlife. Both in the new town ("a zona nova" in Galician, "la zona nueva" in Spanish or "ensanche") and the old town ("a zona vella" in Galician or "la zona vieja" in Spanish, trade-branded as "zona monumental"), a mix of middle-aged residents and younger students maintain a lively presence until the early hours of the morning. Radiating from the centre of the city, the historic cathedral is surrounded by paved granite streets, tucked away in the old town, and separated from the newer part of the city by the largest of many parks throughout the city, "Parque da Alameda".
Santiago gives its name to one of the four military orders of Spain: Santiago, Calatrava, Alcántara and Montesa.
One of the most important economic centres in Galicia, Santiago is the seat for organisations like Association for Equal and Fair Trade Pangaea.
Climate.
Under the Köppen climate classification, Santiago de Compostela has a temperate oceanic ("Cfb") climate, with cool and somewhat dry summers and cool and cold, wet winters. The prevailing winds from the Atlantic and the surrounding mountains combine to give Santiago some of Spain’s highest rainfall: about annually. The climate is mild: frosts are common only in December, January and February, with an average of just 8 days per year, while snow is rare; temperatures over are exceptional.
Population.
The population of the city in 2012 was 95,671 inhabitants, while the metropolitan area reaches 178,695.
In 2010 there were 4,111 foreigners living in the city, representing a 4,3% of the total population. The main nationalities are Brazilians (11%), Portuguese (8%) and Colombians (7%).
By language, according to 2008 data, 21% of the population always speak in Galician, 15% always speak in Spanish and the rest use both interchangeably. 
History.
The area of Santiago de Compostela was a Roman cemetery by the 4th century and was occupied by the Suebi in the early 5th century, when they settled in Galicia and Portugal during the initial collapse of the Roman Empire. The area was later attributed to the bishopric of Iria Flavia in the 6th century, in the partition usually known as Parochiale Suevorum, ordered by king Theodemar. In 585, the settlement was annexed along with the rest of Suebi Kingdom by Leovigild as the sixth province of the Visigothic Kingdom.
Possibly raided from 711 to 739 by the Arabs, the bishopric of Iria was incorporated into the Kingdom of Asturias c. 750. At some point between 818 and 842, during the reign of Alfonso II of Asturias, bishop Theodemar of Iria (d. 847) claimed to have found some remains which were attributed to Saint James the Greater. This discovery was accepted in part because the Pope and Charlemagne—who had died in 814—had acknowledged Asturias as a kingdom and Alfonso II as king, and had also crafted close political and ecclesiastic ties. Around the place of the discovery a new settlement and centre of pilgrimage emerged, which was known to the author Usuard in 865 and which was called "Compostella" by the 10th century.
The cult of Saint James of Compostela was just one of many arising throughout northern Iberia during the 10th and 11th centuries, as rulers encouraged their own region-specific cults, such as Saint Eulalia in Oviedo and Saint Aemilian in Castile. After the centre of Asturian political power moved from Oviedo to León in 910, Compostela became more politically relevant, and several kings of Galicia and of León were acclaimed by the Galician noblemen and crowned and anointed by the local bishop at the cathedral, among them Ordoño IV in 958, Bermudo II in 982, and Alfonso VII in 1111, by which time Compostela had become capital of the Kingdom of Galicia. Later, 12th-century kings were also sepulchered in the cathedral, namely Fernando II and Alfonso IX, last of the Kings of León and Galicia before both kingdoms were united with the Kingdom of Castile.
During this same 10th century and in the first years of the 11th century Viking raiders tried to assault the town—Galicia is known in the Nordic sagas as "Jackobsland" or "Gallizaland"—and bishop Sisenand II, who was killed in battle against them in 968, ordered the construction of a walled fortress to protect the sacred place. In 997 Compostela was assaulted and partially destroyed by Ibn Abi Aamir (known as al-Mansur), Andalusian leader accompanied in his raid by Christian lords, who all received a share of the booty. However, the Andalusian commander showed no interest in the alleged relics of St James. In response to these challenges bishop Cresconio, in the mid-11th century, fortified the entire town, building walls and defensive towers.
According to some authors, by the middle years of the 11th century the site had already become a pan-European place of peregrination, while others maintain that the cult to Saint James was before 11-12th centuries an essentially Galician affair, supported by Asturian and Leonese kings to win over faltering Galician loyalties. Santiago would become in the course of the following century a main Catholic shrine second only to Rome and Jerusalem. In the 12th century, under the impulse of bishop Diego Gelmírez, Compostela became an archbishopric, attracting a large and multinational population. Under the rule of this prelate, the townspeople rebelled, headed by the local council, beginning a secular tradition of confrontation by the people of the city—who fought for self-government—against the local bishop, the secular and jurisdictional lord of the city and of its fief, the semi-independent "Terra de Santiago" ("land of Saint James"). The culminating moment in this confrontation was reached in the 14th century, when the new prelate, the Frenchman Bérenger de Landore, treacherously executed the counselors of the city in his castle of "A Rocha Forte" ("the strong rock, castle"), after inviting them for talks.
Santiago de Compostela was captured and sacked by the French during the Napoleonic Wars; as a result, the remains attributed to the apostle were lost for near a century, hidden inside a cist in the crypts of the cathedral of the city.
The excavations conducted in the cathedral during the 19th and 20th centuries uncovered a Roman "cella memoriae" or martyrium, around which grew a small cemetery in Roman and Suevi times which was later abandoned. This "martyrium", which proves the existence of an old Christian holy place, has been sometimes attributed to Priscillian, although without further proof.
Economy.
Santiago's economy, although still heavily dependent on public administration (i.e. being the headquarters of the autonomous government of Galicia), cultural tourism, industry, and higher education through its university, is becoming increasingly diversified. New industries such as timber transformation (FINSA), the automotive industry (UROVESA), and telecommunications and electronics (Blusens and Televés) have been established. Banco Gallego, a banking institution owned by Novacaixagalicia, has its headquarters in downtown "rúa do Hórreo".
Tourism is very important thanks to the Way of St. James, particularly in Holy Compostelan Years (when 25 July falls on a Sunday). Following the Xunta's considerable investment and hugely successful advertising campaign for the Holy Year of 1993, the number of pilgrims completing the route has been steadily rising. More than 272,000 pilgrims made the trip during the course of the Holy Year of 2010. Following 2010, the next Holy Year will not be for another 11 years when St James feast day again falls on a Sunday. Outside of Holy Years, the city still receives a remarkable number of pilgrims.
Editorial Compostela owns daily newspaper El Correo Gallego, a local TV, and a radio station. Galician language online news portal Galicia Hoxe is also based in the city. Televisión de Galicia, the public broadcaster corporation of Galicia, has its headquarters in Santiago.
Way of St. James.
The legend that St James found his way to the Iberian Peninsula, and had preached there is one of a number of early traditions concerning the missionary activities and final resting places of the apostles of Jesus. Although the 1884 Bull of Pope Leo XIII "Omnipotens Deus" accepted the authenticity of the relics at Compostela, the Vatican remains uncommitted as to whether the relics are those of Saint James the Greater, while continuing to promote the more general benefits of pilgrimage to the site. Pope Benedict XVI under went a ceremonial pilgrimage to the site on his visit to Spain in 2010.
Legends.
According to a tradition that can be traced back at least to the 12th century, when it was recorded in the Codex Calixtinus, Saint James decided to return to the Holy Land after preaching in Galicia. There he was beheaded, but his disciples managed to get his body to Jaffa, where they found a marvelous stone ship which miraculously conducted them and the apostle's body to Iria Flavia, back in Galicia. There, the disciples asked the local pagan queen "Loba" ('She-wolf') for permission to bury the body; she, annoyed, decided to deceive them, sending them to pick a pair of oxen she allegedly had by the "Pico Sacro", a local sacred mountain where a dragon dwelt, hoping that the dragon would kill the Christians, but as soon as the beast attacked the disciples, at the sight of the cross, the dragon exploded. Then the disciples marched to collect the oxen, which were actually wild bulls which the queen used to punish her enemies; but again, at the sight of the Christian's cross, the bulls calmed down, and after being subjected to a yoke they carried the apostle's body to the place where now Compostela is. The legend was again referred with minor changes by the Czech traveller Jaroslav Lev of Rožmitál, in the 15th century.
The relics were said to have been later rediscovered in the 9th century by a hermit named Pelagius, who after observing strange lights in a local forest went for help after the local bishop, Theodemar of Iria, in the west of Galicia. The legend affirms that Theodemar was then guided to the spot by a star, drawing upon a familiar myth-element, hence "Compostela" was given an etymology as a corruption of Campus Stellae, "Field of Stars."
In the 15th century, the red banner which guided the Galician armies to battle, was still preserved in the Cathedral of Santiago de Compostela, in the centre Saint James riding a white horse and wearing a white cloak, sword in hand: The legend of the miraculous armed intervention of Saint James, disguised as a white knight to help the Christians when battling the Muslims, was a recurrent myth during the High Middle Ages.
Establishment of the shrine.
The 1,000-year-old pilgrimage to the shrine of St. James in the Cathedral of Santiago de Compostela is known in English as the Way of St. James and in Spanish as the "Camino de Santiago". Over 100,000 pilgrims travel to the city each year from points all over Europe and other parts of the world. The pilgrimage has been the subject of many books, television programmes, and films, notably Brian Sewell's "The Naked Pilgrim" produced for the British television channel Channel 5 and the Martin Sheen/Emilio Estevez collaboration "The Way".
Pre-Christian legends.
As the lowest-lying land on that stretch of coast, the city's site took on added significance. Legends supposed of Celtic origin made it the place where the souls of the dead gathered to follow the sun across the sea. Those unworthy of going to the Land of the Dead haunted Galicia as the "Santa Compaña" or "Estadea".
In popular culture.
Santiago de Compostela is featured prominently in the 1988 historical fiction novel "Sharpe's Rifles", by Bernard Cornwell, which takes place during the French Invasion of Galicia, January 1809, during the Napoleonic Wars.
Transport.
Santiago de Compostela is served by Santiago de Compostela Airport and a rail service. The town is linked to the Spanish High Speed Railway Network. On 24 July 2013 there was a serious rail accident near the city in which 79 people died and at least 130 were injured when a train derailed on a bend as it approached Compostela station.
International relations.
Twin towns/Sister cities.
Santiago de Compostela is twinned with:

</doc>
<doc id="27672" url="https://en.wikipedia.org/wiki?curid=27672" title="Sailing">
Sailing

Sailing comprises wind propulsion of a craft by means of sails and steering it over water, ice or land, depending on the type of craft. A sailor manages the force of the wind on the sails by adjusting their angle with respect to the moving sailing craft and sometimes by adjusting the sail area. The force transmitted from the sails is resisted by forces from the hull, keel, and rudder of a sailing craft, by forces from skate runners for an iceboat, and by forces from wheels for a land sailing craft to allow steering a course on a point of sail with respect to the true wind.
While there are still some places in the world where sail-powered passenger, fishing and trading vessels are used, these craft have become rarer as internal combustion engines have become economically viable in even the poorest and most remote areas. In most countries sailing is enjoyed as a recreational activity or as a sport. Recreational sailing or yachting can be divided into racing and cruising. Cruising can include extended offshore and ocean-crossing trips, coastal sailing within sight of land, and daysailing.
History.
Throughout history sailing has been instrumental in the development of civilization, affording humanity greater mobility than travel over land, whether for trade, transport or warfare, and the capacity for fishing. The earliest representation of a ship under sail appears on a painted disc found in Kuwait dating between 5000 and 5500 BCE. Advances in sailing technology from the Middle Ages onward enabled Arab, Chinese, Indian and European explorers to make longer voyages into regions with extreme weather and climatic conditions. There were improvements in sails, masts and rigging; improvements in marine navigation including the cross tree and charts, of both the sea and constellations, allowed more certainty in sea travel. From the 15th century onwards, European ships went further north, stayed longer on the Grand Banks and in the Gulf of St. Lawrence, and eventually began to explore the Pacific Northwest and the Western Arctic. Sailing has contributed to many great explorations in the world.
Physics.
Introduction.
The air interacting with the sails of a sailing vessel creates various forces, including reaction forces. If the sails are properly oriented with respect to the wind, then the net force on the sails will move the vessel forward. However, boats propelled by sails cannot sail directly into the wind. They must tack (turn the boat through the eye of the wind) back and forth in order to progress directly upwind (see below "Beating").
Sails as airfoils.
Sails are airfoils that work by using an airflow set up by the wind and the motion of the boat. Sails work in two "modes" to use the wind to generate force (see Forces on sails):
Apparent wind.
The wind that a boat experiences is the combination of the true wind (i.e. the wind relative to a stationary object) and the wind that occurs due to the forward motion of the boat. This combination is the apparent wind, which is the relative velocity of the wind relative to the boat.
When sailing upwind the apparent wind is greater than the true wind and the direction of the apparent wind will be forward of the true wind. Some high-performance boats are capable of traveling faster than the true windspeed on some points of sail, see for example the Hydroptère, which set a world speed record in 2009 by sailing 1.71 times the speed of the wind. Iceboats can typically sail at 5 times the speed of the wind.
The energy that drives a sailboat is harnessed by manipulating the relative movement of wind and water speed: if there is no difference in movement, such as on a calm day or when the wind and water current are moving in the same direction at the same speed, there is no energy to be extracted and the sailboat will not be able to do anything but drift. Where there is a difference in motion, then there is energy to be extracted at the interface. The sailboat does this by placing the sail(s) in the air and the hull(s) in the water.
A sailing vessel is not maneuverable due to sails alone—the forces caused by the wind on the sails would cause the vessel to rotate and travel sideways instead of moving forward. In the same manner that an aircraft requires stabilizers, such as a tailplane with elevators as well as wings, a boat requires a keel and rudder. The forces on the sails as well as those from below the water line on the keel, centreboard, and other underwater foils including the hull itself (especially for catamarans or in a traditional proa) combine and partially cancel each other to produce the motive force for the vessel. Thus, the physical portion of the boat that is below water can be regarded as functioning as a "second sail." The flow of water over the underwater hull portions creates hydrodynamic forces, which combine with the aerodynamic forces from the sails to allow motion in almost any direction except straight into the wind. When sailing close to the wind the force generated by the sail acts at 90° to the sail. This force can be considered as split into a small force acting in the direction of travel, as well as a large sideways force that heels (tips) the boat. To enable maximum forward speed, the force needs to be cancelled out, perhaps using human ballast, leaving only a smaller forward resultant force. Depending on the efficiency of the rig and hull, the angle of travel relative to the true wind can be as little as 35° or may need to be 80° or greater. This angle is half of the tacking angle and defines one side of a 'no-go zone' into the wind, in which a vessel cannot sail directly.
Tacking is essential when sailing upwind. The sails, when correctly adjusted, will generate aerodynamic lift. When sailing downwind, the sails no longer generate aerodynamic lift and airflow is stalled, with the wind push on the sails giving drag only. As the boat is going downwind, the apparent wind is less than the true wind and this, allied to the fact that the sails are not producing aerodynamic lift, serves to limit the downwind speed.
Effects of wind shear.
Wind shear affects sailboats in motion by presenting a different wind speed and direction at different heights along the mast. Wind shear occurs because of friction above a water surface slowing the flow of air. Thus, a difference in true wind creates a different apparent wind at different heights. Sailmakers may introduce sail twist in the design of the sail, where the head of the sail is set at a different angle of attack from the foot of the sail in order to change the lift distribution with height. The effect of wind shear can be factored into the selection of twist in the sail design, but this can be difficult to predict since wind shear may vary widely in different weather conditions. Sailors may also adjust the trim of the sail to account for wind gradient, for example, using a boom vang.
Points of sail.
The point of sail describes a sailing boat's course in relation to the wind direction.
No sailboat can sail directly into the wind (known as being "in irons"), and for a given boat there is a minimum angle that it can sail relative to the wind; attempting to sail closer than that leads to the sails luffing and the boat will slow down and stop. This "no-go zone" (shown shaded in accompanying figure) is about 45° either side of the true wind for a modern sloop.
There are 5 main points of sail. In order from the edge of the no-go zone (or "irons") to directly downwind they are:
The sail trim on a boat is relative to the point of sail one is on: on a beam reach sails are mostly let out, on a run sails are all the way out, and close hauled sails are pulled in very tightly. Two main skills of sailing are trimming the sails correctly for the direction and strength of the wind, and maintaining a course relative to the wind that suits the sails once trimmed.
Close Hauled or "Beating".
A boat can be 'worked to windward', to arrive at an upwind destination, by sailing close-hauled with the wind coming from one side, then tacking (turning the boat through the eye of the wind) and sailing with the wind coming from the other side. By this method of zig-zagging into the wind, known as beating, it is possible to reach any upwind destination. A yacht beating to a mark directly upwind one mile away will cover a distance through the water of at least 1.4 miles, if it can tack through an angle of 90 degrees including leeway. An old adage describes beating as sailing for twice the distance at half the speed and three times the discomfort.
An estimate of the correct tacking distance can be obtained (and thereby the time taken to travel it at various boat speeds) by using Pythagoras' theorem with equal tacks (assume a value of 1). This also assumes a tacking angle of 90°. The straight-line distance is the hypotenuse value of √2
When beating to windward one tack may be more favorable than the other - more in the desired direction. The best strategy is to stay on the favorable tack as much as possible. If the wind shifts in the sailor's favor, called a "lift", so much the better, then this tack is even more favorable. But if it shifts against the sailor's, called a "header", then the opposite tack may become the more favorable course. So when the destination is directly into the wind the best strategy is given by the racing adage "tack on a header." This is true because a header on one tack is a lift on the other.
How closely a boat can sail into the wind depends on the boat's design, sail shape and trim, the sea state, and the wind speed.
Typical minimum pointing angles to the true wind are as follows. Actual course over the ground will be worse due to leeway.
Sailing close-hauled under a large amount of sail, and heeling a great deal, can induce weather helm, or a tendency for the boat to turn into the wind. This requires pulling the tiller to windward (i.e. 'to weather'), or turning the wheel leeward, in order to counteract the effect and maintain the required course. The lee side of the hull is more under water than the weather side and the resulting shape of the submerged parts of the hull usually creates a force that pushes the bow to weather. Driving both the asymmetric heeling hull form and the angled rudder through the water produces drag that slows the boat down. If weather helm builds further, it can limit the ability of the helmsperson to steer the boat, which can be turned towards but not effectively away from the wind. At more extreme angles of heel, the boat will spontaneously 'round up' into the wind during gusts, i.e. it will turn into the wind regardless of any corrective action taken on the helm.
Any action that reduces the angle of heel of a boat that is reaching or beating to windward will help reduce excessive weather helm. Racing sailors use their body weight to bring the boat to a more upright position, but are not allowed to use "movable ballast" during a race. Reducing or reefing the total sail area will have the same effect and many boats will sail faster with less sail in a stiff breeze due to the reduction in underwater drag. Easing the sheets on aft-most sails, such as the mainsail in a sloop or cutter can have an immediate effect, especially to help with maneuvering. Moving or increasing sail area forward can also help, for example by raising the jib (and maybe lowering the staysail) on a cutter. The actual design of the boat may be at fault. In this case adding a bow sprit, re-cutting the main sail, or moving the mast forward may all be part of the solution. Basically anything that moves the center of effort of the sails more forward will have the effect of reducing weather helm.
Reaching.
When the boat is traveling approximately perpendicular to the wind, this is called reaching. A "beam reach" is with the wind at right angles to the boat, a "close reach" is anywhere between beating and a beam reach, and a "broad reach" is between a beam reach and running.
For most modern sailboats, that is boats with fore-and-aft sails, reaching is the fastest way to travel. The direction of the wind is ideal when reaching because it can maximize the lift generated on the sails in the forward direction of the boat, giving the best boat speed. Also when reaching, the boat can be steered exactly in the direction that is most desirable, and the sails can be trimmed for that direction.
Reaching may, however, put the boat on a course parallel with the crests of the waves. When the waves are steep, it may be necessary to sail closer to the wind to avoid waves directly on the beam, which create the danger of capsizing.
Running.
Steering a boat within roughly 30 degrees either side of dead downwind is called a run. This can be the most comfortable point of sail, but requires constant attention. When the wind is coming directly behind the boat, a fore-and-aft rigged vessel may sail "wing on wing," one sail on port the other on starboard. Loss of attention by the helmsperson can lead to an accidental jibe, causing injury to the boat or crew. All on deck must be aware of, and if possible avoid, the potential arc of the boom, mainsheet and other gear in case an accidental jibe occurs during a run. A preventer can be rigged to reduce danger and damage from accidental jibes. Traditional sailing vessels with boomless or square sails are not put at risk by jibing.
Another technique used while running is "sailing by the lee". Here the main sail is placed on the windward side of the boat, leading to a heightened risk of gybing. With the main placed perpendicular to the boat to windward, and then pulled in slightly, the leech is allowed to act as the leading edge of an airfoil. (Usually, the luff is the leading edge, such as when close-hauled.) This position, though unstable to accidental gybes, allows the sail to generate some force from lift, just as when sailing on a broad-reach. In fact, because there is no mast to generate turbulence around the sail's leading edge (as happens on the broad reach) the lift generated is somewhat stronger than might be expected for such an oblique profile.
Another technique often used by cruisers is to set two head sails, one to port and one to starboard. Depending on the sails, this can often give as much sail area as a spinnaker, but is easier to control. It is also easier to handle than going wing and wing, as the main sail is not set and does not disturb the air flow to the head sails. The main boom then can be rigged as a whisker pole too, to stabilize one of the head sails.
Running is generally the most unstable point of sail for a fore-and-aft rigged vessel, but the easiest for a novice to grasp conceptually, making it a common downfall for beginners. In stronger winds, rolling increases as there is less rolling resistance provided by the sails, as they are eased out. Also, having the sails and boom(s) perpendicular to the boat throws weight and some wind force to that side, making the boat harder to balance. In smaller boats, death rolls can build up and lead to capsize.
Also on a run an inexperienced or inattentive sailor can easily misjudge the real wind strength since the boat speed subtracts directly from the true wind speed and makes the apparent wind less. In addition sea conditions can also falsely seem milder than they are as the waves ahead are being viewed from behind making white caps less apparent. When changing course from this point of sail to a reach or a beat, a sailboat that seemed under control can instantly become over-canvassed and in danger. Any boat over-canvassed on a run can round up, heel excessively and stop suddenly in the water. This is called broaching and it can lead to capsize, possible crew injury and loss of crew into the water.
Options for maneuvering are also reduced. On other points of sail, it is easy to stop or slow the boat by heading into the wind; there may be no such easy way out when running, especially in close quarters or when a spinnaker (including an Asymmetrical spinnaker), whisker pole or preventer are set.
Basic sailing techniques.
Trim.
An important aspect of sailing is keeping the boat in "trim".
Together, these points are known as 'The Five Essentials' and constitute the central aspects of sailing.
Tacking and Gybing.
Tacking and gybing are converse ways to change from port tack to starboard tack (or vice versa): either by turning the bow through the eye of the wind, "tacking" or the stern, "jibing". In general sailing, tacking is the safer method and preferred especially when sailing upwind; in windsurfing, Jibing is preferred as this involves much less manoeuvring for the sailor.
For general sailing, during such course changes, there is work that needs to be done. Just before tacking the command "Ready about" is given, at which point the crew must man the sheet lines which need to be changed over to the other tack and the helmsperson gets ready. To execute the tack the command "Lee-ho" or "Hard-to-lee" is given. The latter is a direct order to the helmsperson to push the tiller hard to the leeward side of the boat making the bow of the boat come up and quickly turn through the eye of the wind to prevent the boat being caught in irons. As the boat turns through the eye of the wind, some sails such as those with a boom and a single sheet may self-tack and need only small adjustments of sheeting points, but for jibs and other sails with separate sheets on either side, the original sheet must be loosened and the opposite sheet lines hauled in and set quickly and properly for the new point of sail.
Jibing is often necessary to change course when sailing off the wind or downwind. It is a more dangerous manoeuvre because the boom has further to travel (because the sails are let further out to the side of the boat when travelling downwind) in the same amount of time and therefore must be controlled as the sails catch the new wind direction from astern. An uncontrolled jibe can happen suddenly by itself when sailing downwind if the helmsperson is not paying attention to the wind direction and can be very dangerous as the main boom will sweep across the cockpit very quickly and with great force. Before jibing the command "Ready to jibe" is given. The crew gets ready at their positions. If any sails are constrained with preventers or whisker poles these are taken down. The command "Jibe-ho" is given to execute the turn. The boomed sails must be hauled in and made fast before the stern reaches the eye of the wind, so that they are amidship and controlled as the stern passes through the eye of the wind, and then let out quickly under control and adjusted to the new point of sail.
The choice of which strategy to use (coming-about or jibing) depends on the conditions, sail configuration, and the craft. For light craft such as a Hobie Cat (which has little mass) coming into the wind should only be attempted when moving very quickly such as >8 knots. Of course this happens under strong wind. The timing of the crew shift is also critical when coming into the wind. If in light wind, a jibe is the better choice as there's less danger of the wind tipping the boat. A phrase to help remember this is: "light jibe, hard tack" (light/hard referring to wind strength) Of course being caught in irons near shore/structures in strong wind can be catastrophic.
Reducing sail (reefing).
An important safety aspect of sailing is to adjust the amount of sail to suit the wind conditions. As the wind speed increases the crew should progressively reduce the amount of sail. On a small boat with only jib and mainsail this is done by furling the jib and by partially lowering the mainsail, a process called 'reefing the main'.
Reefing means reducing the area of a sail without actually changing it for a smaller sail. Ideally reefing does not only result in a reduced sail area but also in a lower centre of effort from the sails, reducing the heeling moment and keeping the boat more upright.
There are three common methods of reefing the mainsail:
Mainsail furling systems have become increasingly popular on cruising yachts, as they can be operated shorthanded and from the cockpit, in most cases. However, the sail can become jammed in the mast or boom slot if not operated correctly. Mainsail furling is almost never used while racing because it results in a less efficient sail profile. The classical slab-reefing method is the most widely used. Mainsail furling has an additional disadvantage in that its complicated gear may somewhat increase weight aloft. However, as the size of the boat increases, the benefits of mainsail roller furling increase dramatically.
An old saying goes, "The first time you think of reducing sail you should," and correspondingly, "When you think you are ready to take out a reef, have a cup of tea first."
Sail trimming.
The most basic control of the sail consists of setting its angle relative to the wind. The control line that accomplishes this is called a "sheet." If the sheet is too loose the sail will flap in the wind, an occurrence that is called "luffing." Optimum sail angle can be approximated by pulling the sheet in just so far as to make the luffing stop, or by using of tell-tales - small ribbons or yarn attached each side of the sail that both stream horizontally to indicate a properly trimmed sail. Finer controls adjust the overall shape of the sail.
Two or more sails are frequently combined to maximize the smooth flow of air. The sails are adjusted to create a smooth laminar flow over the sail surfaces. This is called the "slot effect". The combined sails fit into an imaginary aerofoil outline, so that the most forward sails are more in line with the wind, whereas the more aft sails are more in line with the course followed. The combined efficiency of this sail plan is greater than the sum of each sail used in isolation.
More detailed aspects include specific control of the sail's shape, e.g.:
Hull trim.
Hull trim is the adjustment of a boat's loading so as to change its fore-and-aft attitude in the water. In small boats, it is done by positioning the crew. In larger boats the weight of a person has less effect on the hull trim, but it can be adjusted by shifting gear, fuel, water, or supplies. Different hull trim efforts are required for different kinds of boats and different conditions. Here are just a few examples: In a lightweight racing dinghy like a Thistle, the hull should be kept level, on its designed water line for best performance in all conditions. In many small boats, weight too far aft can cause drag by submerging the transom, especially in light to moderate winds. Weight too far forward can cause the bow to dig into the waves. In heavy winds, a boat with its bow too low may capsize by pitching forward over its bow (pitch-pole) or dive under the waves (submarine). On a run in heavy winds, the forces on the sails tend to drive a boat's bow down, so the crew weight is moved far aft.
Heeling.
When a ship or boat leans over to one side, from the action of waves or from the centrifugal force of a turn or under wind pressure or from amount of exposed topsides, it is said to 'heel'. A sailing boat that is over-canvassed and therefore heeling excessively, may sail less efficiently. This is caused by factors such as wind gusts, crew ability, the point of sail, or hull size & design.
When a vessel is subject to a heeling force (such as wind pressure), vessel buoyancy & beam of the hull will counter-act the heeling force. A weighted keel provides additional means to right the boat. In some high-performance racing yachts, water ballast or the angle of a canting keel can be changed to provide additional righting force to counteract heeling. The crew may move their personal weight to the high (upwind) side of the boat, this is called "hiking", which also changes the centre of gravity & produces a righting lever to reduce the degree of heeling. Incidental benefits include faster vessel speed caused by more efficient action of the hull & sails. Other options to reduce heeling include reducing exposed sail area & efficiency of the sail setting & a variant of hiking called "trapezing". This can only be done if the vessel is designed for this, as in dinghy sailing. A sailor can (usually involuntarily) try turning upwind in gusts (it is known as "rounding up"). This can lead to difficulties in controlling the vessel if over-canvassed. Wind can be spilled from the sails by 'sheeting out', or loosening them. The number of sails, their size and shape can be altered. Raising the dinghy centreboard can reduce heeling by allowing more leeway.
The increasingly asymmetric underwater shape of the hull matching the increasing angle of heel may generate an increasing directional turning force into the wind. The sails' centre of effort will also increase this turning effect or force on the vessel's motion due to increasing lever effect with increased heeling which shows itself as increased human effort required to steer a straight course. Increased heeling reduces exposed sail area relative to the wind direction, so leading to an equilibrium state. As more heeling force causes more heel, weather helm may be experienced. This condition has a braking effect on the vessel but has the safety effect in that an excessively hard pressed boat will try and turn into the wind therefore reducing the forces on the sail. Small amounts (≤5 degrees) of weather helm are generally considered desirable because of the consequent aerofoil lift effect from the rudder. This aerofoil lift produces helpful motion to windward & the corollary of the reason why lee helm is dangerous. Lee helm, the opposite of weather helm, is generally considered to be dangerous because the vessel turns away from the wind when the helm is released, thus increasing forces on the sail at a time when the helmsperson is not in control.
Sailing hulls and hull shapes.
Sailing boats with one hull are "monohulls", those with two are "catamarans", those with three are "trimarans". A boat is turned by a rudder, which itself is controlled by a tiller or a wheel, while at the same time adjusting the sheeting angle of the sails. Smaller sailing boats often have a stabilizing, raisable, underwater fin called a centreboard, daggerboard, or leeboard; larger sailing boats have a fixed (or sometimes canting) keel. As a general rule, the former are called dinghies, the latter keelboats. However, up until the adoption of the Racing Rules of Sailing, any vessel racing under sail was considered a yacht, be it a multi-masted ship-rigged vessel (such as a sailing frigate), a sailboard (more commonly referred to as a windsurfer) or remote-controlled boat, or anything in between. (See Dinghy sailing.)
Multihulls use flotation and/or weight positioned away from the centre line of the sailboat to counter the force of the wind. This is in contrast to heavy ballast that can account for up to 90% (in extreme cases like AC boats) of the weight of a monohull sailboat. In the case of a standard catamaran, there are two similarly-sized and -shaped slender hulls connected by beams, which are sometimes overlaid by a deck superstructure. Another catamaran variation is the proa. In the case of trimarans, which have an unballasted centre hull similar to a monohull, two smaller amas are situated parallel to the centre hull to resist the sideways force of the wind. The advantage of multihulled sailboats is that they do not suffer the performance penalty of having to carry heavy ballast, and their relatively lesser draft reduces the amount of drag, caused by friction and inertia, when moving through the water.
One of the most common dinghy hulls in the world is the Laser hull. It was designed by Bruce Kirby in 1969 and unveiled at the New York boat show (1971). It was designed with speed and simplicity in mind. The Laser is 13 feet 10.5 inches long and a 12.5 foot water line and of sail.
Types of sails and layouts.
A traditional modern yacht is technically called a "Bermuda sloop" (sometimes a "Bermudan sloop"). A sloop is any boat that has a single mast and usually a single headsail (generally a jib) in addition to the mainsail (Bermuda rig but c.f. Friendship sloop). A cutter (boat) also has a single mast, set further aft than a sloop and more than one headsail. Additionally, Bermuda sloops only have a single sail behind the mast. Other types of sloops are gaff-rigged sloops and lateen sloops. Gaff-rigged sloops have quadrilateral mainsails with a gaff (a small boom) at their upper edge (the "head" of the sail). Gaff-rigged vessels may also have another sail, called a topsail, above the gaff. Lateen sloops have triangular sails with the upper edge attached to a gaff, and the lower edge attached to the boom, and the boom and gaff are attached to each other via some type of hinge. It is also possible for a sloop to be square rigged (having large square sails like a Napoleonic Wars-era ship of the line). Note that a "sloop of war", in the naval sense, may well have more than one mast, and is not properly a sloop by the modern meaning.
If a boat has two masts, it may be a schooner, a ketch, or a yawl, if it is rigged fore-and-aft on all masts. A schooner may have any number of masts provided the second from the front is the tallest (called the "main mast"). In both a ketch and a yawl, the foremost mast is tallest, and thus the main mast, while the rear mast is shorter, and called the mizzen mast. The difference between a ketch and a yawl is that in a ketch, the mizzen mast is forward of the rudderpost (the axis of rotation for the rudder), while a yawl has its mizzen mast behind the rudderpost. In modern parlance, a brigantine is a vessel whose forward mast is rigged with square sails, while her after mast is rigged fore-and-aft. A brig is a vessel with two masts both rigged square.
As one gets into three or more masts the number of combinations rises and one gets barques, barquentines, and full rigged ships.
A spinnaker is a large, full sail that is only used when sailing off wind either reaching or downwind, to catch the maximum amount of wind.
Rigid foils.
With modern technology, "wings", that is rigid sails, may be used in place of fabric sails. An example of this would be the International C-Class Catamaran Championship and the yacht USA 17 that won the 2010 America's Cup. Such rigid sails are typically made of thin plastic fabric held stretched over a frame. See also AC72 wing-sail catamarans which competed in the 2013 America's Cup.
Alternative wind-powered vessels.
Some non-traditional rigs capture energy from the wind in a different fashion and are capable of feats that traditional rigs are not, such as sailing directly into the wind. One such example is the wind turbine boat, also called the windmill boat, which uses a large windmill to extract energy from the wind, and a propeller to convert this energy to forward motion of the hull. A similar design, called the autogyro boat, uses a wind turbine without the propellor, and functions in a manner similar to a normal sail. A more recent (2010) development is a cart that uses wheels linked to a propeller to "sail" dead downwind at speeds exceeding wind speed.
Kitesurfing and windsurfing.
Kitesurfing and windsurfing are other forms of sailing.
Sailing terminology.
Sailors use traditional nautical terms for the parts of or directions on a vessel: starboard (right), port or larboard (left), forward or fore (front), aft or abaft (rearward), bow (forward part of the hull), stern (aft part of the hull), beam (the widest part). Vertical spars are masts, horizontal spars are booms (if they can hit the sailor), yards, gaffs (if they are too high to reach) or poles (if they cannot hit the sailor). 
Rope and lines.
In most cases, "rope" is the term used only for raw material. Once a section of rope is designated for a particular purpose on a vessel, it generally is called a "line," as in "outhaul line" or "dock line". A very thick line is considered a "cable." Lines that are attached to sails to control their shapes are called "sheets", as in "mainsheet". If a rope is made of wire, it maintains its rope name as in 'wire rope' halyard.
Lines (generally steel cables) that support masts are stationary and are collectively known as a vessel's standing rigging, and individually as "shrouds" or "stays". The stay running forward from a mast to the bow is called the "forestay" or "headstay". Stays running aft are backstays or after stays.
Moveable lines that control sails or other equipment are known collectively as a vessel's running rigging. Lines that raise sails are called "halyards" while those that strike them are called "downhauls". Lines that adjust (trim) the sails are called "sheets". These are often referred to using the name of the sail they control (such as "main sheet", or "jib sheet"). Sail trim may also be controlled with smaller lines attached to the forward section of a boom such as a cunningham; a line used to hold the boom down is called a "vang", or a "kicker" in the United Kingdom. A "topping lift" is used to hold a boom up in the absence of sail tension. "Guys" are used to control the ends of other spars such as spinnaker poles.
Lines used to tie a boat up when alongside are called "docklines", "docking cables" or "mooring warps". In dinghies the single line from the bow is referred to as the "painter". A "rode" is what attaches an anchored boat to its anchor. It may be made of chain, rope, or a combination of the two.
Some lines are referred to as ropes:
Other terms.
Walls are called "bulkheads" or "ceilings", while the surfaces referred to as ceilings on land are called 'overheads' or 'deckheads'. Floors are called 'soles' or "decks". "Broken up" was the fate of a ship that hit a "rocky point" or was simply no longer wanted. The toilet is traditionally called the 'head', the kitchen is the "galley". When lines are tied off, this may be referred to as 'made fast' or 'belayed.' Sails in different sail plans have unchanging names, however. For the naming of sails, see sail-plan.
Knots and line handling.
The tying and untying of knots and hitches as well as the general handling of ropes and lines are fundamental to the art of sailing. The RYA basic 'Start Yachting' syllabus lists the following knots and hitches:
It also lists securing a line around a cleat and the use of winches and jamming cleats.
The RYA Competent Crew syllabus adds the following to the list above, as well as knowledge of the correct use of each: 
In addition it requires competent crewmembers to understand 'taking a turn' around a cleat and to be able to make cleated lines secure. Lines and halyards need to be coiled neatly for stowage and reuse. Dock lines need to be thrown and handled safely and correctly when coming alongside, up to a buoy, and when anchoring, as well as when casting off and getting under way.
Rules and regulations.
Every vessel in coastal and offshore waters is subject to the International Regulations for Preventing Collisions at Sea (the COLREGS). On inland waterways and lakes other similar regulations, such as CEVNI in Europe, may apply. In some sailing events, such as the Olympic Games, which are held on closed courses where no other boating is allowed, specific racing rules such as the Racing Rules of Sailing (RRS) may apply. Often, in club racing, specific club racing rules, perhaps based on RRS, may be "superimposed" onto the more general regulations such as COLREGS or CEVNI.
In general, regardless of the activity, every sailor must
The stand-on vessel must hold a steady course and speed but be prepared to take late avoiding action to prevent an actual collision if the other vessel does not do so in time. The give-way vessel must take early, positive and obvious avoiding action, without crossing ahead of the other vessel.(Rules 16-17)
The COLREGS go on to describe the lights to be shown by vessels under way at night or in restricted visibility. Specifically, for sailing boats, red and green sidelights and a white sternlight are required, although for vessels under 7 metres (23.0 ft) in length, these may be substituted by a torch or white all-round lantern. (Rules 22 & 25)
Sailors are required to be aware not only of the requirements for their own boat, but of all the other lights, shapes and flags that may be shown by other vessels, such as those fishing, towing, dredging, diving etc., as well as sound signals that may be made in restricted visibility and at close quarters, so that they can make decisions under the COLREGS in good time, should the need arise. (Rules 32 - 37)
In addition to the COLREGS, CEVNI and/or any specific racing rules that apply to a sailing boat, there are also
Licensing.
Licensing regulations vary widely across the world. While boating on international waters does not require any license, a license may be required to operate a vessel on coastal waters or inland waters. Some jurisdictions require a license when a certain size is exceeded (e.g., a length of 20 meters), others only require licenses to pilot passenger ships, ferries or tugboats. For example, the European Union issues the International Certificate of Competence, which is required to operate pleasure craft in most inland waterways within the union. The United States in contrast has no licensing, but instead has voluntary certification organizations such as the American Sailing Association. These US certificates are often required to charter a boat, but are not required by any federal or state law.
Sailboat racing.
Sailboat racing generally fits into one of two categories:
Class racing can be further subdivided. Each class has its own set of class rules, and some classes are more restrictive than others.
In a strict one-design class the boats are essentially identical. Examples include the 29er, J/24, Laser, and RS Feva. 
At the other end of the extreme are the development classes based on a box-rule. The box-rule might specify only a few parameters such as maximum length, minimum weight, and maximum sail area, thus allowing creative engineering to develop the fastest boat within the constraints. Examples include the Moth (dinghy), the A Class Catamaran, and the boats used in the America's Cup, Volvo Ocean Race, and Barcelona World Race.
Many classes lie somewhere in between strict one-design and box rule. These classes allows some variation, but the boats are still substantially similar. For instance, both wood and fiberglass hulls are allowed in the Albacore, Wayfarer, and Fireball classes, but the hull shape, weight, and sail area are tightly constrained.
Sailboat racing ranges from single person dinghy racing to large boats with 10 or 20 crew and from small boats costing a few thousand dollars to multimillion-dollar America's Cup or Sydney to Hobart Yacht Race campaigns. The costs of participating in the high end large boat competitions make this type of sailing one of the most expensive sports in the world. However, there are inexpensive ways to get involved in sailboat racing, such as at community sailing clubs, classes offered by local recreation organizations and in some inexpensive dinghy and small catamaran classes. Additionally high schools and colleges may offer sailboat racing programs through the Interscholastic Sailing Association (in the USA) and the Intercollegiate Sailing Association (in the USA and some parts of Canada). Under these conditions, sailboat racing can be comparable to or less expensive than sports such as golf and skiing. Sailboat racing is one of the few sports in which people of all ages and genders can regularly compete with and against each other.
Most sailboat and yacht racing is done in coastal or inland waters. However, in terms of endurance and risk to life, ocean races such as the Volvo Ocean Race, the solo VELUX 5 Oceans Race, and the non-stop solo Vendée Globe, rate as some of the most extreme and dangerous sporting events. Not only do participants compete for days with little rest, but an unexpected storm, a single equipment failure, or collision with an ice floe could result in the sailboat being disabled or sunk hundreds or thousands of miles from search and rescue.
The sport of Sailboat racing is governed by the International Sailing Federation, and the rules under which competitors race are the Racing Rules of Sailing, which can be found on the ISAF web site.
Recreational sailing.
Sailing for pleasure can involve short trips across a bay, day sailing, coastal cruising, and more extended offshore or 'blue-water' cruising. These trips can be singlehanded or the vessel may be crewed by families or groups of friends. Sailing vessels may proceed on their own, or be part of a flotilla with other like-minded voyagers. Sailing boats may be operated by their owners, who often also gain pleasure from maintaining and modifying their craft to suit their needs and taste, or may be rented for the specific trip or cruise. A professional skipper and even crew may be hired along with the boat in some cases. People take cruises in which they crew and 'learn the ropes' aboard craft such as tall ships, classic sailing vessels and restored working boats.
Cruising trips of several days or longer can involve a deep immersion in logistics, navigation, meteorology, local geography and history, fishing lore, sailing knowledge, general psychological coping, and serendipity. Once the boat is acquired it is not all that expensive an endeavor, often much less expensive than a normal vacation on land. It naturally develops self-reliance, responsibility, economy, and many other useful skills. Besides improving sailing skills, all the other normal needs of everyday living must also be addressed. There are work roles that can be done by everyone in the family to help contribute to an enjoyable outdoor adventure for all.
A style of casual coastal cruising called gunkholing is a popular summertime family recreational activity. It consists of taking a series of day sails to out of the way places and anchoring overnight while enjoying such activities as exploring isolated islands, swimming, fishing, etc. Many nearby local waters on rivers, bays, sounds, and coastlines can become great natural cruising grounds for this type of recreational sailing. Casual sailing trips with friends and family can become lifetime bonding experiences.
Passagemaking.
Long-distance voyaging, such as that across oceans and between far-flung ports, can be considered the near-absolute province of the cruising sailboat. Most modern yachts of 25–55 feet long, propelled solely by mechanical powerplants, cannot carry the fuel sufficient for a point-to-point voyage of even 250–500 miles without needing to resupply; but a well-prepared sail-powered yacht of similar length is theoretically capable of sailing anywhere its crew is willing to guide it. Even considering that the cost benefits are offset by a much reduced cruising speed, many people traveling distances in small boats come to appreciate the more leisurely pace and increased time spent on the water.
Since the solo circumnavigation of Joshua Slocum in the 1890s, long-distance cruising under sail has inspired thousands of otherwise normal people to explore distant seas and horizons. The important voyages of Robin Lee Graham, Eric Hiscock, Don Street and others have shown that, while not strictly racing, ocean voyaging carries with it an inherent sense of competition, especially that between man and the elements.
Such a challenging enterprise requires keen knowledge of sailing in general as well as maintenance, navigation (especially celestial navigation), and often even international diplomacy (for which an entire set of protocols should be learned and practiced). But one of the great benefits to sailboat ownership is that one may at least imagine the type of adventure that the average affordable powerboat could never accomplish.

</doc>
<doc id="27675" url="https://en.wikipedia.org/wiki?curid=27675" title="Simple Mail Transfer Protocol">
Simple Mail Transfer Protocol

Simple Mail Transfer Protocol (SMTP) is an Internet standard for electronic mail (email) transmission. First defined by RFC 821 in 1982, it was last updated in 2008 with the Extended SMTP additions by RFC 5321—which is the protocol in widespread use today.
SMTP by default uses TCP port 25. The protocol for mail submission is the same, but uses port 587. SMTP connections secured by SSL, known as SMTPS, default to port 465 (nonstandard, but sometimes used for legacy reasons).
Although electronic mail servers and other mail transfer agents use SMTP to send and receive mail messages, user-level client mail applications typically use SMTP only for sending messages to a mail server for relaying. For retrieving messages, client applications usually use either POP3 or IMAP.
Although proprietary systems (such as Microsoft Exchange and IBM Notes) and webmail systems (such as Outlook.com, Gmail and Yahoo! Mail) use their own non-standard protocols to access mail box accounts on their own mail servers, all use SMTP when sending or receiving email from outside their own systems.
History.
Various forms of one-to-one electronic messaging were used in the 1960s. People communicated with one another using systems developed for specific mainframe computers. As more computers were interconnected, especially in the US Government's ARPANET, standards were developed to allow users of different systems to email one another. SMTP grew out of these standards developed during the 1970s.
SMTP can trace its roots to two implementations described in 1971: the Mail Box Protocol, whose implementation has been disputed, but is discussed in RFC 196 and other RFCs, and the SNDMSG program, which, according to RFC 2235, Ray Tomlinson of BBN invented for TENEX computers to send mail messages across the ARPANET. Fewer than 50 hosts were connected to the ARPANET at this time.
Further implementations include FTP Mail and Mail Protocol, both from 1973. Development work continued throughout the 1970s, until the ARPANET transitioned into the modern Internet around 1980. Jon Postel then proposed a Mail Transfer Protocol in 1980 that began to remove the mail's reliance on FTP. SMTP was published as RFC 788 in November 1981, also by Postel.
The SMTP standard was developed around the same time as Usenet, a one-to-many communication network with some similarities.
SMTP became widely used in the early 1980s. At the time, it was a complement to Unix to Unix Copy Program (UUCP) mail, which was better suited for handling email transfers between machines that were intermittently connected. SMTP, on the other hand, works best when both the sending and receiving machines are connected to the network all the time. Both use a store and forward mechanism and are examples of push technology. Though Usenet's newsgroups are still propagated with UUCP between servers, UUCP as a mail transport has virtually disappeared along with the "bang paths" it used as message routing headers.
Sendmail, released with 4.1cBSD, right after RFC 788, was one of the first mail transfer agents to implement SMTP. Over time, as BSD Unix became the most popular operating system on the Internet, sendmail became the most common MTA (mail transfer agent). Some other popular SMTP server programs include Postfix, qmail, Novell GroupWise, Exim, Novell NetMail, Microsoft Exchange Server and Oracle Communications Messaging Server.
Message submission (RFC 2476) and SMTP-AUTH (RFC 2554) were introduced in 1998 and 1999, both describing new trends in email delivery. Originally, SMTP servers were typically internal to an organization, receiving mail for the organization "from the outside", and relaying messages from the organization "to the outside". But as time went on, SMTP servers (mail transfer agents), in practice, were expanding their roles to become message submission agents for Mail user agents, some of which were now relaying mail "from the outside" of an organization. (e.g. a company executive wishes to send email while on a trip using the corporate SMTP server.) This issue, a consequence of the rapid expansion and popularity of the World Wide Web, meant that SMTP had to include specific rules and methods for relaying mail and authenticating users to prevent abuses such as relaying of unsolicited email (spam). Work on message submission (RFC 2476) was originally started because popular mail servers would often rewrite mail in an attempt to fix problems in it, for example, adding a domain name to an unqualified address. This behavior is helpful when the message being fixed is an initial submission, but dangerous and harmful when the message originated elsewhere and is being relayed. Cleanly separating mail into submission and relay was seen as a way to permit and encourage rewriting submissions while prohibiting rewriting relay. As spam became more prevalent, it was also seen as a way to provide authorization for mail being sent out from an organization, as well as traceability. This separation of relay and submission quickly became a foundation for modern email security practices.
As this protocol started out purely ASCII text-based, it did not deal well with binary files, or characters in many non-English languages. Standards such as Multipurpose Internet Mail Extensions (MIME) were developed to encode binary files for transfer through SMTP. Mail transfer agents (MTAs) developed after Sendmail also tended to be implemented 8-bit-clean, so that the alternate "just send eight" strategy could be used to transmit arbitrary text data (in any 8-bit ASCII-like character encoding) via SMTP. Mojibake was still a problem due to differing character set mappings between vendors, although the email addresses themselves still allowed only ASCII. 8-bit-clean MTAs today tend to support the 8BITMIME extension, permitting binary files to be transmitted almost as easily as plain text. Recently the SMTPUTF8 extension was created to support UTF-8 text, allowing international content and addresses in non-Latin scripts like Cyrillic or Chinese.
Many people contributed to the core SMTP specifications, among them Jon Postel, Eric Allman, Dave Crocker, Ned Freed, Randall Gellens, John Klensin, and Keith Moore.
Mail processing model.
Email is submitted by a mail client (MUA, mail user agent) to a mail server (MSA, mail submission agent) using SMTP on TCP port 587. Most mailbox providers still allow submission on traditional port 25. From there, the MSA delivers the mail to its mail transfer agent (MTA, mail transfer agent). Often, these two agents are just different instances of the same software launched with different options on the same machine. Local processing can be done either on a single machine, or split among various appliances; in the former case, involved processes can share files; in the latter case, SMTP is used to transfer the message internally, with each host configured to use the next appliance as a smart host. Each process is an MTA in its own right; that is, an SMTP server.
The boundary MTA has to locate the target host. It uses the Domain name system (DNS) to look up the mail exchanger record (MX record) for the recipient's domain (the part of the email address on the right of @). The returned MX record contains the name of the target host. The MTA next connects to the exchange server as an SMTP client. (The article on MX record discusses many factors in determining which server the sending MTA connects to.)
Message transfer can occur in a single connection between two MTAs, or in a series of hops through intermediary systems. A receiving SMTP server may be the ultimate destination, an intermediate "relay" (that is, it stores and forwards the message) or a "gateway" (that is, it may forward the message further using some protocol other than SMTP). Each hop implies a formal handoff of responsibility for the message, whereby the receiving server must either deliver the message or properly report the failure to do so.
Once the final hop accepts the incoming message, it hands it to a mail delivery agent (MDA) for local mail delivery. An MDA is able to save messages in the relevant mailbox format. Again, mail reception can be done using many computers or just one —the picture displays two nearby boxes in either case. An MDA may deliver messages directly to storage, or forward them over a network using SMTP, or any other means, including the Local Mail Transfer Protocol (LMTP), a derivative of SMTP designed for this purpose.
Once delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients (MUAs). Mail is retrieved by end-user applications, called email clients, using Internet Message Access Protocol (IMAP), a protocol that both facilitates access to mail and manages stored mail, or the Post Office Protocol (POP) which typically uses the traditional mbox mail file format or a proprietary system such as Microsoft Exchange/Outlook or Lotus Notes/Domino. Webmail clients may use either method, but the retrieval protocol is often not a formal standard.
SMTP defines message "transport", not the message "content". Thus, it defines the mail "envelope" and its parameters, such as the envelope sender, but not the header (except "trace information") nor the body of the message itself. STD 10 and RFC 5321 define SMTP (the envelope), while STD 11 and RFC 5322 define the message (header and body), formally referred to as the Internet Message Format.
Protocol overview.
SMTP is a connection-oriented, text-based protocol in which a mail sender communicates with a mail receiver by issuing command strings and supplying necessary data over a reliable ordered data stream channel, typically a Transmission Control Protocol (TCP) connection. An "SMTP session" consists of commands originated by an SMTP client (the initiating agent, sender, or transmitter) and corresponding responses from the SMTP server (the listening agent, or receiver) so that the session is opened, and session parameters are exchanged. A session may include zero or more SMTP transactions. An "SMTP transaction" consists of three command/reply sequences (see example below.) They are:
Besides the intermediate reply for DATA, each server's reply can be either positive (2xx reply codes) or negative. Negative replies can be permanent (5xx codes) or transient (4xx codes). A reject is a permanent failure by an SMTP server; in this case the SMTP client should send a bounce message. A drop is a positive response followed by message discard rather than delivery.
The initiating host, the SMTP client, can be either an end-user's email client, functionally identified as a mail user agent (MUA), or a relay server's mail transfer agent (MTA), that is an SMTP server acting as an SMTP client, in the relevant session, in order to relay mail. Fully capable SMTP servers maintain queues of messages for retrying message transmissions that resulted in transient failures.
A MUA knows the "outgoing mail" SMTP server from its configuration. An SMTP server acting as client, i.e. "relaying", typically determines which SMTP server to connect to by looking up the MX (Mail eXchange) DNS resource record for each recipient's domain name. Conformant MTAs (not all) fall back to a simple A record in case no MX record can be found. Relaying servers can also be configured to use a smart host.
An SMTP server acting as client initiates a TCP connection to the server on the "well-known port" designated for SMTP: port 25. MUAs should use port 587 to connect to an MSA. The main difference between an MTA and an MSA is that SMTP Authentication is mandatory for the latter only.
SMTP vs mail retrieval.
SMTP is a delivery protocol only. In normal use, mail is "pushed" to a destination mail server (or next-hop mail server) as it arrives. Mail is routed based on the destination server, not the individual user(s) to which it is addressed. Other protocols, such as the Post Office Protocol (POP) and the Internet Message Access Protocol (IMAP) are specifically designed for use by individual users retrieving messages and managing mail boxes. To permit an intermittently-connected mail server to "pull" messages from a remote server on demand, SMTP has a feature to initiate mail queue processing on a remote server (see Remote Message Queue Starting below). POP and IMAP are unsuitable protocols for relaying mail by intermittently-connected machines; they are designed to operate after final delivery, when information critical to the correct operation of mail relay (the "mail envelope") has been removed.
Remote Message Queue Starting.
Remote Message Queue Starting is a feature of SMTP that permits a remote host to start processing of the mail queue on a server so it may receive messages destined to it by sending the TURN command. This feature however was deemed insecure and was extended in RFC 1985 with the ETRN command which operates more securely using an authentication method based on Domain Name System information.
On-Demand Mail Relay.
On-Demand Mail Relay (ODMR) is an SMTP extension standardized in RFC 2645 that allows an intermittently-connected SMTP server to receive email queued for it when it is connected.
Internationalization.
Users whose native script is not Latin based, or who use diacritic not in the ASCII character set have had difficulty with the Latin email address requirement. RFC 6531 was created to solve that problem, providing internationalization features for SMTP, the SMTPUTF8 extension and support for multi-byte and non-ASCII characters in email addresses, such as those with diacritics and other language characters such as Greek and Chinese. 
Current support is limited, but there is strong interest in broad adoption of RFC 6531 and the related RFCs in countries like China that have a large user base where Latin (ASCII) is a foreign script.
Outgoing mail SMTP server.
An email client needs to know the IP address of its initial SMTP server and this has to be given as part of its configuration (usually given as a DNS name). This server will deliver outgoing messages on behalf of the user.
Outgoing mail server access restrictions.
Server administrators need to impose some control on which clients can use the server. This enables them to deal with abuse, for example spam. Two solutions have been in common use:
Restricting access by location.
Under this system, an ISP's SMTP server will not allow access by users who are outside the ISP's network. More precisely, the server may only allow access to users with an IP address provided by the ISP, which is equivalent to requiring that they are connected to the Internet using that same ISP. A mobile user may often be on a network other than that of their normal ISP, and will then find that sending email fails because the configured SMTP server choice is no longer accessible.
This system has several variations. For example, an organisation's SMTP server may only provide service to users on the same network, enforcing this by firewalling to block access by users on the wider Internet. Or the server may perform range checks on the client's IP address. These methods were typically used by corporations and institutions such as universities which provided an SMTP server for outbound mail only for use internally within the organisation. However, most of these bodies now use client authentication methods, as described below.
Where a user is mobile, and may use different ISPs to connect to the internet, this kind of usage restriction is onerous, and altering the configured outbound email SMTP server address is impractical. It is highly desirable to be able to use email client configuration information that does not need to change.
Client authentication.
Modern SMTP servers typically require authentication of clients by credentials before allowing access, rather than restricting access by location as described earlier. This more flexible system is friendly to mobile users and allows them to have a fixed choice of configured outbound SMTP server.
Open relay.
A server that is accessible on the wider Internet and does not enforce these kinds of access restrictions is known as an open relay. This is now generally considered a bad practice worthy of blacklisting.
Ports.
Communication between mail servers generally always uses the standard TCP port 25 designated for SMTP. 
Mail "clients" however generally don't use this, instead using specific "submission" ports. Mail services generally accept email submission from clients on one of:
Port 2525 and others may be used by some individual providers, but have never been officially supported.
Most Internet service providers now block all outgoing port 25 traffic from their customers as an anti-spam measure.
For the same reason, businesses will typically configure their firewall to only allow outgoing port 25 traffic from their designated mail servers.
SMTP transport example.
A typical example of sending a message via SMTP to two mailboxes ("alice" and "theboss") located in the same mail domain ("example.com" or "localhost.com") is reproduced in the following session exchange. (In this example, the conversation parts are prefixed with "S:" and "C:", for "server" and "client", respectively; these labels are not part of the exchange.)
After the message sender (SMTP client) establishes a reliable communications channel to the message receiver (SMTP server), the session is opened with a greeting by the server, usually containing its fully qualified domain name (FQDN), in this case "smtp.example.com". The client initiates its dialog by responding with a codice_1 command identifying itself in the command's parameter with its FQDN (or an address literal if none is available).
The client notifies the receiver of the originating email address of the message in a codice_2 command. In this example, the email message is sent to two mailboxes on the same SMTP server: one for each recipient listed in the To and Cc header fields. The corresponding SMTP command is codice_3. Each successful reception and execution of a command is acknowledged by the server with a result code and response message (e.g., 250 Ok).
The transmission of the body of the mail message is initiated with a codice_4 command after which it is transmitted verbatim line by line and is terminated with an end-of-data sequence. This sequence consists of a new-line (<CR><LF>), a single full stop (period), followed by another new-line. Since a message body can contain a line with just a period as part of the text, the client sends "two" periods every time a line starts with a period; correspondingly, the server replaces every sequence of two periods at the beginning of a line with a single one. Such escaping method is called "dot-stuffing".
The server's positive reply to the end-of-data, as exemplified, implies that the server has taken the responsibility of delivering the message. A message can be doubled if there is a communication failure at this time, e.g. due to a power shortage: Until the sender has received that 250 reply, it must assume the message was not delivered. On the other hand, after the receiver has decided to accept the message, it must assume the message has been delivered to it. Thus, during this time span, both agents have active copies of the message that they will try to deliver. The probability that a communication failure occurs exactly at this step is directly proportional to the amount of filtering that the server performs on the message body, most often for anti-spam purposes. The limiting timeout is specified to be 10 minutes.
The codice_5 command ends the session. If the email has other recipients located elsewhere, the client would codice_5 and connect to an appropriate SMTP server for subsequent recipients after the current destination(s) had been queued. The information that the client sends in the codice_1 and codice_2 commands are added (not seen in example code) as additional header fields to the message by the receiving server. It adds a codice_9 and codice_10 header field, respectively.
Some clients are implemented to close the connection after the message is accepted (codice_11), so the last two lines may actually be omitted. This causes an error on the server when trying to send the codice_12 reply.
Optional extensions.
Although optional and not shown in this example, many clients ask the server for the SMTP extensions that the server supports, by using the codice_13 greeting of the Extended SMTP specification released 1995. Clients fall back to codice_1 only if the server does not respond to codice_13., 
Modern clients may use the ESMTP extension keyword codice_16 to query the server for the maximum message size that will be accepted. Older clients and servers may try to transfer excessively sized messages that will be rejected after consuming network resources, including connect time to network links that is paid by the minute.
Users can manually determine in advance the maximum size accepted by ESMTP servers. The client replaces the codice_1 command with the codice_13 command.
Thus "smtp2.example.com" declares that it will accept a fixed maximum message size no larger than 14,680,064 octets (8-bit bytes). Depending on the server's actual resource usage, it may be currently unable to accept a message this large.
In the simplest case, an ESMTP server will declare a maximum codice_16 immediately after receiving an codice_13. According to RFC 1870, however, the numeric parameter to the codice_16 extension in the codice_13 response is optional. Clients may instead, when issuing a codice_2 command, include a numeric estimate of the size of the message they are transferring, so that the server can refuse receipt of overly-large messages.
Spoofing and spamming.
The original design of SMTP had no facility to authenticate senders, or check that servers were authorized to send on their behalf, with the result that email spoofing is possible, and commonly used in email spam and phishing. 
Occasional proposals are made to modify SMTP extensively or replace it completely. One example of this is Internet Mail 2000, but neither it, nor any other has made much headway in the face of the network effect of the huge installed base of classic SMTP. Instead, mail servers now use a range of techniques, including DomainKeys, DomainKeys Identified Mail, Sender Policy Framework and DMARC, DNSBLs and greylisting to reject or quarantine suspicious emails.

</doc>
<doc id="27676" url="https://en.wikipedia.org/wiki?curid=27676" title="Shuttlecock">
Shuttlecock

A shuttlecock (also called a bird or birdie) is a high-drag projectile used in the sport of badminton. It has an open conical shape: the cone is formed from 16 or so overlapping feathers, usually goose or duck, embedded into a rounded cork base. The cork is covered with thin leather. The shuttlecock's shape makes it extremely aerodynamically stable. Regardless of initial orientation, it will turn to fly cork first, and remain in the cork-first orientation. The name "shuttlecock" is frequently shortened to shuttle. The "shuttle" part of the name was probably derived from its back-and-forth motion during the game, resembling the shuttle of a loom; the "cock" part of the name was probably derived from the resemblance of the feathers to those on a cockerel.
Feathered vs. synthetic shuttlecocks.
The feathers are brittle; shuttlecocks break easily and often need to be replaced several times during a game. For this reason, synthetic shuttlecocks have been developed that replace the feathers with a plastic skirt. Players often refer to synthetic shuttlecocks as "plastics" and feathered shuttlecocks as "feathers". 
Feather Shuttles need to be properly humidified for at least 4 hours prior to play in order to fly the correct distance at the proper speed and to last longer. Correctly humidified feathers flex during play enhancing the shuttle's change-in-speed and durability. Dry feathers are brittle and break easily causing the shuttle to wobble. Saturated feathers are 'mushy', making the feather cone narrow too much when strongly hit, which causes the shuttle to fly overly far and fast. Humidification boxes are often used, but a simple moist sponge inserted in the feather end of the closed shuttle tube will work quite nicely. Water should never touch the cork of the shuttle. Shuttles are tested prior to play to make sure they fly true and at the proper speed and distance. Different weights of shuttles are used to compensate for local atmospheric conditions. Both humidity and height above sea level affect shuttle flight. A proper shuttle will generally travel from the back line of the court to just short of the long doubles service line on the opposite side of the net, with a full underhand hit from an average player. 
The cost of good quality feathers is similar to that of good quality plastics, but plastics are far more durable, typically lasting many matches without any impairment to their flight. 
Most experienced and skillful players greatly prefer feathers, and serious tournaments or leagues are always played using feather shuttlecocks of the highest quality. Experienced players generally prefer the "feel" of feathered shuttlecocks and assert that they are able to control the flight of feathers better than of plastics. In Asia, where feather shuttlecocks are more affordable than in Europe and North America, plastic shuttlecocks are hardly used at all.
The playing characteristics of plastics and feathers are substantially different. Plastics fly more slowly on initial impact, but slow down less towards the end of their flight. While feathers tend to drop straight down on a clear shot, plastics never quite return to a straight drop, falling more on a diagonal. Feather shuttles may come off the strings at speeds in excess of 320 km/h (200 mph) but slow down faster as they drop. For this reason, the feather shuttle makes the game seem faster, but also allows more time to play strokes. Because feather shuttles fly more quickly off the racquet face they also tend to cause less shoulder impact and injury.
Specifications.
A shuttlecock weighs around . It has 16 feathers with each feather in length. The diameter of the cork is and the diameter of the circle that the feathers make is around .

</doc>
<doc id="27679" url="https://en.wikipedia.org/wiki?curid=27679" title="Soldering iron">
Soldering iron

A soldering iron is a hand tool used in soldering. It supplies heat to melt solder so that it can flow into the joint between two workpieces.
A soldering iron is composed of a heated metal tip and an insulated handle. Heating is often achieved electrically, by passing an electric current (supplied through an electrical cord or battery cables) through a resistive heating element. Cordless irons can be heated by combustion of gas stored in a small tank, often using a catalytic heater rather than a flame. Simple irons less commonly used than in the past were simply a large copper bit on a handle, heated in a flame.
Soldering irons are most often used for installation, repairs, and limited production work in electronics assembly. High-volume production lines use other soldering methods. Large irons may be used for soldering joints in sheet metal objects. Less common uses include pyrography (burning designs into wood) and plastic welding.
Types.
Simple iron.
For electrical and electronics work, a low-power iron, a power rating between 15 and 35 watts, is used. Higher ratings are available, but do not run at higher temperature; instead there is more heat available for making soldered connections to things with large thermal capacity, for example, a metal chassis. Some irons are temperature-controlled, running at a fixed temperature in the same way as a soldering station, with higher power available for joints with large heat capacity. Simple irons run at an uncontrolled temperature determined by thermal equilibrium; when heating something large their temperature drops a little, possibly too much to melt solder.
Cordless iron.
Small irons heated by a battery, or by combustion of a gas such as butane in a small self-contained tank, can be used when electricity is unavailable or cordless operation is required. The operating temperature of these irons is not regulated directly; gas irons may change power by adjusting gas flow. Gas-powered irons may have interchangeable tips including different size soldering tips, hot knife for cutting plastics, miniature blow-torch with a hot flame, and small hot air blower for such applications as shrinking heat shrink tubing.
Temperature-controlled soldering iron.
Simple soldering irons reach a temperature determined by thermal equilibrium, dependent upon power input and cooling by the environment and the materials it comes into contact with. The iron temperature will drop when in contact with a large mass of metal such as a chassis; a small iron will lose too much temperature to solder a large connection. More advanced irons for use in electronics have a mechanism with a temperature sensor and method of temperature control to keep the tip temperature steady; more power is available if a connection is large. Temperature-controlled irons may be free-standing, or may comprise a head with heating element and tip, controlled by a base called a soldering station, with control circuitry and temperature adjustment and sometimes display.
A variety of means are used to control temperature. The simplest of these is a variable power control, much like a light dimmer, which changes the equilibrium temperature of the iron without automatically measuring or regulating the temperature. Another type of system uses a thermostat, often inside the iron's tip, which automatically switches power on and off to the element. A thermal sensor such as a thermocouple may be used in conjunction with circuitry to monitor the temperature of the tip and adjust power delivered to the heating element to maintain a desired temperature.
Another approach is to use magnetized soldering tips which lose their magnetic properties at a specific temperature, the Curie point. As long as the tip is magnetic, it closes a switch to supply power to the heating element. When it exceeds the design temperature it opens the contacts, cooling until the temperature drops enough to restore magnetisation. More complex Curie-point irons circulate a high-frequency AC current through the tip, using magnetic physics to direct heating only where the surface of the tip drops below the Curie point.
Soldering station.
A soldering station, invariably temperature-controlled, consists of an electrical power supply, control circuitry with provision for user adjustment of temperature and display, and a soldering iron or soldering head with a tip temperature sensor. The station will normally have a stand for the hot iron when not in use, and a wet sponge for cleaning. It is most commonly used for soldering electronic components. Other functions may be combined; for example a rework station, mainly for surface-mount components may have a hot air gun, vacuum pickup tool, and a soldering head; a desoldering station will have a desoldering head with vacuum pump for desoldering through-hole components, and a soldering iron head.
Soldering tweezers.
For soldering and desoldering small surface-mount components with two terminals, such as some links, resistors, capacitors, and diodes, soldering tweezers can be used; they can be either free-standing or controlled from a soldering station. The tweezers have two heated tips mounted on arms whose separation can be manually varied by squeezing gently against spring force, like simple tweezers; the tips are applied to the two ends of the component. The main purpose of the soldering tweezers is to melt solder in the correct place; components are usually moved by simple tweezers or vacuum pickup.
Hot Knife.
A hot knife is a form of soldering iron equipped with a double-edged blade that is situated on a heating element. These tools can reach temperatures of up to 1,000 degrees Fahrenheit (538 degrees Celsius) allowing for cuts of fabric and foam materials without worry of fraying or beading. Hot knives can be utilized in automotive, marine, and carpeting applications, as well as other industrial and personal uses.
Stands.
A soldering iron stand keeps the iron away from flammable materials, and often also comes with a cellulose sponge and flux pot for cleaning the tip. Some soldering irons for continuous and professional use come as part of a "soldering station," which allows the exact temperature of the tip to be adjusted, kept constant, and sometimes displayed.
Tips.
Most soldering irons for electronics have interchangeable tips, also known as "bits", that vary in size and shape for different types of work. Pyramid tips with a triangular flat face and chisel tips with a wide flat face are useful for soldering sheet metal. Fine conical or tapered chisel tips are typically used for electronics work. Tips may be straight or have a bend. Concave or wicking tips with a chisel face with a concave well in the flat face to hold a small amount of solder are available. Tip selection depends upon the type of work and access to the joint; soldering of 0.5mm pitch surface-mount ICs, for example, is quite different from soldering a through-hole connection to a large area. A concave tip well is said to help prevent bridging of closely spaced leads; different shapes are recommended to correct bridging that has occurred. Due to patent restrictions not all manufacturers offer concave tips everywhere; in particular there are restrictions in the USA.
Older and very cheap irons typically use a bare copper tip, which is shaped with a file or sandpaper. This dissolves gradually into the solder, suffering pitting and erosion of the shape. Copper tips are sometimes filed when worn down. Iron-plated copper tips have become increasingly popular since the 1980s. Because iron is not readily dissolved by molten solder, the plated tip is more durable than a bare copper one, though it will eventually wear out and need replacing. This is especially important when working at the higher temperatures needed for modern lead-free solders. Solid iron and steel tips are seldom used because they store less heat, and rusting can break the heating element.
Cleaning.
When the iron tip oxidises and burnt flux accumulates on it, solder no longer wets the tip, impeding heat transfer and making soldering difficult or impossible; tips must be periodically cleaned in use. Such problems happen with all kinds of solder, but are much more severe with the lead-free solders which have become widespread in electronics work, which require higher temperatures than solders containing lead. Exposed iron plating oxidises; if the tip is kept tinned with molten solder oxidation is inhibited. A clean unoxidised tip is tinned by applying a little solder and flux.
A wetted small sponge, often supplied with soldering equipment, can be used to wipe the tip. For lead-free solder a slightly more aggressive cleaning, with brass shavings, can be used. Soldering flux will help to remove oxide; the more active the flux the better the cleaning, although acidic flux used on circuit boards and not carefully cleaned off will cause corrosion. A tip which is cleaned but not retinned is susceptible to oxidation, particularly if wet.
Soldering iron tips are made of copper core plated with iron. The copper is used for heat transfer and the iron plating is used for durability. Copper is very easily corroded, eating away the tip, particularly in lead-free work; iron is not. Cleaning tips requires the removal of oxide without damaging the iron plating and exposing the copper to rapid corrosion. The use of solder already containing a small amount of copper can slow corrosion of copper tips.
In cases of severe oxidation not removable by gentler methods, abrasion with something hard enough to remove oxide but not so hard as to scratch the iron plating can be used. A brass wire scourer, brush, or wheel on a bench grinder, can be used with care. Sandpaper and other tools may be used but are likely to damage the plating.

</doc>
<doc id="27680" url="https://en.wikipedia.org/wiki?curid=27680" title="Supernova">
Supernova

A supernova is an astronomical event that occurs during the last stellar evolutionary stages of a massive star's life, whose dramatic and catastrophic destruction is marked by one final titanic explosion. For a short time, this causes the sudden appearance of a 'new' bright star, before slowly fading from sight over several weeks or months.
Only three Milky Way naked-eye supernova events have been observed during the last thousand years, though many have been telescopically seen in other galaxies. The most recent directly observed supernova in the Milky Way was Kepler's Star of 1604 (SN 1604), but remnants of two more recent supernovae have been found retrospectively. Statistical observations of supernovae in other galaxies suggest they should occur on average about three times every century in the Milky Way, and that any galactic supernova would almost certainly be observable in modern astronomical equipment.
Supernovae are more energetic than novae. In Latin, "Nova" means "new", referring astronomically to what appears to be a temporary new bright star. Adding the prefix "super-" distinguishes supernovae from ordinary novae, which are far less luminous. The word "supernova" was coined by Walter Baade and Fritz Zwicky in 1931. It is pronounced with the plural supernovae or supernovas (abbreviated SN, plural SNe after "supernovae").
During maximum brightness, the total equivalent radiant energies produced by supernovae may briefly outshine an entire output of a typical galaxy and emit energies equal to that created over the lifetime of any solar-like star. Such extreme catastrophes may also expel much, if not all, of its stellar material away from the star, at velocities up to or 10% of the speed of light. This drives an expanding and fast-moving shock wave into the surrounding interstellar medium, and in turn, sweeping up an expanding shell of gas and dust, which is observed as a supernova remnant. Supernovae create, fuse and eject the bulk of the chemical elements produced by nucleosynthesis. Supernovae play a significant role in enriching the interstellar medium with higher mass elements. Furthermore, the expanding shock waves from supernova explosions can trigger the formation of new stars. A great proportion of primary cosmic rays comes from supernovae, and they are also potentially strong galactic sources of gravitational waves.
Theoretical studies of many supernovae indicate that most are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a degenerate star or the sudden gravitational collapse of a massive star's core. In the first instance, a degenerate white dwarf may accumulate sufficient material from a binary companion, either through accretion or via a merger, to raise its core temperature enough to trigger runaway nuclear fusion, completely disrupting the star. In the second case, the core of a massive star may undergo sudden gravitational collapse, releasing gravitational potential energy as a supernova. While some observed supernovae are more complex than these two simplified theories, the astrophysical collapse mechanics have been established and accepted by most astronomers for some time.
Due to the wide range of astrophysical consequences of these events, astronomers now deem supernovae research, across the fields of stellar and galactic evolution, as an especially important area for investigation.
Observation history.
The earliest recorded supernova, SN 185, was viewed by Chinese astronomers in 185 AD, with the brightest recorded supernova being SN 1006, which occurred in 1006 AD and was described in detail by Chinese and Islamic astronomers. The widely observed supernova SN 1054 produced the Crab Nebula. Supernovae SN 1572 and SN 1604, the latest to be observed with the naked eye in the Milky Way galaxy, had notable effects on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was immutable. Johannes Kepler began observing SN 1604 at its peak on October 17, 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation (after SN 1572 seen by Tycho Brahe in Cassiopeia).
Before the development of the telescope, there have only been five supernovae seen in the last millennium. In the perspective of how long a star's lifetime is, its death is very brief. In fact, a star's death may only last a few months. Due to this, a typical human will only experience this rarity, on average, once in their lifetime. This is a microscopic fraction in comparison to the 100 billion stars that compose a galaxy. However, since the use of modern equipment, particularly in this millennium, professional and amateur astronomers have been finding several hundreds of supernovae each year.
The field of supernova discovery has extended to other galaxies, starting with SN 1885A in the Andromeda galaxy. American astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. In the 1960s, astronomers found that the maximum intensities of supernova explosions could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae recently observed appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernova explosions that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the decay of titanium-44.
The brightest observed supernova, ASASSN-15lh, was detected in June 2015. With a brightness of 570 billion Suns, ASASSN-15lh's peak luminosity was twice that of the previous record holder.
Discovery.
Early work on what was originally believed to be simply a new category of novae was performed during the 1930s by Walter Baade and Fritz Zwicky at Mount Wilson Observatory. The name "super-novae" was first used during 1931 lectures held at Caltech by Baade and Zwicky, then used publicly in 1933 at a meeting of the American Physical Society. By 1938, the hyphen had been lost and the modern name was in use. Because supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies.
Supernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. Most scientific interest in supernovae—as standard candles for measuring distance, for example—require an observation of their peak luminosity. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.
Toward the end of the 20th century astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. Recently the Supernova Early Warning System (SNEWS) project has begun using a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are particles that are produced in great quantities by a supernova explosion, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.
Supernova searches fall into two classes: those focused on relatively nearby events and those looking for explosions farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of "z" = 0.1–0.3—where "z" is a dimensionless measure of the spectrum's frequency shift.
High redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies. (See also Hubble's law).
Naming convention.
Supernova discoveries are reported to the International Astronomical Union's Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is the marker "SN" followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from "A" to "Z". Afterward pairs of lower-case letters are used: "aa", "ab", and so on. Hence, for example, "SN 2003C" designates the third supernova reported in the year 2003. The last supernova of 2005 was SN 2005nc, indicating that it was the 367th supernova found in 2005. Since 2000, professional and amateur astronomers have been finding several hundreds of supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).
Historical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called "Tycho's Nova") and SN 1604 ("Kepler's Star"). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (e.g. SN 1885A, SN 1907A, etc.) — this last happened with SN 1947A. "SN", for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, however, they have been needed every year.
Classification.
As part of the attempt to understand supernovae, astronomers have classified them according to their light curves and the absorption lines of different chemical elements that appear in their spectra. The first element for division is the presence or absence of a line caused by hydrogen. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified "Type II"; otherwise it is "Type I". In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).
Type I.
The type I supernovae are subdivided on the basis of their spectra, with type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as types Ib and Ic, with type Ib showing strong neutral helium lines and type Ic lacking them. The light curves are all similar, although type Ia are generally brighter at peak luminosity, but the light curve is not important for classification of type I supernovae.
A small number of type Ia supernovae exhibit unusual features such as non-standard luminosity or broadened light curves, and these are typically classified by referring to the earliest example showing similar features. For example, the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.
Type II.
The supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the 'n' stands for 'narrow'.
A few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term "Type IIb" is used to describe the combination of features normally associated with Types II and Ib.
Type II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive "plateau" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called type II-P referring to the plateau. Less common are type II-L supernovae that lack a distinct plateau. The "L" signifies "linear" although the light curve is not actually a straight line.
Supernovae that do not fit into the normal classifications are designated peculiar, or 'pec'.
Types III, IV, and V.
Fritz Zwicky defined additional supernovae types, although based on a very few examples that didn't cleanly fit the parameters for a type I or type II supernova. SN 1961i in NGC 4303 was the prototype and only member of the type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the type IV class, with a light curve similar to a type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova imposter with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible type IV or type V supernovae.
These types would now all be treated as peculiar type II supernovae, of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an imposter.
Current models.
[[File:The Rise and Fall of a Supernova.jpg|thumb|Sequence shows the rapid brightening and slower fading of a supernova explosion in the galaxy NGC 1365]]
The type codes, described above given to supernovae, are "taxonomic" in nature: the type number describes the light observed from the supernova, not necessarily its cause. For example, type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors while the spectrally similar type Ib/c are produced from massive Wolf-Rayet progenitors by core collapse. The following summarizes what is currently believe to be the most plausible explanations for supernovae.
Thermal runaway.
A white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorized to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties, and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high red shift, and for small variations in brightness identified by light curve shape or spectrum.
Normal Type Ia.
There are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses () (for a non-rotating star), it would no longer be able to support the bulk of its mass through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%), before collapse is initiated.
Within a few seconds, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–) to unbind the star in a supernova explosion. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.
The model for the formation of this category of supernova is a closed binary star system. The larger of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. Despite widespread acceptance of the basic model, the exact details of initiation and of the heavy elements produced in the explosion are still unclear.
Type Ia supernovae follow a characteristic light curve—the graph of luminosity as a function of time—after the explosion. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about −19.3. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.
Non-standard Type Ia.
Another model for the formation of a Type Ia explosion involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. There is much variation in this type of explosion, and in many cases there may be no supernova at all, but it is expected that they will have a broader and less luminous light curve than the more normal Type Ia explosions.
Abnormally bright Type Ia supernovae are expected when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy.
There is no formal sub-classification for the non-standard Type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.
One specific type of non-standard Type Ia supernova develops hydrogen, and other, emission lines and gives the appearance of mixture between a normal Type Ia and a Type IIn supernova. Examples are SN 2002ic and SN 2005gj. These supernova have been dubbed Type Ia/IIn, Type Ian, Type IIa and Type IIan.
Core collapse.
Very massive stars can undergo core collapse when nuclear fusion suddenly becomes unable to sustain the core against its own gravity; this is the cause of all types of supernova except type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova, or the release of gravitational potential energy may be insufficient and the star may collapse into a black hole or neutron star with little radiated energy.
Core collapse can be caused by several different mechanisms: electron capture; exceeding the Chandrasekhar limit; pair-instability; or photodisintegration. When a massive star develops an iron core larger than the Chandrasekhar mass it will no longer be able to support itself by electron degeneracy pressure and will collapse further to a neutron star or black hole. Electron capture by magnesium in a degenerate O/Ne/Mg core causes gravitational collapse followed by explosive oxygen fusion, with very similar results. Electron-positron pair production in a large post-helium burning core removes thermodynamic support and causes initial collapse followed by runaway fusion, resulting in a pair-instability supernova. A sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.
The table below lists the known reasons for core collapse in massive stars, the types of star that they occur in, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.
Type IIn supernovae are not listed in the table. They can potentially be produced by various types of core collapse in different progenitor stars, possibly even by type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed type IIn supernovae are actually supernova imposters, massive eruptions of LBV-like stars similar to the Great Eruption Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.
When a stellar core is no longer supported against gravity it collapses in on itself with velocities reaching 70,000 km/s (0.23c), resulting in a rapid increase in temperature and density. What follows next depends on the mass and structure of the collapsing core, with low mass degenerate cores forming neutron stars, higher mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.
The initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30 km diameter and a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova explosion.
In lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100 billion kelvin, 6000 times the temperature of the sun's core. At this temperature, neutrino-antineutrino pairs of all flavors are efficiently formed by thermal emission. These thermal neutrinos are several times more abundant than the electron-capture neutrinos. About 1046 joules, approximately 10% of the star's rest mass, is converted into a ten-second burst of neutrinos which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls within milliseconds in the outer core as energy is lost through the dissociation of heavy elements. A process that is is necessary to allow the outer layers of the core to reabsorb around 1044 joules (1 foe) from the neutrino pulse, producing the visible explosion, although there are also other theories on how to power the explosion.
Some material from the outer envelope falls back onto the neutron star, and for cores beyond about there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy of the explosion and the mass of expelled radioactive material, but in some situations it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.
Collapse of massive non-degenerate cores will ignite further fusion. When the core collapse is initiated by pair instability, oxygen fusion begins and the collapse may be halted. For core masses of , the collapse halts and the star remains intact, but core collapse will occur again when a larger core has formed. For cores of around , the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected 56Ni. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.
Type II.
Stars with initial masses less than about eight times the sun never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least (possibly as much as ) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.
If core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a type II supernova. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.
Stars with an initial mass up to about 90 times the sun, or a little less at high metallicity, are expected to result in a type II-P supernova which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a type II-L supernova. At very low metallicity, stars of around will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with type II characteristics but a very large mass of ejected 56Ni and high luminosity.
Type Ib and Ic.
[[Image:Supernova 2008D.jpg|thumb|SN 2008D, a Type Ib supernova, shown in X-ray (left) and visible light (right) at the far upper end of the galaxy]]
These supernovae, like those of Type II, are massive stars that undergo core collapse. However the stars which become Types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf-Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass loss rates. Observations of type Ib/c supernova do not match the observed or expected occurrence of Wolf Rayet stars and alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed. Since a supernova explosion can occur whenever the mass of the star at the time of core collapse is low enough not to cause complete fallback to a black hole, any massive star may result in a supernova if it loses enough mass before core collapse occurs.
Type Ib supernovae are the more common and result from Wolf-Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining and these are the progenitors of type Ic supernovae.
A few percent of the Type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the geometry of the explosion. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell of the explosion to produce a super-luminous supernova.
Ultra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (~0.1 MSun). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be an observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core.
Failed.
The core collapse of some massive stars may not result in a visible supernova. The main model for this is a sufficiently massive core that the explosion is insufficient to reverse the infall of the outer layers onto a black hole. These events are difficult to detect, but large surveys have detected possible candidates.
Light curves.
A historic puzzle concerned the source of energy that can maintain the optical supernova glow for months. Although the energy that disrupts each type of supernovae is delivered promptly, the light curves are mostly dominated by subsequent radioactive heating of the rapidly expanding ejecta. Some have considered rotational energy from the central pulsar. The ejecta gases would dim quickly without some energy input to keep it hot. The intensely radioactive nature of the ejecta gases, which is now known to be correct for most supernovae, was first calculated on sound nucleosynthesis grounds in the late 1960s. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.
It is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the explosion of a Type II Supernova such as SN 1987A is provided its energy by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons , primarily of 847keV and 1238keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half life 6 days) while energy for the later light curve in particular fit very closely with the 77.3 day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.
The visual light curves of the different supernova types all depend at late times on radioactive heating, but they vary in shape and amplitude on the underlying mechanisms of the explosion, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial explosion, but that breakout is hardly detectable optically.
The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of nickel-56 (half life 6 days), which then decays to radioactive cobalt-56 (half life 77 days). These radioisotopes from material ejected in the explosion excite surrounding material to incandescence. Studies of cosmology today rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of Type Ia, which are the "standard candles" of cosmology but whose diagnostic 847keV and 1238keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission becomes dominant from the remaining cobalt-56, although this portion of the light curve has been little-studied.
Type Ib and Ic light curves are basically similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of nickel-56 produced in these types of explosion. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.
The light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial explosion this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.
In type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.
Type IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a hypernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.
Large numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.
Notes:
Asymmetry.
[[Image:Chandra-crab.jpg|thumb|The pulsar in the Crab nebula is travelling at 375 km/s relative to the nebula.]]
A long-standing puzzle surrounding Type II supernovae is why the compact object remaining after the explosion is given a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an asymmetry in the explosion, but the mechanism by which momentum is transferred to the compact object a puzzle. Proposed explanations for this kick include convection in the collapsing star and jet production during neutron star formation.
One possible explanation for the asymmetry in the explosion is large-scale convection above the core. The convection can create variations in the local abundances of elements, resulting in uneven nuclear burning during the collapse, bounce and resulting explosion.
Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova explosion. (A similar model is now favored for explaining long gamma-ray bursts.)
Initial asymmetries have also been confirmed in Type Ia supernova explosions through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the explosion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarization of the emitted light.
Energy output.
Although we are used to thinking of supernovae primarily as luminous visible events, the electromagnetic radiation they release is almost a minor side-effect of the explosion. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.
There is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the following main explosion 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.
Type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the end result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (an anti-electron) path with a half life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.
Core collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher. In the case of supernovae, the energy of gravitational potential energy is converted into kinetic energy that compresses and collapses the core, initially producing electron neutrinos from disintegrating nucleons, followed by all flavours of thermal neutrinos from the super-heated neutron star core. Around 1% of these neutrinos are thought to deposit sufficient energy into the outer layers of the star to drive the resulting explosion, but again the details cannot be reproduced exactly in current models. Kinetic energies and nickel yields are somewhat lower than type Ia supernovae, hence the lower visual luminosity of supernovae, but energy from the de-ionisation of the many solar masses of remaining hydrogen can contribute to a much slower decline in luminosity and produce the plateau phase seen in the majority of core collapse supernovae.
In some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high luminosity supernovae and is thought to be the cause of type Ic hypernovae and long duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.
When a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial explosion energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.
Although pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature of that explosion following core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen, and silicon. The total energy released by the highest mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.
Progenitor.
The supernova classification type is closely tied to the type of star at the time of the explosion. The occurrence of each type of supernova depends dramatically on the metallicity and hence the age of the host galaxy.
Type Ia supernovae are produced from white dwarf stars in binary systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.
Type Ib/c and II-L, and possibly most type IIn, supernovae are only thought to be produced from stars having near-solar metallicity levels that result in high mass loss from massive stars, hence they are less common in older more distant galaxies. The table shows the expected progenitor for the main types of core collapse supernova, and the approximate proportions of each in the local neighbourhood.
There are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the expected progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about and respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. It is now proposed that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.
Until just a few decades ago, hot supergiants were not considered likely to explode, but observations have shown otherwise. Blue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf-Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova.
The expected progenitors of type Ib supernovae, luminous WC stars, are not observed at all. Instead WC stars are found at lower luminosities, apparently post-red supergiant stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disupted.
Interstellar impact.
Source of heavy elements.
Supernovae are the major source of elements heavier than oxygen. These elements are produced by nuclear fusion for nuclei up to 34S, by silicon photodisintegration rearrangement and quasiequilibrium (see Supernova nucleosynthesis) during silicon burning for nuclei between 36Ar and 56Ni, and by rapid captures of neutrons during the supernova explosion for elements heavier than iron. Nucleosynthesis during silicon burning yields nuclei roughly 1000-100,000 times more abundant than the r-process isotopes heavier than iron. Supernovae are the most likely, although not undisputed, candidate sites for the r-process, which is the rapid capture of neutrons that occurs at high temperature and high density of neutrons. Those reactions produce highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. The r-process produces about half of all the heavier isotopes of the elements beyond iron, including plutonium and uranium. The only other major competing process for producing elements heavier than iron is the s-process in large, old, red-giant AGB stars, which produces these elements slowly over longer epochs and which cannot produce elements heavier than lead.
Role in stellar evolution.
The remnant of a supernova explosion consists of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up the surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.
The Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesized in stars and supernovae. Supernovae tend to enrich the surrounding interstellar medium with "metals"—elements other than hydrogen and helium.
These injected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may decisively influence the possibility of having planets orbiting it.
The kinetic energy of an expanding supernova remnant can trigger star formation due to compression of nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.
Evidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.
Effect on Earth.
A near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3000 light-years away. Gamma rays from a supernova would induce a chemical reaction in the upper atmosphere converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful solar radiation. This has been proposed as the cause of the Ordovician–Silurian extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.
In 1996 it was theorized that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted levels of nitrogen oxides, which became trapped in the ice.
Type Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars in binary systems, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest known candidate is IK Pegasi (see below). Recent estimates predict that a Type II supernova would have to be closer than eight parsecs (26 light-years) to destroy half of the Earth's ozone layer, and there are no such candidates closer than about 500 light years.
Milky Way candidates.
The next supernova in the Milky Way will almost certainly be detectable even if it occurs on the far side of the galaxy. It is likely to be produced by the collapse of an unremarkable red supergiant and it is very probable that it will already have been catalogued in infrared surveys such as 2MASS. There is a smaller chance that the next core collapse supernova will be produced by a different type of massive star such as a yellow hypergiant, luminous blue variable, or Wolf-Rayet. The chances of the next supernova being a type Ia produced by a white dwarf are calculated to be about a third of those for a core collapse supernova. Again it should be observable wherever it occurs, but it is less likely that the progenitor will ever have been observed prior to the explosion. It isn't even known exactly what a type Ia progenitor system looks like, and difficult to detect them beyond a few parsecs. The total supernova rate in our galaxy is estimated to be about 4.6 per century, or one every 22 years, although we haven't actually observed one for several centuries.
Statistically, the next supernova is likely to be produced from an otherwise unremarkable red supergiant, but it is difficult to identify which of those supergiants are in the final stages of heavy element fusion in their cores and which have millions of years left. The most massive red supergiants are expected to shed their atmospheres and evolve to Wolf-Rayet stars before their cores collapse. All Wolf-Rayet stars are expected to end their lives from the Wolf-Rayet phase within a million years or so, but again it is difficult to identify those that are closest to core collapse. One class that is expected to have no more than a few thousand years before exploding are the WO Wolf-Rayet stars, which are known to have exhausted their core helium. Only eight of them are known, and only four of those are in the Milky Way.
A number of close or well known stars have been identified as possible core collapse supernova candidates: the red supergiants Antares and Betelgeuze; the yellow hypergiant Rho Cassiopeiae; the luminous blue variable Eta Carinae that has already produced a supernova imposter explosion; and the Wolf–Rayet star Gamma Velorum, Others have gained notoriety as possible, although not very likely, progenitors for a gamma-ray burst; for example WR 104.
Identification of candidates for a type Ia supernova explosion is much more speculative. Any binary with an accreting white dwarf might produce a supernova although the exact mechanism and timescale is still debated. These systems are faint and difficult to identify, but the novae and recurrent novae are such systems that conveniently advertise themselves. One examples is U Scorpii, The nearest known type Ia supernova candidate is IK Pegasi (HR 8210), located at a distance of 150 light-years, but observations suggest it will be several million years before the white dwarf can accrete the critical mass required to become a Type Ia supernova.

</doc>
<doc id="27681" url="https://en.wikipedia.org/wiki?curid=27681" title="Sergei Prokofiev">
Sergei Prokofiev

Sergei Sergeyevich Prokofiev (; ; 11/23 April 1891 – 5 March 1953) was a Russian and Soviet composer, pianist and conductor. As the creator of acknowledged masterpieces across numerous musical genres, he is regarded as one of the major composers of the 20th century. His works include such widely heard works as the March from "The Love for Three Oranges," the suite "Lieutenant Kijé", the ballet "Romeo and Juliet" – from which "Dance of the Knights" is taken – and "Peter and the Wolf." Of the established forms and genres in which he worked, he created – excluding juvenilia – seven completed operas, seven symphonies, eight ballets, five piano concertos, two violin concertos, a cello concerto, a Symphony-Concerto for cello and orchestra, and nine completed piano sonatas.
A graduate of the St Petersburg Conservatory, Prokofiev initially made his name as an iconoclastic composer-pianist, achieving notoriety with a series of ferociously dissonant and virtuosic works for his instrument, including his first two piano concertos. In 1915 Prokofiev made a decisive break from the standard composer-pianist category with his orchestral "Scythian Suite", compiled from music originally composed for a ballet commissioned by Sergei Diaghilev of the Ballets Russes. Diaghilev commissioned three further ballets from Prokofiev – "Chout," "Le pas d'acier" and "The Prodigal Son" – which at the time of their original production all caused a sensation among both critics and colleagues. Prokofiev's greatest interest, however, was opera, and he composed several works in that genre, including "The Gambler" and "The Fiery Angel". Prokofiev's one operatic success during his lifetime was "The Love for Three Oranges," composed for the Chicago Opera and subsequently performed over the following decade in Europe and Russia.
After the Revolution, Prokofiev left Russia with the official blessing of the Soviet minister Anatoly Lunacharsky, and resided in the United States, then Germany, then Paris, making his living as a composer, pianist and conductor. During that time he married a Spanish singer, Carolina Codina, with whom he had two sons. In the early 1930s, the Great Depression diminished opportunities for Prokofiev's ballets and operas to be staged in America and western Europe. Prokofiev, who regarded himself as composer foremost, resented the time taken by touring as a pianist, and increasingly turned to Soviet Russia for commissions of new music; in 1936 he finally returned to his homeland with his family. He enjoyed some success there – notably with "Lieutenant Kijé," "Peter and the Wolf," "Romeo and Juliet," and perhaps above all with "Alexander Nevsky."
The Nazi invasion of the USSR spurred him to compose his most ambitious work, an operatic version of Leo Tolstoy's "War and Peace". In 1948 Prokofiev was criticized for "anti-democratic formalism" and, with his income severely curtailed, was forced to compose Stalinist works, such as "On Guard for Peace". However, he also enjoyed personal and artistic support from a new generation of Russian performers, notably Sviatoslav Richter and Mstislav Rostropovich: for the latter, he composed his Symphony-Concerto, whilst for the former he composed his ninth piano sonata.
Biography.
Early childhood and first compositions.
Prokofiev was born in 1891 in Sontsovka (now Krasne, Krasnoarmiisk Raion, Donetsk Oblast, eastern Ukraine), a remote rural estate in the Yekaterinoslav Governorate of the Russian Empire. His father, Sergei Alexeyevich Prokofiev, was an agronomist. Prokofiev's mother, Maria (née Zhitkova), came from a family of former serfs who had been owned by the Sheremetev family, under whose patronage serf-children were taught theatre and arts from an early age. She was described by Reinhold Glière (Prokofiev's first composition teacher) as "a tall woman with beautiful, clever eyes ... who knew how to create an atmosphere of warmth and simplicity about her." After their wedding in the summer of 1877, the Prokofievs had moved to a small estate in the Smolensk governorate. Eventually Sergei Alexeyevich found employment as a soil engineer, employed by one of his former fellow-students, Dmitri Sontsov, to whose estate in the Ukrainian steppes the Prokofievs moved.
By the time of Prokofiev's birth Maria, having previously lost two daughters, had devoted her life to music; during her son's early childhood she spent two months a year in Moscow or St Petersburg taking piano lessons. Sergei Prokofiev was inspired by hearing his mother practicing the piano in the evenings – mostly works by Chopin and Beethoven – and composed his first piano composition at the age of five, an 'Indian Gallop', which was written down by his mother: this was in the F Lydian mode (a major scale with a raised 4th scale degree) as the young Prokofiev felt 'reluctance to tackle the black notes'. By seven, he had also learned to play chess. Much like music, chess would remain a passion, and he became acquainted with world chess champions José Raúl Capablanca, whom he beat in a simultaneous exhibition match in 1914, and Mikhail Botvinnik, with whom he played several matches in the 1930s. At the age of nine he was composing his first opera, "The Giant", as well as an overture and various other pieces.
Formal education and controversial early works.
In 1902, Prokofiev's mother met Sergei Taneyev, director of the Moscow Conservatory, who initially suggested that Prokofiev should start lessons in piano and composition with Alexander Goldenweiser. When Taneyev was unable to arrange this, he instead organised that composer and pianist Reinhold Glière should spend the summer of 1902 in Sontsovka teaching Prokofiev. This first series of lessons culminated, at the 11-year-old Prokofiev's insistence, with the budding composer making his first attempt to write a symphony. The following summer Glière revisited Sontsovka to give further tuition. When decades later Prokofiev wrote about his lessons with Glière, he gave due credit to his teacher's sympathetic method but complained that Glière had introduced him to "square" phrase structure and conventional modulations which he subsequently had to unlearn. Nonetheless, equipped with the necessary theoretical tools, Prokofiev started experimenting with dissonant harmonies and unusual time signatures in a series of short piano pieces which he called "ditties" (after the so-called "song form" – more accurately ternary form – they were based on), laying the basis for his own musical style.
Despite his growing talent, Prokofiev's parents hesitated over starting their son on a musical career at such an early age, and considered the possibility of his attending a quality high school in Moscow. By 1904, his mother had decided instead on Saint Petersburg, and she and Prokofiev visited the (then) capital to explore the possibility of their moving there for his education. They were introduced to composer Alexander Glazunov, a professor at the Conservatory, who asked to see Prokofiev and his music; Glazunov was so impressed that he urged Prokofiev's mother that her son apply to the Saint Petersburg Conservatory. By this point, Prokofiev had composed two more operas, "Desert Islands" and "The Feast during the Plague", and was working on his fourth, "Undina". He passed the introductory tests and entered the Conservatory that same year.
Several years younger than most of his class, he was viewed as eccentric and arrogant, and he annoyed a number of his classmates by keeping statistics on the errors made by fellow students. During this period, he studied under, among others, Alexander Winkler for piano, Anatoly Lyadov for harmony and counterpoint, Nikolai Tcherepnin for conducting, and Nikolai Rimsky-Korsakov for orchestration (though when Rimsky-Korsakov died in 1908, Prokofiev noted that he had only studied with him "after a fashion" – he was just one of many students in a heavily attended class—and regretted that he otherwise "never had the opportunity to study with him"). He also shared classes with the composers Boris Asafyev and Nikolai Myaskovsky, the latter becoming a relatively close and lifelong friend.
As a member of the Saint Petersburg music scene, Prokofiev developed a reputation as a musical rebel, while getting praise for his original compositions, which he performed himself on the piano. In 1909, he graduated from his class in composition with unimpressive marks. He continued at the Conservatory, studying piano under Anna Yesipova and continuing his conducting lessons under Tcherepnin.
In 1910, Prokofiev's father died and Sergei's financial support ceased. Fortunately he had started making a name for himself as a composer and pianist outside the Conservatory, making appearances at the St Petersburg Evenings of Contemporary Music. There he performed several of his more adventurous piano works, such as his highly chromatic and dissonant Etudes, Op. 2 (1909). His performance of this impressed the organizers of Evenings sufficiently for them to invite Prokofiev to give the Russian premiere of Arnold Schoenberg's Drei Klavierstücke, Op. 11. Prokofiev's harmonic experimentation continued with "Sarcasms" for piano, Op. 17 (1912), which makes extensive use of polytonality. He composed his first two piano concertos around this time, the latter of which caused a scandal at its premiere (23 August 1913, Pavlovsk). According to one account, the audience left the hall with exclamations of "'To hell with this futuristic music! The cats on the roof make better music!'", but the modernists were in rapture.
In 1911, help arrived from renowned Russian musicologist and critic Alexander Ossovsky, who wrote a supportive letter to music publisher Boris P. Jurgenson (son of publishing-firm founder Peter Jurgenson [1836–1904]); thus a contract was offered to the composer. Prokofiev made his first foreign trip in 1913, travelling to Paris and London where he first encountered Sergei Diaghilev's Ballets Russes.
The first ballets.
In 1914, Prokofiev finished his career at the Conservatory by entering the so-called 'battle of the pianos', a competition open to the five best piano students for which the prize was a Schreder grand piano: Prokofiev won by performing his own Piano Concerto No. 1. Soon afterwards, he journeyed to London where he made contact with the impresario Sergei Diaghilev. Diaghilev commissioned Prokofiev's first ballet, "Ala and Lolli"; but when Prokofiev brought the work in progress to him in Italy in 1915 he rejected it as "non-Russian". Urging Prokofiev to write "music that was national in character", Diaghilev then commissioned the ballet "Chout" ("The Fool", the original Russian-language full title was Сказка про шута, семерых шутов перешутившего (Skazka pro shuta, semerykh shutov pereshutivshavo), meaning "The Tale of the Buffoon who Outwits Seven Other Buffoons"). Under Diaghilev's guidance, Prokofiev chose his subject from a collection of folktales by the ethnographer Alexander Afanasyev; the story, concerning a buffoon and a series of confidence tricks, had been previously suggested to Diaghilev by Igor Stravinsky as a possible subject for a ballet, and Diaghilev and his choreographer Léonide Massine helped Prokofiev to shape this into a ballet scenario. Prokofiev's inexperience with ballet led him to revise the work extensively in the 1920s, following Diaghilev's detailed critique, prior to its first production. The ballet's premiere in Paris on 17 May 1921 was a huge success and was greeted with great admiration by an audience that included Jean Cocteau, Igor Stravinsky and Maurice Ravel. Stravinsky called the ballet "the single piece of modern music he could listen to with pleasure," while Ravel called it "a work of genius."
First World War and Revolution.
During World War I, Prokofiev returned to the Conservatory and studied organ in order to avoid conscription. He composed "The Gambler" based on Fyodor Dostoyevsky's novel of the same name, but rehearsals were plagued by problems and the scheduled 1917 première had to be canceled because of the February Revolution. In the summer of that year, Prokofiev composed his first symphony, the "Classical". This was his own name for the symphony, which was written in the style that, according to Prokofiev, Joseph Haydn would have used if he had been alive at the time. It is more or less Classical in style but incorporates more modern musical elements (see Neoclassicism). This symphony was also an exact contemporary of Prokofiev's Violin Concerto No. 1 in D major, Op. 19, which was scheduled to premiere in November 1917. The first performances of both works had to wait until 21 April 1918 and 18 October 1923, respectively. He stayed briefly with his mother in Kislovodsk in the Caucasus. After completing the score of "Seven, They Are Seven", a "Chaldean invocation" for chorus and orchestra, Prokofiev was "left with nothing to do and time hung heavily on my hands". Believing that Russia "had no use for music at the moment", Prokofiev decided to try his fortunes in America until the turmoil in his homeland had passed. He set out for Moscow and Petersburg in March 1918 to sort out financial matters and to arrange for his passport. In May he headed for the USA, having obtained official permission to do so from Anatoly Lunacharsky, the People's Commissar for Education, who told him: "You are a revolutionary in music, we are revolutionaries in life. We ought to work together. But if you want to go to America I shall not stand in your way."
Life abroad.
Arriving in San Francisco after having been released from questioning by immigration officials on Angel Island on 11 August 1918, Prokofiev was soon compared to other famous Russian exiles (such as Sergei Rachmaninoff). His debut solo concert in New York led to several further engagements. He also received a contract from the music director of the Chicago Opera Association, Cleofonte Campanini, for the production of his new opera "The Love for Three Oranges"; however, due to Campanini's illness and death, the premiere was postponed. This delay was another example of Prokofiev's bad luck in operatic matters. The failure also cost him his American solo career, since the opera took too much time and effort. He soon found himself in financial difficulties, and, in April 1920, he left for Paris, not wanting to return to Russia as a failure.
In Paris Prokofiev reaffirmed his contacts with Diaghilev's Ballets Russes. He also completed some of his older, unfinished works, such as the Third Piano Concerto. "The Love for Three Oranges" finally premièred in Chicago, under the composer's baton, on 30 December 1921. Diaghilev became sufficiently interested in the opera to request Prokofiev play the vocal score to him in June 1922, while they were both in Paris for a revival of "Chout", so he could consider it for a possible production. Stravinsky, who was present at the audition, refused to listen to more than the first act. When he then accused Prokofiev of "wasting time composing operas", Prokofiev retorted that Stravinsky "was in no position to lay down a general artistic direction, since he is himself not immune to error". According to Prokofiev, Stravinsky "became incandescent with rage" and "we almost came to blows and were separated only with difficulty". As a result, "our relations became strained and for several years Stravinsky's attitude toward me was critical."
In March 1922, Prokofiev moved with his mother to the town of Ettal in the Bavarian Alps, where for over a year he concentrated on an opera project, "The Fiery Angel", based on the novel by Valery Bryusov. By this time his later music had acquired a following in Russia, and he received invitations to return there, but he decided to stay in Europe. In 1923, Prokofiev married the Spanish singer Carolina Codina (1897–1989, whose stage name was Lina Llubera) before moving back to Paris.
In Paris, several of his works (for example the Second Symphony) were performed, but the audiences' reception was now lukewarm and Prokofiev sensed that he "was evidently no longer a sensation". However the Symphony appeared to prompt Diaghilev to commission "Le pas d'acier" ("The Steel Step"), a 'modernist' ballet score intended to portray the industrialisation of the Soviet Union. It was enthusiastically received by Parisian audiences and critics.
In around 1924, Prokofiev was introduced to Christian Science. He began to practice its teachings, which he believed to be beneficial to his health and to his fiery temperament, and to which, according to biographer Simon Morrison, he remained faithful for the rest of his life.
Prokofiev and Stravinsky restored their friendship, though Prokofiev particularly disliked Stravinsky's "stylization of Bach" in such recent works as the Octet and the Concerto for Piano and Wind Instruments. However, Stravinsky himself described Prokofiev as the greatest Russian composer of his day, after himself.
First visits to the Soviet Union.
In 1927, Prokofiev made his first concert tour in the Soviet Union. Over the course of more than two months, he spent time in Moscow and Leningrad (as Saint Petersburg had been renamed), where he enjoyed a very successful staging of "The Love for Three Oranges" in the Mariinsky Theatre. In 1928, Prokofiev completed his Third Symphony, which was broadly based on his unperformed opera "The Fiery Angel". The conductor Serge Koussevitzky characterized the Third as "the greatest symphony since Tchaikovsky's Sixth."
In the meantime, however, Prokofiev, under the influence of the teachings of Christian Science, had turned against the expressionist style and the subject matter of "The Fiery Angel". He now preferred what he called a "new simplicity", which he believed more sincere than the "contrivances and complexities" of so much modern music of the 1920s. During 1928–29, Prokofiev composed what was to be the last ballet for Diaghilev, "The Prodigal Son". When first staged in Paris on 21 May 1929, with Serge Lifar in the title role, both audience and critics were particularly struck by the final scene in which the prodigal son drags himself across the stage upon his knees to be welcomed by his father. Diaghilev had recognised that in the music to this scene, Prokofiev had "never been more clear, more simple, more melodious, and more tender." Only months later, Diaghilev was dead.
That summer, Prokofiev completed the Divertimento, Op. 43 (which he had started in 1925) and revised his Sinfonietta, Op. 5/48, a work started in his days at the Conservatory. In October that year, he had a car crash while driving his family back to Paris from their holiday: as the car turned over, Prokofiev pulled some muscles on his left hand. Prokofiev was therefore unable to perform in Moscow during his tour shortly after the accident, but he was able to enjoy watching performances of his music from the audience. Prokofiev also attended the Bolshoi Theatre's "audition" of his ballet "Le pas d'acier", and was interrogated by members of the Russian Association of Proletarian Musicians (RAPM) about the work: he was asked whether the factory portrayed "a capitalist factory, where the worker is a slave, or a Soviet factory, where the worker is the master? If it is a Soviet factory, when and where did Prokofiev examine it, since from 1918 to the present he has been living abroad and came here for the first time in 1927 for two weeks ?" Prokofiev replied, "That concerns politics, not music, and therefore I won't answer." The RAPM condemned the ballet as a "flat and vulgar anti-Soviet anecdote, a counter-revolutionary composition bordering on Fascism". The Bolshoi had no option but to reject the ballet.
With his left hand healed, Prokofiev toured the United States successfully at the start of 1930, propped up by his recent European success. That year Prokofiev began his first non-Diaghilev ballet "On the Dnieper", Op. 51, a work commissioned by Serge Lifar, who had been appointed "maitre de ballet" at the Paris Opéra. In 1931 and 1932, he completed his fourth and fifth piano concertos. The following year saw the completion of the Symphonic Song, Op. 57, which Prokofiev's friend Myaskovsky – thinking of its potential audience in the Soviet Union – told him "isn't quite for us ... it lacks that which we mean by monumentalism – a familiar simplicity and broad contours, of which you are extremely capable, but temporarily are carefully avoiding."
By the early 1930s, both Europe and America were suffering from the Great Depression, which inhibited both new opera and ballet productions, though audiences for Prokofiev's appearances as a pianist were—in Europe at least—undiminished. However Prokofiev, who saw himself as a composer first and foremost, increasingly resented the amount of time that was lost to composition through his appearances as a pianist. Having been homesick for some time, Prokofiev began to build substantial bridges with the Soviet Union. Following the dissolution of the RAPM in 1932, he acted increasingly as a musical ambassador between his homeland and western Europe, and his premieres and commissions were increasingly under the auspices of the Soviet Union. One such was "Lieutenant Kijé", which was commissioned as the score to a Soviet film. Another commission, from the Kirov Theatre (as the Mariinsky had now been renamed) in Leningrad, was the ballet "Romeo and Juliet", composed to a scenario created by Adrian Piotrovsky and Sergei Radlov following the precepts of "drambalet" (dramatised ballet, officially promoted at the Kirov to replace works based primarily on choreographic display and innovation). Following Radlov's acrimonious resignation from the Kirov in June 1934, a new agreement was signed with the Bolshoi Theatre in Moscow on the understanding that Piotrovsky would remain involved. However, the ballet's original happy ending (contrary to Shakespeare) provoked controversy among Soviet cultural officials; the ballet's production was then postponed indefinitely when the staff of the Bolshoi was overhauled at the behest of the chairman of the Committee on Arts Affairs, Platon Kerzhentsev. Nikolai Myaskovsky, one of his closest friends, mentioned in a number of letters how he would like Prokofiev to stay in Russia. 
Return to Russia.
In 1936, Prokofiev and his family settled permanently in Moscow. In that year he composed one of his most famous works, "Peter and the Wolf", for Natalya Sats's Central Children's Theatre. Sats also persuaded Prokofiev to write two songs for children – "Sweet Song", and "Chatterbox"; these were eventually joined by "The Little Pigs", published as "Three Children's Songs", Op. 68. Prokofiev also composed the gigantic "Cantata for the 20th Anniversary of the October Revolution," originally intended for performance during the anniversary year but effectively blocked by Kerzhentsev, who demanded at the work's audition before the Committee on Arts Affairs, "Just what do you think you're doing, Sergey Sergeyevich, taking texts that belong to the people and setting them to such incomprehensible music?" The Cantata had to wait until 5 April 1966 for a partial premiere (just over 13 years after the composer's death).
Forced to adapt to the new circumstances (whatever misgivings he had about them in private), Prokofiev wrote a series of "mass songs" (Opp. 66, 79, 89), using the lyrics of officially approved Soviet poets. In 1938, Prokofiev collaborated with Eisenstein on the historical epic "Alexander Nevsky". For this he composed some of his most inventive and dramatic music. Although the film had a very poor sound recording, Prokofiev adapted much of his score into a large-scale cantata for mezzo-soprano, orchestra and chorus, which was extensively performed and recorded. In the wake of "Alexander Nevsky"'s success, Prokofiev composed his first Soviet opera "Semyon Kotko", which was intended to be produced by the director Vsevolod Meyerhold. However the première of the opera was postponed because Meyerhold was arrested on 20 June 1939 by the NKVD (Joseph Stalin's Secret Police), and shot on 2 February 1940. Only months after Meyerhold's arrest, Prokofiev was 'invited' to compose "Zdravitsa" (literally translated 'Cheers!', but more often given the English title "Hail to Stalin") (Op. 85) to celebrate Joseph Stalin's 60th birthday.
Later in 1939, Prokofiev composed his Piano Sonatas Nos. 6, 7, and 8, Opp. 82–84, widely known today as the "War Sonatas." Premiered respectively by Prokofiev (No. 6: 8 April 1940), Sviatoslav Richter (No. 7: Moscow, 18 January 1943) and Emil Gilels (No. 8: Moscow, 30 December 1944), they were subsequently championed in particular by Richter. Biographer Daniel Jaffé argued that Prokofiev, "having forced himself to compose a cheerful evocation of the nirvana Stalin wanted everyone to believe he had created" (i.e. in "Zdravitsa") then subsequently, in these three sonatas, "expressed his true feelings". As evidence of this, Jaffé has pointed out that the central movement of Sonata No. 7 opens with a theme based on a Robert Schumann lied, 'Wehmut' ('Sadness', which appears in Schumann's Liederkreis, Op. 39): the words to this translate "I can sometimes sing as if I were glad, yet secretly tears well and so free my heart. Nightingales ... sing their song of longing from their dungeon's depth ... everyone delights, yet no one feels the pain, the deep sorrow in the song." Ironically (because, it appears, no one had noticed his allusion) Sonata No. 7 received a Stalin Prize (Second Class), and No. 8 a Stalin Prize First Class.
In the meantime, "Romeo and Juliet" was finally staged by the Kirov ballet, choreographed by Leonid Lavrovsky, on 11 January 1940. To the surprise of all its participants, the dancers having struggled to cope with the music's syncopated rhythms and almost having boycotted the production, the ballet was an instant success, and became recognised as the crowning achievement of Soviet dramatic ballet.
War years.
Prokofiev had been considering making an opera out of Leo Tolstoy's epic novel "War and Peace", when news of the German invasion of Russia on 22 June 1941 made the subject seem all the more timely. Prokofiev took two years to compose his original version of "War and Peace". Because of the war he was evacuated together with a large number of other artists, initially to the Caucasus where he composed his Second String Quartet. By this time his relationship with the 25-year-old writer and librettist Mira Mendelson (1915–1968) had finally led to his separation from his wife Lina, although they were never technically divorced: indeed Prokofiev had tried to persuade Lina and their sons to accompany him as evacuees out of Moscow, but Lina opted to stay.
During the war years, restrictions on style and the demand that composers should write in a 'socialist realist' style were slackened, and Prokofiev was generally able to compose in his own way. The Violin Sonata No. 1, Op. 80, "The Year 1941", Op. 90, and the "Ballade for the Boy Who Remained Unknown", Op. 93 all came from this period. In 1943 Prokofiev joined Eisenstein in Alma-Ata, the largest city in Kazakhstan, to compose more film music ("Ivan the Terrible"), and the ballet "Cinderella" (Op. 87), one of his most melodious and celebrated compositions. Early that year he also played excerpts from "War and Peace" to members of the Bolshoi Theatre collective. However, the Soviet government had opinions about the opera which resulted in many revisions. In 1944, Prokofiev spent time at a composer's colony outside Moscow in order to compose his Fifth Symphony (Op. 100). Prokofiev conducted its first performance on 13 January 1945, just a fortnight after the triumphant premieres on 30 December 1944 of his Eighth Piano Sonata and, on the same day, the first part of Eisenstein's "Ivan the Terrible". With the premiere of his Fifth Symphony, which was programmed alongside "Peter and the Wolf" and the "Classical" Symphony (these conducted by Nikolai Anosov), Prokofiev appeared to reach the peak of his celebrity as a leading composer of the Soviet Union. Shortly afterwards, he suffered a concussion after a fall due to chronic high blood pressure. He never fully recovered from this injury, and was forced on medical advice to restrict his composing activity.
Post-war.
Prokofiev had time to write his postwar Sixth Symphony and his Ninth Piano Sonata (for Sviatoslav Richter) before the so-called "Zhdanov Decree". In early 1948, following a meeting of Soviet composers convened by Andrei Zhdanov, the Politburo issued a resolution denouncing Prokofiev, Dmitri Shostakovich, Myaskovsky, and Khachaturian of the crime of "formalism", described as a "renunciation of the basic principles of classical music" in favour of "muddled, nerve-racking" sounds that "turned music into cacophony". Eight of Prokofiev's works were banned from performance: "The Year 1941", "Ode to the End of the War", "Festive Poem", "Cantata for the Thirtieth Anniversary of October", "Ballad of an Unknown Boy", the 1934 piano cycle "Thoughts", and Piano Sonatas Nos 6 and 8. Such was the perceived threat behind the banning of these works that even works that had avoided censure were no longer programmed: by August 1948, Prokofiev was in severe financial straits, his personal debt amounting to 180,000 rubles.
Meanwhile, on 20 February 1948, Prokofiev's wife Lina was arrested for 'espionage', as she had tried to send money to her mother in Spain. After nine months of interrogation, she was sentenced by a three-member Military Collegium of the Supreme Court of the USSR to 20 years of hard labour. She was eventually released after Stalin's death in 1953 and in 1974 left the Soviet Union.
Prokofiev's latest opera projects, among them his desperate attempt to appease the cultural authorities, "The Story of a Real Man", were quickly cancelled by the Kirov Theatre. This snub, in combination with his declining health, caused Prokofiev progressively to withdraw from public life and from various activities, even his beloved chess, and increasingly he devoted himself exclusively to his own work. After a serious relapse in 1949, his doctors ordered him to limit his activities, limiting him to composing for only an hour a day.
In spring 1949 he wrote his Cello Sonata in C, Op. 119, for the 22-year-old Mstislav Rostropovich, who gave the first performance in 1950, with Sviatoslav Richter. For Rostropovich, Prokofiev also extensively recomposed his Cello Concerto, transforming it into a Symphony-Concerto, his last major masterpiece and a landmark in the cello and orchestra repertory today. The last public performance he attended was the première of the Seventh Symphony in 1952. The music was written for the Children's Radio Division.
Death.
Prokofiev died at the age of 61 on 5 March 1953, the same day as Joseph Stalin. He had lived near Red Square, and for three days the throngs gathered to mourn Stalin, making it impossible to carry Prokofiev's body out for the funeral service at the headquarters of the Soviet Composers' Union. He is buried in the Novodevichy Cemetery in Moscow.
The leading Soviet musical periodical reported Prokofiev's death as a brief item on page 116. The first 115 pages were devoted to the death of Stalin. Usually Prokofiev's death is attributed to cerebral hemorrhage. He had been chronically ill for the prior eight years; the precise nature of Prokofiev's terminal illness remains uncertain.
Lina Prokofiev outlived her estranged husband by many years, dying in London in early 1989. Royalties from her late husband's music provided her with a modest income, and she acted as storyteller for a recording of her husband's "Peter and the Wolf" (currently released on CD by Chandos Records) with Neeme Järvi conducting the Scottish National Orchestra. Their sons Sviatoslav (1924–2010), an architect, and Oleg (1928–1998), an artist, painter, sculptor and poet, dedicated a large part of their lives to the promotion of their father's life and work.
Posthumous reputation.
Arthur Honegger proclaimed that Prokofiev would "remain for us the greatest figure of contemporary music," and the American scholar Richard Taruskin has recognised Prokofiev's "gift, virtually unparalleled among 20th-century composers, for writing distinctively original diatonic melodies." Yet for some time Prokofiev's reputation in the West suffered as a result of cold-war antipathies, and his music has never won from Western academics and critics the kind of esteem currently enjoyed by Igor Stravinsky and Arnold Schoenberg, composers purported to have a greater influence on a younger generation of musicians.
Today Prokofiev may well be the most popular composer of 20th-century music. His orchestral music alone is played more frequently in the United States than that of any other composer of the last hundred years, save Richard Strauss, while his operas, ballets, chamber works, and piano music appear regularly throughout the major concert halls world-wide.
The composer received honours in his native Donetsk Oblast, when the Donetsk International Airport was renamed to be "Donetsk Sergey Prokofiev International Airport," and when the Donetsk Musical and Pedagogical Institute was renamed in 1988 to "S.S. Prokofiev State Music Academy of Donetsk."
Works.
Important works include (in chronological order):
Recordings.
Prokofiev was a soloist with the London Symphony Orchestra, conducted by Piero Coppola, in the first recording of his Piano Concerto No. 3, recorded in London by His Master's Voice in June 1932. Prokofiev also recorded some of his solo piano music for HMV in Paris in February 1935; these recordings were issued on CD by Pearl and Naxos. In 1938, he conducted the Moscow Philharmonic Orchestra in a recording of the second suite from his "Romeo and Juliet" ballet; this performance was later released on LP and CD. Another reported recording with Prokofiev and the Moscow Philharmonic was of the First Violin Concerto with David Oistrakh as soloist; Everest Records later released this recording on an LP. Despite the attribution, the conductor was Aleksandr Gauk. A short sound film of Prokofiev playing some of the music from his opera "War and Peace" and then explaining the music has been discovered.
Notes and references.
Notes
References

</doc>
<doc id="27683" url="https://en.wikipedia.org/wiki?curid=27683" title="Satellite">
Satellite

In the context of spaceflight, a satellite is an artificial object which has been intentionally placed into orbit. Such objects are sometimes called artificial satellites to distinguish them from natural satellites such as Earth's Moon.
The world's first artificial satellite, the Sputnik 1, was launched by the Soviet Union in 1957. Since then, thousands of satellites have been launched into orbit around the Earth. Some satellites, notably space stations, have been launched in parts and assembled in orbit. Artificial satellites originate from more than 40 countries and have used the satellite launching capabilities of ten nations. About a thousand satellites are currently operational, whereas thousands of unused satellites and satellite fragments orbit the Earth as space debris. A few space probes have been placed into orbit around other bodies and become artificial satellites to the Moon, Mercury, Venus, Mars, Jupiter, Saturn, Vesta, Eros, Ceres, and the Sun.
Satellites are used for a large number of purposes. Common types include military and civilian Earth observation satellites, communications satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites. Satellite orbits vary greatly, depending on the purpose of the satellite, and are classified in a number of ways. Well-known (overlapping) classes include low Earth orbit, polar orbit, and geostationary orbit.
About 6,600 satellites have been launched. The latest estimates are that 3,600 remain in orbit. Of those, about 1,000 are operational; the rest have lived out their useful lives and are part of the space debris. Approximately 500 operational satellites are in low-Earth orbit, 50 are in medium-Earth orbit (at 20,000 km), the rest are in geostationary orbit (at 36,000 km).
Satellites are propelled by rockets to their orbits. Usually the launch vehicle itself is a rocket lifting off from a launch pad on land. In a minority of cases satellites are launched at sea (from a submarine or a mobile maritime platform) or aboard a plane (see air launch to orbit).
Satellites are usually semi-independent computer-controlled systems. Satellite subsystems attend many tasks, such as power generation, thermal control, telemetry, attitude control and orbit control.
History.
Early conceptions.
"Newton's cannonball", presented as a "thought experiment" in "A Treatise of the System of the World", by Isaac Newton was the first published mathematical study of the possibility of an artificial satellite.
The first fictional depiction of a satellite being launched into orbit is a short story by Edward Everett Hale, "The Brick Moon". The story is serialized in "The Atlantic Monthly", starting in 1869. The idea surfaces again in Jules Verne's "The Begum's Fortune" (1879).
In 1903, Konstantin Tsiolkovsky (1857–1935) published "Exploring Space Using Jet Propulsion Devices" (in Russian: "Исследование мировых пространств реактивными приборами"), which is the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit around the Earth at 8 km/s, and that a multi-stage rocket fuelled by liquid propellants could be used to achieve this. He proposed the use of liquid hydrogen and liquid oxygen, though other combinations can be used.
In 1928, Slovenian Herman Potočnik (1892–1929) published his sole book, "The Problem of Space Travel — The Rocket Motor" (German: "Das Problem der Befahrung des Weltraums — der Raketen-Motor"), a plan for a breakthrough into space and a permanent human presence there. He conceived of a space station in detail and calculated its geostationary orbit. He described the use of orbiting spacecraft for detailed peaceful and military observation of the ground and described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Tsiolkovsky) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays.
In a 1945 "Wireless World" article, the English science fiction writer Arthur C. Clarke (1917–2008) described in detail the possible use of communications satellites for mass communications. Clarke examined the logistics of satellite launch, possible orbits and other aspects of the creation of a network of world-circling satellites, pointing to the benefits of high-speed global communications. He also suggested that three geostationary satellites would provide coverage over the entire planet.
The US military studied the idea of what was referred to as the "earth satellite vehicle" when Secretary of Defense James Forrestal made a public announcement on December 29, 1948, that his office was coordinating that project between the various services.
Artificial satellites.
The first artificial satellite was Sputnik 1, launched by the Soviet Union on October 4, 1957, and initiating the Soviet Sputnik program, with Sergei Korolev as chief designer (there is a crater on the lunar far side which bears his name). This in turn triggered the Space Race between the Soviet Union and the United States.
Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of "Sputnik 1'"s success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.
"Sputnik 2" was launched on November 3, 1957 and carried the first living passenger into orbit, a dog named Laika.
In May, 1946, Project RAND had released the Preliminary Design of an Experimental World-Circling Spaceship, which stated, "A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century."
The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. The United States Air Force's Project RAND eventually released the above report, but did not believe that the satellite was a potential military weapon; rather, they considered it to be a tool for science, politics, and propaganda. In 1954, the Secretary of Defense stated, "I know of no American satellite program." In February 1954 Project RAND released "Scientific Uses for a Satellite Vehicle," written by R.R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with "The Scientific Use of an Artificial Satellite," by H.K. Kallmann and W.W. Kellogg.
In the context of activities planned for the International Geophysical Year (1957–58), the White House announced on July 29, 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On July 31, the Soviets announced that they intended to launch a satellite by the fall of 1957.
Following pressure by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, military interest picked up and in early 1955 the Army and Navy were working on Project Orbiter, two competing programs: the army's which involved using a Jupiter C rocket, and the civilian/Navy Vanguard Rocket, to launch a satellite. At first, they failed: initial preference was given to the Vanguard program, whose first attempt at orbiting a satellite resulted in the explosion of the launch vehicle on national television. But finally, three months after Sputnik 2, the project succeeded; Explorer 1 became the United States' first artificial satellite on January 31, 1958.
In June 1961, three-and-a-half years after the launch of Sputnik 1, the Air Force used resources of the United States Space Surveillance Network to catalog 115 Earth-orbiting satellites.
Early satellites were constructed as "one-off" designs. With growth in geosynchronous (GEO) satellite communication, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 GEO commsat, launched in 1972.
The largest artificial satellite currently orbiting the Earth is the International Space Station.
Space Surveillance Network.
The United States Space Surveillance Network (SSN), a division of The United States Strategic Command, has been tracking objects in Earth's orbit since 1957 when the Soviets opened the space age with the launch of Sputnik I. Since then, the SSN has tracked more than 26,000 objects. The SSN currently tracks more than 8,000 man-made orbiting objects. The rest have re-entered Earth's atmosphere and disintegrated, or survived re-entry and impacted the Earth. The SSN tracks objects that are 10 centimeters in diameter or larger; those now orbiting Earth range from satellites weighing several tons to pieces of spent rocket bodies weighing only 10 pounds. About seven percent are operational satellites (i.e. ~560 satellites), the rest are space debris. The United States Strategic Command is primarily interested in the active satellites, but also tracks space debris which upon reentry might otherwise be mistaken for incoming missiles.
A search of the NSSDC Master Catalog at the end of October 2010 listed 6,578 satellites launched into orbit since 1957, the latest being Chang'e 2, on 1 October 2010.
Non-military satellite services.
There are three basic categories of non-military satellite services:
Fixed satellite services.
Fixed satellite services handle hundreds of billions of voice, data, and video transmission tasks across all countries and continents between certain points on the Earth's surface.
Mobile satellite systems.
Mobile satellite systems help connect remote regions, vehicles, ships, people and aircraft to other parts of the world and/or other mobile or stationary communications units, in addition to serving as navigation systems.
Scientific research satellites (commercial and noncommercial).
Scientific research satellites provide meteorological information, land survey data (e.g. remote sensing), Amateur (HAM) Radio, and other different scientific research applications such as earth science, marine science, and atmospheric research.
Orbit types.
The first satellite, Sputnik 1, was put into orbit around Earth and was therefore in geocentric orbit. By far this is the most common type of orbit with approximately 2,465 artificial satellites orbiting the Earth. Geocentric orbits may be further classified by their altitude, inclination and eccentricity.
The commonly used altitude classifications of geocentric orbit are Low Earth orbit (LEO), Medium Earth orbit (MEO) and High Earth orbit (HEO). Low Earth orbit is any orbit below 2,000 km. Medium Earth orbit is any orbit between 2,000 km-35,786 km. High Earth orbit is any orbit higher than 35,786 km.
Centric classifications.
The general structure of a satellite is that it is connected to the earth stations that are present on the ground and connected through terrestrial links.
Satellite subsystems.
The satellite's functional versatility is imbedded within its technical components and its operations characteristics. Looking at the "anatomy" of a typical satellite, one discovers two modules. Note that some novel architectural concepts such as Fractionated spacecraft somewhat upset this taxonomy.
Spacecraft bus or service module.
The bus module consists of the following subsystems:
Structural subsystem.
The structural subsystem provides the mechanical base structure with adequate stiffness to withstand stress and vibrations experienced during launch, maintain structural integrity and stability while on station in orbit, and shields the satellite from extreme temperature changes and micro-meteorite damage.
Telemetry subsystem.
The telemetry subsystem (aka Command and Data Handling, C&DH) monitors the on-board equipment operations, transmits equipment operation data to the earth control station, and receives the earth control station's commands to perform equipment operation adjustments.
Power subsystem.
The power subsystem consists of solar panels to convert solar energy into electrical power, regulation and distribution functions, and batteries that store power and supply the satellite when it passes into the Earth's shadow. Nuclear power sources (Radioisotope thermoelectric generator have also been used in several successful satellite programs including the Nimbus program (1964–1978).
Thermal control subsystem.
The thermal control subsystem helps protect electronic equipment from extreme temperatures due to intense sunlight or the lack of sun exposure on different sides of the satellite's body (e.g. Optical Solar Reflector)
Attitude and orbit control subsystem.
The attitude and orbit control subsystem consists of sensors to measure vehicle orientation; control laws embedded in the flight software; and actuators (reaction wheels, thrusters) to apply the torques and forces needed to re-orient the vehicle to a desired attitude, keep the satellite in the correct orbital position and keep antennas positioning in the right directions.
Communication payload.
The second major module is the communication payload, which is made up of transponders. A transponder is capable of :
End of life.
When satellites reach the end of their mission, satellite operators have the option of de-orbiting the satellite, leaving the satellite in its current orbit or moving the satellite to a graveyard orbit. Historically, due to budgetary constraints at the beginning of satellite missions, satellites were rarely designed to be de-orbited. One example of this practice is the satellite Vanguard 1. Launched in 1958, Vanguard 1, the 4th manmade satellite put in Geocentric orbit, was still in orbit as of August 2009.
Instead of being de-orbited, most satellites are either left in their current orbit or moved to a graveyard orbit. As of 2002, the FCC requires all geostationary satellites to commit to moving to a graveyard orbit at the end of their operational life prior to launch. In cases of uncontrolled de-orbiting, the major variable is the solar flux, and the minor variables the components and form factors of the satellite itself, and the gravitational perturbations generated by the Sun and the Moon (as well as those exercised by large mountain ranges, whether above or below sea level). The nominal breakup altitude due to aerodynamic forces and temperatures is 78 km, with a range between 72 and 84 km. Solar panels, however, are destroyed before any other component at altitudes between 90 and 95 km.
Launch-capable countries.
This list includes countries with an independent capability of states to place satellites in orbit, including production of the necessary launch vehicle. Note: many more countries have the capability to design and build satellites but are unable to launch them, instead relying on foreign launch services. This list does not consider those numerous countries, but only lists those capable of launching satellites indigenously, and the date this capability was first demonstrated. The list includes the European Space Agency, a multi-national state organization, but does not include private consortiums.
Launch capable private entities.
A few other private companies are capable of sub-orbital launches.
First satellites of countries.
While Canada was the third country to build a satellite which was launched into space, it was launched aboard an American rocket from an American spaceport. The same goes for Australia, who launched first satellite involved a donated U.S. Redstone rocket and American support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (Virginia, United States) with an Italian launch team trained by NASA. By similar occasions, almost all further first national satellites was launched by foreign rockets.
Attempted first satellites.
†-note: Both Chile and Belarus used Russian companies as principal contractors to build their satellites, they used Russian-Ukrainian manufactured rockets and launched either from Russia or Kazakhstan.
Attacks on satellites.
In recent times, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks.
For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from earth. Russia, the United States and China have demonstrated the ability to eliminate satellites. In 2007 the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008.
Jamming.
Due to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.
Also, it is trivial to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring that enables them to pinpoint the source of any carrier and manage the transponder space effectively. 

</doc>
<doc id="27684" url="https://en.wikipedia.org/wiki?curid=27684" title="Steampunk">
Steampunk

Steampunk is a subgenre of science fiction or science fantasy that incorporates technology and aesthetic designs inspired by 19th-century industrial steam-powered machinery. Although its literary origins are sometimes associated with the cyberpunk genre, steampunk works are often set in an alternative history of the 19th century's British Victorian era or American "Wild West", in a post-apocalyptic future during which steam power has maintained mainstream usage, or in a fantasy world that similarly employs steam power. Steampunk may, therefore, be described as neo-Victorian. Steampunk perhaps most recognisably features anachronistic technologies or retro-futuristic inventions as people in the 19th century might have envisioned them, and is likewise rooted in the era's perspective on fashion, culture, architectural style, and art. Such technology may include fictional machines like those found in the works of H. G. Wells and Jules Verne, or the modern authors Philip Pullman, Scott Westerfeld, Stephen Hunt and China Miéville. Other examples of steampunk contain alternative history-style presentations of such technology as lighter-than-air airships, analogue computers, or such digital mechanical computers as Charles Babbage's Analytical Engine.
Steampunk may also incorporate additional elements from the genres of fantasy, horror, historical fiction, alternate history, or other branches of speculative fiction, making it often a hybrid genre. The first known appearance of the term "steampunk" was in 1987, though it now retroactively refers to many works of fiction created even as far back as the 1950s or 1960s.
Steampunk also refers to any of the artistic styles, clothing fashions, or subcultures, that have developed from the aesthetics of steampunk fiction, Victorian-era fiction, art nouveau design, and films from the mid-20th century. Various modern utilitarian objects have been modded by individual artisans into a pseudo-Victorian mechanical "steampunk" style, and a number of visual and musical artists have been described as steampunk.
History.
Precursors.
Steampunk is influenced by and often adopts the style of the 19th-century scientific romances of Jules Verne, H.G. Wells, and Mary Shelley. Several works of art and fiction significant to the development of the genre were produced before the genre had a name. "Titus Alone" (1959), by Mervyn Peake, anticipated many of the tropes of steampunk, and the film "Brazil" (1985) was an important early cinematic influence toward creating the genre. "The Adventures of Luther Arkwright" was an early (1970s) comic version of the Moorcock-style mover between timestreams.
In fine art, Remedios Varo's paintings combine elements of Victorian dress, fantasy, and technofantasy imagery. In television, one of the earliest mainstream manifestations of the steampunk ethos was the original CBS television series "The Wild Wild West" (1965–69), which inspired the film "Wild Wild West" (1999). In print, the "A Nomad of the Time Streams" trilogy by Michael Moorcock, which began in 1971 with "The Warlord of the Air", was also an influential precursor.
Origin of the term.
Although many works now considered seminal to the genre were published in the 1960s and 1970s, the term "steampunk" originated in the late 1980s as a tongue-in-cheek variant of "cyberpunk". It was coined by science fiction author K. W. Jeter, who was trying to find a general term for works by Tim Powers ("The Anubis Gates", 1983); James Blaylock ("Homunculus", 1986); and himself ("Morlock Night", 1979, and "Infernal Devices", 1987)—all of which took place in a 19th-century (usually Victorian) setting and imitated conventions of such actual Victorian speculative fiction as H. G. Wells' "The Time Machine". In a letter to science fiction magazine "Locus", printed in the April 1987 issue, Jeter wrote:
Modern steampunk.
While Jeter's "Morlock Night" and "Infernal Devices", Powers' "The Anubis Gates", and Blaylock's "Lord Kelvin's Machine" were the first novels to which Jeter's neologism would be applied, the three authors gave the term little thought at the time. They were far from the first modern science fiction writers to speculate on the development of steam-based technology or alternative histories. Keith Laumer's "Worlds of the Imperium" (1962) and Ronald W. Clark's "Queen Victoria's Bomb" (1967) apply modern speculation to past-age technology and society. Michael Moorcock's "Warlord of the Air" (1971) is another early example. Harry Harrison's novel "A Transatlantic Tunnel, Hurrah!" (1973) portrays a British Empire of an alternative year 1973, full of atomic locomotives, coal-powered flying boats, ornate submarines, and Victorian dialogue. The Adventures of Luther Arkwright (mid 70s) was the first steampunk comic. In February 1980 Richard A. Lupoff and Steve Stiles published the first "chapter" of their 10-part comic strip "The Adventures of Professor Thintwhistle and His Incredible Aether Flyer".
The first use of the word in a title was in Paul Di Filippo's 1995 "Steampunk Trilogy", consisting of three short novels: "Victoria", "Hottentots", and "Walt and Emily", which, respectively, imagine the replacement of Queen Victoria by a human/newt clone, an invasion of Massachusetts by Lovecraftian monsters, and a love affair between Walt Whitman and Emily Dickinson.
Relationships to retrofuturism, DIY craft and making.
Superficially, steampunk may resemble retrofuturism. Indeed, both sensibilities recall "the older but still modern eras in which technological change seemed to anticipate a better world, one remembered as relatively innocent of industrial decline." But, where retrofuturism is primarily backward looking and relies on stylistic pastiche, steampunk embraces a broader lifestyle and creative vision.
One of steampunk’s most significant contributions is the way in which it mixes the digital with the handmade. As scholars Rachel Bowser and Brian Croxall put it, "the tinkering and tinker-able technologies within steampunk invite us to roll up our sleeves and get to work re-shaping our contemporary world."> In this respect, steampunk bears more in common with DIY craft and making. Travis Bickle's "slide sleeve gun" in "Taxi Driver" is an example of steampunk know-how.
Art, entertainment, and media.
Art and design.
Many of the visualisations of steampunk have their origins with, among others, Walt Disney's film "20,000 Leagues Under the Sea" (1954), including the design of the story's submarine the "Nautilus", its interiors, and the crew's underwater gear; and George Pal's film "The Time Machine" (1960), with the design of the time machine itself. This theme is also carried over to Disney's theme parks in the designs of The Mysterious Island section of Tokyo DisneySea theme park and Disneyland Paris' Discoveryland area. 
Aspects of steampunk design emphasise a balance between the form and function. So too is it like the Arts and Crafts Movement. But John Ruskin, William Morris, and the other reformers in the late nineteenth century rejected machines and industrial production. On the other hand, steampunk enthusiasts present a "non-luddite critique of technology."
Various modern utilitarian objects have been modified by enthusiasts into a pseudo-Victorian mechanical "steampunk" style. Example objects include computer keyboards and electric guitars. The goal of such redesigns is to employ appropriate materials (such as polished brass, iron, wood, and leather) with design elements and craftsmanship consistent with the Victorian era, rejecting the aesthetic of industrial design.
In 1994, the Paris Metro station at Arts et Métiers was redesigned by Belgian artist Francois Schuiten in steampunk style to honor the works of Jules Verne. The station is reminiscent of a submarine, sheathed in brass with giant cogs in the ceiling and portholes that look out onto fanciful scenes.
The artist group "Kinetic Steam Works" brought a working steam engine to the Burning Man festival in 2006 and 2007. The group's founding member, Sean Orlando, created a Steampunk Tree House (in association with a group of people who would later form the "Five Ton Crane Arts Group") that has been displayed at a number of festivals. The Steampunk Tree House is now permanently installed at the Dogfish Head Brewery in Milton, Delaware.
The Neverwas Haul is a three-story, self-propelled mobile art vehicle built to resemble a Victorian house on wheels designed by Shannon O’Hare and built by volunteers in 2006 for presentation at the Burning Man festival from 2006 through 2015. When fully built, the Haul propelled itself at a top speed of 5 miles per hour and required a crew of ten people to operate safely. Currently, the Neverwas Haul makes her home at Obtainium Works, an "art car factory" in Vallejo, CA, owned by O’Hare and home to several other self-styled "contraptionists." 
In May–June 2008, multimedia artist and sculptor Paul St George exhibited outdoor interactive video installations linking London and Brooklyn, New York, in a Victorian era-styled telectroscope. Utilising this device, New York promoter Evelyn Kriete organised a transatlantic wave between steampunk enthusiasts from both cities, briefly prior to White Mischief's "Around the World in 80 Days" steampunk-themed event.
In 2009, artist Tim Wetherell created a large wall piece for Questacon representing the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the moon's terminator in action. The 3D moon movie was created by Antony Williams. 
From October 2009 through February 2010, the Museum of the History of Science, Oxford hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer, Art Donovan who also exhibited his own "electro-futuristic" lighting sculptures and presented by Dr. Jim Bennett, museum director. From redesigned practical items to fantastical contraptions, this exhibition showcased the work of eighteen steampunk artists from across the globe. The exhibition proved to be the most successful and highly attended in the museum's history and attracted more than eighty thousand visitors. The event was detailed in the official artist's journal, "The Art of Steampunk" by curator Donovan.
In November 2010, The Libratory Steampunk Art Gallery was opened by Damien McNamara in Oamaru, New Zealand. Created from papier-mâché to resemble a large subterranean cave and filled with industrial equipment from yesteryear, rayguns and general steampunk quirks. Its purpose is to provide a place for steampunkers in the region to display artwork for sale all year long. A year later, a more permanent gallery, Steampunk HQ, was opened in the former Meeks Grain Elevator Building across the road from The Woolstore, and has since become a notable tourist attraction for Oamaru.
In 2012, the "Mobilis in Mobili: An Exhibition of Steampunk Art and Appliance" art exhibit made its debut. Originally located at New York City's Wooster Street Social Club (itself the subject of the television series "NY Ink"), the exhibit featured working steampunk tattoo systems designed, respectively, by Bruce Rosenbaum of ModVic and owner of the Steampunk House, Joey "Dr. Grymm" Marsocci., and Christopher Conte showing very different approaches. "bicycles, cell phones, guitars, timepieces and entertainment systems" rounded out the display. The opening night exhibition featured a live performance by steampunk band Frenchy and the Punk.
In November 2014, Sky Harbor Airport in Phoenix, AZ opened a museum exhibit entitled "Steampunk: The Exquisite Adventure", featuring both local and nationally known Steampunk artisans. The displays were originally part of an exhibit at Scottsdale Public Library.
Fashion.
Steampunk fashion has no set guidelines but tends to synthesize modern styles with influences from the Victorian era. This may include bustles, corsets, gowns, and petticoats; suits with waistcoats, coats, top hats, tailcoats and spats; or military-inspired garments. Steampunk-influenced outfits are usually accented with several technological and "period" accessories: timepieces, parasols, flying/driving goggles, and ray guns. Modern accessories like cell phones or music players can be found in steampunk outfits, after being modified to give them the appearance of Victorian-made objects. Post-apocalyptic elements, such as gas masks, ragged clothing and tribal motifs, can also be included. Aspects of steampunk fashion have been anticipated by mainstream high fashion, the Lolita fashion and aristocrat styles, neo-Victorianism, and the romantic goth subculture.
In 2005, Kate Lambert, known as "Kato", founded the first steampunk clothing company, "Steampunk Couture", mixing Victorian and post-apocalyptic influences. In 2013, IBM predicted, based on an analysis of more than a half million public posts on message boards, blogs, social media sites and news sources, "that 'steampunk,' a subgenre inspired by the clothing, technology and social mores of Victorian society, will be a major trend to bubble up and take hold of the retail industry". Indeed, high fashion lines such as Prada, Dolce & Gabbana, Versace, Chanel and Christian Dior had already been introducing steampunk styles on the fashion runways. And in episode 7 of "Lifetime"'s "Project Runway: Under the Gunn" reality series, contestants were challenged to create "avant-garde" "steampunk chic" looks. America's Next Top Model tackled Steampunk fashion in a 2012 episode where models competed in a Steampunk themed photo shoot, posing in front of a steam train while holding a live owl.
Literature.
The educational book, "Elementary BASIC - Learning to Program Your Computer in BASIC with Sherlock Holmes" (1981) by Henry Singer and Andrew Ledgar, may have been the first fictional work to depict the use of Charles Babbage's Analytical Engine in an adventure story. The instructional book, aimed at young programming students, depicts Holmes using the engine as an aid in his investigations, and offer program listings to perform simple data processing tasks required to solve the fictional cases. The book even describes a possible enhancement to Babbage's machine; a device that allows the engine to be used remotely, through telegraph lines. Companion volumes, "Elementary Pascal - Learning to Program Your Computer in Pascal with Sherlock Holmes" and "From Baker Street to Binary - An Introduction to Computers and Computer Programming with Sherlock Holmes", were also written.
In 1988, the first version of the science fiction roleplaying game "" was published. It is set in an alternative history in which certain discredited Victorian scientific theories were probable, thus leading to new technologies. Contributing authors included Frank Chadwick, Loren Wiseman, and Marcus Rowland.
William Gibson and Bruce Sterling's novel "The Difference Engine" (1990) is often credited with bringing widespread awareness of steampunk. This novel applies the principles of Gibson and Sterling's cyberpunk writings to an alternative Victorian era where Ada Lovelace and Charles Babbage's proposed steam-powered mechanical computer, which Babbage called a difference engine (a later, more general-purpose version was known as an analytical engine), was actually built, and led to the dawn of the information age more than a century "ahead of schedule". This setting was different from most steampunk settings in that it takes a dim and dark view of this future rather than the more prevalent utopian versions.
Nick Gevers's original anthology "Extraordinary Engines" (2008) features newer steampunk stories by some of the genre's writers, as well as other science fiction and fantasy writers experimenting with neo-Victorian conventions. A retrospective reprint anthology of steampunk fiction was released, also in 2008, by Tachyon Publications; edited by Ann and Jeff VanderMeer and appropriately entitled "Steampunk", it is a collection of stories by James Blaylock, whose "Narbondo" trilogy is typically considered steampunk; Jay Lake, author of the novel "Mainspring", sometimes labeled "clockpunk"; the aforementioned Michael Moorcock; as well as Jess Nevins, known for his annotations to "The League of Extraordinary Gentlemen" (first published in 1999).
Younger readers have also been targeted with steampunk themes by authors such as Philip Reeve and Scott Westerfeld. Reeve's quartet "Mortal Engines" is set far in Earth's future where giant moving cities consume each other in a battle for resources, a concept Reeve coined as "Municipal Darwinism". Westerfeld's "Leviathan" trilogy is set during the First World War, fought between the "clankers" (Central Powers), who use steam technology, and "darwinists" (Allied Powers), who use genetically engineered creatures instead of machines.
"Mash-ups" are also becoming increasingly popular in books aimed at younger readers, mixing steampunk with other genres. Suzanne Lazear's 'Aether Chronicles' series mixes steampunk with faeries and 'The Unnaturalists' by Tiffany Trent combines steampunk with mythological creatures and alternate history.
While most of the original steampunk works had a historical setting, later works often place steampunk elements in a fantasy world with little relation to any specific historical era. Historical steampunk tends to be science fiction that presents an alternative history; it also contains real locales and persons from history with alternative fantasy technology. "Fantasy-world steampunk", such as China Miéville's "Perdido Street Station", Alan Campbell's "Scar Night", and Stephen Hunt's Jackelian novels, on the other hand, presents steampunk in a completely imaginary fantasy realm, often populated by legendary creatures coexisting with steam-era and other anachronistic technologies. However, the works of China Miéville and similar authors are sometimes referred to as belonging to the 'New Weird' rather than steampunk.
Self-described author of "far-fetched fiction" Robert Rankin has increasingly incorporated elements of steampunk into narrative worlds, both Victorian and re-imagined contemporary. In 2009, he was made a Fellow of the Victorian Steampunk Society.
The comic book series "Hellboy" created by Mike Mignola, and the two Hellboy films featuring Ron Perlman and directed by Guillermo del Toro, all have steampunk elements. In the comic book and the first (2004) film, Karl Ruprecht Kroenen is a Nazi SS scientist who has an addiction to surgery and many mechanical prostheses, including a clockwork heart. The character Johann Krauss features in the comic and in the second film, "" (2008), as an ectoplasmic medium (a gaseous form in a partly mechanical suit). This second film also features the Golden Army itself, which is a collection of 4,900 mechanical steampunk warriors.
Steampunk settings.
Alternative world.
Since the 1990s, the application of the steampunk label has expanded beyond works set in recognisable historical periods, to works set in fantasy worlds that rely heavily on steam- or spring-powered technology. One of the earliest short stories relying on steam-powered flying machines is "The Aerial Burglar" of 1844. An example from juvenile fiction is The Edge Chronicles by Paul Stewart and Chris Riddell.
Fantasy steampunk settings abound in tabletop and computer role-playing games. Notable examples include "Skies of Arcadia", ', and '.
The gnomes and goblins in "World of Warcraft" also have technological societies that could be described as steampunk as they are vastly ahead of the technologies of men, but still run on steam and mechanical power.
The Dwarves of the "Elder Scrolls" series, described therein as a race of Elves called the Dwemer, also use steam powered machinery, with gigantic brass like gears throughout their underground cities. However, magical means are used to keep ancient devices in motion despite the Dwemer's ancient disappearance.
The 1998 game as well as the other sequels including its 2014 reboot feature heavy steampunk-inspired architecture, setting and technology.
Amidst the historical and fantasy subgenres of steampunk is a type which takes place in a hypothetical future or a fantasy equivalent of our future, involving the domination of steampunk-style technology and aesthetics. Examples include Jean-Pierre Jeunet & Marc Caro's "The City of Lost Children" (1995), "Turn A Gundam" (1999–2000), "Trigun" and Disney's film "Treasure Planet" (2002). In 2011, musician Thomas Dolby heralded his return to music after a 20-year hiatus with an online steampunk alternate fantasy world called the Floating City, to promote his album, "A Map of the Floating City".
American West.
Another setting is "Western" steampunk, which overlaps with both the Weird West and Science fiction Western subgenres. Several other categories have arisen, sharing similar names, including dieselpunk, clockworkpunk, and others. Most of these terms were coined as supplements to the GURPS role playing game, and are not used in other contexts.
Fantasy and horror.
Kaja Foglio introduced the term "Gaslight Romance", gaslamp fantasy, which John Clute and John Grant define as "steampunk stories ... most commonly set in a romanticised, smoky, 19th-century London, as are Gaslight Romances. But the latter category focuses nostalgically on icons from the late years of that century and the early years of the 20th century--on Dracula, Jekyll and Hyde, Jack the Ripper, Sherlock Holmes and even Tarzan--and can normally be understood as combining supernatural fiction and recursive fantasy, though some gaslight romances can be read as fantasies of history." Author/artist James Richardson-Brown coined the term "steamgoth" to refer to steampunk expressions of fantasy and horror with a "darker" bent.
Post-apocalyptic.
Mary Shelley's "The Last Man", set near the end of the 21st century after a plague had brought down civilization, was probably the ancestor of post-apocalyptic steampunk literature. Post-apocalyptic steampunk is set in a world where some cataclysm has precipitated the fall of civilization and steam power once again gains ascendancy, such as in Hayao Miyazaki's post-apocalyptic anime "Future Boy Conan" (1978), where a war fought with superweapons has devastated the planet. Robert Brown's novel, "The Wrath of Fate" (as well as much of Abney Park's music) is set in A Victorianesque world where an apocalypse was set into motion by a time-traveling mishap. Cherie Priest's Boneshaker series is set in a world where a zombie apocalypse happened during the Civil War era. The Peshawar Lancers by S.M. Stirling is set in a post-apocalyptic future in which a meteor shower in 1878 caused the collapse of Industrialized civilization. The movie 9 (which might be better classified as "stitchpunk" but was largely influenced by steampunk) is also set in a post-apocalyptic world after a self-aware war machine ran amok. Steampunk Magazine even published a book called "A Steampunk's Guide to the Apocalypse", about how steampunks could survive should such a thing actually happen.
Victorian.
In general, the category includes any recent science fiction that takes place in a recognizable historical period (sometimes an alternate history version of an actual historical period) in which the Industrial Revolution has already begun, but electricity is not yet widespread. It places an emphasis on steam- or spring-propelled gadgets. The most common historical steampunk settings are the Victorian and Edwardian eras, though some in this "Victorian steampunk" category can go as early as the beginning of the Industrial Revolution and as late as the end of World War I.
Some examples of this type include the novel "The Difference Engine", the comic book series "League of Extraordinary Gentlemen", the Disney animated film "", Scott Westerfeld's "Leviathan" trilogy, and the roleplaying game "." The anime film "Steamboy" (2004) is another good example of Victorian steampunk, taking place in an alternate 1866 where steam technology is "far" more advanced than it ever was in real life. Some, such as the comic series "Girl Genius", have their own unique times and places despite partaking heavily of the flavor of historic times and settings. Other comic series are of a more familiar London as in Victorian Undead which has Sherlock Holmes, Doctor Watson and others taking on zombies, Doctor Jekyll and Mister Hyde, and Count Dracula with some advanced weapons and devices.
Karel Zeman's film "The Fabulous World of Jules Verne" (1958) is a very early example of cinematic steampunk. Based on Jules Verne novels, Zeman's film imagines a past based on those novels which never was. Another early example of historical steampunk in cinema includes Hayao Miyazaki's anime films such as "" (1986) and "Howl's Moving Castle" (2004), containing many archetypal anachronisms characteristic of the steampunk genre.
"Historical" steampunk usually leans more towards science fiction than fantasy, but a number of historical steampunk stories have incorporated magical elements as well. For example, "Morlock Night", written by K. W. Jeter, revolves around an attempt by the wizard Merlin to raise King Arthur to save the Britain in 1892 from an invasion of Morlocks from the future.
Paul Guinan's "Boilerplate", a 'biography' of a robot in the late 19th century, began as a website that garnered international press coverage when people began believing that Photoshop images of the robot with historic personages were real. The site was adapted into an illustrated hardbound book "Boilerplate: History's Mechanical Marvel," and published by Abrams in October 2009. Because the story was not set in an alternative history, and in fact contained accurate information about the Victorian era, some booksellers referred to the tome as "historical steampunk."
Music.
Steampunk music is very broadly defined. Abney Park’s lead singer, Robert Brown defined it as, "mixing Victorian elements and modern elements."
There is a broad range of musical influences that make up the Steampunk sound, from industrial dance and world music to folk rock, Punk cabaret to straightforward punk, Carnatic to industrial, hip-hop to opera (and even industrial hip-hop opera), darkwave to progressive rock, barbershop to big band.
Steampunk has also appeared in the work of musicians who do not specifically identify as steampunk. For example, the music video of "Turn Me On", by David Guetta and featuring Nicki Minaj, takes place in a steampunk universe where Guetta creates human droids. Another music video is "The Ballad of Mona Lisa", by Panic! at the Disco, which has a distinct Victorian Steampunk theme. A continuation of this theme has in fact been used throughout the 2011 album "Vices And Virtues" in the music videos, album art, and tour set and costumes. In addition, the album "Clockwork Angels" (2012) and its supporting tour by progressive rock band Rush contain lyrics, themes, and imagery based around steampunk. Similarly, Abney Park headlined the first "Steamstock" outdoor steampunk music festival in Richmond, California, which also featured Thomas Dolby, Frenchy and the Punk, Lee Presson and the Nails, Vernian Process, and others.
Television and films.
"The Fabulous World of Jules Verne", 1958, directed by Karel Zeman
"The Fabulous Baron Munchausen", 1962, directed by Karel Zeman
The 1965 television series "The Wild Wild West", as well as the eponymous 1999 film, featured many of the elements of advanced steam-powered technology set in the Wild West time period of the United States.
Despite leaning more towards gothic influences, the 'parallel reality' of Meanwhile, City within the 2009 film Franklyn contains many steampunk themes, such as costumery, architecture, minimal use of electricity (with the preference for gaslight), and absence of modern technology (such as there being no motorised vehicles or advanced weaponry, and the manual management of information with no use of computers).
"Two Years' Vacation" (or "The Stolen Airship"), 1967, directed by Karel Zeman
"Dinner for Adele", 1977, directed by Oldřich Lipský
The 1979 film "Time After Time" has Herbert George "H.G." Wells following a surgeon named John Leslie Stevenson into the future, as John is suspected of being Jack the Ripper. Both use Wells' time machine separately to travel.
"The Mysterious Castle in the Carpathians", 1981, directed by Oldřich Lipský
The 1982 American TV series "Q.E.D.", set in Edwardian England, starred Sam Waterston as Professor Quentin Everett Deverill (the series title is the character's initials, as well as the Latin phrase "quod erat demonstrandum", which translates as "which was to be demonstrated"). The Professor was an inventor and scientific detective, in the mold of Sherlock Holmes. In the show, the lead character was known primarily by his initials, Q.E.D.
The 1986 Japanese film by Hayao Miyazaki "Castle in the Sky", was heavily influenced by steampunk culture, featuring various air ships and steam-powered contraptions as well as the story line centering on a mysterious island which floats through the sky. This is accomplished not through magic as most stories would resort to but instead relies on massive propellers as is fitting for the Victorian motif.
"The Adventures of Brisco County, Jr.", a Fox Network 1993 TV science fiction-western set in the 1890s, featured elements of steampunk as represented by the character Professor Wickwire, whose inventions were described as "the coming thing".
The short-lived 1995 TV show "Legend" on UPN, set in 1876 Arizona, featured such classic inventions as a steam-driven "quadrovelocipede" and night-vision goggles, and starred John de Lancie as a thinly disguised Nikola Tesla. Alan Moore's and Kevin O'Neill's 1999 "The League of Extraordinary Gentlemen" graphic novel series (and the subsequent 2003 film adaption) greatly popularised the steampunk genre.
The 2007 Syfy miniseries "Tin Man" incorporates a considerable amount of steampunk-inspired themes into a re-imagining of L. Frank Baum's "The Wonderful Wizard of Oz".
The Syfy series "Warehouse 13" (which premiered July 7, 2009) features many steampunk-inspired objects and artifacts, including computer designs created by steampunk artisan Richard Nagy, aka "Datamancer".
The BBC series "Doctor Who" (which premiered in 1963) also incorporates steampunk elements. During season 14 of the show (in 1976), the formerly futuristic looking interior set was replaced with a Victorian-styled wood panel and brass affair. In the 1996 American co-production, the TARDIS interior was re-designed to resemble an almost Victorian library with the central control console made up of eclectic and anachronistic objects. Modified and streamlined for the 2005 revival of the series, the TARDIS console continued to incorporate steampunk elements, including a Victorian typewriter and gramophone. Several storylines can be classed as steampunk, for example: "The Evil of the Daleks" (1966), wherein Victorian scientists invent a time travel device.
Steampunk has begun to attract notice from "mainstream" American sources as well. For example, the episode of the TV series "Castle" entitled "Punked" (which aired on October 11, 2010) prominently featured the steampunk subculture and used Los Angeles-area steampunks (such as the League of STEAM) as extras The GSN reality television game show "Steampunk'd" features a competition to create steampunk-inspired art and designs which are judged by notable Steampunks Thomas Willeford, Kato, and Matt King. "
Video games.
A variety of styles of video games have used Steampunk settings. Borderlands 2 which is set in a wasteland/steampunk environment, "The Chaos Engine" is a run and gun video game inspired by the Gibson/Sterling novel "The Difference Engine" (1990), set in a Victorian steampunk age. Developed by the Bitmap Brothers, it was first released on the Amiga in 1993; a sequel was released in 1996. Other steampunk-styled video games include the first-person shooter "BioShock Infinite" (2013), the "Dishonored" (2012) stealth game, the role-playing games "Final Fantasy VI" (1994), "Final Fantasy IX" (2000), "Dark Chronicle" (2002) and the late Middle Ages/Victorian age styled "Thief" series (1998). The graphic adventure puzzle video games "Myst" (1993), "Riven" (1997), and "" (2001) (all produced by Cyan Worlds) take place in an alternate steampunk universe, where elaborate infrastructures have been built to run on steam power. Guild Wars 2 has plenty of steam punk inspired content, the most notable are the Engineer player class, as well as the Charr race, with their industrial technology and aesthetics. Many steampunk themes can be found within World of Warcraft particularly the 'Gnome' race within the game. Many of the items which can be created via the Engineering profession are of a steampunk nature and also named in a similar fashion. The first person shooter "Timeshift" (2007), developed by Saber Interactive, was intended to have a significant steampunk element and some steampunk-style technology did remain, e.g. airships, in the final game. "There is also a free 2D tower defense game "Steampunk Defens", developed in 2012 by Nickelodeon and Dreamgate, which features a fully steampunk-style theme."
Culture and community.
Because of the popularity of steampunk, there is a growing movement of adults that want to establish steampunk as a culture and lifestyle. Some fans of the genre adopt a steampunk aesthetic through fashion, home decor, music, and film. This may be described as neo-Victorianism, which is the amalgamation of Victorian aesthetic principles with modern sensibilities and technologies.
In September 2012, a panel was held at Stan Lee's Comikaze Expo, chaired by steampunk entertainer Veronique Chevalier and with panelists including magician Pop Hadyn and members of the steampunk performance group The League of STEAM, which suggested that because steampunk was inclusive of and incorporated ideas from various other subcultures such as goth, neo-Victorian, and cyberpunk as well as a growing number of fandoms, it was fast becoming a "super-culture" rather than a mere subculture. Other steampunk notables such as Professor Elemental have expressed similar views about steampunk's inclusive diversity.
Some have proposed a steampunk philosophy, sometimes with punk-inspired anti-establishment sentiments, and typically bolstered by optimism about human potential.
Steampunk became a common descriptor for homemade objects on the craft network Etsy between 2009 and 2011, though many of the objects and fashions bear little resemblance to earlier established steampunk descriptions. Thus the craft network may not strike observers as 'sufficiently steampunk' to warrant the description. Comedian April Winchell, author of the book, "Regretsy: Where DIY meets WTF", cataloged some of the most egregious and humorous examples on her website, "Regretsy". The blog was popular among steampunks and even inspired a music video that went viral in the community and was acclaimed by steampunk "notables."
Social events.
2006 saw the first "SalonCon", a neo-Victorian/steampunk convention. It ran for three consecutive years and featured artists, musicians (Voltaire and Abney Park), authors (Catherynne M. Valente, Ekaterina Sedia, and G. D. Falksen), salons led by people prominent in their respective fields, workshops and panels on steampunk—as well as a seance, ballroom dance instruction, and the Chrononauts' Parade. The event was covered by MTV and "The New York Times". Since then a number of popular steampunk conventions have sprung up the world over, with names like Steamcon (Seattle, WA), the Steampunk World's Fair (Piscataway, NJ), Up in the Aether: The Steampunk Convention (Dearborn, MI)., and Steampunk Unlimited (Strasburg Railroad, Lancaster, PA). Each year, on Mother's Day weekend, the city of Waltham, MA turns over its city center and surrounding areas to host the Watch City Steampunk Festival, a U.S. outdoor steampunk festival.
Steampunk has also become a regular feature at San Diego Comic-Con International in recent years, with the Saturday of the four-day event being generally known among steampunks as "Steampunk Day", and culminating with a photo-shoot for the local press. In 2010 this was recorded in the Guinness Book of World Records as the world's largest steampunk photo shoot. In 2013, Comic-Con announced four official 2013 T-shirts: one of them featured the official Rick Geary Comic-Con toucan mascot in steampunk attire. The Saturday steampunk "after-party" has also become a major event on the steampunk social calendar; in 2010 the headliners included The Slow Poisoner, Unextraordinary Gentlemen and Voltaire, with Veronique Chevalier as Mistress of Ceremonies and special appearance by the League of STEAM, and in 2011 UXG returned with Abney Park.
Steampunk also has sprung up recently at Renaissance Festivals and Renaissance Faires, in the USA. Some have organised events or a "Steampunk Day", while other Fests simply support an open environment for donning Steampunk attire. The Bristol Renaissance Faire in Kenosha, Wisconsin, on the Wisconsin/Illinois border, featured a Steampunk costume contest during the 2012 season. The previous two seasons featured increasing participation in the phenomenon.
Steampunk also has a growing following in the UK and Europe. The largest European event is "Weekend at the Asylum", held at The Lawn, Lincoln every September since 2009. Organised as a not-for-profit event by the Victorian Steampunk Society, the Asylum is a dedicated steampunk event which takes over much of the historical quarter of Lincoln, England, along with Lincoln Castle. In 2011 there were over 1000 steampunks in attendance. The event features the Empire Ball, Majors Review, Bazaar Eclectica and the international Tea Duelling final.
Steampunk games.
There is an increasing number of games and sports that are played at Steampunk events. These games include:

</doc>
<doc id="27686" url="https://en.wikipedia.org/wiki?curid=27686" title="Spreadsheet">
Spreadsheet

A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form. Spreadsheets are developed as computerized simulations of paper accounting worksheets. The program operates on data entered in cells of an array, organized in rows and columns. Each cell of the array may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells.
Spreadsheet users may adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for "what-if" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.
Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.
Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.
LANPAR was the first electronic spreadsheet on mainframe and time sharing computers. VisiCalc was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.
Usage.
A spreadsheet consists of a table of "cells" arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, "A", "B", "C", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, "C10" for instance. This system of cell references was introduced in VisiCalc, and known as "A1 notation". Additionally, spreadsheets have the concept of a "range", a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range "A1:A10". 
In modern spreadsheet applications, several spreadsheets, often known as "worksheets" or simply "sheets", are gathered together to form a "workbook". A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, "Sheet 1!C10". Some systems extend this syntax to allow cell references to different workbooks.
Users interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text codice_1, the number codice_2 or the date codice_3. A formula would begin with the equals sign, codice_4, but this would normally be invisible because the display shows the "result" of the calculation, codice_5 in this case, not the formula itself. This may lead to confusion in some cases.
The key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula codice_6 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value codice_7 the result will be codice_5. But C10 might also hold its own formula referring to other cells, and so on.
The ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical step, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the codice_9 function that adds up all the numbers within a range.
Spreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.
A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.
History.
Paper spreadsheets.
The word "spreadsheet" came from "spread" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word "spread-sheet" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a "spread" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed "analysis paper") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.
Early implementations.
Batch spreadsheet report generator.
A batch "spreadsheet" is indistinguishable from a batch compiler with added input data, producing an output report, "i.e.", a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper "Budgeting Models and System Simulation" by Richard Mattessich. The subsequent work by Mattessich (1964a, Chpt. 9, "Accounting and Analytical Methods") and its companion volume, Mattessich (1964b, "Simulation of the Firm through a Budget Computer Program") applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual "cells".
In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled "Business Computer Language" was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed
Applied Data Resources had a FORTRAN preprocessor called Empires.
In the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.
LANPAR spreadsheet compiler.
A key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 on spreadsheet automatic natural order recalculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the CCPA (Predecessor Court of the Federal Circuit) overturning the Patent Office in 1983—establishing that "something does not cease to become patentable merely because the point of novelty is in an algorithm." However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.
The actual software was called LANPAR — LANguage for Programming Arrays at Random. This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order ("Forward Referencing/Natural Order Calculation"). Pardo and Landau developed and implemented the software in 1969.
LANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first "non-procedural" computer languages) as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.
The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and "if/then" statements could be used in any cell, and cells could be presented in any order.
Autoplan/Autotab spreadsheet programming language.
In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. "AutoPlan" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name "AutoTab". (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.) AutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column.
Works Records System.
The Works Records System was a spreadsheet system designed in 1974 at ICI in the UK. It was a company-internal system that ran on IBM mainframes, and was in use essentially unchanged for 27 years. It was intended for use by non-programmers and had a WYSIWIG interface.
IBM Financial Planning and Control System.
The IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.
APLDOT modeling language.
An example of an early "industrial weight" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD. The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a "spreadsheet" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.
VisiCalc.
Because of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time "PC World" magazine called VisiCalc the first electronic spreadsheet.
Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.
VisiCalc went on to become the first killer app, an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.
SuperCalc.
SuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.
Lotus 1-2-3 and other MS-DOS spreadsheets.
The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.
Lotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.
Microsoft Excel.
Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3, and in 2013, IBM discontinued Lotus-1-2-3 altogether.
Open source software.
Gnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the very closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.
Web based spreadsheets.
With the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as EditGrid, Google Sheets, Microsoft Excel Online, Quantrix Qloud, Smartsheet, ZK Spreadsheet, or ZOHO also have strong multi-user collaboration features and/or offer real time updates from remote sources such as stock prices and currency exchange rates.
Other products.
A number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.
Spreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.
Concepts.
The main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.
Cells.
A "cell" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).
An array of cells is called a "sheet" or "worksheet". It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).
A cell may contain a value or a formula, or it may simply be left empty.
By convention, formulas usually begin with = sign.
Values.
A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.
The Spreadsheet "Value Rule"
Computer scientist Alan Kay used the term "value rule" to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.
The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.
Automatic recalculation.
A standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.
Real-time update.
This feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a "remote" spreadsheet. For shared, Web-based spreadsheets, it applies to "immediately" updating cells another user has updated. All dependent cells must be updated also.
Locked cell.
Once entered, selected cells (or the entire spreadsheet) can optionally be "locked" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing "constants" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.
Data format.
A cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example "31/12/2007" or "31 Dec 2007" would default to the cell format of "date".
Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.
Some cell formats such as "numeric" or "currency" can also specify the number of decimal places.
This can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.
Cell formatting.
Depending on the capability of the spreadsheet application, each cell (like its counterpart the "style" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.
A cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.
Named cells.
In most implementations, a cell, or group of cells in a column or row, can be "named" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.
Cell reference.
In place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.
A typical cell reference in "A1" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g., in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative "R1C1" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.
When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).
A cell on the same "sheet" is usually addressed as:
A cell on a different sheet of the same spreadsheet is usually addressed as:
Some spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:
In a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.
A circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.
Cell ranges.
Likewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as "=SUM(A1:A6)" would add all the cells specified and put the result in the cell containing the formula itself.
Sheets.
In the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.
Formulas.
A formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by "clicking" the mouse over a particular cell; otherwise it contains the result of the calculation.
A formula assigns values to a cell or range of cells, and typically has the format:
where the expression consists of:
When a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., codice_13, or codice_14), absolute (e.g., codice_25, or codice_26) or mixed row– or column-wise absolute/relative (e.g., codice_27 is column-wise absolute and codice_28 is row-wise absolute).
The available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.
A formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.
Further examples:
The best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.
A spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.
Functions.
Spreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for "user-defined functions". In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name "sq" is user-assigned, and function "sq" is introduced using the "Visual Basic" editor supplied with Excel. "Name Manager" displays the spreadsheet definitions of named variables "x" & "y".
Subroutines.
Functions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable "x", calculates its square, and writes this value into the corresponding element of named column variable "y". The "y" column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.
Remote spreadsheet.
Whenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a "remote" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.
Charts.
Many spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.
Multi-dimensional spreadsheets.
In the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared and later Quantrix. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation. 
In these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.
Trapeze, a spreadsheet on the Mac, went further and explicitly supported
not just table columns, but also matrix operators.
Logical spreadsheets.
Spreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.
Programming issues.
Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.
End-user development.
Spreadsheets are a popular End-user development tool. EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.
Spreadsheet programs.
A "spreadsheet program" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.
It is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.
Spreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.
Many of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).
Spreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.
Shortcomings.
While spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.
Other problems associated with spreadsheets include:
While there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals "don't know" how their spreadsheets are audited; only 6% invest in a third-party solution
Spreadsheet risk.
Spreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1 billion. Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.
In the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.
Despite this, research carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.
In 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010–13 European austerity programs.

</doc>
<doc id="27687" url="https://en.wikipedia.org/wiki?curid=27687" title="St. Louis">
St. Louis

St. Louis ( or ) is a city and port in the U.S. state of Missouri. The city developed along the western bank of the Mississippi River, which forms Missouri's border with Illinois. In 2010, St. Louis had a population of 319,294; a 2014 estimate put the population at 317,419, making it the 60th-most populous U.S. city and the second-largest city in the state in terms of city proper population, after Kansas City, Missouri. The St. Louis metropolitan area includes the city as well as nearby areas in Missouri and Illinois; with an estimated population of 2,905,893, it is the largest in Missouri and the nineteenth largest in the United States. St. Louis was founded in 1764 by Pierre Laclède and Auguste Chouteau and named after Louis IX of France. Claimed first by the French, who settled mostly east of the Mississippi River, the region in which the city stands was ceded to Spain following France's defeat in the Seven Years' War. Its territory east of the Mississippi was ceded to the Kingdom of Great Britain, the victor. The area of present-day Missouri was part of Spanish Louisiana from 1762 until 1803.
After the United States acquired this territory in the Louisiana Purchase, St. Louis developed as a major port on the Mississippi River. In the late 19th century, St. Louis was ranked as the fourth-largest city in the United States. It separated from St. Louis County in 1877, becoming an independent city and limiting its political boundaries. In 1904, it hosted the Louisiana Purchase Exposition and the Summer Olympics. Immigration has increased, and the city is the center of the largest Bosnian population in the world outside their homeland.
The economy of St. Louis relies on service, manufacturing, trade, transportation of goods, and tourism. The city is home to several major corporations including Anheuser-Busch, Express Scripts, Peabody Energy, Ameren, Ralcorp, Monsanto and Sigma-Aldrich, as well as a large medical and research community. St. Louis has two professional sports teams: the St. Louis Cardinals of Major League Baseball, and the St. Louis Blues of the National Hockey League. The city is commonly identified with the tall Gateway Arch in Downtown St. Louis.
History.
The area that would become St. Louis was a center of the Native American Mississippian culture, which built numerous temple and residential earthwork mounds on both sides of the Mississippi River. Their major regional center was at Cahokia Mounds, active from 900 AD to 1500 AD. Due to numerous major earthworks within St. Louis boundaries, the city was nicknamed as the "Mound City." These mounds were mostly demolished during the city's development. Historic Native American tribes in the area included the Siouan-speaking Osage people, whose territory extended west, and the Illiniwek.
European exploration of the area was first recorded in 1673, when French explorers Louis Jolliet and Jacques Marquette traveled through the Mississippi River valley. Five years later, La Salle claimed the region for France as part of "La Louisiane."
The earliest European settlements in the area were built in Illinois Country (also known as Upper Louisiana) on the east side of the Mississippi River during the 1690s and early 1700s at Cahokia, Kaskaskia, and Fort de Chartres. Migrants from the French villages on the opposite side of the Mississippi River (e.g. Kaskaskia) founded Ste. Genevieve in the 1730s.
In 1721 after leaving Quebec, French traveller Pierre François Xavier de Charlevoix reached the area, calling it "the finest confluence in the world". 
In early 1764, after France lost the Seven Years' War, Pierre Laclède and his stepson Auguste Chouteau founded what was to become the city of St. Louis. (French lands east of the Mississippi had been ceded to Great Britain and the lands west of the Mississippi to Spain; France and Spain were 18th-century allies and both were Catholic nations.) The early French families built the city's economy on the fur trade with the Osage, as well as with more distant tribes along the Missouri River. The Chouteau brothers gained a monopoly from Spain on the fur trade with Santa Fe. French colonists used African slaves as domestic servants and workers in the city.
From 1762 to 1803 European control of the area west of the Mississippi to the northernmost part of the Missouri River basin, called Louisiana, was assumed by the Spanish as part of the Viceroyalty of New Spain. In 1780 during the American Revolutionary War, St. Louis was attacked by British forces, mostly Native American allies, in the Battle of St. Louis.
Founding and before 19th century.
The founding of St. Louis began in 1763. Pierre Laclede led an expedition to set up a fur-trading post farther up the Mississippi River. Before then, Laclede had been a very successful merchant. For this reason, he and his trading partner Gilbert Antoine de St. Maxent were offered monopolies for six years of the fur trading in that area.
Though they were originally only granted rights to set-up a trading post, Laclede and other members of his expedition quickly set up a settlement. Some historians believe that Laclede's determination to create this settlement was the result of his affair with a married woman Marie-Thérèse Bourgeois Chouteau, and so they wanted to get out of New Orleans.
Laclede on his initial expedition was accompanied by Auguste Chouteau. Some historians still debate which of the two men was the true founder of St. Louis. The reason for this lingering question is that all the documentation of the founding was loaned, and subsequently destroyed in a fire.
For the first few years of St. Louis' existence, the city was not recognized by any of the governments. Though originally thought to be under the control of the Spanish government, no one asserted any authority over the settlement, and thus St. Louis had no local government. This led Laclede to assume a position of civil control, and all problems were disposed in public settings, such as conmunal meetings. In addition, Laclede granted new settlers lots in town and the country, to give something to the new settlers to start off with. In hindsight, many of these original settlers thought of these first few years as "the golden age of St. Louis."
However, by 1765, the city began receiving notifications and visits from representatives and military actives from members of the English, French, and Spanish governments. In addition, the Indians in the local area expressed dissatisfaction to being under the command and control of British forces. One of the great Ottawa chieftain, Pontiac, was angered by the change of power and potential for the British to come into their lands. He desired to fight against them with power and force, but many of the St. Louis inhabitants refused.
Around this time, a man named St. Ange came and helped make St. Louis more of successful city, helping deal with all the diplomatic issues from the various nationalities of the British, French, and Spanish governments, along with the local Indians.
19th century.
St. Louis was transferred to the French First Republic in 1800 (although all of the colonial lands continued to be administered by Spanish officials), then sold by the French to the U.S. in 1803 as part of the Louisiana Purchase. St. Louis became the capital of, and gateway to, the new territory. Shortly after the official transfer of authority was made, the Lewis and Clark Expedition was commissioned by President Thomas Jefferson. The expedition departed from St. Louis in May 1804 along the Missouri River to explore the vast territory. There were hopes of finding a water route to the Pacific Ocean, but the party had to go overland in the Upper West. They reached the Pacific Ocean via the Columbia River in summer 1805. They returned, reaching St. Louis on September 23, 1806. Both Lewis and Clark lived in St. Louis after the expedition. Many other explorers, settlers, and trappers (such as Ashley's Hundred) would later take a similar route to the West.
The city elected its first municipal legislators (called trustees) in 1808. Steamboats first arrived in St. Louis in 1818, improving connections with New Orleans and eastern markets. Missouri was admitted as a state in 1821, in which slavery was legal. As the state gained settlers, the first temporary of capital of the state of Missouri was St. Louis then the capitol of Missouri moved to St. Charles, Missouri then moved to the more central location of Jefferson City, Missouri in 1826. St. Louis was incorporated as a city in 1822, and continued to develop largely due to its busy port and trade connections. Slaves worked in many jobs on the waterfront as well as on the riverboats. Given the city's location close to the free state of Illinois and others, some slaves escaped to freedom. Others, especially women with children, sued in court in freedom suits, and several prominent local attorneys aided slaves in these suits. About half the slaves achieved freedom in hundreds of suits before the American Civil War.
Immigrants from Ireland and Germany arrived in St. Louis in significant numbers starting in the 1840s, and the population of St. Louis grew from less than 20,000 in 1840, to 77,860 in 1850, to more than 160,000 by 1860. By the mid-1800s, St. Louis had a greater population than New Orleans.
Settled by many Southerners in a slave state, the city was split in political sympathies and became polarized during the American Civil War. In 1861, 28 civilians were killed in a clash with Union troops. The war hurt St. Louis economically, due to the Union blockade of river traffic to the south on the Mississippi River. The St. Louis Arsenal constructed ironclads for the Union Navy.
After the war, St. Louis profited via trade with the West, aided by the 1874 completion of the Eads Bridge, named for its architect. Industrial developments on both banks of the river were linked by the bridge, the first in the mid-west over the Mississippi River. The bridge connects St. Louis, Missouri to East St. Louis, Illinois. The Eads Bridge became an iconic image of the city of St. Louis, from the time of its erection until 1965 when the Gateway Arch Bridge was constructed. The bridge crosses the St. Louis riverfront between Laclede's Landing, to the north, and the grounds of the Gateway Arch, to the south. Today the road deck has been restored, allowing vehicular and pedestrian traffic to cross the river. The St. Louis MetroLink light rail system has used the rail deck since 1993. An estimated 8,500 vehicles pass through it daily.
On August 22, 1876, the city of St. Louis voted to secede from St. Louis County and become an independent city. Industrial production continued to increase during the late 19th century. Major corporations such as the Anheuser-Busch brewery and Ralston-Purina company were established. St. Louis also was home to Desloge Consolidated Lead Company and several brass era automobile companies, including the Success Automobile Manufacturing Company; St. Louis is the site of the Wainwright Building, an early skyscraper built in 1892 by noted architect Louis Sullivan.
20th century.
In 1904, the city hosted the 1904 World's Fair and the 1904 Summer Olympics, becoming the first non-European city to host the Olympics. Permanent facilities and structures remaining from the fair are Forest Park and associated structures within its boundaries: the St. Louis Art Museum, the St. Louis Zoo and the Missouri History Museum.
In the aftermath of emancipation of slaves following the Civil War, social and racial discrimination in housing and employment were common in St. Louis. Starting in the 1910s, many property deeds included racial or religious restrictive covenants against new immigrants and migrants. In the first half of the 20th century, St. Louis was a destination for many African Americans in the Great Migration from the rural South seeking better opportunities. During World War II, the NAACP campaigned to integrate war factories, and restrictive covenants were prohibited in 1948 by the "Shelley v. Kraemer" U.S. Supreme Court decision, which case originated as a lawsuit in St. Louis. In 1964 civil rights activists protested at the construction of the Gateway Arch to publicize their effort to gain entry for African Americans into the skilled trade unions, where they were underrepresented. The Department of Justice filed the first suit against the unions under the Civil Rights Act of 1964.
"De jure" educational segregation continued into the 1950s, and "de facto" segregation continued into the 1970s, leading to a court challenge and interdistrict desegregation agreement. Students have been bussed mostly from the city to county school districts to have opportunities for integrated classes, although the city has created magnet schools to attract students.
St. Louis, like many Midwestern cities, expanded in the early 20th century due to industrialization, which provided jobs to new generations of immigrants and migrants from the South. It reached its peak population of 856,796 at the 1950 census. Suburbanization from the 1950s through the 1990s dramatically reduced the city's population, as did restructuring of industry and loss of jobs. The effects of suburbanization were exacerbated by the relatively small geographical size of St. Louis due to its earlier decision to become an independent city, and it lost much of its tax base. During the 19th and 20th century, most major cities aggressively annexed surrounding areas as residential development occurred away from the central city; however, St. Louis was unable to do so.
In the 21st century, the city of St. Louis contains only 11% of its total metropolitan population, while among the top 20 metro areas in the United States, the central cities contain an average of 24% of total metropolitan area population. Although small increases in population have taken place in St. Louis during the early 2000s, overall the city lost population from 2000 to 2010. Immigration has continued, with the city attracting Vietnamese, Latinos from Mexico and Central America, and Bosnians, the latter forming the largest Bosnian community outside of Bosnia.
Several urban renewal projects were built in the 1950s, as the city worked to replace old and substandard housing. Some of these were poorly designed and resulted in problems, of which Pruitt-Igoe became a symbol of failure. It was torn down.
Since the 1980s, several revitalization efforts have focused on downtown St. Louis.
21st century.
Urban revitalization continued in the new century. Gentrification has taken place in the Washington Avenue Historic District. In 2006, St. Louis received the World Leadership Award for urban renewal. In 2013 the US Census Bureau estimate that St. Louis had a population of 318,416.
In 2014, St. Louis celebrated its 250th birthday with events throughout the year. These were coordinated by the Missouri History Museum through its nonprofit entity, stl250, with help from the Saint Louis Ambassadors volunteer organization and its U.S. Small Business Institute. Commemorations of the Arch's 50th birthday are planned for 2015.
Geography.
Topography.
According to the United States Census Bureau, St. Louis has a total area of , of which is land and (6.2%) is water. (Not shown on simple maps of the city, the land at its airport is owned by the city, served by its fire department and others, and is an exclave of St. Louis.) The city is built primarily on bluffs and terraces that rise 100–200 feet above the western banks of the Mississippi River, in the Midwestern United States just south of the Missouri-Mississippi confluence. Much of the area is a fertile and gently rolling prairie that features low hills and broad, shallow valleys. Both the Mississippi River and the Missouri River have cut large valleys with wide flood plains.
Limestone and dolomite of the Mississippian epoch underlie the area, and parts of the city are karst in nature. This is particularly true of the area south of downtown, which has numerous sinkholes and caves. Most of the caves in the city have been sealed, but many springs are visible along the riverfront. Coal, brick clay, and millerite ore were once mined in the city. The predominant surface rock, known as "St. Louis limestone", is used as dimension stone and rubble for construction.
Near the southern boundary of the city of St. Louis (separating it from St. Louis County) is the River des Peres, practically the only river or stream within the city limits that is not entirely underground. Most of River des Peres was confined to a channel or put underground in the 1920s and early 1930s. The lower section of the river was the site of some of the worst flooding of the Great Flood of 1993.
The city's eastern boundary is the Mississippi River, which separates Missouri from Illinois. The Missouri River forms the northern line of St. Louis County, except for a few areas where the river has changed its course. The Meramec River forms most of its southern line.
Neighborhoods.
The city is divided into 79 government-designated neighborhoods. The neighborhood divisions have no legal standing, although some neighborhood associations administer grants or hold veto power over historic-district development. 
Climate.
St. Louis lies in the transitional zone between the humid continental climate type and the humid subtropical climate type (Köppen "Dfa" and "Cfa", respectively), with neither large mountains nor large bodies of water to moderate its temperature. The city experiences hot, humid summers and cold winters. It is subject to both cold Arctic air and hot, humid tropical air from the Gulf of Mexico. The average annual temperature recorded at nearby Lambert–St. Louis International Airport, is . Both temperatures can be seen on an average 2 or 3 days per year. Average annual precipitation is about , but annual precipitation has ranged from in 1953 to in 2015. The city has four distinct seasons:
Flora and fauna.
Before the founding of the city, the area was mostly prairie and open forest. Native Americans maintained this environment, good for hunting, by burning underbrush. Trees are mainly oak, maple, and hickory, similar to the forests of the nearby Ozarks; common understory trees include eastern redbud, serviceberry, and flowering dogwood. Riparian areas are forested with mainly American sycamore.
Most of the residential areas of the city are planted with large native shade trees. The largest native forest area is found in Forest Park. In autumn, the changing color of the trees is notable. Most species here are typical of the eastern woodland, although numerous decorative non-native species are found. The most notable invasive species is Japanese honeysuckle, which officials are trying to manage because of its damage to native trees. It is removed from some parks.
Large mammals found in the city include urbanized coyotes and white-tailed deer. Eastern gray squirrel, cottontail rabbit, and other rodents are abundant, as well as the nocturnal Virginia opossum. Large bird species are abundant in parks and include Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Gulls are common along the Mississippi River; these species typically follow barge traffic.
Winter populations of bald eagles are found along the Mississippi River around the Chain of Rocks Bridge. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern US. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. The city has special sites for birdwatching of migratory species, including Tower Grove Park.
Frogs are commonly found in the springtime, especially after extensive wet periods. Common species include the American toad and species of chorus frogs commonly called spring peepers, which are found in nearly every pond. Some years have outbreaks of cicadas or ladybugs. Mosquitos, no-see-ums, and houseflies are common insect nuisances, especially in July and August; because of this, windows are nearly universally fitted with screens. Invasive populations of honeybees have sharply declined in recent years. Numerous native species of pollinator insects have recovered to fill their ecological niche, and armadillos have been regularly seen throughout the St. Louis area, especially since 2005.
Demographics.
St. Louis grew slowly until the American Civil War, when industrialization and immigration sparked a boom. Mid-19th century immigrants included many Irish and Germans; later there were immigrants from southern and eastern Europe. In the early 20th century, African American and white migrants came from the South; the former as part of the Great Migration out of rural areas of the Deep South. Many came from Mississippi and Arkansas.
After years of immigration, migration, and expansion, the city reached its peak population in 1950. That year, the Census Bureau reported St. Louis' population as 82% White and 17.9% African American. After World War II, St. Louis began losing population to the suburbs, first because of increased demand for new housing, unhappiness with city services, ease of commuting by subsidized highways, and later, white flight. St. Louis' population decline has resulted in a significant increase of abandoned residential housing units and vacant lots throughout the city proper; this blight has attracted much wildlife (such as deer and coyotes) to the many abandoned overgrown lots.
St. Louis has lost 62.7% of its population since the 1950 United States Census, the highest percent of any city that had a population of 100,000 or more at the time of the 1950 Census. Detroit, Michigan and Youngstown, Ohio are the only other cities that have had population declines of at least 60% in the same time frame. The population of the city of St. Louis has been in decline since the 1960 census; during this period the population of the St. Louis Metropolitan Area, which includes more than one county, has grown every year and continues to do so. A big factor in the decline has been the rapid increase in suburbanization.
According to the 2010 United States Census, St. Louis had 319,294 people living in 142,057 households, of which 67,488 households were families. The population density was 5,158.2 people per square mile (1,990.6/km²). About 24% of the population was 19 or younger, 9% were 20 to 24, 31% were 25 to 44, 25% were 45 to 64, and 11% were 65 or older. The median age was about 34 years.
The population was about 49.2% African American, 43.9% White (42.2% Non-Hispanic White), 2.9% Asian, 0.3% Native American/Alaska Native, and 2.4% reporting two or more races. Hispanic or Latino of any race were 3.5% of the population.
The African-American population is concentrated in the north side of the city (the area north of Delmar Boulevard is 94.0% black, compared with 35.0% in the central corridor and 26.0% in the south side of St. Louis ). Among the Asian-American population in the city, the largest ethnic group is Vietnamese (0.9%), followed by Chinese (0.6%) and Asian Indians (0.5%). The Vietnamese community has concentrated in the Dutchtown neighborhood of south St. Louis; Chinese are concentrated in the Central West End. People of Mexican descent are the largest Latino group, and make up 2.2% of St. Louis' population. They have the highest concentration in the Dutchtown, Benton Park West (Cherokee Street), and Gravois Park neighborhoods.
In 2000, the median income for a household in the city was $29,156, and the median income for a family was $32,585. Males had a median income of $31,106; females, $26,987. Per capita income was $18,108.
Some 19% of the city's housing units were vacant, and slightly less than half of these were vacant structures not for sale or rent.
In 2010, St. Louis' per-capita rates of online charitable donations and volunteerism were among the highest among major U.S. cities.
As of 2010, 91.05% (270,934) of St. Louis city residents age 5 and older spoke English at home as a primary language, while 2.86% (8,516) spoke Spanish, 0.91% (2,713) Bosnian, 0.74% (2,200) Vietnamese, 0.50% (1,495) African languages, 0.50% (1,481) Chinese, and French was spoken as a main language by 0.45% (1,341) of the population over the age of five. In total, 8.95% (26,628) of St. Louis' population age 5 and older spoke a mother language other than English.
Bosnian population.
About 15 families from Bosnia settled in St. Louis in 1960 and 1970. After the Bosnian War started in 1992, more Bosnian refugees began arriving and by 2000, tens of thousands of Bosnian refugees settled in St. Louis with the help of Catholic aid societies. Many of them were professionals and skilled workers who had to take any job opportunity to be able to support their families. Most Bosnians refugees are Muslim, ethnically Bosniaks (87%); they have settled primarily in south St. Louis and South County. Bosnian-Americans are well integrated into the city, developing many businesses and ethnic/cultural organizations.
An estimated 70,000 Bosnians live in the metro area, the largest population of Bosnians in the United States and the largest Bosnian population outside their homeland. The highest concentration of Bosnians is in the neighborhood of Bevo Mill, and in Affton, Mehlville and Oakville of south St. Louis County.
Economy.
The 2014 Gross Metropolitan Product (GMP) of St. Louis was $145.958 billion up from $144.03 in 2013, $138.403 in 2012 and $133.1 in 2011 making it the 21st-highest in the country. The St. Louis Metropolitan Area had a Per capita GDP of $48,738 in 2014 up 1.6% from 2013. This signals the growth of the St. Louis economy. According to the 2007 Economic Census, manufacturing in the city conducted nearly $11 billion in business, followed by the health care and social service industry with $3.5 billion, professional or technical services with $3.1 billion, and the retail trade with $2.5 billion. The health care sector was the biggest employer in the area with 34,000 workers, followed by administrative and support jobs, 24,000; manufacturing, 21,000, and food service, 20,000.
Major companies and institutions.
As of 2013, the St. Louis Metropolitan Area is home to nine Fortune 500 companies, the third-highest in the Midwestern United States. St. Louis is home to two Fortune 500 companies: Peabody Energy and Ameren. In addition, seven other Fortune 500 companies are headquartered in the MSA: Express Scripts, Emerson Electric, Monsanto, Reinsurance Group of America, Centene, Graybar Electric, and Edward Jones Investments.
Other notable corporations headquartered in the region include Arch Coal, Scottrade, Wells Fargo Advisors (formerly A.G. Edwards), Energizer Holdings, Patriot Coal, Post Foods, United Van Lines, and Mayflower Transit, Ralcorp, Hardee's, Olin, and Enterprise Holdings (a parent company of several car rental companies). Notable corporations with operations in St. Louis include Cassidy Turley, Kerry Group, MasterCard, TD Ameritrade, and BMO Harris Bank.
Health care and biotechnology institutions with operations in St. Louis include Pfizer, the Donald Danforth Plant Science Center, the Solae Company, Sigma-Aldrich, and Multidata Systems International. General Motors manufactures automobiles in Wentzville, while an earlier plant, known as the St. Louis Truck Assembly, built GMC automobiles from 1920 until 1987. Chrysler closed its Saint Louis Assembly production facility in nearby Fenton, Missouri and Ford closed the St. Louis Assembly Plant in Hazelwood.
Several once-independent pillars of the local economy have been purchased by other corporations. Among them are Anheuser-Busch, purchased by Belgium-based InBev; Missouri Pacific Railroad, which was headquartered in St. Louis, merged with the Omaha, Nebraska-based Union Pacific Railroad in 1982; McDonnell Douglas, whose operations are now part of Boeing Defense, Space & Security; Mallinckrodt, purchased by Tyco International; and Ralston Purina, now a wholly owned subsidiary of Nestlé. The May Department Stores Company (which owned Famous-Barr and Marshall Field's stores) was purchased by Federated Department Stores, which has its regional headquarters in the area. The Federal Reserve Bank of St. Louis in downtown is one of two federal reserve banks in Missouri. Most of the assets of Furniture Brands International were sold to Heritage Home Group in 2013, and while that company remained in the area for a brief time, it has moved to North Carolina.
St. Louis is a center of medicine and biotechnology. The Washington University School of Medicine is affiliated with Barnes-Jewish Hospital, the fifth-largest hospital in the world. Both institutions operate the Alvin J. Siteman Cancer Center. The School of Medicine also is affiliated with St. Louis Children's Hospital, one of the country's top pediatric hospitals. Both hospitals are owned by BJC HealthCare. The McDonnell Genome Institute at Washington University played a major role in the Human Genome Project. St. Louis University Medical School is affiliated with SSM Health's Cardinal Glennon Children's Hospital and St. Louis University Hospital. It also has a cancer center, vaccine research center, geriatric center, and a bioethics institute. Several different organizations operate hospitals in the area, including BJC HealthCare, Mercy, SSM Health Care, and Tenet.
Boeing employs nearly 15,000 people in its north St. Louis campus, headquarters to its defense unit. In 2013, the company said it would move about 600 jobs from Seattle, where labor costs have risen, to a new IT center in St. Louis. Other companies, such as LaunchCode and LockerDome, see the city's potential to become the next major tech hub. Programs such as Arch Grants are attracting new startups to the region.
According to the "St. Louis Business Journal", the top employers in the St. Louis metropolitan area as of June 1, 2015, are as follows:
According to St. Louis' 2013 Comprehensive Annual Financial Report, the top employers in the City only for 2012 are:
Culture.
With its French past and waves of Catholic immigrants in the 19th and 20th centuries, from Ireland, Germany and Italy, St. Louis is a major center of Roman Catholicism in the United States. St. Louis also boasts the largest Ethical Culture Society in the United States, and consistently ranks as one of the most generous cities in the United States, ranking ninth in 2013. Several places of worship in the city are noteworthy, such as the Cathedral Basilica of St. Louis, home of the world's largest mosaic installation.
Other locally notable churches include the Basilica of St. Louis, King of France, the oldest Roman Catholic cathedral west of the Mississippi River and the oldest church in St. Louis; the St. Louis Abbey, whose distinctive architectural style garnered multiple awards at the time of its completion in 1962; and St. Francis de Sales Oratory, a neo-Gothic church completed in 1908 in South St. Louis and the second-largest church in the city.
The city is defined by music and the performing arts, especially its association with blues, jazz, and ragtime. St. Louis is home to the St. Louis Symphony, the second-oldest symphony orchestra in the United States, which has toured nationally and internationally to strong reviews. Until 2010, it was also home to KFUO-FM, one of the oldest classical music FM radio stations west of the Mississippi River.
The Gateway Arch marks downtown St. Louis and a historic center that includes the Federal courthouse where the Dred Scott case was first argued, a newly renovated and expanded public library, major churches and businesses, and retail. An increasing downtown residential population has taken to adapted office buildings and other historic structures. In nearby University City is the Delmar Loop, ranked by the American Planning Association as a "great American street" for its variety of shops and restaurants, and the Tivoli Theater, all within walking distance.
Unique city and regional cuisine reflecting various immigrant groups include toasted ravioli, gooey butter cake, provel cheese, the slinger, the Gerber sandwich, the St. Paul sandwich, and St. Louis-style pizza, featuring thin crust and provel cheese. Some St. Louis chefs have begun emphasizing use of local produce, meats and fish, and neighborhood farmers' markets have become increasingly popular, as well as one downtown. Artisan bakeries, salumeria, and chocolatiers also operate in the city.
Also unique to St. Louis is the Ted Drewes "Concrete", which is frozen custard blended with any combination of dozens of ingredients, served in a large yellow cup with a spoon and straw. The mixture is so thick that a spoon inserted into the custard does not fall if the cup is inverted. Ted Drewes owns and operates a pair of frozen custard shops in St. Louis, which have been highlighted in the national media on several occasions. In 2006, the Route 66 (Chippewa) location was featured on the Food Network show "Feasting on Asphalt", hosted by Alton Brown. In 2010, it was recommended by Bobby Flay on the "Sweet Tooth" episode of "The Best Thing I Ever Ate". In 2011, it was featured in a special "Route 66" episode of "Man v. Food Nation", hosted by Adam Richman.
Architecture.
The architecture of St. Louis exhibits a variety of commercial, residential, and monumental architecture. St. Louis is known for the Gateway Arch, the tallest monument constructed in the United States at . The Arch pays homage to Thomas Jefferson and St. Louis' position as the gateway to the West. Architectural influences reflected in the area include French Colonial, German, early American, and modern architectural styles.
Skyscrapers.
Some notable post-modern commercial skyscrapers were built downtown in the 1970s and 1980s, including the One US Bank Plaza (1976), the AT&T Center (1986), and One Metropolitan Square (1989), which is the tallest building in St. Louis. One US Bank Plaza, the local headquarters for US Bancorp, was constructed for the Mercantile Bancorporation in the Structural expressionist style, emphasizing the steel structure of the building.
During the 1990s, St. Louis saw the construction of the largest United States courthouse by area, the Thomas F. Eagleton United States Courthouse (completed in 2000). The Eagleton Courthouse is home to the United States District Court for the Eastern District of Missouri and the United States Court of Appeals for the Eighth Circuit. The most recent high-rise buildings in St. Louis include two residential towers: the Park East Tower in the Central West End and the Roberts Tower located in downtown.
Landmarks and monuments.
Several examples of religious structures are extant from the pre-Civil War period, and most reflect the common residential styles of the time. Among the earliest is the Basilica of St. Louis, King of France (locally referred to as the "Old Cathedral"). The Basilica was built between 1831 and 1834 in the Federal style. Other religious buildings from the period include SS. Cyril and Methodius Church (1857) in the Romanesque Revival style and Christ Church Cathedral (completed in 1867, designed in 1859) in the Gothic Revival style.
Only a few civic buildings were constructed during the early 19th century. The original St. Louis courthouse was built in 1826 and featured a Federal style stone facade with a rounded portico. However, this courthouse was replaced during renovation and expansion of the building in the 1850s. The Old St. Louis County Courthouse (locally known as the "Old Courthouse") was completed in 1864 and was notable for having an early cast iron dome and for being the tallest structure in Missouri until 1894. Finally, a customs house was constructed in the Greek Revival style in 1852, but was demolished and replaced in 1873 by the U.S. Customhouse and Post Office.
Because much of the city's early commercial and industrial development was centered along the riverfront, many pre-Civil War buildings were demolished during construction of the Gateway Arch. The city's remaining architectural heritage of the era includes a multi-block district of cobblestone streets and brick and cast-iron warehouses called Laclede's Landing. Now popular for its restaurants and nightclubs, the district is located north of Gateway Arch along the riverfront. Other industrial buildings from the era include some portions of the Anheuser-Busch Brewery, which date to the early 1860s.
St. Louis saw a vast expansion in variety and number of religious buildings during the late 19th century and early 20th century. The largest and most ornate of these is the Cathedral Basilica of St. Louis, designed by Thomas P. Barnett and constructed between 1907 and 1914 in the Neo-Byzantine style. The St. Louis Cathedral, as it is known, has one of the largest mosaic collections in the world. Another landmark in religious architecture of St. Louis is the St. Stanislaus Kostka, which is an example of the Polish Cathedral style. Among the other major designs of the period were St. Alphonsus Liguori (locally known as "The Rock Church") (1867) in the Gothic Revival and Second Presbyterian Church of St. Louis (1900) in Richardsonian Romanesque. 
Early in the 20th century (and during the years before and after the 1904 World's Fair), several churches moved to the Central West End neighborhood, near Forest Park and the fairgrounds. The neighborhood features the Holy Corners Historic District, which is a concentration of several historic religious structures, such as the First Church of Christ, Scientist (1904).
By the 1900 census, St. Louis was the fourth largest city in the country. In 1904, the city hosted a world's fair at Forest Park called the Louisiana Purchase Exposition. Its architectural legacy is somewhat scattered. Among the fair-related cultural institutions in the park are the Saint Louis Art Museum designed by Cass Gilbert, part of the remaining lagoon at the foot of Art Hill, and the Flight Cage at the St. Louis Zoo. The Missouri History Museum was built afterward, with the profit from the fair. But 1904 left other assets to the city, like Theodore Link's 1894 St. Louis Union Station, and an improved Forest Park.
Louis Sullivan designed Charlotte Dickson Wainwright's tomb on the north side of Bellefontaine Cemetery, surrounded by a collection of similar tombs for the great old St. Louis families, interesting for their late-Gilded Age artwork.
Shortly after the Civil War, St. Louis rapidly increased its school system and hospital system. One of the earliest structures and the oldest extant hospital building in St. Louis is the St. Louis Insane Asylum (now the Metropolitan St. Louis Psychiatric Center). The asylum is built of brick in the Italianate style, complete with cast iron dome and cupola reminiscent of the Old Courthouse.
As St. Louis expanded, the city hall was moved further west of downtown to its present location in 1904 (construction began in 1892). St. Louis City Hall, still in use, was designed by Harvey Ellis in the Renaissance Revival style, reminiscent of the Hôtel de Ville (City Hall) in Paris, France.
Other significant civic buildings from the late 19th century and early 20th century include the U.S. Customhouse and Post Office by Alfred B. Mullett (1873) and the stately St. Louis Public Library by Cass Gilbert (1912). While the Old Post Office has been renovated, the St. Louis Public Library is slated for renovation as of 2010. In 1923 the city passed a $87 million bond issue for re-development of the Civic Plaza along the lines of the City Beautiful movement. This development resulted in some of St. Louis's major civic architecture: the Soldiers' Memorial, the Civil Courts Building, and Kiel Auditorium.
Then into the 1940s and 1950s a certain subgenre of St. Louis modernism emerged, with the locally important architect Harris Armstrong, and a series of daring modern civic landmarks like Gyo Obata's Planetarium, the geodesic-dome Climatron, and the main terminal building at Lambert-St. Louis International Airport. The Poplar Street Bridge, a long (197m) deck girder bridge, was built in 1967 and continues to carry two Interstates (Interstates 55 and 64) and one U.S. route (U.S. 40) across the Mississippi River (until February 2014 it carried a third Interstate, I-70). St. Louis also was the headquarters for postwar modernist bank designer Wenceslaus Sarmiento, whose major work in St. Louis is the Chancery Building (1965) on the grounds of the Cathedral Basilica of St. Louis. The culmination of St. Louis modern architecture is Eero Saarinen's magnificent stainless-steel gesture, the Gateway Arch, centerpiece of the riverside Jefferson National Expansion Memorial.
Sports.
St. Louis is home to Major League Baseball and the National Hockey League, notable collegiate-level soccer teams, and has hosted several collegiate sports tournaments.
Professional sports.
St. Louis is home to two major league sports teams. The St. Louis Cardinals are one of the most successful franchises in Major League Baseball. They have won 19 National League titles (the most pennants for the league franchise in one city) and 11 World Series titles, most recently in 2011. They play at Busch Stadium. Previously, the St. Louis Browns played in the American League from 1902 to 1953 before moving to Baltimore to become the current incarnation of the Orioles.
The St. Louis Blues of the National Hockey League play at the Scottrade Center (formerly the Savvis Center, originally Kiel Center). They were one of the six teams added to the NHL in the 1967 expansion. They have never won a conference championship.
St. Louis has been home to four different National Football League teams. The St. Louis All-Stars played in the city in 1923, the St. Louis Gunners in 1934, the St. Louis Cardinals from 1960 to 1987, and the St. Louis Rams from 1995 to 2015. The Cardinals advanced to the playoffs just three times (1974, 1975 and 1982), never hosting or winning in any appearance. The Cardinals moved to Phoenix in 1988. The St. Louis Rams played at the Edward Jones Dome from 1995 to 2015 and went on to win Super Bowl XXXIV. The Rams returned to Los Angeles in 2016.
The St. Louis Hawks of the NBA played at Kiel Auditorium from 1955 to 1968. They won the NBA Championship in 1958 and made the NBA Finals in 1957 and 1960. In 1969 they became the Atlanta Hawks.
St. Louis also hosts several minor league sports teams. The Gateway Grizzlies and the River City Rascals of the Frontier League (which are not affiliated with Major League Baseball) play in the area. The St. Louis Trotters of the Independent Basketball Association play at Matthews Dickey. Saint Louis FC of the United Soccer League play at World Wide Technology Soccer Park and the River City Raiders play at the Family Arena. The region hosts NHRA drag racing and NASCAR events at the Gateway International Raceway in Madison, Illinois.
Amateur sports.
At the collegiate level, St. Louis has hosted the Final Four of both the women's and men's college basketball NCAA Division I championship tournaments, and the Frozen Four collegiate ice hockey tournament. Although the area does not have a National Basketball Association team, it hosts an American Basketball Association team called the St. Louis Phoenix. Saint Louis University has won 10 NCAA Men's Soccer Championships, and the city has hosted the College Cup several times. In addition to collegiate soccer, many St. Louisans have played for the United States men's national soccer team, and 20 St. Louisans have been elected into the National Soccer Hall of Fame. St. Louis also is the origin of the sport of corkball, a type of baseball in which there is no base running. The St. Louis TV market is the largest in the nation without a Division I college football team.
Chess.
St. Louis is home to the Chess Club and Scholastic Center of Saint Louis (CCSCSL) where the U.S. Chess Championship is held. St. Louisan Rex Sinquefield founded the CCSCSL and moved the World Chess Hall of Fame to St. Louis in 2011. The Sinquefield Cup Tournament started at St. Louis in 2013. In 2014 the Sinquefield Cup was the highest rated chess tournament of all time. Fabiano Caruana won the 2014 Sinquefield tournament winning seven straight games including a game against the world champion Magnus Carlsen. Caruana and U.S. Chess Champion Hikaru Nakamura live in St. Louis.
Parks.
The city operates more than 100 parks, with amenities that include sports facilities, playgrounds, concert areas, picnic areas, and lakes. Forest Park, located on the western edge of city, is the largest, occupying 1,400 acres of land, making it almost twice as large as Central Park in New York City. The park is home to five major institutions, including the St. Louis Art Museum, the St. Louis Zoo, the St. Louis Science Center, the Missouri History Museum, and the Muny amphitheatre. Another significant park in the city is the Jefferson National Expansion Memorial, a National Memorial located on the riverfront in downtown St. Louis. The centerpiece of the park is the tall Gateway Arch, designed by noted architect Eero Saarinen and completed on October 28, 1965. Also part of the historic park is the Old Courthouse, where the first two trials of "Dred Scott v. Sandford" were held in 1847 and 1850.
Other notable parks in the city include the Missouri Botanical Garden, Tower Grove Park, Carondelet Park and Citygarden. The Missouri Botanical Garden, a private garden and botanical research facility, is a National Historic Landmark and one of the oldest botanical gardens in the United States. The Garden features 79 acres of horticultural displays from around the world. This includes a Japanese strolling garden, Henry Shaw's original 1850 estate home and a geodesic dome called the Climatron. Immediately south of the Missouri Botanical Garden is Tower Grove Park, a gift to the City by Henry Shaw. Citygarden is an urban sculpture park located in downtown St. Louis, with art from Fernand Léger, Aristide Maillol, Julian Opie, Tom Otterness, Niki de Saint Phalle, and Mark di Suvero. The park is divided into three sections, each of which represent a different theme: river bluffs; flood plains; and urban gardens. The park also has a restaurant – Death in the Afternoon. Another downtown sculpture park is the Serra Sculpture Park, with the 1982 Richard Serra sculpture "Twain".
Government.
The city of St. Louis has a mayor-council government with legislative authority vested in the Board of Aldermen of the City of St. Louis and with executive authority in the Mayor of St. Louis and six other separately elected officials. The Board of Aldermen is made up of 28 members (one elected from each of the city's wards) plus a board president who is elected city-wide. 
The 2014 fiscal year budget topped $1 billion for the first time, a 1.9% increase over the $985.2 million budget in 2013. 238,253 registered voters lived in the city in 2012, down from 239,247 in 2010, and 257,442 in 2008.
The structure of the management and coordination of city services is:
Local and regional government.
Municipal elections in St. Louis are held in odd numbered years, with the primary elections in March and the general election in April. The mayor is elected in odd numbered years following the United States Presidential Election, as are the aldermen representing odd-numbered wards. The President of the Board of Aldermen and the aldermen from even-numbered wards are elected in the off-years. The Democratic Party has dominated St. Louis city politics for decades. The city has not had a Republican mayor since 1949 and the last time a Republican was elected to another city-wide office was in the 1970s. As of 2006, 27 of the city's 28 Aldermen are Democrats. Forty-five individuals have held the office of mayor of St. Louis, four of whom—William Carr Lane, John Fletcher Darby, John Wimer, and John How—served non-consecutive terms. The most terms served by a mayor was by Lane who served 8 full terms plus the unexpired term of Darby. The current mayor is Francis G. Slay, who took office April 17, 2001, and who won a fourth four-year term on March 5, 2013. As of April 27, 2013, he is the longest-serving mayor of St. Louis. The second-longest serving mayor was Henry Kiel, who took office April 15, 1913 and left office April 21, 1925, a total of 12 years and 9 days over three terms in office. Two others—Raymond Tucker, and Vincent C. Schoemehl—also served three terms as mayor, but served seven fewer days. The shortest serving mayor was Arthur Barret who died 11 days after taking office.
Although St. Louis separated from St. Louis County in 1876, some mechanisms have been put in place for joint funding management and funding of regional assets. The St. Louis Zoo-Museum district collects property taxes from residents of both St. Louis City and County and the funds are used to support cultural institutions including the St. Louis Zoo, St. Louis Art Museum and the Missouri Botanical Gardens. Similarly, the Metropolitan Sewer District provides sanitary and storm sewer service to the city and much of St. Louis County. The Bi-State Development Agency (now known as Metro) runs the region's MetroLink light rail system and bus system.
State and federal government.
St. Louis is split between 11 districts in the Missouri House of Representatives: all of the 76th, 77th, 78th, 79th, 80th, 81st, 82nd, and 84th, and parts of the 66th, 83rd, and 93rd, which are shared with St. Louis County. The 5th Missouri Senate district is entirely within the city, while the 4th is shared with St. Louis County.
At the federal level, St. Louis is the heart of , which also includes part of northern St. Louis County. A Republican has not represented a significant portion of St. Louis in the U.S. House since 1953.
The United States Court of Appeals for the Eighth Circuit and the United States District Court for the Eastern District of Missouri are based in the Thomas F. Eagleton United States Courthouse in downtown St. Louis. St. Louis is also home to a Federal Reserve System branch, the Federal Reserve Bank of St. Louis. The National Geospatial-Intelligence Agency (NGA) also maintains major facilities in the St. Louis area.
The Military Personnel Records Center (NPRC-MPR) located at 9700 Page Avenue in St. Louis, Missouri, USA, is a branch of the National Personnel Records Center and is the repository of over 56 million military personnel records and medical records pertaining to retired, discharged, and deceased veterans of the U.S. armed forces.
Crime.
St. Louis has the highest murder rate in the USA, with 59.23 homicides per 100,000 (2015), making it number 15 on the list of most dangerous cities in the world. St. Louis index crime rates have declined every year since 1995; since 2005, violent crime has declined by 20%, although rates of violent crime and property crime in the city remain about 2% higher than the state and 2.3% above the United States national average.
Education.
The St. Louis Public Schools (SLPS) operate more than 75 schools, attended by more than 25,000 students, including several magnet schools. SLPS operates under provisional accreditation from the state of Missouri and is under the governance of a state-appointed school board called the Special Administrative Board, although a local board continues to exist without legal authority over the district. Since 2000, charter schools have operated in the city of St. Louis using authorization from Missouri state law. These schools are sponsored by local institutions or corporations and take in students from kindergarten through high school. In addition, several private schools exist in the city, and the Archdiocese of St. Louis operates dozens of parochial schools in the city, including parochial high schools. The city also has several private high schools, including secular, Catholic and Lutheran schools.
The city is home to two national research universities, Washington University in St. Louis and Saint Louis University, as classified under the Carnegie Classification of Institutions of Higher Education. Washington University School of Medicine in St. Louis has been ranked among the top 10 medical schools in the country by US News & World Report for as long as the list has been published, and as high as second, in 2003 and 2004.
In addition to Catholic theological institutions such as Kenrick-Glennon Seminary, St. Louis is home to three Protestant seminaries: Eden Theological Seminary of the United Church of Christ, Covenant Theological Seminary of the Presbyterian Church in America, and Concordia Seminary of the St. Louis-based Lutheran Church Missouri Synod.
Media.
Greater St. Louis commands the 19th-largest media market in the United States, a position roughly unchanged for over a decade. All of the major U.S. television networks have affiliates in St. Louis, including KTVI 2 (Fox), KMOV 4 (CBS), KSDK 5 (NBC), KETC 9 (PBS), KPLR-TV 11 (CW), KDNL 30 (ABC), WRBU 46 (Ion), and WPXS 51 Daystar Television Network. Among the area's most popular radio stations are KMOX (AM sports and talk, notable as the longtime flagship station for St. Louis Cardinals broadcasts), KLOU (FM oldies), WIL-FM (FM country), WARH (FM adult hits), and KSLZ (FM Top 40 mainstream). St. Louis also supports public radio's KWMU, an NPR affiliate, and community radio's KDHX. All-sports stations, such as KFNS 590 AM "The Fan", WXOS "101.1 ESPN", and KSLG are also popular.
The "St. Louis Post-Dispatch" is the region's major daily newspaper. Others in the region include the "Suburban Journals", which serve parts of St. Louis County, while the primary alternative newspaper is the "Riverfront Times". Three weeklies serve the African-American community: the "St. Louis Argus", the "St. Louis American", and the "St. Louis Sentinel". "St. Louis Magazine", a local monthly magazine, covers topics such as local history, cuisine, and lifestyles, while the weekly "St. Louis Business Journal" provides coverage of regional business news. St. Louis was served by an online newspaper, the "St. Louis Beacon", but that publication merged with KWMU in 2013.
There have also been countless number of books
and movies been written about St. Louis. A few of the most influential and prominent are: Meet me in St. Louis (movie), American flyers (movie), "The Killing Dance" (novel), "Meet me in St. Louis"(novel), "The Runaway Soul" (novel), "The Rose of Old St. Louis (novel) and "Circus of the Damned"(novel)
Transportation.
Road, rail, ship, and air transportation modes connect the city with surrounding communities in Greater St. Louis, national transportation networks, and international locations. St. Louis also supports a public transportation network that includes bus and light rail service.
Roads and highways.
Four interstate highways connect the city to a larger regional highway system. Interstate 70, an east-west highway, runs roughly from the northwest corner of the city to downtown St. Louis. The north-south Interstate 55 enters the city at the south near the Carondelet neighborhood and runs toward the center of the city, and both Interstate 64 and Interstate 44 enter the city on the west, running parallel to the east. Two of the four interstates (Interstates 55 and 64) merge south of the Jefferson National Expansion Memorial and leave the city on the Poplar Street Bridge into Illinois, while Interstate 44 terminates at Interstate 70 at its new interchange near N Broadway and Cass Ave.
The 563-mile Avenue of the Saints links St. Louis with St. Paul, Minnesota.
Major roadways include the north-south Memorial Drive, located on the western edge of the Jefferson National Expansion Memorial and parallel to Interstate 70, the north-south streets of Grand Boulevard and Jefferson Avenue, both of which run the length of the city, and Gravois Road, which runs from the southeastern portion of the city to downtown and used to be signed as U.S. Route 66. An east-west roadway that connects the city with surrounding communities is Martin Luther King, Jr. Drive, which carries traffic from the western edge of the city to downtown.
Light rail and subway.
Light rail service consists of two lines operating on double track. They both serve all the stations in the city, and branch to different destinations beyond its limits. Both lines enter the city north of Forest Park on the western edge of the city or on the Eads Bridge in downtown St. Louis to Illinois. All of the system track is in independent right of way, with both surface level and underground subways track in the city. All stations are independent entry, while all platforms are flush-level with trains. Rail service is provided by the Bi-State Development Agency (also known as Metro), which is funded by a sales taxes levied in the city and other counties in the region. The Gateway Multimodal Transportation Center acts as the hub station in the city of St. Louis, linking the city's light rail system, local bus system, passenger rail service, and national bus service.
Airports.
Lambert St. Louis International Airport, owned and operated by the City of St. Louis, is 11 miles northwest of downtown along highway I-70 between I-170 and I-270 in St. Louis County. It is the largest and busiest airport in the state. In 2011, the airport saw 255 daily departures to about 90 domestic and international locations and a total of nearly 13 million passengers. The airport serves as a focus city for Southwest Airlines and was a former hub for Trans World Airlines and former focus-city for American Airlines and AmericanConnection. Air cargo transportation is available at Lambert International and at other nearby regional airports, including MidAmerica St. Louis Airport, Spirit of St. Louis Airport, and St. Louis Downtown Airport.
The airport has two terminals with a total of five concourses. International flights and passengers use Terminal 2, whose lower level holds the Immigration and Customs gates. Passengers can move between the terminals on complimentary buses that run continuously, or via MetroLink for a fee. It was possible to walk between the terminals until Concourse D was closed in 2008.
Port authority.
River transportation is available through the Port of St. Louis, which is 19.3 miles of riverbank on the Mississippi River that handles more than 32 million tons of freight annually. The Port is the 2nd largest inland port by trip-ton miles, and the 3rd largest by tonnage in the United States, with more than one hundred docking facilities for barge shipping and 16 public terminals on the river. The Port Authority added 2 new small fire and rescue craft in 2012 and 2013.
Railroad service.
Inter-city rail passenger train service in the city is provided by Amtrak. All Amtrak trains serving St. Louis use the Gateway Multimodal Transportation Center downtown. Amtrak trains terminating in the city include the "Lincoln Service" to Chicago and the "Missouri River Runner" to Kansas City, Missouri. St. Louis is an intermediate stop on the "Texas Eagle" route which provides long-distance passenger service between San Antonio, Texas, and Chicago.
St. Louis is the nation's third-largest freight rail hub, moving Missouri exports such as fertilizer, gravel, crushed stone, prepared foodstuffs, fats, oils, nonmetallic mineral products, grain, alcohol, tobacco products, automobiles, and automobile parts. Freight rail service in St. Louis is provided on tracks owned by Union Pacific Railroad, Norfolk Southern Railway, Foster Townsend Rail Logistics – formerly Manufacturers Railway (St. Louis), Terminal Railroad Association of St. Louis, Affton Trucking, and the BNSF Railway.
The Terminal Railroad Association of St. Louis (reporting mark: TRRA) is a switching and terminal railroad jointly owned by all the major rail carriers in St. Louis. The company operates 30 diesel-electric locomotives to move railcars around the classification yards, deliver railcars to local industries, and ready trains for departure. The TRRA processes and dispatches a significant portion of railroad traffic moving through the city and owns and operates a network of rail bridges and tunnels including the MacArthur Bridge (St. Louis) and the Merchants Bridge. This infrastructure is also used by inter-city rail and long-distance passenger trains serving St. Louis.
Primary train classification yards in St. Louis, their approximate location in the city, and the "main rail lines or major customers served":
Union Pacific Railroad:
Norfolk Southern Railway:
Foster Townsend Rail Logistics – formerly Manufacturers Railway (St. Louis)
Affton Trucking:
BNSF Railway:
Bus service.
Local bus service in the city of St. Louis is provided by the Bi-State Development Agency via MetroBus, with more than 75 routes connecting to MetroLink commuter rail transit and stops in the city and region. The city is also served by Madison County Transit, which connects downtown St. Louis to Madison County, Illinois. National bus service in the city is offered by Greyhound Lines and Amtrak Thruway Motorcoach, with a station at the Gateway Multimodal Transportation Center, and Megabus, with a stop at St. Louis Union Station.
Taxi.
Taxicab service in the city is provided by private companies regulated by the Metropolitan Taxicab Commission. Rates vary by vehicle type, size, passengers and distance, and by regulation all taxicab fares must be calculated using a taximeter and be payable in cash or credit card. Solicitation by a driver is prohibited, although a taxicab may be hailed on the street or at a stand.
Sister cities.
During the early 21st century, St. Louis has 16 sister cities.
With informal relations with Tuguegarao, Philippines.

</doc>
<doc id="27691" url="https://en.wikipedia.org/wiki?curid=27691" title="Social security">
Social security

Social security is a concept enshrined in Article 22 of the Universal Declaration of Human Rights, which states:
Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality.
In simple terms, the signatories agree that society in which a person lives should help them to develop and to make the most of all the advantages (culture, work, social welfare) which are offered to them in the country.
Social security may also refer to the action programs of government intended to promote the welfare of the population through assistance measures guaranteeing access to sufficient resources for food and shelter and to promote health and well-being for the population at large and potentially vulnerable segments such as children, the elderly, the sick and the unemployed. Services providing social security are often called social services.
Terminology in this area in the United States is somewhat different from in the rest of the English-speaking world. The general term for an action program in support of the well being of the population in the United States is "welfare program" and the general term for all such programs is simply "welfare". In American society, the term "welfare" arguably has negative connotations. The term "Social Security", in the United States, refers to a specific social insurance program for the retired and the disabled. Elsewhere the term is used in a much broader sense, referring to the economic security society offers when people are faced with certain risks. In its 1952 Social Security (Minimum Standards) Convention (nr. 102), the International Labour Organization (ILO) defined the traditional contingencies covered by social security as including:
People who cannot reach a guaranteed social minimum for other reasons may be eligible for "social assistance" (or welfare, in American English).
Modern authors often consider the ILO approach too narrow. In their view, social security is not limited to the provision of cash transfers, but also aims at security of work, health, and social participation; and new social risks (single parenthood, the reconciliation of work and family life) should be included in the list as well.
Social security may refer to:
A report published by the ILO in 2014 estimated that only 27% of the world's population has access to comprehensive social security.
History.
While several of the provisions to which the concept refers have a long history (especially in poor relief), the notion of "social security" itself is a fairly recent one. The earliest examples of use date from the 19th century. In a speech to mark the independence of Venezuela, Simón Bolívar (1819) pronounced: "El sistema de gobierno más perfecto es aquel que produce mayor suma de felicidad posible, mayor suma de "seguridad social" y mayor suma de estabilidad política" (which translates to "The most perfect system of government is that which produces the greatest amount of happiness, the greatest amount of social security and the greatest amount of political stability").
In the Roman Empire, social welfare to help the poor was enlarged by the Emperor Trajan. Trajan's program brought acclaim from many, including Pliny the Younger.
In Jewish tradition, charity (represented by tzedakah) is a matter of religious obligation rather than benevolence. Contemporary charity is regarded as a continuation of the Biblical Maaser Ani, or poor-tithe, as well as Biblical practices, such as permitting the poor to glean the corners of a field and harvest during the Shmita (Sabbatical year). Voluntary charity, along with prayer and repentance, is befriended to ameliorate the consequences of bad acts.
The Song dynasty (c.1000AD) government supported multiple forms of social assistance programs, including the establishment of retirement homes, public clinics, and pauper's graveyards
According to Robert Henry Nelson, "The medieval Roman Catholic Church operated a far-reaching and comprehensive welfare system for the poor..."
The concepts of welfare and pension were put into practice in the early Islamic law of the Caliphate as forms of "Zakat" (charity), one of the Five Pillars of Islam, since the time of the Rashidun caliph Umar in the 7th century. The taxes (including "Zakat" and "Jizya") collected in the treasury of an Islamic government were used to provide income for the needy, including the poor, elderly, orphans, widows, and the disabled. According to the Islamic jurist Al-Ghazali (Algazel, 1058–1111), the government was also expected to store up food supplies in every region in case a disaster or famine occurred. (See Bayt al-mal for further information.)
There is relatively little statistical data on transfer payments before the High Middle Ages. In the medieval period and until the Industrial Revolution, the function of welfare payments in Europe was principally achieved through private giving or charity. In those early times, there was a much broader group considered to be in poverty as compared to the 21st century.
Early welfare programs in Europe included the English Poor Law of 1601, which gave parishes the responsibility for providing poverty relief assistance to the poor. This system was substantially modified by the 19th-century Poor Law Amendment Act, which introduced the system of workhouses.
It was predominantly in the late 19th and early 20th centuries that an organized system of state welfare provision was introduced in many countries. Otto von Bismarck, Chancellor of Germany, introduced one of the first welfare systems for the working classes in 1883. In Great Britain the Liberal government of Henry Campbell-Bannerman and David Lloyd George introduced the National Insurance system in 1911, a system later expanded by Clement Attlee. The United States did not have an organized welfare system until the Great Depression, when emergency relief measures were introduced under President Franklin D. Roosevelt. Even then, Roosevelt's New Deal focused predominantly on a program of providing work and stimulating the economy through public spending on projects, rather than on cash payment.
Income maintenance.
This policy is usually applied through various programs designed to provide a population with income at times when they are unable to care for themselves. Income maintenance is based in a combination of five main types of program:
Social protection.
Social protection refers to a set of benefits available (or not available) from the state, market, civil society and households, or through a combination of these agencies, to the individual/households to reduce multi-dimensional deprivation. This multi-dimensional deprivation could be affecting less active poor persons (such as the elderly or the disabled) and active poor persons (such as the unemployed).
This broad framework makes this concept more acceptable in developing countries than the concept of social security. Social security is more applicable in the conditions, where large numbers of citizens depend on the formal economy for their livelihood. Through a defined contribution, this social security may be managed.
But, in the context of widespread informal economy, formal social security arrangements are almost absent for the vast majority of the working population. Besides, in developing countries, the state's capacity to reach the vast majority of the poor people may be limited because of its limited infrastructure and resources. In such a context, multiple agencies that could provide for social protection, including health care, is critical for policy consideration. The framework of social protection is thus holds the state responsible for providing for the poorest populations by regulating non-state agencies.
Collaborative research from the Institute of Development Studies debating Social Protection from a global perspective, suggests that advocates for social protection fall into two broad categories: "instrumentalists" and "activists". Instrumentalists argue that extreme poverty, inequality, and vulnerability is dysfunctional in the achievement of development targets (such as the MDGs). In this view, social protection is about putting in place risk management mechanisms that will compensate for incomplete or missing insurance (and other) markets, until a time that private insurance can play a more prominent role in that society. Activist arguments view the persistence of extreme poverty, inequality, and vulnerability as symptoms of social injustice and structural inequality and see social protection as a right of citizenship. Targeted welfare is a necessary step between humanitarianism and the ideal of a "guaranteed minimum income" where entitlement extends beyond cash or food transfers and is based on citizenship, not philanthropy.

</doc>
<doc id="27692" url="https://en.wikipedia.org/wiki?curid=27692" title="Steam engine">
Steam engine

A steam engine is a heat engine that performs mechanical work using steam as its working fluid.
Steam engines are external combustion engines, where the working fluid is separate from the combustion products. Non-combustion heat sources such as solar power, nuclear power or geothermal energy may be used. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In the cycle, water is heated and transforms into steam within a boiler operating at a high pressure. When expanded through pistons or turbines, mechanical work is done. The reduced-pressure steam is then condensed and pumped back into the boiler.
In general usage, the term "steam engine" can refer to either the integrated steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. Specialized devices such as steam hammers and steam pile drivers are dependent on the steam pressure supplied from a separate boiler.
Using boiling water to produce mechanical motion goes back over 2000 years, but early devices were not practical. The Spanish inventor Jerónimo de Ayanz y Beaumont obtained the first patent for a steam engine in 1606. In 1698 Thomas Savery patented a steam pump that used steam in direct contact with the water being pumped. Savery's steam pump used condensing steam to create a vacuum and draw water into a chamber, and then applied pressurized steam to further pump the water. Thomas Newcomen's "atmospheric engine" was the first commercial true steam engine using a piston, and was used in 1712 for pumping in a mine.
In 1781 James Watt patented a steam engine that produced continuous rotary motion. Watt's ten-horsepower engines enabled a wide range of manufacturing machinery to be powered. The engines could be sited anywhere that water and coal or wood fuel could be obtained. By 1883, engines that could provide 10,000 hp had become feasible. The stationary steam engine was a key component of the Industrial Revolution, allowing factories to locate where water power was unavailable. The atmospheric engines of Newcomen and Watt were large compared to the amount of power they produced, but high pressure steam engines were light enough to be applied to vehicles such as traction engines and the railway locomotives.
Reciprocating piston type steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage, and the ascendancy of steam turbines in power generation. Considering that the great majority of worldwide electric generation is produced by turbine type steam engines, the "steam age" is continuing with energy levels far beyond those of the turn of the 19th century.
History.
Early designs and modifications.
Since the early 18th century, steam power has been applied to a variety of practical uses. At first it was applied to reciprocating pumps, but from the 1780s rotative engines (those converting reciprocating motion into rotary motion) began to appear, driving factory machinery such as spinning mules and power looms. At the turn of the 19th century, steam-powered transport on both sea and land began to make its appearance, becoming more dominant as the century progressed.
Steam engines can be said to have been the moving force behind the Industrial Revolution and saw widespread commercial use driving machinery in factories, mills and mines; powering pumping stations; and propelling transport appliances such as railway locomotives, ships, steamboats and road vehicles. Their use in agriculture led to an increase in the land available for cultivation. There have at one time or another been steam-powered farm tractors, motorcycles (without much success) and even automobiles as the Stanley Steamer.
The weight of boilers and condensers generally makes the power-to-weight ratio of a steam plant lower than for internal combustion engines. For mobile applications steam has been largely superseded by internal combustion engines or electric motors. However, most electric power is generated using steam turbine plant, so that indirectly the world's industry is still dependent on steam power. Recent concerns about fuel sources and pollution have incited a renewed interest in steam both as a component of cogeneration processes and as a prime mover. This is becoming known as the Advanced Steam movement.
Early experiments.
The history of the steam engine stretches back as far as the first century AD; the first recorded rudimentary steam engine being the aeolipile described by Greek mathematician Hero of Alexandria. In the following centuries, the few steam-powered "engines" known were, like the aeolipile,from "Ten Books on Architecture" by Vitruvius (1st century BC), published 17, June, 08 accessed 2009-07-07</ref> essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for fifty steam powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.
Pumping engines.
The first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used condensing steam to create a vacuum which was used to raise water from below, then it used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They proved only to have a limited lift height and were prone to boiler explosions. It received some use in mines, pumping stations and for supplying water wheels used to power textile machinery. An attractive feature of the Savery engine was its low cost. Bento de Moura Portugal introduced an ingenious improvement of Savery's construction "to render it capable of working itself", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.
Piston steam engines.
The first commercially successful true engine, in that it could generate power and transmit it to a machine, was the atmospheric engine, invented by Thomas Newcomen around 1712. It was an improvement over Savery's steam pump, using a piston as proposed by Papin. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable "head". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel.
In 1720 Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work "Theatri Machinarum Hydraulicarum". The engine used two lead-weighted pistons providing a continuous motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four way rotary valve connected directly to a steam boiler.
The next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were "atmospheric". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was due to atmospheric pressure.
Watt proceeded to develop his engine further, modifying it to provide a rotary motion suitable for driving factory machinery. This enabled factories to be sited away from rivers, and further accelerated the pace of the Industrial Revolution.
High-pressure engines.
Watt's patent, which expired in 1800, prevented others from making high pressure and compound engines. Shortly after Watt's patent expired Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.
The Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.
Horizontal stationary engine.
Early builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.
The acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford medal the committee said that "no one invention since Watt's time has so enhanced the efficiency of the steam engine". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.
Marine engines.
Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double and triple expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.
Steam locomotives.
As the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a prototype steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.
His steam locomotive used interior bladed wheels guided by rails or tracks.
The first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.
Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive "Salamanca" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the "Locomotion" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built "The Rocket" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.
Steam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany (where the DR Class 52.80 was produced).
Steam turbines.
The final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.
Present development.
Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. of steam per kWh.
Components and accessories of steam engines.
There are two fundamental components of a steam plant: the boiler or steam generator, and the "motor unit", referred to itself as a "steam engine". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.
The widely used reciprocating engine typically consisted of a cast iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.
Engines equipped with a condenser are a separate type than those that exhaust to the atmosphere.
Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker
Heat source.
The heat required for boiling the water and supplying the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (called variously combustion chamber, firebox). In some cases the heat source is a nuclear reactor, geothermal energy, solar energy or waste heat from an internal combustion engine or industrial process. In the case of model or toy steam engines, the heat source can be an electric heating element.
Boilers.
Boilers are pressure vessels that contain water to be boiled, and some kind of mechanism for transferring the heat to the water so as to boil it.
The two most common methods of transferring heat to the water are:
Fire tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.
Once turned to steam, many boilers raise the temperature of the steam further, turning 'wet steam' into 'superheated steam'. This use of superheating avoids the steam condensing within the engine, and allows significantly greater efficiency.
Motor units.
In a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.
These "motor units" are often called 'steam engines' in their own right. They will also operate on compressed air or other gas.
Cold sink.
As with all heat engines, a considerable quantity of waste heat at relatively low temperature is produced and must be disposed of.
The simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives, as the released steam is released in the chimney so as to increase the draw on the fire, which greatly increases engine power, but is inefficient.
Sometimes the waste heat is useful itself, and in those cases very high overall efficiency can be obtained. For example, combined heat and power (CHP) systems use the waste steam for district heating.
Where CHP is not used, steam turbines in power stations use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water output from the condenser is then put back into the boiler via a pump. A dry type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Evaporative (wet) cooling towers use the rejected heat to evaporate water; this water is kept separate from the condensate, which circulates in a closed system and returns to the boiler. Such towers often have visible plumes due to the evaporated water condensing into droplets carried up by the warm air. Evaporative cooling towers need less water flow than "once-through" cooling by river or lake water; a 700 megawatt coal-fired power plant may use about 3600 cubic metres of make-up water every hour for evaporative cooling, but would need about twenty times as much if cooled by river water. 
Water pump.
The Rankine cycle and most practical steam engines have a water pump to recycle or top up the boiler water, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives.
Monitoring and control.
For safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.
Many engines, stationary and mobile, are also fitted with a governor (see below) to regulate the speed of the engine without the need for human interference (similar to cruise control in some cars).
The most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in "Types of motor units" section).
Governor.
The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt’s partner Boulton saw one at a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.
Engine configuration.
Simple engine.
In a simple engine the charge of steam works only once in a cylinder. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in a high-pressure engine its temperature drops because no heat is added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at low temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.
Compound engines.
A method to lessen the magnitude of this heating and cooling was invented in 1804 by British engineer Arthur Woolf, who patented his "Woolf high-pressure compound engine" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders and as less expansion now occurs in each cylinder less heat is lost by the steam in each. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, torque variability can be reduced. To derive equal work from lower-pressure steam requires a larger cylinder volume as this steam occupies a greater volume. Therefore, the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders.
Double expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a 3-cylinder layout where cylinder and piston diameter are about the same making the reciprocating masses easier to balance.
Two-cylinder compounds can be arranged as:
With two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other ("quartered"). When the double expansion group is duplicated, producing a 4-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine. With the 3-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases all three cranks were set at 120°.
The adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.
Multiple expansion engines.
It is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple expansion engine. Such engines use either three or four expansion stages and are known as "triple" and "quadruple expansion engines" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the 4-cylinder triple-expansion engine popular with large passenger liners (such as the Olympic class), but this was ultimately replaced by the virtually vibration-free turbine engine.
The image to the right shows an animation of a triple expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.
Land-based steam engines could exhaust much of their steam, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications where high vessel speed was not essential. It was however superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.
Types of motor units.
Reciprocating piston.
In most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the cylinder by the same port. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four "events" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a "steam chest" adjacent to the cylinder; the valves distribute the steam by opening and closing steam "ports" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.
The simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Most however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually "shortening the cutoff" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression (""kick back"").
In the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide "lap" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.
Before the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.
The above effects are further enhanced by providing "lead": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve "lead" so that admission occurs a little before the end of the exhaust stroke in order to fill the "clearance volume" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.
Uniflow (or unaflow) engine.
Uniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties.. The Quasiturbine is a uniflow rotary steam engine where steam intakes in hot areas, while exhausting in cold areas.
Turbine engines.
A steam turbine consists of one or more "rotors" (rotating discs) mounted on a drive shaft, alternating with a series of "stators" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.
Steam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.
The main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the "Turbinia"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.
Virtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the U.S.A., more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.
Oscillating cylinder steam engines.
An oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full size working engines, mainly on ships where their compactness is valued.
Rotary steam engines.
It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff is also a serious problem with many such designs.
By the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success.. The Quasiturbine is a new type of uniflow rotary steam engine.
Of the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.
Rocket type.
The aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.
In more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.
Safety.
Steam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety.
Failure modes may include:
Steam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken. This arrangement is considerably safer. 
Lead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.
Steam cycle.
The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.
The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.
The working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an "open loop" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.
The steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.
Efficiency.
The efficiency of an engine can be calculated by dividing the energy output of mechanical work that the engine produces by the energy input to the engine by the burning fuel.
The historical measure of a steam engine's energy efficiency was its "duty". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.
No heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.
The efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range the cycle can operate over is quite small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.
One of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.
In practice, a steam engine exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1-10%, but with the addition of a condenser and multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the regime of 10-20%, and very rarely slightly higher.
A modern large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.
It is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.

</doc>
<doc id="27694" url="https://en.wikipedia.org/wiki?curid=27694" title="Satan">
Satan

Satan ( "satan", meaning "enemy" or "adversary"; "shaitan", meaning; "astray", "distant", or sometimes "devil") is a figure appearing in the texts of the Abrahamic religions who brings evil and temptation, and is known as the deceiver who leads humanity astray. Some religious groups teach that he originated as an angel, or something of the like, who used to possess great piety and beauty, but fell because of hubris, seducing humanity into the ways of sin, and has power in the fallen world. In the Hebrew Bible and the New Testament, Satan is primarily an accuser and adversary, a decidedly malevolent entity, also called the devil, who possesses demonic qualities.
Although Satan is generally viewed as having negative characteristics, some groups have beliefs much unlike the main views. In Theistic Satanism, Satan is considered a positive force and deity who is either worshipped or revered. In LaVeyan Satanism, Satan is regarded as holding virtuous characteristics.
Judaism.
Hebrew Bible.
The original Hebrew term "satan" is a noun from a verb meaning primarily "to obstruct, oppose", as it is found in Numbers 22:22, 1 Samuel 29:4, Psalms 109:6. "Ha-Satan" is traditionally translated as "the accuser" or "the adversary". The definite article "ha-" (English: "the") is used to show that this is a title bestowed on a being, versus the name of a being. Thus, this being would be referred to as "the satan".
Thirteen occurrences.
"Ha-Satan" with the definite article occurs 13 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch.1–2 (10x) and Zechariah 3:1–2 (3x).
"Satan" without the definite article is used in 10 instances, of which two are translated "diabolos" in the Septuagint and "Satan" in the King James Version:
The other eight instances of "satan" without the definite article are traditionally translated (in Greek, Latin and English) as "an adversary", etc., and taken to be humans or obedient angels:
Book of Job.
At the beginning of the book, Job is a good person "who revered God and turned away from evil" (Job 1:1), and has therefore been rewarded by God. When the angels present themselves to God, Satan comes as well. God informs Satan about Job's blameless, morally upright character. Between Job 1:9–10 and 2:4–5, Satan points out that God has given Job everything that a man could want, so of course Job would be loyal to God; Satan suggests that Job's faith would collapse if all he has been given (even his health) were to be taken away from him. God therefore gives Satan permission to test Job. In the end, Job remains faithful and righteous, and there is the implication that Satan is shamed in his defeat.
Second Temple period.
Some scholars see contact with religious dualism in Babylon, and early Zoroastrianism in particular, as influencing Second Temple Judaism, and consequently early Christianity. Subsequent development of Satan as a "deceiver" has parallels with the evil spirit in Zoroastrianism, known as the Lie, who directs forces of darkness.
Septuagint.
In the Septuagint, the Hebrew "ha-Satan" in Job and Zechariah is translated by the Greek word "diabolos" (slanderer), the same word in the Greek New Testament from which the English word devil is derived. Where "satan" is used to refer to human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but transliterated in the Greek as "satan", a neologism in Greek.
Dead Sea scrolls and Pseudepigrapha.
In Enochic Judaism, the concept of Satan being an opponent of God and a chief evil figure among demons seems to have taken root in Jewish pseudepigrapha during the Second Temple period, particularly in the "apocalypses".
The Book of Enoch contains references to Satariel, thought also to be Sataniel and Satan'el (etymology dating back to Babylonian origins). The similar spellings mirror that of his angelic brethren Michael, Raphael, Uriel, and Gabriel, previous to the fall from Heaven.
The Second Book of Enoch, also called the "Slavonic Book of Enoch", contains references to a Watcher (Grigori) called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". A similar story is found in the book of 1 Enoch; however, in that book, the leader of the Grigori is called Semjâzâ.
In the Book of Wisdom, the devil is represented as the being who brought death into the world.
In the Book of Jubilees, Mastema induces God to test Abraham through the sacrifice of Isaac. He is identical to Satan in both name and nature.
Rabbinical Judaism.
In Judaism, Satan is a term used since its earliest biblical contexts to refer to a "human opponent". Occasionally, the term has been used to suggest "evil influence" opposing human beings, as in the Jewish exegesis of the Yetzer hara ("evil inclination" Genesis 6:5). Micaiah's "lying spirit" in 1 Kings 22:22 is sometimes related. Thus, Satan is personified as a character in three different places of the Tenakh, serving as an accuser (Zechariah 3:1–2), a seducer (1 Chronicles 21:1), or as a heavenly persecutor who is "among the sons of God" (Job 2:1). In any case, Satan is always subordinate to the power of God, having a role in the divine plan. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah.
In medieval Judaism, the Rabbis rejected these Enochic literary works into the Biblical canon, making every attempt to root them out. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The Yetzer hara ("evil inclination" Genesis 6:5) is a more common motif for evil in rabbinical texts. Rabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides as identifying the "Adversary" in the prologue of Job as a metaphor.
In Hasidic Judaism, the Kabbalah presents Satan as an agent of God whose function is to tempt one into sin, then turn around and accuse the sinner on high. The Chasidic Jews of the 18th century associated ha-Satan with "Baal Davar".
Christianity.
Satan is traditionally identified as the serpent who tempted Eve to eat the forbidden fruit, as he was in Judaism. Thus Satan has often been depicted as a serpent. Christian agreement with this can be found in the works of Justin Martyr, in Chapters 45 and 79 of "Dialogue with Trypho", where Justin identifies Satan and the serpent. Other early church fathers to mention this identification include Theophilus and Tertullian.
From the fourth century, Lucifer is sometimes used in Christian theology to refer to Satan, as a result of identifying the fallen "son of the dawn" of with the "accuser" of other passages in the Old Testament.
For most Christians, Satan is believed to be an angel who rebelled against God. In the New Testament he is called "the ruler of the demons" (), "the ruler of the world", and "the god of this world" (). The Book of Revelation describes how Satan was cast out of Heaven, having "great anger" and waging war against "those who obey God's commandments". Ultimately, Satan will be thrown into the lake of fire.
The early Christian church encountered opposition from pagans such as Celsus, who claimed that "it is blasphemy...to say that the greatest God...has an adversary who constrains his capacity to do good" and said that Christians "impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God".
Terminology.
In Christianity, there are many synonyms for Satan. The most common English synonym for "Satan" is "devil", which descends from Middle English "devel," from Old English "dēofol," that in turn represents an early Germanic borrowing of Latin "diabolus" (also the source of "diabolical"). This in turn was borrowed from Greek "diabolos" "slanderer", from "diaballein" "to slander": "dia-" "across, through" + "ballein" "to hurl". In the New Testament, "Satan" occurs more than 30 times in passages alongside "diabolos", slanderer, referring to the same person or thing as Satan.
Beelzebub, meaning "Lord of Flies", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably "Ba'al Zabul", meaning "Baal the Prince". This pun was later used to refer to Satan as well.
The Book of Revelation twice refers to "the dragon, that ancient serpent, who is called the devil and Satan" (12:9, 20:2). The Book of Revelation also refers to "the deceiver", from which is derived the common epithet "the great deceiver".
Islam.
"Shaitan" (شيطان) is the equivalent of Satan in Islam. While "Shaitan" (شيطان, from the root šṭn شط⁬ن) is an adjective (meaning "astray" or "distant", sometimes translated as "devil") that can be applied to both man ("al-ins", الإنس) and Jinn, Iblis () is the personal name of the Devil who is mentioned in the Qur'anic account of Genesis. According to the Qur'an, Iblis (the Arabic name used) disobeyed an order from Allah to bow to Adam, and as a result Iblis was forced out of heaven. However, he was given respite from further punishment until the day of judgment.
Etymologically, Iblis means "the desperate (of God's mercy)" in Arabic. Thus, the name "Iblis" can be seen as a sobriquet given to Satan after falling from Grace. Some Muslim scholars hypothesized that Satan's real name before his fall was Azazel. According to the Book of Enoch and the Talmud, Azazel was the chief of the fallen angels who disobeyed God by committing adultery with women and teaching mankind magic and weaponry, leading to the biblical flood of Noah.
According to the Qur'an, God created Satan, along with all of the other jinn, out of "smokeless fire". The primary characteristic of the Devil, besides hubris, is that he has no power other than the power to cast evil suggestions into the hearts of men and women. The Quran says that Satan was among the angels whom God ordered to bow down to Adam after his creation, it says in : And when We said to the angels, "Prostrate to Adam," and they prostrated, except for Iblees. He was of the jinn and departed from the command of his Lord. Then will you take him and his descendants as allies other than Me while they are enemies to you? Wretched it is for the wrongdoers as an exchange. Whether Satan was actually an angel or a Jinn whom God elevated to the angelic assembly is a matter of debate among Muslim scholars. Some scholars, such as Ibn Abbas, believe that Satan was actually an angel whom God created out of fire. He was the most worshipful and knowledgeable of angels. Thus, when the Quran identifies Satan as a Jinn, it means that he belonged to a class of fiery creatures called Jinn, which encompasses both heavenly Jinn (fiery angels) and earthly (ordinary) Jinn. Such a notion is evocative of the biblical seraphim, a rank of angels looking like burning fire. Long before Adam was created, traditions narrate, earthly jinn roamed the earth and spread corruption upon it. God sent an army of angels under the leadership of Satan to fight them. After his victory, Satan's ego conflated; he thought he was better than any other creature, and thus God's favorite. God's creation of Man and his order to the angels to venerate him was a blow to Satan's pride. While all the angels obeyed God and bowed down to Adam, Satan disobeyed haughtily saying : "I am better than him. You created me from fire and created him from clay." Consequently, God expelled Satan from Heaven, with the latter promising to lure mankind into disbelief and evil as an act of revenge from their father, Adam. Also, some scholars call Satan "The Peacock of Angels", referencing his foolish hubris.
On the other hand, other scholars believe that there are no such things as heavenly Jinn or fiery angels, and thus Satan was not an angel. He was a Jinn whom God elevated to Heaven as a reward for his worship and righteousness. This explains why Satan managed to refuse God's order, as angels do not have free will; they obey God's orders without questioning or complaining. As for the angels, they prostrated before Adam to show their homage and obedience to God. However, Satan, adamant in his view that man is inferior, and unlike angels was given the ability to choose, made a choice of not obeying God. This caused him to be expelled by God, a fact that Satan blamed on humanity. Hasan of Basra, an eminent Muslim theologian who lived in the 7th century A.D, was quoted as saying: "Iblis was not an angel even for the time of an eye wink. He is the origin of Jinn as Adam is of Mankind."
It was after Satan's disobedience of God that the title of "Shaitan" was given to him, which can be roughly translated as "Enemy", "Rebel", "Evil", or "Devil". Shaitan then claims that, if the punishment for his act of disobedience is to be delayed until the Day of Judgment, then he will divert many of Adam's own descendants from the straight path during his period of respite. God accepts the claims of Iblis and guarantees recompense to Iblis and his followers in the form of Hellfire. In order to test mankind and jinn alike, Allah allowed Iblis to roam the earth to attempt to convert others away from his path. He was sent to earth along with Adam and Eve, after eventually luring them into eating the fruit from the forbidden tree.
Sufi view of Satan.
Sufism teaches that people should love God without expecting anything in return. Consequently, unrequited love is regarded by Sufis as that perfect type of love because the pining lover expects nothing in return. Thus, some Sufis see Satan as the paradigm of love and the perfect lover. Despite the traditional interpretation of Satan's fall from Grace as an act of excessive pride and rebellion against God, some Sufis see it as an act of self-sacrifice for God's love. Satan refused to bow down to Adam out of his uncompromising monotheism and devotion; he refused to venerate anything or anyone but God. Al-Ghazali, a well-known medieval Sufi Muslim theologian, narrates: Encountering Eblis on the slopes of Sinai, Moses hailed him and asked, “O Eblis, why did you not prostrate before Adam?” Eblis replied, “Heaven forbid that anyone worship anything but the One. […] This command was a test.” Satan believed that God ordered him to bow down to Adam to test his love for him. Satan should maintain his love for God at any cost. So, even if the cost of Satan's refusal to prostrate before Adam is falling from Grace, he should proceed with it out of his unconditional love for God. Abdul Karim Jili, a Muslim Sufi saint, believes that after the Day of Judgement, Hell will cease to exist, and Satan will be back to the service of God as one of his cherished angels.
Yazidism.
An alternative name for the main deity in the tentatively Indo-European pantheon of the Yazidis, Melek Taus, is Shaitan. However, rather than being Satanic, Yazidism can be understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. Conversely the similarity and parallels that Melek Taus has with the entity known as Satan that Christians and Muslims believe in is well noted by scholars and researchers. The connection with Satan, originally made by Muslims, attracted the interest of 19th-century European travelers and esoteric writers.
Bahá'í Faith.
In the Bahá'í Faith, "Satan" is not regarded as an independent evil power as he is in some faiths, but signifies the "lower nature" of humans. `Abdu'l-Bahá explains: "This lower nature in man is symbolized as Satan — the evil ego within us, not an evil personality outside." All other evil spirits described in various faith traditions—such as fallen angels, demons, and jinns—are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God.
Satanism.
Within Satanism, two major trends exists, theistic Satanism and atheistic Satanism, both having different views regarding the essence of Satan.
Theistic Satanism.
Theistic Satanism, commonly referred to as "devil worship", holds that Satan is an actual deity or force to revere or worship that individuals may contact and supplicate to, and represents loosely affiliated or independent groups and cabals which hold the belief that Satan is a real entity rather than an archetype.
Among non-Satanists, much modern Satanic folklore does not originate with the beliefs or practices of theistic or atheistic Satanists, but a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the Satanic ritual abuse scare of the 1980s — beginning with the memoir "Michelle Remembers" — which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.
Atheistic Satanism.
Atheistic Satanism, most commonly referred to as LaVeyan Satanism, holds that Satan does not exist as a literal anthropomorphic entity, but rather as a symbol of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. In this religion, "Satan" is not viewed or depicted as a hubristic, irrational, and fraudulent creature, but is rather seen as being Prometheus-like in terms of attributes, symbolizing liberty and the like.To adherents, he also serves as a conceptual framework and an external metaphorical projection of Satanists highest personal potential. In his essay ""Satanism: The Feared Religion"", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will".
LaVeyan Satanists embrace the original etymological meaning of the word "Satan" (Hebrew: שָּׂטָן "satan", meaning "adversary"). According to Peter H. Gilmore, "The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being"

</doc>
<doc id="27695" url="https://en.wikipedia.org/wiki?curid=27695" title="Structured programming">
Structured programming

Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of subroutines, block structures, for and while loops—in contrast to using simple tests and jumps such as the "goto" statement which could lead to "spaghetti code" which is difficult both to follow and to maintain.
It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages, with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966, and the publication of the influential "Go To Statement Considered Harmful" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term "structured programming".
Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.
Elements.
Control structures.
Following the structured program theorem, all programs are seen as composed of control structures:
Subroutines.
Subroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.
Blocks.
Blocks are used to enable groups of statements to be treated as if they were one statement. "Block-structured" languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by codice_6 as in ALGOL 68, or a code section bracketed by codice_7, as in PL/I, whitespace indentation as in Python - or the curly braces codice_8 of C and many later languages.
Structured programming languages.
It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada – but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
"Structured programming" (sometimes known as modular programming) is a subset of imperative programming that enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.
History.
Theoretical foundation.
The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself. The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.
Debate.
P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:
Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees) with abolishing the GOTO statement. In his 1974 paper, "Structured Programming with Goto Statements", he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.
Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for the "New York Times" research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.
As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled ""GOTO considered harmful" considered harmful". Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.
Outcome.
By the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.
Common deviations.
While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.
Early exit.
The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a codice_9 statement. At the level of loops, this is a codice_10 statement (terminate the loop) or codice_11 statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or test, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.
Multiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered "exceptional" circumstances that prevent it from continuing, hence needing exception handling.
The most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not unallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal don't have this problem.
Most modern languages provide language-level support to prevent such leaks; see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a codice_12. This is most often known as codice_13 and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.
Kent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that "one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors). Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.
In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are a bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as "escape sequencers", defined as "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.
In contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like codice_10 and codice_11 "are just the old codice_12 in sheep's clothing" and strongly advised against their use.
Exception handling.
Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and propose that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as codice_17 Bonang proposes that all single-exit conforming C++ should be written along the lines of:
Peter Ritchie also notes that, in principle, even a single codice_18 right before the codice_9 in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.
David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic 
overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but which a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.
The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like codice_2 loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred." Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like codice_4, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if codice_22 throws an exception in codice_23, then the usual exit point after check() is not reached. Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".
The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like codice_24, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from codice_10 to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.
Multiple entry.
More rarely, subprograms allow multiple "entry." This is most commonly only "re"-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.
It is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.
State machines.
Some programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers (including Knuth) implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.
However, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.

</doc>
<doc id="27696" url="https://en.wikipedia.org/wiki?curid=27696" title="Semiconductor device fabrication">
Semiconductor device fabrication

Semiconductor device fabrication is the process used to create the integrated circuits that are present in everyday electrical and electronic devices. It is a multiple-step sequence of photo lithographic and chemical processing steps during which electronic circuits are gradually created on a wafer made of pure semiconducting material. Silicon is almost always used, but various compound semiconductors are used for specialized applications.
The entire manufacturing process, from start to packaged chips ready for shipment, takes six to eight weeks and is performed in highly specialized facilities referred to as fabs.
History.
When feature widths were far greater than about 10 micrometres, purity was not the issue that it is today in device manufacturing. As devices became more integrated, cleanrooms became even cleaner. Today, the fabs are pressurized with filtered air to remove even the smallest particles, which could come to rest on the wafers and contribute to defects. The workers in a semiconductor fabrication facility are required to wear cleanroom suits to protect the devices from human contamination.
Semiconductor device manufacturing has spread from Texas and California in the 1960s to the rest of the world, including Europe, the Middle East, and Asia. It is a global business today. The leading semiconductor manufacturers typically have facilities all over the world. Intel, the world's largest manufacturer, has facilities in Europe and Asia as well as the U.S. Other top manufacturers include Taiwan Semiconductor Manufacturing Company (Taiwan), United Microelectronics Corporation (Taiwan),
STMicroelectronics (Europe), Analog Devices (US), Integrated Device Technology (US), Atmel (US/Europe), Freescale Semiconductor (US), Samsung (Korea), Texas Instruments (US), IBM (US), GlobalFoundries (Germany, Singapore, US), Toshiba (Japan), NEC Electronics (Japan), Infineon (Europe, US, Asia), Renesas (Japan), Fujitsu (Japan/US), NXP Semiconductors (Europe, Asia and US), Micron Technology (US), Hynix (Korea), and SMIC (China).
Wafers.
A typical wafer is made out of extremely pure silicon that is grown into mono-crystalline cylindrical ingots (boules) up to 300 mm (slightly less than 12 inches) in diameter using the Czochralski process. These ingots are then sliced into wafers about 0.75 mm thick and polished to obtain a very regular and flat surface.
Processing.
In semiconductor device fabrication, the various processing steps fall into four general categories: deposition, removal, patterning, and modification of electrical properties.
Modern chips have up to eleven metal levels produced in over 300 sequenced processing steps.
Front-end-of-line (FEOL) processing.
FEOL processing refers to the formation of the transistors directly in the silicon. The raw wafer is engineered by the growth of an ultrapure, virtually defect-free silicon layer through epitaxy. In the most advanced logic devices, "prior" to the silicon epitaxy step, tricks are performed to improve the performance of the transistors to be built. One method involves introducing a "straining step" wherein a silicon variant such as silicon-germanium (SiGe) is deposited. Once the epitaxial silicon is deposited, the crystal lattice becomes stretched somewhat, resulting in improved electronic mobility. Another method, called "silicon on insulator" technology involves the insertion of an insulating layer between the raw silicon wafer and the thin layer of subsequent silicon epitaxy. This method results in the creation of transistors with reduced parasitic effects.
Gate oxide and implants.
Front-end surface engineering is followed by growth of the gate dielectric (traditionally silicon dioxide), patterning of the gate, patterning of the source and drain regions, and subsequent implantation or diffusion of dopants to obtain the desired complementary electrical properties. In dynamic random-access memory (DRAM) devices, storage capacitors are also fabricated at this time, typically stacked above the access transistor (the now defunct DRAM manufacturer Qimonda implemented these capacitors with trenches etched deep into the silicon surface).
Back-end-of-line (BEOL) processing.
Metal layers.
Once the various semiconductor devices have been created, they must be interconnected to form the desired electrical circuits. This occurs in a series of wafer processing steps collectively referred to as BEOL (not to be confused with "back end" of chip fabrication, which refers to the packaging and testing stages). BEOL processing involves creating metal interconnecting wires that are isolated by dielectric layers. The insulating material has traditionally been a form of SiO2 or a silicate glass, but recently new low dielectric constant materials are being used (such as silicon oxycarbide), typically providing dielectric constants around 2.7 (compared to 3.9 for SiO2), although materials with constants as low as 2.2 are being offered to chipmakers.
Interconnect.
Historically, the metal wires have been composed of aluminum. In this approach to wiring (often called "subtractive aluminum"), blanket films of aluminum are deposited first, patterned, and then etched, leaving isolated wires. Dielectric material is then deposited over the exposed wires. The various metal layers are interconnected by etching holes (called ""vias")" in the insulating material and then depositing tungsten in them with a CVD technique; this approach is still used in the fabrication of many memory chips such as dynamic random-access memory (DRAM), because the number of interconnect levels is small (currently no more than four).
More recently, as the number of interconnect levels for logic has substantially increased due to the large number of transistors that are now interconnected in a modern microprocessor, the timing delay in the wiring has become so significant as to prompt a change in wiring material (from aluminum to copper layer) and a change in dielectric material (from silicon dioxides to newer low-K insulators). This performance enhancement also comes at a reduced cost via damascene processing, which eliminates processing steps. As the number of interconnect levels increases, planarization of the previous layers is required to ensure a flat surface prior to subsequent lithography. Without it, the levels would become increasingly crooked, extending outside the depth of focus of available lithography, and thus interfering with the ability to pattern. CMP (chemical-mechanical planarization) is the primary processing method to achieve such planarization, although dry "etch back" is still sometimes employed when the number of interconnect levels is no more than three.
Wafer test.
The highly serialized nature of wafer processing has increased the demand for metrology in between the various processing steps. For example, thin film metrology based on ellipsometry or reflectometry is used to tightly control the thickness of gate oxide, as well as the thickness, refractive index and extinction coefficient of photoresist and other coatings. Wafer test metrology equipment is used to verify that the wafers haven't been damaged by previous processing steps up until testing; if too many dies on one wafer have failed, the entire wafer is scrapped to avoid the costs of further processing. Virtual metrology has been used to predict wafer properties based on statistical methods without performing the physical measurement itself.
Device test.
Once the front-end process has been completed, the semiconductor devices are subjected to a variety of electrical tests to determine if they function properly. The proportion of devices on the wafer found to perform properly is referred to as the yield. Manufacturers are typically secretive about their yields, but it can be as low as 30%. Process variation is one among many reasons for low yield. 
The fab tests the chips on the wafer with an electronic tester that presses tiny probes against the chip. The machine marks each bad chip with a drop of dye. Currently, electronic dye marking is possible if wafer test data is logged into a central computer database and chips are "binned" (i.e. sorted into virtual bins) according to the predetermined test limits. The resulting binning data can be graphed, or logged, on a wafer map to trace manufacturing defects and mark bad chips. This map can also be used during wafer assembly and packaging.
Chips are also tested again after packaging, as the bond wires may be missing, or analog performance may be altered by the package. This is referred to as the "final test".
Usually, the fab charges for testing time, with prices in the order of cents per second. Testing times vary from a few milliseconds to a couple of seconds, and the test software is optimized for reduced testing time. Multiple chip (multi-site) testing is also possible, because many testers have the resources to perform most or all of the tests in parallel.
Chips are often designed with "testability features" such as scan chains or a "built-in self-test" to speed testing, and reduce testing costs. In certain designs that use specialized analog fab processes, wafers are also laser-trimmed during the testing, in order to achieve tightly-distributed resistance values as specified by the design.
Good designs try to test and statistically manage "corners" (extremes of silicon behavior caused by a high operating temperature combined with the extremes of fab processing steps). Most designs cope with at least 64 corners.
Die preparation.
Once tested, a wafer is typically reduced in thickness before the wafer is scored and then broken into individual dice, a process known as wafer dicing. Only the good, unmarked chips are packaged.
Packaging.
Plastic or ceramic packaging involves mounting the die, connecting the die pads to the pins on the package, and sealing the die. Tiny wires are used to connect the pads to the pins. In the old days, wires were attached by hand, but now specialized machines perform the task. Traditionally, these wires have been composed of gold, leading to a lead frame (pronounced "leed frame") of solder-plated copper; lead is poisonous, so lead-free "lead frames" are now mandated by RoHS.
Chip scale package (CSP) is another packaging technology. A plastic dual in-line package, like most packages, is many times larger than the actual die hidden inside, whereas CSP chips are nearly the size of the die; a CSP can be constructed for each die "before" the wafer is diced.
The packaged chips are retested to ensure that they were not damaged during packaging and that the die-to-pin interconnect operation was performed correctly. A laser then etches the chip's name and numbers on the package.
List of steps.
This is a list of processing techniques that are employed numerous times throughout the construction of a modern electronic device; this list does not necessarily imply a specific order.
Hazardous materials.
Many toxic materials are used in the fabrication process. These include:
It is vital that workers should not be directly exposed to these dangerous substances. The high degree of automation common in the IC fabrication industry helps to reduce the risks of exposure. Most fabrication facilities employ exhaust management systems, such as wet scrubbers, combustors, heated absorber cartridges, etc., to control the risk to workers and to the environment.

</doc>
