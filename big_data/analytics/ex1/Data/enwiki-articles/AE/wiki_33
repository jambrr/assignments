<doc id="26463" url="https://en.wikipedia.org/wiki?curid=26463" title="Rigel">
Rigel

Rigel, also known by its Bayer designation Beta Orionis (β Ori, β Orionis), is the brightest star in the constellation Orion and the seventh brightest star in the night sky, with visual magnitude 0.13. The star as seen from Earth is actually a triple or quadruple star system, with the primary star (Rigel A) a blue-white supergiant that is estimated to be anywhere from 120,000 to 279,000 times as luminous as the Sun, depending on method used to calculate its properties. It has exhausted its core hydrogen and swollen out to between 79 and 115 times the Sun's radius. It pulsates quasi-periodically and is classified as an Alpha Cygni variable. A companion, Rigel B, is 500 times fainter than the supergiant Rigel A and visible only with a telescope. Rigel B is itself a spectroscopic binary system, consisting of two main sequence blue-white stars of spectral type B9V that are estimated to be respectively 3.9 and 2.9 times as massive as the Sun. Rigel B also appears to have a very close visual companion Rigel C of almost identical appearance.
Visibility.
The apparent visual magnitude of Rigel is 0.13, making it on average the seventh brightest star in the celestial sphere excluding the Sun—just fainter than Capella. It is an irregular pulsating variable with a visual range of magnitude 0.05–0.18. Although Rigel has the Bayer designation "beta", it is almost always brighter than Alpha Orionis (Betelgeuse). Since 1943, the spectrum of this star has served as one of the stable anchor points by which other stars are classified. Rigel is the third most inherently luminous first magnitude star after Deneb and Betelgeuse. Rigel has a color index (B–V) of −0.03, meaning it appears white or slightly blue-white.
Culminating at midnight on 12 December, and at 9 pm on 24 January, Rigel is most visible in winter evenings in the northern hemisphere and summer in the southern. In the southern hemisphere, Rigel is the first bright star of Orion visible as the constellation rises.
In stellar navigation, Rigel is one of the most important navigation stars, since it is bright, easily located and equatorial, which means it is visible all around the world's oceans (the exception, areas within 8° of the North Pole).
Parallax.
The new Hipparcos reduction of Rigel's parallax gives a distance of , with a margin of error of about 9%. Earlier spectroscopic estimates placed its distance between .
System.
Rigel has been known as a visual double star since at least 1822, when it was measured by F. G. W. Struve. Although the companion is not particularly faint at magnitude 6.7, with the separation of 9.5 arcsec in position angle 202° (2014), both components are resolvable in most amateur astronomer's telescopes. Its the large difference in brightness makes it a challenging target for telescope apertures smaller than . At Rigel's estimated distance, Rigel B's projected separation from its primary by over 2200 AU; Since discovery, there has been no sign of orbital movement, though both stars share similar common proper motion. The pair would have an minimum orbital period of around 18,000 years.
At various times since the 19th century, Rigel B has been reported to be resolved into a close binary of two equal components, with the measured separation varying from less than 0.1" to nearly 0.2". Speckle interferometry showed in 2009 two almost identical components separated by 0.124". Both stars would have apparent visual magnitudes of 7.6 with a likely orbital period of 63 years.
Rigel B is itself a spectroscopic binary system, consisting of two main sequence stars that orbit each other every 9.86 days. The stars both belong to the spectral class B9. These two stars do not appear to make up the visual binary components B and C, so the system might be a triple star, although the true arrangement of them is unclear.
A 15.4 magnitude star at 44.6 arcsec in north position angle of 1° is catalogued as component D in the system although it is unclear whether it is physically related or a coincidental alignment.
Properties.
Moravveji and colleagues calculate a luminosity for Rigel A of 120,000 times that of the Sun. Its surface temperature is around 12,100 K. The interferometer-measured angular diameter of this star, after correction for limb darkening, is . At its estimated distance, this yields a size of about 79 times the radius of the Sun. Przybilla and colleagues used atmospheric modelling in 2006 to come up with a distance of . They calculated it to be around 218,000 times as luminous as the Sun, and have around 21±3 solar masses and 109±12 times its radius. The CMFGEN code is an atmosphere code used to determine the properties of massive stars from analysing their spectrum and atmosphere. Analysis of Rigel using this method yields a luminosity 279,000 times that of the Sun, a radius 115 times that of the Sun and stellar wind velocity of 671,080 miles per hour.
Rigel A is a blue supergiant that has exhausted burning the hydrogen fuel in its core and left the main sequence, expanding and brightening as it progresses across the Hertzsprung–Russell diagram. Przybilla estimated that it has lost around 3 solar masses since beginning life as a star of 24 ± 3 solar masses 7 to 9 million years ago. It will become a red supergiant and eventually end its stellar life by exploding as a type II supernova, in the process flinging out material that will serve to seed future generations of stars. It is one of the closest known potential supernova progenitors to Earth.
Rigel's variability is complex and is caused by stellar pulsations similar to those of Deneb, the prototype of the class of Alpha Cygni pulsating stars. The radial velocity variations of Rigel proves that it simultaneously oscillates in at least 19 non-radial modes with periods ranging from about 1.2 to 74 days. It is notable among blue supergiant stars in the sense that while its pulsations are powered by the nuclear reactions in a hydrogen-burning shell that is at least partially non-convective, the star also burns helium in its core. Rigel was identified as belonging to the Alpha Cygni variables in 1998 by Christoffel Waelkens and colleagues.
As it is both bright and moving through a region of nebulosity, Rigel lights up several dust clouds in its vicinity, most notably the IC 2118 (the Witch Head Nebula). Rigel is also associated with the Orion Nebula, which—while more or less along the same line of sight as the star—is almost twice as far away from Earth. Despite the difference in distance, projecting Rigel's path through space for its expected age brings it close to the nebula. As a result, Rigel is sometimes classified as an outlying member of the Orion OB1 Association although it is considerably closer than most of the members. Betelgeuze and Saiph lie at a similar distance to Rigel, although Betelgeuze is a runaway star with a complex history and is likely to have originally formed in the main body of the association. It has been listed as a member of the poorly-defined Taurus-Orion R1 Association
The companions Rigel Ba, Bb, and C all appear to be similar B class main sequence stars of , but their properties are not accurately known.
Space photometry.
Rigel was observed with the Canadian MOST satellite for nearly 28 days in 2009. The light variations in this supergiant star were at the milli magnitude level. The gradual changes in the flux highlights the presence of long-period pulsation modes in the star.
Spectroscopy.
The general spectral type of Rigel as B8 is well-established and it has been used as a defining point of the spectral classification sequence for supergiants. However the details of the spectrum vary considerably owing to periodic atmospheric eruptions. The spectral lines show emission, absorption, line doubling, P Cygni profiles, and inverse P Cygni profiles, with no obvious periodicity. This has resulted in classification as B8 Iab, B8 Iae, or blendings by different authors.
Names.
The modern name "Rigel" is first recorded in the Alfonsine Tables of 1252.
It is derived from the Arabic name "", "the left leg (foot) of Jauzah" (i.e. "rijl" meaning "leg, foot").
The Arabic name can be traced to the 10th century. "Jauzah" was a proper name of the Orion figure, an alternative Arabic name was "", "the foot of the great one", which is the source of the rarely used variant names "Algebar" or "Elgebar". The "Alphonsine Tables" saw its name split into "Rigel" and "Algebar", with the note, "et dicitur Algebar. Nominatur etiam Rigel."
Alternate spellings from the 17th century include "Regel" by Giovanni Battista Riccioli, "Riglon" by Wilhelm Schickard, and "Rigel Algeuze" or "Algibbar" by Edmund Chilmead.
S29-Aa17-V28-D61-N14:N35Seba-en-Sah "", which means "toe star" or "foot star".
Rigel is presumably the star known as "Aurvandil's toe" in Norse mythology.
In Chinese astronomy, Rigel is the seventh star of the "Three Stars" asterism, ("").
In Japan, the Minamoto or Genji clan had chosen Rigel and its white color as its symbol, calling the star "Genji-boshi" (), while the Taira or Heike clan adopted Betelgeuse and its red color. The two powerful families fought a legendary war in Japanese history, the stars seen as facing each other off and only kept apart by the Belt. Rigel was also known as "Gin-waki", (), "the Silver (Star) beside ("Mitsu-boshi")."
Rigel was known as "Yerrerdet-kurrk" to the Wotjobaluk koori of southeastern Australia, and held to be the mother-in-law of "Totyerguil" (Altair). The distance between them signified the taboo preventing a man from approaching his mother-in-law.
The indigenous Boorong people of northwestern Victoria named Rigel as "Collowgullouric Warepil".
The Wardaman people of northern Australia know Rigel as the Red Kangaroo Leader "Unumburrgu" and chief conductor of ceremonies in a songline when Orion is high in the sky. The river Eridanus marks a line of stars in the sky leading to it, and the other stars of Orion are his ceremonial tools and entourage. Betelgeuse is "Ya-jungin" "Owl Eyes Flicking", watching the ceremonies.
The Māori people named Rigel as "Puanga" and was said to be a daughter of "Rehua" (Antares), the chief of all stars. Its heliacal rising also presaged the appearance of "Matariki" (the Pleiades) in the dawn sky which marked the Māori New Year in late May or early June. The Moriori people of the Chatham Islands, as well as some Maori groups in New Zealand marked the start of their New Year with Rigel rather than the Pleiades. "Puaka" was a local variant used in the South Island.
The Lacandon people knew it as "tunsel" "little woodpecker".

</doc>
<doc id="26464" url="https://en.wikipedia.org/wiki?curid=26464" title="Reforms of Amānullāh Khān and civil war">
Reforms of Amānullāh Khān and civil war

Amānullāh Khān reigned in Afghanistan from 1919, achieving full independence from the British Empire shortly afterwards. Before the Treaty of Rawalpindi was concluded in 1921, Afghanistan had already begun to establish its own foreign policy, including diplomatic relations with the Russian Soviet Federative Socialist Republic in 1919. During the 1920s, Afghanistan established diplomatic relations with most major countries.
The second round of Anglo–Afghan negotiations for final peace were inconclusive. Both sides were prepared to agree on Afghan independence in foreign affairs, as provided for in the previous agreement. The two nations disagreed, however, on the issue that had plagued Anglo-Afghan relations for decades and would continue to cause friction for many more — authority over Pashtun tribes on both sides of the Durand Line. The British refused to concede Afghan control over the tribes on the British side of the line while the Afghans insisted on it. The Afghans regarded the 1921 agreement as only an informal one.
The rivalry of the great powers in the region might have remained subdued had it not been for the dramatic change in government in Moscow brought about by the Bolshevik Revolution of 1917. In their efforts to placate Muslims within their borders, the new Soviet leaders were eager to establish cordial relations with neighboring Muslim states. In the case of Afghanistan, the Soviets could achieve a dual purpose: by strengthening relations with the leadership in Kabul, they could also threaten Britain, which was one of the Western states supporting counterrevolution in the Soviet Union. In his attempts to unclench British control of Afghan foreign policy, Amanullah sent an emissary to Moscow in 1919; Vladimir Lenin received the envoy warmly and responded by sending a Soviet representative to Kabul to offer aid to Amānullāh's government.
Throughout Amānullāh's reign, Soviet-Afghan relations fluctuated according Afghanistan's value to the Soviet leadership at a given time; Afghanistan was either viewed as a tool for dealing with Soviet Muslim minorities or for threatening the British. Whereas the Soviets sought Amanullah's assistance in suppressing anti-Bolshevik elements in Central Asia in return for help against the British, the Afghans were more interested in regaining lands across the Amu Darya lost to Russia in the nineteenth century. Afghan attempts to regain the oases of Merv and Panjdeh were easily subdued by the Soviet Red Army. 
In May 1921, the Afghans and the Soviets signed a Treaty of Friendship, Afghanistan's first international agreement since gaining full independence in 1919. The Soviets provided Amanullah with aid in the form of cash, technology, and military equipment. Despite this, Amanullah grew increasingly disillusioned with the Soviets, especially as he witnessed the widening oppression of his fellow Muslims across the border.
Anglo-Afghan relations soured over British fear of an Afghan-Soviet friendship, especially with the introduction of a few Soviet planes into Afghanistan. British unease increased when Amanullah maintained contacts with Indian nationalists and gave them asylum in Kabul, and also when he sought to stir up unrest among the Pashtun tribes across the border. The British responded by refusing to address Amanullah as "Your Majesty," and imposing restrictions on the transit of goods through India.
Amānullāh's domestic reforms were no less dramatic than his foreign policy initiatives, but those reforms could not match his achievement of complete, lasting independence. Mahmud Tarzi, Amanullah's father-in-law and Foreign Minister, encouraged the monarch's interest in social and political reform but urged that it be gradually built upon the basis of a strong central government, as had occurred in Turkey under Kemal Atatürk. Socially, Amanullah enjoyed many of Mahmud Tarzi's thoughts at the time, such as giving women more rights and allowing freedom of press through publishing. Tarzi, being heavily influenced by the West, brought this influence to Afghanistan - Amanullah enjoyed Western dress and etiquette. His wife, Queen Soraya Tarzi, became the face of Amanullah Khan's reforms in regard to women.
Amānullāh's reforms touched on many areas of Afghan life. In 1921 he established an air force, albeit with only a few Soviet planes and pilots; Afghan personnel later received training in France, Italy, and Turkey. Although he came to power with army support, Amanullah alienated many army personnel by reducing both their pay and size of the forces and by altering recruiting patterns to prevent tribal leaders from controlling who joined the service. Amanullah's Turkish advisers suggested the king retire the older officers, men who were set in their ways and might resist the formation of a more professional army. Amanullah's minister of war, General Muhammad Nadir Khan, a member of the Musahiban branch of the royal family, opposed these changes, preferring instead to recognize tribal sensitivities. The king rejected Nadir Khan's advice and an anti-Turkish faction took root in the army; in 1924 Nadir Khan left the government to become ambassador to France.
If fully enacted, Amānullāh's reforms would have totally transformed Afghanistan. Most of his proposals, however, died with his abdication. His transforming social and educational reforms included: adopting the solar calendar, requiring Western dress in parts of Kabul and elsewhere, discouraging the veiling and seclusion of women, abolishing slavery and forced labor, introducing secular education (for girls as well as boys); adult education classes and educating nomads. His economic reforms included restructuring, reorganizing, and rationalizing the entire tax structure, antismuggling and anticorruption campaigns, a livestock census for taxation purposes, the first budget (in 1922), implementing the metric system (which did not take hold), establishing the Bank-i-Melli (National Bank) in 1928, and introducing the afghani as the new unit of currency in 1923.
The political and judicial reforms Amānullāh proposed were equally radical for the time and included the creation of Afghanistan's first constitution (in 1923), the guarantee of civil rights (first by decree and later constitutionally), national registration and identity cards for the citizenry, the establishment of a legislative assembly, a court system to enforce new secular penal, civil, and commercial codes, prohibition of blood money, and abolition of subsidies and privileges for tribal chiefs and the royal family.
Although sharia (Islamic law) was to be the residual source of law, it regained prominence after the Khost rebellion of 1923-24. Religious leaders, who had gained influence under Habibullah Khan, were unhappy with Amānullāh's extensive religious reforms.
Conventional wisdom holds that the tribal revolt that overthrew Amanullah grew out of opposition to his reform program, although those people most affected by his reforms were urban dwellers not universally opposed to his policies, rather than the tribes. Nevertheless, the king had managed to alienate religious leaders and army members.
The unraveling began, however, when Shinwari Pashtun tribesmen revolted in Jalalabad in November 1928. When tribal forces advanced on the capital, many of the king's troops deserted. Amanullah faced another threat as well: in addition to the Pashtun tribes, forces led by a Tajik militia were moving toward Kabul from the north. In January 1929, Amanullah abdicated the throne to his oldest brother, Inayatullah Khan, who ruled for only three days before escaping into exile in British-India. Amanullah's efforts to recover power by leading a small, ill-equipped force toward Kabul failed. The deposed king crossed the border into British-India and went into exile in Italy and remained in Europe until his 1960 death in Zürich, Switzerland.
Reign of Habibullah Kalakani, January to October 1929.
The man who seized Kabul from Amānullāh Khān was Habibullah Kalakani. He was an ethnic Tajik and native of Kalakan, a village thirty kilometers north of Kabul. Kalakani's attack on Kabul was shrewdly timed to follow the Shinwari rebellion and the defection of much of the army.
The powerful Pashtun tribes, including the Ghilzai, who had initially supported him against Amanullah, chafed under rule by a non-Pashtun. When Amanullah's last feeble attempt to regain his throne failed, those next in line were the Musahiban brothers. They were also from the Mohammedzai and Barakzai family trees, and whose great-grandfather was an older brother of Dost Mohammad.
The five prominent Musahiban brothers included Nadir Shah, the eldest, who had been Amānullāh's minister of war. They were permitted to cross through the Khyber Pakhtunkhwa to enter Afghanistan and take up arms. Once on the other side, however, they were not allowed back and forth across the border to use British-Indian territory as a sanctuary, nor were they allowed to gather together a tribal army on the British side of the Durand Line. However, the Musahiban brothers and the tribes successfully ignored these restrictions.
During this period anti-Soviet rebels from Central Asia known as Basmachi utilized the period of instability in Afghanistan to launch raids into the Soviet Union. The Basmachi had taken refuge in Afghanistan earlier in the decade after they were expelled from Soviet Central Asia by the Soviet military and they swore allegiance to the Emir of Bukhara, who lived in exile in Kabul. One of these raids was led by Faizal Maksum, who operated under the command of Basmachi commander Ibrahim Bek. Faizal Maksum's forces briefly captured the town of Gharm until they were expelled by Soviet forces. The Basmachi operated in Afghanistan due to their alliance with Habibullah Ghazi and after his fall from power they were expelled from Afghanistan.
After several unsuccessful attempts, Nadir and his brothers finally raised a sufficiently large force—mostly from the British side of the Durand Line—to take Kabul on October 10, 1929. Six days later, Nadir Khan, the eldest of the Musahiban brothers, was proclaimed King Nadir Shah. Habibullah Ghazi fled Kabul but was later captured in Kohistan, and executed on November 3, 1929. Nader also looted and plundered Kabul because the treasury was empty.

</doc>
<doc id="26469" url="https://en.wikipedia.org/wiki?curid=26469" title="Μ-recursive function">
Μ-recursive function

In mathematical logic and computer science, the μ-recursive functions are a class of partial functions from natural numbers to natural numbers that are "computable" in an intuitive sense. In fact, in computability theory it is shown that the μ-recursive functions are precisely the functions that can be computed by Turing machines. The μ-recursive functions are closely related to primitive recursive functions, and their inductive definition (below) builds upon that of the primitive recursive functions. However, not every μ-recursive function is a primitive recursive function—the most famous example is the Ackermann function.
Other equivalent classes of functions are the λ-recursive functions and the functions that can be computed by Markov algorithms.
The set of all recursive functions is known as R in computational complexity theory.
Definition.
The μ-recursive functions (or partial μ-recursive functions) are partial functions that take finite tuples of natural numbers and return a single natural number. They are the smallest class of partial functions that includes the initial functions and is closed under composition, primitive recursion, and the μ operator.
The smallest class of functions including the initial functions and closed under composition and primitive recursion (i.e. without minimisation) is the class of primitive recursive functions. While all primitive recursive functions are total, this is not true of partial recursive functions; for example, the minimisation of the successor function is undefined. The primitive recursive functions are a subset of the total recursive functions, which are a subset of the partial recursive functions. For example, the Ackermann function can be proven to be total recursive, but not primitive.
Initial or "basic" functions: (In the following the subscripting is per Kleene (1952) p. 219. For more about some of the various symbolisms found in the literature see Symbolism below.)
Operators:
The strong equality operator formula_21 can be used to compare partial μ-recursive functions. This is defined for all partial functions "f" and "g" so that
holds if and only if for any choice of arguments either both functions are defined and their values are equal or both functions are undefined.
Equivalence with other models of computability.
In the equivalence of models of computability, a parallel is drawn between Turing machines that do not terminate for certain inputs and an undefined result for that input in the corresponding partial recursive function.
The unbounded search operator is not definable by the rules of primitive recursion as those do not provide a mechanism for "infinite loops" (undefined values).
Normal form theorem.
A normal form theorem due to Kleene says that for each "k" there are primitive recursive functions formula_23 and formula_24 such that for any μ-recursive function formula_25 with "k" free variables there is an "e" such that 
The number "e" is called an index or Gödel number for the function "f". A consequence of this result is that any μ-recursive function can be defined using a single instance of the μ operator applied to a (total) primitive recursive function.
Minsky (1967) observes (as does Boolos-Burgess-Jeffrey (2002) pp. 94–95) that the U defined above is in essence the μ-recursive equivalent of the universal Turing machine:
Symbolism.
A number of different symbolisms are used in the literature. An advantage to using the symbolism is a derivation of a function by "nesting" of the operators one inside the other is easier to write in a compact form. In the following we will abbreviate the string of parameters x1, ..., xn as x: 
Example: Kleene gives an example of how to perform the recursive derivation of f(b, a) = b + a (notice reversal of variables a and b). He starts with 3 initial functions 
He arrives at:

</doc>
<doc id="26470" url="https://en.wikipedia.org/wiki?curid=26470" title="Rex Ingram (director)">
Rex Ingram (director)

Rex Ingram (15 January 1892 – 21 July 1950) was an Irish film director, producer, writer and actor. Director Erich von Stroheim once called him "the world's greatest director."
Early life.
Born Reginald Ingram Montgomery Hitchcock in Dublin, Ireland, he was educated at Saint Columba's College, near Rathfarnham, County Dublin. He spent much of his adolescence living in the Old Rectory, Kinnitty, Birr, County Offaly where his father was the Church of Ireland rector. He emigrated to the United States in 1911.
His brother Francis joined the British Army and fought during World War I where he was awarded the Military Cross and rose to the rank of Colonel. 
Career.
Ingram studied sculpture at the Yale University School of Art, where he contributed to campus humor magazine "The Yale Record". He soon moved into film, first taking acting work from 1913 and then writing, producing and directing. His first work as producer-director was in 1916 on the romantic drama "The Great Problem". He worked for Edison Studios, Fox Film Corporation, Vitagraph Studios, and then MGM, directing mainly action or supernatural films.
In 1920, he moved to Metro, where he was under supervision of executive June Mathis. Mathis and Ingram would go on to make four films together, "Hearts are Trump", "The Four Horsemen of the Apocalypse", "The Conquering Power", and "Turn to the Right". It is believed the two were romantically involved. Ingram and Mathis had begun to grow distant when her new find, Rudolph Valentino, began to overshadow his own fame. Their relationship ended when Ingram eloped with Alice Terry in 1921.
Ingram married twice, first to actress Doris Pawn in 1917; this ended in divorce in 1920. He then married Alice Terry in 1921, with whom he remained for the rest of his life. Both marriages were childless. He and Terry relocated to the French Riviera in 1923. They formed a small studio in Nice and made several films on location in North Africa, Spain, and Italy for MGM and others.
Amongst those who worked for Ingram at MGM on the Riviera during this period was the young Michael Powell, who later went on to direct (with Emeric Pressburger) "The Red Shoes" and other classics. By Powell's own account, Ingram was a major influence on him, especially in its themes in illusion, dreaming, magic and the surreal. David Lean said he was indebted to Ingram. MGM studio chief Dore Schary listed the top creative people in Hollywood as D. W. Griffith, Ingram, Cecil B. DeMille and Erich von Stroheim (in declining order of importance).
Unimpressed with sound, Rex Ingram made only one talkie, "Baroud", filmed for Gaumont British Pictures in Morocco. The film was a not a commercial success and Ingram left the film business, returning to Los Angeles to work as a sculptor and writer. Interested in Islam as early as 1927, he converted to the faith in 1933.
For his contribution to the motion picture industry he has a star on the Hollywood Walk of Fame at 1651 Vine Street.
Death.
Ingram died from a cerebral hemorrhage in North Hollywood on July 21, 1950, aged 58. He was interred in the Forest Lawn Memorial Park Cemetery in Glendale, California.
Filmography.
Ingram's complete filmography as a director:

</doc>
<doc id="26471" url="https://en.wikipedia.org/wiki?curid=26471" title="Rat">
Rat

Rats are various medium-sized, long-tailed rodents of the superfamily Muroidea. "True rats" are members of the genus "Rattus", the most important of which to humans are the black rat, "Rattus rattus", and the brown rat, "Rattus norvegicus". Many members of other rodent genera and families are also referred to as rats, and share many characteristics with true rats.
Rats are typically distinguished from mice by their size. Generally, when someone discovers a large muroid rodent, its common name includes the term "rat", while if it is smaller, the name includes the term "mouse". The muroid family is broad and complex, and the common terms "rat" and "mouse" are not taxonomically specific. Scientifically, the terms are not confined to members of the "Rattus" and "Mus" genera, for example, the pack rat and cotton mouse.
Species and description.
The best-known rat species are the black rat ("Rattus rattus") and the brown rat ("Rattus norvegicus"). The group is generally known as the Old World rats or true rats, and originated in Asia. Rats are bigger than most Old World mice, which are their relatives, but seldom weigh over in the wild.
The term "rat" is also used in the names of other small mammals which are not true rats. Examples include the North American pack rats, a number of species loosely called kangaroo rats, and others. Rats such as the bandicoot rat ("Bandicota bengalensis") are murine rodents related to true rats, but are not members of the genus "Rattus". Male rats are called bucks, unmated females are called does, pregnant or parent females are called dams, and infants are called kittens or pups. A group of rats is referred to as a mischief.
The common species are opportunistic survivors and often live with and near humans; therefore, they are known as commensals. They may cause substantial food losses, especially in developing countries. However, the widely distributed and problematic commensal species of rats are a minority in this diverse genus. Many species of rats are island endemics and some have become endangered due to habitat loss or competition with the brown, black or Polynesian rat.
Wild rodents, including rats, can carry many different zoonotic pathogens, such as "Leptospira", "Toxoplasma gondii", and "Campylobacter". The Black Death is traditionally believed to have been caused by the micro-organism "Yersinia pestis", carried by the tropical rat flea ("Xenopsylla cheopis") which preyed on black rats living in European cities during the epidemic outbreaks of the Middle Ages; these rats were used as transport hosts. Another zoonotic disease linked to the rat is the foot-and-mouth disease.
The average lifespan of any given rat depends on which species is being discussed, but many only live about a year due to predation.
The black and brown rats diverged from other Old World rats during the beginning of the Pleistocene in the forests of Asia.
As pets.
Specially bred rats have been kept as pets at least since the late 19th century. Pet rats are typically variants of the species brown rat, but black rats and giant pouched rats are also known to be kept. Pet rats behave differently from their wild counterparts depending on how many generations they have been kept as pets. Pet rats do not pose any more of a health risk than pets such as cats or dogs. Tamed rats are generally friendly and can be taught to perform selected behaviors.
As subjects for scientific research.
In 1895, Clark University in Worcester, Massachusetts (United States) established a population of domestic albino brown rats to study the effects of diet and for other physiological studies. Over the years, rats have been used in many experimental studies, which have added to our understanding of genetics, diseases, the effects of drugs, and other topics that have provided a great benefit for the health and well-being of humankind. Laboratory rats have also proved valuable in psychological studies of learning and other mental processes (Barnett, 2002), as well as to understand group behavior and overcrowding (with the work of John B. Calhoun on behavioral sink). A 2007 study found rats to possess metacognition, a mental ability previously only documented in humans and some primates.
Domestic rats differ from wild rats in many ways. They are calmer and less likely to bite; they can tolerate greater crowding; they breed earlier and produce more offspring; and their brains, livers, kidneys, adrenal glands, and hearts are smaller (Barnett 2002).
Brown rats are often used as model organisms for scientific research. Since the publication of the rat genome sequence, and other advances, such as the creation of a rat SNP chip, and the production of knockout rats, the laboratory rat has become a useful genetic tool, although not as popular as mice. When it comes to conducting tests related to intelligence, learning, and drug abuse, rats are a popular choice due to their high intelligence, ingenuity, aggressiveness, and adaptability. Their psychology, in many ways, seems to be similar to humans. Entirely new breeds or "lines" of brown rats, such as the Wistar rat, have been bred for use in laboratories. Much of the genome of "Rattus norvegicus" has been sequenced.
General intelligence.
Because of evident displays of their ability to learn, rats were investigated early to see whether they exhibit general intelligence, as expressed by the definition of a "g factor" and observed in larger, more complex animals. Early studies ca. 1930 found evidence both for and against such a "g factor" in rat. Quoting Galsworthy, with regard to the affirmative 1935 Thorndike work:
However, some more contemporary work has not supported the earlier affirmative view. Throughout the 1990s and into the 2000s, series of articles have appeared attempting to address the question of general intelligence in this species, through measurements of tasks performed by rats and mice, e.g., with statistical evaluation by factor analysis, and seeking to correlate general intelligence and brain size (as is done with humans and primates), where the general conclusion was in the affirmative.
Social intelligence.
A 2011 controlled study found that rats are actively prosocial. They demonstrate apparent altruistic behaviour to other rats in experiments, including freeing them from cages: when presented with readily available chocolate chips, test subjects would first free the caged rat, and then share the food. All female rats in the study displayed this behaviour, while 30% of the males did not.
As food.
Rat meat is a food that, while taboo in some cultures, is a dietary staple in others. Taboos include fears of disease or religious prohibition, but in many places, the high number of rats has led to their incorporation into the local diets.
In some cultures, rats are or have been limited as an acceptable form of food to a particular social or economic class. In the Mishmi culture of India, rats are essential to the traditional diet, as Mishmi women may eat no meat except fish, pork, wild birds and rats. Conversely, the Musahar community in north India has commercialised rat farming as an exotic delicacy. In the traditional cultures of the Hawaiians and the Polynesians, rat was an everyday food for commoners. When feasting, the Polynesian people of Rapa Nui could eat rat meat, but the king was not allowed to, due to the islanders' belief in his "state of sacredness" called "tapu". In studying precontact archaeological sites in Hawaii, archaeologists have found the concentration of the remains of rats associated with commoner households accounted for three times the animal remains associated with elite households. The rat bones found in all sites are fragmented, burned and covered in carbonized material, indicating the rats were eaten as food. The greater occurrence of rat remains associated with commoner households may indicate the elites of precontact Hawaii did not consume them as a matter of status or taste.
Rat stew is consumed in American cuisine in the state of West Virginia and it was also eaten in France in old Bordeaux. In France and Victorian Britain rich people ate rat pie. During food rationing due to World War II, British biologists ate laboratory rat, creamed.
Rat meat is eaten in Vietnamese cuisine.
Bandicoot rats are an important food source among some peoples in Southeast Asia, and the United Nations Food and Agriculture Organization estimated rat meat makes up half the locally produced meat consumed in Ghana, where cane rats are farmed and hunted for their meat.
African slaves in the American South were known to hunt wood rats (among other animals) to supplement their food rations, and Aborigines along the coast in southern Queensland, Australia, regularly included rats in their diet.
Ricefield rats "(Rattus argentiventer)" have traditionally been used as food in rice-producing regions such as Valencia, as immortalized by Vicente Blasco Ibáñez in his novel "Cañas y barro". Along with eel and local beans known as "garrafons", "rata de marjal" (marsh rat) is one of the main ingredients in traditional paella (later replaced by rabbit, chicken and seafood). Ricefield rats are also consumed in the Philippines, the Isaan region of Thailand, and Cambodia. In late 2008, Reuters reported the price of rat meat had quadrupled in Cambodia, creating a hardship for the poor who could no longer afford it.
Elsewhere in the world, rat meat is considered diseased and unclean, socially unacceptable, or there are strong religious proscriptions against it. Islam and Kashrut traditions prohibit it, while both the Shipibo people of Peru and Sirionó people of Bolivia have cultural taboos against the eating of rats.
Rats are a common food item for snakes, both in the wild, and as pets. Adult rat snakes and ball pythons, for example, are fed a diet of mostly rats in captivity. Rats are readily available (live or frozen) to individual snake owners, as well as to pet shops and reptile zoos, from many suppliers. In Britain, the government prohibited the feeding of any live mammal to another animal in 2007. The rule says the animal must be dead before it is given to the animal to eat. The rule was put into place mainly because of the pressure of the RSPCA and people who said the feeding of live animals was cruel.
For odor detection.
Rats have a keen sense of smell and are easy to train. These characteristics have been employed, for example, by the Belgian non-governmental organization APOPO, which trains rats (specifically African giant pouched rats) to detect landmines and diagnose tuberculosis through smell.
In the spread of disease.
Rats can serve as zoonotic vectors for certain pathogens and thus spread disease, such as bubonic plague, Lassa fever, leptospirosis, and Hantavirus infection.
As pests.
Rats have long been considered deadly pests. Once considered a modern myth, the rat flood in India has now been verified. Indeed, every fifty years, armies of bamboo rats descend upon rural areas and devour everything in their path. Rats have long been held up as the chief villain in the spread of the Bubonic Plague, however recent studies show that they alone could not account for the rapid spread of the disease through Europe in the Middle Ages. Still, the Center for Disease Control does list nearly a dozen diseases directly linked to rats. Most urban areas battle rat infestations. Rats in New York City are famous for their size and prevalence. The urban legend that the rat population in Manhattan equals that of its human population (a myth definitively refuted by Robert Sullivan in his book "Rats") speaks volumes about New Yorkers' awareness of the presence, and on occasion boldness and cleverness, of the rodents. New York has specific regulations for getting rid of rats—multi-family residences and commercial businesses must use a specially trained and licensed exterminator. Rats have the ability to swim up sewer pipes into toilets. Places to look for rat infestations are around pipes, behind walls and near garbage cans. Effective rat control requires municipal workers and individuals to work together.
As invasive species.
When introduced into locations where rats previously did not exist they can cause a huge amount of environmental degradation. "Rattus rattus", the black rat, is considered to be one of the world's worst invasive species. Also known as the ship rat, it has been carried world-wide as a stowaway on sea-going vessels for millennia and has usually accompanied men to any new area visited or settled by human beings by sea. The similar but more aggressive species "Rattus norvegicus", the brown rat or wharf rat, has also been carried worldwide by ships in recent centuries.
The ship or wharf rat has contributed to the extinction of many species of wildlife including birds, small mammals, reptiles, invertebrates, and plants, especially on islands. True rats are omnivorous and capable of eating a wide range of plant and animal foods. True rats have a very high birth rate. When introduced to a new area, they quickly reproduce to take advantage of the new food supply. In particular, they prey on the eggs and young of forest birds, which on isolated islands often have no other predators and thus have no fear of predators. Some experts believe that rats are to blame for between 40 percent and 60 percent of all seabird and reptile extinctions, with 90 percent of those occurring on islands. Thus man has indirectly caused the extinction of many species by accidentally introducing rats to new areas.
Rat-free areas.
The only rat-free continent is Antarctica, due to its hostile climate which is too severe for rat survival, and its lack of human habitation to provide buildings to shelter them from the weather. However, rats have been introduced to many of the islands near Antarctica, and because of their destructive effect on native flora and fauna, efforts to eliminate them are on-going. In particular, Bird Island (just off rat-infested South Georgia), where breeding seabirds could be badly affected if rats were introduced, is subject to special measures and regularly monitored for rat invasions.
As part of island restoration some islands' rat populations have been eradicated to protect or restore the ecology. Hawadax Island, Alaska was declared rat free after 229 years and Campbell Island, New Zealand after almost 200 years. Breaksea Island in New Zealand was declared rat free in 1988 after an eradication campaign based on a successful trial on the smaller Hawea Island nearby.
In January, 2015 an international "Rat Team" set sail from the Falkland Islands for the British Overseas Territory of South Georgia on board a ship carrying three helicopters and 100 tons of rat poison with the objective of "reclaiming the island for its seabirds". Rats have wiped out more than 90% of the seabirds on South Georgia, and the sponsors hope that once the rats are gone, it will regain its former status as home to the greatest concentration of seabirds in the world. The South Georgia Heritage Trust, which organized the mission describes it as "five times larger than any other rodent eradication attempted worldwide". That would be true if it were not for the rat control program in Alberta (see below).
The Canadian province of Alberta (population 4.1 million) is notable for being the largest inhabited area on Earth (bigger than any country in the European Union including France) which is free of true rats. It has large numbers of pack rats, also called bushy-tailed wood rats, but they are native species which are much less destructive than true rats. They are forest-dwelling vegetarians, and their worst trait is that because of their attraction for shiny objects, they tend to sneak into cabins and hotels and steal jewelry, silverware, and other valuable items.
Alberta is one of only two Canadian provinces with no sea access, and was settled relatively late in North American history. The black rat cannot survive in its climate at all, and brown rats must live near people and their structures. They cannot evade the numerous predators in natural areas or survive the winters in farm fields. It took until 1950 for invading rats to make their way to Alberta over land from Eastern Canada. Immediately upon their arrival at the eastern border with Saskatchewan, the Alberta government implemented an extremely aggressive rat control program to stop them from advancing further. A systematic detection and eradication system was used throughout a control zone about long and wide along the eastern border of the province to eliminate rat infestations before the rats could spread further into the province. Shotguns, bulldozers, high explosives, poison gas, and incendiaries were used to destroy rats. Numerous farm buildings were destroyed in the process. Initially, tons of arsenic trioxide were spread around thousands of farm yards to poison rats, but soon after the program commenced the rodenticide and medical drug warfarin was introduced, which is much safer for people (it is a commonly prescribed medicine), and more effective at killing rats than arsenic.
Forceful government control measures, strong public support and enthusiastic citizen participation continue to keep rat infestations to a minimum. The effectiveness has been aided by a similar but newer program in Saskatchewan which prevents rats from even reaching the Alberta border. The program still actively employs an armed rat patrol (in this case, not just a TV show) along Alberta's borders, about ten single rats are found and killed per year, and occasionally a large localized infestation has to be dug out with heavy machinery, but the number of rat infestations (two or more rats) found in most recent years has averaged about three, and in many years has been zero.
Taxonomy of "Rattus".
The genus "Rattus" is a member of the giant subfamily Murinae. Several other murine genera are sometimes considered part of "Rattus": "Lenothrix", "Anonymomys", "Sundamys", "Kadarsanomys", "Diplothrix", "Margaretamys", "Lenomys", "Komodomys", "Palawanomys", "Bunomys", "Nesoromys", "Stenomys", "Taeromys", "Paruromys", "Abditomys", "Tryphomys", "Limnomys", "Tarsomys", "Bullimus", "Apomys", "Millardia", "Srilankamys", "Niviventer", "Maxomys", "Leopoldamys", "Berylmys", "Mastomys", "Myomys", "Praomys", "Hylomyscus", "Heimyscus", "Stochomys", "Dephomys", and "Aethomys".
The genus "Rattus" proper contains 64 extant species. A subgeneric breakdown of the species has been proposed, but does not include all species.
In culture.
Ancient Romans did not generally differentiate between rats and mice, instead referring to the former as "mus maximus" (big mouse) and the latter as "mus minimus" (little mouse).
On the Isle of Man (a dependency of the British Crown), there is a taboo against the word "rat".
Asian cultures.
The rat (sometimes referred to as a mouse) is the first of the twelve animals of the Chinese zodiac. People born in this year are expected to possess qualities associated with rats, including creativity, intelligence, honesty, generosity, ambition, a quick temper and wastefulness. People born in a year of the rat are said to get along well with "monkeys" and "dragons", and to get along poorly with "horses".
In Indian tradition, rats are seen as the vehicle of Ganesha, and a rat's statue is always found in a temple of Ganesh. In the northwestern Indian city of Deshnoke, the rats at the Karni Mata Temple are held to be destined for reincarnation as Sadhus (Hindu holy men). The attending priests feed milk and grain to the rats, of which the pilgrims also partake.
European cultures.
European associations with the rat are generally negative. For instance, "Rats!" is used as a substitute for various vulgar interjections in the English language. These associations do not draw, "per se", from any biological or behavioral trait of the rat, but possibly from the association of rats (and fleas) with the 14th-century medieval plague called the Black Death. Rats are seen as vicious, unclean, parasitic animals that steal food and spread disease. However, some people in European cultures keep rats as pets and conversely find them to be tame, clean, intelligent, and playful.
Rats are often used in scientific experiments; animal rights activists allege the treatment of rats in this context is cruel. The term "lab rat" is used, typically in a self-effacing manner, to describe a person whose job function requires them to spend a majority of their work time engaged in bench-level research (such as postgraduate students in the sciences).
Terminology.
Rats are frequently blamed for damaging food supplies and other goods, or spreading disease. Their reputation has carried into common parlance: in the English language, "rat" is often an insult or is generally used to signify an unscrupulous character; it is also used, as the term "nark", to mean an individual who works as a police informant or who has turned state's evidence. Writer/director Preston Sturges created the humorous alias "Ratskywatsky" for a soldier who seduced, impregnated, and abandoned the heroine of his 1944 film, "The Miracle of Morgan's Creek". It is a term (noun and verb) in criminal slang for an informant - "to rat on someone" is to betray them by informing the authorities of a crime or misdeed they committed. Describing a person as "rat-like" usually implies he or she is unattractive and suspicious.
Among unions, "rat" is a term for nonunion employers or breakers of union contracts, and this is why unions use inflatable rats.
Fiction.
Depictions of rats in fiction are historically inaccurate and negative. The most common falsehood is the squeaking almost always heard in otherwise realistic portrayals (i.e. nonanthropomorphic). While the recordings may be of actual squeaking rats, the noise is uncommon - they may do so only if distressed, hurt, or annoyed. Normal vocalizations are very high-pitched, well outside the range of human hearing. Rats are also often cast in vicious and aggressive roles when in fact, their shyness helps keep them undiscovered for so long in an infested home.
The actual portrayals of rats vary from negative to positive with a majority in the negative and ambiguous. The rat plays a villain in several mouse societies; from Brian Jacques's "Redwall" and Robin Jarvis's "The Deptford Mice", to the roles of Disney's Professor Ratigan and Kate DiCamillo's Roscuro and Botticelli. They have often been used as a mechanism in horror; being the titular evil in stories like "The Rats" or H.P. Lovecraft's "The Rats in the Walls" and in films like "Willard" and "Ben". Another terrifying use of rats is as a method of torture, for instance in Room 101 in George Orwell's "Nineteen Eighty-Four" or "The Pit and the Pendulum" by Edgar Allan Poe.
Selfish helpfulness —those willing to help for a price— has also been attributed to fictional rats. Templeton, from E. B. White's "Charlotte's Web", repeatedly reminds the other characters that he is only involved because it means more food for him, and the cellar-rat of John Masefield's "The Midnight Folk" requires bribery to be of any assistance.
By contrast, the rats appearing in the Doctor Dolittle books tend to be highly positive and likeable characters, many of whom tell their remarkable life stories in the Mouse and Rat Club established by the animal-loving doctor.
Some fictional works use rats as the main characters. Notable examples include the society created by O'Brien's "Mrs. Frisby and the Rats of NIMH", and others include "Doctor Rat", and Rizzo the Rat from The Muppets. Pixar's 2007 animated film "Ratatouille" is about a rat described by Roger Ebert as "earnest... lovable, determined, gifted" who lives with a Parisian garbage-boy-turned-chef.
"Mon oncle d'Amérique" (""My American Uncle""), a 1980 French film, illustrates Henri Laborit's theories on evolutionary psychology and human behaviors by using short sequences in the storyline showing lab rat experiments.
In Harry Turtledove's science fiction novel "Homeward Bound", humans unintentionally introduce rats to the ecology at the home world of an alien race which previously invaded Earth and introduced some of its own fauna into its environment. And A. Bertram Chandler pitted his space-bound protagonist, Commodore Grimes, against giant, intelligent rats who took over several stellar systems and enslaved their human inhabitants. "The Stainless Steel Rat" is nickname of the (human) protagonist of a series of humorous science fiction novels written by Harry Harrison.
The Pied Piper.
One of the oldest and most historic stories about rats is "The Pied Piper of Hamelin", in which a rat-catcher leads away an infestation with enchanted music. The piper is later refused payment, so he in turn leads away the town's children. This tale, traced to Germany around the late 13th century, has inspired adaptations in film, theatre, literature, and even opera. The subject of much research, some theories have intertwined the tale with events related to the Black Plague, in which black rats may have played an important role. Fictional works based on the tale that focus heavily on the rat aspect include Pratchett's "The Amazing Maurice and his Educated Rodents", and Belgian graphic novel "Le Bal du Rat Mort" ("The Ball of the Dead Rat").

</doc>
<doc id="26472" url="https://en.wikipedia.org/wiki?curid=26472" title="Adobe RoboHelp">
Adobe RoboHelp

RoboHelp is a help authoring tool (HAT) created c. 1991 by Gen Kiyooka of Blue Sky Software for the Microsoft Windows operating system.
Blue Sky Software reformed as eHelp Corporation, then entered a merger agreement with Macromedia in October 2003. Macromedia was, in turn, acquired by Adobe Systems in December 2005.
Features.
RoboHelp can generate help files in the following file formats:

</doc>
<doc id="26473" url="https://en.wikipedia.org/wiki?curid=26473" title="Replicant">
Replicant

A replicant is a fictional bioengineered or biorobotic android in the film "Blade Runner" (1982), The Nexus series of replicants are virtually identical to an adult human, but have superior strength, agility, and variable intelligence depending on the model. Because of their similarity to humans, a replicant can only be detected by means of the fictional Voight-Kampff test, in which emotional responses are provoked; replicants' responses differ from humans' responses. NEXUS 6 replicants also have a safety mechanism, namely a four year lifespan, to prevent them from developing empathic cognition and therefore immunity to a Voight-Kampff machine. A derogatory term for a replicant is "skin-job."
Origin.
Philip K. Dick's novel "Do Androids Dream of Electric Sheep?", the inspiration for "Blade Runner", used the term android (or "andy"), but director Ridley Scott wanted a new term that did not have preconceptions. As David Peoples was rewriting the screenplay he consulted his daughter, who was involved in microbiology and biochemistry. She suggested the term "replicating", the biological process of a cell making a copy of itself. From that, one of them (each would later recall it was the other) came up with "replicant" and it was inserted into Hampton Fancher's screenplay.
Replicants in the film.
Replicants became illegal on Earth after a bloody off-world mutiny by Nexus 6 replicants, before the events of the film. Two weeks before the starting point of the film, six Nexus 6 replicants escaped the off-world colonies, killing 23 people and taking a shuttle to Earth; the film focuses on the pursuit of the replicants by Deckard, a special police officer called a "blade runner", who investigates, tests, and "retires" (kills) replicants found on Earth.
Nexus 6 replicants had been designed to copy humans in every way except for their emotions. The Tyrell Corporation "began to recognize in them strange obsession", and in order to be able to control them better, started to implant false memories into the replicants in order to give them the years of experiences that humans take for granted; these memories created "a cushion or pillow for their emotions". 
Early in the film, Captain Bryant tells Deckard that the Nexus 6 units' possible eventual development of emotional responses was the reason the Tyrell Corporation had made them have a four-year life. Late in the film, Dr. Eldon Tyrell states that the lifespan limitation cannot be circumvented.
Deckard had no experience with Nexus 6 replicants at the beginning of the film; he and Captain Bryant are puzzled as to why they have risked coming back to Earth and Deckard is unsure how effective the Voight-Kampff test would be on them.
Escaped replicants (all Nexus 6 models):
Other replicants:
In "Do Androids Dream of Electric Sheep?", the android manufacturer, known as the Rosen Corporation, did not know how to manufacture an android capable of living beyond four years.
The super-soldiers in "Soldier", the "spin-off sidequel"-spiritual successor to "Blade Runner", are implied to be replicants in the film, as the films are set in a shared universe.
Was Deckard a replicant?
"Blade Runner"'s dark paranoid atmosphere—and multiple versions of the film—adds fuel to the speculation and debate over this issue. In "Do Androids Dream of Electric Sheep?", Rick Deckard (the protagonist) is at one point tricked into following an android, who believes himself to be a police officer, to a fake police station. Deckard then escapes and "retires" some androids there before returning to his own police station. Deckard takes the Voight-Kampff test and it fails to indicate that he is an android.
Harrison Ford, who played Deckard in the film, has said that he did not think Deckard was a replicant, and also states he and the director had discussions that ended in the agreement that the character was human. According to several interviews with director Ridley Scott, Deckard "is" a replicant. He collects photographs which are seen on his piano, yet has no obvious family beyond a reference to his ex-wife (who called him "cold fish"). In a scene where Deckard talks with Rachael, their eyes both appear to shine, suggestive of replicants.
In the Director's Cut, police officer Gaff (played by Edward James Olmos) leaves Deckard an origami Unicorn a day after Deckard dreamed of one. Just before Deckard finds the unicorn, Gaff says to him in passing, "It's too bad she won't live...then again, who does?". A unicorn can also be seen briefly in a scene in J. F. Sebastian's home, amongst scattered toys (to the right of a sleeping Sebastian, while Pris snoops around his equipment). A unicorn also appears in a dream of Deckard's in the Director's Cut and as explained in the film, Rachael's memories are known by her creators, such as the memory Rachael has of spiders hatching. That Gaff is leaving an origami unicorn at Deckard's house may imply that Gaff knows about Deckard's unicorn dream.
Author Will Brooker has written that the dream may not be unique to Deckard and that unicorn dreams may be a "personal touch" added to some or all of the Nexus 6 replicants' "brains." Since we are not privy to the dreams of the other replicants, this is unknown. From this one could also speculate that Gaff is a replicant and may share the same embedded memory.
Paul Sammon, author of "Future Noir: The Making of Blade Runner", has suggested in interviews that Deckard may be a Nexus 7, a new replicant type who possesses no superhuman strength or intelligence but neurological features that complete the human illusion. Ridley Scott has mentioned "Nexus 7" and "Nexus 8" replicants as possibilities in a sequel to the film. Sammon also suggests that Nexus 7 replicants may not have a preset lifespan (i.e., they could be immortal).
Sammon wrote that Ridley Scott thought it would be more provocative to imply that Deckard was a replicant. This ties back into the theme of "what is it to be human?" What is important is not whether Deckard is a replicant but that the ambiguity blurs the line between humans and replicants.
When Scott was asked about the possibility of a "Blade Runner" sequel in October 2012, he said, "It's not a rumor — it's happening. With Harrison Ford? I don't know yet. Is he too old? Well, he was a Nexus 6, so we don't know how long he can live. And that's all I'm going to say at this stage."
Organic or mechanical?
Although the press kit released to the media for the film explicitly defined a replicant as, "A genetically engineered creature composed entirely of organic substance", the physical make-up of the replicants themselves is not clear. In the opening crawl of the film, replicants are said to be the result of "advanced robot evolution." The crawl also states that they were created by "genetic engineers." Characters mention that they have eyes and brains like humans, and they are seen to bleed when injured. The only way of telling a replicant from a human is to ask a series of questions and analyze emotional response, suggesting they are entirely, or almost entirely, organic.
"Do Androids Dream of Electric Sheep?" makes mention of the biological components of the androids, but also alludes to mechanical aspects commonly found in other material relating to robots. It states that the bone marrow can be tested to prove whether it is from a human or replicant.
In May 2012, Ridley Scott confirmed that the replicants were biological in nature, and contrasted them to the androids in the "Alien" series. "Roy Batty was an evolved... He wasn't an engine. If I cut him open, there wasn't metal, he was grown... and then within twenty years you get the first bill not passed in the Senate where they applied for replication of animals, sheep and goats and cattle and animals and they turned it down, but if you can do that, then you can do human beings. If you go deeper into it and say 'Yeah, but if you are going to grow a human being, does he start that big and I've got to see him through everything?' I don't want to answer the question, because of course he does... Ash in "Alien" had nothing to do with Roy Batty, because Roy Batty is more humanoid, whereas Ash was more metal".

</doc>
<doc id="26474" url="https://en.wikipedia.org/wiki?curid=26474" title="Roman Jakobson">
Roman Jakobson

Roman Osipovich Jakobson (; October 11, 1896 – July 18, 1982) was a Russian–American linguist and literary theorist.
As a pioneer of the structural analysis of language, which became the dominant trend in linguistics during the first half of the 20th century, Jakobson was among the most influential linguists of the century. Influenced by the work of Ferdinand de Saussure, Jakobson developed, with Nikolai Trubetzkoy, techniques for the analysis of sound systems in languages, inaugurating the discipline of phonology. He went on to apply the same techniques of analysis to syntax and morphology, and controversially proposed that they be extended to semantics (the study of meaning in language). He made numerous contributions to Slavic linguistics, most notably two studies of Russian case and an analysis of the categories of the Russian verb. Drawing on insights from Charles Sanders Peirce's semiotics, as well as from communication theory and cybernetics, he proposed methods for the investigation of poetry, music, the visual arts, and cinema.
Through his decisive influence on Claude Lévi-Strauss and Roland Barthes, among others, Jakobson became a pivotal figure in the adaptation of structural analysis to disciplines beyond linguistics, including anthropology and literary theory; this generalization of Saussurean methods, known as "structuralism", became a major post-war intellectual movement in Europe and the United States. Meanwhile, though the influence of structuralism declined during the 1970s, Jakobson's work has continued to receive attention in linguistic anthropology, especially through the ethnography of communication developed by Dell Hymes and the semiotics of culture developed by Jakobson's former student Michael Silverstein.
Life and work.
Jakobson was born in Russia on 11 October 1896 to a well-to-do family of Jewish descent, the industrialist Osip Jakobson and chemist Anna Volpert Jakobson, and he developed a fascination with language at a very young age. He studied at the Lazarev Institute of Oriental Languages and then at the Historical-Philological Faculty of Moscow University. As a student he was a leading figure of the Moscow Linguistic Circle and took part in Moscow's active world of avant-garde art and poetry. The linguistics of the time was overwhelmingly neogrammarian and insisted that the only scientific study of language was to study the history and development of words across time (the diachronic approach, in Saussure's terms). Jakobson, on the other hand, had come into contact with the work of Ferdinand de Saussure, and developed an approach focused on the way in which language's structure served its basic function (synchronic approach) – to communicate information between speakers. Jakobson was also well known for his critique of the emergence of sound in film. Jakobson received a master's degree from Moscow University in 1918.
In Czechoslovakia.
1920 was a year of political conflict in Russia, and Jakobson relocated to Prague as a member of the Soviet diplomatic mission to continue his doctoral studies. He immersed himself both into the academic and cultural life of pre-World War II Czechoslovakia and established close relationships with a number of Czech poets and literary figures. Jakobson received his Ph.D. from Charles University in 1930. He became a professor at Masaryk University in Brno in 1933. He also made an impression on Czech academics with his studies of Czech verse. In 1926, together with Vilém Mathesius and others he became one of the founders of the "Prague school" of linguistic theory (other members included Nikolai Trubetzkoi, René Wellek, Jan Mukařovský). There his numerous works on phonetics helped continue to develop his concerns with the structure and function of language. Jakobson's universalizing structural-functional theory of phonology, based on a markedness hierarchy of distinctive features, was the first successful solution of a plane of linguistic analysis according to the Saussurean hypotheses. (This theory achieved its most canonical exposition in a book co-authored with Morris Halle.) This mode of analysis has been since applied to the plane of Saussurean sense by his protégé Michael Silverstein in a series of foundational articles in functionalist linguistic typology.
Escapes before the war.
Jakobson escaped from Prague in early March 1939 via Berlin for Denmark, where he was associated with the Copenhagen linguistic circle, and such intellectuals as Louis Hjelmslev. He fled to Norway on 1 September 1939, and in 1940 walked across the border to Sweden, where he continued his work at the Karolinska Hospital (with works on aphasia and language competence). When Swedish colleagues feared a possible German occupation, he managed to leave on a cargo ship, together with Ernst Cassirer (the former rector of Hamburg University) to New York City in 1941 to become part of the wider community of intellectual émigrés who fled there.
Career in the United States and Later Life.
In New York, he began teaching at The New School, still closely associated with the Czech émigré community during that period. At the École libre des hautes études, a sort of Francophone university-in-exile, he met and collaborated with Claude Lévi-Strauss, who would also become a key exponent of structuralism. He also made the acquaintance of many American linguists and anthropologists, such as Franz Boas, Benjamin Whorf, and Leonard Bloomfield. When the American authorities considered "repatriating" him to Europe, it was Franz Boas who actually saved his life. After the war, he became a consultant to the International Auxiliary Language Association, which would present Interlingua in 1951.
In 1949 Jakobson moved to Harvard University, where he remained until his retirement in 1967. In his last decade he maintained an office at the Massachusetts Institute of Technology, where he was an honorary Professor Emeritus. In the early 1960s Jakobson shifted his emphasis to a more comprehensive view of language and began writing about communication sciences as a whole. He converted to Eastern Orthodox Christianity in 1975.
Jakobson died in Cambridge, Massachusetts on 18 July 1982. His widow died in 1986. His first wife, who was born in 1908, died in 2000.
The communication functions.
Influenced by the Organon-Model by Karl Bühler, Jakobson distinguishes six communication functions, each associated with a dimension or factor of the communication process – Elements from Bühler's theory appear in the diagram below in yellow and pink, Jakobson's elaborations in blue:
One of the six functions is always the dominant function in a text and usually related to the type of text. In poetry, the dominant function is the poetic function: the focus is on the message itself. The true hallmark of poetry is according to Jakobson "the projection of the principle of equivalence from the axis of selection to the axis of combination". Very broadly speaking, it implies that poetry successfully combines and integrates form and function, that poetry turns the poetry of grammar into the grammar of poetry, so to speak. A famous example of this principle is the political slogan "I like Ike." Jakobson's theory of communicative functions was first published in "Closing Statements: Linguistics and Poetics" (in Thomas A. Sebeok, "Style In Language", Cambridge Massachusetts, MIT Press, 1960, pp. 350–377). Despite its wide adoption, the six-functions model has been criticized for lacking specific interest in the "play function" of language that, according to an early review by Georges Mounin, is however "not enough studied in general by linguistics researchers".
Legacy.
Jakobson's three principal ideas in linguistics play a major role in the field to this day: linguistic typology, markedness, and linguistic universals. The three concepts are tightly intertwined: typology is the classification of languages in terms of shared grammatical features (as opposed to shared origin), markedness is (very roughly) a study of how certain forms of grammatical organization are more "natural" than others, and linguistic universals is the study of the general features of languages in the world. He also influenced Nicolas Ruwet's paradigmatic analysis.
Jakobson has also influenced Friedemann Schulz von Thun's four sides model, as well as Michael Silverstein's metapragmatics, Dell Hymes's ethnography of communication and ethnopoetics, the psychoanalysis of Jacques Lacan, and philosophy of Giorgio Agamben.

</doc>
<doc id="26475" url="https://en.wikipedia.org/wiki?curid=26475" title="Rudolph Pariser">
Rudolph Pariser

Rudolph Pariser (born December 8, 1923) is a physical and polymer chemist. He was born in Harbin, China to merchant parents. He attended the Von Hindenburg Schule in Harbin, an American Missionary School in Beijing and American School in Japan in Tokyo. He left for the United States just before World War II broke out.
He received his Bachelor of Science degree from the University of California, Berkeley in 1944, and his Ph. D. degree from the University of Minnesota in physical chemistry in 1950. From 1944 to 1946, during World War II and shortly afterward, he served in the United States Army. He became a naturalized citizen of the United States of America in 1944.
He spent most of his career as a polymer chemist working for DuPont in the Central Research Department at the Experimental Station. He rose to the level of Director of Polymer Sciences, leading it during a time of great innovation. After retiring from DuPont, he formed his own consulting company.
Pariser is best known for his work with Robert G. Parr on the method of molecular orbital computation now known (because it was independently developed by John A. Pople) as the Pariser–Parr–Pople method (PPP method), published both by Pariser and Parr and by Pople in almost simultaneous papers in 1953.
He married Margaret Louise Marsh on July 31, 1972.

</doc>
<doc id="26476" url="https://en.wikipedia.org/wiki?curid=26476" title="Rendezvous with Rama">
Rendezvous with Rama

Rendezvous with Rama is a hard science fiction novel by Arthur C. Clarke first published in 1973. Set in the 2130s, the story involves a cylindrical alien starship that enters Earth's solar system. The story is told from the point of view of a group of human explorers who intercept the ship in an attempt to unlock its mysteries. This novel won both the Hugo and Nebula awards upon its release, and is regarded as one of the cornerstones in Clarke's bibliography.
Plot summary.
After a meteorite falls in Northeast Italy in 2077, creating a major disaster, the government of Earth sets up the Spaceguard system as an early warning of arrivals from deep space.
The "Rama" of the title is an alien starship, initially mistaken for an asteroid categorised as "31/439". It is detected by astronomers in the year 2131 while it is still outside the orbit of Jupiter. Its speed (100,000 km/h) and the angle of its trajectory clearly indicate it is not on a long orbit around the sun, but comes from interstellar space. The astronomers' interest is further piqued when they realise the asteroid has an extremely rapid rotation period of 4 minutes and is exceptionally large. It is named Rama after the Hindu god, and an unmanned space probe dubbed "Sita" is launched from the Mars moon Phobos to intercept and photograph it. The resulting images reveal that Rama is a perfect cylinder, in diameter and long, and completely featureless, making this humankind's first encounter with an alien spacecraft.
The manned solar survey vessel "Endeavour" is sent to study Rama, as it is the only ship close enough to do so in the brief period Rama will spend in our solar system. "Endeavour" manages to rendezvous with Rama one month after it first comes to Earth's attention, when the alien ship is already inside Venus' orbit. The 20+ crew, led by Commander Bill Norton, enters Rama through triple airlocks, and explores the vast 16-km wide by 50-km long cylindrical world of its interior, but the nature and purpose of the starship and its creators remain enigmatic throughout the book. The astronauts discover that Rama is hollow, and that its inner surfaces hold vast "cities" of geometric structures that resemble buildings and are separated by streets with shallow trenches. A mammoth band of water, dubbed the Cylindrical Sea, stretches around Rama's central circumference. Massive cones, which the astronauts theorize are part of Rama's propulsion system, stand at its 'southern' end. They also find that Rama's atmosphere is breathable.
One of the crew members, Jimmy Pak, who has experience with low gravity skybikes, volunteers to ride a smuggled skybike along Rama's axis to the far end, otherwise inaccessible due to the cylindrical sea and the 500-m high cliff on the opposite shore. A few hours later, Jimmy reaches the massive metal cones on the southern end of Rama, and detects a strange magnetic field coming from the cones. He takes some photos of the area and the strange plateau on the southern end of Rama's landmass. As he leaves the area, the electrical charge in its atmosphere increases, resulting in lightning. A discharge hits his skybike, causing him to crash on the isolated southern continent.
When Pak wakes up, he sees a crab-like creature picking up his skybike and chopping it into pieces. He cannot decide whether it is a robot or a biological alien, and keeps his distance while radioing for help. As Pak waits, Norton sends a rescue party across the cylindrical sea, using a small, improvised craft, constructed earlier for exploration of the sea's central island. Pak sees the crablike creature dump the skybike's remains into the sea. The creature then walks toward but ultimately ignores him. Pak explores the surrounding fields while waiting for the rescue party to arrive on the southern cliffs of the cylindrical sea. Amongst the strange geometric structures, he sees an alien flower growing through a cracked tile in the otherwise sterile environment, and decides to take it as both a curiosity and for scientific research.
Pak jumps off the 500 m cliff, his descent slowed by the low gravity and using his shirt as a parachute, and is quickly rescued by the waiting boat. As they ride back, tidal waves form in the cylindrical sea, created by the movements of Rama itself as it makes course corrections. When the crew arrives at base, they see a variety of odd creatures inspecting their camp. When one is found damaged and apparently lifeless, the team's doctor/biologist Surgeon-Commander Laura Ernst inspects it, and discovers it to be a hybrid biological entity and robot--eventually termed a "biot". It, and by assumption the others, are powered by natural internal batteries (much like those of terrestrial electric eels) and possess some intelligence. They are believed to be the servants of Rama's still-absent builders.
The members of the Rama Committee and the United Planets, both based on the moon, have been monitoring events inside Rama and giving feedback. The Hermian colonists have concluded that Rama is a potential threat and send a rocket-mounted nuclear bomb to destroy it should it prove to pose a threat, but Lt. Boris Rodrigo uses a pair of wire cutters to defuse the bomb and its control.
As Rama approaches perihelion, the biots jump into the cylindrical sea, where they are destroyed by aquatic biots ('sharks') and reabsorbed into the mineral-laden water. On their final expedition, some crew members decide to visit the city christened "London" (as it is closest to Rama's "northern" end, the point of their entry), where they use a laser to cut open one of the "buildings" to see what it houses. They discover transparent pedestals containing holograms of various artefacts, which they theorize are used by the Ramans as templates for creating tools and other objects. The most amazing of these appears to be a uniform with bandoliers, straps and pockets that suggests the size and shape of the Ramans. As the crew photographs some of the holograms, the three lights around Rama's circumference start to dim, prompting the explorers to leave. They exit through Rama's northern stairway and the three airlocks, and re-board "Endeavour."
With "Endeavour" a safe distance away, Rama reaches perihelion and utilizes the Sun's gravitational field, and its mysterious "space drive", to perform a slingshot manoeuvre which flings it out of the solar system and toward an unknown destination in the direction of the Large Magellanic Cloud.
Ending.
The book was meant to stand alone, although its final sentence suggests otherwise:
Clarke denied that this sentence was a hint that the story might be continued. In his foreword to the book's sequel, he stated that it was just a good way to end the first book, and that he added it during a final revision.
Reception.
John Leonard of "The New York Times", while finding Clarke "benignly indifferent to the niceties of characterization," praised the novel for conveying "that chilling touch of the alien, the not-quite-knowable, that distinguishes sci-fi at its most technically imaginative." Other reviewers have also commented on Clarke's lack of character development and overemphasis on realism.
Awards and nominations.
The novel was awarded the following soon after publication
Design and geography of Rama.
The interior of Rama is essentially a large cylindrical landscape, dubbed 'The Central Plain' by the crew, 16 kilometres wide and 50 long, with artificial gravity provided by its 0.25 rpm spin. It is split into the 'northern' and 'southern' hemispheres, divided in the middle by a 10-km wide expanse of water the astronauts dub the 'Cylindrical Sea'. In the center of the Cylindrical Sea is an island of unknown purpose covered in tall, skyscraper-like structures, which the astronauts name 'New York' due to an imagined similarity to Manhattan. At each end of the ship are North and South "Poles". The North Pole is effectively the bow and the South Pole the stern, as Rama accelerates in the direction of the north pole and its drive system is at the South Pole.
The North Pole contains Rama's airlocks, and is where the "Endeavour" lands. The airlocks open into the hub of the massive bowl shaped cap at the North Pole, with three 8-kilometre long stair systems, called Alpha, Beta, and Gamma by the crew, leading to the plain.
The Northern hemisphere contains several small 'towns' interconnected by roads, dubbed London, Paris, Peking, Tokyo, Rome, and Moscow. The South Pole has a giant cone-shaped protrusion surrounded by six smaller ones, which are thought to be part of Rama's reactionless space drive.
Both ends of Rama are lit by giant trenches (three in the northern hemisphere and three in the south), equidistantly placed around the cylinder, effectively functioning as giant strip lighting.
Project Spaceguard.
Clarke invented the space study program which detects Rama, Project Spaceguard, as a method of identifying near-Earth objects on Earth-impact trajectories; in the novel it was initiated after an asteroid struck Italy on 11 September 2077, destroying Padua and Verona and sinking Venice.
A real project named Spaceguard was initiated in 1992, named after Clarke's fictional project. After interest in the dangers of asteroid strikes was heightened by a series of Hollywood disaster films, the United States Congress gave NASA authorisation and funding to support Spaceguard.
Books in the series.
Clarke paired up with Gentry Lee for the remainder of the series. Lee did the actual writing, while Clarke read and made editing suggestions. The focus and style of the last three novels are quite different from those of the original with an increased emphasis on characterisation and more clearly portrayed heroes and villains, rather than Clarke's dedicated professionals. These later books did not receive the same critical acclaim and awards as the original.
Gentry Lee also wrote two further novels set in the same "Rama" Universe.
Adaptations.
Video games.
A graphic adventure computer game of the same name with a text parser based on the book was made in 1984 by Trillium (later known as Telarium) and ported to other systems such as the Apple II and Commodore 64. Despite its primitive graphics, it had highly detailed descriptions, and it followed the book very closely along with having puzzles to solve during the game. It was adapted from the Clarke novel in 1983 by Ron Martinez, who went on to design the massively multiplayer online game "10Six", also known as "Project Visitor".
Sierra Entertainment created "Rama" in 1996 as a point and click adventure game in the style of "Myst". Along with highly detailed graphics, Arthur C. Clarke also appeared in the game as the guide for the player. This game featured characters from the sequel book "Rama II".
Radio adaption.
In 2009, BBC Radio 4 produced a two-part radio adaptation of the book as part of a science-fiction season. It was adapted by Mike Walker, and was broadcast on 1 March 2009 (Part 1) and 8 March 2009 (Part 2).
Movie.
In the early 2000s, actor Morgan Freeman expressed his desire to produce a film based on "Rendezvous with Rama", however the film has been stuck in "development hell" for many years. In 2003, after initial problems procuring funding, it appeared the project would go into production. The film was to be produced by Freeman's production company, Revelations Entertainment. David Fincher, touted on Revelations' "Rama" web page as far back as 2001, stated in a late 2007 interview that he was still attached to helm.
However, by late 2008, David Fincher stated the movie was unlikely to be made. "It looks like it's not going to happen. There's no script and as you know, Morgan Freeman's not in the best of health right now. We've been trying to do it but it's probably not going to happen."
Nonetheless, in 2010, Freeman stated in an interview that he was still planning to make the project but that it has been difficult to find the right script. He also stated that it should be made in 3D. In January 2011, Fincher stated in an interview with MTV that he was still planning to make the film after he had completed work on his planned remake of "20,000 Leagues Under the Sea" (which was scheduled to begin production in 2012 but has since been scrapped). He also reiterated Freeman's concerns about the difficulty of finding the right script.
In an interview with Neil deGrasse Tyson in February 2012, Freeman indicated an interest in playing the role of Commander Norton for the film, stating that "my fantasy of commanding a starship is commanding Endeavour". Tyson then asked, "So is this a pitch to be ... that person if they ever make that movie?" to which Freeman reaffirmed, "We ARE going to make that movie." In response to a plea to "make that come out sooner rather than later", Freeman reiterated that difficulty in authoring a quality script is the primary barrier for the film, stating "... the only task you have that's really really hard in making movies, harder than getting money, is getting a script ... a good script".
In popular culture.
American death metal band Cryogen recorded a two song suite inspired by the novel and its sequel Rama II in 2013.
Director Christopher Nolan stated that Clarke's science-fiction vision was a major influence on his 2014 film "Interstellar". This was most visually evident toward the end of the film with "Cooper Station," a rotating Rama-like spacecraft with a bright propulsion center at one end that mimics sunlight.

</doc>
<doc id="26477" url="https://en.wikipedia.org/wiki?curid=26477" title="Rust">
Rust

Rust is an iron oxide, usually red oxide formed by the redox reaction of iron and oxygen in the presence of water or air moisture. Several forms of rust are distinguishable both visually and by spectroscopy, and form under different circumstances. Rust consists of hydrated iron(III) oxides Fe2O3·nH2O and iron(III) oxide-hydroxide (FeO(OH), Fe(OH)3).
Given sufficient time, oxygen, and water, any iron mass will eventually convert entirely to rust and disintegrate. Surface rust is flaky and
friable, and it provides no protection to the underlying iron, unlike the formation of patina on copper surfaces. Rusting is the common term for corrosion of iron and its alloys, such as steel. Many other metals undergo equivalent corrosion, but the resulting oxides are not commonly called rust.
Other forms of rust exist, like the result of reactions between iron and chloride in an environment deprived of oxygen. Rebar used in underwater concrete pillars, which generates green rust, is an example.
Chemical reactions.
Rust is another name for iron oxide, which occurs when iron or an alloy that contains iron, like steel, is exposed to oxygen and moisture for a long period of time. Over time, the oxygen combines with the metal at an atomic level, forming a new compound called an oxide and weakening the bonds of the metal itself. Although some people refer to rust generally as "oxidation", that term is much more general; although rust forms when iron undergoes oxidation, not all oxidation forms rust. Only iron or alloys that contain iron can rust, but other metals can corrode in similar ways.
The main catalyst for the rusting process is water. Iron or steel structures might appear to be solid, but water molecules can penetrate the microscopic pits and cracks in any exposed metal. The hydrogen atoms present in water molecules can combine with other elements to form acids, which will eventually cause more metal to be exposed. If chloride ions are present, as is the case with saltwater, the corrosion is likely to occur more quickly. Meanwhile, the oxygen atoms combine with metallic atoms to form the destructive oxide compound. As the atoms combine, they weaken the metal, making the structure brittle and crumbly.
Oxidation of iron.
When impure (cast) iron is in contact with water, oxygen, other strong oxidants, or acids, it rusts. If salt is present, for example in seawater or salt spray, the iron tends to rust more quickly, as a result of electrochemical reactions. Iron metal is relatively unaffected by pure water or by dry oxygen. As with other metals, like aluminium, a tightly adhering oxide coating, a passivation layer, protects the bulk iron from further oxidation. The conversion of the passivating ferrous oxide layer to rust results from the combined action of two agents, usually oxygen and water.
Other degrading solutions are sulfur dioxide in water and carbon dioxide in water. Under these corrosive conditions, iron hydroxide species are formed. Unlike ferrous oxides, the hydroxides do not adhere to the bulk metal. As they form and flake off from the surface, fresh iron is exposed, and the corrosion process continues until either all of the iron is consumed or all of the oxygen, water, carbon dioxide, or sulfur dioxide in the system are removed or consumed.
When iron rusts, the oxides take up more volume than the original metal; this expansion can generate enormous forces, damaging structures made with iron. See Economic effect for more details.
Associated reactions.
The rusting of iron is an electrochemical process that begins with the transfer of electrons from iron to oxygen. The iron is the reducing agent (gives up electrons) while the oxygen is the oxidising agent (gains electrons). The rate of corrosion is affected by water and accelerated by electrolytes, as illustrated by the effects of road salt on the corrosion of automobiles. The key reaction is the reduction of oxygen:
Because it forms hydroxide ions, this process is strongly affected by the presence of acid. Indeed, the corrosion of most metals by oxygen is accelerated at low pH. Providing the electrons for the above reaction is the oxidation of iron that may be described as follows:
The following redox reaction also occurs in the presence of water and is crucial to the formation of rust:
In addition, the following multistep acid-base reactions affect the course of rust formation:
as do the following dehydration equilibria:
From the above equations, it is also seen that the corrosion products are dictated by the availability of water and oxygen. With limited dissolved oxygen, iron(II)-containing materials are favoured, including FeO and black lodestone or magnetite (Fe3O4). High oxygen concentrations favour ferric materials with the nominal formulae Fe(OH)3-xOx/2. The nature of rust changes with time, reflecting the slow rates of the reactions of solids.
Furthermore, these complex processes are affected by the presence of other ions, such as Ca2+, both of which serve as an electrolyte, and thus accelerate rust formation, or combine with the hydroxides and oxides of iron to precipitate a variety of Ca-Fe-O-OH species.
Onset of rusting can also be detected in laboratory with the use of ferroxyl indicator solution. The solution detects both Fe2+ ions and hydroxyl ions. Formation of Fe2+ ions and hydroxyl ions are indicated by blue and pink patches respectively.
Prevention.
Because of the widespread use and importance of iron and steel products, the prevention or slowing of rust is the basis of major economic activities in a number of specialized technologies. A brief overview of methods is presented here; for detailed coverage, see the cross-referenced articles.
Rust is permeable to air and water, therefore the interior metallic iron beneath a rust layer continues to corrode. Rust prevention thus requires coatings that preclude rust formation.
Rust-resistant alloys.
Stainless steel forms a passivation layer of chromium(III) oxide. Similar passivation behavior occurs with magnesium, titanium, zinc, zinc oxides, aluminium, polyaniline, and other electroactive conductive polymers.
Special "weathering steel" alloys such as Cor-Ten rust at a much slower rate than normal, because the rust adheres to the surface of the metal in a protective layer. Designs using this material must include measures that avoid worst-case exposures, since the material still continues to rust slowly even under near-ideal conditions.
Galvanization.
Galvanization consists of an application on the object to be protected of a layer of metallic zinc by either hot-dip galvanizing or electroplating. Zinc is traditionally used because it is cheap, adheres well to steel, and provides cathodic protection to the steel surface in case of damage of the zinc layer. In more corrosive environments (such as salt water), cadmium plating is preferred. Galvanization often fails at seams, holes, and joints where there are gaps in the coating. In these cases, the coating still provides some partial cathodic protection to iron, by acting as a galvanic anode and corroding itself instead of the underlying protected metal. The protective zinc layer is consumed by this action, and thus galvanization provides protection only for a limited period of time.
More modern coatings add aluminium to the coating as zinc-alume; aluminium will migrate to cover scratches and thus provide protection for a longer period. These approaches rely on the aluminium and zinc oxides re-protecting a once-scratched surface, rather than oxidizing as a sacrificial anode as in traditional galvanized coatings. In some cases, such as very aggressive environments or long design life, both zinc and a coating are applied to provide enhanced corrosion protection.
Typical galvanization of steel products which are to subject to normal day to day weathering in an outside environment consists of a hot dipped 85 µm zinc coating. Under normal weather conditions, this will deteriorate at a rate of 1 µm per year, giving approximately 85 years of protection.
Cathodic protection.
Cathodic protection is a technique used to inhibit corrosion on buried or immersed structures by supplying an electrical charge that suppresses the electro-chemical reaction. If correctly applied, corrosion can be stopped completely. In its simplest form, it is achieved by attaching a sacrificial anode, thereby making the iron or steel the cathode in the cell formed. The sacrificial anode must be made from something with a more negative electrode potential than the iron or steel, commonly zinc, aluminium, or magnesium. The sacrificial anode will eventually corrode away, ceasing its protective action unless it is replaced in a timely manner.
Cathodic protection can also be provided by using a special-purpose electrical device to appropriately induce an electric charge.
Coatings and painting.
Rust formation can be controlled with coatings, such as paint, lacquer, or varnish that isolate the iron from the environment. Large structures with enclosed box sections, such as ships and modern automobiles, often have a wax-based product (technically a "slushing oil") injected into these sections. Such treatments usually also contain rust inhibitors. Covering steel with concrete can provide some protection to steel because of the alkaline pH environment at the steel-concrete interface. However rusting of steel in concrete can still be a problem, as expanding rust can fracture or slowly "explode" concrete from within.
As a closely related example, iron bars were used to reinforce stonework of the Parthenon in Athens, Greece, but caused extensive damage by rusting, swelling, and shattering the marble components of the building.
When only temporary protection is needed for storage or transport, a thin layer of oil, grease, or a special mixture such as Cosmoline can be applied to an iron surface. Such treatments are extensively used when "mothballing" a steel ship, automobile, or other equipment for long-term storage.
Special anti-seize lubricant mixtures are available, and are applied to metallic threads and other precision machined surfaces to protect them from rust. These compounds usually contain grease mixed with copper, zinc, or aluminum powder, and other proprietary ingredients.
Bluing.
Bluing is a technique that can provide limited resistance to rusting for small steel items, such as firearms; for it to be successful, a water-displacing oil is rubbed onto the blued steel and other steel.
Inhibitors.
Corrosion inhibitors, such as gas-phase or volatile inhibitors, can be used to prevent corrosion inside sealed systems. They are not effective when air circulation disperses them, and brings in fresh oxygen and moisture.
Humidity control.
Rust can be avoided by controlling the moisture in the atmosphere. An example of this is the use of silica gel packets to control humidity in equipment shipped by sea.
Treatment.
Rust removal from small iron or steel objects by electrolysis can be done in a home workshop using simple materials such as a plastic bucket, tap water, lengths of rebar, washing soda, baling wire, and a battery charger.
Rust may be treated with commercial products known as rust converter which contain tannic acid which combines with rust.
Economic effect.
Rust is associated with degradation of iron-based tools and structures. As rust has a much higher volume than the originating mass of iron, its build-up can also cause failure by forcing apart adjacent parts — a phenomenon sometimes known as "rust packing". It was the cause of the collapse of the Mianus river bridge in 1983, when the bearings rusted internally and pushed one corner of the road slab off its support.
Rust was an important factor in the Silver Bridge disaster of 1967 in West Virginia, when a steel suspension bridge collapsed in less than a minute, killing 46 drivers and passengers on the bridge at the time. The Kinzua Bridge in Pennsylvania was blown down by a tornado in 2003, largely because the central base bolts holding the structure to the ground had rusted away, leaving the bridge anchored by gravity alone.
Reinforced concrete is also vulnerable to rust damage. Internal pressure caused by expanding corrosion of concrete-covered steel and iron can cause the concrete to spall, creating severe structural problems. It is one of the most common failure modes of reinforced concrete bridges and buildings.
Cultural symbolism.
Rust is a commonly used metaphor for slow decay due to neglect, since it gradually converts robust iron and steel metal into a soft crumbling powder. A wide section of the industrialized American Midwest and American Northeast, once dominated by steel foundries, the automotive industry, and other manufacturers, has experienced harsh economic cutbacks that have caused the region to be dubbed the "Rust Belt".
In music, literature, and art, rust is associated with images of faded glory, neglect, decay, and ruin.
References.
Waldman, J. (2015): "Rust - the longest war." Simon & Schuster, New York. ISBN 978-1-4516-9159-7

</doc>
<doc id="26478" url="https://en.wikipedia.org/wiki?curid=26478" title="Real analysis">
Real analysis

Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable. In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.
Scope.
Construction of the real numbers.
There are several ways of defining the real number system as an ordered field. The "synthetic" approach gives a list of axioms for the real numbers as a "complete ordered field". Under the usual axioms of set theory, one can show that these axioms are categorical, in the sense that there is a model for the axioms, and any two such models are isomorphic. Any one of these models must be explicitly constructed, and most of these models are built using the basic properties of the rational number system as an ordered field. These constructions are described in more detail in the main article.
Order properties of the real numbers.
The real numbers have several important lattice-theoretic properties that are absent in the complex numbers. Most importantly, the real numbers form an ordered field, in which addition and multiplication preserve positivity. Moreover, the ordering of the real numbers is total, and the real numbers have the least upper bound property. These order-theoretic properties lead to a number of important results in real analysis, such as the monotone convergence theorem, the intermediate value theorem and the mean value theorem.
However, while the results in real analysis are stated for real numbers, many of these results can be generalized to other mathematical objects. In particular, many ideas in functional analysis and operator theory generalize properties of the real numbers – such generalizations include the theories of Riesz spaces and positive operators. Also, mathematicians consider real and imaginary parts of complex sequences, or by pointwise evaluation of operator sequences.
Sequences.
A sequence is usually defined as a function whose domain is a countable totally ordered set, although in many disciplines the domain is restricted, such as to the natural numbers. In real analysis a sequence is a function from a subset of the natural numbers to the real numbers. In other words, a sequence is a map "f"("n") : N → R. We might identify "an" = "f"("n") for all "n" or just write "an" : N → R.
Limits.
A limit is the value that a function or sequence "approaches" as the input or index approaches some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.
Continuity.
A function from the set of real numbers to the real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve with no "holes" or "jumps".
There are several ways to make this intuition mathematically rigorous. These definitions are equivalent to one another, so the most convenient definition can be used to determine whether a given function is continuous or not. In the definitions below,
is a function defined on a subset "I" of the set R of real numbers. This subset "I" is referred to as the domain of "f". Some possible choices include "I"=R, the whole set of real numbers, an open interval
Here, "a" and "b" are real numbers.
Uniform continuity.
If "X" and "Y" are subsets of the real numbers, a function "f" : "X" → "Y" is called uniformly continuous if for all "ε" > 0 there exists a "δ" > 0 such that for all "x", "y" ∈ "X", |"x" − "y"| < "δ" implies |"f"("x") − "f"("y")| < "ε.
The difference between being uniformly continuous, and being simply continuous at every point, is that in uniform continuity the value of "δ" depends only on "ε" and not on the point in the domain.
Absolute continuity.
Let formula_3 be an interval in the real line R. A function formula_4 is absolutely continuous on formula_3 if for every positive number formula_6, there is a positive number formula_7 such that whenever a finite sequence of pairwise disjoint sub-intervals formula_8 of formula_3 satisfies
then
The collection of all absolutely continuous functions on "I" is denoted AC("I").
The following conditions on a real-valued function "f" on a compact interval ["a","b"] are equivalent:
If these equivalent conditions are satisfied then necessarily "g" = "f" ′ almost everywhere.
Equivalence between (1) and (3) is known as the fundamental theorem of Lebesgue integral calculus, due to Lebesgue.
Series.
Given an infinite sequence of numbers { "a""n" }, a series is informally the result of adding all those terms together: "a"1 + "a"2 + "a"3 + · · ·. These can be written more compactly using the summation symbol ∑. An example is the famous series from Zeno's dichotomy and its mathematical representation:
The terms of the series are often produced according to a certain rule, such as by a formula, or by an algorithm.
Taylor series.
The Taylor series of a real or complex-valued function "ƒ"("x") that is infinitely differentiable at a real or complex number "a" is the power series
which can be written in the more compact sigma notation as
where "n"! denotes the factorial of "n" and "ƒ" ("n")("a") denotes the "n"th derivative of "ƒ" evaluated at the point "a". The derivative of order zero "ƒ" is defined to be "ƒ" itself and and 0! are both defined to be 1. In the case that , the series is also called a Maclaurin series.
Fourier series.
A Fourier series decomposes periodic functions or periodic signals into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or complex exponentials). The study of Fourier series is a branch of Fourier analysis.
Differentiation.
Formally, the derivative of the function "f" at "a" is the limit
If the derivative exists everywhere, the function is differentiable. One can take higher derivatives as well, by iterating this process.
One can classify functions by their differentiability class. The class "C"0 consists of all continuous functions. The class "C"1 consists of all differentiable functions whose derivative is continuous; such functions are called continuously differentiable. Thus, a "C"1 function is exactly a function whose derivative exists and is of class "C"0. In general, the classes "Ck" can be defined recursively by declaring "C"0 to be the set of all continuous functions and declaring "Ck" for any positive integer "k" to be the set of all differentiable functions whose derivative is in "C""k"−1. In particular, "Ck" is contained in "C""k"−1 for every "k", and there are examples to show that this containment is strict. "C"∞ is the intersection of the sets "Ck" as "k" varies over the non-negative integers. "C"ω is strictly contained in "C"∞.
Integration.
Riemann integration.
The Riemann integral is defined in terms of Riemann sums of functions with respect to "tagged partitions" of an interval. Let ["a","b"] be a closed interval of the real line; then a "tagged partition" of ["a","b"] is a finite sequence
This partitions the interval ["a","b"] into "n" sub-intervals indexed by "i", each of which is "tagged" with a distinguished point . A "Riemann sum" of a function "f" with respect to such a tagged partition is defined as
thus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval "i"; then the "mesh" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The "Riemann integral" of a function "f" over the interval ["a","b"] is equal to "S" if:
When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.
Lebesgue integration.
Lebesgue integration is a mathematical construction that extends the integral to a larger class of functions; it also extends the domains on which these functions can be defined.
Distributions.
Distributions (or generalized functions) are objects that generalize functions. Distributions make it possible to differentiate functions whose derivatives do not exist in the classical sense. In particular, any locally integrable function has a distributional derivative.
Relation to complex analysis.
Real analysis is an area of analysis that studies concepts such as sequences and their limits, continuity, differentiation, integration and sequences of functions. By definition, real analysis focuses on the real numbers, often including positive and negative infinity to form the extended real line. Real analysis is closely related to complex analysis, which studies broadly the same properties of complex numbers. In complex analysis, it is natural to define differentiation via holomorphic functions, which have a number of useful properties, such as repeated differentiability, expressability as power series, and satisfying the Cauchy integral formula.
In real analysis, it is usually more natural to consider differentiable, smooth, or harmonic functions, which are more widely applicable, but may lack some more powerful properties of holomorphic functions. However, results such as the fundamental theorem of algebra are simpler when expressed in terms of complex numbers.
Techniques from the theory of analytic functions of a complex variable are often used in real analysis – such as evaluation of real integrals by residue calculus.
Important results.
Important results include the Bolzano–Weierstrass and Heine–Borel theorems, the intermediate value theorem and mean value theorem, the fundamental theorem of calculus, and the monotone convergence theorem.
Various ideas from real analysis can be generalized from real space to general metric spaces, as well as to measure spaces, Banach spaces, and Hilbert spaces.

</doc>
<doc id="26479" url="https://en.wikipedia.org/wiki?curid=26479" title="Richie Benaud">
Richie Benaud

Richard "Richie" Benaud, (; 6 October 1930 – 10 April 2015) was an Australian cricketer who, after his retirement from international cricket in 1964, became a highly regarded commentator on the game.
Benaud was a Test cricket all-rounder, blending leg spin bowling with lower-order batting aggression. Along with fellow bowling all-rounder Alan Davidson, he helped restore Australia to the top of world cricket in the late 1950s and early 1960s after a slump in the early 1950s. In 1958 he became Australia's Test captain until his retirement in 1964. He became the first player to reach 200 wickets and 2,000 runs in Test cricket, arriving at that milestone in 1963.
Gideon Haigh described him as "perhaps the most influential cricketer and cricket personality since the Second World War." In his review of Benaud's autobiography "Anything But", Sri Lankan cricket writer Harold de Andrado wrote: "Richie Benaud possibly next to Sir Don Bradman has been one of the greatest cricketing personalities as player, researcher, writer, critic, author, organiser, adviser and student of the game."
Early years.
Benaud was born in Penrith, New South Wales, in 1930. He came from a cricket family. His father Louis, a third generation Australian of French Huguenot descent, was a leg spinner who played for Penrith in Sydney Grade Cricket, gaining attention for taking all twenty wickets in a match against St. Marys for 65 runs. Lou later moved to Parramatta region in western Sydney, and played for Cumberland. It was here that Richie Benaud grew up, learning how to bowl leg breaks, googlies and topspinners under his father's watch. Educated at Parramatta High School, Benaud made his first grade debut for Cumberland at age 16, primarily as a batsman.
In November 1948, at the age of 18, Benaud was selected for the New South Wales Colts, the state youth team. He scored 47 not out and took 3/37 in an innings win over Queensland. As a specialist batsman, he made his first class debut for New South Wales at the Sydney Cricket Ground against Queensland in the New Year's match of the 1948–49 season. On a green pitch which was struck by a downpour on the opening day, Benaud's spin was not used by Arthur Morris and he failed to make an impression with the bat in his only innings, scoring only two. New South Wales were the dominant state at the time, and vacancies in the team were scarce, particularly as there were no Tests that season and all of the national team players were available for the whole summer. Relegated to the Second XI after this match, he was struck in the head above the right eye by a ball from Jack Daniel while batting against Victoria in Melbourne, having missed an attempted hook. After 28 X-rays showed nothing, it was finally diagnosed that the crater in his forehead had resulted in a skull fracture and he was sidelined for the remainder of the season, since a second impact could have been fatal. He spent two weeks in hospital for the surgery. This was the only match he played for the second-string state team that summer.
In his early career, Benaud was a batting all-rounder, marked by a looping backlift which made him suspect against fast bowling but allowed him to have a wide attacking stroke range. At the start of the 1949–50 season, he was still in the Second XI, but when the Test players departed for a tour of South Africa soon afterwards, vacancies opened up. Benaud was recalled to the New South Wales First XI in late December for the Christmas and New Year's fixtures. With Ray Lindwall, Keith Miller and Ernie Toshack, three of Australia's leading four bowlers from the 1948 "Invincibles" tour of England unavailable, Benaud bowled heavily in some matches. However, he did not have much success in his five games, taking only five wickets at 54.00.
He took the wicket of Queensland batsman Bill Brown in his third match of the season. Benaud erroneously recalled in an autobiography that this was his maiden wicket—it was his fourth—and described the ball as "the worst I ever bowled". He had more success with the bat, scoring 93 and narrowly missing a century against South Australia. He added another fifty and ended with 250 runs at 31.25.
The next season, England toured Australia, and with the Test players back, Benaud was initially forced out of the team. He was recalled for a match against the Englishmen. He was attacked by the touring batsmen, taking 1/75 from 16.5 overs in his first outing against an international outfit. His only wicket was that of the all-rounder Trevor Bailey. He scored 20 not out and was not called on to bowl in the second innings.
In the next Shield match against Victoria, led by Australian captain Lindsay Hassett, Benaud came in for attack. Hassett was known for his prowess against spin bowling, being the only batsman to score centuries in a match against the leg-spin of Bill O'Reilly, regarded as the finest bowler of his age. Hassett struck 179 in four hours, and took 47 runs from Benaud's seven overs. The young leg spinner claimed Hassett in the second innings when a ball landed in a crack and skidded through onto his foot. He ended with 3/56, the first time he had taken three wickets in a match. In the next match against South Australia, he made 48, took 4/93 and 1/29 and suffered three dropped catches by the wicketkeeper in successive balls. Benaud was cementing his position and was in the senior team for four consecutive matches even with the Test players available. He was selected for an Australian XI match against England, in what was effectively a trial for Test selection, but suffered a chipped bone in his thumb. This put him out of action until the last match of the season, leaving him with little opportunity to impress the national selectors for his rise to international cricket. Benaud returned and scored 37 and took a total of 2/68 in the final match, ending the season with 184 runs at 36.80 and 11 wickets at 34.63.
Early Test career.
The 1951–52 season saw a tour to Australia by the West Indies. Benaud was given a chance against the visiting team when New South Wales played them in Sydney after the First Test. On a green pitch, Benaud came in at 7/96 and featured in a century partnership in only an hour, making 43 himself. The Caribbeans were skittled for 134 in reply and went on to lose the match, although they attacked the young leg-spinner, who took 1/130 in total from 36 overs. Benaud scored his maiden first-class century, 117 against South Australia, in the next match, two years after falling short of the milestone by seven runs. In the next four matches, Benaud passed 15 only once, scoring a 34, and took only seven wickets. Up to this point, in seven matches for the season, the young all-rounder had only scored 307 runs at 27.90 and taken ten wickets at 64.80.
Despite this, Benaud was chosen for his Test debut in the Fifth Test against the West Indies in 1951–52 in Sydney. At this point, Australia had already taken an unassailable 3–1 series lead and decided to try out some young players. Selected as a batsman, he scored 3 and 19. Hassett allowed him to bowl only in the second innings, when nine West Indian wickets had fallen and Australia were on the verge of an inevitable victory. Leading opposition batsman Everton Weekes edged Benaud in his first over, but Gil Langley dropped the catch. Benaud went on to dismiss tail-ender Alf Valentine for his first Test wicket, conceding 14 runs from 4.3 overs. Benaud ended his season with 97 and a total of 3/39 in an innings win over South Australia.
The following Australian season in 1952–53, Benaud started modestly and in the five first-class matches before the Tests, scored 208 runs at 26.00 including a 63 and 69, and 14 wickets at 38.64. This included figures of 2/70 and 4/90 against the touring
South Africa. However, this was not enough to ensure his selection in the First Test, where he was made 12th man. After scoring 60 and 37 and taking 1/60 in an Australian XI against the South Africans following the Test, he was selected for the Second Test. He suffered a smashed gum and a severely cut top lip when a square cut by John Waite in the Third Test against South Africa at the Sydney Cricket Ground hit him in the face while he was fielding at short gully. Doctors told him he was lucky: it could have broken his cheekbones, jaw or removed his eyesight if it had hit any of the surrounding areas. It could have killed him if it had struck him where his skull was previously fractured. He married after the match and had to mumble his wedding vows through a swathe of bandages. Benaud went on to play in the final four Tests. He made 124 runs at 20.66, making double figures in four of seven innings, but was unable to capitalise on his starts, with a top score of 45. His leg spin yielded ten wickets at 30.60, with a best of 4/118 in the Fourth Test in Adelaide when he was given a heavy workload, totalling 58 overs, when Ray Lindwall and Keith Miller broke down during the match. In another match for New South Wales against the touring team, he took a total of 5/95. Up to this point, his first-class batting average was below 30 and his bowling average close to 40, and he had never taken more than four wickets in an innings or six in a match.
The selectors persisted in Benaud despite his unproductive Test performances, selecting him for the squad for the 1953 Ashes tour of England. He had been seventh and eighth in the domestic runscoring and wicket-taking aggregates for the season, but was yet to convert this into international performance. He justified their decision prior to the team's departure, scoring 167 not out and taking match figures of 7/137 for the touring team against a Tasmania Combined XI, his wickets including Test batsmen Miller, Ian Craig and Neil Harvey. He also put on 167 in a partnership with Alan Davidson, the first collaboration between the pair, who would later go on to lead Australia's bowling in the last five years of their career. Benaud then struck an unbeaten 100 and totalled 1/64 in the next match against Western Australia before the Australians departed for England.
On arrival in the British Isles, Benaud quickly made an impression with both bat and ball. After scoring 44 and taking 2/66 in the opening first-class match against Worcestershire, the all-rounder starred in his next match, against Yorkshire. He scored 97 in Australia's only innings and then took 7/46 in the hosts' first innings as the Australians took an innings win. Although his form with the willow dropped off in his remaining six matches before the Tests—a 35 was his only score beyond 20 in seven attempts—Benaud continued to strike regularly with the ball. He took 18 wickets in these matches, including 3/20 and 3/37 against Oxford University, 5/13 against Minor Counties and 4/38 against Hampshire. This was enough for him to gain selection for the start of the Tests.
He managed only eight runs in four innings in the first two Tests, and having taken only two wickets for 136 runs was dropped for the Third. This was part of a month-long run in which he made only 123 runs in eight innings and took only seven wickets in four matches. He was recalled immediately for the Fourth Test, but was dropped for the Fifth after managing seven runs in his only innings and going wicketless. He ended the Test series with 15 runs at 3.00 and two wickets at 87.00. It was thought that the surface at the Oval would favour pacemen, but Australia's selection proved to be a blunder as England's spinners took them to the only win of the series, allowing them to regain the Ashes.
He also showed his hitting ability in a tour match against T.N. Pearce's XI at Scarborough. Opening the batting, he struck 135 in 110 minutes in the second innings, including an Australian record of eleven sixes, four of them in one over. In eight first-class matches after his Test campaign was over, Benaud added a further half-century in addition to the century against Pearce's XI, and took 22 more wickets, including 4/20 against the Gentlemen of England.
Consolidation.
After returning home from his first overseas tour, Benaud was prolific during the 1953–54 Australian season, which was purely domestic with no touring Test team. He contributed significantly with both bat and ball in New South Wales' Sheffield Shield triumph, the first of nine consecutive titles. In the opening match of the season, he struck 158 and took 5/88 and 1/65 against Queensland. He made another century in the return match, striking 144 not out and taking a total of 2/55. Midway through the season, he played in Morris's XI in a testimonial match for Hassett, who captained the other team. Benaud scored 78 and 68 and took a total of 5/238, his dismissals being Davidson and frontline Test batsmen in a 121-run win. He then finished the summer strongly, and ended the season with 811 runs at 62.38 and 35 wickets at 30.54.
Despite his inability to contribute with either bat or ball in England, Benaud was the only bowler selected for all five Tests of the 1954–55 series when England visited Australia. He secured his place after scoring 125 against Queensland at the start of the season, although his lead-up form in two matches against England for his state and an Australian XI was not encouraging.
At this stage of his career, he had played 13 Tests with mediocre results. Selected as a batsman who could bowl, he had totalled 309 runs at 15.45 without passing 50, and taken 23 wickets at 37.87 with only two four-wicket innings hauls. Even so, he was promoted to vice-captain above several senior players when Ian Johnson and Keith Miller missed the 2nd Test at Sydney through injury and Arthur Morris was made temporary captain. He also made 113 against the touring side for the Prime Minister's XI.
Australia's selectors persisted and selected him for the squad to tour the West Indies in 1954–55. Their faith was rewarded by an improvement in performances. Benaud contributed 46 and match figures of 2/73 in a First Test victory at Kingston. After a draw in the Second Test, he took three wickets in four balls to end with 4/15 in the first innings at Georgetown, Guyana, before scoring 68 (his first Test half century) as Australia moved to a 2–0 series lead. In the Fifth Test at Kingston, he struck a century in 78 minutes, despite taking 15 minutes to score his first run. He ended with 121 and took four wickets in the match as Australia won by an innings and took the series 3–0. Benaud had contributed 246 runs at 41 and taken wickets steadily to total 18 at 26.94.
During the 1956 tour to England, he helped Australia to its only victory in the Lord's Test, when he scored a rapid 97 in the second innings in 143 minutes from only 113 balls. His fielding, in particular at gully and short leg, was consistently of a high standard, in particular his acrobatic catch to dismiss Colin Cowdrey. He was unable to maintain the standards he had set in the West Indies, contributing little apart from the Lord's Test. He ended the series with 200 runs at 25 and eight wickets at 42.5.
Benaud's bowling reached a new level on the return leg of Australia's overseas tour, when they stopped in the Indian subcontinent in 1956–57 en route back to Australia. In a one-off Test against Pakistan in Karachi, he scored 56 and took 1/36 as Australia fell to defeat. He claimed his Test innings best of 7/72 in the first innings of the First Test in Madras, allowing Australia to build a large lead and win by an innings. It was his first five-wicket haul in a Test innings. After taking four wickets in the drawn Second Test in Bombay, Benaud bowled Australia to victory in the Third Test in Calcutta, sealing the series 2–0. He took 6/52 and 5/53, his best-ever match analysis, ending the series with 113 runs at 18.83 and 24 wickets at 17.66. It was the first of his successes against India, against whom he took his wickets at an average of 18. This put him in a small group of spinners whose career averages were inferior to their performances against India, generally regarded as the best players of spin in the world. At this stage of his career, he had yet to perform consistently with bat and ball simultaneously, apart from his breakthrough series in the Caribbean. He had managed, in the 14 Tests since then, 559 runs at 27.95 and 67 wickets at 24.98.
Peak years and captaincy.
After a break in the international calendar of a year, the 1957–58 tour to South Africa heralded the start of a phase of three international seasons when Benaud was at his peak. The tour saw his bowling talents come to the fore when he took 106 wickets, surpassing the previous record of 104 by England's Sydney Barnes. He scored 817 runs including four centuries, two of them in Test matches. The first of these came in the First Test at Johannesburg, where after conceding 1/115, Benaud struck 122, his highest Test score, to see Australia reach a draw. In the Second Test at Cape Town, Benaud took 4/95 and then 5/49 in the second innings to secure an innings victory after the home team were forced to follow on. He followed this with 5/114 in a drawn Third Test, before a match-winning all round performance in the Fourth Test in Johannesburg. Benaud struck exactly 100 in the first innings, before taking 4/70 in South Africa's reply. When South Africa followed on, Benaud took 5/84, which left Australia needing only one run to win. He took 5/82 in the second innings of the Fifth Test, the fourth consecutive match in which he had taken five wickets in an innings, as Australia took a 3–0 series win. He had been a major contributor to the series win, scoring 329 runs at 54.83 and taking 30 wickets at 21.93, establishing himself as one of the leading leg spinners of the modern era.
When Ian Craig fell ill at the start of the 1958–59 season, Benaud was promoted to the captaincy ahead of vice-captain Neil Harvey. Harvey and Benaud had been captains of their respective states until Harvey moved in the same season for employment purposes from Victoria to New South Wales and became Benaud's deputy. Benaud had little prior leadership experience, and faced the task of recovering the Ashes from an England team which had arrived in Australia as favourites. He led from the front with his bowling, taking match figures of 7/112 in his debut as captain as Australia claimed the First Test in Brisbane. Benaud's men won the Second Test, before he took 5/83 and 4/94 in the drawn Third Test. Benaud produced an all-round performance of 46, 5/91 and 4/82 in the Fourth Test in Adelaide to take an unassailable 3–0 series lead and regain the Ashes, before scoring 64 and match figures of 5/57 to help take the Fifth Test and a 4–0 series result. Benaud contributed 132 runs at 26.4 and 31 wickets at the low average of 18.83, as well as his shrewd and innovative captaincy. According to Neil Harvey, he also was the first captain who started hosting team meetings, a procedure now followed by his successors after he retired.
Benaud then led Australia on its first full tour of the Indian subcontinent, playing three and five Tests against Pakistan and India respectively. Benaud took 4/69 and 4/42 in the First Test in Dacca (now in Bangladesh), sealing Australia's first win in Pakistan. He took four wickets in a Second Test in Lahore that sealed the series 2–0, the last time Australia would win a Test in Pakistan until Mark Taylor's men in 1998, 37 years later. Six further wickets in the drawn Third Test saw Benaud end the series with 84 runs at 28 and 18 wickets at 21.11. Benaud made a strong start to the series against India, taking 3/0 in the first innings of the First Test in Delhi, before a 5/76-second innings haul secured an innings victory. Benaud had less of an impact on the next two Tests, which Australia lost and drew, totalling 6/244. He returned to form with 5/43 and 3/43 as India were defeated by an innings after being forced to follow on in the Fourth Test in Madras. A further seven wickets from the captain in the Fifth Test saw Australia secure a draw and the series 2–1. Benaud had contributed 91 runs at 15.16 and 29 wickets at 19.59. The first two seasons of the Benaud captaincy had been a resounding success, with Australia winning eight, drawing four and losing only one Test. Benaud's personal form was a major factor in this success. In the previous seasons when he and his team were at their peak, he had scored 636 runs at 31.8 with taken 108 wickets at 20.27 in eighteen Tests, averaging six wickets a match.
Later career.
Benaud took over when Australian cricket was in a low phase with a young team. His instinctive, aggressive captaincy and daring approach to cricket – and his charismatic nature and public relations ability – revitalised cricket interest in Australia. This was exhibited in the 1960–61 Test series against the visiting West Indians, in which the grounds were packed to greater levels than they are today despite Australia's population doubling since then. The First Test in Brisbane ended in the first tie in Test history, which came about after Benaud and Alan Davidson, rather than settle for a draw, decided to risk defeat and play an attacking partnership, which took Australia to the brink of victory. Australia had fallen to 6/92 on the final day chasing a target of 233 with Benaud and Davidson at the crease. Australia's chances of winning looked remote when they reached tea at 6/109 with 124 runs still required with only the tailenders to follow. Despite this, Benaud told chairman of selectors Don Bradman that he would still be going for an improbable victory in accordance with his policy of aggression. With an attacking partnership, the pair took Australia to within sight of the target. Both men were noted for their hitting ability and viewed attack as their most effective chance of survival. Regular boundaries and quickly-run singles took the score to 226, a seventh-wicket partnership of 134. Only seven runs were required with four wickets in hand as time was running short. Benaud hit a ball into the covers and the pair attempted a quick single when a direct hit from Joe Solomon saw Davidson run out. Australia needed six runs from the final over, in which Benaud was caught and the last two wickets fell to run outs while attempting the winning run. The Test was tied when Solomon ran out Ian Meckiff with a direct hit. Benaud had an unpenetrative match with the ball, taking 1/162. He took 4/107 in a seven-wicket victory in Melbourne, before the West Indies levelled the series with a 22-run win in Sydney. Benaud had a heavy load in the match taking 8/199 after Davidson tore a hamstring mid-match. In Adelaide, with Davidson absent, Benaud bowled long spells to take match figures of 7/207 in addition to a score of 77 in the first innings. With Davidson back, Australia won the final Test by two wickets, after a controversial incident in which Australian wicketkeeper Wally Grout was not given out hit wicket when a bail was dislodged and the umpires did not notice. Australia won the series 2–1, and although Benaud was below his best, scoring at 21.77 and taking 23 wickets at 33.87, the series was a success for cricket. The unprecedented public interest saw the Caribbean touring party farewelled with a ticker-tape parade by the Australian public. Along with the West Indian captain Frank Worrell, Benaud's bold leadership enlivened interest in Test cricket among a public who had increasingly regarded it as boring.
On his third and final tour to England in 1961, he was hampered by damaged tendons in his right shoulder, which forced him to miss the Second Test at Lord's known as the "Battle of the Ridge". In all he missed a third of the matches due to injury. Despite this impairment to his bowling shoulder, his team played with an aggressive strategy leading them to lose only one Test match and no other matches during the tour, honouring his pre-series pledge. The First Test at Edgbaston was drawn with Benaud taking three wickets. After Harvey led the team to victory at Lord's, Benaud had an unhappy return in the Third at Headingley scoring two runs in two innings and taking match figures of 2/108 as Australia lost within three days. With the series balanced at 1–1, the Fourth Test at Old Trafford initially brought no improvement, with Benaud scoring 2 and taking 0/80 in the first innings. He made 1 in the second before a last-wicket partnership between Davidson and Graham McKenzie of 98 yielded a defendable target. During England's chase on the final afternoon it became apparent that, with Ted Dexter scoring quickly, Australia would lose the Test unless England were bowled out. Benaud went around the wicket and bowled into the footmarks, having Dexter caught behind and then Peter May bowled around his legs. Benaud's 5/13 in 25 balls instigated an English collapse which saw Australia retain the Ashes. He finished the innings with 6/70. Benaud then took four wickets in the drawn Fifth Test to end the series 2–1. Benaud had a poor series with the bat, scoring 45 runs at 9 and taking 15 wickets at 32.53. He finished the first-class tour with 627 runs and 61 wickets at 23.54. He was awarded an OBE in that year and in 1962 was named as one of the "Wisden" Cricketers of the Year.
The 1961–62 Australian season was purely a domestic one, with no touring international team. Benaud led New South Wales throughout a dominant season, winning the Sheffield Shield with 64 of the 80 possible points. Benaud was the leading wicket-taker of the season with 47 at 17.97. His aggressive tactical style brought large crowds throughout the season, with almost 18,000 watching one match against South Australia. In another match against Victoria, he ordered his team to attempt to score 404 on the final day to take an unlikely victory in accordance with a promise to score at 400 per day. At one stage, New South Wales were six wickets down with less than 150 runs scored, but Benaud refused to attempt to defend for a draw. He made 140, in a seventh-wicket partnership of 255 in just 176 minutes, an Australian record that still stands.
1962–63 saw an English team under Dexter visit Australia. Fred Trueman with 216 Test wickets and Brian Statham with 229 were poised to overtake the record of 236 Test records set by the assistant-manager Alec Bedser. Benaud was another contender with 219 wickets, but it was Statham who broke the record (only to be overtaken by Trueman in New Zealand) and Benaud had to be content with breaking Ray Lindwall's Australian record of 228 Test wickets. In an early tour match Benaud took his best first class innings haul of 18–10–18–7 for New South Wales against the MCC, which lost by an innings and 80 runs, the state's biggest win against the English team. Benaud started the series with seven wickets and a half century as the First Test in Brisbane was drawn. This was followed by three unproductive Tests which yielded only 5/360 and a win apiece. Benaud returned to form with match figures of 5/142 and 57 in the Fifth Test at Sydney, which ended in a draw when Benaud ordered Bill Lawry and Peter Burge to play out the last afternoon for a draw that would retain the Ashes. They were booed and heckled as they left the field and Benaud's reputation as a "go ahead" cricket captain was badly tanished. The draw meant that the series was shared 1–1, the first time he had drawn a series after five successive wins. It was another lean series with the ball, Benaud's 17 wickets costing 40.47, the third consecutive series where his wickets cost more than 30. His batting was reliable, with 227 runs at 32.47.
At the start of the 1963–64 season, Benaud announced that it would be his last at first-class level. The first Test of the season, against the touring South Africans, saw high drama as Australia's left arm paceman Ian Meckiff was called for throwing by Colin Egar and removed from the attack by Benaud after one over. Benaud did not bowl Meckiff from the other end, and at the end of the match Meckiff announced his retirement. Benaud took 5/72 and scored 43 in the First Test, but then injured himself in a grade match, so Bob Simpson captained the team for the Second Test and won the match in Benaud's absence. Upon his return, Benaud advised the Australian Cricket Board that it would be in the better interests of the team if Simpson continued as captain for the remainder of the season. Benaud took 3/116 to complement scores of 43 and 90 on his return in the Third Test in Sydney. His final two Tests saw no fairytale finish, yielding only four wickets and 55 runs. His batting had been steady though with 231 runs at 33, but his bowling unpenetrative with 12 wickets at 37.42.
Benaud was awarded life membership by the New South Wales Cricket Association, but he returned it in protest in 1970 when his younger brother John was removed from the captaincy. In 1967–68 he captained a Commonwealth team against Pakistan, playing in his last five first-class fixtures.
During Benaud's captaincy, Australia did not lose a series, and became the dominant team in world cricket. His success was based on his ability to attack, his tactical boldness and his ability to extract more performance from his players, in particular Davidson. He was known for his unbuttoned shirt, and raised eyebrows with his on-field exuberance. Benaud embraced his players when opposition wickets fell, something that was uncommon at the time. Benaud's bold leadership coupled with his charismatic nature and public relations ability enlivened interest in Test cricket among a public who had increasingly regarded it as boring.
Playing style.
Benaud was not a large spinner of the ball, but he was known for his ability to extract substantial bounce from the surface. In addition to his accurate probing consistency, he possessed a well-disguised googly and topspinner which tricked many batsmen and yielded him many wickets. In his later career, he added the flipper, a combination of the googly and top spinner which was passed to him by Bruce Dooland. Coupled with his subtle variations in flight and angle of the delivery, he kept the batsman under constant pressure. Benaud also had the tendency to bowl around the wicket at a time when he was one of the first players to do so; it had an influence on spin bowlers like Shane Warne and Ashley Giles. Benaud was regarded as one of the finest close-fielders of his era, either at gully or in a silly position. As a batsman, he was tall and lithe, known for his hitting power, in particular his lofted driving ability from the front foot.
Johnnie Moyes said "Certainly Benaud received a little help from the roughened patches, but he could do what the off-spinners could not do: he could turn the ball, mostly slowly, sometimes with more life. His control was admirable, and when Benaud gets a batsman in trouble he rarely if ever gives him a loose one. He keeps him pinned down, probing and probing until the victim is well and truly enmeshed."
Media career.
After the 1956 England tour, Benaud stayed behind in London to take a BBC presenter training course. He took up a journalism position with the "News of the World", beginning as a police roundsman before becoming a sports columnist. In 1960 he made his first radio commentary in the United Kingdom at the BBC, after which he moved into television.
After retiring from playing in 1964, Benaud turned to full-time cricket journalism and commentary, dividing his time between Britain (where he worked for the BBC for many years before joining Channel 4 in 1999), and Australia (for the Nine Network). Overall he played in or commentated on approximately 500 Test matches, as he himself noted in one of his final interviews in Britain when asked if he would miss Test cricket.
He openly criticized the actions by the Chappell brothers (Trevor and Greg) in the post-match reaction to the underarm bowling incident of 1981, proving his moral integrity far outweighed his unconditional patriotism for Australia. He also vacated the commentary booth when New Zealand was about to clinch a test victory at Lord's in 1999, allowing former New Zealand captain-turned-commentator Ian Smith to call the famous victory of his compatriots. Some of his other memorable moments he commentated on included Shane Warne's "Ball of the Century", Ian Botham's dominant all-round display during 1981 Ashes, Dennis Lillee overtaking Benaud's record for most wickets, and subsequent 300th and 310th wickets, and Andrew Symonds' tackle on a streaker.
The idea for what became his trademark—wearing a cream or white jacket during live commentary—came from Channel 9 owner Kerry Packer, who suggested the look to help Benaud stand out from the rest of the commentary team.
He also helped to design a computer-based parody of himself available for download off Channel 4's website called "Desktop Richie". It was developed by the software company Turtlez Ltd. Having downloaded this, cricket fans would be treated to live Test match updates and weather reports from a cartoon version of Benaud with real voice samples such as "Got 'im!" and "That's stumps ... and time for a glass of something chilled". On Channel 4's live commentary, Benaud often made sarcastic comments regarding the advertisement of Desktop Richie.
In 2004, Benaud starred in a series of television advertisements for the Australian Tourism Commission, aimed at promoting Australia as a tourist destination. Benaud's ad featured him in various scenic locations uttering his signature comment, "Marvellous!". It was also emulated by New Zealand broadcaster John Campbell. He also appeared in "Richie Benaud's Greatest XI", a video in which he chooses his own team.
Benaud became a staunch advocate of cricket being available on free-to-view TV. He chose to end his British commentary career, which spanned more than 42 years, when the rights to broadcast live Test match cricket were lost by Channel 4 to the subscription broadcaster British Sky Broadcasting. Thus, the 2005 Ashes series was the last that Benaud commentated on in Britain. His final commentary came near the end of the final day of the Fifth Test at the Oval. His last goodbye was interrupted by Glenn McGrath taking Kevin Pietersen's wicket; Benaud simply wove his description of the dismissal into what he was already saying. Benaud stated he would spend the Northern Hemisphere summer in Britain writing, and would continue working for the Nine Network in Australia.
Benaud commentated for the BBC TV highlights of the 2006–07 Ashes in Australia as part of his continuing commentary work for Australia's Nine Network.
Benaud's distinctive speaking style has been frequently parodied on the Australian comedy series "Comedy Inc." and The Twelfth Man. In the case of the latter, comedian Billy Birmingham's impersonations of Benaud on The Twelfth Man comedy recordings have become very successful, spanning more than twenty years. Chris Barrie of "Red Dwarf" fame also incorporated impressions of Benaud into his stand-up repertoire.
On 18 February 2009, during a radio interview, Benaud announced that he would be retiring from television commentary. Benaud said: "I'll be doing Australian cricket next year—2010—but I don't do any television at all anywhere else now and when I finish next year, then I'll be doing other things ... But that'll be no more television commentary".
It was announced on 15 November 2009, that Benaud had signed a three-year contract with the Nine Network to continue being part of their cricket coverage until 2013, although his role would change from that of ball-by-ball commentary. Benaud said: "I won't be doing live commentary any more." Someone asked me, "Does that mean you'll never again go into the commentary box?", "Well, the answer to that", Benaud replied, "If there is, as there always can be, some emergency or a sensational happening on or off the field where it would be quite ridiculous not to go into the commentary box, of course I'll be in there doing my job and doing it as professionally as I can. But I won't be on the live commentary roster. But I will be doing all sorts of, what I regard as, interesting things for Channel Nine on the cricket—special features on the cricket ...". Richie commentated regularly during the 2011–12 season and was part of Nine's commentating team/roster.
Personal life.
Benaud married Marcia Lavender in 1953 and had two sons, Greg and Jeffery, from this marriage; he divorced Marcia in 1967. In 1967, he married his second wife, Daphne Surfleet, who had worked for the English cricket writer E. W. Swanton. Benaud and Daphne often stayed at their holiday home in Beaulieu-sur-Mer on the French Riviera.
On 29 October 2008, Benaud's mother Irene died, aged 104. He said of his mother, "She improved my love of vegetables by introducing the phrase, 'You can't go out and play cricket until you have eaten all your vegetables.'"
In October 2013 Benaud crashed his vintage 1965 Sunbeam Alpine into a wall while driving near his home in Coogee, a beachside suburb in Sydney's east. He sustained a cracked sternum and shoulder injuries. Slow recovery meant he was unable to commentate for Australia's Channel Nine during the 2013–14 Ashes series.
Benaud had last handed 'Baggy green' caps to Simon Katich and Mitchell Starc when they made their test debut, but Benaud's own was lost early in his test career, and former captain, now commentator and Director of Cricket Australia, Mark Taylor was to present the replacement cap to him at the semi-final of the 2015 Cricket World Cup between Australia and India at the SCG, but Benaud was too unwell to attend, and when the cap arrived at Channel 9 headquarters, it was the day before Benaud died. It was presented to his wife.
Death.
In November 2014, at age 84, Benaud announced that he had been diagnosed with skin cancer . He died in his sleep on 10 April 2015.
Prime Minister Tony Abbott offered his family a state funeral but his widow, Daphne, declined, respecting his wishes for a private funeral. Benaud was buried on 15 April, in a private funeral ceremony attended only by his immediate family. Later that same day, there was a commemoration service officiated by former team-mate turned lay preacher Brian Booth; attendees included his family and close friends, among them former players Shane Warne and Ian Chappell, and then test captain Michael Clarke.
Recognition.
Benaud was made an Officer of the Order of the British Empire (OBE) in 1961 for services to cricket. He was inducted into the Sport Australia Hall of Fame in 1985. In 1999 he was awarded a Logie Award for Most Outstanding Sports Broadcaster. In 2007, he was inducted into the Australian Cricket Hall of Fame at the Allan Border Medal award evening and in 2009 he was inducted into the ICC Cricket Hall of Fame. In November 2015, Benaud became an honouree at Bradman Foundation, having been a long-serving patron in his life. Despite rain interrupted the 2016 SCG test against West Indies, the second day has unofficially become Richie Benaud Day as 501 Benaud impersonators stayed at SCG, which is a day before the annual Jane McGrath Day, a day for Breast Cancer awareness and fundraising, which was again rained out.　
Books.
Benaud wrote a number of books:

</doc>
<doc id="26480" url="https://en.wikipedia.org/wiki?curid=26480" title="Radio Research Project">
Radio Research Project

The Radio Research Project was a social research project funded by the Rockefeller Foundation to look into the effects of mass media on society 1Superscript text.
In 1937, the Rockefeller Foundation started funding research to find the effects of new forms of mass media on society, especially radio. Several universities joined up and a headquarters was formed at the School of Public and International Affairs at Princeton University. The following people were involved:
Among the subjects of the Project's first studies were soap operas, known as radio dramas at the time.
The Radio Project also conducted research on the infamous Halloween broadcast of "The War of the Worlds" in 1938. Of the estimated 6 million people who heard this broadcast, they found that 25% accepted the program's reports of mass destruction. The majority of these did not think they were hearing a literal invasion from Mars, but rather an attack by Germany. The researchers determined that radio broadcasts from the Munich Crisis may have lent credence to this supposition.
A third research project was that of listening habits. Because of this, a new method was developed to survey an audience – this was dubbed the Little Annie Project. The official name was the Stanton-Lazarsfeld Program Analyzer. This allowed one not only to find out if a listener liked the performance, but how they felt at any individual moment, through a dial which they would turn to express their preference (positive or negative). This has since become an essential tool in focus group research.
Theodor Adorno produced numerous reports on the effects of "atomized listening" which radio supported and of which he was highly critical. However, because of profound methodological disagreements with Lazarsfeld over the use of techniques such as listener surveys and "Little Annie" (Adorno thought both grossly simplified and ignored the degree to which expressed tastes were the result of commercial marketing), Adorno left the project in 1941.

</doc>
<doc id="26484" url="https://en.wikipedia.org/wiki?curid=26484" title="Religious pluralism">
Religious pluralism

Religious pluralism is an attitude or policy regarding the diversity of religious belief systems co-existing in society. It can indicate one or more of the following:
Definition and scope.
Religious pluralism, to paraphrase the title of a recent academic work, goes beyond mere toleration. Chris Beneke, in "Beyond Toleration: The Religious Origins of American Pluralism", explains the difference between religious tolerance and religious pluralism by pointing to the situation in the late 18th century United States. By the 1730s, in most colonies religious minorities had obtained what contemporaries called religious toleration: "The policy of toleration relieved religious minorities of some physical punishments and some financial burdens, but it did not make them free from the indignities of prejudice and exclusion. Nor did it make them equal. Those 'tolerated' could still be barred from civil offices, military positions, and university posts." In short, religious toleration is only the absence of religious persecution, and does not necessarily preclude religious discrimination. However, in the following decades something extraordinary happened in the Thirteen Colonies, at least if one views the events from "a late eighteenth-century perspective". Gradually the colonial governments expanded the policy of religious toleration, but then, between the 1760s and the 1780s, they replaced it with "something that is usually called religious liberty". Mark Silka, in "Defining Religious Pluralism in America: A Regional Analysis", states that Religious pluralism "enables a country made up of people of different faiths to exist without sectarian warfare or the persecution of religious minorities. Understood differently in different times and places, it is a cultural construct that embodies some shared conception of how a country's various religious communities relate to each other and to the larger nation whole."
Religious pluralism can be defined as "respecting the otherness of others". Freedom of religion encompasses all religions acting within the law in a particular region. Exclusivist religions teach that theirs is the only way to salvation and to religious truth, and some of them would even argue that it is necessary to suppress the falsehoods taught by other religions. Some Protestant sects argue fiercely against Roman Catholicism, and fundamentalist Christians of all kinds teach that religious practices like those of paganism and witchcraft are pernicious. This was a common historical attitude prior to the Enlightenment, and has appeared as governmental policy into the present day under systems like Afghanistan's Taliban regime, which destroyed the ancient Buddhas of Bamyan.
Giving one religion or denomination special rights that are denied to others can weaken religious pluralism. This situation was observed in Europe through the Lateran Treaty and Church of England. In modern era, many Islamic countries have laws that criminalize the act of leaving Islam to someone born in Muslim family, forbid entry to non-Muslims into Mosques, and forbid construction of Church, Synagogue or Temples inside their countries.
Relativism, the belief that all religions are equal in their value and that none of the religions give access to absolute truth, is an extreme form of inclusivism. Likewise, syncretism, the attempt to take over creeds of practices from other religions or even to blend practices or creeds from different religions into one new faith is an extreme form of inter-religious dialogue. Syncretism must not be confused with ecumenism, the attempt to bring closer and eventually reunite different denominations of one religion that have a common origin but were separated by a schism.
History.
Cultural and religious pluralism has a long history and development that reaches from antiquity to contemporary trends in post-modernity.
Feuerbauch and Ernst Troeltsch concluded that Asian religious traditions, in particular Hinduism and Buddhism were earliest proponents of religious pluralism and granting of freedom to the individual to choose the faith and develop a personal religious construct within it. Jainism, another ancient Indian religion, as well as Daoism have also always been inclusively flexible and have long favored religious pluralism for those who disagree with their religious viewpoints. The Age of Enlightenment in Europe triggered a sweeping transformation about religion, segregation of state and religion, with rising acceptance of religious pluralism. These pluralist trends in Western thought, particularly since the 18th century, brought mainstream Christianity and Judaism closer to the Asian traditions of philosophical pluralism, states Chad Meister.
Bahá'í Faith.
Bahá'u'lláh, founder of Bahá'í Faith, a religion that developed in Persia, though not a sect of Islam, urged the elimination of religious intolerance. He taught that God is one, and has manifested himself to humanity through several historic messengers. Bahá'u'lláh taught that Bahá'ís must associate with peoples of all religions, showing the love of God in relations with them, whether this is reciprocated or not.
Bahá'í's refer to the concept of Progressive revelation, which means that God's will is revealed to mankind progressively as mankind matures and is better able to comprehend the purpose of God in creating humanity. In this view, God's word is revealed through a series of messengers: Abraham, Krishna, Moses, Buddha, Jesus, Mohammed, and Bahá'u'lláh (the founder of the Bahá'í Faith) among them. In the "Kitáb-i-Íqán" ("Book of Certitude"), Bahá'u'lláh explains that messengers of God have a twofold station, one of divinity and one of an individual. According to Bahá'í writings, there will not be another messenger for many hundreds of years. There is also a respect for the religious traditions of the native peoples of the planet who may have little other than oral traditions as a record of their religious figures.
Buddhism.
The earliest reference to Buddhist views on religious pluralism in a political sense is found in the Edicts of Emperor Ashoka:
All religions should reside everywhere, for all of them desire self-control and purity of heart. Rock Edict Nb7 (S. Dhammika)
Contact (between religions) is good. One should listen to and respect the doctrines professed by others. Beloved-of-the-Gods, King Piyadasi, desires that all should be well-learned in the good doctrines of other religions. Rock Edict Nb12 (S. Dhammika)
When asked, "Don’t all religions teach the same thing? Is it possible to unify them?" the Dalai Lama said:
People from different traditions should keep their own, rather than change. However, some Tibetan may prefer Islam, so he can follow it. Some Spanish prefer Buddhism; so follow it. But think about it carefully. Don’t do it for fashion. Some people start Christian, follow Islam, then Buddhism, then nothing.
In the United States I have seen people who embrace Buddhism and change their clothes! Like the New Age. They take something Hindu, something Buddhist, something, something… That is not healthy.
For individual practitioners, having one truth, one religion, is very important. Several truths, several religions, is contradictory.
I am Buddhist. Therefore, Buddhism is the only truth for me, the only religion. To my Christian friend, Christianity is the only truth, the only religion. To my Muslim friend, is the only truth, the only religion. In the meantime, I respect and admire my Christian friend and my Muslim friend. If by unifying you mean mixing, that is impossible; useless.
Classical Greek and Roman paganism.
Ancient Greeks employed "Interpretatio Graeca" whereby the gods of other religions were equated with those of their own pantheon. The Romans easily accomplished this task by subsuming the entire set of gods from other faiths into their own religion. This was done on rare occasion by adding a new god to their own pantheon; on most occasions they identified another religion's gods with their own.
Because divinity was the basis for the mandate of the state, atheism was considered a capital crime in both ancient Greece and Rome. This was further solidified as the "status quo" following the installation of the Roman imperial cult of deification of sitting emperors, and Roman pluralism was not officially extended to Christianity until the Edict of Milan in 313 CE.
Christianity.
Some Christians have argued that religious pluralism is an invalid or self-contradictory concept. Maximal forms of religious pluralism claim that all religions are equally true, or that one religion can be true for some and another for others. Some Christians hold this idea to be logically impossible from the Principle of contradiction.
Other Christians have held that there can be truth value and salvific value in other faith traditions. John Macquarrie, described in the "Handbook of Anglican Theologians" (1998) as "unquestionably Anglicanism's most distinguished systematic theologian in the second half of the twentieth century", wrote that "there should be an end to proselytizing but that equally there should be no syncretism of the kind typified by the Baha'i movement" (p. 2). In discussing 9 founders of major faith traditions (Moses, Zoroaster, Lao-zu, Buddha, Confucius, Socrates, Krishna, Jesus, and Muhammad), which he called "mediators between the human and the divine", Macquarrie wrote that:
I do not deny for a moment that the truth of God has reached others through other channels - indeed, I hope and pray that it has. So while I have a special attachment to one mediator, I have respect for them all. (p. 12)
The Church of Jesus Christ of Latter-day Saints also teaches a form of religious pluralism, that there is at least some truth in almost all religions and philosophies.
Classical Christian views.
Before the Great Schism, mainstream Christianity confessed "one holy catholic and apostolic church", in the words of the Nicene Creed. Roman Catholics, Orthodox Christians, Episcopalians and most Protestant Christian denominations still maintain this belief.
Church unity was something very visible and tangible, and schism was just as serious an offense as heresy. Following the Great Schism, Roman Catholicism sees and recognizes the Orthodox Sacraments as valid. Eastern Orthodoxy does not have the concept of "validity" when applied to Sacraments, but it considers the "form" of Roman Catholic Sacraments to be acceptable, if still devoid of actual spiritual content. Both generally regard each other as "heterodox" and "schismatic", while continuing to recognize each other as Christian. (See ecumenicism).
Modern Christian views.
Some Protestants hold that only believers who believe in certain fundamental doctrines know the true pathway to salvation. The core of this doctrine is that Jesus Christ was a perfect man, is the Son of God and that he died and rose again for the wrongdoing of those who will accept the gift of salvation. They continue to believe in "one" church, believing in fundamental issues there is unity and non-fundamental issues there is liberty. Some evangelicals are doubtful if Roman Catholicism or Eastern Orthodoxy are still valid manifestations of the Church and usually reject religious (typically restorationist) movements rooted in 19th century American Christianity, such as Mormonism, Christian Science, or Jehovah's Witnesses as not distinctly Christian.
Hinduism.
Hinduism is naturally pluralistic. A well-known "Rig Vedic" hymn says: "Truth is One, though the sages know it variously" ("Ékam sat vipra bahudā vadanti"). Similarly, in the "Bhagavad Gītā" (4:11), God, manifesting as an incarnation, states: "As people approach me, so I receive them. All paths lead to me" ("ye yathā māṃ prapadyante tāṃs tathāiva bhajāmyaham mama vartmānuvartante manuṣyāḥ pārtha sarvaśaḥ"). The Hindu religion has no theological difficulties in accepting degrees of truth in other religions. Hinduism emphasizes that everyone actually worships the same God, whether one knows it or not. Just as Hindus worshiping Ganesh is seen as valid by those worshiping Vishnu, so someone worshiping Jesus or Allah is accepted. Many "foreign" deities become assimilated into Hinduism, and some Hindus may sometimes offer prayers to Jesus along with their traditional forms of God.
Islam.
Religious pluralism is a controversial subject in Islam. The primary sources that guide Islam, namely Quran and Hadiths, offer contradictory positions on religious pluralism. Some verses support religious pluralism, while others discourage it. The acceptability of religious pluralism within Islam remains a topic of active debate.
Although it seems like a debate today in Islamic world against the favor of the topic, one of the most important building blocks of Islam, the Quran, has some parts mentioning and recognizing the cultural pluralism which might be examplified in the verse: ""Another of His signs is the creation of the heavens and earth, and the diversity of your languages and colors"(30:22)." This is a clear sign of diversity acknowledgment in the Quran. Adding to that, it is stated in the verse “"And We did not send any messenger except in the language of his people to state clearly for them, and Allah sends astray [thereby whom He wills and guides whom He wills. And He is the Exalted in Might, the Wise"(14:4)"."”that all Prophets and Messengers were sent to their people to tell them about the Islam in their own language which is also a clear indication of diversity both stated in the book and applied in real world. Giving a really effective emphasis on diversity in the eyes of the creator, the Allah in Islam, the verse ""If God had so willed, He would have made you one community"...(5:48)." clearly shows the existence of pluralism in Islam with the aspects of culture, economy, politics, etc. are the core of human civilization in all times.
In several Surah, Quran asks Muslims to remain steadfast with Islam, and not yield to the vain desires of other religions and unbelievers. These verses have been interpreted to imply pluralism in religions. For example, Surah Al-Ma'idah verses 47 through 49 state:
Surah Al-Ankabut verse 45 through 47 state:
Surah Al-E-Imran verses 62 through 66 state:
Surah Al-Kafiroon verse 1 through 6 state:
Several verses of the Quran have been misinterpreted to imply that Islam rejects religious pluralism. For example, Surah Al-Tawba verse 1 through 5 seems to command the Muslim to slay the pagans (with verse 9.5 called the 'sword verse'): 
However, this verse has been explained.
Bernard Lewis presents some of his conclusions about Islamic culture, Shari'a law, jihad, and the modern day phenomenon of terrorism in his text, "Islam: The Religion and the People". He writes of jihad as a distinct "religious obligation", but suggests that "it is a pity" that people engaging in terrorist activities are not more aware of their own religion:
In Surah Al-Tawba, verse 29 demands Muslims to fight all those who do not believe in Islam, including Christians and Jews (People of the Book), until they pay the Jizya, a tax, with willing submission.
Some people have falsely concluded from verse 9:29, that Muslims are commanded to attack all non-Muslims until they pay money. In fact, such an interpretation is completely false and contradicts authentic Islamic teachings. Commenting on this verse, Shaykh Jalal Abualrub writes:
In Surah Al-Nisa, verse 89 has been misquoted to seem that it says to slay the apostates. In actuality, it only commands Muslims to fight those who practice oppression or persecution, or attack the Muslims.
In Surah Al-Bayyina verses 6 through 7 calls People of the Book and Polytheists who reject truth revealed by Islam, the worst of creatures, but the verse has very little to do with the subject of Religious Pluralism:
Sufism.
The Sufis were practitioners of the esoteric mystic traditions within an Islam at a certain point. Sufism is defined by the Sufi master or Pir (Sufism) or fakeer or Wali in the language of the people by dancing and singing and incorporating various philosophies, theologies, ideologies and religions together (e.g., Christiainity, Judaism, Paganism, Platonism, Zoroastrianism, Buddhism, Hinduism, Sikhism and so forth with time). Famous Sufi masters are Rumi, Shadhili, Sheikh Farid, Bulleh Shah, Shah Hussain, Shams Tabrizi, Waris Shah, Ghazali, Mian Mir, Attar of Nishapur, Amir Khusrow, Salim Chishti. See many more famous Sufis at the List of Sufis. The Sufis were considered by many to have divine revelations with messages of peace, tolerance, equality, pluralism, love for all and hate for no one, humanitarians, philosophers, psychologists and much more. Many had the teaching if you want to change the world, change yourself and you will change the whole world. The views of the Sufi poets, philosophers and theologians have inspired multiple forms of modern-day academia as well as philosophers of other religions. See also Blind men and an elephant. But undoubtedly, the most influential Sufi scholar to have embraced the world is Jalaluddin Muhammad Rumi. He was born in 1207 AD in a northern province of Afghanistan, however, he later had to seek refuge in Turkey following the invasion of Afghanistan by Mongols. Rumi, through his poetry and teachings, propagated inter-faith harmony like none other. He served as a uniting figure for people of different faiths and his followers included Muslims, Christians and Jews. Even today, Rumi’s popularity does not cease to exist within the Sufi Muslim community and his message of peace and harmony transcends religious and geographical boundaries.
Rumi says:
I looked for God. I went to a temple, and I didn't find him there. Then I went to a church, and I didn't find him there. And then I went to a mosque, and I didn't find him there. And then finally I looked in my heart, and there he was. 
Rumi also says:
How many paths are there to God? There are as many paths to God as there are souls on the Earth.
Rumi also says:
A true Lover doesn't follow any one religion,
be sure of that.
Since in the religion of Love,
there is no irreverence or faith.
When in Love,
body, mind, heart and soul don't even exist.
Become this,
fall in Love,
and you will not be separated again.
Ahmadiyya.
Ahmadis recognize many founders of world religions to be from God, who all brought teaching and guidance from God to all peoples. According to the Ahmadiyya understanding of the Quran, every nation in the history of mankind has been sent a prophet, as the Quran states: "And there is a guide for every people". Though the Quran mentions only 24 prophets, the founder of Islam, Muhammad states that the world has seen 124,000 prophets. Thus other than the prophets mentioned in the Quran, Ahmadis, with support from theological study also recognize Buddha, Krishna, founders of Chinese religions to be divinely appointed individuals.
The Second Khalifatul Maish of the Ahmadiyya Muslim Community writes:
"According to this teaching there has not been a single people at any time in history or anywhere in the world who have not had a warner from God, a teacher, a prophet. According to the Quran there have been prophets at all times and in all countries. India, China, Russia, Afghanistan, parts of Africa, Europe, America—all had prophets according to the theory of divine guidance taught by the Quran. When, therefore, Muslims hear about prophets of other peoples or other countries, they do not deny them. They do not brand them as liars. Muslims believe that other peoples have had their teachers. If other peoples have had prophets, books, and laws, these constitute no difficulty for Islam."
Mirza Ghulam Ahmad, founder of the Ahmadiyya Muslim Community wrote in his book "A Message of Peace":
"Our God has never discriminated between one people and another. This is illustrated by the fact that all the potentials and capabilities (Prophets) which have been granted to the Aryans (Hindus) have also been granted to the races inhabiting Arabia, Persia, Syria, China, Japan, Europe and America."
In modern practice.
Religious pluralism is a contested issue in modern Islamic countries. Twenty three (23) Islamic countries have laws, as of 2014, which make it a crime, punishable with death penalty or prison, for a Muslim, by birth or conversion, to leave Islam or convert to another religion. In Muslim countries such as Algeria, it is illegal to preach, persuade or attempt to convert a Muslim to another religion. Saudi Arabia and several Islamic nations have strict laws against the construction of Christian churches, Jewish synagogues, Hindu temples and Buddhist stupas anywhere inside the country, by anyone including minorities working there. Brunei in southeast Asia adopted Sharia law in 2013 that prescribes a death penalty for any Muslim who converts from Islam to another religion. Other Islamic scholars state Sharia does not allow non-Muslim minorities to enjoy religious freedoms in a Muslim-majority nation, but other scholars disagree.
Jainism.
"Anekāntavāda", the principle of relative pluralism, is one of the basic principles of Jainism. In this view, the truth or the reality is perceived differently from different points of view, and no single point of view is the complete truth. Jain doctrine states that an object has infinite modes of existence and qualities and they cannot be completely perceived in all its aspects and manifestations, due to inherent limitations of the humans. Only the Kevalins—the omniscient beings—can comprehend the object in all its aspects and manifestations, and all others are capable of knowing only a part of it. Consequently, no one view can claim to represent the absolute truth. Jains compare all attempts to proclaim absolute truth with " andhgajnyaya " or the "maxim of the blind men and elephant", wherein all the blind men claimed to explain the true appearance of the elephant, but could only partly succeed due to their narrow perspective.
Judaism.
The Mosaic law categorically warns the Jews to refrain from polytheism. First and the second commandment, you shall not have another God except me, worship your God with all your heart and with all your soul. Throughout the Hebrew Bible the sovereignty of Yahweh as the only God is the key pillar of a chosen community of Israel.
Sikhism.
The Sikh Gurus (religious leaders) have propagated the message of "many paths" leading to the one God and ultimate salvation for all souls who treading on the path of righteousness. They have supported the view that proponents of all faiths, by doing good and virtuous deeds and by remembering the Lord, can certainly achieve salvation. Sikhs are told to accept all leading faiths as possible vehicles for attaining spiritual enlightenment, provided the faithful study, ponder and practice the teachings of their prophets and leaders. Sikhism had many interactions with Sufism as well as Hinduism, influenced them and was influenced by them. See Islam and Sikhism and Hinduism and Sikhism.
The holy book of the Sikhs (the Sri Guru Granth Sahib) says:
Do not say that the Vedas and the Koran (semitic books i.e. Bible, Torah and Q'uran) are false. Those who do not contemplate them are false. (Guru Granth Sahib page 1350) 
As well as:
Some call the Lord "Ram, Ram", and some "Khuda". Some serve Him as "Gusain", others as "Allah". He is the Cause of causes, and Generous. He showers His Grace and Mercy upon us. Some pilgrims bathe at sacred shrines, others go on Hajj to Mecca. Some do devotional worship, whilst others bow their heads in prayer. Some read the Vedas, and some the Koran. Some wear blue robes, and some wear white. Some call themselves Muslim, and some call themselves Hindu. Some yearn for paradise, and others long for heaven. Says Nanak, one who realizes the Hukam of God's Will, knows the secrets of his Lord Master. (Sri Guru Granth Sahib Page:885) 
One who recognizes that all spiritual paths lead to the One shall be emancipated. One who speaks lies shall fall into hell and burn. In all the world, the most blessed and sanctified are those who remain absorbed in Truth. (SGGS Ang 142) 
The seconds, minutes, and hours, days, weeks and months and various seasons originate from One Sun; O nanak, in just the same way, the many forms originate from the Creator. (Guru Granth Sahib page 12,13)
The Guru Granth Sahib also says that Bhagat Namdev and Bhagat Kabir, who were both believed to be Hindus, both attained salvation though they were born before Sikhism took root and were clearly not Sikhs. This highlights and reinforces the Guru's saying that "peoples of other faiths" can join with God as true and also at the same time signify that Sikhism is not the exclusive path for liberation.
Additionally the Guru Granth Sahib says:
First, Allah (God) created the Light; then, by His Creative Power, He made all mortal beings. From the One Light, the entire universe welled up. So who is good, and who is bad? ||1|| 
Again, the Guru Granth Sahib Ji provides this verse:
Naam Dayv the printer, and Kabeer the weaver, obtained salvation through the Perfect Guru. Those who know God and recognize His Shabad ("word") lose their ego and class consciousness. (Guru Granth Sahib page 67) 
Most of the 15 Sikh Bhagats who are mentioned in their holy book were non-Sikhs and belonged to Hindu and Muslim faiths, which were the most prevalent religions of this region.
The pluralistic dialogue of Sikhism began with the founder of Sikhism Guru Nanak after becoming enlightened saying the words "Na koi hindu na koi musalman" - "There is no Hindu, there is no Muslim". He recognised that religious labels held no value and it is the deeds of human that will be judged in the hereafter what we call ourselves religiously holds no value.
Sikhs have been considered eager exponents of interfaith dialogue and will not only accept the right of others to practice their faith but have in the past fought and laid down their lives to protect this right for others. See the sacrifice of the ninth Sikh Guru, Guru Tegh Bahadar who on the final desperate and heart-rending pleas of the Kashmiri Pandit, agreed to put up a fight for their right to practice their religion. He was executed so another religion besides his own could have the freedom to practice their religion against the tyrannic Moghul empire that were forcing people to accept Islam.
Religious pluralism and human service professions.
The concept of religious pluralism is also relevant to human service professions, such as psychology and social work, as well as medicine and nursing, in which trained professionals may interact with clients from diverse faith traditions. For example, psychologist Kenneth Pargament has described four possible stances toward client religious and spiritual beliefs, which he called "rejectionist", "exclusivist", "constructivist", and "pluralist". Unlike the constructivist stance, the pluralist stance:
...recognizes the existence of a religious or spiritual absolute reality but allows for multiple interpretations and paths toward it. In contrast to the exclusivist who maintains that there is a single path "up the mountain of God," the pluralist recognizes many paths as valid. Although both the exclusivist and the pluralist may agree on the existence of religious or spiritual reality, the pluralist recognizes that this reality is expressed in different cultures and by different people in different ways. Because humans are mortal and limited, a single human religious system cannot encompass all of the religious or spiritual absolute reality... (p. 167)
Importantly, "the pluralistic therapist can hold personal religious beliefs while appreciating those of a client with different religious beliefs. The pluralist recognizes that religious value differences can and will exist between counselors and clients without adversely affecting therapy" (p. 168). The stances implied by these four helping orientations on several key issues, such as "should religious issues be discussed in counseling?", have also been presented in tabular form (p. 362, Table 12.1).
The profession of chaplaincy, a religious profession, must also deal with issues of pluralism and the relevance of a pluralistic stance. For example, Friberg (2001) argues: "With growing populations of immigrants and adherents of religions not previously seen in significant numbers in North America, spiritual care must take religion and diversity seriously. Utmost respect for the residents' spiritual and religious histories and orientations is imperative" (p. 182).

</doc>
<doc id="26485" url="https://en.wikipedia.org/wiki?curid=26485" title="Calendar-based contraceptive methods">
Calendar-based contraceptive methods

Calendar-based methods are various methods of estimating a woman's likelihood of fertility, based on a record of the length of previous menstrual cycles. Various methods are known as the Knaus–Ogino Method and the Rhythm Method. The Standard Days Method is also considered a calendar-based method, because when using it, a woman tracks the days of her menstrual cycle without observing her physical fertility signs. The Standard Days Method is based on a fixed formula taking into consideration the timing of ovulation, the functional life of the sperm and the ovum, and the resulting likelihood of pregnancy on particular days of the menstrual cycle. These methods may be used to achieve pregnancy by timing unprotected intercourse for days identified as fertile, or to avoid pregnancy by avoiding unprotected intercourse during fertile days.
The first formalized calendar-based method was developed in 1930 by John Smulders, a Roman Catholic physician from the Netherlands. It was based on knowledge of the menstrual cycle. This method was independently discovered by Hermann Knaus (Austria), and Kyusaku Ogino (Japan). This system was a main form of birth control available to Catholic couples for several decades, until the popularization of symptoms-based fertility awareness methods. A new development in calendar-based methods occurred in 2002, when Georgetown University introduced the Standard Days Method. The Standard Days Method is promoted in conjunction with a product called CycleBeads, a ring of colored beads which are meant to help the user keep track of her fertile and non-fertile days.
Terminology.
Some sources may treat the terms "rhythm method" and "fertility awareness" as synonymous. However, fertility awareness is usually used as a broad term that includes tracking basal body temperature and cervical mucus as well as cycle length. The World Health Organization considers the rhythm method to be a specific type of calendar-based method, and calendar-based methods to be only one form of fertility awareness.
More effective than calendar-based methods, systems of fertility awareness that track basal body temperature, cervical mucus, or both, are known as symptoms-based methods. Teachers of symptoms-based methods take care to distance their systems from the poor reputation of the rhythm method. Many consider the rhythm method to have been obsolete for at least 20 years, and some even exclude calendar-based methods from their definition of fertility awareness.
Some sources may treat the terms "rhythm method" and "natural family planning" as synonymous. In the early 20th century, the calendar-based method known as the "rhythm method" was promoted by members of the Roman Catholic Church as the only morally acceptable form of family planning. Methods accepted by this church are referred to as natural family planning (NFP): so at one time, the term "the rhythm method" was synonymous with NFP. Today, NFP is an umbrella term that includes symptoms-based fertility awareness methods and the lactational amenorrhea method as well as calendar-based methods such as rhythm. This overlap between uses of the terms "the rhythm method" and "natural family planning" may contribute to confusion. 
The term "the rhythm method" is sometimes used, in error, to describe the behavior of any people who have unprotected vaginal intercourse, yet wish to avoid pregnancy.
The first day of bleeding is considered day one of the menstrual cycle.
History.
Early methods.
It is not known if historical cultures were aware of what part of the menstrual cycle is most fertile. In the year 388, Augustine of Hippo wrote of periodic abstinence. Addressing followers of Manichaeism, his former religion, he said, ""Is it not you who used to counsel us to observe as much as possible the time when a woman, after her purification, is most likely to conceive, and to abstain from cohabitation at that time...?"" If the Manichaieans practiced something like the Jewish observances of menstruation, then the "time... after her purification" would have indeed been when "a woman... is most likely to conceive." Over a century previously, however, the influential Greek physician Soranus had written that ""the time directly before and after menstruation"" was the most fertile part of a woman's cycle; this inaccuracy was repeated in the 6th century by the Byzantine physician Aëtius. Similarly, a Chinese sex manual written close to the year 600 stated that only the first five days following menstruation were fertile. Some historians believe that Augustine, too, incorrectly identified the days immediately after menstruation as the time of highest fertility.
Written references to a "safe period" do not appear again for over a thousand years. Scientific advances prompted a number of secular thinkers to advocate periodic abstinence to avoid pregnancy: in the 1840s it was discovered that many animals ovulate during estrus. Because some animals (such as dogs) have a bloody discharge during estrus, it was assumed that menstruation was the corresponding most fertile time for women. This inaccurate theory was popularized by physicians Bischoff, Félix Archimède Pouchet, and Adam Raciborski. In 1854, an English doctor named George Drysdale correctly taught his patients that the days near menstruation are the "least" fertile, but this remained the minority view for the remainder of the 19th century.
Knaus–Ogino or rhythm method.
In 1905 Theodoor Hendrik van de Velde, a German gynecologist, showed that women only ovulate once per menstrual cycle. In the 1920s, Kyusaku Ogino, a Japanese gynecologist, and Hermann Knaus, from Austria, working independently, each made the discovery that ovulation occurs about fourteen days before the next menstrual period. Ogino used his discovery to develop a formula for use in aiding infertile women to time intercourse to achieve pregnancy.
In 1930, Johannes Smulders, a Roman Catholic physician from the Netherlands, used Knaus and Ogino's discoveries to create a method for "avoiding" pregnancy. Smulders published his work with the Dutch Roman Catholic medical association, and this was the official rhythm method promoted over the next several decades. In 1932 a Catholic physician, Dr. Leo J Latz, published a book titled "The Rhythm of Sterility and Fertility in Women" describing the method, and the 1930s also saw the first U.S. Rhythm Clinic (founded by John Rock) to teach the method to Catholic couples.
Later 20th century to present.
In the first half of the 20th century, most users of the rhythm method were Catholic; they were following their church's teaching that all other methods of birth control were sinful. In 1968 the encyclical "Humanae vitae" included the statement, "It is supremely desirable... that medical science should by the study of natural rhythms succeed in determining a sufficiently secure basis for the chaste limitation of offspring." This is interpreted as favoring the then-new, more reliable symptoms-based fertility awareness methods over the rhythm method. Currently, many fertility awareness teachers consider the rhythm method to have been obsolete for at least 20 years.
New attention was drawn to calendar-based methods in 2002, when the Institute for Reproductive Health at Georgetown University introduced the Standard Days Method. Designed to be simpler to teach and use than the older rhythm method, the Standard Days Method is being successfully integrated into family planning programs worldwide.
Types and effectiveness.
Most menstrual cycles have several days at the beginning that are infertile (pre-ovulatory infertility), a period of fertility, and then several days just before the next menstruation that are infertile (post-ovulatory infertility). The first day of red bleeding is considered day one of the menstrual cycle. To use these methods, a woman is required to know the length of her menstrual cycles.
Imperfect use of calendar-based methods would consist of not correctly tracking the length of the woman's cycles, thus using the wrong numbers in the formula, or of having unprotected intercourse on an identified fertile day. The discipline required to keep accurate records of menstrual cycles, and to abstain from unprotected intercourse, makes imperfect use fairly common. The typical-use failure rate of calendar-based methods is 25% per year.
Rhythm method (Knaus–Ogino method).
To find the estimated length of the pre-ovulatory infertile phase, nineteen (19) is subtracted from the length of the woman's shortest cycle. To find the estimated start of the post-ovulatory infertile phase, ten (10) is subtracted from the length of the woman's longest cycle. A woman whose menstrual cycles ranged in length from 30 to 36 days would be estimated to be infertile for the first 11 days of her cycle (30-19=11), to be fertile on days 12-25, and to resume infertility on day 26 (36-10=26). When used to avoid pregnancy, the rhythm method has a perfect-use failure rate of up to 9% per year.
Standard Days Method.
Developed by Georgetown University's Institute for Reproductive Health, the Standard Days Method has a simpler rule set and is more effective than the rhythm method. A product called CycleBeads was developed alongside the method to help the user keep track of estimated high and low fertility points during her menstrual cycle. The Standard Days Method may only be used by women whose cycles are usually between 26 and 32 days in length. In this system:
When used to avoid pregnancy, the Standard Days Method has perfect-use efficacy of 95+% and typical-use efficacy of 88%.
Software-based systems.
Several Web-based implementations of the cycle method are known.
Advantages.
The Standard Days method (SDM) is increasingly being introduced as part of family planning programs in developing countries. The method is satisfactory for many women and men; offering it through family planning centers results in a significant increase in contraceptive use among couples who do not want to become pregnant. The low cost of the method may also enable it to have a significant positive impact in countries that lack funding to provide other methods of birth control.
Potential concerns.
Failure rate.
One concern related to the use of calendar-based methods is their relatively high failure rate, compared to other methods of birth control. Even when used perfectly, calendar-based methods, especially the rhythm method, result in a high pregnancy rate among couples intending to avoid pregnancy. Of commonly known methods of birth control, only the cervical cap and contraceptive sponge have comparably high failure rates. This lower level of reliability of calendar-based methods is because their formulas make several assumptions that are not always true.
The postovulatory (luteal) phase has a normal length of 12 to 16 days, and the rhythm method formula assumes all women have luteal phase lengths within this range. However, many women have shorter luteal phases, and a few have longer luteal phases. For these women, the rhythm method formula incorrectly identifies a few fertile days as being in the infertile period.
Calendar-based methods use records of past menstrual cycles to predict the length of future cycles. However, the length of the pre-ovulatory phase can vary significantly, depending on the woman's typical cycle length, stress factors, medication, illness, menopause, breastfeeding, and whether she is just coming off hormonal contraception. If a woman with previously regular cycles has a delayed ovulation due to one of these factors, she will still be fertile when the method tells her she is in the post-ovulatory infertile phase. If she has an unusually early ovulation, calendar-based methods will indicate she is still in the pre-ovulatory infertile phase when she has actually become fertile. 
Finally, calendar-based methods assume that all bleeding is true menstruation. However, mid-cycle or anovulatory bleeding can be caused by a number of factors. Incorrectly identifying bleeding as menstruation will cause the method's calculations to be incorrect.
Embryonic health.
It has been suggested that unprotected intercourse in the infertile periods of the menstrual cycle may still result in conceptions, but create embryos incapable of implanting.
It has also been suggested that pregnancies resulting from method failures of periodic abstinence methods are at increased risk of miscarriage and birth defects due to aged gametes at the time of conception. Other research suggests that timing of conception has no effect on miscarriage rates, low birth weight, or preterm delivery.

</doc>
<doc id="26487" url="https://en.wikipedia.org/wiki?curid=26487" title="Roulette">
Roulette

Roulette is a casino game named after the French word meaning "little wheel". In the game, players may choose to place bets on either a single number or a range of numbers, the colors red or black, or whether the number is odd or even.
To determine the winning number and color, a croupier spins a wheel in one direction, then spins a ball in the opposite direction around a tilted circular track running around the circumference of the wheel. The ball eventually loses momentum and falls onto the wheel and into one of 37 (in French/European roulette) or 38 (in American roulette) colored and numbered pockets on the wheel.
History.
The first form of roulette was devised in 18th century France. A century earlier, Blaise Pascal introduced a primitive form of roulette in the 17th century in his search for a perpetual motion machine. The roulette wheel is believed to be a fusion of the English wheel games Roly-Poly, Reiner, Ace of Hearts, and E.O., the Italian board games of Hoca and Biribi, and "Roulette" from an already existing French board game of that name.
The game has been played in its present form since as early as 1796 in Paris. An early description of the roulette game in its current form is found in a French novel "La Roulette, ou le Jour" by Jaques Lablee, which describes a roulette wheel in the Palais Royal in Paris in 1796. The description included the house pockets, "There are exactly two slots reserved for the bank, whence it derives its sole mathematical advantage." It then goes on to describe the layout with, "...two betting spaces containing the bank's two numbers, zero and double zero". The book was published in 1801. An even earlier reference to a game of this name was published in regulations for New France (Québec) in 1758, which banned the games of "dice, hoca, faro, and roulette".
The roulette wheels used in the casinos of Paris in the late 1790s had red for the single zero and black for the double zero. To avoid confusion, the color green was selected for the zeros in roulette wheels starting in the 1800s.
In 1843, in the German spa casino town of Bad Homburg, fellow Frenchmen François and Louis Blanc introduced the single "0" style roulette wheel in order to compete against other casinos offering the traditional wheel with single and double zero house pockets.
In some forms of early American roulette wheels - as shown in the 1886 Hoyle gambling books, there were numbers 1 through 28, plus a single zero, a double zero, and an American Eagle. The Eagle slot, which was a symbol of American liberty, was a house slot that brought the casino extra edge. Soon, the tradition vanished and since then the wheel features only numbered slots. Existing wheels with Eagle symbols are exceedingly rare, with fewer than a half-dozen copies known to exist. Authentic Eagled wheels in excellent condition can fetch tens of thousands of dollars at auction.
According to Hoyle "the single 0, the double 0, and eagle are never bars; but when the ball falls into either of them, the banker sweeps every thing upon the table, except what may happen to be bet on either one of them, when he pays twenty-seven for one, which is the amount paid for all sums bet upon any single figure".
In the 19th century, roulette spread all over Europe and the US, becoming one of the most famous and most popular casino games. When the German government abolished gambling in the 1860s, the Blanc family moved to the last legal remaining casino operation in Europe at Monte Carlo, where they established a gambling mecca for the elite of Europe. It was here that the single zero roulette wheel became the premier game, and over the years was exported around the world, except in the United States where the double zero wheel had remained dominant.
A legend says that François Blanc supposedly bargained with the devil to obtain the secrets of roulette. The legend is based on the fact that the sum of all the numbers on the roulette wheel (from 0 to 36) is 666, which is the "Number of the Beast".
In the United States, the French double zero wheel made its way up the Mississippi from New Orleans, and then westward. It was here, because of rampant cheating by both operators and gamblers, that the wheel was eventually placed on top of the table to prevent devices being hidden in the table or wheel, and the betting layout was simplified. This eventually evolved into the American style roulette game as different from the traditional French game. The American game developed in the gambling dens across the new territories where makeshift games had been set up, whereas the French game evolved with style and leisure in Monte Carlo. However, it is the American style layout with its simplified betting and fast cash action, using either a single or double zero wheel, that now dominates in most casinos around the world.
During the first part of the 20th century, the only casino towns of note were Monte Carlo with the traditional single zero French wheel, and Las Vegas with the American double zero wheel. In the 1970s, casinos began to flourish around the world. By 2008 there were several hundred casinos worldwide offering roulette games. The double zero wheel is found in the U.S., Canada, South America, and the Caribbean, while the single zero wheel is predominant elsewhere.
Rules of play against a casino.
Roulette players have a variety of betting options. Placing inside bets is either selecting the exact number of the pocket the ball will land in, or a small range of pockets based on their proximity on the layout. Players wishing to bet on the 'outside' will select bets on larger positional groupings of pockets, the pocket color, or whether the winning number is odd or even. The payout odds for each type of bet are based on its probability.
The roulette table usually imposes minimum and maximum bets, and these rules usually apply separately for all of a player's inside and outside bets for each spin. For inside bets at roulette tables, some casinos may use separate roulette table chips of various colors to distinguish players at the table. Players can continue to place bets as the ball spins around the wheel until the dealer announces "no more bets" or "rien ne va plus".
When a winning number and color is determined by the roulette wheel, the dealer will place a marker, also known as a dolly, on that winning number on the roulette table layout. When the dolly is on the table, no players may place bets, collect bets, or remove any bets from the table. The dealer will then sweep away all other losing bets either by hand or rake, and determine all of the payouts to the remaining inside and outside winning bets. When the dealer is finished making payouts, the marker is removed from the board where players collect their winnings and make new bets. The winning chips remain on the board.
California Roulette.
In 2004, California legalized a form of roulette known as California Roulette. By law, the game must use cards and not slots on the roulette wheel to pick the winning number. There are at least two variations. In some casinos, the dealer spins a wheel containing 38 cards from 1 to 36, plus 0 and 00, and after betting is closed, stops the wheel; a pointer identifies the winning card, which the dealer removes and shows to the players. In the Cache Creek casino in northern California, a wheel resembling a traditional roulette wheel is used, but it has only alternating red and black slots with no numbers. As the ball is spinning, the dealer takes cards from a shoe and places two of them face down on the table in red and black rectangles. When the ball lands in a red or black slot, the card in the corresponding rectangle is turned over to reveal the winning number.
Roulette wheel number sequence.
The pockets of the roulette wheel are numbered from 0 to 36.
In number ranges from 1 to 10 and 19 to 28, odd numbers are red and even are black. In ranges from 11 to 18 and 29 to 36, odd numbers are black and even are red.
There is a green pocket numbered 0 (zero). In American roulette, there is a second green pocket marked 00. Pocket number order on the roulette wheel adheres to the following clockwise sequence in most casinos:
Roulette table layout.
The cloth covered betting area on a roulette table is known as the "layout". The layout is either single zero or double zero. The European style layout has a single zero, and the American style layout is usually a double zero. The American style roulette table with a wheel at one end is now used in most casinos. The French style table with a wheel in the centre and a layout on either side is rarely found outside of Monte Carlo.
Types of bets.
In roulette, bets can either be inside or outside bets.
Outside bets.
Outside bets typically have smaller payouts with better odds at winning. Except as noted, all of these bets lose if a zero comes up.
In the United Kingdom, the farthest outside bets (low/high, red/black, even/odd) result in the player losing only half of his/her bet if a zero comes up.
Bet odds table.
The payout (except for the special case of Top line bets), for American and European roulette, can be calculated by:
formula_1
where "n" is the number of squares the player is betting on. The initial bet is returned in addition to the mentioned payout. It can be easily demonstrated that this payout formula would lead to a zero expected value of profit if there were only 36 numbers. Having 37 or 38 numbers gives the casino its edge.
Note that Top line (0, 00, 1, 2, 3) has a different expected value because of approximation of the correct 6.2 payout obtained by the formula to 6. Note also that 0 and 00 are not odd or even, or high or low.
En prison rules, when used, reduce the house advantage.
House edge.
The "house average" or "house edge or house advantage" (also called the expected value) is the amount the player loses relative for any bet made, on average. If a player bets on a single number in the American game there is a probability of 1/38 that the player wins 35 times the bet, and a 37/38 chance that the player loses his bet. The expected value is:
For European roulette, a single number wins and loses :
The presence of the green squares on the roulette wheel and on the table is technically the only house edge. Outside bets will always lose when a single or double zero comes up. However, the house also has an edge on inside bets because the pay outs are always set at 35 to 1 when you mathematically have a 1 out of 38 (1 out of 37 for French/European roulette) chance at winning a straight bet on a single number. demonstrate the house edge on inside bets, imagine placing straight $1 wagers on all inside numbers (including 0 and 00) to assure a win: you would only get back $36, having spent $38. The only exceptions are the five numbers bet where the house edge is considerably higher (7.89% on an American wheel), and the "even money" bets in some European games (French Roulette) where the house edge is halved because only half the stake is lost when a zero comes up. This is commonly called the "La Partage" -rule, and it is considered being the main difference of European and French roulette. There is also modification of this rule, which is called "En Prison" -rule. (En prison) These rules cut the house edge into half (1,35%) in French roulette, when playing even-money bets, as half of the even-money bets are given back for player if the zero is draw in the wheel.
The house edge should not be confused with the "hold". The hold is the average percentage of the money originally brought to the table that the player loses before he leaves—the actual "win" amount for the casino. The Casino Control Commission in Atlantic City releases a monthly report showing the win/hold amounts for each casino. The average win/hold for double zero wheels is between 21–30%, significantly more than the 5.26% house edge. This reflects the fact that the player is churning the same money over and over again. A 23.6% hold, for example, would imply that, on average, the player bets the total he brought to the table five times, as 23.6% is approximately equal to 100% - (100% - 5.26%)^5. For example, a player with $100 making $10 bets on red (which has a near 50/50 chance of winning) is highly unlikely to lose all his money after only 10 bets, and will most likely continue to bet until he has lost all of his money or decides to leave. A player making $10 bets on a single number (with only 1/38 chance of success) with a $100 bankroll is far more likely to lose all of his money after only 10 bets.
In the early frontier gambling saloons, the house would set the odds on roulette tables at 27 for 1. This meant that on a $1 bet you would get $27 and the house would keep your initial dollar. Today most casino odds are set by law, and they have to be either 34 to 1 or 35 to 1. This means that the house pays you $34 or $35 and you get to keep your original $1 bet.
Mathematical model.
As an example, we can examine the European roulette model, that is,roulette with only one zero. Since this roulette has 37 cells with equal odds of hitting, this is a final model of field probability formula_2, where formula_3, formula_4 for all formula_5.
Call the bet formula_6 a triple formula_7, where formula_8 is the set of chosen numbers, formula_9 is the size of the bet, and, and formula_10 determines the return of the bet.
The rules of European roulette have 10 types of bets. First we can examine the 'Straight Up' bet. In this case, formula_11, for some formula_12, and formula_13 is determined by
formula_14
The bet's expected net return, or profitability, is equal to 
formula_15
Without details, for a bet, black (or red), the rule is determined as 
formula_16
and the profitability formula_17.
For similar reasons it is simple to see that the profitability is also equal for all remaining types of bets. formula_18.
In reality this means that, the more bets a player makes, the more he is going to lose independent of the strategies (combinations of bet types or size of bets) that he employs: 
formula_19
Here, the profit margin for the roulette owner is equal to approximately 2.7%. Nevertheless, several roulette strategy systems have been developed despite the losing odds. These systems can not change the odds of the game in favor of the player.
It's worth noting that the odds for the player in American roulette are even worse, as the bet profitability is at worst formula_20, and never better than formula_21.
Simplified mathematical model.
For a roulette wheel with n green numbers and 36 other unique numbers the chance of the ball landing on a given number is formula_22. For a betting option with p numbers that define a win, the chance of winning a bet is formula_23
For example, betting on "red", there are 18 red numbers, formula_24, the chance of winning is formula_25.
The payout given by the casino for a win is based on the roulette wheel having 36 outcomes and the payout for a bet is given by formula_26 .
For example, betting on 1-12 there are 12 numbers that define a win, formula_27, the payout is formula_28, so the better wins 3 times their bet.
The average return on a player’s bet is given by formula_29
For formula_30 the average return is always lower than 1 so on average a player will lose money.
With 1 green number formula_31 the average return is formula_32, that is, after a bet the player will on average have formula_32 of their original bet returned to them.
With 2 green numbers formula_34 the average return is formula_35.
This shows that the expected return is independent of the choice of bet.
Called (or call) bets or announced bets.
Although most often named "Call Bets" technically these bets are more accurately referred to as "announced bets". The legal distinction between a "Call Bet" and an "Announced Bet" is that a "Call Bet" is a bet called by the player without him placing any money on the table to cover the cost of the bet. In many jurisdictions (most notably the United Kingdom) this is considered gambling on credit and is illegal in some jurisdictions around the world. An "Announced Bet" is a bet called by the player for which he immediately places enough money to cover the amount of the bet on the table, prior to the outcome of the spin / hand in progress being known.
There are different number series in roulette that have special names attached to them.
Most commonly these bets are known as "the French bets" and each covers a section of the wheel. For the sake of accuracy, Zero spiel although explained below is not a French bet, it is more accurately "the German bet".
Players at a table may bet a set amount per series (or multiples of that amount). The series are based on the way certain numbers lie next to each other on the roulette wheel. Not all casinos offer these bets, and some may offer additional bets or variations on these.
Voisins du zéro ("neighbors of zero").
This is a name, more accurately Grand Voisins du Zéro, for the seventeen numbers which lie between 22 and 25 on the wheel including 22 and 25 themselves. The series is 22,18,29,7,28,12,35,3,26,0,32,15,19,4,21,2,25 (on a single zero wheel).
9 chips or multiples thereof are bet. 2 chips are placed on the 0,2,3 trio; 1 on the 4/7 split; 1 on 12/15; 1 on 18/21; 1 on 19/22; 2 on 25/26/28/29 corner; and 1 on 32/35.
Jeu zéro ("zero game").
Zero game, also known as zero spiel (spiel is German for game or play), is the name for the numbers closest to zero. All numbers in the zero game are included in the big series, but are placed differently. The numbers bet on are as follows: 12, 35, 3, 26, 0, 32, 15.
The bet consists of 4 chips or multiples thereof. 3 chips are bet on splits and 1 chip on straight: 1 chip on 0/3 split, 1 on 12/15 split, 1 on 32/35 split and 1 straight-up on number 26 .
This type of bet is popular in Germany and many European casinos. It is also offered as a 5 piece bet in many Eastern European casinos. As a 5 piece bet it is known as zero spiel naca and includes, in addition to the chips placed as noted above, a straight-up on number 19.
Le tiers du cylindre ("Thirds of the wheel").
This is the name for the twelve numbers which lie on the opposite side of the wheel between 27 and 33 including 27 and 33 themselves. On a single-zero wheel, the series is 27,13,36,11,30,8,23,10,5,24,16,33. The full name (although very rarely used—most players refer to it as "tiers") for this bet is "le tiers du cylindre" (translated from French into English meaning one third of the wheel) because it covers twelve numbers (placed as 6 splits), which is as close to 1/3 of the wheel as one can get.
Very popular in British casinos, tier bets outnumber Voisin and Orphans bets by a massive margin.
6 chips or multiples thereof are bet. 1 chip is placed on each of the following splits: 5/8; 10/11; 13/16; 23/24; 27/30; 33/36.
The Tiers bet is also called the "Small Series" and in some casinos (most notably in South Africa) "Series 5/8" It includes the following wagers which are all splits
A variant known as "Tier 5,8,10,11" has an additional chip placed straight up on 5, 8, 10 and 11; and so is a 10-piece bet. In some places the variant is called "giocco/ Giocco Ferrari" with a straight up on 8, 11, 23 and 30; the bet is marked with a red G-button on the racetrack.
Orphelins ("orphans").
These numbers make up the two slices of the wheel outside the Tiers and Voisins. They contain a total of eight numbers, comprising 17,34,6 and 1,20,14,31,9.
5 chips or multiples thereof are bet on 4 splits and a straight-up: 1 chip is placed straight-up on 1 and 1 chip on each of the splits: 6/9; 14/17; 17/20 and 31/34.
... and the neighbors.
A number may be backed along with the 2 numbers on the either side of it in a 5 piece bet. For example, "0 and the Neighbors" is a 5 piece bet with 1 piece straight-up on 3, 26, 0, 32 and 15. Neighbors bets are often put on in combinations, for example "1, 9, 14 and the neighbors" is a 15 piece bet covering 18, 22, 33, 16 with 1 piece; 9, 31, 20, 1 with 2 pieces and 14 with 3 pieces.
Any of the above bets may be combined, e.g. "Orphelins by 1 and Zero and the Neighbors by 1". The "...and the Neighbors" is often assumed by the croupier.
Final bets.
Another bet offered on the single zero game is "finals". Most often pronounced finaal, but also finale (common with Italian speakers), and finals.
Finaal 4, for example, is a 4 piece bet and consists of 1 piece placed on each of the numbers ending in 4, that is 4, 14, 24 and 34.
Finaal 7 is a 3 piece bet, 1 piece each on 7, 17 and 27.
Finaal bets from finaal 0 (zero) to finaal 6 cost 4 pieces.
Finaal bets 7, 8 and 9 cost 3 pieces.
Some casinos also offer split-finaal bets, for example finaal 5/8 would be a 4 piece bet, 1 piece each on the splits 5/8, 15/18, 25/28 and 35.
Full completes/maximums.
A complete bet places all of the inside bets on a certain number. Full complete bets are most often bet by high rollers as "maximum bets".
The maximum amount allowed to be wagered on a single bet in European Roulette is based on a progressive betting model. If the casino allows a maximum bet of $1000 on a 35-1 straight-up, then on each 17-1 split connected to that straight-up, $2000 may be wagered. Each 8-1 corner that covers four numbers) may have $4000 wagered on it. Each 11-1 street that covers three numbers may have $3000 wagered on it. Each 5-1 six-line may have $6000 wagered on it. Each $1000 incremental bet would be represented by a token or "piece" that is used to specifically identify the player and the amount bet.
For instance, if a patron wished to place a full complete bet on 17, the player would call "17 to the maximum". This bet would require a total of 40 pieces or $40,000. To manually place the same wager, the player would need to bet:
The player calls his bet to the croupier (most often after the ball has been spun) and places enough chips to cover the bet on the table within reach of the croupier. The croupier will immediately announce the bet (repeat what the player has just said), ensure that the correct monetary amount has been given while simultaneously placing a matching markers on the number on the table and the amount wagered.
The payout for this bet if the chosen number wins is 392 pieces, in the case of a $1000 straight-up maximum, $40,000 bet, a payout of $392,000. The player's wagered 40 pieces, as with all winning bets in roulette, are still his property and in the absence of a request to the contrary are left up to possibly win again on the next spin.
Based on the location of the numbers on the layout, the number of chips required to "complete" a number can be determined.
Most typically (Mayfair casinos in London and other top class European casinos) with these "maximum" or "full complete" bets nothing (except the aforementioned maximum button) is ever placed on the layout even in the case of a win. Experienced gaming staff, and the type of customers playing such bets, are fully aware of the payouts and so the croupier simply makes up the correct payout, announces its value to the table inspector (floor person in the USA) and the customer, and then passes it to the customer, but only after a verbal authorization from the inspector has been received.
Also typically at this level of play (house rules allowing) the experienced croupier caters to the needs of the customer and will most often add the customer's winning bet to the payout, as the type of player playing these bets very rarely bets the same number two spins in succession. For example, the winning 40 piece / $40,000 bet on "17 to the maximum" pays 392 pieces / $392,000. The experienced croupier would pay the player 432 pieces / $432,000, that is 392 + 40, with the announcement that the payout "is with your bet down Sir".
There are also several methods to determine the payout should a number adjacent to a chosen number be the winner; for example, player bets 40 pieces on "23 to the maximum" and number 26 is the winning number. The most notable method is known as the "station" system or method. When paying in stations, the dealer counts the number of ways or stations that the winning number hits the complete bet. In the example above, 26 hits four stations – two different corners, one split and one six-line. The dealer takes the number four, multiplies it by 30 and adds the remaining eight to the payout. 4x30=120, 120+8=128. If calculated as stations they would just times 4 by 36 making 144 with the players bet down.
In some casinos, a player may bet full complete for less than the table straight-up maximum; for example, "number 17 full complete by $25" would cost $1000, that is 40 pieces each at $25 value.
Betting strategies and tactics.
Over the years, many people have tried to beat the casino, and turn roulette—a game designed to turn a profit for the house—into one on which the player expects to win. Most of the time this comes down to the use of betting systems, strategies which say that the house edge can be beaten by simply employing a special pattern of bets, often relying on the "Gambler's fallacy", the idea that past results are any guide to the future (for example, if a roulette wheel has come up 10 times in a row on red, that red on the next spin is any more or less likely than if the last spin was black).
All betting systems that rely on patterns, when employed on casino edge games will result, on average, in the player losing money. In practice, players employing betting systems may win, and may indeed win very large sums of money, but the losses (which, depending on the design of the betting system, may occur quite rarely) will outweigh the wins. Certain systems, such as the Martingale, described below, are extremely risky, because the worst-case scenario (which is mathematically certain to happen, at some point) may see the player chasing losses with ever bigger bets until he runs out of money.
The American mathematician Patrick Billingsley said that no betting system can convert a subfair game into a profitable enterprise. 
At least in the 1930s, some professional gamblers were able to consistently gain an edge in roulette by seeking out rigged wheels (not difficult to find at that time) and betting opposite the largest bets.
Prediction methods.
Whereas betting systems are essentially an attempt to beat the fact that a geometric series with initial value of 0.95 (American roulette) or 0.97 (European roulette) will inevitably over time tend to zero, engineers instead attempt to overcome the house edge through predicting the mechanical performance of the wheel, most notably by Joseph Jagger at Monte Carlo in 1873. These schemes work by determining that the ball is more likely to fall at certain numbers, and if sufficiently good will raise the return of the game above 100%, defeating the betting system problem.
Edward O. Thorp (the developer of card counting and an early hedge-fund pioneer) and Claude Shannon (a mathematician and electronic engineer best known for his contributions to information theory) built the first wearable computer to predict the landing of the ball in 1961. This system worked by timing the ball and wheel, and using the information obtained to calculate the most likely octant where the ball would fall. Ironically, this technique works best with an unbiased wheel though it could still be countered quite easily by simply closing the table for betting before beginning the spin.
In 1982, several casinos in Britain began to lose large sums of money at their roulette tables to teams of gamblers from the USA. Upon investigation by the police, it was discovered they were using a legal system of biased wheel-section betting. As a result of this, the British roulette wheel manufacturer John Huxley manufactured a roulette wheel to counteract the problem.
The new wheel, designed by George Melas, was called "low profile" because the pockets had been drastically reduced in depth, and various other design modifications caused the ball to descend in a gradual approach to the pocket area. In 1986, when a professional gambling team headed by Billy Walters won $3.8 million using the system on an old wheel at the Golden Nugget in Atlantic City, every casino in the world took notice, and within one year had switched to the new low-profile wheel.
Thomas Bass, in his book "The Eudaemonic Pie" (1985) (published as "The Newtonian Casino" in Britain), has claimed to be able to predict wheel performance in real time. The book describes the exploits of a group of University of California Santa Cruz students, who called themselves "the Eudaemons", who in the late 1970s used computers in their shoes to win at roulette. This is an updated and improved version of Edward O Thorp's approach, where Newtonian
Laws of Motion are applied to track the roulette ball's deceleration; hence the British title.
In the early 1990s, Gonzalo Garcia-Pelayo believed that casino roulette wheels were not perfectly random, and that by recording the results and analysing them with a computer, he could gain an edge on the house by predicting that certain numbers were more likely to occur next than the 1-in-36 odds offered by the house suggested. This he did at the Casino de Madrid in Madrid, Spain, winning 600,000 euros in a single day, and one million euros in total. Legal action against him by the casino was unsuccessful, it being ruled that the casino should fix its wheel.
To prevent exploits like these, the casinos monitor the performance of their wheels, and rebalance and realign them regularly to try to keep the result of the spins as uniform as possible.
In 2004 it was reported that a group of two Serbs and one Hungarian in London had used a laser scanner hidden inside a mobile phone linked to a computer to predict the sector of the wheel where the ball was most likely to drop. They were arrested, but released without charge as there was no proof they had technically interfered with casino equipment.
Specific betting systems.
The numerous even-money bets in roulette have inspired many players over the years to attempt to beat the game by using one or more variations of a martingale betting strategy, wherein the gamer doubles the bet after every loss, so that the first win would recover all previous losses, plus win a profit equal to the original bet. The problem with this strategy is that, remembering that past results do not affect the future, it is possible for the player to lose so many times in a row, that the player, doubling and redoubling his bets, either runs out of money or hits the table limit. A large financial loss is certain in the long term if the player continued to employ this strategy. Another strategy is the Fibonacci system, where bets are calculated according to the Fibonacci sequence. Regardless of the specific progression, no such strategy can statistically overcome the casino's advantage, since the expected value of each allowed bet is negative.
While not a strategy to win money, former "Los Angeles Times" editor Andrés Martinez described a betting method in his book on Las Vegas titled "24/7". He called it the "dopey experiment". The idea is to divide one's roulette session bankroll into 35 units. This unit is bet on a particular number for 35 consecutive spins. Thus, if the number hits in that time, the gambler wins back the original bankroll and can play subsequent spins with house money. However, there is only a formula_36 = 60.68% probability of winning within 35 spins (assuming a double-zero wheel with 38 pockets).
Labouchère system.
The Labouchère System is a progression betting strategy like the martingale but does not require the gambler to risk his stake as quickly with dramatic double-ups. The Labouchere System involves using a series of numbers in a line to determine the bet amount, following a win or a loss. Typically, the player adds the numbers at the front and end of the line to determine the size of the next bet. When he wins, he crosses out numbers and continues working on the smaller line. If he loses, then he adds his previous bet to the end of the line and continues to work on the longer line. This is a much more flexible progression betting system and there is much room for the player to design his initial line to his own playing preference.
This system is one that is designed so that when the player has won over a third of his bets (less than the expected 18/38), he will win. Whereas the martingale will cause ruin in the event of a long sequence of successive losses, the Labouchère system will cause bet size to grow quickly even where a losing sequence is broken by wins. This occurs because as the player loses, the average bet size in the line increases.
As with all other betting systems, the average value of this system is negative.
D'Alembert system.
The system, also called "montant et demontant" (from French, meaning upwards and downwards), is often called a pyramid system. It is based on a mathematical equilibrium theory devised by a French mathematician of the same name. Like the martingale, this system is mainly applied to the even-money outside bets, and is favored by players who want to keep the amount of their bets and losses to a minimum. The betting progression is very simple: After each loss, you add one unit to the next bet, and after each win, one unit is deducted from the next bet. Starting with an initial bet of, say, 1 units, a loss would raise the next bet to 2 units. If this is followed by a win, the next bet would be 1 units.
This betting system relies on the gambler's fallacy—that the player is more likely to lose following a win, and more likely to win following a loss.
Other systems.
There are numerous other betting systems that rely on this fallacy, or that attempt to follow 'streaks' (looking for patterns in randomness), varying bet size accordingly.
Many betting systems are sold online, and may make outlandish promises that the player can 'beat' the system by following them. One such system was advertised by Jason Gillon of Rotherham, UK, who claimed you could 'earn £200 daily' by following his betting system, described as a 'loophole'. As the system was advertised in the UK press, it was subject to Advertising Standards Authority regulation, and following a complaint, it was ruled by the ASA that Mr. Gillon had failed to support his claims you could earn £200 daily, and that he had failed to show that there was any loophole.
Using the dozen bet.
There are two versions to this system, single-dozen bets and double-dozen bets. In the single-dozen-bet version, the player uses a progressively incrementing stake list starting from the casino table minimum, to the table maximum. The aim here is to use a single-dozen bet to win before the stake list ends. Many techniques are employed, such as betting on the same dozen to appear after two consecutive appearances, betting on the dozen that has appeared most in the last 15, 9, or 5 spins, and betting on the dozen that, after a long absence of 7 or more spins, appears for the first time. The double-dozen bet version uses two dozen bets and half the stake list size of the single-dozen-bet version.

</doc>
<doc id="26488" url="https://en.wikipedia.org/wiki?curid=26488" title="Reformation (disambiguation)">
Reformation (disambiguation)

Reformation may refer to:

</doc>
<doc id="26490" url="https://en.wikipedia.org/wiki?curid=26490" title="Reference counting">
Reference counting

In computer science, reference counting is a technique of storing the number of references, pointers, or handles to a resource such as an object, block of memory, disk space or other resource. It may also refer, more specifically, to a garbage collection algorithm that uses these reference counts to deallocate objects which are no longer referenced.
Use in garbage collection.
As a collection algorithm, reference counting tracks, for each object, a count of the number of references to it held by other objects. If an object's reference count reaches zero, the object has become inaccessible, and can be destroyed.
When an object is destroyed, any objects referenced by that object also have their reference counts decreased. Because of this, removing a single reference can potentially lead to a large number of objects being freed. A common modification allows reference counting to be made incremental: instead of destroying an object as soon as its reference count becomes zero, it is added to a list of unreferenced objects, and periodically (or as needed) one or more items from this list are destroyed.
Simple reference counts require frequent updates. Whenever a reference is destroyed or overwritten, the reference count of the object it references is decremented, and whenever one is created or copied, the reference count of the object it references is incremented.
Reference counting is also used in disk operating systems and distributed systems, where full non-incremental tracing garbage collection is too time consuming because of the size of the object graph and slow access speed.
Advantages and disadvantages.
The main advantage of the reference counting over tracing garbage collection is that objects are reclaimed "as soon as" they can no longer be referenced, and in an incremental fashion, without long pauses for collection cycles and with clearly defined lifetime of every object. In real-time applications or systems with limited memory, this is important to maintain responsiveness. Reference counting is also among the simplest forms of memory management to implement. It also allows for effective management of non-memory resources such as operating system objects, which are often much scarcer than memory (tracing GC systems use finalizers for this, but the delayed reclamation may cause problems). Weighted reference counts are a good solution for garbage collecting a distributed system.
Tracing garbage collection cycles are triggered too often if the set of live objects fills most of the available memory; it requires extra space to be efficient. Reference counting performance does not deteriorate as the total amount of free space decreases.
Reference counts are also useful information to use as input to other runtime optimizations. For example, systems that depend heavily on immutable objects such as many functional programming languages can suffer an efficiency penalty due to frequent copies. However, if the compiler (or runtime system) knows that a particular object has only one reference (as most do in many systems), and that the reference is lost at the same time that a similar new object is created (as in the string append statement codice_1), it can replace the operation with a mutation on the original object.
Reference counting in naive form has two main disadvantages over the tracing garbage collection, both of which require additional mechanisms to ameliorate:
In addition to these, if the memory is allocated from a free list, reference counting suffers from poor locality. Reference counting alone cannot move objects to improve cache performance, so high performance collectors implement a tracing garbage collector as well. Most implementations (such as the ones in PHP and Objective-C) suffer from poor cache performance since they do not implement copying objects.
Graph interpretation.
When dealing with garbage collection schemes, it is often helpful to think of the reference graph, which is a directed graph where the vertices are objects and there is an edge from an object A to an object B if A holds a reference to B. We also have a special vertex or vertices representing the local variables and references held by the runtime system, and no edges ever go to these nodes, although edges can go from them to other nodes.
In this context, the simple reference count of an object is the in-degree of its vertex. Deleting a vertex is like collecting an object. It can only be done when the vertex has no incoming edges, so it does not affect the out-degree of any other vertices, but it can affect the in-degree of other vertices, causing their corresponding objects to be collected as well if their in-degree also becomes 0 as a result.
The connected component containing the special vertex contains the objects that can't be collected, while other connected components of the graph only contain garbage. If a reference-counting garbage collection algorithm is implemented, then each of these garbage components must contain at least one cycle; otherwise, they would have been collected as soon as their reference count (i.e., the number of incoming edges) dropped to zero.
Dealing with inefficiency of updates.
Incrementing and decrementing reference counts every time a reference is created or destroyed can significantly impede performance. Not only do the operations take time, but they damage cache performance and can lead to pipeline bubbles. Even read-only operations like calculating the length of a list require a large number of reads and writes for reference updates with naive reference counting.
One simple technique is for the compiler to combine a number of nearby reference updates into one. This is especially effective for references which are created and quickly destroyed. Care must be taken, however, to put the combined update at the right position so that a premature free be avoided.
The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in data structures, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists.
Another technique devised by Henry Baker involves deferred increments, in which references which are stored in local variables do not immediately increment the corresponding reference count, but instead defer this until it is necessary. If such a reference is destroyed quickly, then there is no need to update the counter. This eliminates a large number of updates associated with short-lived references (such as the above list-length-counting example). However, if such a reference is copied into a data structure, then the deferred increment must be performed at that time. It is also critical to perform the deferred increment before the object's count drops to zero, resulting in a premature free.
A dramatic decrease in the overhead on counter updates was obtained by Levanoni and Petrank. They introduce the update coalescing method which coalesces many of the redundant reference count updates. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object O1, then to an object O2, and so forth until at the end of the interval it points to some object On. A reference counting algorithm would typically execute rc(O1)--, rc(O2)++, rc(O2)--, rc(O3)++, rc(O3)--, ..., rc(On)++. But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform rc(O1)-- and rc(On)++. The rest of the updates are redundant.
Levanoni and Petrank showed in 2001 how to use such update coalescing in a reference counting collector. When using update coalescing with an appropriate treatment of new objects, more than 99% of the counter updates are eliminated for typical Java benchmarks. In addition, the need for atomic operations during pointer updates on parallel processors is eliminated. Finally, they presented an enhanced algorithm that may run concurrently with multithreaded applications employing only fine synchronization.
Blackburn and McKinley's "ulterior reference counting" method in 2003 combines deferred reference counting with a copying nursery, observing that the majority of pointer mutations occur in young objects. This algorithm achieves throughput comparable with the fastest generational copying collectors with the low bounded pause times of reference counting.
More work on improving performance of reference counting collectors can be found in Paz's Ph.D thesis in 2006. In particular, he advocates the use of age oriented collectors and prefetching.
Dealing with reference cycles.
Perhaps the most obvious way to handle reference cycles is to design the system to avoid creating them. A system may explicitly forbid reference cycles; file systems with hard links often do this. Judicious use of "weak" (non-counted) references may also help avoid retain cycles; the Cocoa framework, for instance, recommends using "strong" references for parent-to-child relationships and "weak" references for child-to-parent relationships.
Systems may also be designed to tolerate or correct the cycles they create in some way. Developers may design code to explicitly "tear down" the references in a data structure when it is no longer needed, though this has the cost of requiring them to manually track that data structure's lifetime. This technique can be automated by creating an "owner" object that does the tearing-down when it is destroyed; for instance, a Graph object's destructor could delete the edges of its GraphNodes, breaking the reference cycles in the graph. Cycles may even be ignored in systems with short lives and a small amount of cyclic garbage, particularly when the system was developed using a methodology of avoiding cyclic data structures wherever possible, typically at the expense of efficiency.
Computer scientists have also discovered ways to detect and collect reference cycles automatically, without requiring changes in the data structure design. One simple solution is to periodically use a tracing garbage collector to reclaim cycles; since cycles typically constitute a relatively small amount of reclaimed space, the collector can be run much less often than with an ordinary tracing garbage collector.
Bacon describes a cycle-collection algorithm for reference counting with similarities to tracing collectors, including the same theoretical time bounds. It is based on the observation that a cycle can only be isolated when a reference count is decremented to a nonzero value. All objects which this occurs on are put on a "roots" list, and then periodically the program searches through the objects reachable from the roots for cycles. It knows it has found a cycle that can be collected when decrementing all the reference counts on a cycle of references brings them all down to zero. An enhanced version of this algorithm by Paz et al.
is able to run concurrently with other operations and improve its efficiency by using the update coalescing method of Levanoni and Petrank.
Variants of reference counting.
Although it is possible to augment simple reference counts in a variety of ways, often a better solution can be found by performing reference counting in a fundamentally different way. Here we describe some of the variants on reference counting and their benefits and drawbacks.
Weighted reference counting.
In weighted reference counting, we assign each reference a "weight", and each object tracks not the number of references referring to it, but the total weight of the references referring to it. The initial reference to a newly created object has a large weight, such as 216. Whenever this reference is copied, half of the weight goes to the new reference, and half of the weight stays with the old reference. Because the total weight does not change, the object's reference count does not need to be updated.
Destroying a reference decrements the total weight by the weight of that reference. When the total weight becomes zero, all references have been destroyed. If an attempt is made to copy a reference with a weight of 1, we have to "get more weight" by adding to the total weight and then adding this new weight to our reference, and then splitting it. An alternative in this situation is to create an "indirection" reference object, the initial reference to which is created with a large weight which can then be split.
The property of not needing to access a reference count when a reference is copied is particularly helpful when the object's reference count is expensive to access, for example because it is in another process, on disk, or even across a network. It can also help increase concurrency by avoiding many threads locking a reference count to increase it. Thus, weighted reference counting is most useful in parallel, multiprocess, database, or distributed applications.
The primary problem with simple weighted reference counting is that destroying a reference still requires accessing the reference count, and if many references are destroyed this can cause the same bottlenecks we seek to avoid. Some adaptations of weighted reference counting seek to avoid this by attempting to give weight back from a dying reference to one which is still active.
Weighted reference counting was independently devised by Bevan, in the paper "Distributed garbage collection using reference counting", and Watson & Watson, in the paper "An efficient garbage collection scheme for parallel computer architectures", both in 1987.
Indirect reference counting.
In indirect reference counting, it is necessary to keep track of whom the reference was obtained from. This means that two references are kept to the object: a direct one which is used for invocations; and an indirect one which forms part of a diffusion tree, such as in the Dijkstra-Scholten algorithm, which allows a garbage collector to identify dead objects. This approach prevents an object from being discarded prematurely.
Examples of use.
COM.
Microsoft's Component Object Model (COM) makes pervasive use of reference counting. In fact, two of the three methods that all COM objects must provide (in the IUnknown interface) increment or decrement the reference count. Much of the Windows Shell and many Windows applications (including MS Internet Explorer, MS Office, and countless third-party products) are built on COM, demonstrating the viability of reference counting in large-scale systems.
One primary motivation for reference counting in COM is to enable interoperability across different programming languages and runtime systems. A client need only know how to invoke object methods in order to manage object life cycle; thus, the client is completely abstracted from whatever memory allocator the implementation of the COM object uses. As a typical example, a Visual Basic program using a COM object is agnostic towards whether that object was allocated (and must later be deallocated) by a C++ allocator or another Visual Basic component.
However, this support for heterogeneity has a major cost: it requires correct reference count management by all parties involved. While high-level languages like Visual Basic manage reference counts automatically, C/C++ programmers are entrusted to increment and decrement reference counts at the appropriate time. C++ programs can and should avoid the task of managing reference counts manually by using smart pointers. Bugs caused by incorrect reference counting in COM systems are notoriously hard to resolve, especially because the error may occur in an opaque, third-party component.
Microsoft abandoned reference counting in favor of tracing garbage collection for the .NET Framework. However, it has been reintroduced in the COM-based WinRT and the new C++/CX (Component Extensions) language.
C++.
C++11 provides reference counted smart pointers, via the Shared ptr class. Programmers can use weak pointers (via Shared ptr) to break cycles. C++ does not "require" all objects to be reference counted; in fact, programmers can choose to apply reference counting to only those objects that are truly shared; objects not intended to be shared can be referenced using a Shared ptr, and objects that are shared but not owned can be accessed via an iterator.
In addition, C++11's move semantics further reduce the extent to which reference counts need to be modified.
Cocoa.
Apple's Cocoa and Cocoa Touch frameworks (and related frameworks, such as Core Foundation) use manual reference counting, much like COM. Traditionally this was accomplished by the programmer manually sending codice_2 and codice_3 messages to objects, but Automatic Reference Counting, a Clang compiler feature that automatically inserts these messages as needed, was added in iOS 5 and Mac OS X 10.7. Mac OS X 10.5 introduced a tracing garbage collector as an alternative to reference counting, but it was deprecated in OS X 10.8 and is expected to be removed in a future version. iOS has never supported a tracing garbage collector.
Delphi.
One language that uses reference counting for garbage collection is Delphi. Delphi is mostly not a garbage collected language, in that user-defined types must still be manually allocated and deallocated. It does provide automatic collection, however, for a few built-in types, such as strings, dynamic arrays, and interfaces, for ease of use and to simplify the generic database functionality. It is up to the programmer to decide whether to use the built-in types or not; Delphi programmers have complete access to low-level memory management like in C/C++. So all potential cost of Delphi's reference counting can, if desired, be easily circumvented.
Some of the reasons reference counting may have been preferred to other forms of garbage collection in Delphi include:
GObject.
The GObject object-oriented programming framework implements reference counting on its base types, including weak references. Reference incrementing and decrementing uses atomic operations for thread safety. A significant amount of the work in writing bindings to GObject from high-level languages lies in adapting GObject reference counting to work with the language's own memory management system.
The Vala programming language uses GObject reference counting as its primary garbage collection system, along with copy-heavy string handling.
Perl.
Perl also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Perl does support weak references, which allows programmers to avoid creating a cycle.
PHP.
PHP uses a reference counting mechanism for its internal variable management. Since PHP 5.3, it implements the algorithm from Bacon's above mentioned paper. PHP allows you to turn on and off the cycle collection with user-level functions. It also allows you to manually force the purging mechanism to be run.
Python.
Python also uses reference counting and offers cycle detection as well.
Squirrel.
Squirrel also uses reference counting and offers cycle detection as well.
This tiny language is relatively unknown outside the video game industry; however, it is a concrete example of how reference counting can be practical and efficient (especially in realtime environments).
Tcl.
Tcl 8 uses reference counting for memory management of values (Tcl Obj structs). Since Tcl's values are immutable, reference cycles are impossible to form and no cycle detection scheme is needed. Operations that would replace a value with a modified copy are generally optimized to instead modify the original when its reference count indicates it to be unshared. The references are counted at a data structure level, so the problems with very frequent updates discussed above do not arise.
Xojo.
Xojo also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Xojo does support weak references, which allows programmers to avoid creating a cycle.
File systems.
Many file systems maintain a count of the number of references to any particular block or file, for example the inode "link count" on Unix-style file systems. When the count falls to zero, the file can be safely deallocated. In addition, while references can still be made from directories, some Unixes allow that the referencing can be solely made by live processes, and there can be files that do not exist in the file system hierarchy.

</doc>
<doc id="26491" url="https://en.wikipedia.org/wiki?curid=26491" title="Red-eye effect">
Red-eye effect

The red-eye effect in photography is the common appearance of red pupils in color photographs of eyes. It occurs when using a photographic flash very close to the camera lens (as with most compact cameras), in ambient low light. The effect appears in the eyes of humans, and of animals that have tapetum lucidum.
Theatrical followspot operators, positioned nearly coincidentally with a very bright light and somewhat distant from the actors, occasionally witness red-eye in actors on stage. The effect is not visible to the rest of the audience because it is reliant on the very small angle between the followspot operator and the light.
Causes.
In flash photography the light of the flash occurs too fast for the pupil to close so much of the very bright light from the flash passes into the eye through the pupil, reflects off the fundus at the back of the eyeball and out through the pupil. The camera records this reflected light. The main cause of the red color is the ample amount of blood in the choroid which nourishes the back of the eye and is located behind the retina. The blood in the retinal circulation is far less than in the choroid, and plays virtually no role. The eye contains several photostable pigments that all absorb in the short wavelength region, and hence contribute somewhat to the red eye effect. The lens cuts off deep blue and violet light, below 430 nm (depending on age), and macular pigment absorbs between 400 and 500 nm, but this pigment is located exclusively in the tiny fovea. Melanin, located in the retinal pigment epithelium (RPE) and the choroid, shows a gradually increasing absorption towards the short wavelengths. But blood is the main determinant of the red color, because it is completely transparent at long wavelengths and abruptly starts absorbing at 600 nm. The amount of red light emerging from the pupil depends on the amount of melanin in the layers behind the retina. This amount varies strongly between individuals. Light skinned people with blue eyes have relatively low melanin in the fundus and thus show a much stronger red-eye effect than dark skinned people with brown eyes. The same holds for animals. The color of the iris itself is of virtually no importance for the red-eye effect. This is obvious because the red-eye effect is most apparent when photographing dark adapted subjects, hence with fully dilated pupils. Photographs taken with infrared light through night vision devices always show very bright pupils because, in the dark, the pupils are fully dilated and the infrared light is not absorbed by any ocular pigment.
The role of melanin in red-eye effect is nicely demonstrated in animals with heterochromia: only the blue eye displays the effect. The effect is still more pronounced in humans and animals with albinism. All forms of albinism involve abnormal production and/or deposition of melanin.
Red-eye effect is seen in photographs of children also because children's eyes have more rapid dark adaption: in low light a child's pupils enlarge sooner, and an enlarged pupil accentuates the red-eye effect.
Similar effects.
Similar effects, some related to red-eye effect, are of several kinds:
Photography techniques for prevention and removal.
The red-eye effect can be prevented in a number of ways.
If direct flash must be used, a good rule of thumb is to separate the flash from the lens by 1/20 of the distance of the camera to the subject. For example, if the subject is 2 meters (6 feet) away, the flash head should be at least 10 cm (4 inches) away from the lens.
Professional photographers prefer to use ambient light or indirect flash, as the red-eye reduction system does not always prevent red eyes — for example, if people look away during the pre-flash. In addition, people do not look natural with small pupils, and direct lighting from close to the camera lens is considered to produce unflattering photographs.
Red-eye removal is built into many popular consumer graphics editing software packages, or is supported through red-eye reduction plug-ins; examples include Adobe Photoshop, Apple iPhoto, Corel Photo-Paint, GIMP, Google Picasa, Paint.NET and Microsoft Windows Photo Gallery. Some can automatically find eyes in the image and perform color correction, and can apply it to many photos at once. Others may require the operator to manually select the regions of the pupils to which correction is to be applied. When performed manually, correction may consist of simply converting the red area of pupils to grayscale (desaturating), leaving surface reflections and highlights intact.
As a medical warning sign.
In a photograph of a child's face, red-eye in one eye but not the other may actually be leukocoria, which may be caused by the cancer retinoblastoma, and the child's eyes should be examined by a general physician.
Cultural uses.
The feature film "Blade Runner" intentionally induced a red-eye effect to indicate that certain characters were "replicants," an artificial life-form in the story. The effect relied on a semi-silvered mirror directing light at the actors on the same incidence as the camera. The "Dragon Ball" manga and anime franchise also featured the red-eye effect as a part of the Saiyans' transformation into Great Apes at the full moon.

</doc>
<doc id="26492" url="https://en.wikipedia.org/wiki?curid=26492" title="Ramsay Hunt syndrome type II">
Ramsay Hunt syndrome type II

Ramsay Hunt syndrome (RHS) type 2 also known as herpes zoster oticus is a disorder that is caused by the reactivation of pre-existing Varicella zoster virus in the geniculate ganglion, a nerve cell bundle, of the facial nerve.
Ramsay Hunt syndrome type 2 typically presents with inability to move many facial muscles, pain in the ear, taste loss on the front of the tongue, dry eyes and mouth, and the eruption of an erythematous rash.
Signs and symptoms.
The symptoms and signs include acute facial nerve paralysis, pain in the ear, taste loss in the front two-thirds of the tongue, dry mouth and eyes, and eruption of an erythematous vesicular rash in the ear canal, the tongue, and/or hard palate.
Since the vestibulocochlear nerve is in proximity to the geniculate ganglion, it may also be affected, and patients may also suffer from tinnitus, hearing loss, and vertigo.
Possible involvement of the trigeminal nerve can cause numbness of the face.
Pathophysiology.
RHS type 2 refers to shingles of the geniculate ganglion. After initial infection, varicella zoster virus lies dormant in various nerve cells in the body, where it is kept in check by the patient's immune system. Given the opportunity, for example during an illness that suppresses the immune system, the virus is reactivated and travels to the end of the nerve cell, where it causes the symptoms described above. 
In RHS type 2, the affected ganglion is responsible for the movements of facial muscles, the touch sensation of a part of ear and ear canal, the taste function of the frontal two-thirds of the tongue, and the moisturization of the eyes and the mouth. The syndrome specifically refers to the combination of this entity with weakness of the muscles activated by the facial nerve. In isolation the latter entity would be called Bell's Palsy. 
Like shingles, however, lack of lesions does not definitely exclude the existence of a herpes infection. The virus can be detected, even before the eruption of vesicles, from the skin of the ear.
Prevention.
This disease is prevented by immunizing against the causal virus, varicella zoster, for example through Zostavax, a stronger version of chickenpox vaccine.
Treatment with the corticosteroid Prednisone and acyclovir has been shown to achieve complete recovery in a majority of patients if started within the first three days of facial paralysis, with chances of recovery decreasing as treatment was delayed. Delay of treatment may result in permanent facial nerve paralysis. 
Treatment apparently has no effect on the recovery of hearing loss. 
Diazepam is sometimes used to treat the vertigo.
History.
It is named after James Ramsay Hunt.

</doc>
<doc id="26494" url="https://en.wikipedia.org/wiki?curid=26494" title="Race and intelligence">
Race and intelligence

The connection between race and intelligence has been a subject of debate in both popular science and academic research since the inception of IQ testing in the early 20th century. The debate concerns the interpretation of research findings that test takers identifying as "White" tend on average to score higher than test takers of African ancestry on IQ tests, and subsequent findings that test takers of East Asian background tend to score higher than whites. It is still not resolved what relation, if any, there is between group differences in IQ and race.
The first test showing differences in IQ test results between different population groups in the US was the tests of United States Army recruits in World War I. In the 1920s groups of eugenics lobbyists argued that this demonstrated that African-Americans and certain immigrant groups were of inferior intellect to Anglo-Saxon whites due to innate biological differences, using this as an argument for policies of racial segregation. Soon, other studies appeared, contesting these conclusions and arguing instead that the Army tests had not adequately controlled for the environmental factors such as socio-economic and educational inequality between African-Americans and Whites. The debate reemerged again in 1969, when Arthur Jensen championed the view that for genetic reasons Africans were less intelligent than whites and that compensatory education for African-American children was therefore doomed to be ineffective. In 1994, the book "The Bell Curve," argued that social inequality in America could largely be explained as a result of IQ differences between races and individuals rather than being their cause, and rekindled the public and scholarly debate with renewed force. During the debates following the book's publication the American Anthropological Association and the American Psychological Association (APA) published official statements regarding the issue, both highly skeptical of some of the book's claims, although the APA report called for more empirical research on the issue.
In subsequent decades much research has been published about the relationships between hereditary influences on IQ, group differences in intelligence, race, environmental influences on IQ. Particularly contentious in the ongoing debate has been the definition of both the concept "race" and the concept "intelligence", and especially whether they can in fact be objectively defined and operationalized. While several environmental factors have been shown to affect group differences in intelligence, it has not been demonstrated that they can explain the entire disparity. On the other hand, no genetic factor has been conclusively shown to have a causal relation with group difference in intelligence test scores. Recent summaries of the debate call for more research into the topic to determine the relative contributions of environmental and genetic factors in explaining the apparent IQ disparity among racial groups.
History of the debate.
Claims of races having different intelligence were used to justify colonialism, slavery, racism, social Darwinism, and racial eugenics. Racial thinkers such as Arthur de Gobineau relied crucially on the assumption that black people were innately inferior to Whites in developing their ideologies of White supremacy. Even enlightenment thinkers such as Thomas Jefferson, a slave owner, believed Blacks to be innately inferior to Whites in physique and intellect.
Early IQ testing.
The first practical intelligence test was developed between 1905 and 1908 by Alfred Binet in France for school placement of children. Binet warned that results from his test should not be assumed to measure innate intelligence or used to label individuals permanently. Binet's test was translated into English and revised in 1916 by Lewis Terman (who introduced IQ scoring for the test results) and published under the name the Stanford–Binet Intelligence Scales. As Terman's test was published, there was great concern in the United States about the abilities and skills of recent immigrants. Different immigrant nationalities were sometimes thought to belong to different races, such as Slavs. A different set of tests developed by Robert Yerkes were used to evaluate draftees for World War I, and researchers found that people from southern and eastern Europe scored lower than native-born Americans, that Americans from northern states had higher scores than Americans from southern states, and that Black Americans scored lower than white Americans. The results were widely publicized by a lobby of anti-immigration activists, including the New York patrician and conservationist Madison Grant, who considered the nordic race to be superior, but under threat of immigration by inferior breeds. In his influential work "A Study of American Intelligence" psychologist Carl Brigham used the results of the Army tests to argue for a stricter immigration policy, limiting immigration to countries considered to belong to the "nordic race".
In the 1920s, states like Virginia enacted eugenic laws, such as its 1924 Racial Integrity Act, which established the one-drop rule as law. On the other hand, many scientists reacted to eugenicist claims linking abilities and moral character to racial or genetic ancestry. They pointed to the contribution of environment to test results (such as speaking English as a second language). By the mid-1930s, many United States psychologists adopted the view that environmental and cultural factors played a dominant role in IQ test results, among them Carl Brigham who repudiated his own previous arguments, on the grounds that he realized that the tests were not a measure of innate intelligence. Discussion of the issue in the United States also influenced German Nazi claims of the "nordics" being a "master race", influenced by Grant's writings. As the American public sentiment shifted against the Germans, claims of racial differences in intelligence increasingly came to be regarded as problematic. Anthropologists such as Franz Boas, and Ruth Benedict and Gene Weltfish, did much to demonstrate the unscientific status of many of the claims about racial hierarchies of intelligence. Nonetheless a powerful eugenics and segregation lobby funded largely by textile-magnate Wickliffe Draper, continued to publicize studies using intelligence studies as an argument for eugenics, segregation, anti-immigration legislation.
The Jensenism debates.
As the de-segregation of the American South was begun in the 1950s the debate about Black intelligence resurfaced. Audrey Shuey, funded by Draper's Pioneer fund, published a new analysis of Yerkes' tests, concluding that blacks really were of inferior intellect to whites. This study was used by segregationists as an argument that it was to the advantage of Black children to be educated separately from the superior White children. In the 1960s, the debate was further revived as Nobel laureate William Shockley, publicly defended the argument that Black children were innately unable to learn as well as White children. Arthur Jensen stimulated scholarly discussion of the issue with his "Harvard Education Review" article, "How Much Can We Boost IQ and Scholastic Achievement?" Jensen's article questioned remedial education for African-American children; he suggested their poor educational performance reflected an underlying genetic cause rather than lack of stimulation at home. Jensen continued to publish on the issue until his death in 2012.
The Bell Curve debate.
Another revival of public debate followed the appearance of "The Bell Curve" (1994), a book by Richard Herrnstein and Charles Murray, who strongly emphasized the societal effects of low IQ (focusing in most chapters strictly on the non-Hispanic white population of the United States). In 1994 a group of 52 researchers (mostly psychologists) signed an editorial statement "Mainstream Science on Intelligence" in response to the book. "The Bell Curve" also led to a 1995 report from the American Psychological Association, "", acknowledging a difference between mean IQ scores of whites and blacks as well as the absence of any adequate explanation of it, either environmental or genetic. "The Bell Curve" prompted the publication of several multiple-author books responding from a variety of points of view. They include "The Bell Curve Debate" (1995), ' (1996) and a second edition of "The Mismeasure of Man" (1996) by Stephen Jay Gould. Jensen's last book-length publication, ' was published a few years later in 1998.
The review article "Thirty Years of Research on Race Differences in Cognitive Ability" by Rushton and Jensen was published in 2005. The article was followed by a series of responses, some in support, some critical. Richard Nisbett, another psychologist who had also commented at the time, later included an amplified version of his critique as part of the book "Intelligence and How to Get It: Why Schools and Cultures Count" (2009). Rushton and Jensen in 2010 made a point-by-point reply to this thereafter. A comprehensive review article on the issue was published in the journal "American Psychologist" in 2012.
Some of the authors proposing genetic explanations for group differences have received funding from the Pioneer Fund which was headed by Rushton until his death in 2012. The Southern Poverty Law Center lists the Pioneer Fund as a hate group, citing the fund's history, its funding of race and intelligence research, and its connections with racist individuals. On the other hand, Ulrich Neisser writes that "Pioneer has sometimes sponsored useful research—research that otherwise might not have been done at all." Other researchers have criticized the Pioneer Fund for promoting scientific racism, eugenics and white supremacy.
Validity of race and IQ.
Intelligence, IQ, "g" and IQ tests.
The concept of intelligence and the degree to which intelligence is measurable is a matter of debate. While there is some consensus about how to define intelligence, it is not universally accepted that it is something that can be unequivocally measured by a single figure. A recurring criticism is that different societies value and promote different kinds of skills and that the concept of intelligence is therefore culturally variable and cannot be measured by the same criteria in different societies. Consequently, some critics argue that proposed relationships to other variables are necessarily tentative.
In relation to the study of racial differences in IQ test scores it becomes a crucial question what exactly it is that IQ tests measure. Arthur Jensen was a proponent of the view that there is a correlation between scores on all the known types of IQ tests and that this correlation points to an underlying factor of general intelligence, or "g". In most conceptions of "g" it is considered to be fairly fixed in a given individual and unresponsive to training or other environmental influences. In this view test score differences, especially in those tasks considered to be particularly "g-loaded" reflect the test takers innate capability.
Other psychometricians argue that, while there may or may not be a general intelligence factor, performance on tests rely crucially on knowledge acquired through prior exposure to the types of tasks that such tests contain. This view would mean that tests cannot be expected to reflect only the innate abilities of a given individual, because the expression of potential will always be mediated by experience and cognitive habits. It also means that comparison of test scores from persons with widely different life experiences and cognitive habits is not an expression of their relative innate potentials.
Race.
The biological validity of race is disputed. The current mainstream view in the social sciences and biology is that race is a social construction based on folk ideologies that construct groups based on social disparities and superficial physical characteristics. state, "Race is a socially constructed concept, not a biological one. It derives from people's desire to classify." The concept of human "races" as natural and separate divisions within the human species has also been rejected by the American Anthropological Association. The official position of the AAA, adopted in 1998, finds that advances in scientific knowledge have made it "clear that human populations are not unambiguous, clearly demarcated, biologically distinct groups" and that "any attempt to establish lines of division among biological populations both arbitrary and subjective." However within population genetics there is ongoing debate about whether the social category of "race" can and should be used as a proxy for individual genetic ancestry. With current methods of genetic analysis it is possible to determine the composition of genetic ancestry of an individual with significant precision. This is because different genes occur with different frequencies in different geographically defined populations, and by correlating a large amount of genes through cluster analysis it is probable to determine with high likelihood the geographic origins of an individual through DNA. This suggests to some that the classical socially defined genetic categories really have a biological basis, in the sense that racial categorization is a visual estimate of a person's continental ancestry based on their phenotype—which correlates with genotypical ancestry as determined by DNA tests.
Race in studies of human intelligence is almost always determined using self-reports, rather than based on analyses of the genetic characteristics of the tested individuals. According to psychologist David Rowe, self-report is the preferred method for racial classification in studies of racial differences because classification based on genetic markers alone ignore the "cultural, behavioral, sociological, psychological, and epidemiological variables" that distinguish racial groups. Hunt and Carlson write that "Nevertheless, self-identification is a surprisingly reliable guide to genetic composition. applied mathematical clustering techniques to sort genomic markers for over 3,600 people in the United States and Taiwan into four groups. There was almost perfect agreement between cluster assignment and individuals' self-reports of racial/ethnic identification as white, black, East Asian, or Latino." Sternberg and Grigorenko disagree with Hunt and Carlson's interpretation of Tang, "Tang et al.'s point was that ancient geographic ancestry rather than current residence is associated with self-identification and not that such self-identification provides evidence for the existence of biological race."
Anthropologist C. Loring Brace and geneticist Joseph Graves contradict the notion that cluster analysis and the correlation between self-reported race and genetic ancestry support biological race. They argue that while it is possible to find biological and genetic variation corresponding roughly to the groupings normally defined as races, this is true for almost all geographically distinct populations. The cluster structure of the genetic data is dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clusters would be different. therefore concludes that, while differences in particular allele frequencies can be used to identify populations that loosely correspond to the racial categories common in Western social discourse, the differences are of no more biological significance than the differences found between any human populations (e.g., the Spanish and Portuguese).
Earl B. Hunt agrees that racial categories are defined by social conventions, though he points out that they also correlate with clusters of both genetic traits and cultural traits. Hunt explains that, due to this, racial IQ differences are caused by these variables that correlate with race, and race itself is rarely a causal variable. Researchers who study racial disparities in test scores are studying the relationship between the scores and the many race-related factors which could potentially affect performance. These factors include health, wealth, biological differences, and education.
Group differences.
The study of human intelligence is one of the most controversial topics in psychology. It remains unclear whether group differences in intelligence test scores are caused by heritable factors or by "other correlated demographic variables such as socioeconomic status, education level, and motivation."
Hunt and Carlson outlined four contemporary positions on differences in IQ based on race or ethnicity. The first is that these reflect real differences in average group intelligence, which is caused by a combination of environmental factors and heritable differences in brain function. A second position is that differences in average cognitive ability between races are caused entirely by social and/or environmental factors. A third position holds that differences in average cognitive ability between races do not exist, and that the differences in average test scores are the result of inappropriate use of the tests themselves. Finally, a fourth position is that either or both of the concepts of race and general intelligence are poorly constructed and therefore any comparisons between races are meaningless.
United States test scores.
East Asians have tended to score relatively higher on visuospatial subtests with lower scores in verbal subtests while Ashkenazi Jews score higher in verbal subtests with lower scores in visuospatial subtests. The few Amerindian populations who have been systematically tested, including Arctic Natives, tend to score worse on average than white populations but better on average than black populations.
The racial groups studied in the United States and Europe are not necessarily representative samples for populations in other parts of the world. Cultural differences may also factor in IQ test performance and outcomes. Therefore, results in the United States and Europe do not necessarily correlate to results in other populations.
Global variation of IQ scores.
A number of studies have compared average IQ scores between the world's nations, finding patterns of difference between continental populations similar to those associated with race. Richard Lynn and Tatu Vanhanen have argued that populations in the third world, particularly populations in Africa, tend to have limited intelligence because of their genetic composition and that, consequently, education cannot be effective in creating social and economic development in third world countries. Lynn and Vanhanen's studies have been severely criticized for relying on low quality data and for choosing sources in ways that seem to be biased severely towards underestimating the average IQ potential of developing nations, particularly in Africa. Nonetheless there is a general consensus that the average IQ in developing countries is lower than in developed countries, but subsequent research has favored environmental explanations for this fact, such as lack of basic infrastructure related to health and education.
In the 2002 book "IQ and the Wealth of Nations", and "IQ and Global Inequality" in 2006, Richard Lynn and Tatu Vanhanen created estimates of average IQs for 113 nations. They estimated IQs of 79 other nations based on neighboring nations or other via other manners. They saw a consistent correlation between national development and national IQ averages. They found the highest national IQs among Western and East Asian developed nations and the lowest national IQs in the world's least developed nations in Sub-Saharan Africa and Latin America. 
In a metaanalysis of studies of IQ estimates in Africa, concluded that Lynn and Vanhanen had relied on unsystematic methodology by failing to publish their criteria for including or excluding studies. They found that Lynn and Vanhanen's exclusion of studies had depressed their IQ estimate for sub-Saharan Africa, and that including studies excluded in "IQ and Global Inequality" resulted in average IQ of 82 for sub-Saharan Africa, lower than the average in Western countries, but higher than Lynn and Vanhanen's estimate of 67. Wicherts at al. conclude that this difference is likely due to sub-Saharan Africa having limited access to modern advances in education, nutrition and health care.
A 2007 meta-analysis by Rindermann found many of the same groupings and correlations found by Lynn and Vanhanen, with the lowest scores in sub-Saharan Africa, and a correlation of .60 between cognitive skill and GDP per capita. considers Rindermann's analysis to be much more reliable than Lynn and Vanhanen's. By measuring the relationship between educational data and social well-being over time, this study also performed a causal analysis, finding that when nations invest in education this leads to increased well-being later on.
Flynn effect and the closing gap.
For the past century raw scores on IQ tests have been rising; this score increase is known as the "Flynn effect," named after Jim Flynn. In the United States, the increase was continuous and approximately linear from the earliest years of testing to about 1998 when the gains stopped and some tests even showed decreasing test scores. For example, in the United States the average scores of blacks on some IQ tests in 1995 were the same as the scores of whites in 1945. Flynn has argued that given that these changes take place between one generation and the next it is highly unlikely that genetic factors could account for the increasing scores, which must then be caused by environmental factors. The Flynn Effect has often been used as an argument that the racial gap in IQ test scores must be environmental too, but this is not generally agreed – others have asserted that the two may have entirely different causes. A meta-analysis by Te Nijenhuis and van der Flier (2013) concluded that the Flynn effect and group differences in intelligence were likely to have different causes. They stated that the Flynn effect is caused primarily by environmental factors and that it's unlikely these same environmental factors play an important role in explaining group differences in IQ. The importance of the Flynn effect in the debate over the causes for the IQ gap lies in demonstrating that environmental factors may cause changes in test scores on the scale of 1 SD. This had previously been doubted.
A separate phenomenon from the Flynn effect has been the discovery that the IQ gap has been gradually closing over the last decades of the 20th century, as black test-takers increased their average scores relative to white test-takers. A 2006 study by Dickens and Flynn estimated that the difference between mean scores of blacks and whites closed by about 5 or 6 IQ points between 1972 and 2002, which would be a reduction of about one-third. In the same period the educational achievement disparity also diminished. However this was challenged by Rushton & Jensen who claim the difference remains stable. In a 2006 study, Murray agreed with Dickens and Flynn that there has been a narrowing of the difference; "Dickens' and Flynn's estimate of 3–6 IQ points from a base of about 16–18 points is a useful, though provisional, starting point". But he argued that this has stalled and that there has been no further narrowing for people born after the late 1970s. Recent reviews by Flynn and Dickens (2006), Hunt (2011), Mackintosh (2011), Nisbett et al. 2012 accept the gradual closing of the gap as a fact. cite who consider arguments to the contrary by Rushton, Jensen and Murray to be erroneous.
Some studies reviewed by found that rise in the average achievement of African Americans was caused by a reduction in the number of African American students in the lowest range of scores without a corresponding increase in the number of students in the highest ranges. A 2012 review of the literature found that the IQ gap had diminished by 0.33 standard deviations since first reported.
Environmental influences on group differences in IQ.
The following environmental factors are some of those suggested as explaining a portion of the differences in average IQ between races. These factors are not mutually exclusive with one another, and some may in fact contribute directly to others. Furthermore, the relationship between genetics and environmental factors may be complicated. For example, the differences in socioeconomic environment for a child may be due to differences in genetic IQ for the parents, and the differences in average brain size between races could be the result of nutritional factors. All recent reviews agree that some environmental factors that are unequally distributed between racial groups have been shown to affect intelligence in ways that could contribute to the test score gap. However currently the question is whether these factors can account for the entire gap between white and black test scores, or only part of it. One group of scholars, including Richard Nisbett, James R. Flynn, Joshua Aronson, Diane Halpern, William Dickens, Eric Turkheimer (2012) have argued that the environmental factors so far demonstrated are sufficient to account for the entire gap, Nicholas Mackintosh(2011) considers this a reasonable argument, but argues that probably it is impossible to ever know for sure; Another group including Earl B. Hunt (2010), Arthur Jensen, J. Philippe Rushton and Richard Lynn have argued that this is impossible. Jensen and Rushton consider that it may account for as little as 20% of the gap W.hie Hunt considers this a vast overstatement he nonetheless considers it likely that some portion of the gap will eventually be shown to be caused by genetic factors.
Test bias.
A number of studies have reached the conclusion that IQ tests may be biased against certain groups. The validity and reliability of IQ scores obtained from outside the United States and Europe have been questioned, in part because of the inherent difficulty of comparing IQ scores between cultures. Several researchers have argued that cultural differences limit the appropriateness of standard IQ tests in non-industrialized communities.
However, a 1996 report by the American Psychological Association states that controlled studies show that differences in mean IQ scores were not substantially due to bias in the content or administration of the IQ tests. Furthermore, the tests are equally valid predictors of future achievement for black and white Americans. This view is reinforced by Nicholas Mackintosh in his 1998 book "IQ and Human Intelligence", and by a 1999 literature review by . Today test bias in the sense that some test items systematically give White test takers an unfair advantage because of the way the test has been elaborated is no longer considered a likely cause of the test score gap. However reviews by Hunt (2011) and Mackintosh (2011) do admit the possibility that IQ tests measure a cognitive skill that Blacks have less chance to develop, and that there is in this sense a bias in society that causes one group to perform under their true potential on the tests. But both scholars maintain that there is no evidence that current tests are systemically biased against black test takers.
Stereotype threat and minority status.
Stereotype threat is the fear that one's behavior will confirm an existing stereotype of a group with which one identifies or by which one is defined; this fear may in turn lead to an impairment of performance. Testing situations that highlight the fact that intelligence is being measured tend to lower the scores of individuals from racial-ethnic groups who already score lower on average or are expected to score lower. Stereotype threat conditions cause larger than expected IQ differences among groups. Psychometrician Nicholas Mackintosh considers that there is little doubt that the effects of stereotype threat contribute to the IQ gap between blacks and whites.
A large number of studies have shown that systemically disadvantaged minorities, such as the African American minority of the United States generally perform worse in the educational system and in intelligence tests than the majority groups or less disadvantaged minorities such as immigrant or "voluntary" minorities. The explanation of these findings may be that children of caste-like minorities, due to the systemic limitations of their prospects of social advancement, do not have "effort optimism", i.e. they do not have the confidence that acquiring the skills valued by majority society, such as those skills measured by IQ tests, is worthwhile. They may even deliberately reject certain behaviors that are seen as "acting white."
Socioeconomic environment.
Different aspects of the Socioeconomic environment in which children are raised have been shown to correlate with part of the IQ gap, but they do not account for the entire gap. Generally the difference between mean test scores of blacks and whites is not eliminated when individuals and groups are matched on SES, suggesting that the relationship between IQ and SES is not simply one in which SES determines IQ. Rather it may be the case that differences in intelligence, particularly parental intelligence, may also cause differences in SES, making separating the two factors difficult. summarises data showing that, jointly, SES and parental IQ account for the full gap (in populations of young children, after controlling parental IQ and parental SES, the gap is not statistically different from zero). He argues the SES-linked components reflect parental occupation status, mother's verbal comprehension score and parent-child interaction quality. Hunt also reviews data showing that the correlation between home environment and IQ becomes weaker with age. 
Other research has focussed on different causes of variation within low SES and high SES groups.
In the US, among low-SES groups, genetic differences account for a smaller proportion variance in IQ than among higher SES populations. Such effects are predicted by the "bioecological" hypothesis – that genotypes are transformed into phenotypes through nonadditive synergistic effects of the environment. suggest that high SES individuals are more likely to be able to develop their full biological potential, whereas low SES individuals are likely to be hindered in their development by adverse environmental conditions. The same review also points out that adoption studies generally are biased towards including only high and high middle SES adoptive families, meaning that they will tend to overestimate average genetic effects. They also note that studies of adoption from lower-class homes to middle-class homes have shown that such children experience a 12 - 18 pt gain in IQ relative to children who remain in low SES homes.
Health and nutrition.
Environmental factors including lead exposure, breast feeding, and nutrition can significantly affect cognitive development and functioning. For example, iodine deficiency causes a fall, on average, of 12 IQ points. Such impairments may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. The first two years of life is the critical time for malnutrition, the consequences of which are often irreversible and include poor cognitive development, educability, and future economic productivity. The African American population of the United States is statistically more likely to be exposed to many detrimental environmental factors such as poorer neighborhoods, schools, nutrition, and prenatal and postnatal health care. Mackintosh points out that for American Blacks infant mortality is about twice as high as for whites, and low birthweight is twice as prevalent. At the same time white mothers are twice as likely to breastfeed their infants, and breastfeeding is highly correlated with IQ for low birthweight infants. In this way a wide number of health related factors that influence IQ are unequally distributed between the two groups.
The Copenhagen consensus in 2004 stated that lack of both iodine and iron has been implicated in impaired brain development, and this can affect enormous numbers of people: it is estimated that one-third of the total global population are affected by iodine deficiency. In developing countries, it is estimated that 40% of children aged four and under suffer from anaemia because of insufficient iron in their diets.
Other scholars have found that simply the standard of nutrition has a significant effect on population intelligence, and that the Flynn effect may be caused by increasing nutrition standards across the world. James Flynn has himself argued against this view.
Some recent research has argued that the retardation caused in brain development by infectious diseases, many of which are more prevalent in non-White populations, may be an important factor in explaining the differences in IQ between different regions of the world. The findings of this research, showing the correlation between IQ, race and infectious diseases was also shown to apply to the IQ gap in the US, suggesting that this may be an important environmental factor.
Education.
Several studies have proposed that a large part of the gap can be attributed to differences in quality of education. Racial discrimination in education has been proposed as one possible cause of differences in educational quality between races. According to a paper by Hala Elhoweris, Kagendo Mutua, Negmeldin Alsheikh and Pauline Holloway, teachers' referral decisions for students to participate in gifted and talented educational programs were influenced in part by the students' ethnicity.
The Abecedarian Early Intervention Project, an intensive early childhood education project, was also able to bring about an average IQ gain of 4.4 points at age 21 in the black children who participated in it compared to controls. Arthur Jensen agreed that the Abecedarian project demonstrates that education can have a significant effect on IQ, but also said that no educational program thus far has been able to reduce the black-white IQ gap by more than a third, and that differences in education are thus unlikely to be its only cause.
Rushton and Jensen argue that long-term follow-up of the Head Start Program found large immediate gains for blacks and whites but that these were quickly lost for the blacks although some remained for whites. They argue that also other more intensive and prolonged educational interventions have not produced lasting effects on IQ or scholastic performance. Nisbett argues that they ignore studies such as which found that at the age 12, 87% black of infants exposed to an intervention had IQs in the normal range (above 85) compared to 56% of controls, and none of the intervention-exposed children were mildly retarded compared to 7% of controls. Other early intervention programs have shown IQ effects in the range of 4–5 points, which are sustained until at least age 8–15. Effects on academic achievement can also be substantial. Nisbett also argues that not only early age intervention can be effective, citing other successful intervention studies from infancy to college.
A series of studies by Fagan and Holland, measured the effect of prior exposure to the kind of cognitive tasks posed in IQ tests on test performance. Assuming that the IQ gap was the result of lower exposure to tasks using the cognitive functions usually found in IQ tests among African American test takes, they prepared a group of African Americans in this type of tasks before taking an IQ test. The researchers found that there was no subsequent difference in performance between the African-Americans and White test takers. Daley and Onwugbuezie conclude that Fagan and Holland demonstrate that "differences in knowledge between Blacks and Whites for intelligence test items can be erased when equal opportunity is provided for exposure to the information to be tested". A similar argument is made by David Marks who argues that IQ differences correlate well with differences in literacy suggesting that developing literacy skills through education causes an increase in IQ test performance.
Research into the possible genetic influences on test score differences.
It is well-established that intelligence is highly heritable for individuals, and many different kinds of genetically caused intelligence impairments are known. But the possible relations between genetic differences in intelligence within the normal range are not established. Ongoing research aims to understand the contribution of genes to individual differences in intelligence. Currently there is no non-circumstantial evidence that the test score gap has a genetic component, although some researchers believe that the existing circumstantial evidence makes it plausible to believe that hard evidence for a genetic component will eventually appear. Several lines of investigation have been followed in the attempt to ascertain whether there is a genetic component to the test score gap as well as its relative contribution to the magnitude of the gap.
Genetics of race and intelligence.
The decoding of the human genome has enabled scientists to search for sections of the genome that may contribute to cognitive abilities.
Geneticist, Alan R. Templeton argues that the question about the possible genetic effects on the test score gap is muddled by the general focus on "race" rather than on populations defined by gene frequency or by geographical proximity, and by the general insistence on phrasing the question in terms of heritability. Templeton points out that racial groups neither represent sub-species nor distinct evolutionary lineages, and that therefore there is no basis for making claims about the general intelligence of races. From this point of view the search for possible genetic influences on the black-white test score gap is a priori flawed, because there is no genetic material shared by all Africans or by all Europeans. points out that by using genetic cluster analysis to correlate gene frequencies with continental populations it could possibly be the case that African populations had a higher frequency of certain genetic variants that contribute to an average lower intelligence. Such a hypothetical situation could hold without all Africans carrying the same genes or belonging to a single Evolutionary lineage. According to Mackintosh, a biological basis for the gap thus cannot be ruled out on a priori grounds.
Intelligence is a polygenic trait. This means that intelligence is under the influence of several genes, possibly several thousand. The effect of most individual genetic variants on intelligence is thought to be very small, well below 1% of the variance in "g". Current studies using quantitative trait loci have yielded little success in the search for genes influencing intelligence. Robert Plomin is confident that QTLs responsible for the variation in IQ scores exist, but due to their small effect sizes, more powerful tools of analysis will be required to detect them. Others assert that no useful answers can be reasonably expected from such research before an understanding of the relation between DNA and human phenotypes emerges. Several candidate genes have been proposed to have a relationship with intelligence. However, a review of candidate genes for intelligence published in failed to find evidence of an association between these genes and general intelligence, stating "there is still almost no replicated evidence concerning the individual genes, which have variants that contribute to intelligence differences".
A 2005 literature review article by Sternberg, Grigorenko and Kidd stated that no gene has been shown to be linked to intelligence, "so attempts to provide a compelling genetic link of race to intelligence are not feasible at this time". and concurred, both scholars noting that while several environmental factors have been shown to influence the IQ gap, the evidence for a genetic influence has been circumstantial, and according to Mackintosh negligible. Mackintosh however suggests that it may never become possible to account satisfyingly for the relative contributions of genetic and environmental factors. The 2012 review by the concluded that "Almost no genetic polymorphisms have been discovered that are consistently associated with variation in IQ in the normal range". Hunt and several other researchers however maintain that genetic causes cannot be ruled out and that new evidence may yet show a genetic contribution to the gap. Hunt concurs with Rushton and Jensen who considered the 100% environmental hypothesis to be impossible. Nonetheless, Nisbett and colleagues (2012) consider the entire IQ gap to be explained by the environmental factors that have thus far been demonstrated to influence it, and Mackintosh does not find this view to be unreasonable.
Heritability within and between groups.
Intelligence as tested by IQ tests is generally considered to be highly heritable. Psychometricians have found that intelligence is substantially heritable within populations, with 30–50% of variance in IQ scores in early childhood being attributable to genetic factors in analyzed US populations, increasing to 75–80% by late adolescence. In biology heritability is defined as the ratio of variation attributable to genetic differences in an observable trait to the trait's total observable variation. The heritability of a trait describes the proportion of variation in the trait that is attributable to genetic factors within a particular population. A heritability of 1 indicates that variation correlates fully with genetic variation and a heritability of 0 indicates that there is no correlation between the trait and genes at all. In psychological testing heritability tends to be understood as the degree of correlation between the results of a test taker and those of their biological parents. However, since high heritability is simply a correlation between traits and genes, it does not describe the causes of heritability which in humans can be either genetic or environmental.
Therefore, a high heritability measure does not imply that a trait is genetic or unchangeable, however, as environmental factors that affect all group members equally will not be measured by heritability and the heritability of a trait may also change over time in response to changes in the distribution of genes and environmental factors. High heritability also doesn't imply that all of the heritability is genetically determined, but can also be due to environmental differences that affect only a certain genetically defined group (indirect heritability). The figure to the left demonstrates how heritability works. In both gardens the difference between tall and short cornstalks is 100% heritable as cornstalks that are genetically disposed for growing tall will become taller than those without this disposition, but the difference in height between the cornstalks to the left and those on the right is 100% environmental as it is due to different nutrients being supplied to the two gardens. Hence the causes of differences within a group and between groups may not be the same, even when looking at traits that are highly heritable.
In regards to the IQ gap the question becomes whether racial groups can be shown to be influenced by different environmental factors that may account for the observed differences between them. Jensen originally argued that given the high heritability of IQ the only way that the IQ gap could be explained as caused by the environment would be if it could be shown that all blacks were subject to a single "x-factor" which affected no white populations while affecting all black populations equally. Jensen considered the existence of such an x-factor to be extremely improbable, but Flynn's discovery of the Flynn effect showed that in spite of high heritability environmental factors could cause considerable disparities in IQ between generations of the same population, showing that the existence of such an x-factor was not only possible but real.
Jensen has also argued that heritability of traits rises with age as the genetic potential of individuals becomes expressed. He sees this as related to the fact that the IQ gap between white and black test takers has been shown to appear gradually, with the gap widening as cohorts reach adulthood. This he sees as a further argument in favor of Spearman's hypothesis (see section below).
In contrast, Dickens and Flynn argued that the conventional interpretation ignores the role of feedback between factors, such as those with a small initial IQ advantage, genetic or environmental, seeking out more stimulating environments which will gradually greatly increase their advantage, which, as one consequence in their alternative model, would mean that the "heritability" figure is only in part due to direct effects of genotype on IQ.
Today researchers such as , and consider that rather than a single factor accounting for the entire gap, probably many different environmental factors differ systematically between the environments of White and Black people converge to create part of the gap and perhaps all of it. They argue that it does not make sense to talk about a single universal heritability figure for IQ, rather, they state, heritability of IQ varies between and within groups. They point specifically to studies showing a higher heritability of test scores in White and medium-high SES families, but considerably lower heritability for Black and low-SES families. This they interpret to mean that children who grow up with limited resources do not get to develop their full genetic potential.
Spearman's hypothesis.
Spearman's hypothesis states that the magnitude of the black-white difference in tests of cognitive ability is entirely or mainly a function of the extent to which a test measures general mental ability, or "g". The hypothesis was first formalized by Arthur Jensen who devised the statistical Method of Correlated Vectors to test it. Jensen holds that if Spearman's hypothesis holds true then some cognitive tasks have a higher g-load than others, and that these tasks are exactly the tasks in which the gap between Black and White test takers are greatest. Jensen, and other psychometricians such as Rushton and Lynn, take this to show that the cause of "g" and the cause of the gap are the same—in their view genetic differences.
James argues that there is an inherent flaw in Jensen's argument that the correlation between g-loadings, test scores and heritability support a genetic cause of the gap. He points out that as the difficulty of a task increases a low performing group will naturally fall further behind, and heritability will therefore also naturally increase. The same holds for increases in performance which will first affect the least difficult tasks, but only gradually affect the most difficult ones. Flynn thus sees the correlation between in g-loading and the test score gap to offer no clue to the cause of the gap.
Adoption studies.
A number of studies have been done on the effect of similar rearing conditions on children from different races. The hypothesis is that by investigating whether black children adopted into white families demonstrated gains in IQ test scores relative to black children reared in black families. Depending on whether their test scores are more similar to their biological or adoptive families, that could be interpreted as either supporting a genetic or an environmental hypothesis. The main point of critique in studies like these however whether the environment of black children even when raised in White families are truly comparable to the environment of White children. Several reviews of the adoption study literature has pointed out that it is perhaps impossible to avoid confounding of biological and environmental factors in this type of studies. Given the differing heritability estimates in medium-high SES and low-SES families, argue that adoption studies on the whole tend to overstate the role of genetics because they represent a restricted set of environments, mostly in the medium-high SES range.
The Minnesota Transracial Adoption Study (1976) examined the IQ test scores of 122 adopted children and 143 nonadopted children reared by advantaged white families. The children were restudied ten years later. The study found higher IQ for whites compared to blacks, both at age 7 and age 17. cite the Minnesota study as providing support to a genetic explanation. Nonetheless, acknowledging the existence of confounding factors, Scarr and Weinberg the authors of the original study, did not themselves consider that it provided support for either the hereditarian or environmentalist view.
Three other adoption studies found contrary evidence to the Minnesota study, lending support to a mostly environmental hypothesis: 
Rushton and Jensen have argued that unlike the Minnesota Transracial Adoption Study, these studies did not retest the children post-adolescence when heritability of IQ would presumably be higher. however point out that the difference in heritability between ages 7 and 17 are quite small, and that consequently this is no reason to disregard Moore's findings.
Frydman and Lynn (1989) showed a mean IQ of 119 for Korean infants adopted by Belgian families. After correcting for the Flynn effect, the IQ of the adopted Korean children was still 10 points higher than the indigenous Belgian children.
Reviewing the evidence from adoption studies Mackintosh considers the studies by Tizard and Eyferth to be inconclusive, and the Minnesota study to be consistent only with a partial genetic hypothesis. On the whole he finds that environmental and genetic variables remain confounded and considers evidence from adoption studies inconclusive on the whole, and fully compatible with a 100% environmental explanation.
Racial admixture studies.
Most people have an ancestry from different geographic regions, particularly African Americans typically have ancestors from both Africa and Europe, with, on average, 20% of their genome inherited from European ancestors. If racial IQ gaps have a partially genetic basis, one might expect blacks with a higher degree of European ancestry to score higher on IQ tests than blacks with less European ancestry, because the genes inherited from European ancestors would likely include some genes with a positive effect on IQ. Geneticist Alan Templeton has argued that an experiment based on the Mendelian "common garden" design where specimens with different hybrid compositions are subjected to the same environmental influences, would be the only way to definitively show a causal relation between genes and IQ. Summarizing the findings of admixture studies, he concludes that it has shown no significant correlation between any cognitive and the degree of African or European ancestry.
Studies have employed different ways of measuring or approximating relative degrees of ancestry from Africa and Europe. One set of studies have used skin color as a measure, and other studies have used blood groups. surveys the literature and argues that the blood groups studies may be seen as providing some support to the genetic hypothesis, even though the correlation between ancestry and IQ was quite low. He finds that studies by , Willerman, Naylor & Myrianthopoulos (1970) did not find a correlation between degree of African&/European ancestry and IQ. The latter study did find a difference based on the race of the mother, with children of white mothers with black fathers scoring higher than children of black mothers and white fathers. Loehlin considers that such a finding is compatible with either a genetic or an environmental cause. All in all Loehlin finds admixture studies inconclusive and recommends more research.
Another study cited by , and by , was study which found that adopted mixed-race children's has test scores identical to children with two black parents - receiving no apparent "benefit" from their white ancestry. Rushton and Jensen find admixture studies to have provided overall support for a genetic explanation though this view is not shared by , , , nor by .
Reviewing the evidence from admixture studies considers it to be inconclusive because of too many uncontrolled variables. quotes a statement by to the effect that admixture studies have not provided a shred of evidence in favor of a genetic basis for the gap.
Mental chronometry.
Mental chronometry measures the elapsed time between the presentation of a sensory stimulus and the subsequent behavioral response by the participant. This reaction time (RT) is considered a measure of the speed and efficiency with which the brain processes information. Scores on most types of RT tasks tend to correlate with scores on standard IQ tests as well as with "g", and no relationship has been found between RT and any other psychometric factors independent of "g". The strength of the correlation with IQ varies from one RT test to another, but Hans Eysenck gives 0.40 as a typical correlation under favorable conditions. According to Jensen individual differences in RT have a substantial genetic component, and heritability is higher for performance on tests that correlate more strongly with IQ. Nisbett argues that some studies have found correlations closer to 0.2, and that the correlation is not always found.
Several studies have found differences between races in average reaction times. These studies have generally found that reaction times among black, Asian and white children follow the same pattern as IQ scores. have argued that reaction time is independent of culture and that the existence of race differences in average reaction time is evidence that the cause of racial IQ gaps is partially genetic instead of entirely cultural. Responding to this argument in "Intelligence and How to Get It", Nisbett has pointed to the study in which a group of Chinese Americans had longer reaction times than a group of European Americans, despite having higher IQs. Nisbett also mentions findings in and suggesting that movement time (the measure of how long it takes a person to move a finger after making the decision to do so) correlates with IQ just as strongly as reaction time, and that average movement time is faster for blacks than for whites. considers reaction time evidence unconvincing and points out that other cognitive tests that also correlate well with IQ show no disparity at all, for example the habituation/dishabituation test. And he points out that studies show that rhesus monkeys have shorter reaction times than American college students, suggesting that different reaction times may not tell us anything useful about intelligence.
Brain size.
A number of studies have reported a moderate statistical correlation between differences in IQ and brain size between individuals in the same group. And some scholars have reported differences in average brain sizes between Africans, Europeans and Asians. J. P. Rushton has argued that Africans on average have smaller brain cases and brains than Europeans, and that Europeans have smaller brains than East Asians, and that this is evidence that the gap is biological in nature. Critics of Rushton have argued that Rushton's arguments rest on outdated data collected by unsound methods and should be considered invalid. Recent reviews by and consider that current data does show an average difference in brain size and head-circumference between American Blacks and Whites, but question whether this has any relevance for the IQ gap. Nesbitt et al. argue that crude brain size is unlikely to be a good measure of IQ; for example, brain size also differs between men and women, but without well documented differences in IQ. At the same time newborn Black children have the same average brain size as Whites, suggesting that the difference in average size could be accounted for by differences in postnatal environment. Several factors that reduce brain size have been demonstrated to disproportionately affect Black children.
Earl Hunt states that brain size is found to have a correlation of about .35 with intelligence among whites and cites studies showing that genes may account for as much as 90% of individual variation in brain size. According to Hunt, race differences in average brain size could potentially be an important argument for a possible genetic contribution to racial IQ gaps. Nonetheless, Hunt notes that Rushton's head size data would account for a difference of .09 standard deviations between Black and White average test scores, less than a tenth of the 1.0 standard deviation gap in average scores that is observed.
Policy relevance and ethics.
The 1996 report of the APA commented on the ethics of research on race and intelligence. as well as have also discussed different possible ethical guidelines. "Nature" in 2009 featured two editorials on the ethics of research in race and intelligence by Steven Rose (against) and Stephen J. Ceci and Wendy M. Williams (for).
According to critics, research on group differences in IQ will reproduce the negative effects of social ideologies (such as Nazism or social Darwinism) that were justified in part on claimed hereditary racial differences. Steven Rose maintains that the history of eugenics makes this field of research difficult to reconcile with current ethical standards for science.
Linda Gottfredson argues that suggestion of higher ethical standards for research into group differences in intelligence is a double standard applied in order to undermine disliked results. James R. Flynn has argued that had there been a ban on research on possibly poorly conceived ideas, much valuable research on intelligence testing (including his own discovery of the Flynn effect) would not have occurred.
Jensen and Rushton argued that the existence of biological group differences does not rule out, but raises questions about the worthiness of policies such as affirmative action or placing a premium on diversity. They also argued for the importance of teaching people not to overgeneralize or stereotype individuals based on average group differences, because of the significant overlap of people with varying intelligence between different races.
The environmentalist viewpoint argues for increased interventions in order to close the gaps. Nisbett argues that schools can be greatly improved and that many interventions at every age level are possible. Flynn, arguing for the importance of the black subculture, writes that "America will have to address all the aspects of black experience that are disadvantageous, beginning with the regeneration of inner city neighbourhoods and their schools. A resident police office and teacher in every apartment block would be a good start." Researchers from both sides agree that interventions should be better researched.
Especially in developing nations, society has been urged to take on the prevention of cognitive impairment in children as of the highest priority. Possible preventable causes include malnutrition, infectious diseases such as meningitis, parasites, cerebral malaria, in utero drug and alcohol exposure, newborn asphyxia, low birth weight, head injuries, lead poisoning and endocrine disorders.

</doc>
<doc id="26495" url="https://en.wikipedia.org/wiki?curid=26495" title="Retirement">
Retirement

Retirement is the point where a person stops employment completely. A person may also semi-retire by reducing work hours.
An increasing number of individuals are choosing to put off this point of total retirement, by selecting to exist in the emerging state of Pre-tirement.
Many people choose to retire when they are eligible for private or public pension benefits, although some are forced to retire when physical conditions no longer allow the person to work any longer (by illness or accident) or as a result of legislation concerning their position. In most countries, the idea of retirement is of recent origin, being introduced during the late 19th and early 20th centuries. Previously, low life expectancy and the absence of pension arrangements meant that most workers continued to work until death. Germany was the first country to introduce retirement, in 1889.
Nowadays most developed countries have systems to provide pensions on retirement in old age, which may be sponsored by employers and/or the state. In many poorer countries, support for the old is still mainly provided through the family. Today, retirement with a pension is considered a right of the worker in many societies, and hard ideological, social, cultural and political battles have been fought over whether this is a right. In many western countries this right is mentioned in national constitutions.
Retirement in specific countries.
A person may retire at whatever age they please. However, a country's tax laws and/or state old-age pension rules usually mean that in a given country a certain age is thought of as the "standard" retirement age.
The "standard" retirement age varies from country to country but it is generally between 50 and 70 (according to latest statistics, 2011). In some countries this age is different for males and females, although this has recently been challenged in some countries (e.g., Austria), and in some countries the ages are being brought into line. The table below shows the variation in eligibility ages for public old-age benefits in the United States and many European countries, according to the OECD.
Notes: Parentheses indicate eligibility age for women when different. Sources: Cols. 1–2: OECD Pensions at a Glance (2005), Cols. 3–6: Tabulations from HRS, ELSA and SHARE. Square brackets indicate early retirement for some public employees.
In the United States, while the normal retirement age for Social Security, or Old Age Survivors Insurance (OASI), historically has been age 65 to receive unreduced benefits, it is gradually increasing to age 67. For those turning 65 in 2008, full benefits will be payable beginning at age 66. Public servants are often not covered by Social Security but have their own pension programs. Police officers in the United States are typically allowed to retire at half pay after only 20 years of service or three-quarter pay after 30 years, allowing people to retire in their early forties or fifties. Military members of the US Armed Forces may elect to retire after 20 years of active duty. Their retirement pay (not a pension since they can be involuntarily called back to active duty at any time) is calculated on total number of years on active duty, their final pay grade and the retirement system in place when they entered service. Allowances such as housing and subsistence are not used to calculate a member's retired pay. Members awarded the Medal of Honor qualify for a separate stipend, regardless of the years of service. Military members in the reserve and US National Guard have their retirement based on a point system. 
Data sets.
Recent advances in data collection have vastly improved our ability to understand important relationships between retirement and factors such as health, wealth, employment characteristics and family dynamics, among others. The most prominent study for examining retirement behavior in the United States is the ongoing Health and Retirement Study (HRS), first fielded in 1992. The HRS is a nationally representative longitudinal survey of adults in the U.S. ages 51+, conducted every two years, and contains a wealth of information on such topics as labor force participation (e.g., current employment, job history, retirement plans, industry/occupation, pensions, disability), health (e.g., health status and history, health and life insurance, cognition), financial variables (e.g., assets and income, housing, net worth, wills, consumption and savings), family characteristics (e.g., family structure, transfers, parent/child/grandchild/sibling information) and a host of other topics (e.g., expectations, expenses, internet use, risk taking, psychosocial, time use).
2002 and 2004 saw the introductions of the English Longitudinal Study of Ageing (ELSA) and the Survey of Health, Ageing and Retirement in Europe (SHARE), which includes respondents from 14 continental European countries plus Israel. These surveys were closely modeled after the HRS in sample frame, design and content. A number of other countries (e.g., Japan, South Korea) also now field HRS-like surveys, and others (e.g., China, India) are currently fielding pilot studies. These data sets have expanded the ability of researchers to examine questions about retirement behavior by adding a cross-national perspective.
Notes: MHAS discontinued in 2003; ELSA numbers exclude institutionalized (nursing homes). Source: Borsch-Supan et al., eds. (November 2008). Health, Ageing and Retirement in Europe (2004–2007): Starting the Longitudinal Dimension.
Factors affecting retirement decisions.
Many factors affect people's retirement decisions. Social Security clearly plays an important role. In countries around the world, people are much more likely to retire at the early and normal retirement ages of the public pension system (e.g., ages 62 and 65 in the U.S.). This pattern cannot be explained by different financial incentives to retire at these ages since typically retirement benefits at these ages are approximately actuarially fair; that is, the present value of lifetime pension benefits (pension wealth) conditional on retiring at age "a" is approximately the same as pension wealth conditional on retiring one year later at age "a"+1. Nevertheless, a large literature has found that individuals respond significantly to financial incentives relating to retirement (e.g., to discontinuities stemming from the Social Security earnings test or the tax system).
Greater wealth tends to lead to earlier retirement, since wealthier individuals can essentially "purchase" additional leisure. Generally the effect of wealth on retirement is difficult to estimate empirically since observing greater wealth at older ages may be the result of increased saving over the working life in anticipation of earlier retirement. However, a number of economists have found creative ways to estimate wealth effects on retirement and typically find that they are small. For example, one paper exploits the receipt of an inheritance to measure the effect of wealth shocks on retirement using data from the HRS. The authors find that receiving an inheritance increases the probability of retiring earlier than expected by 4.4 percentage points, or 12 percent relative to the baseline retirement rate, over an eight-year period.
A great deal of attention has surrounded how the financial crisis (2007 - ?) is affecting retirement decisions, with the conventional wisdom saying that fewer people will retire since their savings have been depleted; however recent research suggests that the opposite may happen. Using data from the HRS, researchers examined trends in defined benefit (DB) vs. defined contribution (DC) pension plans and found that those nearing retirement had only limited exposure to the recent stock market decline and thus are not likely to substantially delay their retirement. At the same time, using data from the Current Population Survey (CPS), another study estimates that mass layoffs are likely to lead to an increase in retirement almost 50% larger than the decrease brought about by the stock market crash, so that on net retirements are likely to increase in response to the crisis.
More information tells of how many who retire will continue to work, but not in the career they have had for the majority of their life. Job openings will increase in the next 5 years due to retirements of the baby boomer generation. The Over 50 population is actually the fastest growing labor groups in the US.
A great deal of research has examined the effects of health status and health shocks on retirement. It is widely found that individuals in poor health generally retire earlier than those in better health. This does not necessarily imply that poor health status leads people to retire earlier, since in surveys retirees may be more likely to exaggerate their poor health status to justify their earlier decision to retire. This justification bias, however, is likely to be small. In general, declining health over time, as well as the onset of new health conditions, have been found to be positively related to earlier retirement. Health conditions that can cause someone to retire include hypertension, diabetes mellitus, sleep apnea, joint diseases, and hyperlipidemia.
Most people are married when they reach retirement age; thus, spouse's employment status may affect one's decision to retire. On average, husbands are three years older than their wives in the U.S., and spouses often coordinate their retirement decisions. Thus, men are more likely to retire if their wives are also retired than if they are still in the labor force, and vice versa.
EU Member States.
Researchers analyzed factors affecting retirement decisions in EU Member States:
Saving for retirement.
Retired workers support themselves either through pensions or savings. In most cases the money is provided by the government, but sometimes granted only by private subscriptions to mutual funds. In this latter case, subscriptions might be compulsory or voluntary. In some countries an additional "bonus" is granted "una tantum" (once only) in proportion to the years of work and the average wages; this is usually provided by the employer.
The financial weight of provision of pensions on a government's budget is often heavy and is the reason for political debates about the retirement age. The state might be interested in a later retirement age for economic reasons.
The cost of health care in retirement is large, because people tend to be ill more frequently in later life. Most countries provide universal health insurance coverage for seniors, although in the United States many people retire before they become eligible for Medicare at age 65. In 2006, Medicare Part D went into effect, expanding benefits to include prescription drug coverage.
A poll made in Washington said many people were unaware that "medicare doesn't pay for the most common types of long-term care" (Neergaard); 37 percent of Americans who took the survey believe that it does cover it. Medicaid is a federal-state program for the needy and the main source seniors use to pay their long-term care.
Overall, income after retirement can come from state pensions, occupational pensions, private savings and investments (private pension funds, owned housing), donations (e.g. by children), and social benefits. On a personal level, the rising cost of living during retirement is a serious concern to many older adults. Health care costs play an important role.
Retirement calculators.
A useful and straightforward calculation can be done if we assume that interest, after expenses, taxes and inflation is zero. Assume that in real (after-inflation) terms, your salary never changes during your "w" years of working life. During your "p" years of pension, you have a living standard which costs a replacement ratio "R" times as much as your living standard in your working life. Your working life living standard is your salary less the proportion of salary Z that you need to save. Calculations are per unit salary, e.g. assume salary = 1.
Then after "w" years work, retirement age accumulated savings = "wZ". To pay for pension for "p" years, necessary savings at retirement = "Rp(1-Z)"
Equate these: "wZ" = "Rp"("1-Z") and solve to give "Z" = "Rp" / ("w + Rp"). For example, if "w" = 35, "p" = 30 and "R" = 0.65 we find that we need to save a proportion "Z" = 35.78% of our salary.
Retirement calculators generally accumulate a proportion of salary up to retirement age. This shows a straightforward case which nonetheless could be practically useful for optimistic people hoping to work for only as long as they are likely to be retired. References relevant to the zero real interest assumption are listed here
For more complicated situations, there are several online retirement calculators on the Internet. Many retirement calculators project how much an investor needs to save, and for how long, to provide a certain level of retirement expenditures. Some retirement calculators, appropriate for safe investments, assume a constant, unvarying rate of return. Monte Carlo retirement calculators take volatility into account, and project the probability that a particular plan of retirement savings, investments and expenditures will outlast the retiree. Retirement calculators vary in the extent to which they take taxes, social security, pensions, and other sources of retirement income and expenditures into account.
The assumptions keyed into a retirement calculator are critical. One of the most important assumptions, is the assumed rate of real (after inflation) investment return. A conservative return estimate could be based on the real yield of Inflation-indexed bonds offered by some governments, including the United States, Canada, and the United Kingdom. The TIP$TER retirement calculator projects the retirement expenditures that a portfolio of inflation-linked bonds, coupled with other income sources like Social Security, would be able to sustain. Current real yields on United States Treasury Inflation Protected Securities (TIPS) are available at the US Treasury site. Current real yields on Canadian 'Real Return Bonds' are available at the Bank of Canada's site. As of December, 2011, US Treasury inflation-linked bonds (TIPS) were yielding about 0.8% real per annum for the 30 year maturity and a noteworthy slightly negative real return for the 7 year maturity.
Many individuals use 'retirement calculators' on the Internet to determine the proportion of their pay which they should be saving in a tax advantaged-plan (e.g. IRA or 401-K in the US, RRSP in Canada, personal pension in the UK, superannuation in Australia).
After expenses and any taxes, a reasonable (though arguably pessimistic) long-term assumption for a safe real rate of return is zero. So in real terms, interest does not help the savings grow. Each year of work must pay its share of a year of retirement. For someone planning to work for 40 years and to be retired for 20 years, each year of work pays for itself and for half a year of retirement. Hence 33.33% of pay must be saved and 66.67% can be spent when earned. After 40 years of saving 33.33% of pay we have accumulated assets of 13.33 years of pay, as in the graph. In the graph to the right, the lines are straight, which is appropriate given the assumption of a zero real investment return.
The graph above can be compared with those generated by many retirement calculators. However, most retirement calculators use nominal (not 'real' dollars), and therefore require a projection of both the expected inflation rate and the expected nominal rate of return. One way to work around this limitation is to, for example, enter '0% return, 0% inflation' inputs into the calculator. The Bloomberg retirement calculator gives the flexibility to specify, for example, zero inflation and zero investment return and to reproduce the graph above. The MSN retirement calculator in 2011 has as the defaults a realistic 3% per annum inflation rate and optimistic 8% return assumptions; consistency with the December 2011 US nominal bond and inflation-protected bond market rates requires a change to about 3% inflation and 4% investment return before and after retirement.
Ignoring tax, someone wishing to work for a year and to then relax for a year on the same living standard needs to save 50% of pay. Similarly, someone wishing to work from age 25 to 55 and to be retired for 30 years till 85 needs to save 50% of pay if government and employment pensions are not a factor, and if it is considered appropriate to assume a zero real investment return. The problem that the lifespan is not known in advance can be reduced in some countries by the purchase at retirement of an inflation-indexed life annuity.
Retirement calculations.
For most people, employer pensions, government pensions and the tax situation in their country are important factors, typically taken account of in calculations by actuaries. Ignoring those significant nation-specific factors but not necessarily assuming zero real interest rates, a 'not to be relied upon' calculation of required personal savings rate zprop can be made using a little mathematics. It helps to have a dimly-remembered acquaintance with geometric series, maybe in the form
You work for w years, saving a proportion zprop of pay at the end of each year. So the after-savings purchasing power is (1-zprop) of pay while you are working. You need a pension for p years. Let's say that at retirement you are earning S per year and require to replace a ratio Rrepl of your pre-retirement living standard. So you need a pension of (1 – zprop ) Rrepl S, indexed to price inflation.
Let's assume that the investments, after price inflation fprice, earn a real rate ireal in real terms where
Let's assume that the investments, after wage inflation fpay, earn a real rate i rel to pay where
Size of lump sum required.
To pay for your pension, assumed for simplicity to be received at the end of each year, and taking discounted values in the manner of a net present value calculation, you need a lump sum available at retirement of:
Above we have used the standard mathematical formula for the sum of a geometric series. (Or if ireal =0 then the series in braces sums to p since it then has p equal terms). As an example, assume that S=60,000 per year and that it is desired to replace Rrepl=0.80, or 80%, of pre-retirement living standard for p=30 years. Assume for current purposes that a proportion z prop=0.25 (25%) of pay was being saved. Using ireal=0.02, or 2% per year real return on investments, the necessary lump sum is given by the formula as (1-0.25)*0.80*60,000*annuity-series-sum(30)=36,000*22.396=806,272 in the nation's currency in 2008–2010 terms. To allow for inflation in a straightforward way, it is best to talk of the 806,272 as being '13.43 years of retirement age salary'. It may be appropriate to regard this as being the necessary lump sum to fund 36,000 of annual supplements to any employer or government pensions that are available. It is common to not include any house value in the calculation of this necessary lump sum, so for a homeowner the lump sum pays primarily for non-housing living costs.
Size of lump sum saved.
Will you have saved enough at retirement? Use our necessary but unrealistic assumption of a constant after-pay-rises rate of interest. At retirement you have accumulated
 = zprop S ((1+i rel to pay)w- 1)/i rel to pay
Equate and derive necessary saving proportion.
To make the accumulation match with the lump sum needed to pay your pension:
zprop S (((1+i rel to pay )) w - 1)/i rel to pay = (1-zprop ) R repl S (1 – ((1+i real)) -p )/i real 
Bring zprop to the left hand side to give our answer, under this rough and unguaranteed method, for the proportion of pay that we should be saving:
zprop = R repl (1 – ((1+i real )) -p )/i real / [(((1+i rel to pay )) w - 1)/i rel to pay + R repl (1 – ((1+i real )) -p )/i real ] (Ret-03)
You are encouraged to download the use-at-your-own-financial-risk spreadsheet. The results in the spreadsheet can be seen to make sense. For example, working for 5 years and drawing a pension for 5 years requires you to save almost half your pay, with interest helping only a little.
Note that the special case i rel to pay =0 = i real means that we instead sum the geometric series by noting that we have p or w identical terms and hence z prop = p/(w+p). This corresponds to our graph above with the straight line real-terms accumulation.
Sample results.
The result for the necessary zprop given by (Ret-03) depends critically on the assumptions that you make. As an example, you might assume that price inflation will be 3.5% per year forever and that your pay will increase only at that same rate of 3.5%. If you assume a 4.5% per year nominal rate of interest, then (using 1.045/1.035 in real terms ) your pre-retirement and post-retirement net interest rates will remain the same, irel to pay = 0.966 percent per year and ireal = 0.966 percent per year. These assumptions may be reasonable in view of the market returns available on inflation-indexed bonds, after expenses and any tax. Equation (Ret-03) is readily coded in Excel and with these assumptions gives the required savings rates in the accompanying picture.
Monte Carlo: better allowance for randomness.
Finally, a newer method for determining the adequacy of a retirement plan is Monte Carlo simulation. This method has been gaining popularity and is now employed by many financial planners. Monte Carlo retirement calculators allow users to enter savings, income and expense information and run simulations of retirement scenarios. The simulation results show the probability that the retirement plan will be successful.
Early retirement.
Early retirement can be at any age, but is generally before the age (or tenure) needed for eligibility for support and funds from government or employer-provided sources. Thus, early-retirees rely on their own savings and investments to be initially self-supporting, until they start receiving such external support. Early retirement is also a euphemistic term for accepting termination of employment before retirement age as part of the employer's labor force rationalization. In this case, a monetary inducement may be involved.
Savings needed for early retirement.
While conventional wisdom has it that one can retire and take 7% or more out of a portfolio year after year, this would not have worked very often in the past. When making periodic inflation-adjusted withdrawals from retirement savings, can make meaningless many assumptions that are based on long term average investment returns.
Those contemplating early retirement will want to know if they have enough to survive possible bear markets such as the one that would cause the hypothetical 1973 retiree's fund to be exhausted after only 20 years.
The history of the US stock market shows that one would need to live on about 4% of the initial portfolio per year to ensure that the portfolio is not depleted before the end of the retirement; this rule of thumb is a summary of one conclusion of the Trinity study, though the report is more nuanced and the conclusions and very approach have been heavily criticized (see Trinity study for details). This allows for increasing the withdrawals with inflation to maintain a consistent spending ability throughout the retirement, and to continue making withdrawals even in dramatic and prolonged bear markets. (The 4% figure does not assume any pension or change in spending levels throughout the retirement.)
When retiring prior to age , there is a 10% IRS penalty on withdrawals from a retirement plan such as a 401(k) plan or a Traditional IRA. Exceptions apply under certain circumstances. At age 59 and six months, the penalty-free status is achieved and the 10% IRS penalty no longer applies.
To avoid the 10% penalty prior to age , a person should consult a lawyer about the use of IRS rule 72 T. This rule must be applied for with the IRS. It allows the distribution of an IRA account prior to age in equal amounts of a period of either 5 years or until the age of , whichever is the longest time period, without a 10% penalty. Taxes still must be paid on the distributions.
Calculations using actual numbers.
Although the 4% initial portfolio withdrawal rate described above can be used as a rough gauge, it is often desirable to use a retirement planning tool that accepts detailed input and can render a result that has more precision. Some of these tools model only the retirement phase of the plan while others can model both the savings or accumulation phase as well as the retirement phase of the plan. For example, an analysis by "Forbes" reckoned that in 90% of historical markets, a 4% rate would have lasted for at least 30 years, while in 50% of the historical markets, a 4% rate would have been sustained for more than 40 years.
The effects of making inflation-adjusted withdrawals from a given starting portfolio can be modeled with a downloadable spreadsheet that uses historical stock market data to estimate likely portfolio returns. Another approach is to employ a retirement calculator that also uses historical stock market modeling, but adds provisions for incorporating pensions, other retirement income, and changes in spending that may occur during the course of the retirement.
Life after retirement.
Retirement might coincide with important life changes; a retired worker might move to a new location, for example a retirement community, thereby having less frequent contact with their previous social context and adopting a new lifestyle. Often retirees volunteer for charities and other community organizations. Tourism is a common marker of retirement and for some becomes a way of life, such as for so-called grey nomads. Some retired people even choose to go and live in warmer climates in what is known as retirement migration.
It has been found that Americans have six lifestyle choices as they age: continuing to work full-time, continuing to work part-time, retiring from work and becoming engaged in a variety of leisure activities, retiring from work and becoming involved in a variety of recreational and leisure activities, retiring from work and later returning to work part-time, and retiring from work and later returning to work full-time. An important note to make from these lifestyle definitions are that four of the six involve working. America is facing an important demographic change in that the Baby Boomer generation is now reaching retirement age. This poses two challenges: whether there will be a sufficient number of skilled workers in the work force, and whether the current pension programs will be sufficient to support the growing number of retired people. The reasons that some people choose to never retire, or to return to work after retiring include not only the difficulty of planning for retirement but also wages and fringe benefits, expenditure of physical and mental energy, production of goods and services, social interaction, and social status may interact to influence an individual’s work force participation decision.
Often retirees are called upon to care for grandchildren and occasionally aged parents. For many it gives them more time to devote to a hobby or sport such as golf or sailing. On the other hand, many retirees feel restless and suffer from depression as a result of their new situation. Although it is not scientifically possible to directly show that retirement either causes or contributes to depression, the newly retired are one of the most vulnerable societal groups when it comes to depression most likely due to confluence of increasing age and deteriorating health status. Retirement coincides with deterioration of one's health that correlates with increasing age and this likely plays a major role in increased rates of depression in retirees. Longitudinal and cross-sectional studies have shown that healthy elderly and retired people are as happy or happier and have an equal quality of life as they age as compared to younger employed adults, therefore retirement in and of itself is not likely to contribute to development of depression.
Many people in the later years of their lives, due to failing health, require assistance, sometimes in extremely expensive treatments – in some countries – being provided in a nursing home. Those who need care, but are not in need of constant assistance, may choose to live in a retirement home.

</doc>
<doc id="26501" url="https://en.wikipedia.org/wiki?curid=26501" title="Boeing RC-135">
Boeing RC-135

The Boeing RC-135 is a family of large reconnaissance aircraft built by Boeing and modified by a number of companies, including General Dynamics, Lockheed, LTV, E-Systems, and L-3 Communications, and used by the United States Air Force and Royal Air Force to support theater and national level intelligence consumers with near real-time on-scene collection, analysis and dissemination capabilities. Based on the C-135 Stratolifter airframe, various types of RC-135s have been in service since 1961. Unlike the C-135 and KC-135 which are recognized by Boeing as the Model 717, the RC-135 is internally designated as the Model 739 by the company. Many variants have been modified numerous times, resulting in a large variety of designations, configurations, and program names.
Design and development.
The first RC-135 variant, the RC-135A, was ordered in 1962 by the United States Air Force to replace the Boeing RB-50 Superfortress. Originally nine were ordered but this was later reduced to four. Boeing allocated the variant the designation "Boeing 739-700" but they were modified variant of the KC-135A then in production. They used the same Pratt & Whitney J57 turbojet engines as the tanker variant, but carried cameras in a bay just aft of the nose wheel bay where the forward fuel tank was normally located. They had no refueling system fitted and they were to be used for photographic and surveying tasks.
The next variant ordered was the RC-135B to be used as an electronic intelligence aircraft to replace the Boeing RB-47H Stratojet on ELINT duties. Unlike the earlier variants, the RC-135Bs were fitted with Pratt & Whitney TF33 turbofans rather than the older J57s. These ten aircraft were delivered directly Martin Aircraft beginning in 1965 for installation of their operational electronics suite. By 1967, they emerged as RC-135Cs and were all delivered that year. The refueling boom was not fitted and the boom operator station was used as a camera bay for a KA-59 camera. Externally, the aircraft were fitted with sideways looking airborne radar (SLAR) antenna on the lower forward fuselage.
The RC-135Bs were the last of the new aircraft built. All further reconnaissance variants that followed were modified aircraft, either from earlier RC-135 variants or from tankers.
In 2005, the RC-135 fleet completed a series of significant airframe, navigation and powerplant upgrades which include re-engining from the TF33 to the CFM International CFM-56 (F108) engines used on the KC-135R and T Stratotanker and upgrade of the flight deck instrumentation and navigation systems to the AMP standard. The AMP standard includes conversion from analog readouts to a digital "glass cockpit" configuration.
Operational history.
The current RC-135 fleet is the latest iteration of modifications to this pool of aircraft dating back to the early 1960s. Initially employed by Strategic Air Command for reconnaissance, the RC-135 fleet has participated in every armed conflict involving U.S. forces during its tenure. RC-135s supported operations in Vietnam War, the Mediterranean for Operation El Dorado Canyon, Grenada for Operation Urgent Fury, Panama for Operation Just Cause, the Balkans for Operations Deliberate Force and Allied Force, and Southwest Asia for Operations Desert Shield, Desert Storm, Enduring Freedom and Iraqi Freedom. RC-135s have maintained a constant presence in Southwest Asia since the early 1990s. They were stalwarts of Cold War operations, with missions flown around the periphery of the USSR and its client states in Europe and around the world.
Originally, all RC-135s were operated by Strategic Air Command. Since 1992 they have been assigned to Air Combat Command. The RC-135 fleet is permanently based at Offutt Air Force Base, Nebraska and operated by the 55th Wing, using forward operating locations worldwide. The 55th Wing operates 22 platforms in three variants: three RC-135S Cobra Ball, two RC-135U Combat Sent, and 17 RC-135V/W Rivet Joint.
On August 9, 2010, the Rivet Joint recognized its 20th anniversary of continued service in Central Command, dating back to the beginning of Desert Storm. This represents the longest unbroken presence of any aircraft in the Air Force inventory. During this time it has flown over 8,000 combat missions supporting air and ground forces of Operations Desert Storm, Desert Shield, Northern Watch, Southern Watch, Iraqi Freedom and Enduring Freedom, which continues to this day.
On 22 March 2010 the British Ministry of Defence announced that it had reached agreement with the US Government to purchase three RC-135W Rivet Joint aircraft to replace the Nimrod R1, which was subsequently retired in June 2011. The aircraft, to be styled as 'Airseeker', are scheduled to be delivered by 2017 at a total cost of around £650 million, including provision of ground infrastructure, training of personnel and ground supporting systems. In 2013, the UK government confirmed that crews from the RAF's 51 Squadron had been training and operating alongside their USAF colleagues since 2011, having achieved in excess of 32,000 flying hours and 1,800 sorties as part of the US 55th Strategic Reconnaissance Wing at Offutt AFB. The RAF received the first RC-135W in September 2013, which was deployed from July 2014 to support coalition action against combat Islamic State of Iraq and the Levant militants in Iraq. The second aircraft was delivered seven months ahead of schedule in September 2015, with over sixty improvements incorporated ranging from upgrades to the aircraft’s mission systems to engine improvements providing increased fuel efficiency and durability. In due course, the first Airseeker will receive the same upgrades. The aircraft will be air-to-air refuelled in service by USAF tankers based in Europe, as the UK does not operate boom-equipped refueling aircraft and has no plans to adapt drogue-equipped aircraft.
Variants.
KC-135A Reconnaissance Platforms.
At least four KC-135A tankers were converted into makeshift reconnaissance platforms with no change of Mission Design Series (MDS) designation. KC-135As 55-3121, 55-3127, 59-1465, and 59-1514 were modified beginning in 1961. That year the Soviet Union announced its intention to detonate a 100 megaton thermonuclear device on Novaya Zemlya, the so-called Tsar Bomba. A testbed KC-135A (55-3127) was modified under the Big Safari program to the SPEED LIGHT BRAVO configuration in order to obtain intelligence information on the test. The success of the mission prompted conversion of additional aircraft for intelligence gathering duties.
KC-135R Rivet Stand / Rivet Quick.
Not to be confused with the CFM F108-powered KC-135R tanker, the KC-135R MDS was applied in July 1967, to the three KC-135A reconnaissance aircraft under the Rivet Stand program name. The three aircraft were 55-3121, 59-1465, 59-1514, with KC-135A 58-0126 converted in 1969 to replace 59-1465 which had crashed at Offutt AFB, Nebraska in 1967. Externally the aircraft had varied configurations throughout their lives, but generally they were distinguished by five "towel bar" antennas along the spine of the upper fuselage and a radome below the forward fuselage. The first three aircraft retained the standard tanker nose radome, while 58-0126 was fitted with the 'hog nose' radome commonly associated with an RC-135. A trapeze-like structure in place of the refueling boom which was used to trail an aerodynamic shape housing a specialized receiver array (colloquially known as a "blivet") on a wire was installed. This was reported to be used for "Briar Patch" and "Combat Lion" missions. There were four small optically flat windows on each side of the forward fuselage. On some missions a small wing-like structure housing sensors was fitted to each side of the forward fuselage, with a diagonal brace below it. With the loss of 59-1465, KC-135A 58-0126 was modified to this standard under the Rivet Quick operational name. All four aircraft have now been lost or converted to KC-135R tanker configuration. They are among the few KC-135 tankers equipped with an aerial refueling receptacle above the cockpit, left over from their service as intelligence gathering platforms.
KC-135T Cobra Jaw.
KC-135R 55-3121 was modified in 1969 by Lockheed Air Services to the unique KC-135T configuration under the Cobra Jaw program name. Externally distinguished by the 'hog nose' radome, the aircraft also featured spinning "fang" receiver antennas below the nose radome, a large blade antenna above the forward fuselage, a single 'towel bar' antenna on the spine, teardrop antennas forward of the horizontal stabilizers on each side, and the trapeze-like structure in place of the refueling boom. The aircraft briefly carried nose art consisting of the Ford Cobra Jet cartoon cobra. It was later modified into an RC-135T Rivet Dandy.
RC-135A.
Four RC-135A (63-8058 through 8061) were photo mapping platforms utilized briefly by the Air Photographic & Charting Service, based at Turner Air Force Base, Georgia and later at Forbes Air Force Base, Kansas as part of the 1370th Photographic Mapping Wing. The mission was soon taken over by satellites, and the RC-135As were de-modified and used as staff transports. In the early 1980s they were further converted to tankers with the designation KC-135D (of the same basic configuration as the KC-135E, plus some leftover special mission equipment). Due to delays in reinstalling their original equipment, the RC-135As were the last of the entire C-135 series delivered to the USAF. The Boeing model number for the RC-135A is 739-700.
RC-135B.
The as-delivered version of the RC-135. The RC-135B was never used operationally, as it had no mission equipment installed by Boeing. The entire RC-135B production run of ten aircraft was delivered directly to Martin Aircraft in Baltimore, Maryland for modification and installation of mission equipment under the Big Safari program. Upon completion, the RC-135Bs were re-designated RC-135C. The Boeing model number for the RC-135B is 739-445B.
RC-135C Big Team.
Modified and re-designated RC-135B aircraft used for strategic reconnaissance duties, equipped with the AN/ASD-1 electronic intelligence (ELINT) system. This system was characterized by the large 'cheek' pods on the forward fuselage containing the Automated ELINT Emitter Locating System (AEELS – not Side Looking Airborne Radar – SLAR, as often quoted), as well as numerous other antennae and a camera position in the refuelling pod area of the aft fuselage. The aircraft was crewed by two pilots, two navigators, numerous intelligence gathering specialists, inflight maintenance technicians and airborne linguists. When the RC-135C was fully deployed, SAC was able to retire its fleet of RB-47H Stratojets from active reconnaissance duties. All ten continue in active service as either RC-135V Rivet Joint or RC-135U Combat Sent platforms.
RC-135D Office Boy / Rivet Brass.
The RC-135Ds, originally designated KC-135A-II, were the first reconnaissance configured C-135's given the 'R' MDS designation, although they were not the first reconnaissance-tasked members of the C-135 family. They were delivered to Eielson Air Force Base, Alaska in 1962 as part of the Office Boy Project. Serial numbers were 60-0356, 60-0357, and 60-0362. The aircraft began operational missions in 1963. These three aircraft were ordered as KC-135A tankers, but delivered without refueling booms, and known as "falsie C-135As" pending the delivery of the first actual C-135A cargo aircraft in 1961. The primary Rivet Brass mission flew along the northern border of the Soviet Union, often as a shuttle mission between Eielson and RAF Upper Heyford, Oxfordshire, and later RAF Mildenhall, Suffolk, UK. The RC-135D was also used in Southeast Asia during periods when the RC-135M (see below) was unavailable. In the late 1970s, with the expansion of the RC-135 fleet powered by TF33 turbofan engines, the RC-135Ds were converted into tankers, and remain in service as receiver-capable KC-135Rs.
RC-135E Lisa Ann / Rivet Amber.
Originally designated C-135B-II, project name Lisa Ann, the RC-135E Rivet Amber was a one-of-a-kind aircraft equipped with a large 7 MW Hughes Aircraft phased-array radar system. Originally delivered as a C-135B, 62-4137 operated from Shemya Air Force Station, Alaska from 1966-1969. Its operations were performed in concert with the RC-135S Rivet Ball aircraft (see below). The radar system alone weighed over 35,000 pounds and cost over US$35 million (1960 dollars), making Rivet Amber both the heaviest C-135-derivative aircraft flying and the most expensive Air Force aircraft for its time. The radiation generated by the radar was sufficient to be a health hazard to the crew, and both ends of the radar compartment were shielded by thick lead bulkheads. This prevented the forward and aft crew areas from having direct contact after boarding the aircraft. The system could track an object the size of a soccer ball from a distance of , and its mission was to monitor Soviet ballistic missile testing in the reentry phase. The power requirement for the phased array radar was enormous, necessitating an additional power supply. This took the form of a podded Lycoming T55-L5 turboshaft engine in a pod under the left inboard wing section, driving a 350kVA generator dedicated to powering mission equipment. On the opposite wing in the same location was a podded heat exchanger to permit cooling of the massive electronic components on board the aircraft. This configuration has led to the mistaken impression that the aircraft had six engines. On June 5, 1969, Rivet Amber was lost at sea on a ferry flight from Shemya to Eielson AFB for maintenance, and no trace of the aircraft or its crew was ever found.
RC-135M Rivet Card.
The RC-135M was an interim type with more limited ELINT capability than the RC-135C but with extensive additional COMINT capability. They were converted from Military Airlift Command C-135B transports, and operated by the 82d Reconnaissance Squadron during the Vietnam War from Kadena AB, gathering signals intelligence over the Gulf of Tonkin and Laos with the program name Combat Apple (originally Burning Candy). There were six RC-135M aircraft, 62-4131, 62-4132, 62-4134, 62-4135, 62-4138 and 62-4139, all of which were later modified to and continue in active service as RC-135W Rivet Joints by the early 1980s.
RC-135S Nancy Rae / Wanda Belle / Rivet Ball.
Rivet Ball was the predecessor program to Cobra Ball and was initiated with a single RC-135S (serial 59-1491, formerly a JKC-135A) on December 31, 1961. The aircraft first operated under the Nancy Rae project name as an asset of Air Force Systems Command and later as an RC-135S reconnaissance platform with Strategic Air Command under the project name Wanda Belle. The name Rivet Ball was assigned in January 1967. The aircraft operated from Shemya AFB, Alaska. Along with most other RC-135 variants, the RC-135S had an elongated nose radome housing an S band receiving antenna. The aircraft was characterized by ten large optically flat quartz windows on the right side of the fuselage used for tracking cameras. Unlike any other RC-135S, Rivet Ball also had a pleixiglass dome mounted top center on its fuselage for the Manual Tracker position. It holds the distinction of obtaining the very first photographic documentation of Soviet Multiple Reentry vehicle (MRV) testing on October 4, 1968. On January 13, 1969 Rivet Ball was destroyed in a landing accident at Shemya when it hydroplaned off the end of runway 28 with no fatalities.
RC-135S Cobra Ball.
The RC-135S Cobra Ball is a measurement and signature intelligence MASINT collector equipped with special electro-optical instruments designed to observe ballistic missile flights at long range. The Cobra Ball monitors missile-associated signals and tracks missiles during boost and re-entry phases to provide reconnaissance for treaty verification and theater ballistic missile proliferation. The aircraft are extensively modified C-135Bs. The right wing and engines are traditionally painted black on Cobra Ball aircraft.
There are three aircraft in service and they are part of the 55th Wing, 45th Reconnaissance Squadron based at Offutt Air Force Base, Nebraska. Cobra Ball aircraft were originally assigned to Shemya and used to observe ballistic missile tests on the Kamchatka peninsula in conjunction with Cobra Dane and Cobra Judy. Two aircraft were converted for Cobra Ball in 1969 and following the loss of an aircraft in 1981 another aircraft was converted in 1983. The sole RC-135X was also converted into an RC-135S in the late 1990s to supplement the other aircraft.
Following the loss of one RC-135T aircraft, an EC-135B was modified in 1985 as a TC-135S (62-4133) for use as a training aircraft for the RC-135S crews to enable them to train with the different aerodynamic effects from standard aircraft. It remains in service, and does not carry any mission equipment.
RC-135T Rivet Dandy.
KC-135T 55-3121 was modified to RC-135T Rivet Dandy configuration in 1971. It was used to supplement the RC-135C/D/M fleet, then in short supply due to ongoing upgrades requiring airframes to be out of service. It operated under the Burning Candy operational order. In 1973 the aircraft's SIGINT gear was removed and transferred to KC-135E 58-0126, resulting in 55-3121 assuming the role of trainer, a role which it fulfilled for the remainder of its life. Externally the aircraft retained the 'hog nose' radome and some other external modifications, but the aerial refueling boom and trapeze below the tail were removed, and it had no operational reconnaissance role. In this configuration it operated variously with the 376th Strategic Wing at Kadena AB, Okinawa, the 305th AREFW at Grissom AFB, Indiana, and the 6th Strategic Wing at Eielson AFB, Alaska. In 1982 the aircraft was modified with Pratt & Whitney TF33-PW102 engines and other modifications common to the KC-135E tanker program, and returned to Eielson AFB. It crashed while on approach to Valdez Airport, Alaska on 25 February 1985 with the loss of three crew members. The wreckage was not found until August 1985, six months after the accident.
RC-135U Combat Sent.
The RC-135U Combat Sent is designed to collect technical intelligence on adversary radar emitter systems. Combat Sent data is collected to develop new or upgraded radar warning receivers, radar jammers, decoys, anti-radiation missiles, and training simulators.
Distinctly identified by the antennae arrays on the fuselage chin, tailcone, and wing tips, three RC-135C aircraft were converted to RC-135U (63-9792, 64-14847, & 64-14849) in the early 1970s. 63-9792 was later converted into a Rivet Joint in 1978, and all aircraft remain in service based at Offutt Air Force Base, Nebraska. Minimum crew requirements are 2 pilots, 2 navigators, 3 systems engineers, 10 electronic warfare officers, and 6 area specialists.
RC-135V/W Rivet Joint.
The RC-135V/W is the USAF's standard airborne SIGINT platform. Its sensor suite allows the mission crew to detect, identify and geolocate signals throughout the electromagnetic spectrum. The mission crew can then forward gathered information in a variety of formats to a wide range of consumers via Rivet Joint's extensive communications suite. The crew consists of the cockpit crew, electronic warfare officers, intelligence operators, and airborne systems maintenance personnel. All Rivet Joint airframe and mission systems modifications are performed by L-3 Communications in Greenville, Texas, under the oversight of the Air Force Materiel Command.
All RC-135s are assigned to Air Combat Command. The RC-135 is permanently based at Offutt Air Force Base, Nebraska., and operated by the 55th Wing, using various forward deployment locations worldwide.
Under the "BIG SAFARI" program name, RC-135Vs were upgraded from the RC-135C "Big Team" configuration. RC-135Ws were originally delivered as C-135B transports, and most were modified from RC-135Ms. This is the only difference between the V and W variants; both carry the same mission equipment. For many years, the RC-135V/W could be identified by the four large disc-capped MUCELS antennas forward, four somewhat smaller blade antennae aft and myriad of smaller underside antennas. Baseline 8 Rivet Joints (in the 2000s) introduced the first major change to the external RC-135V/W configuration replacing the MUCELS antennas with plain blade antennas. The configuration of smaller underside antennas was also changed significantly.
In addition to the operational fleet, two TC-135Ws (62-4127 and 4129) are in service for crew training, and do not carry operable mission equipment.
RC-135X Cobra Eye.
The sole RC-135X Cobra Eye was converted during the mid-to-late-1980s from a C-135B Telemetry/Range Instrumented Aircraft, serial number 62-4128, with the mission of tracking ICBM reentry vehicles. In 1993, it was converted into an additional RC-135S Cobra Ball.
RC-135W Rivet Joint (Project Airseeker).
The United Kingdom is buying three KC-135R aircraft for conversion to RC-135W Rivet Joint standard under the Airseeker project. Acquisition of the three aircraft is budgeted at £634m (~US$905m), with entry into service planned for October 2014. The aircraft will form No. 51 Squadron RAF, based at RAF Waddington along with the RAF's other ISTAR assets. They are expected to remain in British service until 2045.
Previously, the Royal Air Force had gathered signals intelligence with three Nimrod R1, converted in the 1970s from the Nimrod MR1 maritime patrol aircraft. When the time came to upgrade the maritime Nimrods to MRA4 standard, Project Helix was launched in August 2003 to study options for extending the life of the R1 out to 2025. The option of switching to Rivet Joint was added to Helix in 2008, and the retirement of the R1 became inevitable when the MRA4 was cancelled under the UK's 2010 budget cuts. The R1's involvement over Libya in Operation Ellamy delayed its retirement until June 2011.
Helix became Project Airseeker, under which three KC-135R airframes are being converted to RC-135W standard by L-3 Communications. L-3 will also provide ongoing maintenance and upgrades under a long-term agreement. The three airframes are former United States Air Force KC-135Rs, all of which first flew in 1964 but will be modified to the latest RC-135W standard before delivery. The three airframes on offer to the UK are the youngest KC-135s in the USAF fleet. As of September 2010 the aircraft had approximately 23,200 flying hours, 22,200 hours and 23,200 hours.
51 Sqn personnel began training at Offutt in January 2011 for conversion to the RC-135. The first RC-135W (ZZ664) was delivered ahead of schedule to the Royal Air Force on 12 November 2013, for final approval and testing by the Defence Support and Equipment team prior to its release to service from the UK MAA. The second one was once again delivered ahead of schedule on 4 September 2015 at RAF Mildenhall in Suffolk. The third is scheduled to be delivered and fully operational by December 2017.
Operators.
United States Air Force - "Air Combat Command"
Royal Air Force

</doc>
<doc id="26502" url="https://en.wikipedia.org/wiki?curid=26502" title="Rumiko Takahashi">
Rumiko Takahashi

Takahashi is one of Japan's most affluent manga artists. Her works are popular worldwide, where they have been translated into a variety of languages. Takahashi is also the best selling female comics artist in history; as of February 2010, over 170 million copies of her various works had been sold. She has twice won the Shogakukan Manga Award: once in 1980 for "Urusei Yatsura", and again in 2002 for "InuYasha".
Career and major works.
She was born in Niigata, Japan. Takahashi showed little interest in manga during her childhood; though she was said to occasionally doodle in the margins of her papers while attending Niigata Chūō High School, Takahashi's interest in manga did not start until later. In an interview in 2000, Takahashi said that she had always wanted to become a professional comic author since she was a child. During her university years, she enrolled in Gekiga Sonjuku, a manga school founded by Kazuo Koike, manga author of "Crying Freeman" and "Lone Wolf and Cub". Under his guidance Rumiko Takahashi began to publish her first dōjinshi creations in 1975, such as "Bye-Bye Road" and "Star of Futile Dust". Koike often urged his students to create well-thought out, interesting characters, and this influence would greatly impact Rumiko Takahashi's works throughout her career.
Takahashi's professional career began in 1978. Her first published work was the one-shot "Katte na Yatsura", for which she was awarded the Shogakkan New Comics Award. Later that same year, she began her first serialized story "Urusei Yatsura", a comedic science fiction story. She had difficulty meeting deadlines to begin with, so chapters were published sporadically until 1980. During the run of the series, she shared a small apartment with two assistants, and often slept in a closet due to a lack of space. During the same year, she published "Time Warp Trouble", "Shake Your Buddha", and the "Golden Gods of Poverty" in "Shōnen Sunday" magazine, which would remain the home to most of her major works for the next twenty years.
During 1980, Takahashi started her second major series, "Maison Ikkoku", in "Big Comic Spirits" magazine. Written for an older audience, "Maison Ikkoku" is a romantic comedy, and Takahashi used her own experience living in an apartment to create the series. Takahashi managed to work on "Maison Ikkoku" on and off simultaneously with "Urusei Yatsura". She concluded both series in 1987, with "Urusei Yatsura" ending at 34 volumes, and "Maison Ikkoku" being 15.
During the 1980s, Takahashi became a prolific writer of short story manga. Her stories "The Laughing Target", "Maris the Chojo", and "Fire Tripper" all were adapted into original video animations (OVAs). In 1984, during the writing of "Urusei Yatsura" and "Maison Ikkoku", Takahashi took a different approach to storytelling and began the dark, macabre "Mermaid Saga". This series of short segments was published sporadically until 1994, with the final story being "Mermaid's Mask".
Another short work of Takahashi's to be published sporadically was "One-Pound Gospel". Takahashi concluded the series in 2007 after publishing chapters in 1998, 2001 and 2006. One-Pound Gospel was adapted into a TV drama, which ran for 9 of its originally scheduled 11 episodes.
Later in 1987, Takahashi began her third major series, "Ranma ½". Following the late 80s and early 90s trend of shōnen martial arts manga, "Ranma ½" features a gender-bending twist. The series continued for nearly a decade until 1996, when it ended at 38 volumes. "Ranma ½" is popular amongst manga fans outside Japan.
During the later half of the 1990s, Rumiko Takahashi continued with short stories and her installments of "Mermaid Saga" and "One-Pound Gospel" until beginning her fourth major work, "InuYasha". While "Ranma ½", "Urusei Yatsura", and "Maison Ikkoku" all were heavily seated in the romantic comedy genre, "InuYasha" was more akin to her dark "Mermaid Saga". The series featured action, romance, horror, fantasy, (folklore-based) historical fiction, and comedy. This series was serialized in "Shōnen Sunday" magazine and is her longest work by far, and ended in 2008. On March 5, 2009, Rumiko Takahashi released her one-shot short story "Unmei No Tori". On March 16, 2009, Rumiko Takahashi collaborated with Mitsuru Adachi, creator of "Touch" and "Cross Game", to release a one-shot story called "My Sweet Sunday". Her latest manga series, "Kyōkai no Rinne" started on April 22, 2009. This is Rumiko Takahashi's first new manga series since the end of her previous manga series "InuYasha" in June 2008.
"Urusei Yatsura", "Maison Ikkoku", "Ranma 1/2" and "InuYasha" manga were all published in English in the United States by Viz Comics; however, Viz's 1989 release of "Urusei Yatsura" halted after only a few volumes were translated, and is long out of print.
In February 2014, she was nominated for entry into the Eisner Hall of Fame.
Comics Alliance listed Takahashi as one of twelve women cartoonists deserving of lifetime achievement recognition. 
Animation.
In 1981, "Urusei Yatsura" became the first of Takahashi's works to be animated. This series first aired on Japanese television on October 14, and went through multiple director changes during its run. Though the 195-episode TV series ended in March 1986, "Urusei Yatsura" was kept alive in anime form through OVA and movie releases through 1991. Most notable of the series directors was Mamoru Oshii, who made "Beautiful Dreamer", the second "Urusei Yatsura" movie. AnimEigo has released the entire TV series and all of the OVAs and movies except for "Beautiful Dreamer" (which was released by Central Park Media in the U.S.) in the United States in English-subtitled format, with English dubs also made for the first two TV episodes (as "Those Obnoxious Aliens") and for all of the movies.
Kitty Animation, the studio that produced "Urusei Yatsura" with animation assistance from Studio Pierrot and then Studio Deen, continued their cooperation and adapted Rumiko Takahashi's second work, "Maison Ikkoku" in 1986; it debuted the week after the final TV episode of "UY". The TV series ran for 96 episodes, 3 OVAs, a movie and also a live-action movie. Studio Deen also provided animation duties on "Maison Ikkoku" and "Ranma".
"Maris the Chojo", "Fire Tripper", and "Laughing Target" were all made into OVAs during the mid-80s. Her stories "Mermaid's Forest" and "Mermaid's Scar" were also made as OVAs in Japan on 1991. They were all released, subtitled in English, in the U.S.
In 1989, Kitty Animation produced its last major series, "Ranma ½". The series went through ups and downs in ratings until Kitty Animation finally went out of business. "Ranma ½" was never concluded in animated form despite being 161 episodes and two movies in length. The TV series ended in 1992 amid internal turmoil within Kitty; Kitty and Studio Deen continued to produce "Ranma" OVAs until 1996.
Sunrise was the first studio after Kitty Animation to adapt a major Rumiko Takahashi series. "InuYasha" debuted in 2000 and ended in 2004. The TV series went on for 167 episodes and spawned four major films. The first anime ended before the manga did, thus wrapping up inconclusively. However, a second InuYasha anime series called "InuYasha the Final Act" debuted in Japan in the fall of 2009 and ended in March 2010, finishing the series.
Viz Communications has released the anime of "Maison Ikkoku", "Ranma" and "InuYasha" in English, in both subtitled and dubbed formats.
The year 2008 marked the 50th anniversary of "Weekly Shōnen Sunday" and the 30th anniversary of the first publication of "Urusei Yatsura", and Rumiko Takahashi's manga work was honoured in "It's a Rumic World", a special exhibition held from July 30 to August 11 at the Matsuya Ginza department store in Tokyo. Several new pieces of animation accompanied the exhibit, including new half-hour "Ranma 1/2" and "InuYasha" ("Black Tetsusaiga") OVAs and an introductory sequence featuring characters from "Urusei Yatsura", "Ranma" and "InuYasha" (starring the characters' original anime voice talents), which has become a popular video on YouTube. The "It's a Rumic World" exhibit was scheduled to re-open in Sendai in December 2008, at which time a new half-hour "Urusei Yatsura" OVA was scheduled to premiere. A special DVD release containing all three new OVAs was announced as coming out on January 29, 2010, with a trailer posted in September 2009. However it is not known whether any of the new episodes will ever be released outside Japan.
The latest Rumiko Takahashi TV animation adapts many of her short stories from the 80s. "Rumiko Takahashi Anthology", animated by TMS Entertainment, features her stories "The Tragedy of P", "The Merchant of Romance", "Middle-Aged Teen", "Hidden in the Pottery", "Aberrant Family F", "As Long As You Are Here", "One Hundred Years of Love", "In Lieu of Thanks", "Living Room Lovesong", "House of Garbage", "One Day Dream", "Extra-Large Size Happiness", and "The Executive's Dog". Also, a TV series of "Mermaid Saga" was produced in 2003, animating 13 of her stories.
Popularity and impact on the western world.
Many of Takahashi's works have been translated into English, as well as other European languages. Takahashi said that she did not know why her works are relatively popular with English speakers. Takahashi said "Sure, there are cultural differences in my work. When I see an American comedy, even though the jokes are translated, there's always a moment when I feel puzzled and think, ‘Ah, Americans would probably laugh at this more.' I suppose the same thing must happen with my books. It's inevitable. And yet, that doesn't mean my books can't be enjoyed by English-speaking readers. I feel confident that there's enough substance to them that people from a variety of cultural backgrounds can have a lot of fun reading them."

</doc>
<doc id="26509" url="https://en.wikipedia.org/wiki?curid=26509" title="Riverside">
Riverside

Riverside may refer to:

</doc>
<doc id="26511" url="https://en.wikipedia.org/wiki?curid=26511" title="RPN">
RPN

The initials RPN may refer to:
RPN may also refer to:

</doc>
<doc id="26513" url="https://en.wikipedia.org/wiki?curid=26513" title="Reverse Polish notation">
Reverse Polish notation

Reverse Polish notation (RPN) is a mathematical notation in which every operator follows all of its operands, in contrast to Polish notation (PN), which puts the operator before its operands. It is also known as postfix notation and does not need any parentheses as long as each operator has a fixed number of operands. The description "Polish" refers to the nationality of logician Jan Łukasiewicz, who invented (prefix) Polish notation in the 1920s.
The reverse Polish scheme was proposed in 1954 by Burks, Warren, and Wright and was independently reinvented by F. L. Bauer and E. W. Dijkstra in the early 1960s to reduce computer memory access and utilize the stack to evaluate expressions. The algorithms and notation for this scheme were extended by Australian philosopher and computer scientist Charles Hamblin in the mid-1950s.
During the 1970s and 1980s, RPN was well-known to many calculator users, as Hewlett-Packard used it in their pioneering 9100A and HP-35 scientific calculators, the succeeding Voyager series - and also the "cult" HP-12C financial calculator. 
In computer science, postfix notation is often used in stack-based and concatenative programming languages. It is also common in dataflow and pipeline-based systems, including Unix pipelines.
Most of what follows is about binary operators. An example of a unary operator whose standard notation uses postfix is the factorial.
Explanation.
In reverse Polish notation the operators follow their operands; for instance, to add 3 and 4, one would write "3 4 +" rather than "3 + 4". If there are multiple operations, the operator is given immediately after its second operand; so the expression written "3 − 4 + 5" in conventional notation would be written "3 4 − 5 +" in RPN: 4 is first subtracted from 3, then 5 added to it. An advantage of RPN is that it removes the need for parentheses that are required by infix. While "3 − 4 × 5" can also be written "3 − (4 × 5)", that means something quite different from "(3 − 4) × 5". In postfix, the former could be written "3 4 5 × −", which unambiguously means "3 (4 5 ×) −" which reduces to "3 20 −"; the latter could be written "3 4 − 5 ×" (or 5 3 4 − ×, if keeping similar formatting), which unambiguously means "(3 4 −) 5 ×".
Despite the name, reverse Polish notation is not exactly the reverse of Polish notation, for the operands of non-commutative operations are still written in the conventional order (e.g. "÷ 6 3" in Polish notation and "6 3 ÷" in reverse Polish both evaluate to 2, whereas "3 6 ÷" in reverse Polish notation would evaluate to ½).
Practical implications.
In comparison testing of reverse Polish notation with algebraic notation, reverse Polish has been found to lead to faster calculations, for two reasons. Because reverse Polish calculators do not need expressions to be parenthesized, fewer operations need to be entered to perform typical calculations. Additionally, users of reverse Polish calculators made fewer mistakes than for other types of calculator. Later research clarified that the increased speed from reverse Polish notation may be attributed to the smaller number of keystrokes needed to enter this notation, rather than to a smaller cognitive load on its users. However, anecdotal evidence suggests that reverse Polish notation is more difficult for users to learn than algebraic notation.
Postfix algorithm.
The algorithm for evaluating any postfix expression is fairly straightforward:
Example.
The infix expression "5 + ((1 + 2) × 4) − 3" can be written down like this in RPN:
The expression is evaluated left-to-right, with the inputs interpreted as shown in the following table (the "Stack" is the list of values the algorithm is "keeping track of" after the "Operation" given in the middle column has taken place):
When a computation is finished, its result remains as the top (and only) value in the stack; in this case, 14.
The above example could be rewritten by following the "chain calculation" method described by HP for their series of RPN calculators:
As was demonstrated in the Algebraic mode, it is usually easier (fewer keystrokes) in working a problem like this to begin with the arithmetic operations inside the parentheses first.
Converting from infix notation.
Edsger Dijkstra invented the shunting-yard algorithm to convert infix expressions to postfix (RPN), so named because its operation resembles that of a railroad shunting yard.
There are other ways of producing postfix expressions from infix notation. Most operator-precedence parsers can be modified to produce postfix expressions; in particular, once an abstract syntax tree has been constructed, the corresponding postfix expression is given by a simple post-order traversal of that tree.
Implementations.
History of implementations.
The first computers to implement architectures enabling RPN were the English Electric Company's KDF9 machine, which was announced in 1960 and delivered (i.e. made available commercially) in 1963, and the American Burroughs B5000, announced in 1961 and also delivered in 1963. One of the designers of the B5000, Robert S. Barton, later wrote that he developed RPN independently of Hamblin sometime in 1958 after reading a 1954 textbook on symbolic logic by Irving Copi, where he found a reference to Polish notation, which made him read the works of Jan Łukasiewicz as well, and before he was aware of Hamblin's work. Designed by Robert "Bob" Appleby Ragen, Friden introduced RPN to the desktop calculator market with the EC-130 supporting a four-level stack in June 1963. The successor EC-132 added a square root function in April 1965. Around 1966, the Monroe Epic calculator supported an unnamed input scheme resembling RPN as well.
Hewlett-Packard.
Hewlett-Packard engineers designed the 9100A Desktop Calculator in 1968 with RPN with only three stack levels, a RPN variant later referred to as "three-level RPN". This calculator popularized RPN among the scientific and engineering communities. The HP-35, the world's first handheld scientific calculator, introduced the classical "four-level RPN" in 1972. HP used RPN on every handheld calculator it sold, whether scientific, financial, or programmable, until it introduced the HP-10 adding machine calculator in 1977. By this time HP was the leading manufacturer of calculators for professionals, including engineers and accountants.
Later LCD-based calculators in the early 1980s such as the HP-10C, HP-11C, HP-15C, HP-16C, and the financial calculator, the HP-12C also used RPN. In 1988 Hewlett-Packard introduced a business calculator, the HP-19B, without RPN, but its 1990 successor, the HP-19BII, gave users the option of using algebraic notation or RPN.
Around 1987, HP introduced RPL, an object-oriented successor to RPN. It deviates from classical RPN by utilizing a stack only limited by the amount of available memory (instead of three or four fixed levels) and which can hold all kinds of data objects (including symbols, strings, lists, matrices, graphics, programs, etc.) instead of just numbers. It also changed the behaviour of the stack to no longer duplicate the top register on drops (since in an unlimited stack there is no longer a top register) and the behaviour of the key so that it no longer duplicates values into Y under certain conditions, both part of the specific ruleset of the "automatic operational stack" in "classical RPN" in order to ease some calculations and to save keystrokes, but which had shown to also sometimes cause confusion among users not familiar with these properties. From 1990 to 2003 HP manufactured the HP-48 series of graphing RPL calculators and in 2006 introduced the HP 50g.
As of 2011, Hewlett-Packard was offering the calculator models 12C, 12C Platinum, 17bII+, 20b, 30b, 33s, 35s, 48gII (RPL) and 50g (RPL) which support RPN. While calculators emulating classical models continue to support "classical RPN", new RPN models feature a variant of RPN, where the Enter key behaves as in RPL. This latter variant is sometimes known as "entry RPN". In 2013, the HP Prime introduced a "128-level" form of entry RPN called "Advanced RPN". By early 2016, only the 12C, 12C Platinum, 17bii+, 35s and Prime remain active HP models supporting RPN.
WP 31S and WP 34S.
The community-developed calculators WP 31S and WP 34S, which are based on the HP 20b/HP 30b hardware platform, support Hewlett-Packard-style classical RPN with either a four- or an eight-level stack. An eight-level stack was already suggested by John A. Ball in 1978.
Sinclair Radionics.
In Britain, Clive Sinclair's Sinclair Scientific and Scientific Programmable models used RPN.
Prinztronic.
Prinz and Prinztronic were own-brand trade names of the British Dixons photographic and electronic goods stores retail chain, which was later rebranded as Currys Digital stores, and became part of DSG International. A variety of calculator models was sold in the 1970s under the Prinztronic brand, all made for them by other companies.
Among these was the PROGRAM Programmable Scientific Calculator which featured RPN.
Heathkit.
The Heathkit OC-1401 aka Aircraft Navigation Computer OC-1401 used "5-level RPN" in 1978.
Soviet Union.
Soviet programmable calculators (MK-52, MK-61, B3-34 and earlier B3-21 models) used RPN for both automatic mode and programming. Modern Russian calculators MK-161 and MK-152, designed and manufactured in Novosibirsk since 2007 and offered by Semico, are backward compatible with them. Their extended architecture is also based on reverse Polish notation.
Current implementations.
Existing implementations using reverse Polish notation include:

</doc>
<doc id="26514" url="https://en.wikipedia.org/wiki?curid=26514" title="Roald Hoffmann">
Roald Hoffmann

Roald Hoffmann (born Roald Safran; July 18, 1937) is an American theoretical chemist who won the 1981 Nobel Prize in Chemistry. He has also published plays and poetry. He is the Frank H. T. Rhodes Professor of Humane Letters, Emeritus, at Cornell University, in Ithaca, New York.
Early life.
Escape from the Holocaust.
Hoffmann was born in Złoczów, Poland (now Ukraine), to a Jewish family, and was named in honor of the Norwegian explorer Roald Amundsen. His parents were Clara (Rosen), a teacher, and Hillel Safran, a civil engineer. After Germany invaded Poland and occupied the town, his family was placed in a labor camp where his father, who was familiar with much of the local infrastructure, was a valued prisoner. As the situation grew more dangerous, with prisoners being transferred to liquidation camps, the family bribed guards to allow an escape and arranged with a Ukrainian neighbor named Mykola Dyuk for Hoffmann, his mother, two uncles and an aunt to hide in the attic and a storeroom of the local schoolhouse, where they remained for eighteen months, from January 1943 to June 1944, while Hoffmann was aged 5 to 7.
His father remained at the labor camp, but was able to occasionally visit, until he was tortured and killed by the Germans for his involvement in a plot to arm the camp prisoners. When she received the news, his mother attempted to contain her sorrow by writing down her feelings in a notebook her husband had been using to take notes on a relativity textbook he had been reading. While in hiding his mother kept Hoffmann entertained by teaching him to read and having him memorize geography from textbooks stored in the attic, then quizzing him on it. He referred to the experience as having been enveloped in a cocoon of love.
Most of the rest of the family perished in the Holocaust, though one grandmother and a few others survived. They migrated to the United States on the troop carrier "Ernie Pyle" in 1949.
Hoffmann married Eva Börjesson in 1960. They have two children, Hillel Jan and Ingrid Helena. Hoffmann visited Zolochiv with his adult son (by then a parent of a five-year-old) in 2006 and found that the attic where he had hidden was still intact, but the storeroom had been incorporated, ironically enough, into a chemistry classroom. In 2009, a monument to Holocaust victims was built in Zolochiv on Hoffmann's initiative.
Education and academic credentials.
Hoffmann graduated in 1955 from New York City's Stuyvesant High School, where he won a Westinghouse science scholarship. He received his bachelor of arts degree at Columbia University (Columbia College) in 1958. He earned his master of arts degree in 1960 from Harvard University. He earned his doctor of philosophy degree from Harvard University while working under joint supervision of Martin Gouterman and subsequent 1976 Nobel Prize in Chemistry winner William N. Lipscomb, Jr. Hoffman worked on the molecular orbital theory of polyhedral molecules. Under Lipscomb's direction the Extended Hückel method was developed by Lawrence Lohr and by Roald Hoffmann. This method was later extended by Hoffmann. He went to Cornell in 1965 and has remained there, becoming professor emeritus.
Scientific research.
Hoffmann is interested in the electronic structure of stable and unstable molecules, and in the study of transition states in reactions. He has investigated the structure and reactivity of both organic and inorganic molecules, and examined problems in organo-metallic and solid-state chemistry. Hoffman has developed semiempirical and nonempirical computational tools and methods such as the extended Hückel method for determining molecular orbitals, which he proposed in 1963.
With Robert Burns Woodward he developed rules for elucidating reaction mechanisms (the Woodward–Hoffmann rules). They realized that chemical transformations could be approximately predicted from subtle symmetries and asymmetries in the electron "orbitals" of complex molecules. Their rules predict differing outcomes, such as the types of products that will be formed when two compounds are activated by heat compared with those produced under activation by light. Hoffmann also introduced the isolobal analogy for predicting the bonding properties of organometallic compounds. For this work Hoffmann received the 1981 Nobel Prize in chemistry, sharing it with Japanese chemist Kenichi Fukui, who had independently resolved similar issues. (Woodward was not included in the prize, which is given only to living persons.)
Some of Hoffman's most recent work, with Neil Ashcroft and Vanessa Labet, examines bonding in matter under extreme high pressure.
Artistic interests.
"The World Of Chemistry" with Roald Hoffmann.
Hoffmann is the co-host of the Annenberg/CPB educational series, "The World of Chemistry", with Don Showalter.
"Entertaining Science".
Since the spring of 2001, Hoffmann has been the host of the monthly series "Entertaining Science" at New York City's Cornelia Street Cafe, which explores the juncture between the arts and science.
Non-fiction.
He has published books on the connections between art and science: "Roald Hoffmann on the Philosophy, Art, and Science of Chemistry" and "Beyond the Finite: The Sublime in Art and Science".
Poetry.
Hoffmann is also a writer of poetry. His collections include "The Metamict State" (1987, ISBN 0-8130-0869-7), "Gaps and Verges" (1990, ISBN 0-8130-0943-X), and "Chemistry Imagined", co-produced with artist Vivian Torrence.
Plays.
He co-authored with Carl Djerassi the play "Oxygen", about the discovery of oxygen and the experience of being a scientist. Hoffman's play, "Should’ve" (2006) about ethics in science in art, has been produced in workshops; as has a play based on his experiences in the holocaust, "We Have Something That Belongs to You" (2009),later published in book form by Dos Madres Press 
Music.
Hoffmann and Brian Alan produced an English cover of Wei Wei's song "Dedication of Love", part of an international music project raising funds to help the victims of the 2008 Sichuan earthquake.
Honors and awards.
Nobel Prize in Chemistry.
In 1981, Hoffmann received the Nobel Prize in Chemistry, which he shared with Kenichi Fukui "for their theories, developed independently, concerning the course of chemical reactions".
Other awards.
Hoffmann has won many other awards, and is the recipient of more than 25 honorary degrees.
Hoffmann is a member of the International Academy of Quantum Molecular Science and the Board of Sponsors of The Bulletin of the Atomic Scientists.
In August 2007, the American Chemical Society held a symposium at its biannual national meeting to honor Hoffmann's 70th birthday.

</doc>
<doc id="26515" url="https://en.wikipedia.org/wiki?curid=26515" title="Rhotic consonant">
Rhotic consonant

In phonetics, rhotic consonants, or "R-like" sounds, are liquid consonants that are traditionally represented orthographically by symbols derived from the Greek letter rho, including , in the Latin script and , in the Cyrillic script. They are transcribed in the International Phonetic Alphabet by upper- or lower-case variants of Roman , : , , , , , , , and .
This class of sounds is difficult to characterise phonetically; from a phonetic standpoint, there is no single articulatory correlate common to rhotic consonants. Rhotics have instead been found to carry out similar phonological functions or to have certain similar phonological features across different languages. Although some have been found to share certain acoustic peculiarities, such as a lowered third formant, further study has revealed that this does not hold true across different languages. For example, the acoustic quality of lowered third formants pertains almost exclusively to American varieties of English. Being "R-like" is an elusive and ambiguous concept phonetically and the same sounds that function as rhotics in some systems may pattern with fricatives, semivowels or even stops in others—for example, "tt" in American English "better" is often pronounced as an alveolar tap, a rhotic consonant in many other languages.
Types.
The most typical rhotic sounds found in the world's languages are the following:
Characteristics.
In broad transcription rhotics are usually symbolised as unless there are two or more types of rhotic in the same language; for example, most Australian Aboriginal languages, which contrast approximant and trill , use the symbols "r" and "rr" respectively. The IPA has a full set of different symbols which can be used whenever more phonetic precision is required: an "r" rotated 180° for the alveolar approximant, a small capital "R" for the uvular trill, and a flipped small capital "R" for the voiced uvular fricative or approximant.
The fact that the sounds conventionally classified as "rhotics" vary greatly in both place and manner in terms of articulation, and also in their acoustic characteristics, has led several linguists to investigate what, if anything, they have in common that justifies grouping them together. One suggestion that has been made is that each member of the class of rhotics shares certain properties with other members of the class, but not necessarily the same properties with all; in this case, rhotics have a "family resemblance" with each other rather than a strict set of shared properties. Another suggestion is that rhotics are defined by their behaviour on the sonority hierarchy, namely, that a rhotic is any sound that patterns as being more sonorous than a lateral consonant but less sonorous than a vowel. The potential for variation within the class of rhotics makes them a popular area for research in sociolinguistics.
Variable rhoticity.
English has rhotic and non-rhotic accents. Rhotic speakers pronounce a historical in all instances, while non-rhotic speakers only pronounce before or between vowels. A number of other languages have variable rhotic consonants.
Other Germanic languages.
The rhotic consonant is dropped or vocalized under similar conditions in other Germanic languages, notably German, Danish and Dutch from the eastern Netherlands (because of Low German influence) and southern Sweden (possibly because of its Danish history). In most varieties of German (with the notable exception of Swiss Standard German), in the syllable coda is frequently realized as a vowel or a semivowel, or . In the traditional standard pronunciation, this happens only in the unstressed ending "-er" and after long vowels: for example "besser" , "sehr" . In common speech, the vocalization is usual after short vowels as well, and additional contractions may occur: for example "Dorn" ~ , "hart" ~ . Similarly, Danish after a vowel is, unless followed by a stressed vowel, either pronounced ("mor" "mother" , "næring" "nourishment" ) or merged with the preceding vowel while usually influencing its vowel quality ( and or are realised as long vowels and , and , and are all pronounced ) ("løber" "runner" , "Søren Kierkegaard" (personal name) ).
Astur-Leonese.
In Asturian, word final is always lost in infinitives if they are followed by an enclitic pronoun, and this is reflected in the writing; e.g. The infinitive form "dar" plus the 3rd plural dative pronoun "-yos" "da-yos" (give to them) or the accusative form "los" "dalos" (give them). This will happen even in southern dialects where the infinitive form will be "dare" , and both the and the vowel will drop (da-yos, not *dáre-yos). However, most of the speakers also drop the rhotics in the infinitive before a lateral consonant of a different word, and this doesn't show in the writing. e.g. "dar los dos" (give the two ). This doesn't occur in the middle of words. e.g. the name "Carlos" .
Catalan.
In some Catalan dialects, word final is lost in coda position not only in suffixes on nouns and adjectives denoting the masculine singular and plural (written as "-r", "-rs") but also in the "-"ar", -"er", -"ir"" suffixes of infinitives; e.g. "forner" "(male) baker", "forners" , "fer" "to do", "lluir" "to shine, to look good". However, rhotics are "recovered" when followed by the feminine suffix "-a" , and when infinitives have single or multiple enclitic pronouns (notice the two rhotics are neutralized in the coda, with a tap occurring between vowels, and a trill elsewhere); e.g. "fornera" "(female) baker", "fer-lo" "to do it (masc.)", "fer-ho" "to do it/that/so", "lluir-se" "to excel, to show off".
Chinese.
In Mandarin, many words are pronounced with the coda , originally a diminutive ending. The sound did not appear in Mandarin until the 17th century, when a vowel epenthesis (i.e. /ɑ/) was added to (approximate pronunciation in early Mandarin in the 14th century). But this happens only in some areas, mainly in the Northern region, notably including Beijing dialect; as vast majority of Chinese varieties (e.g. Cantonese, Min, Wu) had been separated from early Mandarin by the late 13th century, in other areas it tends to be omitted. But in words with an inherent coda, such as the number two (), , the is pronounced.
French.
Final R is generally not pronounced in words ending in -er. The R in "parce que" (because) is not pronounced in informal speech.
Indonesian and Malaysian Malay.
In Indonesian, which is a form of Malay, the final is pronounced, while it is not in the various forms of Malay spoken on the Malay Peninsula.
Khmer.
Historical final has been lost from all Khmer dialects but Northern.
Portuguese.
In some dialects of Brazilian Portuguese, is unpronounced or aspirated. This occurs most frequently with verbs in the infinitive, which is always indicated by a word-final . In some states, however, it happens mostly with any when preceding a consonant.
Spanish.
Among the Spanish dialects, Andalusian Spanish, Caribbean Spanish (descended from and still closely similar to Andalusian and Canarian Spanish), Castúo (Spanish dialect of Extremadura), Northern Colombian Spanish (in cities like Cartagena, Montería, San Andrés and Santa Marta, but not Barranquilla, which is mostly rhotic) and the Argentine dialect spoken in the Tucumán province have an unpronounced word-final , especially in infinitives, which mirrors the situation in some dialects of Brazilian Portuguese. However, in the Caribbean Antillean forms, word-final in infinitives and non-infinitives is often in free variation with word-final and may relax to the point of being articulated as .
Turkish.
Among the Turkic languages, Turkish displays more or less the same feature, as syllable-final is dropped. For example, it is very common to hear phrases like "gidiyo" instead of "gidiyor", in spoken Turkish. In some parts of Turkey, e.g. Kastamonu, the syllable-final is almost never pronounced, e.g. "gidiya" instead of "gidiyor" (meaning "she/he is going"), "gide" instead of "gider" (meaning "she/he goes"). In "gide", the preceding vowel e is lengthened and pronounced somewhat between an e and a.
Uyghur.
Among the Turkic languages, Uyghur displays more or less the same feature, as syllable-final is dropped, while the preceding vowel is lengthened: for example "Uyghurlar" ‘Uyghurs’. The may, however, sometimes be pronounced in unusually "careful" or "pedantic" speech; in such cases, it is often mistakenly inserted after long vowels even when there is no phonemic there.
Yaqui.
Similarly in Yaqui, an indigenous language of northern Mexico, intervocalic or syllable-final is often dropped with lengthening of the previous vowel: "pariseo" becomes , "sewaro" becomes .

</doc>
<doc id="26517" url="https://en.wikipedia.org/wiki?curid=26517" title="Richard Hell">
Richard Hell

Richard Hell (born Richard Lester Meyers) is an American singer, songwriter, bass guitarist, and writer.
Richard Hell was an innovator of punk music and fashion. He was one of the first to spike his hair and wear torn, cut and drawn-on shirts, often held together with safety pins. Malcolm McLaren, manager of the Sex Pistols, has credited Hell as a source of inspiration for the Sex Pistols' look and attitude, as well as the safety-pin and graphics accessorized clothing that McLaren sold in his London shop, Sex.
Hell was in several important, early punk bands, including Neon Boys, Television, and The Heartbreakers, after which he formed Richard Hell & The Voidoids. Their 1977 album "Blank Generation" influenced many other punk bands. Its title song was named "One of the 500 Songs That Shaped Rock" by music writers in the Rock and Roll Hall of Fame listing and is ranked as one of the all-time Top 10 punk songs by a 2006 poll of original British punk figures, as reported in the "Rough Guide to Punk".
Since the late 1980s, Hell has devoted himself primarily to writing, publishing two novels and several other books. He was the film critic for "BlackBook" magazine from 2004 to 2006.
Biography.
Early life and career.
Richard Hell grew up in Lexington, Kentucky, in the 1950s. His father, a secular Jew, was an experimental psychologist, researching animal behavior. He died when Hell was 7 years old. Hell was then raised by his mother, who came from Methodists of Welsh and English ancestry. After her husband's death, she returned to school and eventually became a professor.
Hell attended the Sanford School in Delaware for one year, where he became friends with Tom Miller, who later changed his name to Tom Verlaine. They ran away from school together and were arrested in Alabama for arson and vandalism a short time later.
Hell never finished high school, instead moving to New York City to make his way as a poet. In New York he met fellow young poet David Giannini, and moved to Santa Fe, New Mexico for several months, where Giannini and Meyers co-founded "Genesis:Grasp". They used an AM VariTyper with changeable fonts to publish the magazine. They began publishing books and magazines, but decided to go their separate ways in 1971, after which Hell created and published Dot Books. Before he was 21, his own poems were published in numerous periodicals, ranging from "Rolling Stone" to the New Directions "Annual"s. In 1971, along with Verlaine, Hell also published under the pseudonym Theresa Stern, a fictional poet whose photo was actually a combination of both his and Verlaine's faces in drag, superimposed over one another to create a new identity.
The Neon Boys, Television, and the Heartbreakers.
In 1972, Verlaine joined Hell in New York and formed the Neon Boys. In 1974, the band added a second guitarist, Richard Lloyd, and changed their name to Television.
Television's performances at CBGB helped kick-start the first wave of punk bands, inspiring a number of different artists including Patti Smith, who wrote the first press review of Television for the "Soho Weekly News" in June 1974. She formed a highly successful band of her own, The Patti Smith Group. Television was one of the early bands to play at CBGB because their manager, Terry Ork, persuaded owner Hilly Kristal to book them to alongside the Ramones. They also built the club's first stage.
Hell started playing his punk rock anthem "Blank Generation" during his time in Television. In early 1975, Hell parted ways with Television after a dispute over creative control. Hell claimed that he and Verlaine had originally divided the songwriting evenly but that later Verlaine sometimes refused to play Hell's songs. Verlaine remained silent on the subject.
Hell left Television the same week that Jerry Nolan and Johnny Thunders quit the New York Dolls. In May 1975, the three of them formed The Heartbreakers; not to be confused with Tom Petty's band, which adopted the same name the following year. After one show, Walter Lure joined The Heartbreakers as a second guitarist.
Richard Hell and the Voidoids.
In early 1976, Hell quit The Heartbreakers and started Richard Hell and the Voidoids with Robert Quine, Ivan Julian and Marc Bell. The band released two albums, though the second, "Destiny Street", retained only Quine from the original group, with Naux (Juan Maciel) on guitar and Fred Maher on drums, and suffered from Hell's distractions, narcotics especially, during recording. Hell's best known songs with the Voidoids included "Blank Generation", "Love Comes in Spurts", "The Kid With the Replaceable Head" and "Time". In 2009, the guitar tracks on "Destiny Street" were re-recorded and released as "Destiny Street Repaired", with guitarists Julian, Marc Ribot and Bill Frisell playing with the original rhythm tracks.
Also in 2009, Hell gave his blessing to the public access program Pancake Mountain to create an animated music video for "The Kid with the Replaceable Head". It was the Voidoids first and only official music video. The cut used for the animation appears on Hell's 2005 retrospective album, "Spurts, The Richard Hell Story".
Dim Stars and Hell's books, further life.
Hell's only other album release was as part of the band Dim Stars, for which he came out of retirement for a month in the early 1990s. Dim Stars featured guitarist Thurston Moore and drummer Steve Shelley from Sonic Youth, Gumball's guitarist Don Fleming, and Quine. They formed only to record the one album and one EP, both titled "Dim Stars", and played one show in public, a WFMU benefit at the The Ritz in Manhattan. Hell played bass, sang lead vocals and wrote the lyrics for the album.
Hell also co-wrote and sang lead vocals on the song "Never Mind" by The Heads, a 1996 collaborative effort between three former members of Talking Heads.
In 1996, Hell wrote a novel, "Go Now", drawn largely from his own experiences. He released a collection of short pieces (poems, essays and drawings) called "Hot and Cold" in 2001. His second novel, "Godlike", was published in 2005 by Akashic Books as part of Dennis Cooper's Little House on the Bowery Series. All three books were highly praised. Also published in 2005 was "Rabbit Duck", a book of 13 poems written in collaboration with David Shapiro. Hell's nonfiction has been widely anthologized as well, including a number of appearances in "best music writing" collections.
Hell's archive of his manuscripts, tapes, correspondence (written and email), journals, and other documents of his life was purchased for $50,000 by New York University's Fales Library in 2003.
Hell has appeared in several low-budget films, most notably Susan Seidelman's "Smithereens". (Other acting appearances include Uli Lommell's "Blank Generation", Nick Zedd's "Geek Maggot Bingo", Rachel Amadeo's "What About Me?" and Rachid Kerdouche's "Final Reward". Hell had a non-speaking cameo role as Madonna's murdered boyfriend in Susan Seidelman's 1985 "Desperately Seeking Susan".)
Hell was married to Scandal's Patty Smyth for two years during 1985–86, and they had a daughter, Ruby. Hell married Sheelagh Bevan in 2002.

</doc>
<doc id="26520" url="https://en.wikipedia.org/wiki?curid=26520" title="Rob Roy (cocktail)">
Rob Roy (cocktail)

The Rob Roy is a cocktail created in 1894 by a bartender at the Waldorf Astoria in Manhattan, New York City. The drink was named in honor of the premiere of "Rob Roy", an operetta by composer Reginald De Koven and lyricist Harry B. Smith loosely based upon Scottish folk hero Rob Roy MacGregor.
A Rob Roy is similar to a Manhattan but is made exclusively with Scotch whisky, while the Manhattan is traditionally made with rye and today commonly made with bourbon or Canadian whisky.
Like the Manhattan, the Rob Roy can be made "sweet", "dry", or "perfect". The standard Rob Roy is the sweet version, made with sweet vermouth, so there is no need to specify a "sweet" Rob Roy when ordering. A "dry" Rob Roy is made by replacing the sweet vermouth with dry vermouth. A "perfect" Rob Roy is made with equal parts sweet and dry vermouth.
The Rob Roy is usually served in a cocktail glass and garnished with 2 maraschino cherries on a skewer (for the standard version) or a lemon twist (for the perfect and dry versions).

</doc>
<doc id="26521" url="https://en.wikipedia.org/wiki?curid=26521" title="Rob Roy">
Rob Roy

Rob Roy may refer to:

</doc>
<doc id="26524" url="https://en.wikipedia.org/wiki?curid=26524" title="Rogue">
Rogue

Rogue may refer to:

</doc>
<doc id="26526" url="https://en.wikipedia.org/wiki?curid=26526" title="Referential transparency">
Referential transparency

Referential transparency and referential opacity are properties of parts of computer programs. An expression is said to be referentially transparent if it can be replaced with its value without changing the behavior of a program (in other words, yielding a program that has the same effects and output on the same input). The opposite term is referential opacity.
With referential transparency, no distinction is made nor difference recognized between a reference to a thing and the corresponding thing itself. Without referential transparency, such difference can be easily made and utilized in programs.
While in mathematics all function applications are referentially transparent, in programming this is not always the case. The importance of referential transparency is that it allows the programmer and the compiler to reason about program behavior as a rewrite system. This can help in proving correctness, simplifying an algorithm, assisting in modifying code without breaking it, or optimizing code by means of memoization, common subexpression elimination, lazy evaluation, or parallelization.
Referential transparency is one of the principles of functional programming; only referentially transparent functions can be memoized (transformed into equivalent functions which cache results). Some programming languages provide means to guarantee referential transparency.
Some functional programming languages enforce referential transparency for all functions.
As referential transparency requires the same results for a given set of inputs at any point in time, a referentially transparent expression is therefore deterministic.
History.
The concept (although not the term) seems to have originated in Alfred North Whitehead and Bertrand Russel's "Principia Mathematica" (1910–13). It was adopted in analytical philosophy by Willard Van Orman Quine in "Word and Object" (1960):
A mode of containment φ is referentially transparent if, whenever an occurrence of a singular term t is purely referential in a term or sentence ψ(t), it is purely referential also in the containing term or sentence φ(ψ(t)).
The term appeared in its contemporary usage, in the discussion of variables in programming languages, in Christopher Strachey's seminal set of lecture notes "Fundamental Concepts in Programming Languages" (1967). The lecture notes referenced Quine's "Word and Object" in the bibliography.
Examples and counterexamples.
If all functions involved in the expression are pure functions, then the expression is referentially transparent. Also, some impure functions can be included in the expression if their values are discarded and their side effects are insignificant.
Consider a function that returns the input from some source. In pseudocode, a call to this function might be codice_1 where codice_2 might identify a particular disk file, the keyboard, etc. Even with identical values of codice_2, the successive return values will be different. Therefore, function codice_4 is neither deterministic nor referentially transparent.
A more subtle example is that of a function that has a free variable, i.e., depends on some input that is not explicitly passed as a parameter. This is then resolved according to name binding rules to a non-local variable, such as a global variable, a variable in the current execution environment (for dynamic binding), or a variable in a closure (for static binding). Since this variable can be altered without changing the values passed as parameter, the results of subsequent calls to the function may differ even if the parameters are identical. However, in pure functional programming, destructive assignment is not allowed, and thus if the free variable is statically bound to a value, the function is still referentially transparent, as neither the non-local variable nor its value can change, due to static binding and immutability, respectively.
Arithmetic operations are referentially transparent: codice_5 can be replaced by codice_6, for instance. In fact, all functions in the mathematical sense are referentially transparent: codice_7 is transparent, since it will always give the same result for each particular codice_8.
Assignments are not transparent. For instance, the C expression codice_9 changes the value assigned to the variable codice_8. Assuming codice_8 initially has value codice_12, two consecutive evaluations of the expression yield, respectively, codice_13 and codice_14. Clearly, replacing codice_9 with either codice_13 or codice_14 gives a program with different meaning, and so the expression is not referentially transparent. However, calling a function such as codice_18 "is" transparent, as it will not implicitly change the input x and thus has no such side effects.
codice_19 is not transparent, as if you evaluate it and replace it by its value (say, "Jan 1, 2001"), you don't get the same result as you will if you run it tomorrow. This is because it depends on a state (the time).
In languages with no side-effects, like Haskell, we can substitute equals for equals because codice_20 for every value of x. This does not hold for languages with side-effects.
Contrast to imperative programming.
If the substitution of an expression with its value is valid only at a certain point in the execution of the program, then the expression is not referentially transparent. The definition and ordering of these sequence points are the theoretical foundation of imperative programming, and part of the semantics of an imperative programming language.
However, because a referentially transparent expression can be evaluated at any time, it is not necessary to define sequence points nor any guarantee of the order of evaluation at all. Programming done without these considerations is called purely functional programming.
One advantage of writing code in a referentially transparent style is that given an intelligent compiler, static code analysis is easier and better code-improving transformations are possible automatically. For example, when programming in C, there will be a performance penalty for including a call to an expensive function inside a loop, even if the function call could be moved outside of the loop without changing the results of the program. The programmer would be forced to perform manual code motion of the call, possibly at the expense of source code readability. However, if the compiler is able to determine that the function call is referentially transparent, it can perform this transformation automatically.
The primary disadvantage of languages that enforce referential transparency is that they make the expression of operations that naturally fit a sequence-of-steps imperative programming style more awkward and less concise. Such languages often incorporate mechanisms to make these tasks easier while retaining the purely functional quality of the language, such as definite clause grammars and monads.
Another example.
As an example, let's use two functions, one which is referentially opaque, and the other which is referentially transparent:
The function codice_21 is referentially transparent, which means that codice_22 if codice_23. For instance, codice_24, and so on. However, we can't say any such thing for codice_25 because it uses a global variable that it modifies.
The referential opacity of codice_25 makes reasoning about programs more difficult. For example, say we wish to reason about the following statement:
One may be tempted to simplify this statement to:
However, this will not work for codice_27 because each occurrence of codice_28 evaluates to a different value. Remember that the return value of codice_25 is based on a global value that isn't passed in and which gets modified on each call to codice_25. This means that mathematical identities such as formula_1 no longer hold.
Such mathematical identities "will" hold for referentially transparent functions such as codice_21.
However, a more sophisticated analysis can be used to simplify the statement to:
This takes more steps and requires a degree of insight into the code infeasible for compiler optimization.
Therefore, referential transparency allows us to reason about our code which will lead to more robust programs, the possibility of finding bugs that we couldn't hope to find by testing, and the possibility of seeing opportunities for optimization.

</doc>
<doc id="26529" url="https://en.wikipedia.org/wiki?curid=26529" title="Reaganomics">
Reaganomics

Reaganomics (; a portmanteau of "Reagan" and "economics" attributed to Paul Harvey) refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.
The four pillars of Reagan's economic policy were to reduce the growth of government spending, reduce the federal income tax and capital gains tax, reduce government regulation, and tighten the money supply in order to reduce inflation.
Historical context.
Prior to the Reagan administration, the United States economy experienced a decade of rising unemployment and inflation (known as stagflation). Political pressure favored stimulus resulting in an expansion of the money supply. President Richard Nixon's wage and price controls were phased out. The federal oil reserves were created to ease any future short term shocks. President Jimmy Carter had begun phasing out price controls on petroleum, while he created the Department of Energy. Much of the credit for the resolution of the stagflation is given to two causes: a three-year contraction of the money supply by the Federal Reserve Board under Paul Volcker, initiated in the last year of Carter's presidency, and long-term easing of supply and pricing in oil during the 1980s oil glut.
In stating his intention was to lower taxes, Reagan's approach was a departure from his immediate predecessors. Reagan enacted lower marginal tax rates as well as simplified income tax codes and continued deregulation. During Reagan's presidency the annual deficits averaged 4.2% of GDP after inheriting an annual deficit of 2.7% of GDP in 1980 under president Carter. The real (inflation adjusted) rate of growth in federal spending fell from 4% under Jimmy Carter to 2.5% under Ronald Reagan. GDP per working-age adult, which had increased at only a 0.8% annual rate during the Carter administration, increased at a 1.8% rate during the Reagan administration. The increase in productivity growth was even higher: output per hour in the business sector, which had been roughly constant in the Carter years, increased at a 1.4% rate in the Reagan years.
During the Nixon and Ford Administrations, before Reagan's election, a combined supply and demand side policy was considered unconventional by the moderate wing of the Republican Party. While running against Reagan for the Presidential nomination in 1980, George H. W. Bush had derided Reaganomics as "voodoo economics". Similarly, in 1976, Gerald Ford had severely criticized Reagan's proposal to turn back a large part of the Federal budget to the states.
Justifications.
In his 1980 campaign speeches, Reagan presented his economic proposals as a return to the free enterprise principles, free market economy that had been in favor before the Great Depression and FDR's New Deal policies. At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics. This movement produced some of the strongest supporters for Reagan's policies during his term in office.
The contention of the proponents, that the tax rate cuts would more than cover any increases in federal debt, was influenced by a theoretical taxation model based on the elasticity of tax rates, known as the Laffer curve. Arthur Laffer's model predicts that excessive tax rates actually reduce potential tax revenues, by lowering the incentive to produce; the model also predicts that insufficient tax rates (rates below the optimum level for a given economy) lead directly to a reduction in tax revenues.
Policies.
Reagan lifted remaining domestic petroleum price and allocation controls on January 28, 1981, and lowered the oil windfall profits tax in August 1981. He ended the oil windfall profits tax in 1988. In 1982 Reagan agreed to a rollback of corporate tax cuts and a smaller rollback of individual income tax cuts. The 1982 tax increase undid a third of the initial tax cut. In 1983 Reagan instituted a payroll tax increase on Social Security and Medicare hospital insurance. In 1984 another bill was introduced that closed tax loopholes. According to tax historian Joseph Thorndike, the bills of 1982 and 1984 "constituted the biggest tax increase ever enacted during peacetime".
With the Tax Reform Act of 1986, Reagan and Congress sought to eliminate many deductions, reduce the highest marginal rates, and simplify the number of tax brackets. In 1983, Democrats Bill Bradley and Dick Gephardt had offered a proposal; in 1984 Reagan had the Treasury Department produce its own plan. The eventual bipartisan 1986 act aimed to be revenue-neutral: while it reduced the top marginal rate, it also cleaned up the tax base by removing certain tax write-offs, preferences, and exceptions, thus raising the effective tax on activities previously specially favored by the code. Ultimately, the combination of the decrease in deductions and decrease in rates raised revenue equal to about 4% of existing tax revenue.
The primary effect of the tax changes over the course of Reagan's term in office was a change in the composition of federal receipts, towards more payroll taxes and new investment taxes, and away from higher earners and capital gains on existing investments. Federal revenue share of GDP fell from 19.6% in fiscal 1981 to 17.3% in 1984, before rising back to 18.4% by fiscal year 1989. Personal income tax revenues fell during this period relative to GDP, while payroll tax revenues rose relative to GDP. Reagan's 1981 cut in the top regular tax rate on unearned income reduced the maximum capital gains rate to only 20% – its lowest level since the Hoover administration. In 1986 President Reagan set tax rates on capital gains at the same level as the rates on ordinary income like salaries and wages, with both topping out at 28%.
Reagan significantly increased public expenditures, primarily the Department of Defense, which rose (in constant 2000 dollars) from $267.1 billion in 1980 (4.9% of GDP and 22.7% of public expenditure) to $393.1 billion in 1988 (5.8% of GDP and 27.3% of public expenditure); most of those years military spending was about 6% of GDP, exceeding this number in 4 different years. All these numbers had not been seen since the end of U.S. involvement in the Vietnam War in 1973. In 1981, Reagan significantly reduced the maximum tax rate, which affected the highest income earners, and lowered the top marginal tax rate from 70% to 50%; in 1986 he further reduced the rate to 28%. The federal deficit under Reagan peaked at 6% of GDP in 1983, falling to 3.2% of GDP in 1987 and to 3.1% of GDP in his final budget. The inflation-adjusted rate of growth in federal spending fell from 4% under Jimmy Carter to 2.5% under Ronald Reagan; however, federal deficit as percent of GDP was up throughout the Reagan presidency from 2.7% at the end of (and throughout) the Carter administration. As a short-run strategy to reduce inflation and lower nominal interest rates, the U.S. borrowed both domestically and abroad to cover the Federal budget deficits, raising the national debt from $997 billion to $2.85 trillion. This led to the U.S. moving from the world's largest international creditor to the world's largest debtor nation. Reagan described the new debt as the "greatest disappointment" of his presidency.
According to William A. Niskanen, one of the architects of Reaganomics, "Reagan delivered on each of his four major policy objectives, although not to the extent that he and his supporters had hoped", and notes that the most substantial change was in the tax code, where the top marginal individual income tax rate fell from 70.1% to 28.4%, and there was a "major reversal in the tax treatment of business income", with effect of "reducing the tax bias among types of investment but increasing the average effective tax rate on new investment". Roger Porter, another architect of the program, acknowledges that the program was weakened by the many hands that changed the President's calculus, such as Congress. President Reagan raised taxes eleven times over the course of his presidency, all in the name of fiscal responsibility, but the overall tax burden went down during his presidency. According to Paul Krugman, "Over all, the 1982 tax increase undid about a third of the 1981 cut; as a share of GDP, the increase was substantially larger than Mr. Clinton's 1993 tax increase." According to historian and domestic policy adviser Bruce Bartlett, Reagan's tax increases over the course of his presidency took back half of the 1981 tax cut. Though since the Reagan tax reductions, top marginal tax rates have remained lower than at any point in US history since 1931, when the top marginal rate was raised from 25% to 63%.
Results.
Overview.
Spending during Reagan's two terms (FY 1981–88) averaged 22.4% GDP, well above the 20.6% GDP average from 1971 to 2009. In addition, the public debt rose from 26% GDP in 1980 to 41% GDP by 1988. In dollar terms, the public debt rose from $712 billion in 1980 to $2.052 trillion in 1988, a roughly three-fold increase. The unemployment rate rose from 7% in 1980 to 10.8% in 1982, then declined to 5.4% in 1988. The inflation rate declined from 10% in 1980 to 4% in 1988.
Some economists have stated that Reagan's policies were an important part of bringing about the second longest peacetime economic expansion in U.S. history. During the Reagan administration, the American economy went from a GDP growth of -0.3% in 1980 to 4.1% in 1988 (in constant 2005 dollars), averaging 7.91% annual growth in current dollars. This reduced the unemployment rate by 1.6%, from 7.1% in 1980 to 5.5% in 1988. A net job increase of about 21 million also occurred through mid-1990. Reagan's administration is the only one not to have raised the minimum wage. The inflation rate, 13.5% in 1980, fell to 4.1% in 1988, which was achieved by applying high interest rates by the Federal Reserve (peaking at 20% in June 1981). The latter contributed to a relatively brief recession in late 1981 and early 1982 where unemployment rose to 9.7% and GDP fell by 1.9%.
The misery index, defined as the inflation rate added to the unemployment rate, shrunk from 19.33 when he began his administration to 9.72 when he left, the greatest improvement record for a President since Harry S. Truman left office. In terms of American households, the percentage of total households making less than $10,000 a year (in real 2007 dollars) shrunk from 8.8% in 1980 to 8.3% in 1988 while the percentage of households making over $75,000 went from 20.2% to 25.7% during that period, both signs of progress.
Unemployment rates.
The job growth under the Reagan administration was an average of 2.1% per year, with unemployment averaging 7.5%. The unemployment averaged 6.4% under President Carter and 7.8% under President Ford. Towards the end of his second term, however, the unemployment rate dropped to 5.4%.
Growth rates.
Comparing the recovery after the 1981–82 recession (1983–1990) with the prior 1970s decade, between 1971 (end of a recession) through 1980, shows that the rate of growth of real GDP per capita averaged 3.05% under Reagan versus 2.14% under Carter. Following the 1981 recession, the unemployment rate had averaged slightly higher (6.75% vs. 6.35%), productivity growth lower (1.38% vs. 1.92%), and private investment as a percentage of GDP slightly less (16.08% vs. 16.86%). Real wages were lower following the recession, while real median family income grew by $4,000 during the Reagan period. But, using data pf U.S. Bureau of Economic Analysis, real U.S. GDP in 2009-chained dollars divided by the U.S. population, shows that real per-capita GDP went from $26,196.55 in the 4th quarter of 1976 to $28,447.21 4th quarter of 1980, a real increase of 8.6% during President Carter’s four years. President Reagan’s eight years in office saw per-capita GDP grow another 23.4% to $35,097.83.
GDP growth.
Comparing the recovery after the 1981–82 recession (1983–1990) with the prior 1970s decade, between 1971 (end of a recession) through 1980, shows that the rate of growth of real GDP per capita averaged 3.05% under Reagan versus 2.14% under Carter.
Poverty level.
During the period 1980–1988, the percentage of the total population below the poverty level ranged from a low of 13.0% in 1980 and 1988 to a high of 15.2% in 1983, yet dropped 1.2% during Reagan's administration and dropped 3.3% from the high in 1983 to the low in 1988. During Reagan's first term, critics noted homelessness as a visible problem in U.S. urban centers. In the closing weeks of his presidency, Reagan told "The New York Times" that the homeless "make it their own choice for staying out there". His policies became widely known as "trickle-down economics", due to the significant cuts in the upper tax brackets, as that extra money for the wealthy could trickle along to low-income groups. Supporters pointed to the drop in poverty by the end of Reagan's term to validate that the tax cuts did indeed trickle down to the poor; opponents noted that the poverty rate quickly shot up even higher in the first year of his successor's term, implying that the full effect of Reagan's policies led to a net increase in poverty.
Federal income tax and payroll tax levels.
During the Reagan administration, federal receipts grew from $618 billion to $991 billion (an increase of 60%); while outlays grew from $746 billion to $1144 billion (an increase of 53%). According to a 1996 report of the Joint Economic Committee of the United States Congress, during Reagan's two terms, and through 1993, the top 10% of taxpayers paid an increased share of income taxes (not including payroll taxes) to the Federal government, while the lowest 50% of taxpayers paid a reduced share of income tax revenue. Personal income tax revenues declined from 9.4% GDP in 1981 to 8.3% GDP in 1989, while payroll tax revenues increased from 6.0% GDP to 6.7% GDP during the same period.
Tax receipts.
According to a 2003 Treasury study, the tax cuts in the Economic Recovery Tax Act of 1981 resulted in a significant decline in revenue relative to a baseline without the cuts, approximately $111 billion (in 1992 dollars) on average during the first four years after implementation or nearly 3% GDP annually. Other tax bills had neutral or, in the case of the Tax Equity and Fiscal Responsibility Act of 1982, a (~+1% of GDP) increase in revenue as a share of GDP. It should be however noted that the study did not examine the longer-term impact of Reagan tax policy, including sunset clauses and "the long-run, phased-in effect of the tax bills". The fact that tax receipts "as a percentage of GDP" fell following the Economic Recovery Tax Act of 1981 shows a decrease in tax burden as share of GDP and a commensurate increase in the deficit, as spending did not fall relative to GDP. Total tax revenue from income tax receipts increased during Reagan's two terms, with the exception of 1982–1983.
The effect of Reagan's 1981 tax cuts (reduced revenue relative to a baseline without the cuts) were at least partially offset by phased in Social Security payroll tax increases that had been enacted by President Jimmy Carter and the 95th Congress in 1977, and further increases by Reagan in 1983 An accounting indicated nominal tax receipts increased from $599 billion in 1981 to $1.032 trillion in 1990, an increase of 72% in current dollars. In 2005 dollars, the tax receipts in 1990 were $1.5 trillion, an increase of 20% above inflation. From 1991 to 2000, receipts increased by 90% in current dollars, or 60% in 2005 dollars. An analysis from the Center on Budget and Policy Priorities calculated that the average annual growth rate of real income-tax receipts per working-age person eventually rose over the two decades from 0.2% (1981–90) to 3.1% (1990–01).
Debt and government expenditures.
Reagan was inaugurated in January 1981, so the first fiscal year he budgeted was 1982 and the final year was 1989. 
Analysis.
According to a 1996 study by the Cato Institute, a libertarian think tank, on 8 of the 10 key economic variables examined, the American economy performed better during the Reagan years than during the pre- and post-Reagan years. Real median family income grew by $4,000 during the Reagan period after experiencing no growth in the pre-Reagan years; it experienced a loss of almost $1,500 in the post-Reagan years. Interest rates, inflation, and unemployment fell faster under Reagan than they did immediately before or after his presidency. The only economic variable that was lower during period than in both the pre- and post-Reagan years was the savings rate, which fell rapidly in the 1980s. The productivity rate was higher in the pre-Reagan years but lower in the post-Reagan years.
Economist Stephen Moore stated in the Cato analysis, "No act in the last quarter century had a more profound impact on the U.S. economy of the eighties and nineties than the Reagan tax cut of 1981." He argued that Reagan's tax cuts, combined with an emphasis on federal monetary policy, deregulation, and expansion of free trade created a sustained economic expansion, the greatest American sustained wave of prosperity ever. He also claims that the American economy grew by more than a third in size, producing a $15 trillion increase in American wealth. Consumer and investor confidence soared. Cutting federal income taxes, cutting the U.S. government spending budget, cutting useless programs, scaling down the government work force, maintaining low interest rates, and keeping a watchful inflation hedge on the monetary supply was Ronald Reagan's formula for a successful economic turnaround.
Milton Friedman stated, "Reaganomics had four simple principles: Lower marginal tax rates, less regulation, restrained government spending, noninflationary monetary policy. Though Reagan did not achieve all of his goals, he made good progress." Further, the Heritage Foundation stated, "the U.S. government must allow the entrepreneur to enjoy the rewards of success. If taxes take away most profit, then the entrepreneur will have less incentive to take a risk. If there are great restrictions on how the entrepreneur can use his profit, then there is little reason for the entrepreneur to take a risk. The entrepreneur's courage to take a risk is what leads to new American discoveries and what drives the U.S. economy forward. Reaganomics knows this. It is one of the reasons why Ronald Reagan has reduced American taxes dramatically."
The Tax Reform Act of 1986 and its impact on the alternative minimum tax (AMT) reduced nominal rates on the wealthy and eliminated tax deductions, while raising tax rates on lower-income individuals. The across the board tax system reduced marginal rates and further reduced bracket creep from inflation. The highest income earners (with incomes exceeding $1,000,000) received a tax break, restoring a flatter the tax system. In 2006, the IRS's National Taxpayer Advocate's report characterized the effective rise in the AMT for individuals as a problem with the tax code. Through 2007, the revised AMT had brought in more tax revenue than the former tax code, which has made it difficult for Congress to reform.
Economist Paul Krugman argued the economic expansion during the Reagan administration was primarily the result of the business cycle and the monetary policy by Paul Volcker. Krugman argues that there was nothing unusual about the economy under Reagan because unemployment was reducing from a high peak and that it is consistent with Keynesian economics for the economy to grow as employment increases if inflation remains low.
The CBO Historical Tables indicate that federal spending during Reagan's two terms (FY 1981–88) averaged 22.4% GDP, well above the 20.6% GDP average from 1971 to 2009. In addition, the public debt rose from 26.1% GDP in 1980 to 41.0% GDP by 1988. In dollar terms, the public debt rose from $712 billion in 1980 to $2,052 billion in 1988, a three-fold increase. Krugman argued in June 2012 that Reagan's policies were consistent with Keynesian stimulus theories, pointing to the significant increase in per-capita spending under Reagan.
William Niskanen noted that during the Reagan years privately held federal debt increased from 22% to 38% of GDP, despite a long peacetime expansion. Second, the savings and loan problem led to an additional debt of about $125 billion. Third, greater enforcement of U.S. trade laws increased the share of U.S. imports subjected to trade restrictions from 12% in 1980 to 23% in 1988.
Economists Raghuram Rajan and Luigi Zingales pointed out that many deregulation efforts had either taken place or had begun before Reagan (note the deregulation of airlines and trucking under Carter, and the beginning of deregulatory reform in railroads, telephones, natural gas, and banking). They stated, "The move toward markets preceded the leader who is seen as one of their saviors." Economists Paul Joskow and Roger Noll made a similar contention.
Economist William A. Niskanen, a member of Reagan's Council of Economic Advisers wrote that deregulation had the "lowest priority" of the items on the Reagan agenda given that Reagan "failed to sustain the momentum for deregulation initiated in the 1970s" and that he "added more trade barriers than any administration since Hoover." By contrast, economist Milton Friedman has pointed to the number of pages added to the Federal Register each year as evidence of Reagan's anti-regulation presidency (the Register records the rules and regulations that federal agencies issue per year). The number of pages added to the Register each year declined sharply at the start of the Ronald Reagan presidency breaking a steady and sharp increase since 1960. The increase in the number of pages added per year resumed an upward, though less steep, trend after Reagan left office. In contrast, the number of pages being added each year increased under Ford, Carter, George H. W. Bush, Clinton, and others. The number of pages in Federal Register is however criticized as an extremely crude measure of regulatory activity, because it can be easily manipulated (e.g. font sizes have been changed to keep page count low). The apparent contradiction between Niskanen's statements and Friedman's data may be resolved by seeing Niskanen as referring to "statutory" deregulation (laws passed by Congress) and Friedman to "administrative" deregulation (rules and regulations implemented by federal agencies).

</doc>
<doc id="26531" url="https://en.wikipedia.org/wiki?curid=26531" title="Roland Corporation">
Roland Corporation

The is a Japanese manufacturer of electronic musical instruments, electronic equipment and software. It was founded by Ikutaro Kakehashi in Osaka on April 18, 1972, with ¥33 million in capital. In 2005, Roland's headquarters relocated to Hamamatsu in Shizuoka Prefecture. Today it has factories in Italy, Taiwan, Japan, and the USA. As of March 31, 2010, it employed 2,699 employees. It has existed in different forms since 1960, making it relatively old among still-operating manufacturers of musical electronics. Known for hundreds of popular synthesizers, drum machines, and other instruments, Roland has been one of the top names in professional music equipment since the late 1970s.
In 2014, Roland Corporation was subject to a management buyout by Roland's CEO Junichi Miki, supported by Taiyo Pacific Partners.
Origin of the Roland name.
Kakehashi founded Ace Electronic Industries in 1960, a manufacturer of numerous combo organs, guitar amplifiers, and effects pedals. He was also contracted by Hammond to produce rhythm machines for the company's line of home organs. In 1973, Kakehashi cut ties with both companies to found Roland.
As with many Japanese start-ups of the period, the name Roland was selected for export purposes as Kakehashi was interested in a name that was easy to pronounce for his worldwide target markets. Rumour has long circulated that he named his company after the French epic poem "La Chanson de Roland." In reality, the name Roland was found in a telephone directory. Kakehashi opted for it as he was satisfied with the simple two-syllable word and its soft consonants. The letter "R" was chosen because it was not used by many other music equipment companies, and would therefore stand out in trade show directories and industry listings. Kakehashi did not learn of "The Song Of Roland" until later.
Brands.
Roland markets products under a number of brand names, each of which are used on products geared toward a different niche.
At one point, Roland acquired the then-defunct Rhodes name, and released a number of digital keyboards bearing the Rhodes brand, but it no longer owns the name. The late Harold Rhodes regained the right to the name in 2000. Rhodes was dissatisfied with Roland's treatment of the marque, and had plans to re-introduce his iconic electric piano, but died before he was able to bring it to market.
Timeline of noteworthy products.
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
"DIGITAL RECORDERS"
"HOME/ACCOMPANIMENT PRODUCTS"
"GUITAR SYNTHS"
"MASTER KEYBOARDS"
"PIANOS"
"RHYTHM PRODUCTS"
"SAMPLERS"
"SEQUENCERS"
"SOUND CANVASES"
"SYNTHS & HI-TECH"
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
AMPS, MIXERS & SPEAKERS
DIGITAL ACCORDIONS
FR5 & FR7
DIGITAL RECORDERS & MIXERS
EDIROL PRODUCTS
GUITAR SYNTHS
HOME/ARRANGER KEYBOARDS
ORGANS
PIANOS
RHYTHM PRODUCTS
SYNTHS & HI-TECH
2005
2006
2007
2008
2009
2010
2011
2012
2014
As of May 23, 2015, SubBass Academy of Electronic Music will be hosting the world's first Roland certified Aira Academy. David Barnard, Roland's education consultant said he was delighted to welcome SubBass as the world’s first AIRA Academy and looks forward to developing a range of courses that balance technical knowledge with musical know-how.
2015

</doc>
<doc id="26532" url="https://en.wikipedia.org/wiki?curid=26532" title="Rhys ap Gruffydd">
Rhys ap Gruffydd

Rhys ap Gruffydd or ap Gruffudd (often anglicised to "Griffith") (1132 – 28 April 1197) was the ruler of the kingdom of Deheubarth in south Wales from 1155 to 1197. Today, he is commonly known as The Lord Rhys, in Welsh "Yr Arglwydd Rhys", although this title may have not been used in his lifetime. He usually used the title "Proprietary Prince of Deheubarth" or "Prince of South Wales", but two documents have been discovered in which he uses the title "Prince of Wales" or "Prince of the Welsh". Rhys was one of the most successful and powerful Welsh princes, and, after the death of Owain Gwynedd of Gwynedd in 1170, the dominant power in Wales.
Rhys's grandfather, Rhys ap Tewdwr, was king of Deheubarth, and was killed at Brecon in 1093 by Bernard de Neufmarché. Following his death, most of Deheubarth was taken over by the Normans. Rhys's father, Gruffydd ap Rhys, eventually was able to become ruler of a small portion, and more territory was won back by Rhys's older brothers after Gruffydd's death. Rhys became ruler of Deheubarth in 1155. He was forced to submit to King Henry II of England in 1158. Henry invaded Deheubarth in 1163, stripped Rhys of all his lands and took him prisoner. A few weeks later he was released and given back a small part of his holdings. Rhys made an alliance with Owain Gwynedd and, after the failure of another invasion of Wales by Henry in 1165, was able to win back most of his lands.
In 1171 Rhys made peace with King Henry and was confirmed in possession of his recent conquests as well as being named Justiciar of South Wales. He maintained good relations with King Henry until the latter's death in 1189. Following Henry's death Rhys revolted against Richard I and attacked the Norman lordships surrounding his territory, capturing a number of castles. In his later years Rhys had trouble keeping control of his sons, particularly Maelgwn and Gruffydd, who maintained a feud with each other. Rhys launched his last campaign against the Normans in 1196 and captured a number of castles. The following year he died unexpectedly and was buried in St David's Cathedral.
Genealogy and early life.
Rhys was the second son of Gruffydd ap Rhys, ruler of part of Deheubarth, by his second wife Gwenllian ferch Gruffydd, daughter of Gruffudd ap Cynan, king of Gwynedd. His elder brother was Maredudd ap Gruffydd, and there were two younger brothers, Morgan and Maelgwn. He also had two older half-brothers, Anarawd and Cadell, from his father's first marriage. Rhys married Gwenllian ferch Madog, daughter of Madog ap Maredudd, the last Prince of all Powys.
His grandfather, Rhys ap Tewdwr, had been king of all Deheubarth until his death in 1093. Rhys ap Tewdwr was killed in Brycheiniog, and most of his kingdom was taken over by Norman lords. Gruffydd ap Rhys was forced to flee to Ireland. He later returned to Deheubarth and ruled a portion of the kingdom, but was forced to flee to Ireland again in 1127. When Rhys was born in 1132, his father held only the commote of Caeo in Cantref Mawr.
The death of King Henry I of England and the ensuing rivalry between Stephen and Matilda gave the Welsh the opportunity to rise against the Normans. A revolt spread through south Wales in 1136, and Gruffydd ap Rhys, aided by his two eldest sons, Anarawd and Cadell, defeated the Normans in a battle near Loughor, killing over five hundred. After driving Walter de Clifford out of Cantref Bychan, Gruffydd set off to Gwynedd to enlist the help of his father-in-law, Gruffudd ap Cynan. In the absence of her husband, Gwenllian led an army against the Norman lordship of Cydweli (Kidwelly), taking along her two youngest sons, Morgan and Maelgwn. She was defeated and killed by an army commanded by Maurice de Londres of Oystermouth Castle. Morgan was also killed and Maelgwn captured.
Gruffydd formed an alliance with Gwynedd, and later in 1136 the sons of Gruffudd ap Cynan, Owain Gwynedd and Cadwaladr ap Gruffydd, led an army to Ceredigion. Their combined forces won a decisive victory over the Normans at the Battle of Crug Mawr. Ceredigion was reclaimed from the Normans, but was annexed by Gwynedd as the senior partner in the alliance. Gruffydd ap Rhys continued his campaign against the Normans in 1137, but died later that year. The leadership of the family now passed to Rhys's half-brother Anarawd ap Gruffydd. In 1143, when Rhys was eleven, Anarawd was murdered by the bodyguard of Cadwaladr ap Gruffydd, brother of Owain Gwynedd, king of Gwynedd. Owain punished Cadwaladr by depriving him of his lands in Ceredigion.
First battles (1146–1155).
Anarawd's brother, Cadell ap Gruffydd, took over as head of the family. Gilbert de Clare, Earl of Pembroke, rebuilt Carmarthen castle in 1145 then began a campaign to reclaim Ceredigion. He built a castle in the commote of Mabudryd, but Cadell, aided by Hywel ab Owain Gwynedd who held Ceredigion for Gwynedd, destroyed it in 1146. Rhys appears in the annals for the first time in 1146, fighting alongside his brothers Cadell and Maredudd in the capture by assault of Llansteffan Castle. This was followed by the capture of Wiston in 1147, Carmarthen in 1150 and Loughor in 1151. In 1151 Cadell was attacked while out hunting by a group of Norman and Flemish knights from Tenby, and left for dead. He survived, but suffered injuries which left him unable to play an active role, and in 1153 he left on a pilgrimage to Rome.
Maredudd became ruler of Deheubarth and continued a campaign, begun in 1150, aimed at recovering Ceredigion, which had been held by Gwynedd since 1136. Maredudd and Rhys were able to drive Hywel ab Owain Gwynedd from Ceredigion by 1153. The same year Rhys is recorded as an independent commander for the first time, leading an army to capture the Norman castle of St Clears. Maredudd and Rhys also destroyed the castles at Tenby and Aberafan that year. Maredudd died in 1155 at the age of twenty-five and left Rhys as ruler of Deheubarth. Around this time he married Gwenllian ferch Madog, daughter of Madog ap Maredudd, prince of Powys.
Early reign.
Loss of territory (1155–1163).
Shortly after becoming ruler of Deheubarth, Rhys heard rumours that Owain Gwynedd was planning to invade Ceredigion in order to reclaim it for Gwynedd. Rhys responded by building a castle at Aberdyfi in 1156. The threatened invasion did not take place, and Turvey claims that Owain's intention may have been to test the resolve of the new ruler.
King Stephen had died in October 1154, bringing to an end the long dispute with the Empress Matilda which had helped Anarawd, Cadell and Maredudd to extend their rule in Deheubarth. With disunity within the realm no longer a problem, the new king of England, Henry II, soon turned his attention to Wales. He began with an invasion of Gwynedd in 1157. This invasion was not entirely successful, but Owain Gwynedd was induced to seek terms and to give up some territory in the north-east of Wales.
The following year, Henry prepared an invasion of Deheubarth. Rhys made plans to resist, but was persuaded by his council to meet the king to discuss peace terms. The terms were much harsher than those offered to Owain: Rhys was stripped of all his possessions apart from Cantref Mawr, though he was promised one other cantref. The other territories were returned to their Norman lords.
Among the Normans who returned to their holdings was Walter de Clifford, who reclaimed Cantref Bychan, then invaded Rhys's lands in Cantref Mawr. An appeal to the king produced no response, and Rhys resorted to arms, first capturing Clifford's castle at Llandovery then seizing Ceredigion. King Henry responded by preparing another invasion, and Rhys submitted without resistance. He was obliged to give hostages, probably including his son Hywel.
The king was absent in France in 1159, and Rhys took the opportunity to attack Dyfed and then to lay siege to Carmarthen, which was saved by a relief force led by Earl Reginald of Cornwall. Rhys retreated to Cantref Mawr, where an army led by five earls, the Earls of Cornwall, Gloucester, Hertford, Pembroke and Salisbury, marched against him. The earls were assisted by Cadwaladr, brother of Owain Gwynedd, and Owain's sons, Hywel and Cynan. However they were forced to withdraw and a truce was arranged. In 1162, Rhys again attempted to recover some of his lost lands, and captured Llandovery castle. The following year Henry II returned to England after an absence of four years and prepared for another invasion of Deheubarth. Rhys met the king to discuss terms and was obliged to give more hostages, including another son, Maredudd. He was then seized and taken to England as a prisoner. Henry appears to have been uncertain what to do with Rhys, but after a few weeks decided to free him and allow him to rule Cantref Mawr. Rhys was summoned to appear before Henry at Woodstock to do homage together with Owain Gwynedd and Malcolm IV of Scotland.
Welsh uprising (1164–1170).
In 1164 all the Welsh princes united in an uprising. Warren suggests that when Rhys and Owain were obliged to do homage to Henry in 1163 they were forced to accept a status of dependent vassalage instead of their previous client status, and that this led to the revolt. Rhys had other reasons for rebellion, for he had returned to Deheubarth from England to find that the neighbouring Norman lords were threatening Cantref Mawr. His nephew, Einion ab Anarawd, who was the captain of his bodyguard, had been murdered at the instigation of Roger de Clare, Earl of Hertford. The murderer had been given the protection of the Clares in Ceredigion. Rhys first appealed to the king to intercede; when this failed, he invaded Ceredigion and recaptured all of it apart from the town and castle of Cardigan. The Welsh revolt led to another invasion of Wales by King Henry in 1165. Henry attacked Gwynedd first, but instead of following the usual invasion route along the north coast he attacked from the south, following a route over the Berwyn hills. He was met by the united forces of the Welsh princes, led by Owain Gwynedd and including Rhys. According to "Brut y Tywysogion":
Torrential rain forced Henry's army to retreat in disorder without fighting a major battle, and Henry vented his spleen on the hostages, having Rhys's son Maredudd blinded. Rhys's other son, Hywel, was not among the victims. Rhys returned to Deheubarth where he captured and burned Cardigan Castle. He allowed the garrison to depart, but held the castellan, Robert Fitz-Stephen, as a prisoner. Shortly afterwards Rhys captured Cilgerran castle.
In 1167 he joined Owain Gwynedd in an attack on Owain Cyfeiliog of southern Powys, and spent three weeks helping Owain besiege the Norman castle of Rhuddlan. In 1168 he attacked the Normans at Builth, destroying its castle. Rhys benefited from the Norman invasion of Ireland in 1169 and 1170, which was largely led by the Cambro-Norman lords of south Wales. In 1167 the King of Leinster, Diarmait Mac Murchada, who had been driven out of his kingdom, had asked Rhys to release Robert Fitz-Stephen from captivity to take part in an expedition to Ireland. Rhys did not oblige at the time, but released him the following year and in 1169 Fitz-Stephen led the vanguard of a Norman army which landed in Wexford. The leader of the Norman forces, Richard de Clare, 2nd Earl of Pembroke, known as "Strongbow", followed in 1170. According to Warren:
The departure of the Norman lords enabled Rhys to strengthen his position, and the death of Owain Gwynedd in late 1170 left him as the acknowledged leader of the Welsh princes.
Later reign.
Peace with King Henry (1171–1188).
In 1171 King Henry II arrived in England from France, on his way to Ireland. Henry wished to ensure that Richard de Clare, who had married Diarmait's daughter and become heir to Leinster, did not establish an independent Norman kingdom in Ireland. His decision to try a different approach in his dealings with the Welsh was influenced by the events in Ireland, although Warren suggests that "it seems likely that Henry began rethinking his attitude to the Welsh soon after the débâcle of 1165". Henry now wished to make peace with Rhys, who came to Newnham to meet him. Rhys was to pay a tribute of 300 horses and 4,000 head of cattle, but was confirmed in possession of all the lands he had taken from Norman lords, including the Clares. They met again in October that year at Pembroke as Henry waited to cross to Ireland. Rhys had collected 86 of the 300 horses, but Henry agreed to take only 36 of them and remitted the remainder of the tribute until after his return from Ireland. Rhys's son, Hywel, who had been held as a hostage for many years, was returned to him. Henry and Rhys met once more at Laugharne as Henry returned from Ireland in 1172, and shortly afterwards Henry appointed Rhys "justice on his behalf in all Deheubarth". According to A. D. Carr:
The agreement between Henry and Rhys was to last until Henry's death in 1189. When Henry's sons rebelled against him in 1173 Rhys sent his son Hywel Sais to Normandy to aid the king, then in 1174 personally led an army to Tutbury in Staffordshire to assist at the siege of the stronghold of the rebel Earl William de Ferrers. When Rhys returned to Wales after the fall of Tutbury, he left a thousand men with the king for service in Normandy. King Henry held a council at Gloucester in 1175 which was attended by a large gathering of Welsh princes, led by Rhys. It appears to have concluded with the swearing of a mutual assistance pact for the preservation of peace and order in Wales. In 1177 Rhys, Dafydd ab Owain, who had emerged as the main power in Gwynedd, and Cadwallon ap Madog from Rhwng Gwy a Hafren swore fealty and liege homage to Henry at a council held at Oxford. At this council the king gave Meirionnydd, part of the kingdom of Gwynedd, to Rhys. There was some fighting in Meirionnydd the following year, but Rhys apparently made no serious attempt to annex it.
Rhys built a number of stone castles, starting with Cardigan castle, which was the earliest recorded native-built stone castle in Wales. He also built Carreg Cennen castle near Llandeilo, a castle set in a spectacular position on a mountain top. He held a festival of poetry and song at his court at Cardigan over Christmas 1176. This is generally regarded as the first recorded Eisteddfod. The festival was announced a year in advance throughout Wales and in England, Scotland, Ireland and possibly France. Two chairs were awarded as prizes, one for the best poem and the other for the best musical performance. J. E. Caerwyn Williams suggests that this event may be an adaptation of the similar French "puys". R.R. Davies suggests that the texts of Welsh law, traditionally codified by Hywel Dda at Whitland, were first assembled in book form under the aegis of Rhys.
Rhys founded two religious houses during this period. Talley Abbey was the first Premonstratensian abbey in Wales, while Llanllyr was a Cistercian nunnery, only the second nunnery to be founded in Wales and the first to prosper. He became the patron of the abbeys of Whitland and Strata Florida and made large grants to both houses. Giraldus Cambrensis, who was related to Rhys, gives an account of his meetings with Rhys in 1188 when Giraldus accompanied Archbishop Baldwin around Wales to raise men for the Third Crusade. Some Welsh clerics were not happy about this visit, but Rhys was enthusiastic and gave the Archbishop a great deal of assistance. Giraldus says that Rhys decided to go on crusade himself and spent several weeks making preparations, but was eventually persuaded to change his mind by his wife Gwenllian, "by female artifices".
Final campaigns (1189–1196).
Henry II died in 1189 and was succeeded by Richard I. Rhys considered that he was no longer bound by the agreement with King Henry and attacked the Norman lordships surrounding his territory. He ravaged Pembroke, Haverfordwest, and Gower and captured the castles of St. Clear's, Laugharne, and Llansteffan. Richard's brother, Prince John (later King John), came to Wales in September and tried to make peace. He persuaded Rhys to raise the siege of Carmarthen and accompany him to Oxford to meet Richard. Rhys arrived at Oxford to discover that Richard was not prepared to travel there to meet him, and hostilities continued.
In his later years Rhys had trouble keeping control of his sons, particularly Maelgwn and Gruffydd. In 1189 Gruffydd persuaded Rhys to imprison Maelgwn, and he was given into Gruffydd's keeping at Dinefwr. Gruffydd handed him over to his father-in-law, William de Braose. Gruffydd is also said to have persuaded his father to annex the lordship of Cemais and its chief castle of Nevern, held by William FitzMartin, in 1191. This action was criticized by Giraldus Cambrensis, who describes Gruffydd as "a cunning and artful man". William FitzMartin was married to Rhys's daughter Angharad, and, according to Giraldus, Rhys "had solemnly sworn, by the most precious relics, that his indemnity and security should be faithfully maintained". Rhys had also annexed the Norman lordships of Cydweli and Carnwyllion in 1190. In 1192 Rhys secured Maelgwn's release, but by now Maelgwn and Gruffydd were bitter enemies. In 1194 Rhys was defeated in battle by Maelgwn and Hywel, who imprisoned him in Nevern castle, though Hywel later released his father without Maelgwn's consent. Giraldus suggests that Rhys's incarceration in Nevern castle was divine vengeance for the dispossession of William FitzMartin. In 1195 two other sons, Rhys Gryg and Maredudd, seized Llanymddyfri and Dinefwr, and Rhys responded by imprisoning them. Rhys launched his last campaign against the Normans in 1196. He captured a number of castles, including Carmarthen, Colwyn, Radnor and Painscastle, and defeated an army led by Roger de Mortimer and Hugh de Say near Radnor, with forty knights among the dead. This, the Battle of Radnor, was Rhys' last battle. William de Braose offered terms, and Painscastle was returned to him.
Death and aftermath (1197).
In April 1197 Rhys died unexpectedly and was buried in St David's Cathedral. The chronicler of "Brut y Tywysogion" records for 1197:
Rhys died excommunicate, having quarreled with the Bishop of St. David's, Peter de Leia, over the theft of some of the bishop's horses some years previously. Before he could be buried in the cathedral, the bishop had his corpse scourged in posthumous penance.
Rhys had nominated his eldest legitimate son, Gruffydd ap Rhys, as his successor, and soon after his father's death Gruffydd met the Justiciar, Archbishop Hubert Walter, on the border and was confirmed as heir. Maelgwn, the eldest son but illegitimate, refused to accept this and was given military assistance by Gwenwynwyn ab Owain of Powys. Maelgwn took the town and castle of Aberystwyth and captured Gruffydd, whom he handed over to the custody of Gwenwynwyn. Gwenwynwyn later handed him over to the king, who imprisoned him at Corfe Castle. Gruffydd was set free the following year and regained most of Ceredigion. In 1201 Gruffydd died, but this did not end the fighting between rival claimants. In 1216 Llywelyn the Great of Gwynedd held a council at Aberdyfi where he allocated parts of Deheubarth to several sons and grandsons of Rhys.
Character and historical assessment.
Giraldus Cambrensis frequently mentions Rhys in his writings and describes him as "a man of excellent wit and quick in repartee". Gerald tells the story of a banquet at Hereford in 1186 where Rhys sat between two members of the Clare family. What could have been a tense affair, since Rhys had seized lands in Ceredigion previously held by the Clare family, passed off with an exchange of courteous compliments, followed by some good-natured banter between Rhys and Gerald about their family connections. Rhys gave Gerald and Archbishop Baldwin a great deal of assistance when they visited Wales to raise troops for the crusade in 1188, and Gerald several times refers to his "kindness" and says that Rhys accompanied them all the way from Cardigan to the northern border of Ceredigion "with a liberality peculiarly praiseworthy in so illustrious a prince".
Another contemporary writer also wrote of Rhys if Roger Turvey is correct in stating that Walter Map's piece "Of the King Appollonides" deals with Rhys under a pseudonym. Map was less favourably disposed toward Rhys, describing him as "This king I have seen and know, and hate", but goes on to say "I would not have my hatred blacken his worth; it is not my wish ever to suppress any man's excellence through envy". He tells the following story about Apollonides/Rhys:
Davies provides the following assessment of Rhys:
Davies also notes two flaws in Rhys's achievement. One was the personal nature of his accord with Henry II, which meant that it did not survive Henry's death. The other was his inability to control his sons and to force the other sons to accept Gruffydd as his successor.
Children.
Rhys had at least nine sons and eight daughters. Confusingly, three of the sons were named Maredudd and two of the daughters were named Gwenllian.

</doc>
<doc id="26534" url="https://en.wikipedia.org/wiki?curid=26534" title="Rachel Summers">
Rachel Summers

Rachel Anne Summers (also known as Rachel Grey) is a fictional superheroine appearing in American comic books published by Marvel Comics, Rachel was created by writer Chris Claremont and artist/co-writer John Byrne.
In her first appearance, the character's surname was not revealed; later publications and retcons further expanded her backstory to involve central characters of mainstream continuity. She is the daughter of the alternate future counterparts to Cyclops (Scott Summers) and Jean Grey-Summers from a harsh dystopia, the half sister of Cable, a niece of Havok and Vulcan, and a powerful mutant in her own right.
Rachel Summers inherited her mother's vast telepathic and telekinetic talents. She also inherited her mother's original code names Phoenix and Marvel Girl. Although the character is considered unique to the Marvel Comics "multiverse", her name has been used to designate the mother of Marvel characters Hyperstorm and Dream Richards in respective timelines.
Publication history.
Rachel first appeared in "The Uncanny X-Men" #141 (January 1981) and has since been affiliated with several comic book superhero teams including the X-Men and Excalibur.
Fictional character biography.
Future adolescence.
Rachel Anne Summers comes from an alternate future Earth known as Earth-811, as seen in the "Days of Future Past" storyline from "The Uncanny X-Men" #141–142. In this reality, the assassination of Senator Robert Kelly provoked the ratification of the Mutant Registration Act, leading to a dystopian future where the mutant-hunting Sentinel robots rule North America. Rachel was abducted by operatives working for Ahab, who used drugs and hypnotherapy to turn Rachel into a "Hound," a mutant who tracks down other mutants. She fulfilled her duties, but her psychic powers linked her to her victims; fueling her grief and despair until she attacked Ahab and scarred him. In return, he sent her to the mutant concentration camps. There, she befriended the surviving mutant rebels, including Wolverine, Magneto, Colossus, Storm, Kate Pryde, and her lover, the adult Franklin Richards.
Rachel managed to send Kate's consciousness into the past to her younger self to prevent the assassination, but it did not change their time. Rachel sent her astral form into the past to find out why and discovered she had sent Kate into an alternate past. On the way back, she encountered the disembodied Phoenix Force and it followed her to her present. Rachel passed out from the strain of astral projection and the Phoenix Force revealed itself to Kate, who asked it to give Rachel a fresh start.
When Rachel and Kate broke into "Project: Nimrod" on a suicide mission to destroy a new model of Sentinel, they became trapped. When Kate spoke the words "Dark Phoenix," the Phoenix Force ripped Rachel from her timeline and sent her body back to the alternate past to which she had sent Kate's consciousness. This was a past where she learned Jean Grey was dead and that her father was married to someone else. Rachel experienced additional heartache and displacement trauma when she discovered that her father's new wife, Madelyne Pryor, was pregnant with a son (Nathan Summers), because in her timeline she was the first-born child of Scott Summers.
X-Men.
Rachel had a brief membership in the X-Men before finding a Shi'ar holoempathic crystal at her grandparents' home. The crystal was imprinted with a portion of Rachel's mother's essence inside it as a tribute to the family shortly after Jean Grey's death. After Rachel took a vow to remember her mother with the uniform and name of Phoenix, the Phoenix Force fully bonded with her. She was granted access to its power on a cosmic magnitude, albeit in a much more limited fashion than the Dark Phoenix. Soon after, the grudge which she had begun with Selene boiled over when Rachel secretly invaded the Hellfire Club. She did this with the intention of taking vengeance on Selene for the murders she had committed, particularly that of nightclub owner Nicholas Damiano, who had previously taken Rachel into his home after Selene had attacked her. Selene proved to be no match for Rachel's newly increased powers, but just as she was about to finish Selene, Wolverine arrived and was forced to stab Rachel in the chest. Mortally injured, Rachel was lured into Spiral's "Body Shoppe."
Excalibur.
Months later, while recuperating from injuries on Muir Island, Shadowcat and Nightcrawler both had the same dream, where they were actors on a weird set and helped Rachel, who was trapped there, escape. Shortly thereafter, Rachel escaped from the alternate reality of Mojoworld. Rachel has once been cited having a flashback to her time there where she is held in chains and tortured. The three former X-Men were joined by Captain Britain and Meggan and founded the British superhero team Excalibur. While part of the team, she discovered that this universe's version of her mother, Jean Grey, was alive. She attempted to bond with Jean, but Jean, upon discovering Rachel was the present host for the Phoenix, rejected any contact with her. Jean still resented the Phoenix Force for stealing a portion of her life. She also rejected Rachel because she felt that Rachel's existence was a constant reminder of the dystopian future she feared could still come to pass. Eventually, however, Jean moved past those feelings and formally welcomed Rachel into her life.
Askani.
Rachel remained with Excalibur until an incident caused Captain Britain to be lost in the timestream. She exchanged places with the time-lost Captain Britain and emerged two thousand years in the future, in a world conquered by Apocalypse and crushed under his iron fist. She gathered together a group of rebels and founded the Askani. She trained one of her followers to travel back in time and bring her "brother" Nathan forward in time when he was infected with a techno-organic virus. The Askani cloned Nathan in case he was not able to survive the virus. Apocalypse's followers attacked the Askani and took the clone (who would later become the supervillain Stryfe), leaving Rachel critically injured. Hooked up to life support, she drew the minds of Scott and Jean into the future, as "Slym" and "Redd", to raise Nathan and tutor him in the use of his powers. Rachel finally died ten years later and sent Scott and Jean back to their original bodies seconds after they had left.
After Nathan, now known as Cable, had finally defeated Apocalypse, he went into the timeline to retrieve Rachel. There, he discovered a Rachel "sans" Phoenix Force. With the premature death of Apocalypse, the Askani timeline had been diverged from the mainstream Marvel Universe, Earth-616. As a result, she had been flung into the far future, yet subjectively a short time after she had been lost in the timestream, as the slave of a creature called "Gaunt," who had used her to lead Cable there for a "battle of the ages." Cable defeated Gaunt in the battle and Rachel, now free, was able to use her residual Phoenix Force to return them both to the present. She then decided to take a break from superheroics and enrolled in college after she made Cable promise he would not tell anyone she was back. Despite her efforts to live a normal life, however, she was kidnapped by the telepath Elias Bogan and subsequently rescued by the X-Men.
Rachel Grey.
She decided to rejoin the X-Men, taking the name "Marvel Girl" to honor her mother (who had recently died yet again) and wearing a costume her mother had designed but never worn; a variation on Jean's first green costume. She also changed her last name to "Grey", possibly to express disapproval at her father's betrayal of Jean, as well as his continuing relationship with Emma Frost; though she and Emma made a truce of sorts during one of the team's missions in Hong Kong. Rachel and Nightcrawler began to have an attraction towards each other, kissing at one point, but nothing came of it as Nightcrawler also had an attraction to Storm at the time, who was in somewhat of a romantic "friendship" with Wolverine. Her stint with this team also included a visit to the Savage Land. In the storyline "World's End", which was heavily criticized by readers, Rachel was subjected to the mind-control of a tribe of advanced dinosaur people, the Hauk'ka, causing her to believe she belonged to their species. Afterward, she subconsciously used her telekinesis to change her own genome in their image. This was eventually reverted. After "House of M" and "Decimation", where most of the world's mutants lost their powers, the government had Sentinels instituted at the X-Mansion to protect the mutants in case any enemies used this low point to attack. Though their intentions were good this time, it reminded Rachel too much of the previous timeline when Sentinels herded mutants into concentration camps.
Rachel had a short stint with newly re-formed Excalibur, reminiscent of the former team, shortly after the "House of M" events. She assisted the team in battling Shadow-X and the Shadow King (in the guise of Professor X).
End of Greys.
Rachel spent some time with her grandparents, bonding with her grandfather. At a family reunion with all her relatives, a commando unit under the order of the Shi'ar attacked the party, killing everyone in an effort to wipe out the Grey genome. The commando unit was unable to kill Rachel; instead, one member was able to graft a "deathmark" on her back that would allow them to find her wherever she went. It is assumed that the only remaining member of the Grey family now left on Earth besides Rachel is Cable. Afterward, at the graves of the Grey family, Rachel vowed a terrible vengeance on the Shi'ar and was quoted as saying, "I'm not my mom. I'm not the Phoenix. I'm my own woman. And by the time I'm done... they'll wish I "was" the Phoenix."
The Death Commandos later tracked Rachel to the psychiatry office of Dr. Maureen Lyszinski. Rachel, with the help of Psylocke, Nightcrawler, Bishop, and Cannonball, saved the doctor and took down the Death Commandos. She decided to imprison them, instead of killing them, by telling them, "I mean to find destiny in a way that brings us both Grey honor." She is also sometimes referenced as "Starchilde" in this series.
Rise and Fall of the Shi'ar Empire.
After Rachel was kidnapped, along with Cyclops, by her paternal uncle Vulcan, he freed Darwin from inside him. Later, Professor X recruited Rachel, along with Havok, Nightcrawler, Warpath, Darwin, and Polaris, for a space mission to stop Vulcan from laying waste to the Shi'ar empire. Xavier, who recently was stripped of his powers, recruited Rachel to serve as his telepathic "eyes and ears" during their mission. Aware of Rachel's vendetta against the Shi'ar, Xavier agreed to use their trip into space to find out who in the Shi'ar Empire gave the order to wipe out all members of the Grey family, and he warned Rachel that they will deal with the people responsible for her recent losses Xavier's way.
While in space, the team was attacked by Korvus, a Shi'ar warrior sent to kill Rachel. Korvus' ancestor, Rook'shir, was a previous host of the Phoenix Force, and a small portion of the Phoenix's power was left behind in his sword, the Blade of the Phoenix. With this power, Korvus made short work of the other X-Men, but when Rachel blocked the sword, their minds were involuntarily linked. Through this link, Rachel learned that Korvus' family was also murdered by the Shi'ar government because of their connection to the Phoenix. The remaining echo of the Phoenix power from the sword was then transferred to Rachel. Rachel claimed that rather than having taken the power, the power chose to go to her, saying, "The Phoenix knows me, remember? It likes me." When this happened, Rachel's normally gold energy aura turned blue, the same color as the Blade of the Phoenix. She then telekinetically disabled an explosive implant that the Shi'ar chancellor was using to force Korvus' obedience.
Due to Rachel's connection to Korvus through the sword, she discovers the Phoenix Force formerly in the blade is just an echo, a "blue shadow", of the Force. The shadow of the Phoenix begins influencing Rachel's behavior, causing her to design a new darker uniform and begin a romance with Korvus. She soon breaks off the relationship after she realizes their bond is only because of the residual Phoenix Force.
Leading up to the fight with Vulcan, Rachel is shown using her powers to kill the guards who stand in her way. Havok warns her not to, but Rachel tells him that they deserve to die after what they did to her family. When it comes to the big fight, Rachel shows just how powerful she is by protecting Korvus from one of Vulcan's blasts. Rachel is one of the X-Men stranded in Shi'ar space when their ship is sent back to Earth.
After the death of her other grandfather, Corsair, at the hands of Vulcan, she, along with Havok, Polaris, Korvus, Ch'od, and Raza, become the new Starjammers. They elect to remain in Shi'ar space and restore Lilandra to the throne or die trying. As her uncle states, "If they fail, he has no doubt that Vulcan will head for Earth."
Starjammers.
During the conflict, the Starjammers find another threat in the form of the Scy'ar Tal (translates as "Death to the Shi'ar"). Rachel makes contact with the eldest Scy'ar Tal and discovers their true origin. The Scy'ar Tal were originally called the M'Kraan. Early in their history, the Shi`ar attacked them, killing a great number of their people and making the rest flee for their lives. Eventually, the Shi'ar settled on their planet, took the M'Kraan Crystal as their own, and passed down the legend of the M'Kraan Crystal as a sacred gift from their deities, Sharra and K'ythri. The M'Kraan then changed their name to Scy'ar Tal and devoted their culture and society to the destruction of the Shi`ar Empire. With their first attack, they destroyed Feather's Edge by transporting a star to obliterate it. After which, Vulcan made contact with the Starjammers to call a temporary ceasefire.
During the ceasefire, Rachel comes into contact with the Death Commandos again and attempts to kill them to avenge the deaths of her relatives; however, she is deterred by Polaris. At the end, all the Starjammers are captured by the Shi'ar except Rachel, Korvus, and Lilandra.
X-Men: Kingbreaker and War of Kings.
Rachel and the Starjammers play a large role in the sequel to the "Emperor Vulcan" miniseries called "X-Men: Kingbreaker". She is also seen prominently in the "War of Kings" storyline, which features Vulcan, the Inhumans, Nova, and the Guardians of the Galaxy.
While with the Starjammers, in battle with Vulcan's new guard, the fragment of the "blue" Phoenix within her and Korvus' blade mysteriously leaves them. After the Phoenix echo leaves Rachel, she says "please... not now... Mom." From this frame onward, the "hound" markings reappear on Rachel's face.
In agreement with the Inhumans, the Starjammers and the Guardians of the Galaxy assault a Shi'ar vessel in order to free Lilandra, hoping to end the conflict while restoring her to the throne. Even without her Phoenix powers, Rachel is powerful enough to entrap Gladiator in an illusion in order to keep him distracted from battle. Their gambit pays off and the group is able to free Lilandra.
Rachel is next seen as Lilandra's bodyguard along with the rest of the Starjammers. On the home planet of the Shi'ar, Lilandra assumes her throne, but while making a ceremonial gesture is killed by the murderer known as Razor, who possesses the Darkhawk armor. The only person who perceives this is Rachel, since Razor is shielded from the perceptions of others.
After Lilandra is assassinated, Rachel fights alongside the Starjammers against the Shi'ar Guard and Araki, who has summoned the same Shi'ar commandos that killed Rachel's family and branded her with the Shi'ar death mark. Rachel uses her powers to implode Black Cloak's head, saying, "He was the one... He killed my family," though killing him does not make her feel happier. Gladiator finishes the job by killing Araki himself. Rachel, along with the rest of the Starjammers, regroup later on and mourn the Shi'ar, as they doubt that they will recover from this war.
Realm of Kings.
It is known through Ch'od, and apparently due to the incident where she and Korvus both lost the connection to the Phoenix Force, that Rachel and Korvus, along with Havok and Polaris, have departed for Earth.
Age of X.
While on the way back to Earth, Rachel attempted to contact Professor Xavier or Emma Frost with a message. However at that moment, Moira (a powerful alternate personality of the mutant Legion) warped reality taking Rachel's mind with it creating the amnesic Revenant. Once reality was restored, Rachel's mind is separated from her body which according to her is "half a universe away". Because of Moira's actions, Rachel no longer remembers the message and her mind retains the form of her Age of X counterpart. Scott promised her that they would return her home.
Schism.
Being actually a mental manifestation, Rachel soon began fading. She asks Rogue to connect with her to see what will happen. When the two of them touch, Rogue sees a vision of where Rachel, Havok and Polaris are as Rachel then returns to her body. When she awakens she is met by an unseen villain holding a gun and telling her he has killed her friends.
Borrowing one of Legion's manifold powers, Rogue teleports alongside Gambit, Magneto and Frenzy to rescue their teammates on the other side of the galaxy. Once there, Rachel is retrieved from a band of pirates as Rogue becomes their new leader. The remaining X-Men discover they've arrived at a space station called Gul Damar which is in a state of upheaval due to the rebellion of insectoid creatures called Grad Nan Holt against their Shi'ar enslavers. They are also being pulled into an exploding sun and the entire civil war is revealed to be orchestrated by a powerful Grad Nan Holt telepath known as "Friendless." A number of battles with the creature proves unsuccessful for Rachel, but with the combined efforts of Rogue and the X-Men they are able to defeat him and return home via the wormhole that was created from the collapsing star.
Rachel as part of a team of Wolverine's X-Men attempted to psychically battle Exodus in an attempt to prevent the assassination of Cyclops. The team is eventually beaten and the X-Men are saved by Generation Hope.
X-Men: Regenesis.
Rachel is then invited to hold a position as senior staff member of the "Jean Grey School for Higher Learning," which was rebuilt from the Xavier Institute and has Wolverine as acting Headmaster and Kitty Pryde as Headmistress.
Avengers Vs. X-Men.
During the events of AVX, when the X-Men on Utopia escape from the invading Avengers, Rachel contacts Cyclops to provide Hope's location. Afterwards, Rachel states that she knows the Phoenix Force better than anyone else on Earth and that she is living proof that it can be controlled. She also says that if the Phoenix has chosen her and that is the destiny that Hope wants, she will do everything in her power to help her. She and Iceman tell Wolverine at the school that they are going to help Cyclops in the battle.
"X-Men" (vol. 4).
In 2013, Marvel revealed an all new all female series simply named "X-Men". Written by Brian Wood with art by Olivier Coipel, "X-Men" will feature an all female cast including Storm, Jubilee, Rogue, Kitty Pryde, Rachel Grey and Psylocke.
The X-Men are in pursuit of Arkea who has attempted to revive the X-Men's long lost enemies. The X-Men are attacked by the newly formed Sisterhood and Rachel is telepathically locked down by Madelyne Pryor. Storm strikes a deal for Rachels freedom allowing Madelyne and Selene to walk free.
"AXIS".
Storm sends the entire student body against the Red Onslaught in an attempt to save Earth. Rachel is shown to be reverted to her hound form by Dr. Doom and Wanda Maximoffs spell.
"Amazing X-Men".
Rachel is part of Storm's team of X-Men who attempts to stop the Living Monolith from becoming the new Juggernaut.
"Spider-Man and the X-Men".
Rachel Grey is distrusting of Spider-Man's newly appointed position as Jean Grey School guidance counseller. Using her telepathy she seeks to expose his secret identity only to lose that memory by being mind-wiped by Martha Johansson.
"Storm (2014-2015)".
Rachel is shown as a teacher at the Jean Grey Institute in the battle against Kenji.
Powers and abilities.
Rachel possesses various psionic abilities, generally exhibited through telepathy, psychometry, telekinesis, and limited time manipulation.
Telepathy.
Marvel Girl's "virtually unlimited" telepathy allows her to receive, broadcast, and manipulate cognitive processes (such as thoughts) in an intricate manner. Examples of Rachel’s aptitudes for this include creating durable mind-links across distances, projecting blasts of psionic energy that disrupt aspects of brain functioning, shielding her mind from other telepaths, creating illusions, and rendering someone invisible to the five senses. In addition, Rachel has demonstrated the ability to telepathically suppress superpowers; control, repair, and exchange minds (even cross-temporally: see Chronoskimming); as well as safely editing memories. Rachel has also harnessed her telepathy to sense, locate, and track other mutants based on their thought patterns, but has a moral apprehension about using this skill due to her experiences as a Hound.
It has been suggested that Rachel's telepathy, although immeasurable in raw power, is mitigated by her limited training and finesse. Emma Frost was able to outflank an incredulous Rachel in a contest on the astral plane. In the same issue, Emma offered her educative services; and later still, Rachel received training from Professor Charles Xavier (while he was depowered), giving her access to his vast knowledge and expertise in telepathy.
Telekinesis.
By using telekinesis, Marvel Girl can remotely manipulate matter even on a sub-atomic level. She can channel this ability to create protective force fields and blasts of concussive force. By using her telekinesis to levitate herself, Marvel Girl can fly at incredible speeds. Rachel has been able to create a micro black hole (sans Phoenix Force), levitate an entire city for a time, sustain shields that withstood Jovian atmospheric pressures, and direct blows from Thor's hammer, Mjolnir. Moreover, Rachel's telekinetic fine-motor control has allowed her to alter molecular valences, mentally alter clothing with ease, create a telekinetic/psionic sword (much like Psylocke's telekinetic katana), a telekinetic hammer powerful enough to knock Thor off his feet, and even rewrite human genomes.
While all depictions portray Rachel as an extremely powerful telekinetic, the limitations of her hard-psi capabilities have been somewhat inconsistent. Some instances have depicted Rachel's telekinetic potential to be nigh-unlimited, whereas others have shown her struggling against, and even outmatched by, lesser developed telekinetics such as Psylocke.
Chronoskimming.
Marvel Girl utilizes her psionic talents in conjunction with her ability to manipulate time in a variety of ways. "Chronoskimming" describes her ability to temporarily transplant a person's mind and send it through time into a younger/older version, a close ancestor/descendant, or as a disembodied astral form.
Rachel unconsciously emanates a fourth dimensional pulse, effectively creating a chrono-shield that protects her from changes in the timeline. She can also sense and manipulate residual psychic energy in the form of psychometry.
Phoenix Force.
When Rachel was bonded to the cosmic entity known as the Phoenix Force, she demonstrated heightened psi-powers, the ability to manipulate energy and life-forces, and limited cosmic awareness. Rachel's connection to the Phoenix power was lost in the distant future and did not return with her when she traveled back to the early 21st century (present) of Earth-616 (Marvel's mainstream universe).
Most recently, Marvel Girl absorbed a residual echo of the Phoenix Force left in the sword of a previous host, a Shi'ar named Rook'shir. It was revealed that this energy source was a less powerful (but easier to wield) form of the Phoenix Force. The echo was powerful enough to allow Rachel to survive in and fly through the vacuum of space without the need for additional protection, as well as being able to hold her own in combat against the tremendous physical power of Gladiator. These demonstrations were short lived, however, due to its disappearance, which Rachel attributes to Jean Grey. She now exhibits her standard power levels.
Power signature.
As a host for the Phoenix Force, Rachel manifested the cosmic firebird in displays of great power. During her 2000s "Uncanny X-Men" appearances, Marvel Girl also exhibited a Phoenix emblem over her left eye whenever she demonstrated psionic feats. It was at first accompanied by a "shadow form" (similar to the one Jean Grey manifested when she absorbed the telepathic powers of Psylocke). However, the illustration of this shadow form ceased without explanation. After regaining a small portion of the Phoenix Force (echo), the emblem over her eye changed from a gold Phoenix shape to a static version made of electric blue flame. Her display of power was once more altered in "X-Men: Emperor Vulcan" #3, where she produced the familiar fiery raptor with which the Phoenix Force is commonly associated (see profile image).
Skills and abilities.
At times, Rachel has been shown to learn new skills extremely quickly. For example, she mastered a set of "demon ninja" sword skills simply by watching her teammate Shadowcat perform them. Along with sword fighting, Rachel has experience in lock-picking, vehicular repair (such as engines), and use of advanced technology and weaponry. However, these abilities have not been evident in her more recent appearances.
Potential and limitations.
Rachel's power level and scope of abilities have been inconsistently described by writers over the years. However, she is usually depicted with "virtually unlimited" potential in her dual psionic talents. In most cases, she displayed greater feats as the Phoenix and even matched Gladiator's strength with the aid of a "Phoenix echo". Rachel is considered by many to be an Omega-level mutant (like her mother), but the only literary reference to this attribute is when the future Sentinel, Nimrod, classified Rachel as an "Omega class subject" several years before the term was established in Marvel canon.
Even with the omnipotent strength of the Phoenix, magic and magical objects are able to resist Rachel's powers. When the Soulsword appeared near the Excalibur lighthouse headquarters seeking Kitty Pryde to become its new wielder, Rachel attempted to remove it from bedrock to alleviate her friend's apprehension. Despite using the full extent of power permitted by the Phoenix Force, Rachel was unable to remove the sword, surmising that only Kitty could remove it.
Other versions.
In the very first issue of the "Uncanny X-Men" story arc "Season of the Witch", Rachel and Psylocke were transported to the White Hot Room as an indirect result of the reality-shift performed by a mentally unstable Scarlet Witch. While there, it was established that the Rachel appearing in Earth-616 (originally from Earth-811) has no true alternate counterparts within the Marvel multiverse. Rather, all other incarnations of "Rachel Summers" that exist in parallel timelines (see below) are linked only by having the same name, or attributes.
House of M.
The subsequent issues had Rachel and Psylocke return to their dimension and thus be engrossed in the Scarlet Witch’s reconstruction of the world. In this reality, Rachel was the bodyguard and traveling companion to Psylocke, who was crowned British royalty after her brother, Brian, became ruler of all England. Rachel then became involved with Captain Britain's mission to seal the breach in reality (rift) that was created by the Scarlet Witch's manipulations.
Variations of Days of Future Past.
In at least three alternate future timelines derived from "Days of Future Past", a Rachel Summers married Franklin Richards and procreated mutant children. One such child was the time-traveling supervillain Hyperstorm (Jonathan Richards). Hyperstorm was responsible for causing the Fantastic Four to think that Mister Fantastic (Jonathan's grandfather) and Doctor Doom were dead; he was only defeated when he was trapped in another dimension by Galactus. The second child was Dream Summers, who possessed empathic superpowers. She was a superhero who appeared in the "Spider-Man/X-Men: Time's Arrow" trilogy of novels (although Marvel Comics novels tend to be considered non-canon). In a third reality, they produced a child named David Richards, who was rescued from a concentration camp by the interdimensional traveling Exiles and raised by the "Age of Apocalypse" version of Sabretooth. David's traumatic experiences at the camp motivated him to become a fanatical murderer.
In another variation of the "Days of Future Past", shown in "Weapon X: Days of Future Now", a Rachel Summers was captured by Weapon X and detained in the "Neverland" concentration camp.
Exiles.
In the so-called "Legacy Earth" reality, in which the Legacy Virus mutated into a techno-organic plague, a Rachel Summers was a member of the Avengers, the last superhero group. At Morph's behest, she contacted Thor and the Asgardians to help them against the Vi-Locks, a race of beings infected with the techno-organic virus.
"X-Men: The End".
The miniseries "" (written by Chris Claremont) details the last adventures of the X-Men in a possible future. In this reality, Rachel played a central role, and was the political campaign manager of Kitty Pryde (Chicago mayoral candidate). However, she abandoned this position after Cassandra Nova led a series of attacks on the X-Men and their allies. She traveled alongside Cable into Shi'ar space. After a Professor X was taken hostage by Nova, and she entered his psyche in order to free him. However, Nova traps her and Lilandra within. She seemingly kills them both, but Rachel used the power of the Phoenix to save herself, and incapacitates Nova. They leave Xavier's mind, and continue the battle in Rachel's mind, where Rachel is overpowered. Cassandra, using Rachel's body, kills Jean Grey and Cyclops, leaving Rachel forced to watch the ordeal. Nova then leaves Rachel's body, stealing the Phoenix force. The Phoenix being the only thing keeping her alive after her Cassandra destroyed her mental form, she simply died.

</doc>
