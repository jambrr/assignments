<doc id="25757" url="https://en.wikipedia.org/wiki?curid=25757" title="Robertson Davies">
Robertson Davies

William Robertson Davies, CC, OOnt, FRSC, FRSL (August 28, 1913 – December 2, 1995) was a Canadian novelist, playwright, critic, journalist, and professor. He was one of Canada's best known and most popular authors, as well as one of its most distinguished "men of letters", a term Davies is variously said to have gladly accepted for himself and to have detested. Davies was the founding Master of Massey College, a graduate residential college associated with the University of Toronto.
Biography.
Early life.
Robertson Davies was born in Thamesville, Ontario, the third son of William Rupert Davies and Florence Sheppard McKay. Growing up, Davies was surrounded by books and lively language. His father, Senator William Rupert Davies, was a newspaperman from Welshpool, Wales; and both parents were voracious readers. He, similarly, read everything he could. He also participated in theatrical productions as a child, when he developed a lifelong interest in drama.
He spent his formative years in Renfrew, Ontario (Blairlogie in his "What's Bred in the Bone"); many of his novel's characters are named after families he knew there. He attended Upper Canada College in Toronto from 1926 to 1932 and while there attended services at the Church of St. Mary Magdalene. He would later leave the Presbyterian Church and join Anglicanism over objections to Calvinist theology. Davies later used his experience of the ceremonial of High Mass at St Mary Magdalene's in his novel "The Cunning Man".
After Upper Canada College, he studied at Queen's University at Kingston, Ontario from 1932 until 1935. At Queen's, he was enrolled as a special student not working towards a degree, and wrote for the student paper, "The Queen's Journal". He left Canada to study at Balliol College, Oxford, where he received a BLitt degree in 1938. The next year he published his thesis, "Shakespeare's Boy Actors", and embarked on an acting career outside London. In 1940, he played small roles and did literary work for the director at the Old Vic Repertory Company in London. Also that year, Davies married Australian Brenda Mathews, whom he had met at Oxford, and who was then working as stage manager for the theatre. They spent their honeymoon in the Welsh countryside at Fronfraith Hall, Abermule, Montgomery; the family house owned by Rupert Davies.
Davies' early life provided him with themes and material to which he would often return in his later work, including the theme of Canadians returning to England to finish their education, and the theatre.
Middle years.
Davies and his new bride returned to Canada in 1940, where he affected an upper-class English accent and took the position of literary editor at "Saturday Night" magazine. Two years later, he became editor of the "Peterborough Examiner" in the small city of Peterborough, Ontario, northeast of Toronto. Again he was able to mine his experiences here for many of the characters and situations which later appeared in his novels and plays.
Davies, along with family members William Rupert Davies and Arthur Davies, purchased several media outlets. Along with the "Examiner" newspaper, they owned the "Kingston Whig-Standard" newspaper, CHEX-AM, CKWS-AM, CHEX-TV, and CKWS-TV.
During his tenure as editor of the "Examiner", which lasted from 1942 to 1955 (he subsequently served as publisher from 1955 to 1965), Davies published a total of 18 books, produced several of his own plays, and wrote articles for various journals. For example, Davies set out his theory of acting in his "Shakespeare for Young Players" (1947), and then put theory into practice when he wrote "Eros at Breakfast", a one-act play which was named best Canadian play of the year by the 1948 Dominion Drama Festival.
"Eros at Breakfast" was followed in close succession by "Fortune, My Foe" in 1949 and "At My Heart's Core", a three-act play, in 1950. Meanwhile, Davies was writing humorous essays in the "Examiner" under the pseudonym Samuel Marchbanks. Some of these were collected and published in "The Diary of Samuel Marchbanks" (1947), "The Table Talk of Samuel Marchbanks" (1949), and later in "Samuel Marchbanks' Almanack" (1967). An omnibus edition of the three Marchbanks books, with new notes by the author, was published under the title "The Papers of Samuel Marchbanks" in 1985.
During the 1950s, Davies played a major role in launching the Stratford Shakespearean Festival of Canada. He served on the Festival's board of governors, and collaborated with the Festival's director, Sir Tyrone Guthrie, in publishing three books about the Festival's early years.
Although his first love was drama and he had achieved some success with his occasional humorous essays, Davies found his greatest success in fiction. His first three novels, which later became known as The Salterton Trilogy, were "Tempest-Tost" (1951, originally drafted as a play), "Leaven of Malice" (1954) which won the Stephen Leacock Award for Humour, and "A Mixture of Frailties" (1958). These novels explored the difficulty of sustaining a cultural life in Canada, and life on a small-town newspaper, subjects of which Davies had first-hand knowledge.
1960s.
In 1963, he became the Master of Massey College, the University of Toronto's new graduate college. During his stint as Master, he initiated a tradition of writing and telling ghost stories at the yearly Christmas celebrations. His stories were later collected in the book, "High Spirits" (1982).
1970s.
Davies drew on his interest in Jungian psychology to create "Fifth Business" (1970), a novel that relies heavily on Davies' own experiences, his love of myth and magic and his knowledge of small-town mores. The narrator, like Davies, is of immigrant Canadian background, with a father who runs the town paper. The book's characters act in roles that roughly correspond to Jungian archetypes according to Davies' belief in the predominance of spirit over the things of the world. 
Davies built on the success of "Fifth Business" with two more novels: "The Manticore" (1972), a novel cast largely in the form of a Jungian analysis (for which he received that year's Governor General's Literary Award), and "World of Wonders" (1975). Together these three books came to be known as The Deptford Trilogy.
1980s and 1990s.
When Davies retired from his position at the university, his seventh novel, a satire of academic life, "The Rebel Angels" (1981), was published, followed by "What's Bred in the Bone" (1985) which was short-listed for the Booker Prize for fiction in 1986. "The Lyre of Orpheus" (1988) follows these two books in what became known as The Cornish Trilogy.
During his retirement from academe he continued to write novels which further established him as a major figure in the literary world: "Murther and Walking Spirits" (1991) and "The Cunning Man" (1994). A third novel in what would have been a further trilogy — the Toronto Trilogy — was in progress at the time of Davies' death. He also realized a long-held dream when he penned the libretto to an opera: "The Golden Ass", based on "The Metamorphoses" of Lucius Apuleius, just like that written by one of the characters in Davies' 1958 "A Mixture of Frailties". The opera was performed by the Canadian Opera Company at the Hummingbird Centre in Toronto, in April, 1999, several years after Davies' death.
Davies was a fine public speaker—deft, often humorous, and unafraid to be unfashionable. Often asked if he used a computer, Davies said in 1987: "I don't want a word-processor. I process my own words. Helpful people assure me that a word-processor would save me a great deal of time. But I don't want to save time. I want to write the best book I can, and I have whatever time it takes to make that attempt." In its obituary, The Times wrote: 'Davies encompassed all the great elements of life...His novels combined deep seriousness and psychological inquiry with fantasy and exuberant mirth.' He remained close friends with John Kenneth Galbraith, attending Galbraith's eighty-fifth birthday party in Boston in 1993, and became so close a friend and colleague of the American novelist John Irving that Irving gave one of the scripture readings at Davies' funeral in Trinity College, Toronto chapel. He also wrote in support of Salman Rushdie when the latter was threatened by a "fatwā" from Ayatollah Ruhollah Khomeini of Iran in reaction to supposed anti-Islam expression in his novel "The Satanic Verses".
Works.
Essays.
Fictional essays
edited by the author into:
Criticism

</doc>
<doc id="25758" url="https://en.wikipedia.org/wiki?curid=25758" title="RNA">
RNA

Ribonucleic acid (RNA) is a polymeric molecule implicated in various biological roles in coding, decoding, regulation, and expression of genes. RNA and DNA are nucleic acids, and, along with proteins and carbohydrates, constitute the three major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA it is more often found in nature as a single-strand folded onto itself, rather than a paired double-strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the letters G, U, A, and C to denote the nitrogenous bases guanine, uracil, adenine, and cytosine) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.
Some RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function wherein mRNA molecules direct the assembly of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form proteins.
Comparison with DNA.
The chemical structure of RNA is very similar to that of DNA, but differs in three main ways:
Like DNA, most biologically active RNAs, including mRNA, tRNA, rRNA, snRNAs, and other non-coding RNAs, contain self-complementary sequences that allow parts of the RNA to fold and pair with itself to form double helices. Analysis of these RNAs has revealed that they are highly structured. Unlike DNA, their structures do not consist of long double helices, but rather collections of short helices packed together into structures akin to proteins.
In this fashion, RNAs can achieve chemical catalysis, like enzymes. For instance, determination of the structure of the ribosome—an enzyme that catalyzes peptide bond formation—revealed that its active site is composed entirely of RNA.
Structure.
Each nucleotide in RNA contains a ribose sugar, with carbons numbered 1' through 5'. A base is attached to the 1' position, in general, adenine (A), cytosine (C), guanine (G), or uracil (U). Adenine and guanine are purines, cytosine and uracil are pyrimidines. A phosphate group is attached to the 3' position of one ribose and the 5' position of the next. The phosphate groups do not have a negative charge each at physiological pH, making RNA a charged molecule (polyanion). The bases form hydrogen bonds between cytosine and guanine, between adenine and uracil and between guanine and uracil. However, other interactions are possible, such as a group of adenine bases binding to each other in a bulge,
or the GNRA tetraloop that has a guanine–adenine base-pair.
An important structural feature of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2' position of the ribose sugar. The presence of this functional group causes the helix to mostly adopt the A-form geometry, although in single strand dinucleotide contexts, RNA can rarely also adopt the B-form most commonly observed in DNA. The A-form geometry results in a very deep and narrow major groove and a shallow and wide minor groove. A second consequence of the presence of the 2'-hydroxyl group is that in conformationally flexible regions of an RNA molecule (that is, not involved in formation of a double helix), it can chemically attack the adjacent phosphodiester bond to cleave the backbone.
RNA is transcribed with only four bases (adenine, cytosine, guanine and uracil), but these bases and attached sugars can be modified in numerous ways as the RNAs mature. Pseudouridine (Ψ), in which the linkage between uracil and ribose is changed from a C–N bond to a C–C bond, and ribothymidine (T) are found in various places (the most notable ones being in the TΨC loop of tRNA). Another notable modified base is hypoxanthine, a deaminated adenine base whose nucleoside is called inosine (I). Inosine plays a key role in the wobble hypothesis of the genetic code.
There are more than 100 other naturally occurring modified nucleosides, The greatest structural diversity of modifications can be found in tRNA, while pseudouridine and nucleosides with 2'-O-methylribose often present in rRNA are the most common. The specific roles of many of these modifications in RNA are not fully understood. However, it is notable that, in ribosomal RNA, many of the post-transcriptional modifications occur in highly functional regions, such as the peptidyl transferase center and the subunit interface, implying that they are important for normal function.
The functional form of single-stranded RNA molecules, just like proteins, frequently requires a specific tertiary structure. The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule. This leads to several recognizable "domains" of secondary structure like hairpin loops, bulges, and internal loops. Since RNA is charged, metal ions such as Mg2+ are needed to stabilise many secondary and tertiary structures.
The naturally occurring enantiomer of RNA is D-RNA composed of D-ribonucleotides. All chirality centers are located in the D-ribose. By the use of L-ribose or rather L-ribonucleotides, L-RNA can be synthesized. L-RNA is much more stable against degradation by RNase.
Like other structured biopolymers such as proteins, one can define topology of a folded RNA molecule. This is often done based on arrangement of intra-chain contacts within a folded RNA, termed as circuit topology.
Synthesis.
Synthesis of RNA is usually catalyzed by an enzyme—RNA polymerase—using DNA as a template, a process known as transcription. Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA (usually found "upstream" of a gene). The DNA double helix is unwound by the helicase activity of the enzyme. The enzyme then progresses along the template strand in the 3’ to 5’ direction, synthesizing a complementary RNA molecule with elongation occurring in the 5’ to 3’ direction. The DNA sequence also dictates where termination of RNA synthesis will occur.
Primary transcript RNAs are often modified by enzymes after transcription. For example, a poly(A) tail and a 5' cap are added to eukaryotic pre-mRNA and introns are removed by the spliceosome.
There are also a number of RNA-dependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA. For instance, a number of RNA viruses (such as poliovirus) use this type of enzyme to replicate their genetic material. Also, RNA-dependent RNA polymerase is part of the RNA interference pathway in many organisms.
Types of RNA.
Overview.
Messenger RNA (mRNA) is the RNA that carries information from DNA to the ribosome, the sites of protein synthesis (translation) in the cell. The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced. However, many RNAs do not code for protein (about 97% of the transcriptional output is non-protein-coding in eukaryotes).
These so-called non-coding RNAs ("ncRNA") can be encoded by their own genes (RNA genes), but can also derive from mRNA introns. The most prominent examples of non-coding RNAs are transfer RNA (tRNA) and ribosomal RNA (rRNA), both of which are involved in the process of translation. There are also non-coding RNAs involved in gene regulation, RNA processing and other roles. Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules, and the catalysis of peptide bond formation in the ribosome; these are known as ribozymes.
In translation.
Messenger RNA (mRNA) carries information about a protein sequence to the ribosomes, the protein synthesis factories in the cell. It is coded so that every three nucleotides (a codon) correspond to one amino acid. In eukaryotic cells, once precursor mRNA (pre-mRNA) has been transcribed from DNA, it is processed to mature mRNA. This removes its introns—non-coding sections of the pre-mRNA. The mRNA is then exported from the nucleus to the cytoplasm, where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA. In prokaryotic cells, which do not have nucleus and cytoplasm compartments, mRNA can bind to ribosomes while it is being transcribed from DNA. After a certain amount of time the message degrades into its component nucleotides with the assistance of ribonucleases.
Transfer RNA (tRNA) is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation. It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding.
Ribosomal RNA (rRNA) is the catalytic component of the ribosomes. Eukaryotic ribosomes contain four different rRNA molecules: 18S, 5.8S, 28S and 5S rRNA. Three of the rRNA molecules are synthesized in the nucleolus, and one is synthesized elsewhere. In the cytoplasm, ribosomal RNA and protein combine to form a nucleoprotein called a ribosome. The ribosome binds mRNA and carries out protein synthesis. Several ribosomes may be attached to a single mRNA at any time. Nearly all the RNA found in a typical eukaryotic cell is rRNA.
Transfer-messenger RNA (tmRNA) is found in many bacteria and plastids. It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling.
Regulatory RNAs.
Several types of RNA can downregulate gene expression by being complementary to a part of an mRNA or a gene's DNA. MicroRNAs (miRNA; 21-22 nt) are found in eukaryotes and act through RNA interference (RNAi), where an effector complex of miRNA and enzymes can cleave complementary mRNA, block the mRNA from being translated, or accelerate its degradation.
While small interfering RNAs (siRNA; 20-25 nt) are often produced by breakdown of viral RNA, there are also endogenous sources of siRNAs. siRNAs act through RNA interference in a fashion similar to miRNAs. Some miRNAs and siRNAs can cause genes they target to be methylated, thereby decreasing or increasing transcription of those genes. Animals have Piwi-interacting RNAs (piRNA; 29-30 nt) that are active in germline cells and are thought to be a defense against transposons and play a role in gametogenesis.
Many prokaryotes have CRISPR RNAs, a regulatory system similar to RNA interference. Antisense RNAs are widespread; most downregulate a gene, but a few are activators of transcription. One way antisense RNA can act is by binding to an mRNA, forming double-stranded RNA that is enzymatically degraded. There are many long noncoding RNAs that regulate genes in eukaryotes, one such RNA is Xist, which coats one X chromosome in female mammals and inactivates it.
An mRNA may contain regulatory elements itself, such as riboswitches, in the 5' untranslated region or 3' untranslated region; these cis-regulatory elements regulate the activity of that mRNA. The untranslated regions can also contain elements that regulate other genes.
In RNA processing.
Many RNAs are involved in modifying other RNAs.
Introns are spliced out of pre-mRNA by spliceosomes, which contain several small nuclear RNAs (snRNA), or the introns can be ribozymes that are spliced by themselves.
RNA can also be altered by having its nucleotides modified to other nucleotides than A, C, G and U.
In eukaryotes, modifications of RNA nucleotides are in general directed by small nucleolar RNAs (snoRNA; 60-300 nt), found in the nucleolus and cajal bodies. snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA. These enzymes then perform the nucleotide modification. rRNAs and tRNAs are extensively modified, but snRNAs and mRNAs can also be the target of base modification. RNA can also be methylated.
RNA genomes.
Like DNA, RNA can carry genetic information. RNA viruses have genomes composed of RNA that encodes a number of proteins. The viral genome is replicated by some of those proteins, while other proteins protect the genome as the virus particle moves to a new host cell. Viroids are another group of pathogens, but they consist only of RNA, do not encode any protein and are replicated by a host plant cell's polymerase.
In reverse transcription.
Reverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA; these DNA copies are then transcribed to new RNA. Retrotransposons also spread by copying DNA and RNA from one another, and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes.
Double-stranded RNA.
Double-stranded RNA (dsRNA) is RNA with two complementary strands, similar to the DNA found in all cells. dsRNA forms the genetic material of some viruses (double-stranded RNA viruses). Double-stranded RNA such as viral RNA or siRNA can trigger RNA interference in eukaryotes, as well as interferon response in vertebrates.
Key discoveries in RNA biology.
Research on RNA has led to many important biological discoveries and numerous Nobel Prizes. Nucleic acids were discovered in 1868 by Friedrich Miescher, who called the material 'nuclein' since it was found in the nucleus. It was later discovered that prokaryotic cells, which do not have a nucleus, also contain nucleic acids. The role of RNA in protein synthesis was suspected already in 1939. Severo Ochoa won the 1959 Nobel Prize in Medicine (shared with Arthur Kornberg) after he discovered an enzyme that can synthesize RNA in the laboratory. However, the enzyme discovered by Ochoa (polynucleotide phosphorylase) was later shown to be responsible for RNA degradation, not RNA synthesis. In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by X-ray crystallography.
The sequence of the 77 nucleotides of a yeast tRNA was found by Robert W. Holley in 1965, winning Holley the 1968 Nobel Prize in Medicine (shared with Har Gobind Khorana and Marshall Nirenberg).
In 1967, Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life (self-replicating molecules) could have relied on RNA both to carry genetic information and to catalyze biochemical reactions—an RNA world.
During the early 1970s, retroviruses and reverse transcriptase were discovered, showing for the first time that enzymes could copy RNA into DNA (the opposite of the usual route for transmission of genetic information). For this work, David Baltimore, Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975.
In 1976, Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome, that of bacteriophage MS2.
In 1977, introns and RNA splicing were discovered in both mammalian viruses and in cellular genes, resulting in a 1993 Nobel to Philip Sharp and Richard Roberts.
Catalytic RNA molecules (ribozymes) were discovered in the early 1980s, leading to a 1989 Nobel award to Thomas Cech and Sidney Altman. In 1990, it was found in "Petunia" that introduced genes can silence similar genes of the plant's own, now known to be a result of RNA interference.
At about the same time, 22 nt long RNAs, now called microRNAs, were found to have a role in the development of "C. elegans".
Studies on RNA interference gleaned a Nobel Prize for Andrew Fire and Craig Mello in 2006, and another Nobel was awarded for studies on the transcription of RNA to Roger Kornberg in the same year. The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA, such as siRNA, to silence genes.
Evolution.
In March 2015, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, were reportedly formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

</doc>
<doc id="25762" url="https://en.wikipedia.org/wiki?curid=25762" title="Russian Revolution">
Russian Revolution

"Russian Revolution" is the collective term for a pair of revolutions in Russia in 1917, which dismantled the Tsarist autocracy and led to the eventual rise of the Soviet Union. The Russian Empire collapsed with the abdication of Emperor Nicholas II, and the old regime was replaced by a provisional government during the first revolution of February 1917 (March in the Gregorian calendar; the older Julian calendar was in use in Russia at the time). In the second revolution that October, the Provisional Government was removed and replaced with a Bolshevik (Communist) government.
The February Revolution (March 1917) was a revolution focused around Petrograd (now Saint Petersburg), then capital of Russia. In the chaos, members of the Imperial parliament or Duma assumed control of the country, forming the Russian Provisional Government. The army leadership felt they did not have the means to suppress the revolution, resulting in Nicholas' abdication. The Soviets (workers' councils), which were led by more radical socialist factions, initially permitted the Provisional Government to rule, but insisted on a prerogative to influence the government and control various militias. The February Revolution took place in the context of heavy military setbacks during the First World War (1914–18), which left much of the Russian army in a state of mutiny.
A period of dual power ensued, during which the Provisional Government held state power while the national network of Soviets, led by socialists, had the allegiance of the lower classes and the political left. During this chaotic period there were frequent mutinies, protests and many strikes. When the Provisional Government chose to continue fighting the war with Germany, the Bolsheviks and other socialist factions campaigned for stopping the conflict. The Bolsheviks turned workers militias under their control into the Red Guards (later the Red Army) over which they exerted substantial control.
In the October Revolution (November in the Gregorian calendar), the Bolshevik party, led by Vladimir Lenin, and the workers' Soviets overthrew the Provisional Government in Petrograd and established the Russian SFSR, eventually shifting the capital to Moscow in 1918. The Bolsheviks appointed themselves as leaders of various government ministries and seized control of the countryside, establishing the Cheka to quash dissent. To end Russia’s participation in the First World War, the Bolshevik leaders signed the Treaty of Brest-Litovsk with Germany in March 1918.
Civil war erupted among the "Reds" (Bolsheviks), the "Whites" (anti-socialist factions), and non-Bolshevik socialists. It continued for several years, during which the Bolsheviks defeated both the Whites and all rival socialists. In this way, the Revolution paved the way for the creation of the Union of Soviet Socialist Republics (USSR) in 1922. While many notable historical events occurred in Moscow and Petrograd, there was also a visible movement in cities throughout the state, among national minorities throughout the empire and in the rural areas, where peasants took over and redistributed land.
Background.
The Russian Revolution of 1905 was said to be a major factor to the February Revolutions of 1917. The events of Bloody Sunday triggered a line of protests. A council of workers called the St. Petersburg Soviet was created in all this chaos, and the beginning of a communist political protest had begun.
World War I prompted a Russian outcry directed at Tsar Nicholas II. It was another major factor contributing to the retaliation of the Russian Communists against their royal opponents. After the entry of the Ottoman Empire on the side of the Central Powers in October 1914, Russia was deprived of a major trade route through Ottoman Empire, which followed with a minor economic crisis, in which Russia became incapable of providing munitions to their army in the years leading to 1917. However, the problems were merely administrative, and not industrial as Germany was producing great amounts of munitions whilst constantly fighting on two major battlefronts.
The war also developed a weariness in the city, owing to a lack of food in response to the disruption of agriculture. Food scarcity had become a considerable problem in Russia, but the cause of this did not lie in any failure of the harvests, which had not been significantly altered during war-time. The indirect reason was that the government, in order to finance the war, had been printing millions of ruble notes, and by 1917 inflation had made prices increase up to four times what they had been in 1914. The peasantry were consequently faced with the higher cost of purchases, but made no corresponding gain in the sale of their own produce, since this was largely taken by the middlemen on whom they depended. As a result, they tended to hoard their grain and to revert to subsistence farming. Thus the cities were constantly short of food. At the same time rising prices led to demands for higher wages in the factories, and in January and February 1916 revolutionary propaganda, aided by German funds, led to widespread strikes. The outcome of all this, however, was a growing criticism of the government rather than any war-weariness. The original fever of patriotic excitement, which had caused the name of St. Petersburg to be changed to the less German sounding "Petrograd," may have subsided a little in the subsequent years, but it had not turned to defeatism and during the initial risings in Petrograd in February 1917, the crowds in the streets clearly objected to the banners proclaiming "down with the war." Heavy losses during the war also strengthened thoughts that Tsar Nicholas II was unfit to rule.
The Liberals were now better placed to voice their complaints, since they were participating more fully through a variety of voluntary organizations. Local industrial committees proliferated. In July 1915, a Central War Industries Committee was established under the chairmanship of a prominent Octobrist, Guchkov, and including ten workers' representatives. The Petrograd Mensheviks agreed to join despite the objections of their leaders abroad. All this activity gave renewed encouragement to political ambitions, and, in September 1915, a combination of Octobrists and Kadets in the Duma demanded the forming of a responsible government. The Tsar rejected these proposals. He had now taken over the position of commander-in-chief of the armed forces and, during his absence from Petrograd while at his military headquarters at Mogilev, he left most of the day-to-day government in the hands of the Empress. She was intensely unpopular, owing, in part, to her German origin and to the influence that Rasputin, an unsavoury "monk", exercised over her.
All these factors had given rise to a sharp loss of confidence in the regime by 1916. Early in that year, Guchkov had been taking soundings among senior army officers and members of the Central War Industries Committee about a possible coup to force the abdication of the Tsar. In November, Pavel Milyukov in the Duma openly accused the government of contemplating peace negotiations with Germany. In December, a small group of nobles assassinated Rasputin, and in January 1917 the Tsar's uncle, Grand Duke Nicholas, was asked indirectly by Prince Lvov whether he would be prepared to take over the throne from his nephew, Tsar Nicholas II. None of these incidents were in themselves the immediate cause of the February Revolution, but they do help to explain why the monarchy survived only a few days after it had broken out.
Meanwhile, the Social Democrat leaders in exile, now mostly in Switzerland, had been the glum spectators of the collapse of international socialist solidarity. French and German Social Democrats had voted in favour of their respective governments. Georgi Plekhanov in Paris had adopted a violently anti-German stand, while Parvus supported the German war effort as the best means of ensuring a revolution in Russia. The Mensheviks largely maintained that Russia had the right to defend herself against Germany, although Martov (a prominent Menshevik), now on the left of his group, demanded an end to the war and a settlement on the basis of national self-determination, with no annexations or indemnities.
It was these views of Martov that predominated in a manifesto drawn up by Leon Trotsky (at the time a Menshevik) at a conference in Zimmerwald, attended by 35 Socialist leaders in September 1915. Inevitably Vladimir Lenin, supported by Zinoviev and Radek, strongly contested them. Their attitudes became known as the Zimmerwald Left. Lenin rejected both the defence of Russia and the cry for peace. Since the autumn of 1914, he had insisted that "from the standpoint of the working class and of the labouring masses from the lesser evil would be the defeat of the Tsarist Monarchy"; the war must be turned into a civil war of the proletarian soldiers against their own governments, and if a proletarian victory should emerge from this in Russia, then their duty would be to wage a revolutionary war for the liberation of the masses throughout Europe. Thus, Lenin remained the "enfant terrible" of the Russian Social Democratic Labour Party, although at this point in the war his following in Russia was as few as 10,000 and he must have seemed no more than the leader of an extremist wing of a bankrupt organization. Lenin then executed the protests of Petrograd which set off the 1917 Russian Revolution.
Economic and social changes.
An elementary theory of property, believed by many peasants, was that land should belong to those who work on it. At the same time, peasant life and culture was changing constantly. Change was facilitated by the physical movement of growing numbers of peasant villagers who migrated to and from industrial and urban environments, but also by the introduction of city culture into the village through material goods, the press, and word of mouth.
Workers also had good reasons for discontent: overcrowded housing with often deplorable sanitary conditions, long hours at work (on the eve of the war a 10-hour workday six days a week was the average and many were working 11–12 hours a day by 1916), constant risk of injury and death from poor safety and sanitary conditions, harsh discipline (not only rules and fines, but foremen’s fists), and inadequate wages (made worse after 1914 by steep war-time increases in the cost of living). At the same time, urban industrial life was full of benefits, though these could be just as dangerous, from the point of view of social and political stability, as the hardships. There were many encouragements to expect more from life. Acquiring new skills gave many workers a sense of self-respect and confidence, heightening expectations and desires. Living in cities, workers encountered material goods such as they had never seen in villages. Most important, living in cities, they were exposed to new ideas about the social and political order.
The social causes of the Russian Revolution mainly came from centuries of oppression of the lower classes by the Tsarist regime, and Nicholas's failures in World War I. While rural agrarian peasants had been emancipated from serfdom in 1861, they still resented paying redemption payments to the state, and demanded communal tender of the land they worked. The problem was further compounded by the failure of Sergei Witte's land reforms of the early 20th century. Increasing peasant disturbances and sometimes actual revolts occurred, with the goal of securing ownership of the land they worked. Russia consisted mainly of poor farming peasants, with 1.5% of the population owning 25% of the land.
The rapid industrialization of Russia also resulted in urban overcrowding and poor conditions for urban industrial workers (as mentioned above). Between 1890 and 1910, the population of the capital, Saint Petersburg, swelled from 1,033,600 to 1,905,600, with Moscow experiencing similar growth. This created a new 'proletariat' which, due to being crowded together in the cities, was much more likely to protest and go on strike than the peasantry had been in previous times. In one 1904 survey, it was found that an average of sixteen people shared each apartment in Saint Petersburg, with six people per room. There was also no running water, and piles of human waste were a threat to the health of the workers. The poor conditions only aggravated the situation, with the number of strikes and incidents of public disorder rapidly increasing in the years shortly before World War I. Because of late industrialization, Russia's workers were highly concentrated. By 1914 40% of Russian workers were employed in factories of +1,000 workers (32% in 1901). 42% worked in 100–1,000 worker enterprises, 18% in 1–100 worker businesses (in the USA, 1914, the figures were 18, 47 and 35 respectively).
World War I added to the chaos. Conscription swept up the unwilling across Russia. The vast demand for factory production of war supplies and workers caused many more labor riots and strikes. Conscription stripped skilled workers from the cities, who had to be replaced with unskilled peasants, and then, when famine began to hit due to the poor railway system, workers abandoned the cities in droves seeking food. Finally, the soldiers themselves, who suffered from a lack of equipment and protection from the elements, began to turn against the Tsar. This was mainly because, as the war progressed, many of the officers who were loyal to the Tsar were killed, and were replaced by discontented conscripts from the major cities, who had little loyalty to the Tsar.
Political issues.
Many sections of the country had reason to be dissatisfied with the existing autocracy. Nicholas II was a deeply conservative ruler and maintained a strict authoritarian system. Individuals and society in general were expected to show self-restraint, devotion to community, deference to the social hierarchy and a sense of duty to the country. Religious faith helped bind all of these tenets together as a source of comfort and reassurance in the face of difficult conditions and as a means of political authority exercised through the clergy. Perhaps more than any other modern monarch, Nicholas II attached his fate and the future of his dynasty to the notion of the ruler as a saintly and infallible father to his people.
This idealized vision of the Romanov monarchy blinded him to the actual state of his country. With a firm belief that his power to rule was granted by Divine Right, Nicholas assumed that the Russian people were devoted to him with unquestioning loyalty. This ironclad belief rendered Nicholas unwilling to allow the progressive reforms that might have alleviated the suffering of the Russian people. Even after the 1905 revolution spurred the Tsar to decree limited civil rights and democratic representation, he worked to limit even these liberties in order to preserve the ultimate authority of the crown.
Despite constant oppression, the desire of the people for democratic participation in government decisions was strong. Since the Age of Enlightenment, Russian intellectuals had promoted Enlightenment ideals such as the dignity of the individual and the rectitude of democratic representation. These ideals were championed most vociferously by Russia’s liberals, although populists, Marxists, and anarchists also claimed to support democratic reforms. A growing opposition movement had begun to challenge the Romanov monarchy openly well before the turmoil of World War I.
Dissatisfaction with Russian autocracy culminated in the huge national upheaval that followed the Bloody Sunday massacre of January 1905, in which hundreds of unarmed protesters were shot by the Tsar's troops. Workers responded to the massacre with a crippling general strike, forcing Nicholas to put forth the October Manifesto, which established a democratically elected parliament (the State Duma). The Tsar undermined this promise of reform but a year later with Article 87 of the 1906 Fundamental State Laws, and subsequently dismissed the first two Dumas when they proved uncooperative. Unfulfilled hopes of democracy fueled revolutionary ideas and violent outbursts targeted at the monarchy.
One of the Tsar’s principal rationales for risking war in 1914 was his desire to restore the prestige that Russia had lost amid the debacles of the Russo-Japanese war. Nicholas also sought to foster a greater sense of national unity with a war against a common and ancient enemy. The Russian Empire was an agglomeration of diverse ethnicities that had shown significant signs of disunity in the years before the First World War. Nicholas believed in part that the shared peril and tribulation of a foreign war would mitigate the social unrest over the persistent issues of poverty, inequality, and inhuman working conditions. Instead of restoring Russia's political and military standing, World War I led to the horrifying slaughter of Russian troops and military defeats that undermined both the monarchy and society in general to the point of collapse.
World War I.
The outbreak of war in August 1914 initially served to quiet the prevalent social and political protests, focusing hostilities against a common external enemy, but this patriotic unity did not last long. As the war dragged on inconclusively, war-weariness gradually took its toll. More important, though, was a deeper fragility: although many ordinary Russians joined anti-German demonstrations in the first few weeks of the war, the most widespread reaction appears to have been skepticism and fatalism. Hostility toward the Kaiser and the desire to defend their land and their lives did not necessarily translate into enthusiasm for the Tsar or the government.
Russia's first major battle of the war was a disaster: in the 1914 Battle of Tannenberg, over 30,000 Russian troops were killed or wounded and 90,000 captured, while Germany suffered just 12,000 casualties. However, Austro-Hungarian forces allied to Germany were driven back deep into the Galicia region by the end of the year. In the autumn of 1915, Nicholas had taken direct command of the army, personally overseeing Russia's main theatre of war and leaving his ambitious but incapable wife Alexandra in charge of the government. Reports of corruption and incompetence in the Imperial government began to emerge, and the growing influence of Grigori Rasputin in the Imperial family was widely resented. In the eyes of Lynch, a revisionist historian who focuses on the role of the people, Rasputin was a "fatal disease" to the Tsarist regime.
In 1915, things took a critical turn for the worse when Germany shifted its focus of attack to the Eastern front. The superior German army – better led, better trained and better supplied – was terrifyingly effective against the ill-equipped Russian forces, driving the Russians out of Galicia, as well as Russian Poland, during the Gorlice–Tarnów Offensive campaign. By the end of October 1916, Russia had lost between 1,600,000 and 1,800,000 soldiers, with an additional 2,000,000 prisoners of war and 1,000,000 missing, all making up a total of nearly 5,000,000 men.
These staggering losses played a definite role in the mutinies and revolts that began to occur. In 1916, reports of fraternizing with the enemy started to circulate. Soldiers went hungry, and lacked shoes, munitions, and even weapons. Rampant discontent lowered morale, which was further undermined by a series of military defeats.
Casualty rates were the most vivid sign of this disaster. Already, by the end of 1914, only five months into the war, around 390,000 Russian men had lost their lives and nearly 1,000,000 were injured. Far sooner than expected, barely trained recruits had to be called up for active duty, a process repeated throughout the war as staggering losses continued to mount. The officer class also saw remarkable changes, especially within the lower echelons, which were quickly filled with soldiers rising up through the ranks. These men, usually of peasant or working-class backgrounds, were to play a large role in the politicization of the troops in 1917.
The huge losses on the battlefields were not limited to men. The army quickly ran short of rifles and ammunition (as well as uniforms and food), and, by mid-1915, men were being sent to the front bearing no arms. It was hoped that they could equip themselves with the arms that they recovered from fallen soldiers, of both sides, on the battlefields. With good reason, the soldiers did not feel that they were being treated as human beings, or even as valuable soldiers, but rather as raw materials to be squandered for the purposes of the rich and powerful.
By the spring of 1915, the army was in steady retreat, which was not always orderly; desertion, plunder and chaotic flight were not uncommon. By 1916, however, the situation had improved in many respects. Russian troops stopped retreating, and there were even some modest successes in the offensives that were staged that year, albeit at great loss of life. Also, the problem of shortages was largely solved by a major effort to increase domestic production. Nevertheless, by the end of 1916, morale among soldiers was even worse than it had been during the great retreat of 1915. The fortunes of war may have improved, but the fact of the war, still draining away strength and lives from the country and its many individuals and families, remained an oppressive inevitability. The crisis in morale (as was argued by Allan Wildman, a leading historian of the Russian army in war and revolution) "was rooted fundamentally in the feeling of utter despair that the slaughter would ever end and that anything resembling victory could be achieved."
The war devastated not only soldiers. By the end of 1915, there were manifold signs that the economy was breaking down under the heightened strain of wartime demand. The main problems were food shortages and rising prices. Inflation dragged incomes down at an alarmingly rapid rate, and shortages made it difficult to buy even what one could afford. These shortages were a problem especially in the capital, St. Petersburg, where distance from supplies and poor transportation networks made matters particularly bad. Shops closed early or entirely for lack of bread, sugar, meat and other provisions, and lines lengthened massively for what remained. It became increasingly difficult both to afford and actually buy food.
Not surprisingly, strikes increased steadily from the middle of 1915, and so did crime; but, for the most part, people suffered and endured, scouring the city for food. Working class women in St. Petersburg reportedly spent about forty hours a week in food lines, begging, turning to prostitution or crime, tearing down wooden fences to keep stoves heated for warmth, grumbling about the rich, and wondering when and how this would all come to an end.
Government officials responsible for public order worried about how long people's patience would last. A report by the St. Petersburg branch of the security police, the Okhrana, in October 1916, warned bluntly of "the possibility in the near future of riots by the lower classes of the empire enraged by the burdens of daily existence."
Nicholas was blamed for all of these crises, and what little support he had left began to crumble. As discontent grew, the State Duma issued a warning to Nicholas in November 1916. It stated that, inevitably, a terrible disaster would grip the country unless a constitutional form of government was put in place. In typical fashion, however, Nicholas ignored them, and Russia's Tsarist regime collapsed a few months later during the February Revolution of 1917. One year later, the Tsar and his entire family were executed. Ultimately, Nicholas's inept handling of his country and the war destroyed the Tsar and ended up costing him both his reign and his life.
February Revolution.
At the beginning of February, Petrograd workers began several strikes and demonstrations. On , workers at Putilov, Petrograd's largest industrial plant, announced a strike.
The next day, a series of meetings and rallies were held for International Women's Day, which gradually turned into economic and political gatherings. Demonstrations were organised to demand bread, and these were supported by the industrial working force who considered them a reason for continuing the strikes. The women workers marched to nearby factories bringing out over 50,000 workers on strike. By , virtually every industrial enterprise in Petrograd had been shut down, together with many commercial and service enterprises. Students, white-collar workers and teachers joined the workers in the streets and at public meetings.
To quell the riots, the Tsar looked to the army. At least 180,000 troops were available in the capital, but most were either untrained or injured. Historian Ian Beckett suggests around 12,000 could be regarded as reliable, but even these proved reluctant to move in on the crowd, since it included so many women. It was for this reason that when, on , the Tsar ordered the army to suppress the rioting by force, troops began to mutiny. Although few actively joined the rioting, many officers were either shot or went into hiding; the ability of the garrison to hold back the protests was all but nullified, symbols of the Tsarist regime were rapidly torn down around the city, and governmental authority in the capital collapsed – not helped by the fact that Nicholas had prorogued the Duma that morning, leaving it with no legal authority to act. The response of the Duma, urged on by the liberal bloc, was to establish a Temporary Committee to restore law and order; meanwhile, the socialist parties establish the Petrograd Soviet to represent workers and soldiers. The remaining loyal units switched allegiance the next day.
The Tsar took a train back towards Petrograd, which was stopped on , by a group of disloyal troops. When the Tsar finally reached his destination, the Army Chiefs and his remaining ministers (those who had not fled under pretense of a power-cut) suggested in unison that he abdicate the throne. He did so on , on behalf of himself, and then, having taken advice, on behalf of his son, the Tsarevich. Nicholas nominated his brother, the Grand Duke Michael Alexandrovich, to succeed him. But the Grand Duke realised that he would have little support as ruler, so he declined the crown on , stating that he would take it only if that was the consensus of democratic action. Six days later, Nicholas, no longer Tsar and addressed with contempt by the sentries as "Nicholas Romanov", was reunited with his family at the Alexander Palace at Tsarskoye Selo. He was placed under house arrest with his family by the Provisional Government.
The immediate effect of the February Revolution was a widespread atmosphere of elation and excitement in Petrograd. On , a provisional government was announced. The center-left was well represented, and the government was initially chaired by a liberal aristocrat, Prince Georgy Yevgenievich Lvov, a member of the Constitutional Democratic party (KD). The socialists had formed their rival body, the Petrograd Soviet (or workers' council) four days earlier. The Petrograd Soviet and the Provisional Government competed for power over Russia.
Between February and throughout October: "Dual Power" ("dvoevlastie").
The effective power of the Provisional Government was challenged by the authority of an institution that claimed to represent the will of workers and soldiers and could, in fact, mobilize and control these groups during the early months of the revolution – the Petrograd Soviet of Workers' Deputies. The model for the soviet were workers' councils that had been established in scores of Russian cities during the 1905 revolution. In February 1917, striking workers elected deputies to represent them and socialist activists began organizing a citywide council to unite these deputies with representatives of the socialist parties. On 27 February, socialist Duma deputies, mainly Mensheviks and Socialist Revolutionaries, took the lead in organizing a citywide council. The Petrograd Soviet met in the Tauride Palace, the same building where the new government was taking shape.
The leaders of the Petrograd Soviet believed that they represented particular classes of the population, not the whole nation. They also believed Russia was not ready for socialism. So they saw their role as limited to pressuring hesitant "bourgeoisie" to rule and to introduce extensive democratic reforms in Russia (the replacement of the monarchy by a republic, guaranteed civil rights, a democratic police and army, abolition of religious and ethnic discrimination, preparation of elections to a constituent assembly, and so on). They met in the same building as the emerging Provisional Government not to compete with the Duma Committee for state power but to best exert pressure on the new government, to act, in other words, as a popular democratic lobby.
The relationship between these two major powers was complex from the beginning and would shape the politics of 1917. The representatives of the Provisional Government agreed to "take into account the opinions of the Soviet of Workers' Deputies", though they were also determined to prevent "interference in the actions of the government", which would create "an unacceptable situation of dual power." In fact, this was precisely what was being created, though this "dual power" (dvoevlastie) was the result less of the actions or attitudes of the leaders of these two institutions than of actions outside their control, especially the ongoing social movement taking place on the streets of Russia’s cities, in factories and shops, in barracks and in the trenches, and in the villages.
A series of political crises – see the chronology below – in the relationship between population and government and between the Provisional Government and the soviets (which developed into a nationwide movement with a national leadership, The All-Russian Central Executive Committee of Soviets (VTsIK)) undermined the authority of the Provisional Government but also of the moderate socialist leaders of the Soviet. Although the Soviet leadership initially refused to participate in the "bourgeois" Provisional Government, Alexander Kerensky, a young and popular lawyer and a member of the Socialist Revolutionary Party (SRP), agreed to join the new cabinet, and became an increasingly central figure in the government, eventually taking leadership of the Provisional Government. As minister of war and later Prime Minister, Kerensky promoted freedom of speech, released thousands of political prisoners, did his very best to continue the war effort and even organised another offensive (which, however, was no more successful than its predecessors). Nevertheless, Kerensky still faced several great challenges, highlighted by the soldiers, urban workers and peasants, who claimed that they had gained nothing by the revolution:
The political group that proved most troublesome for Kerensky, and would eventually overthrow him, was the Bolshevik Party, led by Vladimir Lenin. Lenin had been living in exile in neutral Switzerland and, due to democratization of politics after the February Revolution, which legalized formerly banned political parties, he perceived the opportunity for his Marxist revolution. Although return to Russia had become a possibility, the war made it logistically difficult. Eventually, German officials arranged for Lenin to pass through their territory, hoping that his activities would weaken Russia or even – if the Bolsheviks came to power – lead to Russia's withdrawal from the war. Lenin and his associates, however, had to agree to travel to Russia in a sealed train: Germany would not take the chance that he would foment revolution in Germany. After passing through the front, he arrived in Petrograd in April 1917.
With Lenin's arrival, the popularity of the Bolsheviks increased steadily. Over the course of the spring, public dissatisfaction with the Provisional Government and the war, in particular among workers, soldiers and peasants, pushed these groups to radical parties. Despite growing support for the Bolsheviks, buoyed by maxims that called most famously for "all power to the Soviets," the party held very little real power in the moderate-dominated Petrograd Soviet. In fact, historians such as Sheila Fitzpatrick have asserted that Lenin's exhortations for the Soviet Council to take power were intended to arouse indignation both with the Provisional Government, whose policies were viewed as conservative, and the Soviet itself, which was viewed as subservient to the conservative government. By some historians' accounts, Lenin and his followers were unprepared for how their groundswell of support, especially among influential worker and soldier groups, would translate into real power in the summer of 1917.
On 18 June, the Provisional Government launched an attack against Germany that failed miserably. Soon after, the government ordered soldiers to go to the front, reneging on a promise. The soldiers refused to follow the new orders. The arrival of radical Kronstadt sailors – who had tried and executed many officers, including one admiral – further fueled the growing revolutionary atmosphere. The sailors and soldiers, along with Petrograd workers, took to the streets in violent protest, calling for "all power to the Soviets." The revolt, however, was disowned by Lenin and the Bolshevik leaders and dissipated within a few days. In the aftermath, Lenin fled to Finland under threat of arrest while Trotsky, among other prominent Bolsheviks, was arrested. The July Days confirmed the popularity of the anti-war, radical Bolsheviks, but their unpreparedness at the moment of revolt was an embarrassing gaffe that lost them support among their main constituent groups: soldiers and workers.
The Bolshevik failure in the July Days proved temporary. The Bolsheviks had undergone a spectacular growth in membership. Whereas, in February 1917, the Bolsheviks were limited to only 24,000 members, by September 1917 there were 200,000 members of the Bolshevik faction. Previously, the Bolsheviks had been in the minority in the two leading cities of Russia—St. Petersburg and Moscow behind the Mensheviks and the Socialist Revolutionaries, by September the Bolsheviks were in the majority in both cities. Furthermore, the Bolshevik-controlled Moscow Regional Bureau of the Party also controlled the Party organizations of the thirteen (13) provinces around Moscow. These thirteen provinces held 37% of Russia's population and 20% of the membership of the Bolshevik faction.
In August, poor or misleading communication led General Lavr Kornilov, the recently appointed Supreme Commander of Russian military forces, to believe that the Petrograd government had already been captured by radicals, or was in serious danger thereof. In response, he ordered troops to Petrograd to pacify the city. To secure his position, Kerensky had to ask for Bolshevik assistance. He also sought help from the Petrograd Soviet, which called upon armed Red Guards to "defend the revolution". The Kornilov Affair failed largely due to the efforts of the Bolsheviks, whose influence over railroad and telegraph workers proved vital in stopping the movement of troops. With his coup failing, Kornilov surrendered and was relieved of his position. The Bolsheviks' role in stopping the attempted coup further strengthened their position.
In early September, the Petrograd Soviet freed all jailed Bolsheviks and Trotsky became chairman of the Petrograd Soviet. Growing numbers of socialists and lower-class Russians viewed the government less and less as a force in support of their needs and interests. The Bolsheviks benefited as the only major organized opposition party that had refused to compromise with the Provisional Government, and they benefited from growing frustration and even disgust with other parties, such as the Mensheviks and Socialist Revolutionaries, who stubbornly refused to break with the idea of national unity across all classes.
In Finland, Lenin had worked on his book "State and Revolution" and continued to lead his party, writing newspaper articles and policy decrees. By October, he returned to Petrograd (St. Petersburg), aware that the increasingly radical city presented him no legal danger and a second opportunity for revolution. Recognising the strength of the Bolsheviks, Lenin began pressing for the immediate overthrow of the Kerensky government by the Bolsheviks. Lenin was of the opinion that taking power should occur in both St. Petersburg and Moscow simultaneously, parenthetically stating that it made no difference which city rose up first, but expressing his opinion that Moscow may well rise up first. The Bolshevik Central Committee drafted a resolution, calling for the dissolution of the Provisional Government in favor of the Petrograd Soviet. The resolution was passed 10–2 (Lev Kamenev and Grigory Zinoviev prominently dissenting) and the October Revolution began.
October Revolution.
The October Revolution was led by Vladimir Lenin and was based upon Lenin's writing on the ideas of Karl Marx, a political ideology often known as Marxism-Leninism. It marked the beginning of the spread of communism in the 20th century. It was far less sporadic than the revolution of February and came about as the result of deliberate planning and coordinated activity to that end.
Though Lenin was the leader of the Bolshevik Party, it has been argued that since Lenin was not present during the actual takeover of the Winter Palace, it was really Trotsky's organization and direction that led the revolution, merely spurred by the motivation Lenin instigated within his party. Critics on the Right have long argued that the financial and logistical assistance of German intelligence via their key agent, Alexander Parvus was a key component as well, though historians are divided, since there is little evidence supporting that claim.
On 7 November 1917, Bolshevik leader Vladimir Lenin led his leftist revolutionaries in a revolt against the ineffective Provisional Government (Russia was still using the Julian Calendar at the time, so period references show a 25 October date). The October revolution ended the phase of the revolution instigated in February, replacing Russia's short-lived provisional parliamentary government with government by soviets, local councils elected by bodies of workers and peasants. Liberal and monarchist forces, loosely organized into the White Army, immediately went to war against the Bolsheviks' Red Army, in a series of battles that would become known as the Russian Civil War.
Soviet membership was initially freely elected, but many members of the Socialist-Revolutionary Party, anarchists, and other leftists created opposition to the Bolsheviks through the soviets themselves. When it became clear that the Bolsheviks had little support outside of the industrialized areas of Saint Petersburg and Moscow, they simply barred non-Bolsheviks from membership in the soviets. Not surprisingly, this caused mass domestic tension with many individuals who called for another series of political reform, revolting, and calling for "a third Russian revolution," a movement that received a significant amount of support. The most notable instances of this anti-Bolshevik mentality were expressed in the Tambov rebellion, 1919–1921, and the Kronstadt rebellion in March 1921. These movements, which made a wide range of demands and lacked effective coordination, were eventually defeated along with the White Army during the Civil War.
Civil war.
The Russian Civil War, which broke out in 1918 shortly after the revolution, brought death and suffering to millions of people regardless of their political orientation. The war was fought mainly between the Red Army ("Reds"), consisting of the uprising majority led by the Bolshevik minority, and the "Whites" – army officers and cossacks, the "bourgeoisie", and political groups ranging from the far Right to the Socialist Revolutionaries who opposed the drastic restructuring championed by the Bolsheviks following the collapse of the Provisional Government to the soviets (under clear Bolshevik dominance). The Whites had backing from nations such as Great Britain, France, USA and Japan, while the Reds possessed internal support which proved to be much more effective. Though the Allied nations, using external interference, provided substantial military aid to the loosely knit anti-Bolshevik forces, they were ultimately defeated.
The Bolsheviks firstly assumed power in Petrograd, expanding their rule outwards. They eventually reached the Easterly Siberian Russian coast in Vladivostok, 4 years after the war began, an occupation that is believed to have ended all significant military campaigns in the nation. Less than one year later the last area controlled by the White Army, the Ayano-Maysky District, directly to the north of the Krai containing Vladivostok, was given up when General Anatoly Pepelyayev capitulated in 1923.
Several revolts were initiated against the Bolsheviks and their army near the end of the war, notably the Kronstadt Rebellion. This was a naval mutiny engineered by Soviet Baltic sailors, former Red Army soldiers, and the people of Kronstadt. This armed uprising was fought against the antagonizing Bolshevik economic policies that farmers were subjected to, including seizures of grain crops by the Communists. This all amounted to large-scale discontent. When delegates representing the Kronstadt sailors arrived at Petrograd for negotiations, they raised 15 demands primarily pertaining to the Russian right to freedom The Government firmly denounced the rebellions and labelled the requests as a reminder of the Social Revolutionaries, a political party that was popular among Soviets before Lenin, but refused to cooperate with the Bolshevik Army. The Government then responded with an armed suppression of these revolts and suffered 10 thousand casualties before entering the city of Kronstadt. This ended the rebellions fairly quickly, causing many of the rebels to flee to political exile.
During the Civil War, Nestor Makhno led a Ukrainian anarchist movement, the Black Army allied to the Bolsheviks thrice, one of the powers ending the alliance each time. However, a Bolshevik force under Mikhail Frunze destroyed the Makhnovist movement, when the Makhnovists refused to merge into the Red Army. In addition, the so-called "Green Army" (peasants defending their property against the opposing forces) played a secondary role in the war, mainly in the Ukraine.
Execution of the imperial family.
The Bolsheviks executed the tsar and his family on 16 July 1918. In early March, the Provisional Government placed Nicholas and his family under house arrest in the Alexander Palace at Tsarskoe Selo, south of Petrograd. In August 1917 the Kerensky government evacuated the Romanovs to Tobolsk in the Urals, to protect them from the rising tide of revolution during the Red Terror. After the Bolsheviks came to power in October 1917, the conditions of their imprisonment grew stricter and talk of putting Nicholas on trial increased. As the counter revolutionary White movement gathered force, leading to full-scale civil war by the summer, the Romanovs were moved during April and May 1918 to Yekaterinburg, a militant Bolshevik stronghold.
During the early morning of 16 July, Nicholas, Alexandra, their children, their physician, and several servants were taken into the basement and shot. According to Edvard Radzinsky and Dmitrii Volkogonov, the order came directly from Lenin and Sverdlov in Moscow. That the order came from the top has long been believed, although there is a lack of hard evidence. The execution may have been carried out on the initiative of local Bolshevik officials, or it may have been an option pre-approved in Moscow should White troops approach Yekaterinburg. Radzinsky noted that Lenin's bodyguard personally delivered the telegram ordering the execution and that he was ordered to destroy the evidence.
The Russian revolution and the world.
Leon Trotsky said that the goal of socialism in Russia would not be realized without the success of the world revolution. Indeed, a revolutionary wave caused by the Russian Revolution lasted until 1923. Despite initial hopes for success in the German Revolution of 1918–1919, in the short-lived Hungarian Soviet Republic and others like it, no other Marxist movement at the time succeeded in keeping power in its hands.
This issue is subject to conflicting views on the communist history by various Marxist groups and parties. Joseph Stalin later rejected this idea, stating that socialism was possible in one country.
The confusion regarding Stalin's position on the issue stems from the fact that he, after Lenin's death in 1924, successfully used Lenin's argument – the argument that socialism's success needs the workers of other countries in order to happen – to defeat his competitors within the party by accusing them of betraying Lenin and, therefore, the ideals of the October Revolution.
Historiography.
Few events in historical research have been as conditioned by political influences as the October Revolution. The historiography of the Revolution generally divides into three camps: the Soviet-Marxist view, the Western-Totalitarian view, and the Revisionist view. Since the fall of Communism in Russia in 1991, the Western-Totalitarian view has again become dominant and the Soviet-Marxist view has practically vanished.
Lenin's biographer Robert Service, says he, "laid the foundations of dictatorship and lawlessness. Lenin had consolidated the principle of state penetration of the whole society, its economy and its culture. Lenin had practised terror and advocated revolutionary amoralism."
Chronologies.
Chronology of events leading to the Revolution of 1917.
"Dates are correct for the Julian calendar, which was used in Russia until 1918. It was twelve days behind the Gregorian calendar during the 19th century and thirteen days behind it during the 20th century."
Cultural portrayal.
George Orwell's classic novella "Animal Farm" is an allegory of the Russian Revolution. It describes the dictator Stalin as a big Berkshire boar by the name of Napoleon. Trotsky is represented by a pig called Snowball who is a brilliant talker and makes magnificent speeches. However, Napoleon overthrows Snowball as Stalin overthrew Trotsky and Napoleon took over the farm on which the animals were living on. Napoleon became a tyrant and used force and propaganda to oppress the animals.
Film.
The Russian Revolution has been portrayed in several films.

</doc>
<doc id="25764" url="https://en.wikipedia.org/wiki?curid=25764" title="Raven Software">
Raven Software

Raven Software (or Raven Entertainment Software, Inc.) is an American video game developing company based in Wisconsin and founded in 1990. In 1997, Raven made an exclusive publishing deal with Activision and was subsequently acquired by them. After the acquisition, many of the studio's original developers, largely responsible for creating the "Heretic" and "" games, left to form Human Head Studios.
History.
Raven Software was founded in 1990 by brothers Brian and Steve Raffel. The company was independent until 1997 when it was acquired by Activision.
Raven has a history of working with id Software, who were briefly located on the same street. They used id's engines for many of their games, such as "Heretic" in 1994. They took over development of id's "Quake" franchise for "Quake 4" and the 2009 iteration of id's "Wolfenstein" series.
The company started with three development teams. In August 2009 following poor performance and possible over-budget of "Wolfenstein," the company made a major layoff of 30-35 staff, leaving two development teams. This was reduced to one after more layoffs in October 2010, after delays with "Singularity;" as many as 40 staff were released.
Games.
In 2012, Raven began hiring employees for a next generation game, and were announced as collaborating with Infinity Ward on "" in May 2013.
On 3 April 2013 following the closure of LucasArts, Raven Software released the source codes for ' and ' on Sourceforge.
As of April 2014, the company is the lead developer of the free-to-play Chinese "Call of Duty" title, "".

</doc>
<doc id="25765" url="https://en.wikipedia.org/wiki?curid=25765" title="RNA world">
RNA world

The RNA world refers to the self-replicating ribonucleic acid (RNA) molecules hypothesised to have been the precursors to all current life on Earth. The hypothesis that current life on Earth descends from an RNA world is widely accepted, although alternative chemical paths to life have been proposed, and RNA-based life may not have been the first life to exist.
The RNA world would have eventually been replaced by the DNA, RNA and protein world of today, likely through an intermediate stage of ribonucleoprotein enzymes such as the ribosome and ribozymes, since it is argued that proteins large enough to self-fold and have useful activities would only have come about after RNA was available to catalyze peptide ligation or amino acid polymerization. DNA is thought to have taken over the role of data storage due to its increased stability, while proteins, through a greater variety of monomers (amino acids), replaced RNA's role in specialized biocatalysis.
The RNA world hypothesis is supported by many independent lines of evidence, such as the observations that RNA is central to the translation process and that small RNAs can catalyze all of the chemical group and information transfers required for life. The structure of the ribosome has been called the "smoking gun," as it showed that the ribosome is a ribozyme, with a central core of RNA and no amino acid side chains within 18 angstroms of the active site where peptide bond formation is catalyzed. Many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. This would mean that the RNA and nucleotide cofactors in modern cells are an evolutionary remnant of an RNA-based enzymatic system that preceded the protein-based one seen in all extant life.
History.
One of the challenges in studying abiogenesis is that the system of reproduction and metabolism utilized by all extant life involves three distinct types of interdependent macromolecules (DNA, RNA, and protein). This suggests that life could not have arisen in its current form, and mechanisms have then been sought whereby the current system might have arisen from a simpler precursor system. The concept of RNA as a primordial molecule can be found in papers by Francis Crick and Leslie Orgel, as well as in Carl Woese's 1967 book "The Genetic Code". In 1962 the molecular biologist Alexander Rich, of the Massachusetts Institute of Technology, had posited much the same idea in an article he contributed to a volume issued in honor of Nobel-laureate physiologist Albert Szent-Györgyi. Hans Kuhn in 1972 laid out a possible process by which the modern genetic system might have arisen from a nucleotide-based precursor, and this led Harold White in 1976 to observe that many of the cofactors essential for enzymatic function are either nucleotides or could have been derived from nucleotides. He proposed that these nucleotide cofactors represent "fossils of nucleic acid enzymes". The phrase "RNA World" was first used by Nobel laureate Walter Gilbert in 1986, in a commentary on how recent observations of the catalytic properties of various forms of RNA fit with this hypothesis.
Properties of RNA.
The properties of RNA make the idea of the RNA world hypothesis conceptually plausible, though its general acceptance as an explanation for the origin of life requires further evidence. RNA is known to form efficient catalysts and its similarity to DNA makes its ability to store information clear. Opinions differ, however, as to whether RNA constituted the ﬁrst autonomous self-replicating system or was a derivative of a still-earlier system. One version of the hypothesis is that a different type of nucleic acid, termed "pre-RNA", was the first one to emerge as a self-reproducing molecule, to be replaced by RNA only later. On the other hand, the recent ﬁnding that activated pyrimidine ribonucleotides can be synthesized under plausible prebiotic conditions means that it is premature to dismiss the RNA-ﬁrst scenarios. Suggestions for 'simple' "pre-RNA" nucleic acids have included Peptide nucleic acid (PNA), Threose nucleic acid (TNA) or Glycol nucleic acid (GNA). Despite their structural simplicity and possession of properties comparable with RNA, the chemically plausible generation of "simpler" nucleic acids under prebiotic conditions has yet to be demonstrated.
RNA as an enzyme.
RNA enzymes, or ribozymes, are found in today's DNA-based life and could be examples of living fossils. Ribozymes play vital roles, such as those in the ribosome, which is vital for protein synthesis. Many other ribozyme functions exist; for example, the hammerhead ribozyme performs self-cleavage and an RNA polymerase ribozyme can synthesize a short RNA strand from a primed RNA template.
Among the enzymatic properties important for the beginning of life are:
RNA in information storage.
RNA is a very similar molecule to DNA, and only has two chemical differences. The overall structure of RNA and DNA are immensely similar—one strand of DNA and one of RNA can bind to form a double helical structure. This makes the storage of information in RNA possible in a very similar way to the storage of information in DNA. However RNA is less stable.
Comparison of DNA and RNA structure.
The major difference between RNA and DNA is the presence of a hydroxyl group at the 2'-position of the ribose sugar in RNA ("illustration, right"). This group makes the molecule less stable because when not constrained in a double helix, the 2' hydroxyl can chemically attack the adjacent phosphodiester bond to cleave the phosphodiester backbone. The hydroxyl group also forces the ribose into the C3'-"endo" sugar conformation unlike the C2'-"endo" conformation of the deoxyribose sugar in DNA. This forces an RNA double helix to change from a B-DNA structure to one more closely resembling A-DNA.
RNA also uses a different set of bases than DNA—adenine, guanine, cytosine and uracil, instead of adenine, guanine, cytosine and thymine. Chemically, uracil is similar to thymine, differing only by a methyl group, and its production requires less energy. In terms of base pairing, this has no effect. Adenine readily binds uracil or thymine. Uracil is, however, one product of damage to cytosine that makes RNA particularly susceptible to mutations that can replace a GC base pair with a GU (wobble) or AU base pair.
RNA is thought to have preceded DNA, because of their ordering in the biosynthetic pathways. The deoxyribonucleotides used to make DNA are made from ribonucleotides, the building blocks of RNA, by removing the 2'-hydroxyl group. As a consequence a cell must have the ability to make RNA before it can make DNA.
Limitations of information storage in RNA.
The chemical properties of RNA make large RNA molecules inherently fragile, and they can easily be broken down into their constituent nucleotides through hydrolysis. These limitations do not make use of RNA as an information storage system impossible, simply energy intensive (to repair or replace damaged RNA molecules) and prone to mutation. While this makes it unsuitable for current 'DNA optimised' life, it may have been acceptable for more primitive life.
RNA as a regulator.
Riboswitches have been found to act as regulators of gene expression, particularly in bacteria, but also in plants and archaea. Riboswitches alter their secondary structure in response to the binding of a metabolite. This change in structure can result in the formation or disruption of a terminator, truncating or permitting transcription respectively. Alternatively, riboswitches may bind or occlude the Shine-Dalgarno sequence, affecting translation. It has been suggested that these originated in an RNA-based world. In addition, RNA thermometers regulate gene expression in response to temperature changes.
Support and difficulties.
The RNA world hypothesis is supported by RNA's ability to store, transmit, and duplicate genetic information, as DNA does. RNA can act as a ribozyme, a special type of enzyme. Because it can perform the tasks of both DNA and enzymes, RNA is believed to have once been capable of supporting independent life forms. Some viruses use RNA as their genetic material, rather than DNA. Further, while nucleotides were not found in Miller-Urey's origins of life experiments, their formation in prebiotically plausible conditions has now been reported, as noted above; the purine base known as adenine is merely a pentamer of hydrogen cyanide. Experiments with basic ribozymes, like Bacteriophage Qβ RNA, have shown that simple self-replicating RNA structures can withstand even strong selective pressures (e.g., opposite-chirality chain terminators).
Since there were no known chemical pathways for the abiogenic synthesis of nucleotides from pyrimidine nucleobases cytosine and uracil under prebiotic conditions, it is thought by some that nucleic acids did not contain these nucleobases seen in life's nucleic acids. The nucleoside cytosine has a half-life in isolation of 19 days at and 17,000 years in freezing water, which some argue is too short on the geologic time scale for accumulation. Others have questioned whether ribose and other backbone sugars could be stable enough to find in the original genetic material, and have raised the issue that all ribose molecules would have had to be the same enantiomer, as any nucleotide of the wrong chirality acts as a chain terminator.
Pyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesised by a sequence of reactions that by-pass free sugars and assemble in a stepwise fashion by including nitrogenous and oxygenous chemistries. In a series of publications, The "Sutherland Group" at the School of Chemistry, University of Manchester have demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater, of possible interest towards biological homochirality. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallised out from a mixture of the other pentose aminooxazolines. Aminooxazolines can react with cyanoacetylene in a mild and highly efficient manner, controlled by inorganic phosphate, to give the cytidine ribonucleotides. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry; one problem with this chemistry is the selective phosphorylation of alpha-cytidine at the 2' position. However, in 2009 they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerise into RNA. Organic chemist Donna Blackmond described this finding as "strong evidence" in favour of the RNA world. However, John Sutherland said that while his team's work suggests that nucleic acids played an early and central role in the origin of life, it did not necessarily support the RNA world hypothesis in the strict sense, which he described as a "restrictive, hypothetical arrangement".
The Sutherland group's 2009 paper also highlighted the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates. A potential weakness of these routes is the generation of enantioenriched glyceraldehyde, or its 3-phosphate derivative (glyceraldehyde prefers to exist as its keto tautomer dihydroxyacetone).
On August 8, 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of RNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. On August 29, 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
"Molecular biologist's dream".
"Molecular biologist's dream" is a phrase coined by Gerald Joyce and Leslie Orgel to refer to the problem of emergence of self-replicating RNA molecules, as any movement towards an RNA world on a properly modeled prebiotic early Earth would have been continuously suppressed by destructive reactions. It was noted that many of the steps needed for the nucleotides formation do not proceed efficiently in prebiotic conditions. Joyce and Orgel specifically referred the molecular biologist's dream to "a magic catalyst" that could "convert the activated nucleotides to a random ensemble of polynucleotide sequences, a subset of which had the ability to replicate".
Joyce and Orgel further argued that nucleotides cannot link unless there is some activation of the phosphate group, whereas the only effective activating groups for this are "totally implausible in any prebiotic scenario", particularly adenosine triphosphate. According to Joyce and Orgel, in case of the phosphate group activation, the basic polymer product would have 5',5'-pyrophosphate linkages, while the 3',5'-phosphodiester linkages, which are present in all known RNA, would be much less abundant. The associated molecules would have been also prone to addition of incorrect nucleotides or to reactions with numerous other substances likely to have been present. The RNA molecules would have been also continuously degraded by such destructive process as spontaneous hydrolysis, present on the early Earth. Joyce and Orgel proposed to reject "the myth of a self-replicating RNA molecule that arose "de novo" from a soup of random polynucleotides" and hypothesised a scenario where the prebiotic processes furnish pools of enantiopure beta-D-ribonucleosides.
Prebiotic RNA synthesis.
Nucleotides are the fundamental molecules that combine in series to form RNA. They consist of a nitrogenous base attached to a sugar-phosphate backbone. RNA is made of long stretches of specific nucleotides arranged so that their sequence of bases carries information. The RNA world hypothesis holds that in the primordial soup (or sandwich), there existed free-floating nucleotides. These nucleotides regularly formed bonds with one another, which often broke because the change in energy was so low. However, certain sequences of base pairs have catalytic properties that lower the energy of their chain being created, enabling them to stay together for longer periods of time. As each chain grew longer, it attracted more matching nucleotides faster, causing chains to now form faster than they were breaking down.
These chains have been proposed by some as the first, primitive forms of life. In an RNA world, different sets of RNA strands would have had different replication outputs, which would have increased or decreased their frequency in the population, i.e. natural selection. As the fittest sets of RNA molecules expanded their numbers, novel catalytic properties added by mutation, which benefitted their persistence and expansion, could accumulate in the population. Such an autocatalytic set of ribozymes, capable of self replication in about an hour, has been identified. It was produced by molecular competition ("in vitro" evolution) of candidate enzyme mixtures.
Competition between RNA may have favored the emergence of cooperation between different RNA chains, opening the way for the formation of the first protocell. Eventually, RNA chains developed with catalytic properties that help amino acids bind together (a process called peptide-bonding). These amino acids could then assist with RNA synthesis, giving those RNA chains that could serve as ribozymes the selective advantage. The ability to catalyze one step in protein synthesis, aminoacylation of RNA, has been demonstrated in a short (five-nucleotide) segment of RNA.
One of the problems with the RNA world hypothesis is to discover the pathway by which RNA became upgraded to the DNA system. Geoffrey Diemer and Ken Stedman at Portland State University in Oregon, may have found a solution. While conducting a survey of viruses in a hot acidic lake in Lassen Volcanic National Park, California, they uncovered evidence that a simple DNA virus had acquired a gene from a completely unrelated RNA-based virus. Virologist Luis Villareal of the University of California Irvine also suggests that viruses capable of converting an RNA-based gene into DNA and then incorporating it into a more complex DNA-based genome might have been common in the Virus world during the RNA to DNA transition some 4 billion years ago. This finding bolsters the argument for the transfer of information from the RNA world to the emerging DNA world before the emergence of the Last Universal Common Ancestor. From the research, the diversity of this virus world is still with us.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under conditions found only in outer space, using starting chemicals, like pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in giant red stars or in interstellar dust and gas clouds, according to the scientists.
Viroids.
Additional evidence supporting the concept of an RNA world has resulted from research on viroids, the first representatives of a novel domain of "subviral pathogens."
Viroids are mostly plant pathogens, which consist of short stretches (a few hundred nucleobases) of highly complementary, circular, single-stranded, and non-coding RNA without a protein coat. Compared with other infectious plant pathogens, viroids are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection are about 2,000 nucleobases long.
In 1989, Diener proposed that, based on their characteristic properties, viroids are more plausible "living relics" of the RNA world than are introns or other RNAs then so considered. If so, viroids have attained potential significance beyond plant pathology to evolutionary biology, by representing the most plausible macromolecules known capable of explaining crucial intermediate steps in the evolution of life from inanimate matter (see: abiogenesis).
Apparently, Diener's hypothesis lay dormant until 2014, when Flores et al. published a review paper, in which Diener's evidence supporting his hypothesis was summarized. In the same year, a New York Times science writer published a popularized version of Diener's proposal, in which, however, he mistakenly credited Flores et al. with the hypothesis' original conception.
Pertinent viroid properties listed in 1989 are:
The existence, in extant cells, of RNAs with molecular properties predicted for RNAs of the RNA World constitutes an additional argument supporting the RNA World hypothesis.
Origin of sex.
Eigen et al. and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present-day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene).
Vulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. However, for such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number. The protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells.
A cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with a damage in an essential RNA gene would simply die.
This model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, “mating” can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as “multiplicity reactivation”. Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation.
Further developments.
Patrick Forterre has been working on a novel hypothesis, called "three viruses, three domains": that viruses were instrumental in the transition from RNA to DNA and the evolution of Bacteria, Archaea, and Eukaryota. He believes the last common ancestor (specifically, the "last universal cellular ancestor") was RNA-based and evolved RNA viruses. Some of the viruses evolved into DNA viruses to protect their genes from attack. Through the process of viral infection into hosts the three domains of life evolved. Another interesting proposal is the idea that RNA synthesis might have been driven by temperature gradients, in the process of thermosynthesis.
Single nucleotides have been shown to catalyze organic reactions.
Steven Benner has argued that chemical conditions on the planet Mars, such as the presence of boron, molybdenum and oxygen, may have been better for initially producing RNA molecules than those on Earth. If so, life-suitable molecules, originating on Mars, may have later migrated to Earth via panspermia or similar process.
Alternative hypotheses.
The hypothesized existence of an RNA world does not exclude a "Pre-RNA world", where a metabolic system based on a different nucleic acid is proposed to pre-date RNA. A candidate nucleic acid is peptide nucleic acid (PNA), which uses simple peptide bonds to link nucleobases. PNA is more stable than RNA, but its ability to be generated under prebiological conditions has yet to be demonstrated experimentally.
Threose nucleic acid (TNA) has also been proposed as a starting point, as has glycol nucleic acid (GNA), and like PNA, also lack experimental evidence for their respective abiogenesis.
An alternative — or complementary — theory of RNA origin is proposed in the PAH world hypothesis, whereby polycyclic aromatic hydrocarbons (PAHs) mediate the synthesis of RNA molecules. PAHs are the most common and abundant of the known polyatomic molecules in the visible Universe, and are a likely constituent of the primordial sea. PAHs, along with fullerenes (also implicated in the origin of life), have been recently detected in nebulae.
The iron-sulfur world theory proposes that simple metabolic processes developed before genetic materials did, and these energy-producing cycles catalyzed the production of genes.
Some of the difficulties of producing the precursors on earth are bypassed by another alternative or complementary theory for their origin, panspermia. It discusses the possibility that the earliest life on this planet was carried here from somewhere else in the galaxy, possibly on meteorites similar to the Murchison meteorite. This does not invalidate the concept of an RNA world, but posits that this world or its precursors originated not on Earth but rather another, probably older, planet.
There are hypotheses that are in direct conflict to the RNA world hypothesis. The relative chemical complexity of the nucleotide and the unlikelihood of it spontaneously arising, along with the limited number of combinations possible among four base forms, as well as the need for RNA polymers of some length before seeing enzymatic activity, have led some to reject the RNA world hypothesis in favor of a metabolism-first hypothesis, where the chemistry underlying cellular function arose first, along with the ability to replicate and facilitate this metabolism.
RNA-peptide coevolution.
Another proposal is that the dual-molecule system we see today, where a nucleotide-based molecule is needed to synthesize protein, and a protein-based molecule is needed to make nucleic acid polymers, represents the original form of life. This theory is called RNA-peptide coevolution, or the Peptide-RNA world, and offers a possible explanation for the rapid evolution of high-quality replication in RNA (since proteins are catalysts), with the disadvantage of having to postulate the formation of two complex molecules, an enzyme (from peptides) and a RNA (from nucleotides). In this Peptide-RNA World scenario, RNA would have contained the instructions for life, while peptides (simple protein enzymes) would have accelerated key chemical reactions to carry out those instructions. The study leaves open the question of exactly how those primitive systems managed to replicate themselves — something neither the RNA World hypothesis nor the Peptide-RNA World theory can yet explain, unless polymerases (enzymes that rapidly assemble the RNA molecule) played a role.
A research project completed in March 2015 by the Sutherland group found that a network of reactions beginning with hydrogen cyanide and hydrogen sulfide, in streams of water irradiated by UV light, could produce the chemical components of proteins and lipids, alongside those of RNA. The researchers used the term "cyanosulfidic" to describe this network of reactions.
Implications of the RNA world.
The RNA world hypothesis, if true, has important implications for the definition of life. For most of the time that followed Watson and Crick's elucidation of DNA structure in 1953, life was largely defined in terms of DNA and proteins: DNA and proteins seemed the dominant macromolecules in the living cell, with RNA only aiding in creating proteins from the DNA blueprint.
The RNA world hypothesis places RNA at center-stage when life originated. This has been accompanied by many studies in the last ten years that demonstrate important aspects of RNA function not previously known—and supports the idea of a critical role for RNA in the mechanisms of life. The RNA world hypothesis is supported by the observations that ribosomes are ribozymes: the catalytic site is composed of RNA, and proteins hold no major structural role and are of peripheral functional importance. This was confirmed with the deciphering of the 3-dimensional structure of the ribosome in 2001. Specifically, peptide bond formation, the reaction that binds amino acids together into proteins, is now known to be catalyzed by an adenine residue in the rRNA.
Other interesting discoveries demonstrate a role for RNA beyond a simple message or transfer molecule. These include the importance of small nuclear ribonucleoproteins (snRNPs) in the processing of pre-mRNA and RNA editing, RNA interference (RNAi), and reverse transcription from RNA in eukaryotes in the maintenance of telomeres in the telomerase reaction.

</doc>
<doc id="25766" url="https://en.wikipedia.org/wiki?curid=25766" title="Ribosome">
Ribosome

The ribosome () is a complex molecular machine found within all living cells, that serves as the site of biological protein synthesis (translation). Ribosomes link amino acids together in the order specified by messenger RNA (mRNA) molecules. Ribosomes consist of two major components: the small ribosomal subunit, which reads the RNA, and the large subunit, which joins amino acids to form a polypeptide chain. Each subunit is composed of one or more ribosomal RNA (rRNA) molecule and a variety of proteins. The ribosomes and associated molecules are also known as the "translational apparatus".
The sequence of DNA, which encodes the sequence of the amino acids in a protein, is copied into a messenger RNA chain. It may be copied many times into RNA chains. Ribosomes can bind to a messenger RNA chain and use its sequence for determining the correct sequence of amino acids. Amino acids are selected, collected, and carried to the ribosome by transfer RNA (tRNA) molecules, which enter one part of the ribosome and bind to the messenger RNA chain. It is during this binding that the correct translation of nucleic acid sequence to amino acid sequence occurs. For each coding triplet in the messenger RNA there is a distinct transfer RNA that matches and which carries the correct amino acid for that coding triplet. The attached amino acids are then linked together by another part of the ribosome. Once the protein is produced, it can then fold to produce a specific functional three-dimensional structure although during synthesis some proteins start folding into their correct form.
A ribosome is made from complexes of RNAs and proteins and is therefore a ribonucleoprotein. Each ribosome is divided into two subunits: 1. a smaller subunit which binds to a larger subunit and the mRNA pattern, and 2. a larger subunit which binds to the tRNA, the amino acids, and the smaller subunit. When a ribosome finishes reading an mRNA molecule, these two subunits split apart. Ribosomes are ribozymes, because the catalytic peptidyl transferase activity that links amino acids together is performed by the ribosomal RNA. Ribosomes are often embedded in the intracellular membranes that make up the rough endoplasmic reticulum.
Ribosomes from bacteria, archaea and eukaryotes (the three domains of life on Earth) resemble each other to a remarkable degree, evidence of a common origin. They differ in their size, sequence, structure, and the ratio of protein to RNA. The differences in structure allow some antibiotics to kill bacteria by inhibiting their ribosomes, while leaving human ribosomes unaffected. In bacteria and archaea, more than one ribosome may move along a single mRNA chain at one time, each "reading" its sequence and producing a corresponding protein molecule.
The ribosomes in the mitochondria of eukaryotic cells (mitoribosomes), are produced from mitochondrial genes, and functionally resemble many features of those in bacteria, reflecting the likely evolutionary origin of mitochondria.
Discovery.
Ribosomes were first observed in the mid-1950s by Romanian cell biologist George Emil Palade using an electron microscope as dense particles or granules for which, in 1974, he would win a Nobel Prize. The term "ribosome" was proposed by scientist Richard B. Roberts in 1958:
Albert Claude, Christian de Duve, and George Emil Palade were jointly awarded the Nobel Prize in Physiology or Medicine, in 1974, for the discovery of the ribosomes. The Nobel Prize in Chemistry 2009 was awarded to Venkatraman Ramakrishnan, Thomas A. Steitz and Ada E. Yonath for determining the detailed structure and mechanism of the ribosome.
Structure.
The ribosome is responsible for the synthesis of proteins in cells and is found in all cellular organisms. It serves to convert the instructions found in messenger RNA (mRNA, which itself is made from instructions in DNA) into the chains of amino-acids that make up proteins.
The ribosome is a cellular machine which is highly complex. It is made up of dozens of distinct proteins (the exact number varies slightly between species) as well as a few specialized RNA molecules known as ribosomal RNA (rRNA). Note – these rRNAs do not carry instructions to make specific proteins like mRNAs. The ribosomal proteins and rRNAs are arranged into two distinct ribosomal pieces of different size, known generally as the large and small subunit of the ribosome. Ribosomes consist of two subunits that fit together (Figure 2) and work as one to translate the mRNA into a polypeptide chain during protein synthesis (Figure 1). Because they are formed from two subunits of non-equal size, they are slightly longer in the axis than in diameter. Prokaryotic ribosomes are around 20 nm (200 Å) in diameter and are composed of 65% rRNA and 35% ribosomal proteins. Eukaryotic ribosomes are between 25 and 30 nm (250–300 Å) in diameter with an rRNA-to-protein ratio that is close to 1. Bacterial ribosomes are composed of one or two rRNA strands. Eukaryotic ribosomes contain one or three very large rRNA molecules and multiple smaller protein molecules. Crystallographic work has shown that there are no ribosomal proteins close to the reaction site for polypeptide synthesis. This proves that the protein components of ribosomes do not directly participate in peptide bond formation catalysis, but rather suggests that these proteins act as a scaffold that may enhance the ability of rRNA to synthesize protein (See: Ribozyme).
The ribosomal subunits of prokaryotes and eukaryotes are quite similar.
The unit of measurement is the Svedberg unit, a measure of the rate of sedimentation in centrifugation rather than size. This accounts for why fragment names do not add up: for example, prokaryotic 70S ribosomes are made of 50S and 30S subunits.
Prokaryotes have 70S ribosomes, each consisting of a small (30S) and a large (50S) subunit. Their small subunit has a 16S RNA subunit (consisting of 1540 nucleotides) bound to 21 proteins. The large subunit is composed of a 5S RNA subunit (120 nucleotides), a 23S RNA subunit (2900 nucleotides) and 31 proteins. Affinity label for the tRNA binding sites on the E. coli ribosome allowed the identification of A and P site proteins most likely associated with the peptidyltransferase activity; labelled proteins are L27, L14, L15, L16, L2; at least L27 is located at the donor site, as shown by E. Collatz and A.P. Czernilofsky. Additional research has demonstrated that the S1 and S21 proteins, in association with the 3'-end of 16S ribosomal RNA, are involved in the initiation of translation.
Eukaryotes have 80S ribosomes, each consisting of a small (40S) and large (60S) subunit. Their 40S subunit has an 18S RNA (1900 nucleotides) and 33 proteins. The large subunit is composed of a 5S RNA (120 nucleotides), 28S RNA (4700 nucleotides), a 5.8S RNA (160 nucleotides) subunits and 46 proteins. During 1977, Czernilofsky published research that used affinity labeling to identify tRNA-binding sites on rat liver ribosomes. Several proteins, including L32/33, L36, L21, L23, L28/29 and L13 were implicated as being at or near the peptidyl transferase center.
The ribosomes found in chloroplasts and mitochondria of eukaryotes also consist of large and small subunits bound together with proteins into one 70S particle. These organelles are believed to be descendants of bacteria (see Endosymbiotic theory) and, as such, their ribosomes are similar to those of bacteria.
The various ribosomes share a core structure, which is quite similar despite the large differences in size. Much of the RNA is highly organized into various tertiary structural motifs, for example pseudoknots that exhibit coaxial stacking. The extra RNA in the larger ribosomes is in several long continuous insertions, such that they form loops out of the core structure without disrupting or changing it. All of the catalytic activity of the ribosome is carried out by the RNA; the proteins reside on the surface and seem to stabilize the structure.
The differences between the bacterial and eukaryotic ribosomes are exploited by pharmaceutical chemists to create antibiotics that can destroy a bacterial infection without harming the cells of the infected person. Due to the differences in their structures, the bacterial 70S ribosomes are vulnerable to these antibiotics while the eukaryotic 80S ribosomes are not. Even though mitochondria possess ribosomes similar to the bacterial ones, mitochondria are not affected by these antibiotics because they are surrounded by a double membrane that does not easily admit these antibiotics into the organelle.
High-resolution structure.
The general molecular structure of the ribosome has been known since the early 1970s. In the early 2000s, the structure has been achieved at high resolutions, of the order of a few Å.
The first papers giving the structure of the ribosome at atomic resolution were published almost simultaneously in late 2000. The 50S (large prokaryotic) subunit was determined from the archaeon "Haloarcula marismortui" and the bacterium "Deinococcus radiodurans", and the structure of the 30S subunit was determined from "Thermus thermophilus". These structural studies were awarded the Nobel Prize in Chemistry in 2009. In May 2001 these coordinates were used to reconstruct the entire "T. thermophilus" 70S particle at 5.5 Å resolution.
Two papers were published in November 2005 with structures of the "Escherichia coli" 70S ribosome. The structures of a vacant ribosome were determined at 3.5-Å resolution using x-ray crystallography. Then, two weeks later, a structure based on cryo-electron microscopy was published, which depicts the ribosome at 11–15Å resolution in the act of passing a newly synthesized protein strand into the protein-conducting channel.
The first atomic structures of the ribosome complexed with tRNA and mRNA molecules were solved by using X-ray crystallography by two groups independently, at 2.8 Å and at 3.7 Å. These structures allow one to see the details of interactions of the "Thermus thermophilus" ribosome with mRNA and with tRNAs bound at classical ribosomal sites. Interactions of the ribosome with long mRNAs containing Shine-Dalgarno sequences were visualized soon after that at 4.5- to 5.5-Å resolution.
In 2011, the first complete atomic structure of the eukaryotic 80S ribosome from the yeast "Saccharomyces cerevisiae" was obtained by crystallography. The model reveals the architecture of eukaryote-specific elements and their interaction with the universally conserved core. At the same time, the complete model of a eukaryotic 40S ribosomal structure in "Tetrahymena thermophila" was published and described the structure of the 40S subunit, as well as much about the 40S subunit's interaction with eIF1 during translation initiation. Similarly, the eukaryotic 60S subunit structure was also determined from "Tetrahymena thermophila" in complex with eIF6.
Function.
Translation.
Ribosomes are the workplaces of protein biosynthesis, the process of translating mRNA into protein. The mRNA comprises a series of codons that dictate to the ribosome the sequence of the amino acids needed to make the protein. Using the mRNA as a template, the ribosome traverses each codon (3 nucleotides) of the mRNA, pairing it with the appropriate amino acid provided by an aminoacyl-tRNA. Aminoacyl-tRNA contains a complementary anticodon on one end and the appropriate amino acid on the other. For fast and accurate recognition of the appropriate tRNA, the ribosome utilizes large conformational changes (conformational proofreading) 
The small ribosomal subunit, typically bound to an aminoacyl-tRNA containing the amino acid methionine, binds to an AUG codon on the mRNA and recruits the large ribosomal subunit. The ribosome contains three RNA binding sites, designated A, P and E. The A site binds an aminoacyl-tRNA; the P site binds a peptidyl-tRNA (a tRNA bound to the peptide being synthesized); and the E site binds a free tRNA before it exits the ribosome. Protein synthesis begins at a start codon AUG near the 5' end of the mRNA. mRNA binds to the P site of the ribosome first. The ribosome is able to identify the start codon by use of the Shine-Dalgarno sequence of the mRNA in prokaryotes and Kozak box in eukaryotes.
Although catalysis of the peptide bond involves the C2 hydroxyl of RNA's P-site adenosine in a proton shuttle mechanism, other steps in protein synthesis (such as translocation) are caused by changes in protein conformations. Since their catalytic core is made of RNA, ribosomes are classified as "ribozymes," and it is thought that they might be remnants of the RNA world.
In Figure 5, both ribosomal subunits (small and large) assemble at the start codon (towards the 5' end of the RNA). The ribosome uses RNA that matches the current codon (triplet) on the mRNA to append an amino acid to the polypeptide chain. This is done for each triplet on the RNA, while the ribosome moves towards the 3' end of the mRNA. Usually in bacterial cells, several ribosomes are working parallel on a single RNA, forming what is called a "polyribosome" or "polysome".
Addition of translation-independent amino acids.
Presence of a ribosome quality control protein Rqc2 is associated with mRNA-independent protein elongation. This elongation is a result of ribosomal addition (via tRNAs brought by Rqc2) of "CAT tails": ribosomes extend the C-terminus of a stalled protein with random, translation-independent sequences of alanines and threonines.
Ribosome locations.
Ribosomes are classified as being either "free" or "membrane-bound".
Free and membrane-bound ribosomes differ only in their spatial distribution; they are identical in structure. Whether the ribosome exists in a free or membrane-bound state depends on the presence of an ER-targeting signal sequence on the protein being synthesized, so an individual ribosome might be membrane-bound when it is making one protein, but free in the cytosol when it makes another protein.
Ribosomes are sometimes referred to as organelles, but the use of the term "organelle" is often restricted to describing sub-cellular components that include a phospholipid membrane, which ribosomes, being entirely particulate, do not. For this reason, ribosomes may sometimes be described as "non-membranous organelles".
Free ribosomes.
Free ribosomes can move about anywhere in the cytosol, but are excluded from the cell nucleus and other organelles. Proteins that are formed from free ribosomes are released into the cytosol and used within the cell. Since the cytosol contains high concentrations of glutathione and is, therefore, a reducing environment, proteins containing disulfide bonds, which are formed from oxidized cysteine residues, cannot be produced within it.
Membrane-bound ribosomes.
When a ribosome begins to synthesize proteins that are needed in some organelles, the ribosome making this protein can become "membrane-bound". In eukaryotic cells this happens in a region of the endoplasmic reticulum (ER) called the "rough ER". The newly produced polypeptide chains are inserted directly into the ER by the ribosome undertaking vectorial synthesis and are then transported to their destinations, through the secretory pathway. Bound ribosomes usually produce proteins that are used within the plasma membrane or are expelled from the cell via "exocytosis".
Biogenesis.
In bacterial cells, ribosomes are synthesized in the cytoplasm through the transcription of multiple ribosome gene operons. In eukaryotes, the process takes place both in the cell cytoplasm and in the nucleolus, which is a region within the cell nucleus. The assembly process involves the coordinated function of over 200 proteins in the synthesis and processing of the four rRNAs, as well as assembly of those rRNAs with the ribosomal proteins.
Origin.
The ribosome may have first originated in an RNA world, appearing as a self-replicating complex that only later evolved the ability to synthesize proteins when amino acids began to appear. Studies suggest that ancient ribosomes constructed solely of rRNA could have developed the ability to synthesize peptide bonds. In addition, evidence strongly points to ancient ribosomes as self-replicating complexes, where the rRNA in the ribosomes had informational, structural, and catalytic purposes because it could have coded for tRNAs and proteins needed for ribosomal self-replication. As amino acids gradually appeared in the RNA world under prebiotic conditions, their interactions with catalytic RNA would increase both the range and efficiency of function of catalytic RNA molecules. Thus, the driving force for the evolution of the ribosome from an ancient self-replicating machine into its current form as a translational machine may have been the selective pressure to incorporate proteins into the ribosome’s self-replicating mechanisms, so as to increase its capacity for self-replication.
Heterogeneity.
Heterogeneity in ribosome composition has been proposed to be involved in translational control of protein synthesis. Vincent Mauro and Gerald Edelman proposed ribosome filter hypothesis to explain the regulatory functions of ribosomes.

</doc>
<doc id="25767" url="https://en.wikipedia.org/wiki?curid=25767" title="Real-time computing">
Real-time computing

In computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a "real-time constraint", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as "deadlines". Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually "guarantee" a response within any timeframe, although "actual" or "expected" response times may be given.
A real-time system has been described as one which "controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time." The term "real-time" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean "without significant delay".
Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.
Systems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding. Real-time processing "fails" if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.
History.
The term "real-time" derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. 
Minicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as CAT scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operatings System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOS (and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.
Once when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Commodore Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems thanks to this facility. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.
Criteria for real-time computing.
A system is said to be "real-time" if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline:
Thus, the goal of a "hard real-time system" is to ensure that all deadlines are met, but for "soft real-time systems" the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high priority tasks meeting their deadlines.
Hard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). For example, a car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Other examples of hard real-time embedded systems include medical systems such as heart pacemakers and industrial process controllers. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems such as the Atari 2600 and Cinematronics vector graphics had hard real-time requirements because of the nature of the graphics and timing hardware.
In the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). Other scheduling algorithms include earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.
Soft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. An example can be software that maintains and updates the flight plans for commercial airliners: the flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time; violation of constraints results in degraded quality, but the system can continue to operate and also recover in the future using workload prediction and reconfiguration methodologies.
Real-time in digital signal processing.
In a real-time digital signal processing (DSP) process, the analyzed (input) and generated (output) samples can be processed (or generated) continuously in the time it takes to input and output the same set of samples "independent" of the processing delay. It means that the processing delay must be bounded even if the processing continues for an unlimited time. That means that the mean processing time per sample, including overhead, is no greater than the sampling period, which is the reciprocal of the sampling rate. This is the criterion whether the samples are grouped together in large segments and processed as blocks or are processed individually and whether there are long, short, or non-existent input and output buffers.
Consider an audio DSP example; if a process requires 2.01 seconds to analyze, synthesize, or process 2.00 seconds of sound, it is not real-time. However, if it takes 1.99 seconds, it is or can be made into a real-time DSP process.
A common life analog is standing in a line or queue waiting for the checkout in a grocery store. If the line asymptotically grows longer and longer without bound, the checkout process is not real-time. If the length of the line is bounded, customers are being "processed" and output as rapidly, on average, as they are being inputted and that process "is" real-time. The grocer might go out of business or must at least lose business if they cannot make their checkout process real-time; thus, it is fundamentally important that this process is real-time.
A signal processing algorithm that cannot keep up with the flow of input data with output falling farther and farther behind the input is not real-time. But if the delay of the output (relative to the input) is bounded regarding a process that operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.
Real-time signal processing is necessary, but not sufficient in and of itself, for live signal processing such as what is required in live event support. Live audio digital signal processing requires both real-time operation and a sufficient limit to throughput delay so as to be tolerable to performers using stage monitors or in-ear monitors and not noticeable as lip sync error by the audience also directly watching the performers. Tolerable limits to latency for live, real-time processing is a subject of investigation and debate but is estimated to be between 6 and 20 milliseconds.
Real-time and high-performance.
Real-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification. For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect) but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. Therefore, the most important requirement of a real-time system is predictability and not performance.
Some kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, while real-time is the ability to get done with the processing to yield a useful output in the available time.
Near real-time.
The term ""near real-time"" or ""nearly real-time"" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.
The distinction between the terms "near real time" and "real time" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as "real-time" would be more accurately described as "near real-time".
Near real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so that they can sync/share common data in "near real-time" with each other.
The distinction between "near real-time" and "real-time" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically of the order of several seconds to several minutes.
Design methods.
Several methods exist to aid the design of real-time systems, an example of which is MASCOT, an old but very successful method which represents the concurrent structure of the system. Other examples are HOOD, Real-Time UML, AADL, the Ravenscar profile, and Real-Time Java.

</doc>
<doc id="25768" url="https://en.wikipedia.org/wiki?curid=25768" title="Ruby (programming language)">
Ruby (programming language)

Ruby is a dynamic, reflective, object-oriented, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.
According to its creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp. It supports multiple programming paradigms, including functional, object-oriented, and imperative. It also has a dynamic type system and automatic memory management.
History.
Early concept.
Ruby was conceived on February 24, 1993. In a 1999 post to the "ruby-talk" mailing list, Ruby author Yukihiro Matsumoto describes some of his early ideas about the language:
Matsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.
The name "Ruby".
The name "Ruby" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language. Initially two names were proposed: "Coral" and "Ruby". Matsumoto chose the latter in a later e-mail to Ishitsuka. Matsumoto later noted a factor in choosing the name "Ruby" – it was the birthstone of one of his colleagues.
First publication.
The first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995. Subsequently three more versions of Ruby were released in two days. The release coincided with the launch of the Japanese-language "ruby-list" mailing list, which was the first mailing list for the new language.
Already present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.
Early releases.
Following the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:
In 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.
In 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.
In 1999, the first English language mailing list "ruby-talk" began, which signaled a growing interest in the language outside Japan. In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, "The Object-oriented Scripting Language Ruby" (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.
By 2000, Ruby was more popular than Python in Japan. In September 2000, the first English language book "Programming Ruby" was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language "ruby-talk" mailing list was receiving more messages than the Japanese-language "ruby-list", demonstrating Ruby's increasing popularity in the English-speaking world.
Ruby 1.8.
Ruby 1.8 was initially released in August 2003, was stable for a long time, and was retired June 2013. Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.
Ruby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011 and an international standard (ISO/IEC 30170) in 2012.
Around 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.
Ruby 1.9.
Ruby 1.9 was released in December 2007. Effective with Ruby 1.9.3, released October 31, 2011, Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license. Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.
Ruby 1.9 introduces many significant changes over the 1.8 series. Examples:
Ruby 1.9 has been obsolete since February 23, 2015, and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.
Ruby 2.0.
Ruby 2.0 added several new features, including:
Ruby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.
It has been obsolete since February 22, 2016 [https://www.ruby-lang.org/en/news/2016/02/24/support-plan-of-ruby-2-0-0-and-2-1/] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.
Ruby 2.1.
Ruby 2.1.0 was released on Christmas Day in 2013. The release includes speed-ups, bugfixes, and library updates.
Starting with 2.1.0, Ruby's versioning policy is more like semantic versioning. Although similar, Ruby's versioning policy is not compatible with semantic versioning:
Semantic versioning also provides additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format, not available at Ruby.
Ruby 2.2.
Ruby 2.2.0 was released on Christmas Day in 2014. The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), and added support for the Unicode 7.0 specification.
Features that were made obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger::Application as well as various C API functions.
Ruby 2.3.
Ruby 2.3.0 was released on December 25, 2015. A few notable changes include:
The 2.3 branch also includes many performance improvements, updates, and bugfixes including changes to Proc#call, Socket and IO use of exception keywords, Thread#name handling, default passive Net::FTP connections, and Rake being removed from stdlib.
Philosophy.
Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design. At a Google Tech Talk in 2008 Matsumoto further stated, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language." He stresses that systems design needs to emphasize human, rather than computer, needs:
Ruby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby, but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.
Matsumoto defined it this way in an interview:
Semantics.
Ruby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and "null"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become members of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as "global" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.
Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.
According to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk but it differs greatly from Python."
Syntax.
The syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords, whereas code blocks can be both defined by keywords or braces. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. For practical purposes there is no distinction between expressions and statements. Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.
One of the differences of Ruby compared to Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (codice_11, codice_12, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.
Python's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare codice_13 and codice_14 methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.
See the Examples section below for samples of code demonstrating Ruby syntax.
Differences from other languages.
Some features that differ notably from languages such as C or Perl:
A list of so-called gotchas may be found in Hal Fulton's book "The Ruby Way", 2nd ed (ISBN 0-672-32884-4), Section 1.5. A similar list in the 1st edition pertained to an older version of Ruby (version 1.6), some problems of which have been fixed in the meantime. For example, codice_47 now works with codice_48, codice_49, and codice_50, as well as with iterators.
Interaction.
The Ruby official distribution also includes codice_51, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using codice_51:
Examples.
The following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing codice_53.
Classic Hello world example:
Some basic Ruby code:
Input:
Conversions:
Strings.
There are a variety of ways to define strings in Ruby.
The following assignments are equivalent:
Strings support variable interpolation:
The following assignments are equivalent and produce raw strings:
Collections.
Constructing and using an array:
Constructing and using an associative array (in Ruby, called a "hash"):
Control structures.
If statement:
Blocks and iterators.
The two syntaxes for creating a code block:
A code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:
Parameter-passing a block to be a closure:
Creating an anonymous function:
Returning closures from a method:
Yielding the flow of program control to a block that was provided at calling time:
Iterating over enumerations and arrays using blocks:
A method such as codice_54 can accept both a parameter and a block. The codice_54 method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the codice_56 function in functional programming languages. For example:
On the first pass, the block receives 10 (the argument to inject) as codice_57, and 1 (the first element of the array) as codice_58. This returns 11, which then becomes codice_57 on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.
Using an enumeration and a block to square the numbers 1 to 10 (using a "range"):
Or invoke a method on each item (codice_60 is a synonym for codice_61):
Classes.
The following code defines a class named codice_62. In addition to codice_63, the usual constructor to create new objects, it has two methods: one to override the codice_64 comparison operator (so codice_65 can sort by age) and the other to override the codice_66 method (so codice_67 can format its output). Here, codice_12 is an example of metaprogramming in Ruby: codice_69 defines getter and setter methods of instance variables, but codice_12 only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit codice_71 statement.
The preceding code prints three names in reverse age order:
codice_62 is a constant and is a reference to a codice_15 object.
Open classes.
In Ruby, classes are never closed: methods can always be added to an existing class. This applies to "all" classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's codice_74 class:
Adding methods to previously defined classes is often called monkey-patching. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.
Exceptions.
An exception is raised with a codice_75 call:
An optional message can be added to the exception:
Exceptions can also be specified by the programmer:
Alternatively, an exception instance can be passed to the codice_75 method:
This last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:
Exceptions are handled by the codice_77 clause. Such a clause can catch exceptions that inherit from codice_78. Other flow control keywords that can be used when handling exceptions are codice_79 and codice_80:
It is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:
Or catch particular exceptions:
It is also possible to specify that the exception object be made available to the handler clause:
Alternatively, the most recent exception is stored in the magic global codice_81.
Several exceptions can also be caught:
Metaprogramming.
Ruby code can programmatically modify, at runtime, aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of metaprogramming can be used to write more concise code and effectively extend the language.
For example, the following Ruby code generates new methods for the built-in codice_82 class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.
The generated methods could then be used like this:
To implement the equivalent in many other languages, the programmer would have to write each method (codice_83, codice_84, codice_85, etc.) separately.
Some other possible uses for Ruby metaprogramming include:
More examples.
More sample Ruby code is available as algorithms in the following article:
Implementations.
Matz's Ruby Interpreter.
The official Ruby interpreter often referred to as the Matz's Ruby Interpreter or MRI. This implementation is written in C and uses its own Ruby-specific virtual machine.
The standardized and retired Ruby 1.8 implementation was written in C, as a single-pass interpreted language.
Starting with Ruby 1.9, and continuing with Ruby 2.x and above, the official Ruby interpreter has been YARV ("Yet Another Ruby VM"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.
Alternate implementations.
, there are a number of alternative implementations of Ruby, including JRuby, Rubinius, MagLev, IronRuby, MacRuby (and its iOS counterpart, RubyMotion), mruby, HotRuby, Topaz and Opal. Each takes a different approach, with IronRuby, JRuby, MacRuby and Rubinius providing just-in-time compilation and MacRuby and mruby also providing ahead-of-time compilation.
Ruby has two major alternate implementations:
Other Ruby implementations include:
Other now defunct Ruby implementations were:
The maturity of Ruby implementations tends to be measured by their ability to run the Ruby on Rails (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called "the Rails singularity". The reference implementation (MRI), JRuby, and Rubinius are all able to run Rails unmodified in a production environment. IronRuby is starting to be able to run Rails test cases, but is still far from being production-ready.
Platform support.
Matsumoto originally did Ruby development on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.
By 1999, Ruby was known to work across many different operating systems, including NEWS-OS, SunOS, AIX, SVR4, Solaris, NEC UP-UX, NeXTSTEP, BSD, Linux, Mac OS, DOS, Windows, and BeOS.
Modern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, OS X, Windows, Windows Phone, Windows CE, Symbian OS, BeOS, and IBM i.
Repositories and libraries.
RubyGems is Ruby's package manager. A Ruby package is called a "gem" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as IDEs. There are over 70,000 Ruby gems hosted on RubyGems.org.
Many new and existing Ruby libraries are hosted on GitHub, a service that offers version control repository hosting for Git.

</doc>
<doc id="25774" url="https://en.wikipedia.org/wiki?curid=25774" title="Render farm">
Render farm

A render farm is a high performance computer system, e.g. a computer cluster, built to render computer-generated imagery (CGI), typically for film and television visual effects.
This is different from a render wall, which is a networked, tiled display used for real-time rendering. The rendering of images is a highly parallelizable activity, as frames and sometimes tiles can be calculated independently of the others, with the main communication between processors being the upload of the initial source material, such as models and textures, and the download of the finished images.
Over the decades, advances in computer capability would allow an image to take less time to render. However, the increased computation is appropriated to meet demands to achieve state-of-the-art image quality. While simple images can be produced rapidly, more realistic and complicated higher-resolution images can now be produced in more reasonable amounts of time. The time spent producing images can be limited by production time-lines and deadlines, and the desire to create high-quality work drives the need for increased computing power, rather than simply wanting the same images created faster.
To manage large farms, one must introduce a "queue manager" that automatically distributes processes to the many processors. Each "process" could be the rendering of one full image, a few images, or even a sub-section (or "tile") of an image. The software is typically a client–server package that facilitates communication between the processors and the queue manager, although some queues have no central manager. Some common features of queue managers are: re-prioritization of the queue, management of software licenses, and algorithms to best optimize throughput based on various types of hardware in the farm. Software licensing handled by a queue manager might involve dynamic allocation of licenses to available CPUs or even cores within CPUs.
A tongue-in-cheek job title for systems engineers who work primarily in the maintenance and monitoring of a render farm is a "render wrangler" to further the "farm" theme. This job title can be seen in film credits.

</doc>
<doc id="25775" url="https://en.wikipedia.org/wiki?curid=25775" title="Render">
Render

Render, rendered, or rendering may refer to:

</doc>
<doc id="25776" url="https://en.wikipedia.org/wiki?curid=25776" title="Robert Borden">
Robert Borden

Sir Robert Laird Borden, (June 26, 1854 – June 10, 1937) was a Canadian lawyer and politician. He served as the eighth Prime Minister of Canada from October 10, 1911, to July 10, 1920, and was the third Nova Scotian to hold this office. After retiring from public life, he served as the chancellor of Queen's University. His portrait appears on Canadian $100 notes produced since 1976.
Early life and career.
Robert Laird Borden was born and educated in Grand-Pré, Nova Scotia, a farming community at the eastern end of the Annapolis Valley, where his great-grandfather Perry Borden, Sr. of Tiverton, Rhode Island had taken up Acadian land in 1760 as one of the New England Planters. Also arriving in this group was a great great grandfather, Robert Denison, who had come from Connecticut at about the same time. Perry had accompanied his father, Samuel Borden, the chief surveyor chosen by the government of Massachusetts to survey the former Acadian land and draw up new lots for the Planters in Nova Scotia. Robert Borden was the last Canadian Prime Minister born before Confederation.
Borden's father Andrew Borden was judged by his son to be "a man of good ability and excellent judgement", of a "calm, contemplative and philosophical" turn of mind, but "he lacked energy and had no great aptitude for affairs". His mother Eunice Jane Laird was more driven, possessing "very strong character, remarkable energy, high ambition and unusual ability". Her ambition was transmitted to her first-born child, who applied himself to his studies while assisting his parents with the farm work he found so disagreeable. His cousin Sir Frederick Borden was a prominent Liberal politician.
Lawyer.
From 1868 to 1874, he worked as a teacher in Grand-Pré and Matawan, New Jersey. Seeing no future in teaching, he returned to Nova Scotia in 1874. Despite have no formal university education, he went to article for four years at a Halifax law firm. In August 1878, he was called to the Nova Scotia Bar, placing first in the bar examinations. Borden went to Kentville, Nova Scotia, as the junior partner of the Conservative lawyer John P. Chipman. In 1880, he was inducted into the Freemasons - St Andrew's lodge #1.
In 1882, he was asked by Wallace Graham to move to Halifax and join the Conservative law firm headed by Graham and 
Charles Hibbert Tupper. In the Autumn of 1889, when he was only 35, Borden became the senior partner following the departure of Graham and Tupper for the bench and politics, respectively. His financial future guaranteed, on September 25, 1889, he married Laura Bond (1863–1940), the daughter of a Halifax hardware merchant. They would have no children. In 1894, he bought a large property and home on the south side of Quinpool Road, which the couple called "Pinehurst". In 1893, Borden successfully argued the first of two cases which he took to the Judicial Committee of the Privy Council. He represented many of the important Halifax businesses, and sat on the boards of Nova Scotian companies including the Bank of Nova Scotia and the Crown Life Insurance Company. In 1896, he became President of the Nova Scotia Barristers' Society, and took the initiative in organizing the founding meetings of the Canadian Bar Association in Montreal within the same year. By the time he was prevailed upon to enter politics, Borden had what some judged to be the largest legal practice in the Maritime Provinces, and had become a wealthy man.
Conservative Party in opposition.
Borden was a Liberal until he broke with the party in 1891 over the issue of Reciprocity.
He was elected to Parliament in the 1896 federal election as a Conservative and in 1901 was selected by the Conservative caucus to succeed Sir Charles Tupper as leader of the Conservative Party. He was defeated in his Halifax seat in the 1904 federal election and re-entered the House of Commons the next year via a by-election in Carleton. Over the next decade he worked to rebuild the party and establish a reform policy, the Halifax Platform of 1907 which he described as "the most advanced and progressive policy ever put forward in Federal affairs". It called for reform of the Senate and the civil service, a more selective immigration policy, free rural mail delivery, and government regulation of telegraphs, telephones, and railways and eventually national ownership of telegraphs and telephones. Despite his efforts, his party lost the 1908 federal election to Wilfrid Laurier's Liberals. Borden was however elected again for Halifax. His party's fortunes turned around in the 1911 federal election, however, when the Conservatives successfully campaigned against Laurier's proposals for a Reciprocity (free trade) agreement with the United States. Borden countered with a revised version of John A. Macdonald's National Policy and appeals of loyalty to the British Empire and ran on the slogan "Canadianism or Continentalism". In British Columbia, the party ran on the slogan "A White Canada," playing to the fears of British Columbians that resented the increasing presence of cheap Asian labour and the resulting depression in wages. In Quebec, concurrently, Henri Bourassa led a campaign against what he saw as Laurier's capitulation to British imperialism, playing a part in the defeat of Laurier's government and the election of Borden's Tories.
Prime Minister (1911-1920).
First World War.
As Prime Minister of Canada during the First World War, he transformed his government to a wartime administration, passing the "War Measures Act" in 1914. Borden committed Canada to provide half a million soldiers for the war effort. However, volunteers had quickly dried up when Canadians realized there would be no quick end to the war. Borden's determination to meet that huge commitment led to the "Military Service Act" and the Conscription Crisis of 1917, which split the country on linguistic lines. In 1917 Borden recruited members of the Liberals (with the notable exception of leader Wilfrid Laurier) to create a Unionist government. The 1917 election saw the "Government" candidates (including a number of Liberal-Unionists) crush the Opposition "Laurier Liberals" in English Canada resulting in a large parliamentary majority for Borden.
Sir Robert Borden pledged himself during the campaign to equal suffrage for women. With his return to power, he introduced a bill in 1918 for extending the franchise to women. This passed without division.
The war effort also enabled Canada to assert itself as an independent power. Borden wanted to create a single Canadian army, rather than have Canadian soldiers split up and assigned to British divisions as had happened during the Boer War. Sam Hughes, the Minister of Militia, generally ensured that Canadians were well-trained and prepared to fight in their own divisions, although with mixed results such as the Ross Rifle. Arthur Currie provided sensible leadership for the Canadian divisions in Europe, although they were still under overall British command. Nevertheless, Canadian troops proved themselves to be among the best in the world, fighting at the Somme, Ypres, Passchendaele, and especially at the Battle of Vimy Ridge.
During Borden's first term as prime minister, the National Research Council of Canada was established in 1916.
Borden and the Treaty of Versailles.
In world affairs, Borden played a crucial role in transforming the British Empire into a partnership of equal states, the Commonwealth of Nations, a term that was first discussed at an Imperial Conference in London during the war. Borden also introduced the first Canadian income tax, which at the time was meant to be temporary, but was never repealed.
Convinced that Canada had become a nation on the battlefields of Europe, Borden demanded that it have a separate seat at the Paris Peace Conference. This was initially opposed not only by Britain but also by the United States, who perceived such a delegation as an extra British vote. Borden responded by pointing out that since Canada had lost a far larger proportion of its men compared to the U.S. in the war (although not more in absolute numbers), Canada at least had the right to the representation of a "minor" power. British Prime Minister David Lloyd George eventually relented, and convinced the reluctant Americans to accept the presence of separate Canadian, Indian, Australian, Newfoundland, New Zealand and South African delegations. Despite this, Borden boycotted the opening ceremony, protesting at the precedence given to the prime minister of the much smaller Newfoundland over him.
Not only did Borden's persistence allow him to represent Canada in Paris as a nation, it also ensured that each of the dominions could sign the Treaty of Versailles in its own right, and receive a separate membership in the League of Nations. During the conference Borden tried to act as an intermediary between the United States and other members of the British Empire delegation, particularly Australia and New Zealand over the issue of Mandates. Borden also discussed with Lloyd George, the possibility of Canada taking over the administration of Belize and the West Indies, but no agreement was reached.
At Borden's insistence, the treaty was ratified by the Canadian Parliament. Borden was the last prime minister to be knighted after the House of Commons indicated its desire for the discontinuation of the granting of any future titles to Canadians in 1919 with the adoption of the Nickle Resolution.
Post-war government.
That same year, Borden approved the use of troops to put down the Winnipeg General Strike, which was feared to be the result of Bolshevik agitation from the Soviet Union.
Post-political career.
Sir Robert Borden retired from office in 1920. He was the Chancellor of Queen's University from 1924 to 1930 and also was Chancellor of McGill University from 1918 to 1920 while still Prime Minister. Borden also served as Vice-President of The Champlain Society between 1923 and 1925. He was the Society's first Honorary President between 1925 and 1938. Borden's successor Arthur Meighen was defeated by the new Liberal leader William Lyon Mackenzie King in the 1921 election. Nevertheless, Borden would go on to represent Canada once more on the international stage when he attended the Washington Naval Conference in 1922 and signed the resulting arms reduction treaty on Canada's behalf.
At the time of his death, Borden stood as president of two financial institutions: Barclays Bank of Canada and the Crown Life Insurance Company. Borden died on June 10, 1937, in Ottawa and is buried in the Beechwood Cemetery marked by a simple stone cross.
Family.
Robert Laird Borden married Laura Bond, youngest daughter of the late T. H. Bond, September 1889. She served as president of the Local Council of Women of Halifax, until her resignation in 1901. She served as President of the Aberdeen Association, Vice-President of the Women's Work Exchange in Halifax, and Corresponding Secretary of the Associated Charities of the United States. He is a distant relative of American accused murderer Lizzie Borden.
Supreme Court appointments.
Borden chose the following jurists to sit as justices of the Supreme Court of Canada:
References.
By Sir Robert
By others

</doc>
<doc id="25781" url="https://en.wikipedia.org/wiki?curid=25781" title="Robot">
Robot

A robot is a mechanical or virtual artificial agent, usually an electro-mechanical machine that is guided by a computer program or electronic circuitry. Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's "Advanced Step in Innovative Mobility" (ASIMO) and TOSY's "TOSY Ping Pong Playing Robot" (TOPIO) to industrial robots, medical operating robots, patent assist robots, dog therapy robots, collectively programmed "swarm" robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own.
The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.
From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.
The word 'robot' was first used to denote a fictional humanoid in a 1921 play "R.U.R." by the Czech writer, Karel Čapek. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948. The first digital and programmable robot was invented by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.
Robots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea.
There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.
Summary.
The word "robot" can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to do some or all of the following: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior — especially behavior which mimics humans or other animals. Closely related to the concept of a "robot" is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.
History.
The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the "Lie Zi".
Early beginnings.
Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.
In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures." In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria , a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.
The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. 
In ancient China, the 3rd century text of the "Lie Zi" describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the "Han Fei Zi" and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds ("ma yuan") that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.
The beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.
In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his "Vitruvian Man". It is not known whether he attempted to build it.
In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century "Karakuri zui" ("Illustrated Machinery", 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the "Butai karakuri", which were used in theatre, the "Zashiki karakuri", which were small and used in homes, and the "Dashi karakuri" which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.
In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.
Remote-controlled systems.
Remotely operated vehicles were demonstrated in the late 19th Century in the form of several types of remotely controlled torpedos. The early 1870s saw remotely controlled torpedos by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).
The Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first "practical" guided missile". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.
Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.
Humanoid robots.
The term 'robot' was first used to denote fictional automata in a 1921 play "R.U.R." by the Czech writer, Karel Čapek. The word 'robot' is of Czech origin.
In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot Eric's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.
Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.
Modern autonomous robots.
The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named "Elmer" and "Elsie", were constructed between 1948 and 1949 and were often described as "tortoises" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.
Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's "turtles" may be found in the form of BEAM robotics.
The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol’s patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.
The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.
Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.
Future development and trends.
Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.
As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.
Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.
Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.
As early as 1982 people were confident that someday robots would:
1. clean parts by removing molding flash
2. spray paint automobiles with absolutely no human presence
3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes
4. make electrical cable harness
5. load trucks with boxes—a packing problem
6. handle soft goods, such as garments and shoes
7. shear sheep
8. prosthesis
9. cook fast food and work in other service industries
10. household robot.
Generally such predictions are overly optimistic in timescale.
New functionalities and prototypes.
In 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving. 
A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.
Baxter is a new robot which is different from other industrial robots because it can learn. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. 
Etymology.
The word "robot" was introduced to the public by the Czech interwar writer Karel Čapek in his play "R.U.R. (Rossum's Universal Robots)", published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called "robots." The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).
Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the "Oxford English Dictionary" in which he named his brother, the painter and writer Josef Čapek, as its actual originator.
In an article in the Czech journal "Lidové noviny" in 1933, he explained that he had originally wanted to call the creatures "laboři" ("workers", from Latin "labor"). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word "robota" means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as "robot" in Hungarian). Traditionally the "robota" (Hungarian "robot") was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) "rabota" "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root "*orbh-". "Robot" is cognate with the German root "Arbeit" (work).
The word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the ""Three Laws of Robotics"" which are a recurring theme in his books. These have since been used by many others to define laws used in fact and fiction.
Modern robots.
Mobile robot.
Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the "automated guided vehicle" or "automatic guided vehicle" (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.
Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.
Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.
Industrial robots (manipulating).
Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.
The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:
"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."
This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees. The RIA subdivides robots into four classes: devices that manipulate objects with manual control, automated devices that manipulate objects with predetermined cycles, programmable and servo-controlled robots with continuous point-to-point trajectories, and robots of this last type which also acquire information from the environment and move intelligently in response. -->
Service robot.
Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."
Educational robot.
Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.
There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.
There have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.
Modular robot.
Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These “ANAT robots” can be designed with “n” DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.
Modular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.
Collaborative robots.
A "collaborative robot" or "cobot" is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.
The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.
Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. , 190 companies in the US have bought Baxters and they are being used commercially in the UK.
Robots in society.
Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.
Autonomy and ethical questions.
As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called "Plug & Pray".
Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.
In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.
Military robots.
Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.
One robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.
Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.
Relationship to unemployment.
A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.
Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to revise redundancy laws.
Contemporary uses.
At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.
Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. Of course, all robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.
General-purpose autonomous robots.
General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.
Factory robots.
Car production.
Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.
Packaging.
Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.
Electronics.
Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.
Automated guided vehicles (AGVs).
Mobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.
Early AGV-style robots.
Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.
Interim AGV technologies.
Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.
Intelligent AGVs (i-AGVs).
Such as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.
Dirty, dangerous, dull or inaccessible tasks.
There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.
Space probes.
Almost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, and others.
Telerobots.
Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).
Automated fruit harvesting machines.
Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.
Domestic robots.
Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.
Military robots.
Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.
Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.
The AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.
Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies." Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.
Mining robots.
Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous vehicle fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia.
Drilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.
Healthcare.
Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.
Home automation for the elderly and disabled.
Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.
The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.
FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.
Pharmacies.
Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient’s medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it’s the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.
McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.
Research robots.
While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.
Bionic and biomimetic robots.
One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.
Nanorobots.
Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.
Reconfigurable robots.
A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.
Soft-bodied robots.
Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.
Swarm robots.
Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.
Haptic interface robots.
Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.
Robots in popular culture.
Literature.
Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.
The first reference in Western literature to mechanical servants appears in Homer's "Iliad". In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." Of course, the words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).
Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.
According to the "Oxford English Dictionary," the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word "robotics". Asimov was not initially aware of this; he assumed the word already existed by analogy with "mechanics," "hydraulics," and other similar terms denoting branches of applied knowledge.
Cinema.
Robots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the "Star Wars" franchise.
Problems depicted in popular culture.
Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. (See "The Mechanical Man", "The Terminator, Runaway, RoboCop", the Replicators in "Stargate", the Cylons in "Battlestar Galactica", the Cybermen and Daleks in "Doctor Who", "The Matrix", "Enthiran" and "I, Robot".) Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are "", "Red Planet" and "Enthiran". Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely. "Frankenstein" (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or monster advancing beyond its creator. In the TV show, Futurama, the robots are portrayed as humanoid figures that live alongside humans, not as robotic butlers. They still work in industry, but these robots carry out daily lives. Other problems may include events pertaining to robot surrogates (e.g. the movie "Surrogates") where tissue of living organisms is interchanged with robotic systems. These problems can leave many possibilities where electronic viruses or an electro magnetic pulse (EMP) can destroy not only the robot but kill the host/operator as well.

</doc>
<doc id="25783" url="https://en.wikipedia.org/wiki?curid=25783" title="R. B. Bennett">
R. B. Bennett

Richard Bedford Bennett, 1st Viscount Bennett, (3 July 1870 – 26 June 1947) was a Canadian lawyer, businessperson, politician, and philanthropist. He served as the 11th Prime Minister of Canada from 7 August 1930 to 23 October 1935, during the worst of the Great Depression years. Following his defeat as prime minister, Bennett moved to England, and was elevated to the peerage as Viscount Bennett.
Most historians consider his premiership to have been a failure at a time of severe economic crisis. Blair Neatby says categorically that "as a politician he was a failure". J. L. Granatstein and Norman Hillmer, comparing him to all other Canadian prime ministers concluded, "Bennett utterly failed as a leader. Everyone was alienated by the end—Cabinet, caucus, party, voter and foreigner."
Early life.
R. B. Bennett was born on 3 July 1870, when his mother, Henrietta Stiles, was visiting at her parents' home in Hopewell Hill, New Brunswick, Canada. He grew up nearby at the home of his father, Henry John Bennett, at Hopewell Cape, the shire town of Albert County, then a town of 1,800 people.
His father was descended from English ancestors who had emigrated to Connecticut in the 17th century. His great-great-grandfather Bennett migrated from Connecticut to Nova Scotia c. 1765, before the American Revolution, taking the lands forcibly removed from the deported Acadians during the Great Upheaval.
R. B. Bennett's family was poor, subsisting mainly on the produce of a small farm. His early days inculcated a lifelong habit of thrift. The driving force in his family was his mother. She was a Wesleyan Methodist and passed this faith and the Protestant ethic on to her son. His principle ever after was: work as hard as you can, earn all you can, save all you can, and then give all you can. Bennett's father does not appear to have been a good provider for his family, though the reason is unclear. He operated a general store for a while and tried to develop some gypsum deposits.
The Bennetts had previously been a relatively prosperous family, operating a shipyard in Hopewell Cape, but the change to steam-powered vessels in the mid-19th century meant the gradual winding down of their business. However, the household was a literate one, subscribing to three newspapers. They were strong Conservatives; indeed one of the largest and last ships launched by the Bennett shipyard (in 1869) was the "Sir John A. Macdonald".
Educated in the local school, Bennett was a good student, but something of a loner. In addition to his Protestant faith, Bennett grew up with an abiding love of the British Empire, then at its apogee.
Some important friendships.
One day, while Bennett was crossing the Miramichi River on the ferry boat, a well-dressed lad about nine years younger came over to him and struck up a conversation. This was the beginning of an improbable but important friendship with Max Aitken, later the industrialist and British press baron, Lord Beaverbrook. The agnostic Aitken liked to tease the Methodist Bennett, whose fiery temper contrasted with Aitken's ability to turn away wrath with a joke. This friendship would become important to his success later in life, as would his friendship with the Chatham lawyer, Lemuel J. Tweedie, a prominent Conservative politician. He began to study law with Tweedie on weekends and during summer holidays. Another important friendship was with the prominent Shirreff family of Chatham, the father being High Sheriff of Northumberland County for 25 years. The son, Harry, joined the E.B. Eddy Company, a large pulp and paper industrial concern, and was transferred to Halifax. His sister moved there to study nursing, and soon Bennett joined them to study law at Dalhousie University. Their friendship was renewed there, and became crucial to his later life when Jennie Shirreff married the head of the Eddy Company. She later made Bennett the lawyer for her extensive interests.
University, early legal career.
Bennett started at Dalhousie University in 1890, graduating in 1893 with a law degree. He worked his way through with a job as assistant in the library, being recommended by Dr. R. C. Weldon.
He was then a partner in the Chatham law firm of Tweedie and Bennett. Max Aitken (later to become Lord Beaverbrook) was his office boy, while articling as a lawyer, acting as a stringer for the Montreal Gazette, and selling life insurance. Aitken persuaded him to run for alderman in the first Town Council of Chatham, and managed his campaign. Bennett was elected by one vote, and was later furious with Aitken when he heard all the promises he had made on Bennett's behalf.
Moving west.
Despite his election to the Chatham town council, Bennett's days in the town were numbered. He was ambitious and saw that the small community was too narrow a field for him. He was already negotiating with Sir James Lougheed to move to the North-West Territories and become his law partner in Calgary. Lougheed was Calgary's richest man and most successful lawyer.
Bennett moved to Calgary in 1897. A lifelong bachelor and teetotaler (although Bennett was known by select associates to occasionally drink alcohol when the press was not around to observe this), he led a rather lonely life in a hotel and later, in a boarding house. For a while a younger brother roomed with him. He ate his noon meal on workdays at the Alberta Hotel. Social life, such as it was, centred on church. There was, however, no scandal attached to his personal life. Bennett worked hard and gradually built up his legal practice. In 1908 he was one of five people appointed to the first Library Board for the city of Calgary and was instrumental in establishing the Calgary Public Library.
In 1910, Bennett became a director of Calgary Power Ltd. (now formally TransAlta Corporation) and just a year later he became President. During his leadership projects completed included the first storage reservoir at Lake Minnewanka, a second transmission line to Calgary and the construction of the Kananaskis Falls hydro station. At that time, he was also director of Rocky Mountains Cement Company and Security Trust.
Bennett developed an extensive legal practice in Calgary. In 1929-30, he served as national President of the Canadian Bar Association. His successor in that office was Louis St. Laurent, another future Prime Minister.
Early political career.
He was elected to the Legislative Assembly of the North-West Territories in the 1898 general election, representing the riding of West Calgary. He was re-elected to a second term in office in 1902 as an Independent in the North-West Territories legislature.
In 1905, when Alberta was carved out of the territories and made a province, Bennett became the first leader of the Alberta Conservative Party. In 1909, he won a seat in the provincial legislature, before switching to federal politics.
Elected to the Canadian House of Commons in 1911, Bennett returned to the provincial scene to again lead the Alberta Tories in the 1913 provincial election, but kept his federal seat in Ottawa when his Tories failed to take power in the province; such practice was later forbidden.
At age 44, he tried to enlist in the Canadian military once World War I broke out, but was turned down as being medically unfit. In 1916, Bennett was appointed director general of the National Service Board, which was in charge of identifying the number of potential recruits in the country.
While Bennett supported the Conservatives, he opposed Prime Minister Robert Borden's proposal for a Union Government that would include both Conservatives and Liberals, fearing that this would ultimately hurt the Conservative Party. While he campaigned for Conservative candidates in the 1917 federal election he did not stand for re-election himself.
Cabinet minister, Conservative party leader.
Nevertheless, Borden's successor, Arthur Meighen appointed Bennett Minister of Justice in his government, as it headed into the 1921 federal election in which both the government and Bennett were defeated. Bennett won the seat of Calgary West in the 1925 federal election and was returned to government as Minister of Finance in Meighen's short-lived government in 1926. The government was defeated in the 1926 federal election. Meighen stepped down as Tory leader, and Bennett became the party's leader in 1927 at the first Conservative leadership convention.
As Opposition leader, Bennett faced off against the more experienced Liberal Prime Minister William Lyon Mackenzie King in Commons debates, and took some time to acquire enough experience to hold his own with King. In 1930, King blundered badly when he made overly partisan statements in response to criticism over his handling of the economic downturn, which was hitting Canada very hard. King's worst error was in stating that he "would not give Tory provincial governments a five-cent piece!" This serious mistake, which drew wide press coverage, gave Bennett his needed opening to attack King, which he did successfully in the election campaign which followed.
Prime minister (1930-1935).
Confronting the Depression.
By defeating William Lyon Mackenzie King in the 1930 federal election, he had the misfortune of taking office during the Great Depression. Bennett tried to combat the depression by increasing trade within the British Empire and imposing tariffs for imports from outside the Empire, promising that his measures would "blast" Canadian exports into world markets. His success was limited however, and his own wealth (often openly displayed) and impersonal style alienated many struggling Canadians.
When his "Imperial Preference" policy failed to generate the desired result, Bennett's government had no real contingency plan. The party's pro-business and pro-banking inclinations provided little relief to the millions of increasingly desperate and agitated unemployed. Despite the economic crisis, "laissez-faire" persisted as the guiding economic principle of Conservative Party ideology. Government relief to the unemployed was considered a disincentive to individual initiative, and was therefore only granted in the most minimal amounts and attached to work programs. An additional concern of the federal government was that large numbers of disaffected unemployed men concentrating in urban centres created a volatile situation. As an "alternative to bloodshed on the streets", the stop-gap solution for unemployment chosen by the Bennett government was to establish military-run and -styled relief camps in remote areas throughout the country, where single unemployed men toiled for twenty cents a day. Any relief beyond this was left to provincial and municipal governments, many of which were either insolvent or on the brink of bankruptcy, and which railed against the inaction of other levels of government. Partisan differences began to sharpen on the question of government intervention in the economy, since lower levels of government were largely in Liberal hands, and protest movements were beginning to send their own parties into the political mainstream, notably the Cooperative Commonwealth Federation and William Aberhart's Social Credit Party in Alberta.
Hosts, dominates 1932 Imperial Conference.
Bennett hosted the 1932 Imperial Economic Conference in Ottawa; this was the first time Canada had hosted the meetings. It was attended by the leaders of the independent dominions of the British Empire (which later became the Commonwealth of Nations). Bennett dominated the meetings, which were ultimately unproductive, due to the inability of leaders to agree on policies, mainly to combat the economic woes dominating the world at the time.
Anti-Communism.
A nickname that would stick with Bennett for the remainder of his political career, "Iron Heel Bennett", came from a 1932 speech he gave in Toronto that ironically, if unintentionally, alluded to Jack London's socialist novel:
What do they offer you in exchange for the present order? Socialism, Communism, dictatorship. They are sowing the seeds of unrest everywhere. Right in this city such propaganda is being carried on and in the little out of the way places as well. And we know that throughout Canada this propaganda is being put forward by organizations from foreign lands that seek to destroy our institutions. And we ask that every man and woman put the iron heel of ruthlessness against a thing of that kind. 
Reacting to fears of Communist subversion, Bennett invoked the controversial Section 98 of the Criminal Code. Enacted in the aftermath of the Winnipeg General Strike, section 98 dispensed with the presumption of innocence in outlawing potential threats to the state: specifically, anyone belonging to an organization that officially advocated the violent overthrow of the government. Even if the accused had never committed an act of violence or personally supported such an action, they could be incarcerated merely for attending meetings of such an organization, publicly speaking in its defense, or distributing its literature. Despite the broad power authorized under section 98, it targeted specifically the Communist Party of Canada. Eight of the top party leaders, including Tim Buck, were arrested and convicted under section 98 in 1931. This plan to stamp out communism backfired, however, and proved to be a damaging embarrassment for the government, especially after Buck was the target of an apparent assassination attempt. While confined to his cell during a prison riot, despite not participating in the riot, shots were fired into his cell. When an agit-prop play depicting these events, "Eight Men Speak", was suppressed by the Toronto police, a protest meeting was held where activist A.E. Smith repeated the play's allegations, and he was consequently arrested for sedition. This created a storm of public protest, compounded when Buck was called as a witness to the trial and repeated the allegations in open court. Although the remarks were stricken from the record, they still discredited the prosecution's case and Smith was acquitted. As a result, the government's case against Buck lost any credibility, and Buck and his comrades were released early and fêted as heroic champions of civil liberties.
A 2001 book by Quebec nationalist writer Normand Lester, "Le Livre noir du Canada anglais" (later translated as "The Black Book of English Canada") accused Bennett of having a political affiliation with, and of having provided financial support to, fascist Quebec writer Adrien Arcand. This is based on a series of letters sent to Bennett following his election as Prime Minister by Arcand, his colleague Ménard and two Conservative caucus members asking for financial support for Arcand's antsemitic newspaper "Le Goglu". The book also claims that in a 1936 letter to Bennett, A. W. Reid, a Conservative organizer, estimated that Conservative Party members gave Arcand a total of $27,000 (the modern equivalent $359,284).
Relief camp protest.
Having survived section 98, and benefiting from the public sympathy wrought by persecution, Communist Party members set out to organize workers in the relief camps. Camp workers laboured on a variety of infrastructure projects, including such things as municipal airports, roads, and park facilities, along with a number of make-work schemes. Conditions in the camps were abhorrent, not only because of the low pay, but the lack of recreational facilities, isolation from family and friends, poor quality food, and the use of military discipline, which made the camps feel like penal colonies. Communists thus had ample grounds on which to organize camp inmates. The Relief Camp Workers' Union was formed and affiliated with the Workers' Unity League, the trade union umbrella of the Communist Party. Camp workers in BC struck on 4 April 1935, and, after two months of protesting in Vancouver, began the On-to-Ottawa Trek to bring their grievances to Bennett's doorstep. The Prime Minister and his Minister of Justice, Hugh Guthrie, treated the trek as an attempted insurrection, and ordered it to be stopped. The Royal Canadian Mounted Police (RCMP) halted the Trek in Regina on 1 July 1935, by attacking a crowd of 3,000 strikers and their supporters, resulting in two deaths and dozens of injured. All told, Bennett's anti-Communist policy would not bode well for his political career.
Bennett's New Deal.
Following the lead of President Roosevelt's New Deal in the United States, Bennett, under the advice of William Duncan Herridge, who was Canada's Envoy to the United States, the government eventually began to follow the Americans' lead. In a series of five radio speeches to the nation in January 1935, Bennett introduced a Canadian version of the "New Deal," involving unprecedented public spending and federal intervention in the economy. Progressive income taxation, a minimum wage, a maximum number of working hours per week, unemployment insurance, health insurance, an expanded pension programme, and grants to farmers were all included in the plan.
In one of his addresses to the nation, Bennett said:
Bennett's conversion, however, was seen as too little too late, and he faced criticism that his reforms either went too far, or did not go far enough, including from one of his cabinet ministers H. H. Stevens, who bolted the government to form the Reconstruction Party of Canada. Some of the measures were alleged to have encroached on provincial jurisdictions laid out in section 92 of the British North America Act. The courts, including the Judicial Committee of the Privy Council, agreed and eventually struck down virtually all of Bennett's reforms. However, some of Bennett's initiatives, such as the Bank of Canada, which he founded in 1934, remain in place to this day, and the Canadian Wheat Board remained in place until 2011 when the government of Stephen Harper abolished it.
Defeat.
Although there was no unity among the motley political groups that constituted Bennett's opposition, a consensus emerged that his handling of the economic crisis was insufficient and inappropriate, even from Conservative quarters. Bennett personally became a symbol of the political failings underscoring the depression. Car owners, for example, who could no longer afford gasoline, had horses pull their vehicles, named them Bennett Buggies. Unity in his own administration suffered, notably by the defection of his Minister of Trade, Henry Herbert Stevens. Stevens left the Conservatives and formed the Reconstruction Party of Canada, after Bennett refused to implement Stevens' plan for drastic economic reform to deal with the economic crisis.
The beneficiary of the overwhelming opposition during Bennett's tenure was the Liberal Party. The Tories were decimated in the October 1935 general election, winning only 40 seats to 173 for Mackenzie King's Liberals. The Tories would not form a majority government again in Canada until 1958. King's government soon implemented its own moderate reforms, including the replacement of relief camps with a scaled down provincial relief project scheme, and the repeal of section 98. King had earlier outlined his plans with his 1918 book "Industry and Economy". Many of King's other reforms continue today, including the Canadian Broadcasting Corporation, the nationalized Bank of Canada, versions of minimum wage, maximum hours of work, pension, and unemployment insurance legislation. Ultimately, Canada pulled out of the depression as a result of government-funded jobs associated with the preparation for and onset of the Second World War.
Coat of arms.
Bennett's Coat of Arms was designed by Alan Beddoe "Argent within two bendlets Gules three maple leaves proper all between two demi-lions rampant couped gules. Crest, a demi-lion Gules grapsing in the dexter paw a battle axe in bend sinister Or and resting the sinister paw on an escallop also Gules. Supporters, Dexter a buffalo, sinister a moose, both proper. Motto, To be Pressed not Oppressed." 
Legacy.
While Bennett was, and is still, often criticized for lack of compassion for the impoverished masses, he stayed up through many nights reading and responding to personal letters from ordinary citizens asking for his help, and often dipped into his personal fortune to send a five-dollar bill to a starving family. The total amount he gave personally is uncertain, although he personally estimated that in 1927–37 he spent well over 2.3 million dollars. Bennett was a controlling owner of the E. B. Eddy match company, which was the largest safety match manufacturer in Canada, and he was one of the richest Canadians at that time. Bennett helped put many poor, struggling young men through university. Relative to the times he lived in, he was likely the wealthiest Canadian to become prime minister.
Bennett worked an exhausting schedule throughout his years as prime minister, often more than 14 hours per day, and dominated his government, usually holding several cabinet posts. He lived in a suite in the Chateau Laurier hotel, a short walk from Parliament Hill. The respected author Bruce Hutchison wrote that had the economic times been more normal, Bennett would likely have been regarded as a good, perhaps great, Canadian prime minister.
Bennett was also a noted talent spotter. He took note of and encouraged the young Lester Pearson in the early 1930s, and appointed Pearson to significant roles on two major government inquiries: the 1931 Royal Commission on Grain Futures, and the 1934 Royal Commission on Price Spreads. Bennett saw that Pearson was recognized with an OBE after he shone in that work, arranged a bonus of $1,800, and invited him to a London conference. Former Prime Minister John Turner, who as a child, knew Bennett while he was prime minister, praised Bennett's promotion of Turner's economist mother to the highest civil service post held by a Canadian woman to that time.
Retirement and death.
Bennett retired to Britain in 1938, and, on 12 June 1941, became the first and only former Canadian Prime Minister to be elevated to the peerage as Viscount Bennett of Mickleham in the County of Surrey and of Calgary and Hopewell in the Dominion of Canada. Bennett's interest in increasing public awareness and accessibility to Canada's historical records, led him to serve as Vice-President of The Champlain Society from 1933 until his death.
He died after suffering a heart attack while taking a bath on 26 June 1947 at Mickleham. He was exactly one week shy of his 77th birthday. He is buried there in St. Michael's Churchyard, Mickleham. The tomb, and Government of Canada marker outside, are steps from the front doors of the church. He is the only deceased former Canadian Prime Minister not buried in Canada. Unmarried, Bennett was survived by nephews William Herridge, Jr., and Robert Coats and by brother Ronald V. Bennett. The viscountcy became extinct on his death.
Bennett was ranked #12 by a survey of Canadian historians out of the then 20 Prime Ministers of Canada through Jean Chrétien. The results of the survey were included in the book "Prime Ministers: Ranking Canada's Leaders" by J. L. Granatstein and Norman Hillmer.
Supreme Court appointments.
Bennett chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:
Other appointments.
Bennett was the Honorary Colonel of The Calgary Highlanders from the year of their designation as such in 1921 to his death in 1947. He visited the Regiment in England during the Second World War, and always ensured the 1st Battalion had a turkey dinner at Christmas every year they were overseas, including the Christmas of 1944 when the battalion was holding front line positions in the Nijmegen Salient.
Bennett served as the Rector of Queen's University in Kingston, Ontario from 1935 to 1937, even while he was still prime minister. At the time, this role covered mediation for significant disputes between Queen's students and the university administration.
Electoral record.
Northwest Territories (West Calgary).
The by-election was caused by the resignation of Richard Bennett, who resigned his seat to run for the Canadian House of Commons in the 1900 Canadian federal election.

</doc>
<doc id="25784" url="https://en.wikipedia.org/wiki?curid=25784" title="Renewable energy">
Renewable energy

[[File:Global public support for energy sources (Ipsos 2011).png|thumb|alt=a survey by isos shows that global support is strongest for solar and wind, followed by (in declining order) hydro, natural gas, coal and nuclear|Global public support for different energy sources (2011) based on a poll by Ipsos Global @dvisor]]
Renewable energy is generally defined as energy that is collected from resources which are naturally replenished on a human timescale, such as sunlight, wind, rain, tides, waves, and geothermal heat. Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.
Based on REN21's 2014 report, renewables contributed 19 percent to humans' global energy consumption and 22 percent to their generation of electricity in 2012 and 2013, respectively. This energy consumption is divided as 9% coming from traditional biomass, 4.2% as heat energy (non-biomass), 3.8% hydro electricity and 2% is electricity from wind, solar, geothermal, and biomass. Worldwide investments in renewable technologies amounted to more than US$214 billion in 2013, with countries like China and the United States heavily investing in wind, hydro, solar and biofuels.
Renewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. The results of a recent review of the literature concluded that as greenhouse gas(GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. Some places and at least two countries, Iceland and Norway generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future. For example, in Denmark the government decided to switch the total energy supply (electricity, mobility and heating/cooling) to 100% renewable energy by 2050.
While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. As most of renewables provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: For example, electricity can be converted to heat without losses and even reach higher temperatures than fossil fuels, can be converted into mechanical energy with high efficiency and is clean at the point of consumpion. In addition to that electrification with renewable energy is much more efficient and therefore leads to a significant reduction in primary energy requirements, because most renewables don't have a steam cycle with high losses (fossil power plants usually have losses of 40 to 65%).
Overview.
Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:
Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits. It would also reduce environmental pollution such as air pollution caused by burning of fossil fuels and improve public health, reduce premature mortalities due to pollution and save associated health costs that amount to several 100 billion dollars annually only in the United States. Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the sun is expected to make the surface of the earth too hot for liquid water to exist.
Climate change and global warming concerns, coupled with high oil prices, peak oil, and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors. According to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment.
As of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.
Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services:
History.
Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from 790,000 years ago. Use of biomass for fire did not become commonplace until many hundreds of thousands of years later, sometime between 200,000 and 400,000 years ago. Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships on the Nile. Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass. A graph of energy use in the United States up until 1900 shows oil and natural gas with about the same importance in 1900 as wind and solar played in 2010.
In the 1860s and '70s there were already fears that civilization would run out of fossil fuels and the need was felt for a better source. In 1873 Professor Augustine Mouchot wrote:
The time will arrive when the industry of Europe will cease to find those natural resources, so necessary for it. Petroleum springs and coal mines are not inexhaustible but are rapidly diminishing in many places. Will man, then, return to the power of water and wind? Or will he emigrate where the most powerful source of heat sends its rays to all? History will show what will come.
In 1885, Werner von Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:
In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.
Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus, published in 1905.
Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 "Scientific American" article: "in the far distant future, natural fuels having been exhausted power will remain as the only means of existence of the human race".
The theory of peak oil was published in 1956. In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.
The IEA 2014 World Energy Outlook projects a growth of renewable energy supply from 1,700 gigawatts in 2014 to 4,550 gigawatts in 2040. Fossil fuels received about $550 billion in subsidies in 2013, compared to $120 billion for all renewable energies.
Mainstream technologies.
Wind power.
Airflows can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 5 MW of rated power, although turbines with rated output of 1.5–3 MW have become the most common for commercial use; the power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. Areas where winds are stronger and more constant, such as offshore and high altitude sites, are preferred locations for wind farms. Typically full load hours of wind turbines vary between 16 and 57 percent annually, but might be higher in particularly favorable offshore sites.
Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land stationed turbines. In 2013 wind generated almost 3% of the worlds total electricity.
Hydropower.
In 2013 hydropower generated almost 16% of the worlds total electricity. Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:
Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. For counties having the largest percentage of electricity from renewables, the top 50 are primarily hydroelectric. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.
Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, has currently no economic feasibility.
Solar energy.
Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics (CPV), solar architecture and artificial photosynthesis. Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).
A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect. Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP. Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.
In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".
Geothermal energy.
High Temperature Geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective "geothermal" originates from the Greek roots "geo", meaning earth, and "thermos", meaning heat.
The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to .
From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation.
Low Temperature Geothermal refers to the use of the outer crust of the earth as a Thermal Battery to facilitate Renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of Geothermal, a Geothermal Heat Pump and Ground-coupled heat exchanger are used together to move heat energy into the earth (for cooling) and out of the earth (for heating) on a varying seasonal basis. Low temperature Geothermal (generally referred to as "GHP") is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus Low Temperature Geothermal/GHP is becoming an increasing national priority with multiple tax credit support and focus as part of the ongoing movement toward Net Zero Energy. New York City has even just passed a law to require GHP anytime is shown to be economical with 20 year financing including the Socialized Cost of Carbon.
Bio energy.
Biomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: "thermal", "chemical", and "biochemical" methods. Wood remains the largest biomass energy source today; examples include forest residues – such as dead trees, branches and tree stumps –, yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).
Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. Some examples of these plants are wheat, which typically yield 7.5–8 tonnes of grain per hectare, and straw, which typically yield 3.5–5 tonnes per hectare in the UK. The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first generation biofuel.
Biomass can be converted to other usable forms of energy like methane gas or transportation fuels like ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gasalso called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products like vegetable oils and animal fats. Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research. There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it's a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the United Kingdom.
Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels. Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcoho] made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.
With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns. Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.
Biomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SOx), nitrous oxides (NOx), and particulate matter (PM) are produced from the combustion of biomass; the World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution. Biomass combustion is a major contributor. The life cycle of the plants is sustainable, the lives of people less so.
Heat pump.
A heat pump is a device that provides heat energy from a source of heat to a destination called a "heat sink". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A heat pump uses some amount of external power to accomplish the work of transferring energy from the heat source to the heat sink.
While air conditioners and freezers are familiar examples of heat pumps, the term "heat pump" is more general and applies to many HVAC (heating, ventilating, and air conditioning) devices used for space heating or space cooling. When a heat pump is used for heating, it employs the same basic refrigeration-type cycle used by an air conditioner or a refrigerator, but in the opposite direction - releasing heat into the conditioned space rather than the surrounding environment. In this use, heat pumps generally draw heat from the cooler external air or from the ground. In heating mode, heat pumps are three to four times more efficient in their use of electric power than simple electrical resistance heaters.
It has been concluded that heat pumps are the single technology that could reduce the greenhouse gas emissions of households better than every other technology that is available on the market. With a market share of 30% and (potentially) clean electricity, heat pumps could reduce global CO2 emissions by 8% annually. Using ground source heat pumps could reduce around 60% of the primary energy demand and 90% of CO2 emissions in Europe in 2050 and make handling high shares of renewable energy easier. Using surplus renewable energy in heat pumps is regarded as the most effective household means to reduce to reduce global warming and fossil fuel depletion.
Energy storage.
Energy storage is a collection of methods used to store electrical energy on an electrical power grid, or off it. Electrical energy is stored during times when production (especially from intermittent power plants such as renewable electricity sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption.
Market and industry trends.
Growth of renewables.
From the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. For wind power and many other renewable technologies, growth accelerated in 2009 relative to the previous four years. More wind power capacity was added during 2009 than any other renewable technology. However, grid-connected PV increased the fastest of all renewables technologies, with a 60% annual average growth rate. In 2010, renewable power constituted about a third of the newly built power generation capacities.
Projections vary, but scientists have advanced a plan to power 100% of the world's energy with wind, hydroelectric, and solar power by the year 2030.
According to a 2011 projection by the International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing the emissions of greenhouse gases that harm the environment. Cedric Philibert, senior analyst in the renewable energy division at the IEA said: "Photovoltaic and solar-thermal plants may meet most of the world's demand for electricity by 2060and half of all energy needswith wind, hydropower and biomass plants supplying much of the remaining generation". "Photovoltaic and concentrated solar power together can become the major source of electricity", Philibert said.
In 2014 global wind power capacity expanded 16% to 369,553 MW. Yearly wind energy production is also growing rapidly and has reached around 4% of worldwide electricity usage, 11.4% in the EU, and it is widely used in Asia, and the United States. In 2014, worldwide installed photovoltaics capacity increased to 177 gigawatts (GW), sufficient to supply 1 percent of global electricity demands. Solar thermal energy stations operate in the USA and Spain, and as of 2016, the largest of these is the 392 MW Ivanpah Solar Electric Generating System in California. The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the USA.
Economic trends.
Renewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2011 IEA report said: "A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support," and added that "cost reductions in critical technologies, such as wind and solar, are set to continue."
Hydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where "oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today". A series of studies by the US National Renewable Energy Laboratory modeled the "grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power." In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of "between $0.47 and $1.28 to each MegaWatt hour generated"; however, the savings in the cost of the fuels saved "adds up to $7 billion, meaning the added costs are, at most, two percent of the savings."
Hydroelectricity.
Only 25% of the worlds estimated hydroelectric potential of 14,000 TWh/year has been developed, with Africa, Asia and Latin America having the greatest potential. The Three Gorges Dam in Hubei, China, has the world's largest instantaneous generating capacity (22,500 MW), with the Itaipu Dam in Brazil/Paraguay in second place (14,000 MW). The Three Gorges Dam is operated jointly with the much smaller Gezhouba Dam (3,115 MW). , the total generating capacity of this two-dam complex is 25,615 MW. In 2008, this complex generated 98 TWh of electricity (81 TWh from the Three Gorges Dam and 17 TWh from the Gezhouba Dam), which is 3% more power in one year than the 95 TWh generated by Itaipu in 2008.
Wind power development.
Wind power is widely used in Europe, China, and the United States. From 2004 to 2014, worldwide installed capacity of wind power has been growing from 47 GW to 369 GW—a more than sevenfold increase within 10 years with 2014 breaking a new record in global installations (51 GW). As of the end of 2014, China, the United States and Germany combined accounted for half of total global capacity. Several other countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, and 14% in Ireland in 2010 and have since continued to expand their installed capacity. More than 80 countries around the world are using wind power on a commercial basis.
Solar thermal.
The United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.
The oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California. The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW. The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.
The solar thermal power industry is growing rapidly with 1.3 GW under construction in 2012 and more planned. Spain is the epicenter of solar thermal power development with 873 MW under construction, and a further 271 MW under development. In the United States, 5,600 MW of solar thermal power projects have been announced. Several power plants have been constructed in the Mojave Desert, Southwestern United States. The Ivanpah Solar Power Facility being the most recent. In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.
Photovoltaic development.
Photovoltaics (PV) uses solar cells assembled into solar panels to convert sunlight into electricity. It's a fast-growing technology doubling its worldwide installed capacity every couple of years. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and has reached grid parity in at least 30 different markets by 2014. Financial institutions are predicting a second solar "gold rush" in the near future.
At the end of 2014, worldwide PV capacity reached at least 177,000 megawatts. Photovoltaics grew fastest in China, followed by Japan and the United States, while Germany remains the world's largest overall producer of photovoltaic power, contributing about 7.0 percent to the overall electricity generation. Italy meets 7.9 percent of its electricity demands with photovoltaic power—the highest share worldwide. For 2015, global cumulative capacity is forecasted to increase by more than 50 gigawatts (GW). By 2018, worldwide capacity is projected to reach as much as 430 gigawatts. This corresponds to a tripling within five years. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.
Photovoltaic power stations.
Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Solar PV is rapidly becoming an inexpensive, low-carbon technology to harness renewable energy from the Sun.
Many solar photovoltaic power stations have been built, mainly in Europe, China and the USA. The 579 MW Solar Star, in the United States, is the world's largest PV power station.
Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems. There are no fuel costs or emissions during operation of the power stations.
However, when it comes to renewable energy systems and PV, it is not just large systems that matter. Building-integrated photovoltaics or "onsite" PV systems use existing land and structures and generate power close to where it is consumed.
Carbon-neutral and negative fuels.
Carbon-neutral fuels are synthetic fuels (including methane, gasoline, diesel fuel, jet fuel or ammonia) produced by hydrogenating waste carbon dioxide recycled from power plant flue-gas emissions, recovered from automotive exhaust gas, or derived from carbonic acid in seawater. Such fuels are considered carbon-neutral because they do not result in a net increase in atmospheric greenhouse gases. To the extent that synthetic fuels displace fossil fuels, or if they are produced from waste carbon or seawater carbonic acid, and their combustion is subject to carbon capture at the flue or exhaust pipe, they result in negative carbon dioxide emission and net carbon dioxide removal from the atmosphere, and thus constitute a form of greenhouse gas remediation.
Such renewable fuels alleviate the costs and dependency issues of imported fossil fuels without requiring either electrification of the vehicle fleet or conversion to hydrogen or other fuels, enabling continued compatible and affordable vehicles. Carbon-neutral fuels offer relatively low cost energy storage, alleviating the problems of wind and solar intermittency, and they enable distribution of wind, water, and solar power through existing natural gas pipelines. Nighttime wind power is considered the most economical form of electrical power with which to synthesize fuel, because the load curve for electricity peaks sharply during the warmest hours of the day, but wind tends to blow slightly more at night than during the day, so, the price of nighttime wind power is often much less expensive than any alternative. Germany has built a 250 kilowatt synthetic methane plant which they are scaling up to 10 megawatts.
The George Olah carbon dioxide recycling plant in Grindavík, Iceland has been producing 2 million liters of methanol transportation fuel per year from flue exhaust of the Svartsengi Power Station since 2011. It has the capacity to produce 5 million liters per year.
Biofuel development.
Biofuels provided 3% of the world's transport fuel in 2010. Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.
Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump. Unfortunately, Operation Car Wash has seriously eroded public trust in oil companies and has implicated several high ranking Brazilian officials.
Nearly all the gasoline sold in the United States today is mixed with 10% ethanol, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell "flexible-fuel" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol. By mid-2006, there were approximately 6 million ethanol compatible vehicles on U.S. roads.
Geothermal development.
Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.
The International Geothermal Association (IGA) has reported that 10,715 MW of geothermal power in 24 countries is online, which is expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in geothermal power online capacity since 2005. IGA projects this will grow to 18,500 MW by 2015, due to the large number of projects presently under consideration, often in areas previously assumed to have little exploitable resource.
In 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants; the largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines follows the US as the second highest producer of geothermal power in the world, with 1,904 MW of capacity online; geothermal power makes up approximately 18% of the country's electricity generation.
Developing countries.
Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.
Technology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who don't have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene. As of 2010, an estimated 3 million households get power from small solar PV systems. Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 12 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.
Micro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulostic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.
Renewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.
Industry and policy trends.
U.S. President Barack Obama's American Recovery and Reinvestment Act of 2009 includes more than $70 billion in direct spending and tax credits for clean energy and associated transportation programs. Leading renewable energy companies include First Solar, Gamesa, GE Energy, Hanwha Q Cells, Sharp Solar, Siemens, SunOpta, Suntech Power, and Vestas.
The military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.
The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed on 26 January 2009, by 75 countries signing the charter of IRENA. As of March 2010, IRENA has 143 member states who all are considered as founding members, of which 14 have also ratified the statute.
As of 2011, 119 countries have some form of national renewable energy policy target or renewable support policy. National targets now exist in at least 98 countries. There is also a wide range of policies at state/provincial and local levels.
United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. In October 2011, he "announced the creation of a high-level group to drum up support for energy access, energy efficiency and greater use of renewable energy. The group is to be co-chaired by Kandeh Yumkella, the chair of UN Energy and director general of the UN Industrial Development Organisation, and Charles Holliday, chairman of Bank of America".
100% renewable energy.
The incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of “stabilization wedges” that can allow us to maintain our quality of life while avoiding catastrophic climate change, and "renewable energy sources," in aggregate, constitute the largest number of their "wedges." 
Using 100% renewable energy was first suggested in a Science paper published in 1975 by Danish physicist Bent Sørensen. It was followed by several other proposals, until in 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europa and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europa, America, Australia and other parts of the world.
In 2011 Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University, and Mark Delucchi published a study on 100% renewable global energy supply in the journal Energy Policy. They found producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be "primarily social and political, not technological or economic". They also found that energy costs with a wind, solar, water system should be similar to today's energy costs.
Similarly, in the United States, the independent National Research Council has noted that “sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand." .
The most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 "Post Carbon Pathways" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.
Emerging technologies.
Other renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy. These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.
There are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.
Multiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.
Debate.
Renewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being variable or intermittent, but is not true for concentrated solar, geothermal and biofuels, that have continuity. In any case, the International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.
There have been "not in my back yard" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction. In the USA, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.
A recent UK Government document states that "projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake". In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.
The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.

</doc>
<doc id="25789" url="https://en.wikipedia.org/wiki?curid=25789" title="Romulus Augustulus">
Romulus Augustulus

Romulus Augustus (; born c. AD 461 – died after AD 476, and was apparently still alive as late as 507) was an emperor (alleged usurper) reigning over the Western Roman Empire from 31 October AD 475 until 4 September AD 476. His deposition by Odoacer traditionally marks the end of the Western Roman Empire, the fall of ancient Rome, and the beginning of the Middle Ages in Western Europe.
He is mostly known by his nickname "Romulus Augustulus", though he ruled officially as "Romulus Augustus". The Latin suffix "-ulus" is a diminutive; hence, "Augustulus" effectively means "Little Augustus".
The historical record contains few details of Romulus's life. He was proclaimed as emperor by his father Orestes, the "magister militum" (master of soldiers) of the Roman army after forcing Emperor Julius Nepos to quit Italy. Romulus, little more than a child, acted as a figurehead for his father's rule. Reigning for only ten months, his legitimacy and authority disputed beyond Italy, Romulus was deposed by Odoacer, who had defeated and executed Orestes. Odoacer sent Romulus to live in the Castellum Lucullanum in Campania, after which he disappears from the historical record.
Life.
Romulus' father Orestes was a Roman citizen, originally from Pannonia, who had served as a secretary and diplomat for Attila the Hun and later rose through the ranks of the Roman army. The future emperor was named "Romulus" after his maternal grandfather, a nobleman from Poetovio in Noricum. Many historians have noted the coincidence that the last western emperor bore the names of both Romulus, the legendary founder and first king of Rome, and Augustus, the first emperor.
Orestes was appointed Magister militum by Julius Nepos in 475. Shortly after his appointment, Orestes launched a rebellion and captured Ravenna, the capital of the Western Roman Empire since 402, on 28 August 475. Nepos fled to Dalmatia, where his uncle had ruled a semi-autonomous state in the 460s. Orestes, however, refused to become emperor, "from some secret motive", opined historian Edward Gibbon. Instead, he installed his son on the throne on 31 October 475.
The empire Augustus ruled was a shadow of its former self and had shrunk significantly over the previous 80 years. Imperial authority had retreated to the Italian borders and parts of southern Gaul: Italia and Gallia Narbonensis, respectively. The Eastern Roman Empire treated its western counterpart as a client state. The Eastern Emperor Leo, who died in 474, had appointed the western emperors Anthemius and Julius Nepos, and Constantinople never recognized the new government. Neither Zeno nor Basiliscus, the two generals fighting for the eastern throne at the time of Romulus' accession, accepted him as ruler.
As a proxy for his father, Romulus made no decisions and left no monuments, though coins bearing his name were minted in Rome, Milan, Ravenna, and Gaul. Several months after Orestes took power, a coalition of Heruli, Scirian and Turcilingi mercenaries demanded that he give them a third of the land in Italy. When Orestes refused, the tribes revolted under the leadership of the Scirian chieftain Odoacer. Orestes was captured near Piacenza on 28 August 476 and swiftly executed.
Odoacer advanced on Ravenna, capturing the city and the young emperor. Romulus was compelled to abdicate the throne on 4 September 476. This act has been cited as the end of the Western Roman Empire, although Romulus' deposition did not cause any significant disruption at the time. Rome had already lost its hegemony over the provinces, Germans dominated the Roman army and Germanic generals like Odoacer had long been the real powers behind the throne. Italy would suffer far greater devastation in the next century when Emperor Justinian I reconquered it.
After the abdication of Romulus, the Roman Senate, on behalf of Odoacer, sent representatives to the Eastern Roman Emperor Zeno, whom it asked to formally reunite the two halves of the Empire: "the west… no longer required an emperor of its own: one monarch sufficed for the world". He was also asked to make Odoacer a patrician, and administrator of Italy in Zeno's name. Zeno pointed out that the Senate should rightfully have first requested that Julius Nepos take the throne once more, but he nonetheless agreed to their requests. Odoacer then ruled Italy in Zeno's name.
Later life.
The ultimate fate of Romulus is a mystery. The "Anonymus Valesianus" wrote that Odoacer, "taking pity on his youth", spared Romulus' life and granted him an annual pension of 6,000 solidi before sending him to live with relatives in Campania. Jordanes and Marcellinus Comes say Odoacer exiled Romulus to Campania but do not mention any financial support from the Germanic king.
The sources do agree that Romulus took up residence in the Lucullan Villa, an ancient castle originally built by Lucullus in Campania. From here, contemporary histories fall silent. In the "History of the Decline and Fall of the Roman Empire", Edward Gibbon notes that the disciples of Saint Severinus of Noricum were invited by a "Neapolitan lady" to bring his body to the villa in 488; Gibbon conjectures from this that Augustulus "was probably no more." The villa was converted into a monastery before 500 to hold the saint's remains.
Cassiodorus, then a secretary to Theodoric the Great, wrote a letter in 507 to a "Romulus" confirming a pension. Thomas Hodgkin, a translator of Cassiodorus' works, wrote in 1886 that it was "surely possible" the Romulus in the letter was the same person as the last western emperor. The letter would match the description of Odoacer's coup in the "Anonymus Valesianus", and Romulus could have been alive in the early sixth century. But Cassiodorus does not supply any details about his correspondent or the size and nature of his pension, and Jordanes, whose history of the period abridges an earlier work by Cassiodorus, makes no mention of a pension.
Last emperor.
As Romulus was an alleged usurper, Julius Nepos claimed to hold legally the title of the emperor when Odoacer took power. However, few of Nepos' contemporaries were willing to support his cause after he ran away to Dalmatia. Some historians regard Julius Nepos, who ruled in Dalmatia until being murdered in 480, as the last lawful Western Roman Emperor.
Following Odoacer's coup, the Roman Senate sent a letter to Zeno stating that "the majesty of a sole monarch is sufficient to pervade and protect, at the same time, both the East and the West". While Zeno told the Senate that Nepos was their lawful sovereign, he did not press the point, and he accepted the imperial insignia brought to him by the senate.

</doc>
<doc id="25791" url="https://en.wikipedia.org/wiki?curid=25791" title="List of Roman emperors">
List of Roman emperors

Roman Emperors and Empresses were rulers of the Roman Empire, wielding power over its citizens and military. The empire was developed as the Roman Republic invaded and occupied most of Europe and portions of northern Africa and western Asia. Under the republic, regions of the empire were ruled by provincial governors answerable to and authorised by the "Senate and People of Rome". Rome and its senate were ruled by a variety of magistrates – of whom the consuls were the most powerful. The republic ended, and the emperors were created, when these magistrates became legally and practically subservient to one citizen with power over all other magistrates. Augustus, the first emperor, was careful to maintain the facade of republican rule, taking no specific title for his position and calling the concentration of magisterial power "Princeps Senatus" (the first man of the senate). This style of government lasted for 300 years, and is thus called the "Principate" era. The modern word 'emperor' derives from the title "imperator", which was granted by an army to a successful general; during the initial phase of the empire, it still had to be earned by the 'Princeps'. The term emperor is a modern construction, used when describing rulers of the Roman Empire because it emphasises the strong links between the ruler and the army (on whose support the ruler's power depended), and does not discriminate between the personal styles of rule and titles in different phases of the Empire.
In the late 3rd century, after the Crisis of the Third Century, Diocletian formalised and embellished the recent manner of imperial rule, establishing the so-called 'Dominate' period of the Roman Empire. This was characterised by the explicit increase of authority in the person of the Emperor, and the use of the style 'Dominus Noster' ('Our Lord'). The rise of powerful Barbarian tribes along the borders of the empire and the challenge they posed to defense of far-flung borders and unstable imperial succession led Diocletian to experiment with sharing imperial titles and responsibilities among several individuals - a partial reversion to pre-Augustian Roman traditions. For nearly two centuries thereafter there was often more than one emperor at a time, frequently dividing the administration of the vast territories between them. As Henry Moss warned, "Yet it is important to remember that in the eyes of contemporaries the Empire was still one and indivisible. It is false to the ideas of this time to speak of 'the Eastern and Western Empire'; the two halves of Empire were thought of as 'the Eastern, or Western parts' ("partes orientis vel occidentis".)" However, after the death of Theodosius I (395), the split became firmly entrenched (see: Western and Eastern) The last pretense of such division was formally ended by Zeno after the death of Julius Nepos in 480. For the remaining thousand years of Roman history there would only ever be one legitimate senior emperor, ruling from Constantinople and maintaining claim to the increasingly unstable territories in the west. After 480, multiple claims to be the imperial title of "Augustus" (or "Basileus" for Greek speakers) necessarily meant civil war, although the experiment with designating junior emperors (called now "Caesars"), usually to indicate the intended successor, occasionally reappeared.
The Empire and chain of emperors continued until the death of Constantine XI and the capture of Constantinople by the Ottoman Empire in 1453. The use of the terms "Byzantium," "Byzantine Empire," and "Byzantine Emperor" to refer to the medieval period of the Empire has been common, but not universal, among Western scholars since the 18th century, and continues to be a subject of specialist debate today.
Legitimacy.
The emperors listed in this article are those generally agreed to have been 'legitimate' emperors, and who appear in published regnal lists. The word 'legitimate' is used by most authors, but usually without clear definition, perhaps not surprisingly, since the emperorship was itself rather vaguely defined legally. In Augustus' original formulation, the "princeps" was selected by either the Senate or "the people" of Rome, but quite quickly the legions became an acknowledged stand-in for "the people." A person could be proclaimed as emperor by their troops or by "the mob" in the street, but in theory needed to be confirmed by the Senate. The coercion that frequently resulted was implied in this formulation. Furthermore, a sitting emperor was empowered to name a successor and take him on as apprentice in government and in that case the Senate had no role to play, although it sometimes did when a successor lacked the power to inhibit bids by rival claimants. By the medieval (or "Byzantine") period, the very definition of the Senate became vague as well, adding to the complication.
Lists of legitimate emperors are therefore partly influenced by the subjective views of those compiling them, and also partly by historical convention. Many of the 'legitimate' emperors listed here acceded to the position by usurpation, and many 'illegitimate' claimants had a legitimate claim to the position. Historically, the following criteria have been used to derive emperor lists:
So for instance, Aurelian, though acceding to the throne by usurpation, was the sole and undisputed monarch between 270–275 AD, and thus was a legitimate emperor. Gallienus, though not in control of the whole Empire, and plagued by other claimants, was the legitimate heir of (the legitimate emperor) Valerian. Claudius Gothicus, though acceding illegally, and not in control of the whole Empire, was the only claimant accepted by the Senate, and thus, for his reign, was the legitimate emperor. Equally, during the Year of the Four Emperors, all claimants, though not undisputed, were at some point accepted by the Senate and are thus included; conversely, during the Year of the Five Emperors neither Pescennius Niger nor Clodius Albinus were accepted by the Senate, and are thus not included. There are a few examples where individuals were made co-emperor, but never wielded power in their own right (typically the child of an emperor); these emperors are legitimate, but are not included in regnal lists, and in this article are listed together with the 'senior' emperor.
Emperors after 395.
After 395, the list of emperors in the East is based on the same general criteria, with the exception that the emperor only had to be in undisputed control of the Eastern part of the empire, or be the legitimate heir of the Eastern emperor.
The situation in the West is more complex. Throughout the final years of the Western Empire (395–480) the Eastern emperor was considered the senior emperor, and a Western emperor was only legitimate if recognized as such by the Eastern emperor. Furthermore, after 455 the Western emperor ceased to be a relevant figure and there was sometimes no claimant at all. For the sake of historical completeness, all Western Emperors after 455 are included in this list, even if they were not recognized by the Eastern Empire; some of these technically illegitimate emperors are included in regnal lists, while others are not. For instance, Romulus Augustulus was technically a usurper who ruled only the Italian peninsula and was never legally recognized. However, he was traditionally considered the "last Roman Emperor" by 18th and 19th century western scholars and his overthrow by Odoacer used as the marking point between historical epochs, and as such he is usually included in regnal lists. However, modern scholarship has confirmed that Romulus Augustulus' predecessor, Julius Nepos continued to rule as Emperor in the other Western holdings and as a figurehead for Odoacer's rule in Italy until Nepos' death in 480. Since the question of what constitutes an emperor can be ambiguous, and dating the "fall of the Western Empire" arbitrary, this list includes details of both figures.

</doc>
<doc id="25792" url="https://en.wikipedia.org/wiki?curid=25792" title="Roman calendar">
Roman calendar

The Roman calendar changed its form several times between the founding of Rome and the fall of the Roman Empire. The common calendar widely used today is known as the Gregorian calendar and is a refinement of the Julian calendar where the average length of the year has been adjusted from 365.25 days to 365.2425 days (a 0.002% change).
From at least the period of Augustus on, calendars were often inscribed in stone and displayed publicly. Such calendars are called "fasti".
History.
The original Roman calendar is believed to have been a lunar calendar, which may have been based on one of the Greek lunar calendars. As the time between new moons averages 29.5 days its months were constructed to be either hollow (29 days) or full (30 days).
Calendar of Romulus.
Roman writers attributed the original Roman calendar to Romulus, the mythical founder of Rome, though there is no other evidence for the existence of such a calendar and Romulus was often cited as the founder of practices whose origins were unknown to later Romans. According to these writers, Romulus' calendar had ten months with the spring equinox in the first month (likely based on the names of the last months of the year):
The regular calendar year thus consisted of 304 days, with the winter days after the end of December and before the beginning of the following March not being assigned to any month.
The origins of the names are also not entirely clear or agreed upon by modern scholars. Some ancient explanations are: Martius in honour of Mars, the god of war; Aprilis from "aperiō", to open: Earth opens to receive seed; Maius from Maia, goddess of growth ("maior", elder); Iunius from "iunior" (younger). The remaining six months were named with respect to their position on the calendar: the numbers five to ten in Latin being "quinque", "sex", "septem", "octo", "novem" and "decem", the months were named "Quintilis", "Sextilis", "September", "October", "November", and "December".
Calendar of Numa.
Further reforms were attributed, again without firm evidence, to Numa Pompilius, the second of the seven traditional kings of Rome. The Romans considered even numbers to be unlucky, so Numa took one day from each of the six months with 30 days, reducing the number of days in the 10 previously defined months by a total of six days. There were 51 previously unallocated winter days, to which were added the six days from the reductions in the days in the months, making a total of 57 days. These he made into two months, January and February, which he prefixed to the previous 10 months. January was given 29 days, while February had the unlucky number of 28 days, suitable for the month of purification ("Februa", the Roman festival of purification). This made a regular year (of 12 lunar months) 355 days long in place of the previous 304 days of the Romulus calendar. Of the 11 months with an odd number of days, four had 31 days each and seven had 29 days each:
Reforms of Gnaeus Flavius.
In 304 BC Gnaeus Flavius, a pontifical secretary, introduced a series of reforms. It is generally believed that he initiated the custom of publishing the calendar in advance of the month, depriving the pontiffs of some of their power, but allowing for a more consistent calendar for official business.
The Julian calendar.
Julius Caesar, as "Pontifex Maximus", reformed the calendar in 46 BC. The new calendar became known as the Julian calendar. Quintilis was renamed "Iulius" (July) in honour of Julius Caesar in 44 BC by Mark Antony. A further change was made during the reign of his successor Augustus, when, apparently following the Senate, the plebiscite "Lex Pacuvia de mense augusto" renamed Sextilis "Augustus" (August) in 8 BC. Some documents state that the date of the change of the name started between 26 and 23 BC but the date of the Lex Pacuvia is certain.
Intercalation.
The regular calendar had only 355 days, which meant that it would quickly be unsynchronized with the solar year, causing, for example, agricultural festivals to occur out of season. The Roman solution to this problem was to periodically lengthen the calendar by adding extra days to February. February consisted of two parts, each with an odd number of days. The first part ended with the "Terminalia" on the 23rd, which was considered the end of the religious year, and the five remaining days formed the second part. To keep the calendar year roughly aligned with the solar year, a leap month, called the "Mensis Intercalaris" ("intercalary month"), was added from time to time between these two parts of February. The second part of February was incorporated in the intercalary month as its last five days, with no change either in their dates or the festivals observed on them. This follows naturally from the fact that the days after the Ides of February (in an ordinary year) or the Ides of Intercalaris (in an intercalary year) both counted down to the Kalends of March. The nones and ides of Intercalaris occupied the normal positions of the 5th and 13th of the month.
The third-century writer Censorinus says:
When it was thought necessary to add (every two years) an intercalary month of 22 or 23 days, so that the civil year should correspond to the natural (solar) year, this intercalation was in preference made in February, between Terminalia and Regifugium [24th.
The fifth-century writer Macrobius says that the Romans intercalated 22 and 23 days in alternate years ("Saturnalia", 1.13.12), the intercalation was placed after 23 February and the remaining five days of February followed ("Saturnalia", 1.13.15). To avoid the nones falling on a nundine, where necessary an intercalary day was inserted "in the middle of the Terminalia, where they placed the intercalary month". ("Saturnalia", 1.13.16, 1.13.19).
This is historically correct. In 167 BC Intercalaris began on the day after 23 February and in 170 BC it began on the second day after 23 February. Varro, writing in the first century BC, says "the twelfth month was February, and when intercalations take place the five last days of this month are removed." Since all the days after the Ides of Intercalaris were counted down to the beginning of March Intercalaris had either 27 days (making 377 for the year) or 28 (making 378 for the year).
There is another theory which says that in intercalary years February had 23 or 24 days and Intercalaris had 27. No date is offered for the Regifugium in 378 - day years.
The Pontifex Maximus determined when an intercalary month was to be inserted. On average, this happened in alternate years. The system of aligning the year through intercalary months broke down at least twice: the first time was during and after the Second Punic War. It led to the reform of the Lex Acilia in 191 BC, the details of which are unclear, but it appears to have successfully regulated intercalation for over a century. The second breakdown was in the middle of the first century BC and may have been related to the increasingly chaotic and adversarial nature of Roman politics at the time. The position of Pontifex Maximus was not a full-time job; it was held by a member of the Roman elite, who would almost invariably be involved in the machinations of Roman politics. Because the term of office of elected Roman magistrates was defined in terms of a Roman calendar year, a Pontifex Maximus would have reason to lengthen a year in which he or his allies were in power, or shorten a year in which his political opponents held office. For example, Julius Caesar made the year of his third consulship in 46 BC 445 days long.
Months.
In the earliest times, the three reference dates were probably declared publicly, when appropriate lunar conditions were observed. After the reforms of Numa, they occurred on fixed days.
The day preceding the Kalends, Nones, or Ides was "Pridie", e.g., "Prid. Id. Mart." = 14 March. Other days were denoted by ordinal number, counting back from a named reference day. The reference day itself counted as the first, so that two days before was denoted the third day. Dates were written as "a.d. NN", an abbreviation for "ante diem NN", meaning "on the Nth (Numerus) day before the named reference day ("Nomen")", e.g., "a.d. III Kal. Nov." = on the third day before the November Kalends = 30 October. The value two was not used to denote a day before the fixed point, because second was the same as "pridie". Further examples of date equivalence are: "a.d. IV Non. Jan." = 2 January; "a.d. VI Non. Mai." = 2 May; "a.d. VIII Id. Apr." = 6 April; "a.d. VIII Id. Oct." = 8 October; "a.d. XVII Kal. Nov." = 16 October.
In detail, the system worked as follows:
Months were grouped in days such that the "Kalends" was the first day of the month, the "Ides" was the 13th day of short months, or the 15th day of long months, and the "Nones" was the 9th day (counted inclusively) before the "Ides" (i.e., the fifth or seventh day of the month). All other days of the month were counted backward (inclusively) from these three dates. In both long and short months (except February and the "mensis intercalaris") there were 16 days between the "Ides" of the month and the "Kalends" of the next month, and the date referred to the name of the next month, not that of the current month; thus, for example, the date of the 16th day of March was "a.d. XVII Kal. Apr". In intercalary years, the first part of February was terminated on the 23rd, i.e., the day of the "Terminalia", and the festivals normally held in the last five days of February were held instead in the last five days of the intercalary month, immediately before the "Kalends" of March. The first 22 or 23 days of the intercalary month were inserted between these two parts.
So:
Some dates were also sometimes known by the name of a festival that occurred on them, or shortly afterwards. Examples of such dates are recorded for the "Feralia, Quirinalia", and the "Terminalia", though not yet for the "Lupercalia". The known examples of such dates are all after the Ides of February, which suggests they are connected with resolving an ambiguity that could arise in intercalary years: dates of the form "a.d. Kal. Mart." were dates in late February in regular years, but were a month later in intercalary years. However, it is much debated whether there was a fixed rule for using festival-based dates. It has been variously proposed that a date like "a.d. X Terminalia" (known from an inscription in 94 BC) implied that its year 'was', 'was not', or 'might have been' intercalary.
When Julius Caesar added days to some months, he added them to the end of the month, so as not to disturb the dates of festivals in those months. This increased the count of all days after the Ides in those months, and had some odd effects. For example, the emperor Augustus was born in 63 BC on the 23rd day of September. In the pre-Julian calendar, this is seven days before the Kalends of October (or, in Roman style, counting inclusively, "a.d. VIII Kal. Oct."), but in the Julian calendar, it is eight days ("a.d. IX Kal. Oct."). Because of this ambiguity, his birthday was sometimes celebrated on both dates. See discussion in Julian calendar.
Nundinal cycle.
The Romans of the Republic, like the Etruscans, used a "market week" of eight days, marked as A to H in the calendar. A "nundinum" was the market day; etymologically, the word is related to "novem", "nine", because the Roman system of counting was inclusive. The market "week" is the nundinal cycle. Since the length of the year was not a multiple of eight days, the letter for the market day (known as a "nundinal letter") changed every year. For example, if the letter for market days in some year was A and the year was 355 days long, then the letter for the next year would be F.
The nundinal cycle formed one rhythm of day-to-day Roman life; the market day was the day when country people would come to the city, and the day when city people would buy their eight days' worth of groceries. For this reason, a law was passed in 287 BC (the "Lex Hortensia") that forbade the holding of meetings of the "comitia" (for example to hold elections) on market days, but permitted the holding of legal actions. In the late republic, a superstition arose that it was unlucky to start the year with a market day (i.e., for the market day to fall on 1 January, with a letter A), and the pontiffs, who regulated the calendar, took steps to avoid it.
Because the nundinal cycle was absolutely fixed at eight days under the Republic, information about the dates of market days is one of the most important tools used for working out the Julian equivalent of a Roman date in the pre-Julian calendar. In the early Empire, the Roman market day was occasionally changed. The details of this are not clear, but one likely explanation put forward is that it would be moved by one day if it fell on the same day as the festival of "Regifugium", an event that could occur at intervals of three years. The reason for this movement has not been explained.
The nundinal cycle was eventually replaced by the modern seven-day week, which first came into use in Italy during the early imperial period, after the Julian calendar had come into effect in 45 BC. The system of nundinal letters was also adapted for the week. (See dominical letter.) For a while, the week and the nundinal cycle coexisted, but by the time the week was officially adopted by Constantine in AD 321, the nundinal cycle had fallen out of use. For further information on the week, see week and days of the week.
Character of the day.
Each day of the Roman calendar was marked on the "fasti" with a letter that designated its religious and legal character. These were:
Years.
The calendar year originally began on 1 March, as is shown by the names of the six months following June (Quintilis = fifth month, Sextilis = sixth month, September = seventh month, etc.). It is not known when the start of the calendar year was changed to 1 January. Ancient authors attributed it to Numa Pompilius. Varro states that, according to M. Fulvius Nobilior (consul in 189 BC), who had composed a commentary on a "fasti" preserved in the temple of Hercules Musarum, January was named after Janus because the god faced both ways, which implies the calendar year started in January in his time, before the consular year started beginning on 1 January in 153 BC. A surviving calendar from the late Republic proves the calendar year started in January before the Julian reform.
How years were identified during the Roman monarchy is not known. During the Roman Republic, years were named after the consuls, who were elected annually (see List of Republican Roman Consuls). Thus, the name of the year identified a consular term of office, not a calendar year. For example, 205 BC was "The year of the consulship of Publius Cornelius Scipio Africanus and Publius Licinius Crassus", who took office on 15 March of that year, and their consular year ran until 14 March 204 BC. Lists of consuls were maintained in the "fasti".
The first day of the consular term changed several times during Roman history. The Senate changed it to 1 January in 153 BC in order to allow consul Quintus Fulvius Nobilior to attack the city of Segeda (in Aragon, Spain) during the Celtiberian Wars. Before then, it was 15 March. Earlier changes are a little less certain. There is good reason to believe it was 1 May for most of the third century BC, until 222 BC. Livy mentions earlier consular years starting on 1 Sextilis (August), 15 May, 15 December, 1 October and 1 Quintilis (July).
In the later Republic, historians and scholars began to count years from the founding of the city of Rome. Different scholars used different dates for this event. The date most widely used today is that calculated by Varro, 753 BC, but other systems varied by up to several decades. Dates given by this method are numbered "ab urbe condita" (meaning "from the founding of the city", and abbreviated AUC), and correspond to consular years. When reading ancient works using AUC dates, care must be taken to determine the epoch used by the author before translating the date into a Julian year.
In parts of the Roman empire, it was common to date by the provincial year ("anno prouinciarium", A.PP.). In Roman Africa, year 1 was AD 39; in Hispania it was 38. Thus, to arrive at AD dates for Africa and Spain it is necessary to add 39 and 38, respectively, to the provincial year. The Spanish provincial year was the basis of the Spanish era, the dating system common throughout Spain during the Middle Ages.
Extant fasti.
A considerable number of inscribed calendars, or "fasti", have been discovered. The Praenestine calendar ("Fasti praenestini"), discovered in 1770, arranged by the famous grammarian Verrius Flaccus, contains the months of January, March, April, and December, and a portion of February. The tablets give an account of festivals, and also of the triumphs of Augustus and Tiberius. There are still two complete calendars in existence, an official list by Philocalus (354), and a Christian version of the official calendar, made by Polemius Silvius (448).
Converting pre-Julian dates.
The fact that the modern world uses the same month names as the Romans can lead to an erroneous assumption that a Roman date occurred on the same Julian date as its modern equivalent. Even early Julian dates, before the leap year cycle was stabilised, are not quite what they appear to be. For example, Macrobius says 45 BC was not a leap year.
Finding the exact Julian equivalent of a pre-Julian date is complex. As there exists an essentially complete list of the consuls, a Julian year can be found to correspond to the pre-Julian year.
However, the sources rarely reveal which years were regular, which were intercalary, and how long an intercalary year was. Nevertheless, the pre-Julian calendar could be substantially out of alignment with the Julian calendar. Two precise astronomical synchronisms given by Livy show that in 168 BC, the two calendars were misaligned by more than two months, and in 190 BC, they were four months out of alignment.
A number of other clues are available to reconstruct the Julian equivalent of pre-Julian dates. First, the precise Julian date for the start of the Julian calendar is known, although some uncertainty occurs even about that. Detailed sources for the previous decade or so are found, mostly in the letters and speeches of Cicero. Combining these with what is known about how the calendar worked, especially the nundinal cycle, an accurate conversion of Roman dates after 58 BC relative to the start of the Julian calendar can be performed.
The histories of Livy give exact Roman dates for two eclipses in 190 BC and 168 BC, and a few loose synchronisms to dates in other calendars provide rough (and sometimes exact) solutions for the intervening period. Before 190 BC, the alignment between the Roman and Julian years is determined by clues such as the dates of harvests mentioned in the sources.
Combining these sources of data, an estimate can be computed for approximate Julian equivalents of Roman dates back to the start of the First Punic War in 264 BC. However, while there are enough data to make such reconstructions, the number of years before 45 BC for which pre-Julian Roman dates can be converted to Julian dates with certainty is very small, and several different reconstructions of the pre-Julian calendar are possible. A detailed reconstruction giving conversions from pre-Julian dates into Julian dates is available.

</doc>
<doc id="25794" url="https://en.wikipedia.org/wiki?curid=25794" title="Revolver">
Revolver

A revolver is a repeating handgun that has a revolving cylinder containing multiple chambers and at least one barrel for firing. Revolvers might be regarded as a subset of pistols, or as an equal-ranking subset of handguns, distinct from pistols. Though the term "revolver" usually only refers to handguns, other firearms may also have a revolving chamber. These include some models of grenade launchers, shotguns, and rifles. 
Though the original name was "revolving gun", the short-hand "revolver" is universally used. (Cannon using this mechanism are known as revolver cannon.) Nearly all early revolvers and many modern ones have six chambers in the cylinder, giving rise to the slang term six-shooter; however, revolvers with a number of different chambers have been made, with most modern revolvers having 5 or 6 chambers.
The revolver allows the user to fire multiple rounds without reloading. Each time the user cocks the hammer, the cylinder revolves to align the next chamber and round with the hammer and barrel, which gives this type of firearm its name. In a single-action revolver, the user pulls the hammer back with his free hand or thumb; the trigger pull only releases the hammer. In a double-action revolver, pulling the trigger moves the hammer back, then releases it, which requires a longer and heavier trigger pull than single-action. Loading and unloading a double-action revolver requires the operator to swing out the cylinder and insert the proper ammunition, all while keeping the gun pointed in a safe direction.
The first guns with multichambered cylinders that revolved to feed one barrel were made in the late 16th century in Europe. They were expensive and rare curiosities. Not until the 19th century would revolvers become common weapons of industrial production. One of the first was a flintlock revolver patented by Elisha Collier in 1814. The first percussion revolver was made by Lenormand of Paris in 1820 and the first percussion cap revolver was invented by the Italian Francesco Antonio Broccu in 1833. He received a prize of 300 francs for his invention; although he did not patent it, his revolver was shown to King Charles Albert of Sardinia. However, in 1835 a similar handgun was patented by Samuel Colt, who would go on to make the first mass-produced revolver.
The first cartridge revolvers were produced around 1854 by Eugene Lefaucheux.
Revolvers soon became standard for nearly all uses. In the early 20th century, semi-automatic pistols were developed, which can hold more rounds, and are faster to reload. "Automatic" pistols also have a flat profile, more suitable for concealed carry. Semi-auto pistols were not considered reliable enough for serious police work or self-defense until the later half of the century, however, and revolvers were the dominant handgun for police and civilians until modern pistols such as the Beretta 92 and Glock 17 were developed in the 70s and 80s.
Automatic pistols have almost completely replaced revolvers in military and law enforcement use (in military use, from 1910-1960; in law enforcement, in the 1980s and 1990s).
Revolvers still remain popular as back-up and off-duty handguns among American law enforcement officers and security guards. Also, revolvers are still common in the American private sector as defensive and sporting/hunting firearms. Famous police and military revolvers include the Webley, the Colt Single Action Army, the Colt Police Special, the Smith & Wesson Model 36, the Smith & Wesson Model 10, the Smith & Wesson 1917, the Smith & Wesson Model 3 the Nagant M1895.
History.
In the development of firearms, an important limiting factor was the time it took to reload the weapon after it was fired. While the user was reloading, the weapon was useless, and an adversary might be able to take advantage of the situation and kill or wound the user. Several approaches to the problem of increasing the rate of fire were developed, the earliest being multi-barrelled weapons which allowed two or more shots without reloading. 
Later weapons featured multiple barrels revolving along a single axis.
The earliest examples of what today is called a revolver were made in Germany in the late 16th century. These weapons featured a single barrel with a revolving cylinder holding the powder and ball. They would soon be made by many European gun-makers, in numerous designs and configurations. However, these weapons were difficult to use, complicated and prohibitively expensive to make, as such they were not widely distributed. It would be several hundred years before the revolver would see widespread use.
In 1836, an American, Samuel Colt patented the first revolver mechanism that led to the widespread use of the revolver. According to Samuel Colt, he came up with the idea for the revolver while at sea, inspired by the capstan, which had a ratchet and pawl mechanism on it, a version of which was used in his guns to rotate the cylinder. Revolvers proliferated largely due to Colt's ability as a salesman. But his influence spread in other ways as well; the build quality of his company's guns became famous, and its armories in America and England trained several seminal generations of toolmakers and other machinists, who had great influence in other manufacturing efforts of the next half century.
Early revolvers were caplocks and loaded as a muzzle-loader: the user poured black powder into each chamber, rammed down a bullet on top of it, then placed percussion caps on the nipple at the rear of each chamber, where the hammer would fall on it. This was similar to loading a traditional single-shot muzzle-loading pistol, except that the powder and shot could be loaded directly into the front of the cylinder rather than having to be loaded down the whole length of the barrel. Importantly, this allowed the barrel itself to be rifled, since the user wasn't required to force the tight fitting bullet down the barrel in order to load it (a traditional muzzle-loading pistol had a smoothbore and relatively loose fitting shot, which allowed easy loading, but gave much less accuracy). When firing the next shot, the user would raise his pistol vertically as he cocked the hammer back so as to let the fragments of the burst percussion cap fall out so as to not jam the mechanism. Some of the most popular cap-and-ball revolvers were the Colt Model 1851 "Navy" Mode, 1860 "Army" Model, and Colt Pocket Percussion revolvers, all of which saw extensive use in the American Civil War. Although American revolvers were the most common, European arms makers were making numerous revolvers by that time as well, many of which found their way into the hands of the American forces, including the single action Lefaucheux and LeMat revolver and the Beaumont–Adams and Tranter revolvers, which were early double-action weapons, in spite of being muzzle-loaders.
In 1854, Eugene Lefaucheux introduced the Lefaucheux Model 1854, the first revolver to use self-contained metallic cartridges rather than loose powder, pistol ball, and percussion caps. It is a single-action, pinfire revolver holding six rounds.
On November 17, 1856, Daniel B. Wesson and Horace Smith signed an agreement for the exclusive use of the Rollin White Patent at a rate of 25 cents for every revolver. Smith & Wesson began production late in 1857 and enjoyed years of exclusive production of rear-loading cartridge revolvers in America, due to their association with Rollin White, who held the patent and vigorously defended it against any perceived infringement by other manufacturers (much as Colt had done with his original patent on the revolver). Although White held the patent, other manufacturers were able to sell firearms using the design, provided they were willing to pay royalties.
After White's patent finally expired in April 1869, a 3rd extension was refused. Other gun-makers were finally allowed to produce their own weapons using the rear-loading method, without having to pay a royalty on each gun sold. Early guns were often conversions of earlier cap-and-ball revolvers, modified to accept metallic cartridges loaded from the rear, but later models, such as the Colt Model 1872 "Open Top" and the Smith & Wesson Model 3, were designed from the start as cartridge revolvers.
In 1873, Colt introduced the famous Model 1873, also known as the Single Action Army, the "Colt .45" (not to be confused with Colt made models of the M1911 semi-automatic) or simply, "the Peacemaker", one of the most famous handguns ever made. This popular design, which was a culmination of many of the advances introduced in earlier weapons, fired 6 metallic cartridges and was offered in over 30 different calibers and various barrel lengths. It is still in production, along with numerous clones and lookalikes, and its overall appearance has remained the same since 1873. Although originally made for the United States Army, the Model 1873 was widely distributed and popular with civilians, ranchers, lawmen, and outlaws alike. Its design has influenced countless other revolvers. Colt has discontinued its production twice, but brought it back due to popular demand and continues to make it to this day.
In the U.S. the traditional single-action revolver still reigned supreme until the late 19th century. In Europe, however, arms makers were quick to adopt the double-action trigger. While the US was producing weapons like the Model 1873, the Europeans were building double-action models like the French MAS Modèle 1873 and the somewhat later British Enfield Mk I and II revolvers (Britain relied on cartridge conversions of the earlier Beaumont–Adams double-action prior to this). Colt's first attempt at a double action revolver to compete with the European manufacturers was the Colt Model 1877, which earned lasting notoriety for its overly complex, expensive and fragile trigger mechanism, which in addition to failing frequently, also had a terrible trigger pull unless given the attentions of a competent gunsmith.
In 1889, Colt introduced the Model 1889, the first truly modern double action revolver, which differed from earlier double action revolvers by having a "swing-out" cylinder, as opposed to a "top-break" or "side-loading" cylinder. Swing out cylinders quickly caught on, because they combined the best features of earlier designs. Top-break actions gave the ability to eject all empty shells simultaneously, and exposed all chambers for easy reloading, but having the frame hinged into two halves weakened the gun and negatively effected accuracy, due to lack of rigidity. "Side-loaders", like the earlier Colt Model 1871 and 1873, gave a rigid frame, but required the user to eject and load one cylinder at a time, as they rotated the cylinder to line each chamber up with the side-mounted loading gate. Smith & Wesson followed 7 years later with the "Hand Ejector, Model 1896" in .32 S&W Long caliber, followed by the very similar, yet improved, Model 1899 (later known as the Model 10), which introduced the new .38 Special cartridge. The Model 10 went on to become the best selling handgun of the 20th century, at 6,000,000 units, and the .38 Special is still the most popular chambering for revolvers in the world. These new guns were an improvement over the Colt 1889 design since they incorporated a combined center-pin and ejector rod to lock the cylinder in position. The 1889 did not use a center pin and the cylinder was prone to move out of alignment.
Revolvers have remained popular to the present day in many areas, although in the military and law enforcement, they have largely been supplanted by magazine-fed semi-automatic pistols such as the Beretta M9, especially in circumstances where reload time and higher cartridge capacity are deemed important.
Patents.
Elisha Collier of Boston, Massachusetts patented a flintlock revolver in Britain in 1818, and significant numbers were being produced in London by 1822. The origination of this invention is in doubt, as similar designs were patented in the same year by Artemus Wheeler in the United States and by Cornelius Coolidge in France. Samuel Colt submitted a British patent for his revolver in 1835 and an American patent (number 138) on February 25, 1836 for a "Revolving gun", and made the first production model on March 5 of that year.
Another revolver patent was issued to Samuel Colt on August 29, 1839. The February 25, 1836 patent was then reissued as entitled "Revolving gun" on October 24, 1848. This was followed by on September 3, 1850 for a "Revolver", and by on September 10, 1850 for a "Revolver". was issued to Roger C. Field for an economical device for minimizing the flash gap of a revolver between the barrel and the cylinder. In 1855, Rollin White patented the bored-through cylinder entitled "Improvement in revolving fire-arms" . In 1856 Horace Smith & Daniel Wesson formed a partnership (S&W), developed and manufactured a revolver chambered for a self-contained metallic cartridge.
Design.
A revolver works by having several firing chambers arranged in a circle in a cylindrical block that are brought into alignment with the firing mechanism and barrel one at a time. In contrast, other repeating firearms, such as bolt-action, lever-action, pump-action, and semi-automatic, have a single firing chamber and a mechanism to load and extract cartridges into it.
A single-action revolver requires the hammer to be pulled back by hand before each shot, which also revolves the cylinder. This leaves the trigger with just one "single action" left to perform - releasing the hammer to fire the shot - so the force and distance required to pull the trigger can be minimal. In contrast, with a self-cocking revolver, one long squeeze of the trigger pulls back the hammer and revolves the cylinder, then finally fires the shot. They can generally be fired faster than a single-action, but with reduced accuracy in the hands of most shooters.
Most modern revolvers are "traditional double-action", which means they may operate either in single-action or self-cocking mode. The accepted meaning of "double-action" has, confusingly, come to be the same as "self-cocking", so modern revolvers that cannot be pre-cocked are called "double-action-only". These are intended for concealed carry, because the hammer of a traditional design is prone to snagging on clothes when drawn. Most revolvers do not come with accessory rails, which are used for mounting lights and lasers, except for the Smith & Wesson M&P R8 (.357 Magnum), Smith & Wesson Model 325 Thunder Ranch (.45 ACP), and all versions of the Chiappa Rhino (.357 Magnum, 9×19mm, .40 S&W, or 9×21mm) except for the 2" model, respectively. However, certain revolvers, such as the Taurus Judge and Charter Arms revolvers, can be fitted with accessory rails.
Most commonly, such revolvers have 5 or 6 chambers, hence the common names of "six-gun" or "six-shooter". However, some revolvers have 7, 8, 9, or 10 chambers, often depending on the caliber, and at least one revolver has 12 chambers (the US Fire Arms Model 12/22). Each chamber has to be reloaded manually, which makes reloading a revolver a much slower procedure than reloading a semi-automatic pistol.
Compared to autoloading handguns, a revolver is often much simpler to operate and may have greater reliability. For example, should a semiautomatic pistol fail to fire, clearing the chamber requires manually cycling the action to remove the errant round, as cycling the action normally depends on the energy of a cartridge firing. With a revolver, this is not necessary as none of the energy for cycling the revolver comes from the firing of the cartridge, but is supplied by the user either through cocking the hammer or, in a double-action design, by just squeezing the trigger. Another significant advantage of revolvers is superior ergonomics, particularly for users with small hands. A revolver's grip does not hold a magazine, and it can be designed or customized much more than the grip of a typical semi-automatic. Partially because of these reasons, revolvers still hold significant market share as concealed carry and home-defense weapons.
A revolver can be kept loaded and ready to fire without fatiguing any springs and is not very dependent on lubrication for proper firing. Additionally, in the case of double-action-only revolvers there is no risk of accidental discharge from dropping alone, as the hammer is cocked by the trigger pull. However, the revolver's clockwork-like internal parts are relatively delicate and can become misaligned after a severe impact, and its revolving cylinder can become jammed by excessive dirt or debris.
Over the long period of development of the revolver, many calibers have been used. Some of these have proved more durable during periods of standardization and some have entered general public awareness. Among these are the .22 rimfire, a caliber popular for target shooting and teaching novice shooters; .38 Special and .357 Magnum, known for police use; the .44 Magnum, famous from Clint Eastwood's "Dirty Harry" films; and the .45 Colt, used in the Colt revolver of the Wild West. Introduced in 2003, the Smith & Wesson Model 500 is one of the most powerful revolvers, utilizing the .500 S&W Magnum cartridge.
Because the rounds in a revolver are headspaced on the rim, some revolvers are capable of chambering more than one type of ammunition. The .44 Magnum round will chamber the shorter .44 Special and shorter .44 Colt, likewise the .357 Magnum will safely chamber .38 Special and .38 Short Colt. In 1996 a revolver known as the Medusa M47 was made that could chamber 25 different cartridges with bullet diameters between .355" and .357".
Revolver technology lives on in other weapons used by the military. Some autocannons and grenade launchers use mechanisms similar to revolvers, and some riot shotguns use spring-loaded cylinders holding up to 12 rounds. In addition to serving as backup guns, revolvers still fill the specialized niche role as a shield gun; law enforcement personnel using a "bulletproof" ballistic shield (Gun shield) sometimes opt for a revolver instead of a self-loading pistol, because the slide of a pistol may strike the front of the shield when fired. Revolvers do not suffer from this disadvantage. A second revolver may be secured behind the shield to provide a quick means of continuity of fire. Many police also still use revolvers as their duty weapon due to their relative mechanical simplicity and user friendliness.
With the advancement of technology and design in 2010 major revolver manufacturers are coming out with polymer frame revolvers like the Ruger LCR, Smith & Wesson Bodyguard 38, and Taurus Protector Polymer. The new innovative design incorporates advanced polymer technology that lowers weight significantly, helps absorbs recoil, and strong enough to handle +P and .357 Magnum loads. The polymer is only used on the lower frame and joined to a metal alloy upper frame, barrel, and cylinder. Polymer technology is considered one of the major advancements in revolver history because the frame has always been metal alloy and mostly one piece frame design.
Another recent development in revolver technology is the Rhino, a revolver introduced by Italian manufacturer Chiappa in 2009 and first sold in the U.S. in 2010. The Rhino, built with the U.S. concealed carry market in mind, is designed so that the bullet fires from the bottom chamber of the cylinder instead of the top chamber as in standard revolvers. This is intended to reduce muzzle flip, allowing for faster and more accurate repeat shots. In addition, the cylinder cross-section is hexagonal instead of circular, further reducing the weapon's profile.
Loading and unloading.
Front loading.
The first revolvers were "front loading", and were a bit like muskets in that the powder and bullet were loaded separately. These were caplocks or "cap and ball" revolvers, because the caplock method of priming was the first to be compact enough to make a practical revolver feasible. When loading, each chamber in the cylinder was rotated out of line with the barrel, and charged from the front with loose powder and an oversized bullet. Next, the chamber was aligned with the ramming lever underneath the barrel. Pulling the lever would drive a rammer into the chamber, pushing the ball securely in place. Finally, the user would place percussion caps on the nipples on the rear face of the cylinder.
After each shot, a user was advised to raise his revolver vertically while cocking back the hammer so as to allow the fragments of the spent percussion cap to fall out safely. Otherwise, the fragments could fall into the revolver's mechanism and jam it. Caplock revolvers were vulnerable to "chain fires", wherein hot gas from a shot ignited the powder in the other chambers. This could be prevented by sealing the chambers with cotton, wax, or grease.
Loading a cylinder in this manner was a slow and awkward process and generally could not be done in the midst of battle. Some soldiers solved this by carrying multiple revolvers in the field. Another solution was to use a revolver with a detachable cylinder design. These revolvers allowed the shooter to quickly remove a cylinder and replace it with a full one.
Fixed cylinder designs.
In many of the first generation of cartridge revolvers (especially those that were converted after manufacture), the base pin on which the cylinder revolved was removed, and the cylinder taken from the revolver for loading. Most revolvers using this method of loading are single-action revolvers, although Iver Johnson produced double-action models with removable cylinders. The removable-cylinder design is employed in some modern "micro-revolvers" (usually in .22 caliber), in order to simplify their design. These weapons are small enough to fit in the palm of the hand.
Later single-action revolver models with a fixed cylinder used a loading gate at the rear of the cylinder that allowed insertion of one cartridge at a time for loading, while a rod under the barrel could be pressed rearward to eject the fired case.
The loading gate on the original Colt designs (and on nearly all single-action revolvers since, such as the famous Colt Single Action Army) is on the right side, which was done to facilitate loading while on horseback; with the revolver held in the left hand with the reins of the horse, the cartridges can be ejected and loaded with the right hand.
Because the cylinders in these types of revolvers are firmly attached at the front and rear of the frame, and the frame is typically full thickness all the way around, fixed cylinder revolvers are inherently strong designs. Accordingly, many modern large caliber hunting revolvers tend to be based on the fixed cylinder design. Fixed cylinder revolvers can fire the strongest and most powerful cartridges, but at the price of being the slowest to load and reload and they cannot use speedloaders or moon clips for loading, as only one chamber is exposed at a time to the loading gate.
Top break.
In a top-break revolver, the frame is hinged at the bottom front of the cylinder. Releasing the lock and pushing the barrel down exposes the rear face of the cylinder. In most top-break revolvers, this act also operates an extractor that pushes the cartridges in the chambers back far enough that they will fall free, or can be removed easily. Fresh rounds are then inserted into the cylinder. The barrel and cylinder are then rotated back and locked in place, and the revolver is ready to fire.
Top break revolvers can be loaded more rapidly than fixed-frame revolvers, especially with the aid of a speedloader or moon clip. However, this design is much weaker and cannot handle high pressure rounds. While this design is mostly obsolete today, supplanted by the stronger yet equally convenient swing-out design, manufacturers have begun making reproductions of late 19th century designs for use in cowboy action shooting.
The most commonly found top-break revolvers were manufactured by Smith & Wesson, Webley & Scott, Iver Johnson, Harrington & Richardson, Manhattan Fire Arms, Meriden Arms and Forehand & Wadsworth.
Tip up.
The tip-up was the first revolver design for use with metallic cartridges in the Smith & Wesson Model 1. It is similar to the break-open design that had a hinge on the top rear of the frame, but in the case of the tip-up, the barrel release catch is located on the side of the frame in front of the trigger. Smith & Wesson discontinued it in the third series of the Smith & Wesson Model 1 1/2 but it was fairly widely used in Europe in the 19th century, after a patent by Spirlet in 1870, which also included an ejector.
Swing out cylinder.
The most modern method of loading and unloading a revolver is by means of the "swing out cylinder". The cylinder is mounted on a pivot that is parallel to the chambers, and the cylinder swings out and down (to the left in most cases). An extractor is fitted, operated by a rod projecting from the front of the cylinder assembly. When pressed, it will push all fired rounds free simultaneously (as in top break models, the travel is designed to not completely extract longer, unfired rounds). The cylinder may then be loaded, singly or again with a speedloader, closed, and latched in place.
The pivoting part that supports the cylinder is called the crane; it is the weak point of swing-out cylinder designs. Using the method often portrayed in movies and television of flipping the cylinder open and closed with a flick of the wrist can in fact cause the crane to bend over time, throwing the cylinder out of alignment with the barrel. Lack of alignment between chamber and barrel is a dangerous condition, as it can impede the bullet's transition from chamber to barrel. This gives rise to higher pressures in the chamber, bullet damage, and the potential for an explosion if the bullet becomes stuck.
The shock of firing can exert a great deal of stress on the crane, as in most designs the cylinder is only held closed at one point, the rear of the cylinder. Stronger designs, such as the Ruger Super Redhawk, use a lock in the crane as well as the lock at the rear of the cylinder. This latch provides a more secure bond between cylinder and frame, and allows the use of larger, more powerful cartridges. Swing out cylinders are rather strong, but not as strong as fixed cylinders, and great care must be taken with the cylinder when loading, so as not to damage the crane.
Action.
Single-action.
In a single-action revolver, the hammer is manually cocked, usually with the thumb of the firing or supporting hand. This action advances the cylinder to the next round and locks the cylinder in place with the chamber aligned with the barrel. The trigger, when pulled, releases the hammer, which fires the round in the chamber. To fire again, the hammer must be manually cocked again. This is called "single-action" because the trigger only performs a single action, of releasing the hammer. Because only a single action is performed and trigger pull is lightened, firing a revolver in this way allows most shooters to achieve greater accuracy. Additionally, the need to cock the hammer manually acts as a safety. The Colt Paterson Revolver, the Walker Colt, the Colt's Dragoon and the Colt Single Action Army pistol of the American Frontier era are all good examples of this system.
Double-action.
In double-action (DA), the stroke of the trigger pull generates three actions: 
Thus, DA means that a cocking action separate from the trigger pull is unnecessary; every trigger pull will result in a complete cycle. This allows uncocked carry, while also allowing draw-and-fire using only the trigger. A longer and harder trigger stroke is the trade-off. However, this drawback can also be viewed as a safety feature, as the gun is safer against accidental discharges from being dropped.
Most double-action revolvers may be fired in two ways.
Certain revolvers, called "double-action-only" (DAO) or, more correctly but less commonly, "self-cocking", lack the latch that enables the hammer to be locked to the rear, and thus can only be fired in the double-action mode. With no way to lock the hammer back, DAO designs tend to have "bobbed" or "spurless" hammers, and may even have the hammer completely covered by the revolver's frame (i.e., shrouded or hooded). These are generally intended for concealed carrying, where a hammer spur could snag when the revolver is drawn. The potential reduction in accuracy in aimed fire is offset by the increased capability for concealment.
DA and DAO revolvers were the standard-issue sidearm of countless police departments for many decades. Only in the 1990s did the semiautomatic pistol begin to make serious inroads after the advent of safe actions. The reasons for these choices are the modes of carry and use. Double action is good for high-stress situations because it allows a mode of carry in which "draw and pull the trigger" is the only requirement—no safety catch release nor separate cocking stroke is required.
Other.
In the cap-and-ball days of the mid 19th century, two revolver models, the English Tranter and the American Savage “Figure Eight”, used a method whereby the hammer was cocked by the shooter’s middle finger pulling on a second trigger below the main trigger.
Iver Johnson made an unusual model from 1940 to 1947, called the "Trigger Cocking Double Action". If the hammer was down, pulling the trigger would cock the hammer. If the trigger was pulled with the hammer cocked, it would then fire. This meant that to fire the revolver from a hammer down state, the trigger must be pulled twice.
3D printed revolver.
The Zig zag revolver is a 3D printed .38 Revolver made public in May 2014. It was created using an $500 plastic 3D-printer used, the name of the printer was not revealed by the creator. It was created by a Japanese citizen from Kawasaki named Yoshitomo Imura He was arrested in May 2014 after he had posted a video online of himself firing a 3D printed Zig Zag revolver. It is the first 3D printed Japanese gun in the world which can discharge live cartridges.
Use with suppressors.
As a general rule, revolvers cannot be effective with a sound suppressor ("silencer"), as there is usually a small gap between the revolving cylinder and the barrel which a bullet must traverse or jump when fired. From this opening, a rather loud report is produced. A suppressor can only suppress noise coming from the muzzle.
A suppressible revolver design does exist in the Nagant M1895, a Belgian designed revolver used by Imperial Russia and later the Soviet Union from 1895 through World War II. This revolver uses a unique cartridge whose case extends beyond the tip of the bullet, and a cylinder that moves forward to place the end of the cartridge inside the barrel when ready to fire. This bridges the gap between the cylinder and the barrel, and expands to seal the gap when fired. While the tiny gap between cylinder and barrel on most revolvers is insignificant to the internal ballistics, the seal is especially effective when used with a suppressor, and a number of suppressed Nagant revolvers have been used since its invention.
There is a modern revolver of Russian design, the OTs-38, which uses ammunition that incorporates the silencing mechanism into the cartridge case, making the gap between cylinder and barrel irrelevant as far as the suppression issue is concerned. The OTs-38 does need an unusually close and precise fit between the cylinder and barrel due to the shape of bullet in the special ammunition (Soviet SP-4), which was originally designed for use in a semi-automatic.
Additionally, the US Military experimented with designing a special version of the Smith & Wesson Model 29 for Tunnel Rats, called the Quiet Special Purpose Revolver or QSPR. Using special .40 caliber ammunition, it never entered official service.
Automatic revolvers.
The term "automatic revolver" has two different meanings, the first being used in the late nineteenth and early twentieth centuries when "automatic" referred not to the operational mechanism of firing, but of extraction and ejection of spent casings. An "automatic revolver" in this context is one which extracts empty fired cases "automatically," i.e., upon breaking open the action, rather than requiring manual extraction of each case individually with a sliding rod or pin (as in the Colt Single Action Army design). This term was widely used in the advertising of the period as a way to distinguish such revolvers from the far more common rod-extraction types.
In the second sense, "automatic revolver" refers to the mechanism of firing rather than extraction. Double-action revolvers use a long trigger pull to cock the hammer, thus negating the need to manually cock the hammer between shots. The disadvantage of this is that the long, heavy pull cocking the hammer makes the double-action revolver much harder to shoot accurately than a single-action revolver (although cocking the hammer of a double-action reduces the length and weight of the trigger pull). A rare class of revolvers, called automatic for its firing design, attempts to overcome this restriction, giving the high speed of a double-action with the trigger effort of a single-action. The Webley-Fosbery Automatic Revolver is the most famous commercial example. It was recoil-operated, and the cylinder and barrel recoiled backwards to cock the hammer and revolve the cylinder. Cam grooves were milled on the outside of the cylinder to provide a means of advancing to the next chamber—half a turn as the cylinder moved back, and half a turn as it moved forward. .38 caliber versions held eight shots, .455 caliber versions six. At the time, the few available automatic pistols were larger, less reliable, and more expensive. The automatic revolver was popular when it first came out, but was quickly superseded by the creation of reliable, inexpensive semi-automatic pistols.
In 1997, the Mateba company developed a type of recoil-operated automatic revolver, commercially named the Mateba Autorevolver, which uses the recoil energy to auto-rotate a normal revolver cylinder holding six or seven cartridges, depending on the model. The company has made several versions of its Autorevolver, including longer-barrelled and carbine variations, chambered for .357 Magnum, .44 Magnum and .454 Casull.
The Pancor Jackhammer is a combat shotgun based on a similar mechanism to an automatic revolver. It uses a Blow-Forward action to move the barrel forward (which unlocks it from the cylinder) and then rotate the cylinder and cock the hammer.
Revolving long guns.
Revolvers were not limited to handguns and as a longer barrelled arm is more useful in military applications than a sidearm, the idea was applied to both rifles and shotguns throughout the history of the revolver mechanism with mixed degrees of success.
Rifles.
Revolving rifles were an attempt to increase the rate of fire of rifles by combining them with the revolving firing mechanism that had been developed earlier for revolving pistols. Colt began experimenting with revolving rifles in the early 19th century, making them in a variety of calibers and barrel lengths. Colt revolving rifles were the first repeating rifles adopted by the U.S. Government, but they had their problems. They were officially given to soldiers because of their rate of fire. But after firing six shots, the shooter had to take an excessive amount of time to reload. Also, on occasion Colt rifles discharged all their rounds at once, endangering the shooter. Even so, an early model was used in the Seminole Wars in 1838. During the Civil War a LeMat Carbine was made based on the LeMat revolver.
Shotguns.
Colt briefly manufactured several revolving shotguns that were met with mixed success. The Colt Model 1839 Shotgun was manufactured between 1839 and 1841. Later, the Colt Model 1855 Shotgun, based on the Model 1855 revolving rifle, was manufactured between 1860 and 1863. Because of their low production numbers and age they are among the rarest of all Colt firearms.
The Armsel Striker was a modern take on the revolving shotgun that held 10 rounds of 12 Gauge ammunition in its cylinder. It was copied by Cobray as the Streetsweeper.
Taurus manufactures a carbine variant of the Taurus Judge revolver along with its Australian partner company, Rossi known as the "Taurus/Rossi Circuit Judge". It comes in the original combination chambering of .410 bore and .45 Long Colt, as well as the .44 Remington Magnum chambering. The rifle has small blast shields attached to the cylinder to protect the shooter from hot gases escaping between the cylinder and barrel.
Six gun.
A Six Gun is a revolver that holds six cartridges. The cylinder in a six gun is often called a 'wheel', and the six gun is itself often called a 'wheel gun'. Although a "Six Gun" can refer to any six-chambered revolver, it is typically a reference to the Colt Single Action Army, or its modern look-alikes such as the Ruger Vaquero and Beretta Stampede.
Until the 1970s, when older-design revolvers such as Colt Single Action Armys and Ruger Blackhawks were re-engineered with drop safeties (such as firing pin blocks, hammer blocks, or transfer bars) that prevent the firing pin from contacting the cartridge's primer unless the trigger is pulled, safe carry required the hammer being positioned over an empty chamber, reducing the available cartridges from six to five, or, on some models, in between chambers on either a pin or in a groove for that purpose, thus keeping the full six rounds available. This kept the uncocked hammer from resting directly on the primer of a cartridge. If not used in this manner, the hammer rests directly on a primer and unintentional firing may occur if the gun is dropped or the hammer is struck. Some holster makers provided a thick leather thong to place underneath the hammer that both allowed the carry of a gun fully loaded with all six rounds and secured the gun in the holster to help prevent its accidental loss.
Six guns are used commonly by Single-Action Shooting enthusiasts in shooting competitions, designed to mimic the gunfights of the Old West, and for general target shooting, hunting and personal defense.

</doc>
<doc id="25795" url="https://en.wikipedia.org/wiki?curid=25795" title="Robert Freitas">
Robert Freitas

Robert A. Freitas Jr. (born 1952) is a Senior Research Fellow, one of four researchers at the nonprofit foundation Institute for Molecular Manufacturing in Palo Alto, California.
Career.
Freitas holds a 1974 Bachelor's degree majoring in both physics and psychology from Harvey Mudd College, and a 1978 Juris Doctor (J.D.) degree from Santa Clara University. He has written more than 150 technical papers, book chapters, or popular articles on a diverse set of scientific, engineering, and legal topics. He co-edited the 1980 NASA feasibility analysis of self-replicating space factories and later authored the first detailed technical design study of a hypothetical medical nanorobot, the respirocyte, ever published in a refereed medical journal.
In 1977-78 Robert Freitas created the concept sentience quotient (SQ) as a way to describe the information processing rate in living organisms or computers. Freitas is authoring the multi-volume text "Nanomedicine", the first book-length technical discussion of the potential medical applications of hypothetical molecular nanotechnology and medical nanorobotics. Volume I was published in October 1999 by Landes Bioscience while Freitas was a Research Fellow at the Institute for Molecular Manufacturing . He published Volume IIA in October 2003 with Landes Bioscience while serving as a research scientist at Zyvex Corp., a nanotechnology company headquartered in Richardson, Texas, during 2000-2004.
Also in 2004, Robert Freitas and Ralph Merkle coauthored and published "Kinematic Self-Replicating Machines", the first complete survey of the field of physical and hypothetical self-replicating machines. In 2006, Freitas and Merkle co-founded the Nanofactory Collaboration, a research program to develop the first working diamondoid nanofactory.
He received the 2007 Foresight Prize in Communication from the Foresight Institute. In 2009, Freitas was awarded the Feynman Prize in Nanotechnology for Theory.
In 2010, Freitas was granted a patent for what was at the time (2004) the first patent application ever filed on diamond mechanosynthesis.

</doc>
<doc id="25797" url="https://en.wikipedia.org/wiki?curid=25797" title="Robert Morris">
Robert Morris

Robert or Bob Morris may refer to:

</doc>
<doc id="25798" url="https://en.wikipedia.org/wiki?curid=25798" title="Reykjavík">
Reykjavík

Reykjavík (, , ; , ) is the capital and largest city of Iceland. It has a latitude of 64°08' N, making it the world's northernmost capital of a sovereign state, and is a popular tourist destination. It is located in southwestern Iceland, on the southern shore of the Faxaflói Bay. With a population of around 120,000 (and over 200,000 in the Capital Region), it is the heart of Iceland's cultural, economic and governmental activity.
Reykjavík is believed to be the location of the first permanent settlement in Iceland, which, according to Ingólfur Arnarson, was established in AD 874. Until the 18th century, there was no urban development in the city location. The city was founded in 1786 as an official trading town and grew steadily over the next decades, as it transformed into a regional and later national center of commerce, population, and governmental activities. It is among the cleanest, greenest, and safest cities in the world.
History.
The first permanent settlement in Iceland by Norsemen is believed to have been established at Reykjavík by Ingólfur Arnarson from Norway around AD 870; this is described in "Landnámabók", or the Book of Settlement. Ingólfur Arnarson is said to have decided the location of his settlement using a traditional Norse method; he cast his high seat pillars (Öndvegissúlur) into the ocean when he saw the coastline, then settled where the pillars came to shore. Steam from hot springs in the region is said to have inspired Reykjavík's name, which loosely translates to Smoke Cove (the city is sometimes referred to as "Bay of Smoke" or "Smoky Bay" in English language travel guides). The original name was Reykja"r"vík with an additional "r" that vanished around 1300.
Reykjavík is not mentioned in any medieval sources except as being covered by farmland, but the 18th century saw the beginning of urban concentration. The Danish rulers of Iceland backed the idea of domestic industry in Iceland that would stimulate much-needed development on the island. In 1752, the King of Denmark, Frederik V, donated the estate of Reykjavík to the Innréttingar Corporation; the name comes from the Danish language word "indretninger", meaning institution. The leader of this movement was . In the 1750s several houses were built to house the wool industry that was to be Reykjavík's most important employer for a few decades and the original reason for its existence. Other crafts were also practised by the Innréttingar, such as fisheries, sulphur mining, agriculture, and shipbuilding.
The Danish Crown abolished monopoly trading in 1786 and granted six communities around the country an exclusive trading charter, Reykjavík was one of them and the only one to hold on to the charter permanently. The year 1786 is regarded as the date of the city's founding; its 200th anniversary was celebrated in 1986. Trading rights were still limited to the subjects of the Danish Crown, and Danish traders continued to dominate trade in Iceland. Over the following decades, their business in Iceland expanded. After 1880, free trade was expanded to all nationalities and the influence of Icelandic merchants started to grow.
Rise of nationalism.
Icelandic nationalist sentiment gained influence in the 19th century and the idea of Icelandic independence became widespread. Reykjavík, as Iceland's only city, was central to such ideas. Advocates of an independent Iceland realized that a strong Reykjavík was fundamental to that objective. All the important events in the history of the independence struggle were important to Reykjavík as well. In 1845 Alþingi, the general assembly formed in 930 AD, was re-established in Reykjavík; it had been suspended a few decades earlier when it was located at Þingvellir. At the time it functioned only as an advisory assembly, advising the King about Icelandic affairs. The location of Alþingi in Reykjavík effectively established the city as the capital of Iceland.
In 1874 Iceland was given a constitution; with it, Alþingi gained some limited legislative powers and in essence became the institution that it is today. The next step was to move most of the executive power to Iceland: Home Rule was granted in 1904 when the office of Minister For Iceland was established in Reykjavík. The biggest step towards an independent Iceland was taken on 1 December 1918 when Iceland became a sovereign country under the Crown of Denmark, the Kingdom of Iceland.
By the 1920s and 1930s most of the growing Icelandic fishing trawler fleet sailed from Reykjavík and salt-cod production was its main industry, but the Great Depression hit Reykjavík hard with unemployment and labour union struggles occurring that sometimes became violent.
World War II.
On the morning of 10 May 1940, following the German occupation of Denmark and Norway on 9 April 1940, four British warships approached Reykjavík and anchored in the harbour. In a few hours, the allied occupation of Reykjavík was complete. There was no armed resistance, and taxi and truck drivers even assisted the invasion force, which initially had no motor vehicles. The Icelandic government had received many requests from the British government to consent to the occupation, but it always declined on the basis of the Neutrality Policy. For the remaining years of World War II, British and later American soldiers occupied camps in Reykjavík, and the number of foreign soldiers in Reykjavík became about the same as the local population of the city. The Royal Regiment of Canada (RREGTC) formed part of the garrison in Iceland during the early part of the war.
The economic effects of the occupation were positive for Reykjavík: the unemployment of the Depression years vanished and construction work began. The British built Reykjavík Airport, which is still in service today, mostly serving domestic flights. The Americans, meanwhile, built Keflavík Airport, situated west of Reykjavík, which would become Iceland's primary international airport. In 1944, the Republic of Iceland was founded and a president, elected by the people, replaced the King; the office of the president was placed in Reykjavík.
Post-war development.
In the post-war years the growth of Reykjavík accelerated. A mass exodus from the rural countryside began, largely due to improved technology in agriculture that reduced the need for manpower, and because of a population boom resulting from better living conditions in the country. A once primitive village was rapidly transformed into a modern city. Private cars became common and modern apartment complexes rose in the expanding suburbs. Much of Reykjavík lost its village feel. In 1972, Reykjavík hosted the world chess championship between Bobby Fischer and Boris Spassky.
Reykjavík has in the last three decades become a significant player in the global community. The 1986 Reykjavík Summit between Ronald Reagan and Mikhail Gorbachev underlined Reykjavík's new-found international status. Deregulation in the financial sector and the computer revolution of the 1990s again transformed Reykjavík. The financial and IT sectors are now significant employers in the city. The city has fostered some world-famous talents in recent decades, such as Björk, Ólafur Arnalds and bands Múm, Sigur Rós, and Of Monsters and Men, and poet Sjón.
Geography.
Reykjavík is located in southwest Iceland. The Reykjavík area coastline is characterized by peninsulas, coves, straits, and islands.
During the Ice Age (up to 10,000 years ago) a large glacier covered parts of the city area, reaching as far out as Álftanes. Other parts of the city area were covered by sea water. In the warm periods and at the end of the Ice Age, some hills like Öskjuhlíð were islands. The former sea level is indicated by sediments (with clams) reaching (at Öskjuhlíð, for example) as far as above the current sea level. The hills of Öskjuhlíð and Skólavörðuholt appear to be the remains of former shield volcanoes which were active during the warm periods of the Ice Age.
After the Ice Age the land rose as the heavy load of the glaciers fell away, and began to look as it does today.
The capital city area continued to be shaped by earthquakes and volcanic eruptions, like the one 4,500 years ago in the mountain range Bláfjöll, when the lava coming down the Elliðaá valley reached the sea at the bay of Elliðavogur.
The largest river to run through Reykjavík is the Elliðaá River, which is non-navigable. It is one of the best salmon fishing rivers in the country. Mount Esja, at , is the highest mountain in the vicinity of Reykjavík.
The city of Reykjavík is mostly located on the Seltjarnarnes peninsula, but the suburbs reach far out to the south and east. Reykjavík is a spread-out city: most of its urban area consists of low-density suburbs, and houses are usually widely spaced. The outer residential neighbourhoods are also widely spaced from each other; in between them are the main traffic arteries and a lot of empty space.
Climate.
Temperatures very rarely drop below in the winter. This is because the Icelandic coastal weather in winter is moderated by the North Atlantic Current, itself an extension of the Gulf Stream (see also Extratropical cyclone). The climate is cold temperate subpolar oceanic (Koppen "Cfc"), and the city is on the northern edge of the temperate zone, closely bordering a tundra climate ("ET"). The city's coastal location does make it prone to wind, however, and gales are common in winter. Summers are cool, with temperatures fluctuating between , rarely exceeding . Reykjavík averages 147 days with measurable precipitation every year. Droughts are uncommon although they occur in some summers. In the summer of 2007, no rain was measured for one month. Spring tends to be the sunniest season, May particularly. Annual sunshine hours in Reykjavík are around 1,300, which is comparable with other places in Northern and North-Eastern Europe. The highest ever recorded temperature in Reykjavík was , recorded on July 30, 2008, while the lowest ever recorded temperature was , recorded on January 30, 1971.
City administration.
The Reykjavík City Council governs the city of Reykjavík according to law number 45/1998 and is directly elected by those aged over 18 domiciled in the city. The council has 15 members who are elected using the open list method for four year terms.
The council selects members of boards, and each board controls a different field under the city council's authority. The most important board is the City Board that wields the executive rights along with the City Mayor. The City Mayor is the senior public official and also the director of city operations. Other public officials control city institutions under the mayor's authority. Thus, the administration consists of two different parts:
Political control.
The Independence Party was traditionally the ruling party for the city, having an overall majority from its establishment in 1929 until 1978, when it was narrowly lost. From 1978 to 1982, a three party coalition composed of the People's Alliance, the Social Democratic Party, and the Progressive Party formed the majority of the council. In 1982, the Independence Party regained an overall majority of the seats which it held for three consecutive terms. In 1994, Icelandic socialist parties formed an alliance called the Reykjavíkurlistinn (R-list) which was led by Ingibjörg Sólrún Gísladóttir to victory. The alliance stood for election for three consecutive city council elections and won a majority in all of them, until it was dissolved for the city council election of 2006 when five different parties were on the ballot. The Independence Party obtained seven members of the council, and thus failed to gain overall control, but together with the Progressive Party, and its one council member, they were able to form a new majority in the council which took over in June 2006.
In October 2007 a new majority was formed on the council, consisting of members of the Progressive Party (1), the Social Democratic Alliance (4), the Left-Greens (2) and the F-list (1) (liberals and independents), after controversy regarding REI, a subsidiary of OR, the city's energy company. However three months later the leader of the F-list formed a new majority together with the Independence Party. Ólafur F. Magnússon, the leader of the F-list, was elected mayor on 24 January 2008, and in March 2009 the Independence Party was due to appoint a new mayor. This changed once again on 14 August 2008 when the fourth majority of the term was formed, when the Independence Party and the Social Democratic Alliance formed a majority, with Hanna Birna Kristjánsdóttir becoming mayor.
The City Council election in May 2010 saw a new political party, The Best Party, win six of 15 seats and they formed a coalition with the Social Democratic Alliance with comedian Jón Gnarr becoming mayor. At the 2014 election, the Social Democratic Alliance had its best showing yet gaining five seats in the council, while Bright future (successor to the Best Party) received two seats and the two parties formed a coalition with the Left-Green movement and the Pirate party both of which received one councilor each. The Independence Party received its worst election with only four seats in the council.
Mayor.
The mayor is appointed by the city council; usually one of the council members is chosen but they may also appoint a mayor who is not a member of the council.
The post was created in 1907 and advertised in 1908. Two applications were received, from Páll Einarsson, sheriff and town mayor of Hafnarfjörður and from Knud Zimsen, town councillor in Reykjavík. Páll was appointed on 7 May and was mayor for six years. At that time the city mayor received a salary of 4500 ISK per year and 1500 ISK for office expenses. The current mayor is Dagur B. Eggertsson.
Demographics.
Reykjavík is the largest and most populous settlement in Iceland. Present-day Reykjavík is a city with people from at least 100 countries. The most common ethnic minorities are Poles, Lithuanians, and Danes. In 2009, foreign-born individuals made up 8% of the total population. Children of foreign origin, many of whom are adopted, form a more considerable minority in the city's schools: as many as a third in places. The city is also visited by thousands of tourists, students, and other temporary residents, at times outnumbering natives in the city centre.
The population of Reykjavík in 2011 was 119,848, and the combined population of the Capital Region was about 202,341. Six of the municipalities of Iceland are in the capital city area:
Districts.
Reykjavík is divided into 10 districts.
Economy.
Borgartún is the financial centre of Reykjavík, hosting a large number of companies and three investment banks.
Reykjavík has been at the centre of Iceland's economic growth and subsequent economic contraction over the last decade, a period referred to in foreign media as the "Nordic Tiger" years, or "Iceland's Boom Years". The economic boom led to a sharp increase in construction, with large redevelopment projects such as Harpa concert hall and conference centre and others.
In 2009, Reykjavík was listed as the richest city in the world in 2007 by The Economist Group.
Infrastructure.
Roads.
Per capita car ownership in Iceland is among the highest in the world at roughly 522 vehicles per 1,000 residents, though Reykjavík is not severely affected by congestion. Several multi-lane highways (mainly dual carriageways) run between the most heavily populated areas and most frequently driven routes. Parking spaces are also plentiful in most areas. Public transportation consists of a bus system called Strætó bs. Route 1 (the Ring Road) runs through the city outskirts and connects the city to the rest of Iceland.
Airports and seaports.
Reykjavík Airport, the second largest airport in the country (after Keflavík International Airport), is positioned inside the city, just south of the city centre. It is mainly used for domestic flights, as well as flights to Greenland and the Faroe Islands. It was built there by the British occupation force during World War II, when it was on the outskirts of the then much smaller Reykjavík. Since 1962, there has been some controversy regarding the location of the airport, since it takes up a lot of valuable space in central Reykjavík.
Reykjavík has two seaports, the old harbour near the city centre which is mainly used by fishermen and cruise ships and "Sundahöfn" in the east city which is the largest cargo port in the country.
Railways.
There are no public railways in Iceland, due to its sparse population, but the locomotives used to build the docks are on display.
District heating.
Volcanic activity provides Reykjavík with geothermal heating systems for both residential and industrial districts. In 2008, natural hot water was used to heat roughly 90% of all buildings in Iceland. Of total annual use of geothermal energy of 39 PJ, space heating accounted for 48%.
Most of the district heating in Iceland comes from three main geothermal power plants:
Cultural heritage.
The "Culture House" was opened in 1909 and has a number of important exhibits. Originally built to house the National Library and National Archives and also previously the location of the National Museum and Natural History Museum, in 2000 it was re-modeled to promote the Icelandic national heritage. Many of Iceland's national treasures are on display, such as the Poetic Edda, and the Sagas in their original manuscripts. There are also changing exhibitions of various topics.
Lifestyle.
Nightlife.
Reykjavík is famous for its weekend nightlife. Icelanders tend to go out late, so bars that look rather quiet can fill up suddenly—usually after midnight on a weekend.
Alcohol is expensive at bars. People tend to drink at home before going out. Beer was banned in Iceland until 1 March 1989, but has since become popular among many Icelanders as their alcoholic drink of choice.
There are over 100 different bars and clubs in Reykjavík; most of them are located on Laugavegur and its side streets. It is very common for an establishment that is a café before dinner to turn into a bar in the evening. Closing time is usually around 4:30 am at weekends and 1 am during the week. The Iceland Airwaves music festival is annually staged in October.
New Year's Eve.
The arrival of the new year is a particular cause for celebration to the people of Reykjavík. Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. As a result, every New Year's Eve the city is lit up with fireworks displays.
Recreation.
Reykjavik Golf Club was established in 1934. It is the oldest and largest golf club in Iceland. It consists of two 18-hole courses - one at Grafarholt and the other at Korpa. The Grafarholt golf course opened in 1963, which makes it the oldest 18-hole golf course in Iceland. The Korpa golf course opened in 1997.
Twin towns and sister cities.
Reykjavík is twinned with:
In July 2013, mayor Jón Gnarr filed a motion before the city council to terminate the city's relationship with Moscow, in response to a trend of anti-gay legislation in Russia. According to "The Daily Telegraph", "Mr Gnarr has long been an advocate for gay rights, appearing in Gay Pride parades in drag"; in 2009, Iceland was the first modern country to have an openly LGBT head of government (Jóhanna Sigurðardóttir, who is a lesbian), and the Alþingi unanimously legalized same-sex marriage in 2010.

</doc>
<doc id="25799" url="https://en.wikipedia.org/wiki?curid=25799" title="Retrovirus">
Retrovirus

"Retroviridae" is a family of enveloped viruses that replicate in a host cell through the process of reverse transcription. A retrovirus is a single-stranded positive-sense RNA virus with a DNA intermediate and, as an obligate parasite, targets a host cell. Once inside the host cell cytoplasm, the virus uses its own reverse transcriptase enzyme to produce DNA from its RNA genome — the reverse of the usual pattern, thus "retro" (backwards). This new DNA is then incorporated into the host cell genome by an integrase enzyme, at which point the retroviral DNA is referred to as a provirus. The host cell then treats the viral DNA as part of its own genome, translating and transcribing the viral genes along with the cell's own genes, producing the proteins required to assemble new copies of the virus. It is difficult to detect the virus until it has infected the host. At that point, the infection will persist indefinitely.
In most viruses, DNA is transcribed into RNA, and then RNA is translated into protein. However, retroviruses function differently – their RNA is reverse-transcribed into DNA, which is integrated into the host cell's genome (when it becomes a provirus), and then undergoes the usual transcription and translational processes to express the genes carried by the virus. So, the information contained in a retroviral gene is used to generate the corresponding protein via the sequence: RNA → DNA → RNA → polypeptide. This extends the fundamental process identified by Francis Crick (one gene-one peptide) in which the sequence is: DNA → RNA → peptide (proteins are made of one or more polypeptide chain; e.g. haemoglobin is a four-chain peptide).
Retroviruses are valuable research tools in molecular biology, and have been used successfully in gene delivery systems.
Structure.
Virions of retroviruses consist of enveloped particles about 100 nm in diameter. The virions also contain two identical single-stranded RNA molecules 7–10 kilobases in length. Although virions of different retroviruses do not have the same morphology or biology, all the virion components are very similar.
The main virion components are:
Multiplication.
"See Retrovirus Replication"
When retroviruses have integrated their own genome into the germ line, their genome is passed on to a following generation. These endogenous retroviruses (ERVs), contrasted with exogenous ones, now make up 5-8% of the human genome. Most insertions have no known function and are often referred to as "junk DNA". However, many endogenous retroviruses play important roles in host biology, such as control of gene transcription, cell fusion during placental development in the course of the germination of an embryo, and resistance to exogenous retroviral infection. Endogenous retroviruses have also received special attention in the research of immunology-related pathologies, such as autoimmune diseases like multiple sclerosis, although endogenous retroviruses have not yet been proven to play any causal role in this class of disease.
While transcription was classically thought to occur only from DNA to RNA, reverse transcriptase transcribes RNA into DNA. The term "retro" in retrovirus refers to this reversal (making DNA from RNA) of the central dogma of molecular biology. Reverse transcriptase activity outside of retroviruses has been found in almost all eukaryotes, enabling the generation and insertion of new copies of retrotransposons into the host genome. These inserts are transcribed by enzymes of the host into new RNA molecules that enter the cytosol. Next, some of these RNA molecules are translated into viral proteins. For example, the "gag" gene is translated into molecules of the capsid protein, the "pol" gene is translated into molecules of reverse transcriptase, and the "env" gene is translated into molecules of the envelope protein. It is important to note that a retrovirus must "bring" its own reverse transcriptase in its capsid, otherwise it is unable to utilize the enzymes of the infected cell to carry out the task, due to the unusual nature of producing DNA from RNA.
Industrial drugs that are designed as protease and reverse transcriptase inhibitors are made such that they target specific sites and sequences within their respective enzymes. However these drugs can quickly become ineffective due to the fact that the gene sequences that code for the protease and the reverse transcriptase quickly mutate. These changes in bases cause specific codons and sites with the enzymes to change and thereby avoid drug targeting by losing the sites that the drug actually targets.
Because reverse transcription lacks the usual proofreading of DNA replication, a retrovirus mutates very often. This enables the virus to grow resistant to antiviral pharmaceuticals quickly, and impedes the development of effective vaccines and inhibitors for the retrovirus.
One difficulty faced with some retroviruses, such as the Moloney retrovirus, involves the requirement for cells to be actively dividing for transduction. As a result, cells such as neurons are very resistant to infection and transduction by retroviruses. This gives rise to a concern that insertional mutagenesis due to integration into the host genome might lead to cancer or leukemia. This is unlike "Lentivirus", a genus of "Retroviridae", which are able to integrate their RNA into the genome of non-dividing host cells.
Genes.
Retrovirus genomes commonly contain these three open reading frames that encode for proteins that can be found in the mature virus:
Provirus.
This DNA can be incorporated into host genome as a provirus that can be passed on to progeny cells. The retrovirus DNA is inserted at random into the host genome. Because of this, it can be inserted into oncogenes. In this way some retroviruses can convert normal cells into cancer cells. Some provirus remains latent in the cell for a long period of time before it is activated by the change in cell environment.
Early evolution.
Studies of retroviruses led to the first demonstrated synthesis of DNA from RNA templates, a fundamental mode for transferring genetic material that occurs in both eukaryotes and prokaryotes. It has been speculated that the RNA to DNA transcription processes used by retroviruses may have first caused DNA to be used as genetic material. In this model, the RNA world hypothesis, cellular organisms adopted the more chemically stable DNA when retroviruses evolved to create DNA from the RNA templates.
Gene therapy.
Gammaretroviral and lentiviral vectors for gene therapy have been developed that mediate stable genetic modification of treated cells by chromosomal integration of the transferred vector genomes. This technology is of use, not only for research purposes, but also for clinical gene therapy aiming at the long-term correction of genetic defects, e.g., in stem and progenitor cells. Retroviral vector particles with tropism for various target cells have been designed. Gammaretroviral and lentiviral vectors have so far been used in more than 300 clinical trials, addressing treatment options for various diseases. Retro viral mutations can be developed to make transgenic mouse models to study various cancers and their metastatic models.
Cancer.
Retroviruses that cause tumor growth include "Rous sarcoma virus" and "Mouse mammary tumor virus". Cancer can be triggered by proto-oncogenes that were mistakenly incorporated into proviral DNA or by the disruption of cellular proto-oncogenes. Rous sarcoma virus contains the src gene that triggers tumor formation. Later it was found that a similar gene in cells is involved in cell signaling, which was most likely excised with the proviral DNA. Nontransforming viruses can randomly insert their DNA into proto-oncogenes, disrupting the expression of proteins that regulate the cell cycle. The promoter of the provirus DNA can also cause over expression of regulatory genes.
Classification.
Exogenous.
These are infectious RNA-containing viruses which are transmitted from human to human.
The following genera are included here:
These were previously divided into three subfamilies ("Oncovirinae", "Lentivirinae", and "Spumavirinae"), but are now divided into two: "Orthoretrovirinae" and "Spumaretrovirinae". The term oncovirus is now commonly used to describe a cancer-causing virus.
Retroviruses were in 2 groups of the Baltimore classification.
Group VI viruses.
All members of Group VI use virally encoded reverse transcriptase, an RNA-dependent DNA polymerase, to produce DNA from the initial virion RNA genome. This DNA is often integrated into the host genome, as in the case of retroviruses and pseudoviruses, where it is replicated and transcribed by the host.
Group VI includes:
Group VII viruses.
Both families in Group VII have DNA genomes contained within the invading virus particles. The DNA genome is transcribed into both mRNA, for use as a transcript in protein synthesis, and pre-genomic RNA, for use as the template during genome replication. Virally encoded reverse transcriptase uses the pre-genomic RNA as a template for the creation of genomic DNA.
Group VII includes:
Endogenous.
Endogenous retroviruses are not formally included in this classification system, and are broadly classified into three classes, on the basis of relatedness to exogenous genera:
Treatment.
Antiretroviral drugs are medications for the treatment of infection by retroviruses, primarily HIV. Different classes of antiretroviral drugs act on different stages of the HIV life cycle. Combination of several (typically three or four) antiretroviral drugs is known as highly active anti-retroviral therapy (HAART).
Treatment of veterinary retroviruses.
"Feline leukemia virus" and "Feline immunodeficiency virus" infections are treated with biologics, including the only immunomodulator currently licensed for sale in the United States, Lymphocyte T-Cell Immune Modulator (LTCI).

</doc>
<doc id="25801" url="https://en.wikipedia.org/wiki?curid=25801" title="Round (music)">
Round (music)

A round or perpetual canon is a musical composition, a limited type of canon, in which a minimum of three voices sing exactly the same melody at the unison (and may continue repeating it indefinitely), but with each voice beginning at different times so that different parts of the melody coincide in the different voices, but nevertheless fit harmoniously together . It is one of the easiest forms of part singing, as only one line of melody need be learned by all parts, and is part of a popular musical tradition. They were particularly favoured in glee clubs, which combined amateur singing with regular drinking (, especially at 21: "Catch-singing is unthinkable without a supply of liquor to hand..."). The earliest known rounds date from the 12th century.
Though not all rounds are nursery rhymes, "Row, Row, Row Your Boat" is a well-known children's round for four voices. Other well-known examples are "Frère Jacques" and "Three Blind Mice" .
A catch is a round in which a phrase that is not apparent in a single line of lyrics emerges when the lyrics are split between the different voices. "Perpetual canon" refers to the end of the melody leading back to the beginning, allowing easy and immediate repetition. Often, "the final cadence is the same as the first measure" .
History.
The term "round" first appears in English in the early 16th century, though the form was found much earlier. In medieval England, they were called rota or rondellus. Later, an alternative term was "roundel" (e.g., David Melvill's manuscript "Ane Buik off Roundells", Aberdeen, 1612). Special types of rounds are the "catch" (a comic English form found from about 1580 to 1800), and a specialized use of the word "canon", in 17th- and 18th-century England designating rounds with religious texts . The oldest surviving round in English is "Sumer Is Icumen In" , which is for four voices, plus two bass voices singing a ground (that is, a never-changing repeating part), also in canon. However, the earliest known rounds are two works with Latin texts found in the eleventh fascicle of the Notre Dame manuscript Pluteo 29.1. They are "Leto leta concio" (a two-voice round) and "O quanto consilio" (a four-voice round). The former dates from before 1180 and may be of German origin . The first published rounds in English were printed by Thomas Ravenscroft in 1609... "Three Blind Mice" appears in this collection, although in a somewhat different form from today's children's round:
Mechanics.
What makes a round work is that after the work is divided into equal-sized blocks of a few measures each, corresponding notes in each block either are the same, or are different notes in the same chord. This is easiest with one chord, as in "Row, Row, Row Your Boat":
A new part can join the singing by starting at the beginning whenever another part reaches any asterisk in the above music. If one ignores the sixteenth notes that pass between the main chords, every single note is in the tonic triad—in this case, a C, E, or G.
Many rounds involve more than one chord, as in "Frère Jacques" :
The texture is simpler, but it uses a few more notes; this can perhaps be more easily seen if all four parts are run together into the same two measures:
The second beat of each measure does not sketch out a tonic triad, it outlines a dominant seventh chord (or "V7 chord").
Classical.
Serious composers who turned their hand to the round format include Thomas Arne, John Blow, William Byrd, Henry Purcell, Louis Hardin, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Benjamin Britten (for example, "Old Joe Has Gone Fishing", sung by the villagers in the pub to keep the peace, at the end of act 1 of "Peter Grimes") . Examples by J. S. Bach include the regular canons, Var. 3 and Var. 24 of the Goldberg Variations, and the perpetual canons, Canon 7 of The Musical Offering and Canon a 2 Perpetuus (BWV 1075) . Several rounds are included amongst Arnold Schoenberg's thirty-plus canons, which "within their natural limitations ... are brilliant pieces, containing too much of the composer's characteristically unexpected blend of seriousness, humour, vigour and tenderness to remain unperformed" .

</doc>
<doc id="25806" url="https://en.wikipedia.org/wiki?curid=25806" title="Reincarnation">
Reincarnation

Reincarnation is the philosophical or religious concept that the soul or spirit, after biological death, can begin a new life in a new body. This doctrine is a central tenet of the Hindu religion. The Buddhist concept of rebirth is also often referred to as reincarnation and is a belief that was held by such historic figures as Pythagoras, Plato, and Socrates. It is also a common belief of various ancient and modern religions such as Spiritism, Theosophy, and Eckankar and is found as well in many tribal societies around the world, in places such as Australia, East Asia, Siberia, and South America.
Although the majority of sects within the Abrahamic religions of Judaism, Christianity, and Islam do not believe that individuals reincarnate, particular groups within these religions do refer to reincarnation; these groups include the mainstream historical and contemporary followers of Kabbalah, the Cathars, Alawites, the Druze, and the Rosicrucians. The historical relations between these sects and the beliefs about reincarnation that were characteristic of Neoplatonism, Orphism, Hermeticism, Manicheanism, and Gnosticism of the Roman era as well as the Indian religions have been the subject of recent scholarly research.
In recent decades, many Europeans and North Americans have developed an interest in reincarnation. Contemporary films, books, and popular songs frequently mention reincarnation.
Conceptual definitions.
The word "reincarnation" derives from Latin, literally meaning, "entering the flesh again". The Greek equivalent "metempsychosis" (μετεμψύχωσις) roughly corresponds to the common English phrase "transmigration of the soul" and also usually connotes reincarnation after death, as either human, animal, though emphasizing the continuity of the soul, not the flesh. The term has been used by modern philosophers such as Kurt Gödel and has entered the English language. Another Greek term sometimes used synonymously is "palingenesis", "being born again".
There is no word corresponding exactly to the English terms "rebirth", "metempsychosis", "transmigration" or "reincarnation" in the traditional languages of Pāli and Sanskrit. The entire universal process that gives rise to the cycle of death and rebirth, governed by karma, is referred to as "Samsara" while the state one is born into, the individual process of being born or coming into the world in any way, is referred to simply as "birth" ("jāti"). "Devas" (gods) may also die and live again. Here the term "reincarnation" is not strictly applicable, yet Hindu gods are said to have reincarnated ("see Avatar"): Lord Vishnu is known for his ten incarnations, the "Dashavatars". Celtic religion seems to have had reincarnating gods also. Many Christians regard Jesus as a divine incarnation. Some Christians and Muslims believe he and some prophets may incarnate again. Most Christians, however, believe that Jesus will come again in the Second Coming at the end of the world, although this is not a reincarnation. Some ghulat Shi'a Muslim sects also regard their founders as in some special sense divine incarnations ("hulul").
Philosophical and religious beliefs regarding the existence or non-existence of an unchanging "self" have a direct bearing on how reincarnation is viewed within a given tradition. The Buddha lived at a time of great philosophical creativity in India when many conceptions of the nature of life and death were proposed. Some were materialist, holding that there was no existence and that the self is annihilated upon death. Others believed in a form of cyclic existence, where a being is born, lives, dies and then is reborn, but in the context of a type of determinism or fatalism in which karma played no role. Others were "eternalists", postulating an eternally existent self or soul comparable to that in Judaic monotheism: the ātman survives death and reincarnates as another living being, based on its karmic inheritance. This is the idea that has become dominant (with certain modifications) in modern Hinduism.
The Buddhist concept of reincarnation differs from others in that there is no eternal "soul", "spirit" or "self" but only a "stream of consciousness" that links life with life. The actual process of change from one life to the next is called "punarbhava" (Sanskrit) or "punabbhava" (Pāli), literally "becoming again", or more briefly "bhava", "becoming", and some English-speaking Buddhists prefer the term "rebirth" or "re-becoming" to render this term as they take "reincarnation" to imply a fixed entity that is reborn. Popular Jain cosmology and Buddhist cosmology as well as a number of schools of Hinduism posit rebirth in many worlds and in varied forms. In Buddhist tradition the process occurs across five or six realms of existence, including the human, any kind of animal and several types of supernatural being. It is said in Tibetan Buddhism that it is very rare for a person to be reborn in the immediate next life as a human.
"Gilgul", "Gilgul neshamot" or "Gilgulei Ha Neshamot" (Heb. גלגול הנשמות) refers to the concept of reincarnation in Kabbalistic Judaism, found in much Yiddish literature among Ashkenazi Jews. "Gilgul" means "cycle" and "neshamot" is "souls". Version of Kabbalistic reincarnation says that humans reincarnate only to humans and to the same sex only: men to men, women to women.
The equivalent Arabic term is "tanasukh": the belief is found among Shi'a ghulat Muslim sects.
History.
Origins.
The origins of the notion of reincarnation are obscure. Discussion of the subject appears in the philosophical traditions of India. The Greek Pre-Socratics discussed reincarnation, and the Celtic Druids are also reported to have taught a doctrine of reincarnation.
The ideas associated with reincarnation may have arisen independently in different regions, or they might have spread as a result of cultural contact. Proponents of cultural transmission have looked for links between Iron Age Celtic, Greek and Vedic philosophy and religion, some even suggesting that belief in reincarnation was present in Proto-Indo-European religion. In ancient European, Iranian and Indian agricultural cultures, the life cycles of birth, death, and rebirth were recognized as a replica of natural agricultural cycles.
Early Jainism, Hinduism and Buddhism.
Patrick Olivelle asserts that the origin of the concept of the cycle of birth and death, the concept of samsara, and the concept of liberation in the Indian tradition, were in part the creation of the non-Vedic Shramana tradition. Another possible source of reincarnation beliefs is that they are from the prehistoric Dravidian traditions of South India. Some scholars suggest that the idea is original to the Buddha.
In Jainism, the soul and matter are considered eternal, not created and perpetual. There is a constant interplay between the two, resulting in bewildering cosmic manifestations in material, psychic and emotional spheres around us. This led to the theories of transmigration and rebirth. Changes but not total annihilation of spirit and matter is the basic postulate of Jain philosophy. The life as we know now, after death therefore moves on to another form of life based on the merits and demerits it accumulated in its current life. The path to becoming a supreme soul is to practice non-violence and be truthful.
In Hinduism's Rigveda, the oldest extant Indo-Aryan text, numerous references are made to transmigration, rebirth (punarjanma), and redeath (punarmrtyu) in the Brahmanas.
Indian discussion of reincarnation enters the historical record from about the 6th century BCE, with the development of the Advaita Vedanta tradition in the early Upanishads (around the middle of the first millennium BCE), Gautama Buddha (623–543 BCE) as well as Mahavira, the 24th Tirthankara of Jainism.
The systematic attempt to attain first-hand knowledge of past lives has been developed in various ways in different places.
The early Buddhist texts discuss techniques for recalling previous births, predicated on the development of high levels of meditative concentration. The later Yoga Sutras of Patanjali, which incorporated elements of Buddhist thought, give similar instructions on how to attain the ability. The Buddha reportedly warned that this experience can be misleading and should be interpreted with care. Tibetan Buddhism has developed a unique "science" of death and rebirth, a good deal of which is set down in what is popularly known as "The Tibetan Book of the Dead".
Early Greece.
Early Greek discussion of the concept likewise dates to the 6th century BCE. An early Greek thinker known to have considered rebirth is Pherecydes of Syros (fl. 540 BCE). His younger contemporary Pythagoras (c. 570–c. 495 BCE), its first famous exponent, instituted societies for its diffusion. Plato (428/427–348/347 BCE) presented accounts of reincarnation in his works, particularly the "Myth of Er".
Authorities have not agreed on how the notion arose in Greece: sometimes Pythagoras is said to have been Pherecydes' pupil, sometimes to have introduced it with the doctrine of Orphism, a Thracian religion that was to be important in the diffusion of reincarnation, or else to have brought the teaching from India. In "Phaedo", Plato makes his teacher Socrates, prior to his death, state: "I am confident that there truly is such a thing as living again, and that the living spring from the dead." However Xenophon does not mention Socrates as believing in reincarnation and Plato may have systematised Socrates' thought with concepts he took directly from Pythagoreanism or Orphism.
Classical Antiquity.
The Orphic religion, which taught reincarnation, first appeared in Thrace in Bulgaria and north-eastern Greece, about the 6th century BC, organized itself into mystery schools at Eleusis and elsewhere, and produced a copious literature. Orpheus, its legendary founder, is said to have taught that the immortal soul aspires to freedom while the body holds it prisoner. The wheel of birth revolves, the soul alternates between freedom and captivity round the wide circle of necessity. Orpheus proclaimed the need of the grace of the gods, Dionysus in particular, and of self-purification until the soul has completed the spiral ascent of destiny to live for ever.
An association between Pythagorean philosophy and reincarnation was routinely accepted throughout antiquity. In the "Republic" Plato makes Socrates tell how Er, the son of Armenius, miraculously returned to life on the twelfth day after death and recounted the secrets of the other world. There are myths and theories to the same effect in other dialogues, in the Chariot allegory of the Phaedrus, in the Meno, Timaeus and Laws. The soul, once separated from the body, spends an indeterminate amount of time in "formland" (see The Allegory of the Cave in "The Republic") and then assumes another body.
In later Greek literature the doctrine is mentioned in a fragment of Menander and satirized by Lucian. In Roman literature it is found as early as Ennius, who, in a lost passage of his "Annals", told how he had seen Homer in a dream, who had assured him that the same soul which had animated both the poets had once belonged to a peacock. Persius in his satires (vi. 9) laughs at this, it is referred to also by Lucretius and Horace.
Virgil works the idea into his account of the Underworld in the sixth book of the Aeneid. It persists down to the late classic thinkers, Plotinus and the other Neoplatonists. In the Hermetica, a Graeco-Egyptian series of writings on cosmology and spirituality attributed to Hermes Trismegistus/Thoth, the doctrine of reincarnation is central.
In Greco-Roman thought, the concept of metempsychosis disappeared with the rise of Early Christianity, reincarnation being incompatible with the Christian core doctrine of salvation of the faithful after death. It has been suggested that some of the early Church Fathers, especially Origen still entertained a belief in the possibility of reincarnation, but evidence is tenuous, and the writings of Origen as they have come down to us speak explicitly against it.
Some early Christian Gnostic sects professed reincarnation. The Sethians and followers of Valentinus believed in it. The followers of Bardaisan of Mesopotamia, a sect of the 2nd century deemed heretical by the Catholic Church, drew upon Chaldean astrology, to which Bardaisan's son Harmonius, educated in Athens, added Greek ideas including a sort of metempsychosis. Another such teacher was Basilides (132–? CE/AD), known to us through the criticisms of Irenaeus and the work of Clement of Alexandria. "(see also Neoplatonism and Gnosticism and Buddhism and Gnosticism)"
In the third Christian century Manichaeism spread both east and west from Babylonia, then within the Sassanid Empire, where its founder Mani lived about 216–276. Manichaean monasteries existed in Rome in 312 AD. Noting Mani's early travels to the Kushan Empire and other Buddhist influences in Manichaeism, Richard Foltz attributes Mani's teaching of reincarnation to Buddhist influence. However the inter-relation of Manicheanism, Orphism, Gnosticism and neo-Platonism is far from clear.
The Celts.
In the 1st century BCE Alexander Cornelius Polyhistor wrote:
Julius Caesar recorded that the druids of Gaul, Britain and Ireland had metempsychosis as one of their core doctrines:
Judaism.
In Judaism, the Zohar, first publicized in the 13th century, discusses reincarnation at length, especially in the Torah portion "Balak." The most comprehensive kabbalistic work on reincarnation, "Shaar HaGilgulim", was written by Rabbi Chaim Vital, based on the teachings of his mentor, the 16th century kabbalist Rabbi Isaac Luria, who was said to know the past lives of each person through his semi-prophetic abilities. The 18th century Lithuanian master scholar and kabbalist, Rabbi Elijah, known as the Vilna Gaon (Elijah of Vilna), authored a commentary on the biblical Book of Jonah as an allegory of reincarnation.
According to the "Jewish Encyclopedia", the philosophy of metempsychosis entered Judaism during the eighth century, under the influences of Islamic mysticism.
There is an extensive literature of Jewish folk and traditional stories that refer to reincarnation.
Taoism.
Taoist documents from as early as the Han Dynasty claimed that Lao Tzu appeared on earth as different persons in different times beginning in the legendary era of Three Sovereigns and Five Emperors. The (ca. 3rd century BC) "Chuang Tzu" states: "Birth is not a beginning; death is not an end. There is existence without limitation; there is continuity without a starting-point. Existence without limitation is Space. Continuity without a starting point is Time. There is birth, there is death, there is issuing forth, there is entering in."
Middle Ages.
Around the 11–12th century several reincarnationist movements were persecuted as heresies, through the establishment of the Inquisition in the Latin west. These included the Cathar, Paterene or Albigensian church of western Europe, the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria.
Christian sects such as the Bogomils and the Cathars, who professed reincarnation and other gnostic beliefs, were referred to as "Manichean", and are today sometimes described by scholars as "Neo-Manichean". As there is no known Manichaean mythology or terminology in the writings of these groups there has been some dispute among historians as to whether these groups truly were descendants of Manichaeism.
Norse mythology.
Reincarnation also appears in Norse mythology, in the "Poetic Edda". The editor of the "Poetic Edda" says that Helgi Hjörvarðsson and his mistress, the valkyrie Sváfa, whose love story is told in the poem "Helgakviða Hjörvarðssonar", were reborn as Helgi Hundingsbane and the valkyrie Sigrún. Helgi and Sigrún's love story is the matter of a part of the "Völsunga saga" and the lays "Helgakviða Hundingsbana I and II". They were reborn a second time as Helgi Haddingjaskati and the valkyrie Kára, but unfortunately their story, "Káruljóð", only survives in a probably modified form in the "Hrómundar saga Gripssonar".
The belief in reincarnation may have been commonplace among the Norse since the annotator of the "Poetic Edda" wrote that people formerly used to believe in it:
Renaissance and Early Modern period.
While reincarnation has been a matter of faith in some communities from an early date it has also frequently been argued for on principle, as Plato does when he argues that the number of souls must be finite because souls are indestructible, Benjamin Franklin held a similar view. Sometimes such convictions, as in Socrates' case, arise from a more general personal faith, at other times from anecdotal evidence such as Plato makes Socrates offer in the "Myth of Er".
During the Renaissance translations of Plato, the Hermetica and other works fostered new European interest in reincarnation. Marsilio Ficino argued that Plato's references to reincarnation were intended allegorically, Shakespeare made fun but Giordano Bruno was burned at the stake by authorities after being found guilty of heresy by the Roman Inquisition for his teachings. But the Greek philosophical works remained available and, particularly in north Europe, were discussed by groups such as the Cambridge Platonists.
19th to 20th centuries.
By the 19th century the philosophers Schopenhauer and Nietzsche could access the Indian scriptures for discussion of the doctrine of reincarnation, which recommended itself to the American Transcendentalists Henry David Thoreau, Walt Whitman and Ralph Waldo Emerson and was adapted by Francis Bowen into "Christian Metempsychosis".
By the early 20th century, interest in reincarnation had been introduced into the nascent discipline of psychology, largely due to the influence of William James, who raised aspects of the philosophy of mind, comparative religion, the psychology of religious experience and the nature of empiricism. James was influential in the founding of the American Society for Psychical Research (ASPR) in New York City in 1885, three years after the British Society for Psychical Research (SPR) was inaugurated in London, leading to systematic, critical investigation of paranormal phenomena.
At this time popular awareness of the idea of reincarnation was boosted by the Theosophical Society's dissemination of systematised and universalised Indian concepts and also by the influence of magical societies like The Golden Dawn. Notable personalities like Annie Besant, W. B. Yeats and Dion Fortune made the subject almost as familiar an element of the popular culture of the west as of the east. By 1924 the subject could be satirised in popular children's books.
Théodore Flournoy was among the first to study a claim of past-life recall in the course of his investigation of the medium Hélène Smith, published in 1900, in which he defined the possibility of cryptomnesia in such accounts.
Carl Gustav Jung, like Flournoy based in Switzerland, also emulated him in his thesis based on a study of cryptomnesia in psychism. Later Jung would emphasise the importance of the persistence of memory and ego in psychological study of reincarnation: "This concept of rebirth necessarily implies the continuity of personality... (that) one is able, at least potentially, to remember that one has lived through previous existences, and that these existences were one's own..." Hypnosis, used in psychoanalysis for retrieving forgotten memories, was eventually tried as a means of studying the phenomenon of past life recall.
Reincarnation research.
Psychiatrist Ian Stevenson, from the University of Virginia, investigated many reports of young children who claimed to remember a past life. He conducted more than 2,500 case studies over a period of 40 years and published twelve books, including "Twenty Cases Suggestive of Reincarnation" and "Where Reincarnation and Biology Intersect". Stevenson methodically documented each child's statements and then identified the deceased person the child identified with, and verified the facts of the deceased person's life that matched the child's memory. He also matched birthmarks and birth defects to wounds and scars on the deceased, verified by medical records such as autopsy photographs, in "Reincarnation and Biology".
Stevenson searched for disconfirming evidence and alternative explanations for the reports, and believed that his strict methods ruled out all possible "normal" explanations for the child’s memories. However, a significant majority of Stevenson's reported cases of reincarnation originated in Eastern societies, where dominant religions often permit the concept of reincarnation. Following this type of criticism, Stevenson published a book on "European Cases of the Reincarnation Type". Other people who have undertaken reincarnation research include Jim B. Tucker, Antonia Mills, Satwant Pasricha, Godwin Samararatne, and Erlendur Haraldsson.
Skeptics such as Paul Edwards have analyzed many of these accounts, and called them anecdotal, while also suggesting that claims of evidence for reincarnation originate from selective thinking and from the false memories that often result from one's own belief system and basic fears, and thus cannot be counted as empirical evidence. Carl Sagan referred to examples apparently from Stevenson's investigations in his book "The Demon-Haunted World" as an example of carefully collected empirical data, though he rejected reincarnation as a parsimonious explanation for the stories. Sam Harris cited Stevenson's works in his book "The End of Faith" as part of a body of data that seems to attest to the reality of psychic phenomena.
Stevenson claimed there were a handful of cases that suggested evidence of xenoglossy. These included two where a subject under hypnosis could allegedly converse with people speaking the foreign language, instead of merely being able to recite foreign words. Sarah Thomason, a linguist at the University of Michigan, reanalyzed these cases, concluding that "the linguistic evidence is too weak to provide support for the claims of xenoglossy."
Ian Wilson argued that a large number of Stevenson’s cases consisted of poor children remembering wealthy lives or belonging to a higher caste. He speculated that such cases may represent a scheme to obtain money from the family of the alleged former incarnation. The philosopher Keith Augustine has written "the vast majority of Stevenson's cases come from countries where a religious belief in reincarnation is strong, and rarely elsewhere, seems to indicate that cultural conditioning (rather than reincarnation) generates claims of spontaneous past-life memories." According to the research of Robert Baker many of the alleged past-life experiences investigated by Stevenson and other parapsychologists can be explained in terms of known psychological factors. Baker has written the recalling of past lives is a mixture of cryptomnesia and confabulation. The philosopher Paul Edwards noted that reincarnation invokes assumptions and is inconsistent with modern science.
Objections to claims of reincarnation include the facts that the vast majority of people do not remember previous lives and there is no mechanism known to modern science that would enable a personality to survive death and travel to another body, barring the idea of biocentrism. Researchers such as Stevenson have acknowledged these limitations.
Reincarnation in the West.
During recent decades, many people in the West have developed an interest in reincarnation. Feature films, several based on popular novels, such as "The Reincarnation of Peter Proud", "Dead Again", "Kundun", "Fluke", "What Dreams May Come", "The Mummy", "Birth", "Chances Are" and "Cloud Atlas", contemporary books by authors such as Carol Bowman and Vicki Mackenzie, as well as popular songs, deal with reincarnation.
Recent studies have indicated that some Westerners accept the idea of reincarnation including certain contemporary people who were to Catholic families, modern Neopagans, followers of Spiritism, Theosophists and students of esoteric philosophies such as Kabbalah, and Gnostic and Esoteric Christianity as well as of Indian religions. Demographic survey data from 1999–2002 shows a significant minority of people from Europe and America, where there is reasonable freedom of thought and access to ideas but no outstanding recent reincarnationist tradition, believe we had a life before we were born, will survive death and be born again physically. The mean for the Nordic countries is 22%. The belief in reincarnation is particularly high in the Baltic countries, with Lithuania having the highest figure for the whole of Europe, 44%. The lowest figure is in East Germany, 12%. In Russia, about one-third believes in reincarnation. The effect of communist anti-religious ideas on the beliefs of the populations of Eastern Europe seems to have been rather slight, if any, except apparently in East Germany. Overall, 22% of respondents in Western Europe believe in reincarnation. According to a 2005 Gallup poll 20 percent of U.S. adults believe in reincarnation. Recent surveys by the Barna Group, a Christian research nonprofit organization, have found that a quarter of U.S. Christians, including 10 percent of all born-again Christians, embrace the idea.
Skeptic Carl Sagan asked the Dalai Lama what he would do if a fundamental tenet of his religion (reincarnation) were definitively disproved by science. The Dalai Lama answered, "If science can disprove reincarnation, Tibetan Buddhism would abandon reincarnation… but it's going to be mighty hard to disprove reincarnation."
Ian Stevenson reported that belief in reincarnation is held (with variations in details) by adherents of almost all major religions except Christianity and Islam. In addition, between 20 and 30 percent of persons in western countries who may be nominal Christians also believe in reincarnation.
One 1999 study by Walter and Waterhouse reviewed the previous data on the level of reincarnation belief and performed a set of thirty in-depth interviews in Britain among people who did not belong to a religion advocating reincarnation. The authors reported that surveys have found about one fifth to one quarter of Europeans have some level of belief in reincarnation, with similar results found in the USA. In the interviewed group, the belief in the existence of this phenomenon appeared independent of their age, or the type of religion that these people belonged to, with most being Christians. The beliefs of this group also did not appear to contain any more than usual of "new age" ideas (broadly defined) and the authors interpreted their ideas on reincarnation as "one way of tackling issues of suffering", but noted that this seemed to have little effect on their private lives.
Waterhouse also published a detailed discussion of beliefs expressed in the interviews. She noted that although most people "hold their belief in reincarnation quite lightly" and were unclear on the details of their ideas, personal experiences such as past-life memories and near-death experiences had influenced most believers, although only a few had direct experience of these phenomena. Waterhouse analyzed the influences of second-hand accounts of reincarnation, writing that most of the people in the survey had heard other people's accounts of past-lives from regression hypnosis and dreams and found these fascinating, feeling that there "must be something in it" if other people were having such experiences.
Contemporary religious philosophies.
Hinduism.
Reincarnation – known as Punarjanma – it is one of the core beliefs of Hinduism that is generally accepted by many of its practitioners.
Reincarnation is the natural process of birth, death and rebirth. Hindus believe that the Jiva or Atman (soul) is intrinsically pure. However, because of the layers of I-ness and My-ness, the jiva goes through transmigration in the cycle of births and deaths. Death destroys the physical body, but not the jiva. The jiva is eternal. It takes on another body with respect to its karmas. Every karma produces a result which must be experienced either in this or some future life. As long as the jiva is enveloped in ignorance, it remains attached to material desires and subject to the cycles of births and deaths (Samsara).
There is no permanent heaven or hell in Hinduism. After services in the afterlife, the "jiva" enters the karma and rebirth system, reborn as an animal, a human or a divinity. This reincarnation continues until "mokṣa", the final release, is gained.
The Bhagavad Gita states;
and,
According to the Hindu sage Adi Shankaracharya, the world – as we ordinarily understand it – is like a dream: fleeting and illusory. To be trapped in samsara (the cycle of birth and death) is a result of ignorance of the true nature of our existence. It is ignorance ("avidya") of one's true self that leads to ego-consciousness, grounding one in desire and a perpetual chain of reincarnation. The idea is intricately linked to action ("karma"), a concept first recorded in the Upanishads. Every action has a reaction and the force determines one's next incarnation. One is reborn through desire: a person "desires" to be born because he or she wants to enjoy a body, which can never bring deep, lasting happiness or peace ("ānanda"). After many births every person becomes dissatisfied and begins to seek higher forms of happiness through spiritual experience. When, after spiritual practice (sādhanā), a person realizes that the true "self" is the immortal soul rather than the body or the ego all desires for the pleasures of the world will vanish since they will seem insipid compared to spiritual "ānanda". When all desire has vanished the person will not be born again. When the cycle of rebirth thus comes to an end, a person is said to have attained liberation ("moksha"). All schools agree this implies the cessation of worldly desires and freedom from the cycle of birth and death, though the exact definition differs. Followers of the Advaita Vedanta school believe they will spend eternity absorbed in the perfect peace and happiness of the realization that all existence is One "Brahman" of which the soul is part. Dvaita schools perform worship with the goal of spending eternity in a spiritual world or heaven ("loka") in the blessed company of the Supreme Being.
Reasons for Reincarnation.
Hindus provide several reasons why the jiva takes on various physical bodies:
Jainism.
Jainism is historically connected with the "sramana" tradition with which the earliest mentions of reincarnation are associated.
Karma forms a central and fundamental part of Jain faith, being intricately connected to other of its philosophical concepts like transmigration, reincarnation, liberation, non-violence ("ahiṃsā") and non-attachment, among others. Actions are seen to have consequences: some immediate, some delayed, even into future incarnations. So the doctrine of karma is not considered simply in relation to one life-time, but also in relation to both future incarnations and past lives. "Uttarādhyayana-sūtra" 3.3–4 states: "The "jīva" or the soul is sometimes born in the world of gods, sometimes in hell. Sometimes it acquires the body of a demon; all this happens on account of its karma. This "jīva" sometimes takes birth as a worm, as an insect or as an ant." The text further states (32.7): "Karma is the root of birth and death. The souls bound by karma go round and round in the cycle of existence."
Actions and emotions in the current lifetime affect future incarnations depending on the nature of the particular karma. For example, a good and virtuous life indicates a latent desire to experience good and virtuous themes of life. Therefore, such a person attracts karma that ensures that his future births will allow him to experience and manifest his virtues and good feelings unhindered. In this case, he may take birth in heaven or in a prosperous and virtuous human family. On the other hand, a person who has indulged in immoral deeds, or with a cruel disposition, indicates a latent desire to experience cruel themes of life. As a natural consequence, he will attract karma which will ensure that he is reincarnated in hell, or in lower life forms, to enable his soul to experience the cruel themes of life.
There is no retribution, judgment or reward involved but a natural consequences of the choices in life made either knowingly or unknowingly. Hence, whatever suffering or pleasure that a soul may be experiencing in its present life is on account of choices that it has made in the past. As a result of this doctrine, Jainism attributes supreme importance to pure thinking and moral behavior.
The Jain texts postulate four "gatis", that is states-of-existence or birth-categories, within which the soul transmigrates. The four "gatis" are: "deva" (demi-gods), "manuṣya" (humans), "nāraki" (hell beings) and "tiryañca" (animals, plants and micro-organisms). The four "gatis" have four corresponding realms or habitation levels in the vertically tiered Jain universe: demi-gods occupy the higher levels where the heavens are situated; humans, plants and animals occupy the middle levels; and hellish beings occupy the lower levels where seven hells are situated.
Single-sensed souls, however, called "nigoda", and element-bodied souls pervade all tiers of this universe. "Nigodas" are souls at the bottom end of the existential hierarchy. They are so tiny and undifferentiated, that they lack even individual bodies, living in colonies. According to Jain texts, this infinity of "nigodas" can also be found in plant tissues, root vegetables and animal bodies. Depending on its karma, a soul transmigrates and reincarnates within the scope of this cosmology of destinies. The four main destinies are further divided into sub-categories and still smaller sub-sub-categories. In all, Jain texts speak of a cycle of 8.4 million birth destinies in which souls find themselves again and again as they cycle within "samsara".
In Jainism, God has no role to play in an individual's destiny; one's personal destiny is not seen as a consequence of any system of reward or punishment, but rather as a result of its own personal karma. A text from a volume of the ancient Jain canon, "Bhagvati sūtra" 8.9.9, links specific states of existence to specific karmas. Violent deeds, killing of creatures having five sense organs, eating fish, and so on, lead to rebirth in hell. Deception, fraud and falsehood lead to rebirth in the animal and vegetable world. Kindness, compassion and humble character result in human birth; while austerities and the making and keeping of vows lead to rebirth in heaven.
Each soul is thus responsible for its own predicament, as well as its own salvation. Accumulated karma represent a sum total of all unfulfilled desires, attachments and aspirations of a soul. It enables the soul to experience the various themes of the lives that it desires to experience. Hence a soul may transmigrate from one life form to another for countless of years, taking with it the karma that it has earned, until it finds conditions that bring about the required fruits. In certain philosophies, heavens and hells are often viewed as places for eternal salvation or eternal damnation for good and bad deeds. But according to Jainism, such places, including the earth are simply the places which allow the soul to experience its unfulfilled karma.
Buddhism.
The early Buddhist texts make it clear that there is no permanent consciousness that moves from life to life. Gautama Buddha taught a distinct concept of rebirth constrained by the concepts of anattā, that there is no irreducible ātman or "self" tying these lives together (which serves as a contrast to Hinduism, where everything is connected, and in a sense, "everything is everything"), and anicca, that all compounded things are subject to dissolution, including all the components of the human person and personality.
In Buddhist doctrine the evolving consciousness (Pali: "samvattanika-viññana") or stream of consciousness (Pali: "viññana-sotam", Sanskrit: "vijñāna-srotām, vijñāna-santāna", or "citta-santāna") upon death (or "the dissolution of the aggregates" (P. "khandha"s, S. "skandha"s)), becomes one of the contributing causes for the arising of a new aggregation. At the death of one personality, a new one comes into being, much as the flame of a dying candle can serve to light the flame of another. The consciousness in the new person is neither identical to nor entirely different from that in the deceased but the two form a causal continuum or stream. Transmigration is the effect of "karma" ("kamma") or volitional action. The basic cause is the abiding of consciousness in ignorance (Pali: "avijja", Sanskrit: "avidya"): when ignorance is uprooted, rebirth ceases.
The Buddha's detailed conception of the connections between action (karma), rebirth and causality is set out in the twelve links of dependent origination. The empirical, changing self does not only affect the world about it, it also generates, consciously and unconsciously, a subjective image of the world in which it lives as "reality". It "tunes in" to a particular level of consciousness which has a particular range of objects, selectively notices such objects and forms a partial model of reality in which the ego is the crucial reference point. Vipassana meditation uses "bare attention" to mind-states without interfering, owning or judging. Observation reveals each moment as an experience of an individual mind-state such as a thought, a memory, a feeling or a perception that arises, exists and ceases. This limits the power of desire, which, according to the second noble truth of Buddhism, is the cause of suffering ("dukkha"), and leads to "Nirvana" ("nibbana", vanishing (of the self-idea)) in which self-oriented models are transcended and "the world stops". Thus consciousness is a continuous birth and death of mind-states: rebirth is the persistence of this process.
Buddhist traditions vary in precise views on rebirth. The Tibetan schools hold to the notion of a bardo (intermediate state) that can last up to forty-nine days. An accomplished or realized practitioner (by maintaining conscious awareness during the death process) can choose to return to samsara. They believe many lamas choose to be born again and again as humans and are called "tulkus" or incarnate lamas. The Sarvastivada school believed that between death and rebirth there is a sort of limbo in which beings do not yet reap the consequences of their previous actions but may still influence their rebirth. The death process and this intermediate state were believed to offer a uniquely favourable opportunity for spiritual awakening. Theravada Buddhism generally denies there is an intermediate state—though some early Buddhist texts seem to support the idea-- but asserts that rebirth is immediate.
Within Japanese Zen, reincarnation is accepted by some, but wholly rejected by others. A distinction can be drawn between "folk Zen", as in the Zen practiced by devotional lay people, and "philosophical Zen". Folk Zen generally accepts the various supernatural elements of Buddhism such as rebirth. Philosophical Zen, however, places such emphasis on the present moment that rebirth may be considered irrelevant because, even if it does exist, it can never be consciously experienced. Specifically, in Zen the past and future are considered to be merely ideas which are held in the present. Because as living beings rebirth can only be viewed as something which may have happened in the past or that might happen in the future, we must essentially reject the present moment, or Dharma, in order to even consider it. For this reason, rebirth is often either rejected or considered unknowable in Zen and therefore a distraction. Dōgen Zenji, the founder of Japanese Sōtō Zen, writes the following regarding reincarnation:
Some schools conclude that karma continues to exist and adhere to the person until it works out its consequences. For the Sautrantika school, each act "perfumes" the individual or "plants a seed" that later germinates. Tibetan Buddhism stresses the state of mind at the time of death. To die with a peaceful mind will stimulate a virtuous seed and a fortunate rebirth; a disturbed mind will stimulate a non-virtuous seed and an unfortunate rebirth. The medieval Pali scholar Buddhaghosa labeled the consciousness that constitutes the condition for a new birth as described in the early texts "rebirth-linking consciousness" ("patisandhi").
Still other Buddhists regard samsara as merely a metaphor of the human condition.
Sant mystics and Sikhism.
Reincarnation remained a tenet of the Sant Bhakti movement and of related mystics on the frontiers of Islam and Hinduism such as the Baul minstrels, the Kabir panth and the Sikh Panth. Sikhs believe the soul is passed from one body to another until Liberation. If we perform good deeds and actions and remember the Creator, we attain a better life while, if we carry out evil actions and sinful deeds, we will be incarnated in “lower” life forms. God may pardon wrongs and release us. Otherwise reincarnation is due to the law of cause and effect but does not create any caste or differences among people. Some scholars consider Eckankar a Western presentation of Sant mysticism. It teaches that Soul is eternal and either chooses an incarnation for growth or else an incarnation is imposed or agreed to because of Karma. Soul is perfected through a series of incarnations until it arrives at a level of spiritual development that obviates the need for further experience in what are described as the "lower worlds" of experience in time and space.
African Vodun.
The Yoruba believe in reincarnation within the family. The names Babatunde (Father returns), Yetunde (Mother returns), Babatunji (Father wakes once again) and Sotunde (The wise man returns) all offer vivid evidence of the Ifa concept of familial or lineal rebirth. There is no simple guarantee that your grandfather or great uncle will "come back" in the birth of your child, however.
Whenever the time arrives for a spirit to return to Earth (otherwise known as The Marketplace) through the conception of a new life in the direct bloodline of the family, one of the component entities of a person's being returns, while the other remains in Heaven (Ikole Orun). The spirit that returns does so in the form of a Guardian Ori. One's Guardian Ori, which is represented and contained in the crown of the head, represents not only the spirit and energy of one's previous blood relative, but the accumulated wisdom he or she has acquired through a myriad of lifetimes. This is not to be confused with one’s spiritual Ori, which contains personal destiny, but instead refers to the coming back to The Marketplace of one's personal blood Ori through one's new life and experiences.
Judaism.
Jewish mystical texts (the Kabbalah), from their classic Medieval canon onwards, teach a belief in "Gilgul Neshamot" (Hebrew for metempsychosis of souls: literally "soul cycle", plural ""gilgulim""). It is a common belief in contemporary Hasidic Judaism, which regards the Kabbalah as sacred and authoritative, though unstressed in favour of a more innate psychological mysticism. Kabbalah also teaches that "The soul of Moses is reincarnated in every generation." Other, Non-Hasidic, Orthodox Jewish groups while not placing a heavy emphasis on reincarnation, do acknowledge it as a valid teaching. Its popularisation entered modern secular Yiddish literature and folk motif.
The 16th century mystical renaissance in communal Safed replaced scholastic Rationalism as mainstream traditional Jewish theology, both in scholarly circles and in the popular imagination. References to "gilgul" in former Kabbalah became systemised as part of the metaphysical purpose of creation. Isaac Luria (the Ari) brought the issue to the centre of his new mystical articulation, for the first time, and advocated identification of the reincarnations of historic Jewish figures that were compiled by Haim Vital in his Shaar HaGilgulim. "Gilgul" is contrasted with the other processes in Kabbalah of Ibbur ("pregnancy"), the attachment of a second soul to an individual for (or by) good means, and Dybuk ("possession"), the attachment of a spirit, demon, etc. to an individual for (or by) "bad" means.
In Lurianic Kabbalah, reincarnation is not retributive or fatalistic, but an expression of Divine compassion, the microcosm of the doctrine of cosmic rectification of creation. "Gilgul" is a heavenly agreement with the individual soul, conditional upon circumstances. Luria's radical system focused on rectification of the Divine soul, played out through Creation. The true essence of anything is the divine spark within that gives it existence. Even a stone or leaf possesses such a soul that "came into this world to receive a rectification". A human soul may occasionally be exiled into lower inanimate, vegetative or animal creations. The most basic component of the soul, the nefesh, must leave at the cessation of blood production. There are four other soul components and different nations of the world possess different forms of souls with different purposes. Each Jewish soul is reincarnated in order to fulfil each of the 613 Mosaic commandments that elevate a particular spark of holiness associated with each commandment. Once all the Sparks are redeemed to their spiritual source, the Messianic Era begins. Non-Jewish observance of the 7 Laws of Noah assists the Jewish people, though Biblical adversaries of Israel reincarnate to oppose.
Among the many rabbis who accepted reincarnation are Nahmanides (the Ramban) and Rabbenu Bahya ben Asher, Levi ibn Habib (the Ralbah), Shelomoh Alkabez, Moses Cordovero, Moses Chaim Luzzatto, the Baal Shem Tov and later Hasidic masters, DovBer Pinson and the Mitnagdic Vilna Gaon and Chaim Volozhin and their school, Ben Ish Chai of Baghdad and the Baba Sali. Rabbis who have rejected the idea include Saadia Gaon, David Kimhi, Hasdai Crescas, Joseph Albo, Abraham ibn Daud, Leon de Modena, Solomon ben Aderet, Maimonides and Asher ben Jehiel. Among the Geonim, Hai Gaon argued in favour of "gilgulim".
Native American nations.
Reincarnation is an intrinsic part of many Native American and Inuit traditions. In the now heavily Christian Polar North (now mainly parts of Greenland and Nunavut), the concept of reincarnation is enshrined in the Inuit language.
The following is a story of human-to-human reincarnation as told by Thunder Cloud, a Winnebago (Ho-Chunk tribe) shaman referred to as T. C. in the narrative. Here T. C. talks about his two previous lives and how he died and came back again to this his third lifetime. He describes his time between lives, when he was “blessed” by Earth Maker and all the abiding spirits and given special powers, including the ability to heal the sick.
T. C.'s Account of His Two Reincarnations:
Christianity.
Though the major Christian denominations reject the concept of reincarnation, a large number of Christians profess the belief. In a survey by the Pew Forum in 2009, 24% of American Christians expressed a belief in reincarnation. In a 1981 Survey in Europe 31% of regular churchgoing Catholics expressed a belief in reincarnation.
Geddes MacGregor, an Episcopalian priest and professor of Philosophy, makes a case for the compatibility of Christian doctrine and reincarnation.
There is evidence that the writing of Origen, a Church father in early Christian times, was mistranslated into Latin due to religious bias and that he taught reincarnation in his lifetime. One of the epistles written by St. Jerome, "To Avitus" (Letter 124 ; Ad Avitum. Epistula CXXIV), asserts that Origen's "On First Principles" (Latin: "De Principiis"; Greek: Περὶ Ἀρχῶν) was mistranscribed from Greek into Latin:
Under the impression that Origen was a heretic like Arius, St. Jerome criticizes ideas described in "On First Principles". Further in "To Avitus" (Letter 124), St. Jerome writes about "convincing proof" that Origen teaches reincarnation in the original version of the book:
The original text of "On First Principles" has almost completely disappeared. It remains extant as "De Principiis" in fragments faithfully translated into Latin by St. Jerome and in "the not very reliable Latin translation of Rufinus."
Islam.
Shia Islam.
The idea of reincarnation is accepted by a few Muslim sects, particularly of the Ghulat, and by other sects in the Muslim world. Historically, South Asian Isma'ilis performed chantas yearly, one of which is for seeking forgiveness of sins committed in past lives. Alawis belonging to Shia denomination of Islam hold that they were originally stars or divine lights that were cast out of heaven through disobedience and must undergo repeated reincarnation (or metempsychosis) before returning to heaven. They can be reincarnated as Christians or others through sin and as animals if they become infidels.
Reincarnation was also accepted by some streams of Sufism. Modern Sufis who embrace the idea include Bawa Muhaiyadeen. However Hazrat Inayat Khan has criticized the idea as unhelpful to the spiritual seeker.
Druze.
Reincarnation is a paramount tenet in the Druze faith. There is an eternal duality of the body and the soul and it is impossible for the soul to exist without the body. Therefore, reincarnations occur instantly at one's death. While in the Hindu and Buddhist belief system a soul can be transmitted to any living creature, in the Druze belief system this is not possible and a human soul will only transfer to a human body. Furthermore, a male Druze can only be reincarnated as another male Druze and a female Druze can only be reincarnated as another female Druze. Additionally, souls cannot be divided and the number of souls existing is finite.
Very few Druzes are able to recall their past but, if they are able to they are called a "Nateq". Typically souls who have died violent deaths in their previous incarnation will be able to recall memories. Since death is seen as a quick transient state, mourning is discouraged. Unlike other Abrahamic faiths, heaven and hell are spiritual. Heaven is the ultimate happiness received when soul escapes the cycle of rebirths and reunites with the Creator. While hell is conceptualized the bitterness of being unable to reunite with the Creator and escape from the cycle of rebirth.
New religious and spiritual movements.
Theosophy.
The Theosophical Society draws much of its inspiration from India. The idea is, according to a recent Theosophical writer, "the master-key to modern problems", including heredity. In the Theosophical world-view reincarnation is the vast rhythmic process by which the soul, the part of a person which belongs to the formless non-material and timeless worlds, unfolds its spiritual powers in the world and comes to know itself. It descends from sublime, free, spiritual realms and gathers experience through its effort to express itself in the world. Afterwards there is a withdrawal from the physical plane to successively higher levels of reality, in death, a purification and assimilation of the past life. Having cast off all instruments of personal experience it stands again in its spiritual and formless nature, ready to begin its next rhythmic manifestation, every lifetime bringing it closer to complete self-knowledge and self-expression. However it may attract old mental, emotional, and energetic "karma" patterns to form the new personality.
Modern Astrology.
Inspired by Helena Blavatsky's major works, including "Isis Unveiled" and "The Secret Doctrine", astrologers in the early twentieth-century integrated the concepts of karma and reincarnation into the practice of Western astrology. Notable astrologers who advanced this development included Alan Leo, Charles E. O. Carter, Marc Edmund Jones, and Dane Rudhyar. A new synthesis of East and West resulted as Hindu and Buddhist concepts of reincarnation were fused with Western astrology's deep roots in Hermeticism and Neoplatonism. In the case of Rudhyar, this synthesis was enhanced with the addition of Jungian depth psychology. This dynamic integration of astrology, reincarnation and depth psychology has continued into the modern era with the work of astrologers Steven Forrest and Jeffrey Wolf Green. Their respective schools of Evolutionary Astrology are based on "an acceptance of the fact that human beings incarnate in a succession of lifetimes."
Anthroposophy.
Anthroposophy describes reincarnation from the point of view of Western philosophy and culture. The ego is believed to transmute transient soul experiences into universals that form the basis for an individuality that can endure after death. These universals include ideas, which are intersubjective and thus transcend the purely personal (spiritual consciousness), intentionally formed human character (spiritual life), and becoming a fully conscious human being (spiritual humanity). Rudolf Steiner described both the general principles he believed to be operative in reincarnation, such as that one's will activity in one life forms the basis for the thinking of the next, and a number of successive lives of various individualities.
Eckankar.
Awareness of past lives, dreams, and soul travel are spiritual disciplines practiced by students of Eckankar. Eckankar teaches that each person is Soul, which transcends time and space. Soul travel is a term specific to Eckankar that refers to a shift in consciousness. Eckists believe the purpose of being aware of past lives is to help with understanding personal conditions in the present. Practicing students of Eckankar can become aware of past lives, through dreams, soul travel, and spiritual exercises called contemplations. This form of contemplation is the active, unconditional practice of going within to connect with the "Light and Sound of God" known as the divine life current or Holy Spirit.
Scientology.
Past reincarnation, usually termed "past lives", is a key part of the principles and practices of the Church of Scientology. Scientologists believe that the human individual is actually a "thetan", an immortal spiritual entity, that has fallen into a degraded state as a result of past-life experiences. Scientology auditing is intended to free the person of these past-life traumas and recover past-life memory, leading to a higher state of spiritual awareness. This idea is echoed in their highest fraternal religious order, the Sea Organization, whose motto is "Revenimus" or "We Come Back", and whose members sign a "billion-year contract" as a sign of commitment to that ideal. L. Ron Hubbard, the founder of Scientology, does not use the word "reincarnation" to describe its beliefs, noting that: "The common definition of reincarnation has been altered from its original meaning. The word has come to mean 'to be born again in different life forms' whereas its actual definition is 'to be born again into the flesh of another body.' Scientology ascribes to this latter, original definition of reincarnation."
The first writings in Scientology regarding past lives date from around 1951 and slightly earlier. In 1960, Hubbard published a book on past lives entitled "Have You Lived Before This Life". In 1968 he wrote "Mission into Time", a report on a five-week sailing expedition to Sardinia, Sicily and Carthage to see if specific evidence could be found to substantiate L. Ron Hubbard's recall of incidents in his own past, centuries ago.
Meher Baba.
The Indian spiritual teacher Meher Baba stated that reincarnation occurs due to desires and once those desires are extinguished the ego-mind ceases to reincarnate:
Spiritism.
Spiritism is a Christian philosophy codified in the 19th century by the French educator Allan Kardec. Spiritism soon spread to other countries, having today 35 countries represented in the International Spiritist Council. In countries like Brazil the movement had spread and became widely accepted, mostly due to Chico Xavier's works. Today the official spiritist community has about 20 million adepts, though due to local syncretism, it is accepted and somehow practiced by three times as many across the country. Some statistics even mention an adherence to Spiritist practices by 40 million people in Brazil.
Spiritism teaches reincarnation or rebirth into human life after death. This basically distinguishes Spiritism from Spiritualism. According to the Spiritist doctrine, reincarnation explains the moral and intellectual differences among men. It also provides the path to man's moral and intellectual perfection by amending for his mistakes and increasing his knowledge in successive lives. For this reason Spiritism does not accept rebirth in animals as this would be retrogressive. Reincarnation is the natural method of the perfection process through which the Spirit faces countless different situations, problems and obstacles, and needs to learn how to deal with them. The central tenet of Spiritist doctrine is the belief in spiritual life. The spirit is eternal, and evolves through a series of incarnations in the material world. The true life is the spiritual one; life in the material world is just a short-termed stage, where the spirit has the opportunity to learn and develop its potentials. Reincarnation is the process where the spirit, once free in the spiritual world, comes back to the world for further learning.
Wicca.
Wicca is a neo-pagan religion focused on nature, guided by the philosophy of Wiccan Rede that advocates Harm None, Do As Ye Will. The concept of karmic return in Wicca states that our deeds return to us threefold, or multiple times to teach us lessons (The Threefold Law), whether in this lifetime or the next. Reincarnation therefore is an accepted part of the Wiccan faith. Wiccans also believe in death and afterlife as important experiences for the soul to transform and prepare for future lifetimes.
In popular culture.
Reincarnation has been the theme of many films, including "Madhumati" (1958), which was one of the earliest Bollywood films of the theme. The 2010 Thai film, "Uncle Boonmee Who Can Recall His Past Lives", won the Palme d'Or at the 2010 Cannes Film Festival. The John Craigie song "So Many Lives" has been called a "reincarnation love song" and features a character that goes from being a caterpillar, to a bumble bee, to a sperm whale, to a chimpanzee. In the 1974 movie "Sonar Kella", directed by Satyajit Ray, the character of Mukul is believed to be a reincarnation, and this becomes the main theme of the plot. The animated series , as well as it's namesake live-action film version and sequal series heavily focus on reincarnation, as both title characters (Avatar Aang and Avatar Korra) are stated to be reincarnations of previous, deceased, Avatars (Korra being Aang's reincarnation), and both title characters have the ability to communicate with their 'past lives'.

</doc>
<doc id="25808" url="https://en.wikipedia.org/wiki?curid=25808" title="Robert Noyce">
Robert Noyce

Robert Norton Noyce (December 12, 1927 – June 3, 1990), nicknamed "the Mayor of Silicon Valley," co-founded Fairchild Semiconductor in 1957 and Intel Corporation in 1968. He is also credited (along with Jack Kilby) with the realization of the first integrated circuit or microchip that fueled the personal computer revolution and gave Silicon Valley its name.
Biography.
Active all his life, Noyce enjoyed reading Hemingway, flying his own airplane, hang gliding, and scuba diving. Noyce believed that microelectronics would continue to advance in complexity and sophistication well beyond its current state, leading to the question of what use society would make of the technology. In his last interview, Noyce was asked what he would do if he were "emperor" of the United States. He said that he would, among other things, "…make sure we are preparing our next generation to flourish in a high-tech age. And that means education of the lowest and the poorest, as well as at the graduate school level."
Early life.
Noyce was born on December 12, 1927, in Burlington, Iowa as the third of four sons of the Rev. Ralph Brewster Noyce. His father had graduated from Doane College (1915), Oberlin College (1920), and the Chicago Theological Seminary (1923). He was also nominated for a Rhodes Scholarship. The Reverend Noyce worked as a Congregational clergyman and as the associate superintendent of the Iowa Conference of Congregational Churches in the 1930s and 1940s.
His mother, Harriet May Norton, was the daughter of the Rev. Milton J. Norton, a Congregational clergyman, and of Louise Hill. She graduated from Oberlin College in 1921 and had dreamed of becoming a missionary prior to her marriage. She has been described as an intelligent woman with a commanding will.
Bob Noyce had three siblings: Donald Sterling Noyce, Gaylord Brewster Noyce and Ralph Harold Noyce. His earliest childhood memory involved beating his father at ping pong and feeling absolutely shocked when his mother reacted to the thrilling news of his victory with a distracted "Wasn't that nice of Daddy to let you win?" Even at the age of five, Noyce felt offended by the notion of intentionally losing at anything. "That's not the game", he sulked to his mother. "If you're going to play, play to win!"
In the summer of 1940, at the age of 12, he built a boy-sized aircraft with his brother, which they used to fly from the roof of the Grinnell College stables. Later he built a radio from scratch and motorized his sled by welding a propeller and an engine from an old washing machine to the back of it. His parents were both religious but Noyce became an agnostic and irreligious in later life.
Education.
He grew up in Grinnell, Iowa, and attended the local schools. He exhibited a talent for mathematics and science while in high school and took the Grinnell College freshman physics course in his senior year. He graduated from Grinnell High School in 1945 and entered Grinnell College in the fall of that year. He was the star diver on the 1947 Midwest Conference Championship swim team. While at Grinnell College, Noyce sang, played the oboe and acted. In Noyce’s junior year, he got in trouble for stealing a 25-pound pig from the mayor of Grinnell’s farm and roasting it at a school luau. The mayor sent a letter home to Noyce’s parents stating that “In the agricultural state of Iowa, stealing a domestic animal is a felony which carries a minimum penalty of a year in prison and a fine of one thousand dollars.” So essentially, Noyce would have to be expelled from Grinnell College. Grant Gale, Noyce’s physics professor and the President of Grinnell College, did not want to lose a student like Robert who had so much potential. They were able to compromise with the mayor so that the college would compensate him for the pig, Noyce would only be suspended for one semester, and no further charges would be pressed. He returned to Grinnell in February 1949. He graduated Phi Beta Kappa with a BA in physics and mathematics from Grinnell College in 1949. He also received a signal honor from his classmates: the Brown Derby Prize, which recognized "the senior man who earned the best grades with the least amount of work".
While an undergraduate, Noyce attended a physics course of the professor Grant Gale and was fascinated by the physics. Gale got hold of two of the very first transistors ever to come out of Bell Labs and showed them off to his class and Noyce was hooked. Grant Gale suggested that he apply to the doctoral program in physics at MIT, which he did.
Noyce had a mind so quick that his graduate school friends called him "Rapid Robert." He received his doctorate in physics from Massachusetts Institute of Technology in 1953.
Career.
After graduating from the Massachusetts Institute of Technology in 1953, he took his first job as a research engineer at the Philco Corporation in Philadelphia. He left in 1956 for the Shockley Semiconductor Laboratory in Mountain View, California.
He joined William Shockley, a co-inventor of the transistor and eventual Nobel Prize winner, at the Shockley Semiconductor Laboratory, a division of Beckman Instruments.
Noyce left with the "traitorous eight" in 1957, upon having issues with respect to the quality of its management, and co-founded the influential Fairchild Semiconductor corporation. According to Sherman Fairchild, Noyce's impassioned presentation of his vision was the reason Fairchild had agreed to create the semiconductor division for the traitorous eight.
Noyce and Gordon Moore founded Intel in 1968 when they left Fairchild Semiconductor. Arthur Rock, the chairman of Intel's board and a major investor in the company said that for Intel to succeed, Intel needed Noyce, Moore and Andrew Grove. And it needed them in that order. Noyce: the visionary, born to inspire; Moore: the virtuoso of technology; and Grove: the technologist turned management scientist. The relaxed culture that Noyce brought to Intel was a carry-over from his style at Fairchild Semiconductor. He treated employees as family, rewarding and encouraging teamwork. His follow-your-bliss management style set the tone for many Valley success stories. Noyce's management style could be called a "roll up your sleeves" style. He shunned fancy corporate cars, reserved parking spaces, private jets, offices, and furnishings in favor of a less-structured, relaxed working environment in which everyone contributed and no one received lavish benefits. By declining the usual executive perks he stood as a model for future generations of Intel CEOs.
At Intel, he oversaw Ted Hoff's invention of the microprocessor, which was his second revolution.
Personal life.
In 1953, Noyce married Elizabeth Bottomley. She was a 1951 graduate of Tufts University. During this time, the couple lived in Los Altos, California. They had four children: William B., Pendred, Priscilla, and Margaret. Elizabeth loved New England, so the family acquired a 50-acre coastal summer home in Bremen, Maine. Elizabeth and the children would summer there. Robert would visit during the summer, but he continued working at Intel during the summer. The couple divorced in 1974.
On November 27, 1974, Noyce married Ann Schmeltz Bowers. Bowers, a 1959 graduate of Cornell University, also received an honorary Ph.D. from Santa Clara University, where she was a trustee for nearly 20 years. She was the first Director of Personnel for Intel Corporation and the first Vice President of Human Resources for Apple Inc. She currently serves as Chair of the Board and the founding trustee of the Noyce Foundation.
Death.
Noyce suffered a heart attack at age 62 at home on June 3, 1990, and later died at the Seton Medical Center in Austin, Texas.
Awards and honors.
In July 1959, he filed for "Semiconductor Device and Lead Structure", a type of integrated circuit. This independent effort was recorded only a few months after the key findings of inventor Jack Kilby. For his co-invention of the integrated circuit and its world-transforming impact, three presidents of the United States honored him.
Noyce was a holder of many honors and awards. President Ronald Reagan awarded him the National Medal of Technology in 1987. Two years later, he was inducted into the U.S. Business Hall of Fame sponsored by Junior Achievement, during a black tie ceremony keynoted by President George H. W. Bush. In 1990 Noycealong with, among others, Jack Kilby and transistor inventor John Bardeenreceived a "Lifetime Achievement Medal" during the bicentennial celebration of the Patent Act.
Noyce received the Franklin Institute's Stuart Ballantine Medal in 1966. He was awarded the IEEE Medal of Honor in 1978 "for his contributions to the silicon integrated circuit, a cornerstone of modern electronics." In 1979, he was awarded the National Medal of Science. Noyce was elected a Fellow of the American Academy of Arts and Sciences in 1980. The National Academy of Engineering awarded him its 1989 Charles Stark Draper Prize.
The science building at his alma mater, Grinnell College, is named after him.
On December 12, 2011, Noyce was honored with a Google Doodle celebrating the 84th anniversary of his birth.
December 8, 2000 According to the book 'The Innovators' Noyce was mentioned/credited as the honorary co-recipient in the Nobel Prize acceptance speech given by Kilby http://www.nobelprize.org/nobel_prizes/physics/laureates/2000/kilby-lecture.html
Legacy.
The Noyce Foundation was founded in 1991 by his family. The foundation is dedicated to improving public education in mathematics and science in grades K-12. The foundation announced that it would end operations in 2015.
Patents.
Noyce was granted 15 patents.

</doc>
<doc id="25809" url="https://en.wikipedia.org/wiki?curid=25809" title="Riemann zeta function">
Riemann zeta function

The Riemann zeta function or Euler–Riemann zeta function, "ζ"("s"), is a function of a complex variable "s" that analytically continues the sum of the infinite series 
which converges when the real part of "s" is greater than 1. More general representations of "ζ"("s") for all "s" are given below. The Riemann zeta function plays a pivotal role in analytic number theory and has applications in physics, probability theory, and applied statistics.
As a function of a real variable, Leonhard Euler first introduced and studied it in the first half of the eighteenth century without using complex analysis, which was not available at the time. Bernhard Riemann's 1859 article "On the Number of Primes Less Than a Given Magnitude" extended the Euler definition to a complex variable, proved its meromorphic continuation and functional equation, and established a relation between its zeros and the distribution of prime numbers.
The values of the Riemann zeta function at even positive integers were computed by Euler. The first of them, "ζ"(2), provides a solution to the Basel problem. In 1979 Apéry proved the irrationality of "ζ"(3). The values at negative integer points, also found by Euler, are rational numbers and play an important role in the theory of modular forms. Many generalizations of the Riemann zeta function, such as Dirichlet series, Dirichlet L-functions and L-functions, are known.
Definition.
The Riemann zeta function "ζ"("s") is a function of a complex variable "s" = "σ" + "it". (The notation "s", "σ", and "t" is used traditionally in the study of the "ζ"-function, following Riemann.)
The following infinite series converges for all complex numbers "s" with real part greater than 1, and defines "ζ"("s") in this case:
It can also be defined by the integral
where Γ("s") is the gamma function.
The Riemann zeta function is defined as the analytic continuation of the function defined for σ > 1 by the sum of the preceding series.
Leonhard Euler considered the above series in 1740 for positive integer values of "s", and later Chebyshev extended the definition to real "s" > 1.
The above series is a prototypical Dirichlet series that converges absolutely to an analytic function for "s" such that and diverges for all other values of "s". Riemann showed that the function defined by the series on the half-plane of convergence can be continued analytically to all complex values . For "s" = 1 the series is the harmonic series which diverges to +∞, and
Thus the Riemann zeta function is a meromorphic function on the whole complex "s"-plane, which is holomorphic everywhere except for a simple pole at "s" = 1 with residue 1.
Specific values.
For any positive even integer "2n":
where "B"2"n" is a Bernoulli number.
For negative integers, one has
for , so in particular "ζ" vanishes at the negative even integers because "B""m" = 0 for all odd "m" other than 1. For odd positive integers, no such simple expression is known, although these values are thought to be related to the algebraic K-theory of the integers; see Special values of L-functions.
Via analytic continuation, one can show that 
Euler product formula.
The connection between the zeta function and prime numbers was discovered by Euler, who proved the identity
where, by definition, the left hand side is "ζ"("s") and the infinite product on the right hand side extends over all prime numbers "p" (such expressions are called Euler products):
Both sides of the Euler product formula converge for Re("s") > 1. The proof of Euler's identity uses only the formula for the geometric series and the fundamental theorem of arithmetic. Since the harmonic series, obtained when "s" = 1, diverges, Euler's formula (which becomes formula_20) implies that there are infinitely many primes.
The Euler product formula can be used to calculate the asymptotic probability that "s" randomly selected integers are set-wise coprime. Intuitively, the probability that any single number is divisible by a prime (or any integer), "p" is 1/"p". Hence the probability that "s" numbers are all divisible by this prime is 1/"p""s", and the probability that at least one of them is "not" is . Now, for distinct primes, these divisibility events are mutually independent because the candidate divisors are coprime (a number is divisible by coprime divisors "n" and "m" if and only if it is divisible by "nm", an event which occurs with probability 1/("nm")). Thus the asymptotic probability that "s" numbers are coprime is given by a product over all primes,
The functional equation.
The Riemann zeta function satisfies the functional equation (known as the Riemann functional equation or Riemann's functional equation)
where Γ("s") is the gamma function, which is an equality of meromorphic functions valid on the whole complex plane. This equation relates values of the Riemann zeta function at the points "s" and . Owing to the zeros of the sine function, the functional equation implies that "ζ"("s") has a simple zero at each even negative integer "s" = −2"n" — these are known as the trivial zeros of "ζ"("s"). When "s" is an even positive integer, the product <br> sin("πs"/2)Γ(1−"s") on the right is regular and non-zero because Γ(1−"s") has a simple pole: the functional equation thus relates the values of the Riemann zeta function at odd negative integers and even positive integers.
The functional equation was established by Riemann in his 1859 paper "On the Number of Primes Less Than a Given Magnitude" and used to construct the analytic continuation in the first place. An equivalent relationship had been conjectured by Euler over a hundred years earlier, in 1749, for the Dirichlet eta function (alternating zeta function)
Incidentally, this relation is interesting also because it actually exhibits "ζ"("s") as a Dirichlet series (of the "η"-function) which is convergent (albeit non-absolutely) in the larger half-plane "σ" > 0 (not just "σ" > 1), up to an elementary factor.
Riemann also found a symmetric version of the functional equation (which he assigned the letter "ξ" xi), given by first defining
The functional equation is then given by
Zeros, the critical line, and the Riemann hypothesis.
The functional equation shows that the Riemann zeta function has zeros at .. . These are called the trivial zeros. They are trivial in the sense that their existence is relatively easy to prove, for example, from sin(π"s"/2) being 0 in the functional equation. The non-trivial zeros have captured far more attention because their distribution not only is far less understood but, more importantly, their study yields impressive results concerning prime numbers and related objects in number theory. It is known that any non-trivial zero lies in the open strip {"s" ∈ C : 0 < Re("s") < 1}, which is called the critical strip. The Riemann hypothesis, considered one of the greatest unsolved problems in mathematics, asserts that any non-trivial zero "s" has Re("s") = 1/2. In the theory of the Riemann zeta function, the set {"s" ∈ C : Re("s") = 1/2} is called the critical line. For the Riemann zeta function on the critical line, see Z-function.
The Hardy–Littlewood conjectures.
In 1914, Godfrey Harold Hardy proved that formula_26 has infinitely many real zeros.
Hardy and John Edensor Littlewood formulated two conjectures on the density and distance between the zeros of formula_26 on intervals of large positive real numbers. In the following, formula_28 is the total number of real zeros and formula_29 the total number of zeros of odd order of the function formula_26 lying in the interval formula_31.
These two conjectures opened up new directions in the investigation of the Riemann zeta function.
Other results.
The location of the Riemann zeta function's zeros is of great importance in the theory of numbers. The prime number theorem is equivalent to the fact that there are no zeros of the zeta function on the Re("s") = 1 line. A better result that follows from an effective form of Vinogradov's mean-value theorem is that ≠ 0 whenever | "t" | ≥ 3 and
The strongest result of this kind one can hope for is the truth of the Riemann hypothesis, which would have many profound consequences in the theory of numbers.
It is known that there are infinitely many zeros on the critical line. Littlewood showed that if the sequence (γ"n") contains the imaginary parts of all zeros in the upper half-plane in ascending order, then
The critical line theorem asserts that a positive percentage of the nontrivial zeros lies on the critical line.
In the critical strip, the zero with smallest non-negative imaginary part is (). The fact that formula_45 for all complex implies that the zeros of the Riemann zeta function are symmetric about the real axis. Combining this symmetry with the functional equation, furthermore, one sees that the non-trivial zeros are symmetric about the critical line Re("s") = 1/2.
Various properties.
For sums involving the zeta-function at integer and half-integer values, see rational zeta series.
Reciprocal.
The reciprocal of the zeta function may be expressed as a Dirichlet series over the Möbius function μ("n"):
for every complex number "s" with real part > 1. There are a number of similar relations involving various well-known multiplicative functions; these are given in the article on the Dirichlet series.
The Riemann hypothesis is equivalent to the claim that this expression is valid when the real part of "s" is greater than 1/2.
Universality.
The critical strip of the Riemann zeta function has the remarkable property of universality. This zeta-function universality states that there exists some location on the critical strip that approximates any holomorphic function arbitrarily well. Since holomorphic functions are very general, this property is quite remarkable. The very first proof of universality was provided by Sergei Mikhailovitch Voronin in 1975, which results non-applicable to solve Riemann's Hypothesis. A latter and more accurate proof was given in 2003 by Garunkštis. In 2015, Schwarzenberg gave an even more accurate and general proof of this theorem. Nevertheless, none of the proofs above are definitive or complete, since a definitive proof would imply that Riemann's Hypothesis is true.
Estimates of the maximum of the modulus of the zeta function.
Let the functions formula_47 and formula_48 be defined by the equalities
Here formula_50 is a sufficiently large positive number, formula_51, formula_52, formula_53, formula_54. Estimating the values formula_55 and formula_56 from below shows, how large (in modulus) values formula_57 can take on short intervals of the critical line or in small neighborhoods of points lying in the critical strip formula_58.
The case formula_59 was studied by Ramachandra; the case formula_60, where formula_61 is a sufficiently large constant, is trivial.
Karatsuba proved, in particular, that if the values formula_62 and formula_63 exceed certain sufficiently small constants, then the estimates
hold, where formula_65 are certain absolute constants.
The argument of the Riemann zeta-function.
The function formula_66 is called the argument of the Riemann zeta function.
Here formula_67 is the increment of an arbitrary continuous branch of formula_68 along the broken line joining the points formula_69 and formula_70
There are some theorems on properties of the function formula_71. Among those results are the mean value theorems for formula_71 and its first integral formula_73 on intervals of the real line, and also the theorem claiming that every interval formula_36 for formula_75 contains at least
points where the function formula_71 changes sign. Earlier similar results were obtained by Atle Selberg for the case
formula_78.
Representations.
Dirichlet series.
An extension of the area of convergence can be obtained by rearranging the original series (Konrad Knopp, Theory of Functions,
1945, p. 51-55). The series
converges for formula_80,
while 
converges even for formula_82. In this way, the area of convergence can be extended to formula_83 for any formula_84.
Mellin transform.
The Mellin transform of a function "ƒ"("x") is defined as
in the region where the integral is defined. There are various expressions for the zeta-function as a Mellin transform. If the real part of "s" is greater than one, we have
where Γ denotes the Gamma function. By modifying the contour, Riemann showed that
for all "s", where the contour C starts and ends at +∞ and circles the origin once.
We can also find expressions which relate to prime numbers and the prime number theorem. If π("x") is the prime-counting function, then
for values with .
A similar Mellin transform involves the Riemann prime-counting function "J"("x"), which counts prime powers "p""n" with a weight of 1/"n", so that
Now we have
These expressions can be used to prove the prime number theorem by means of the inverse Mellin transform. Riemann's prime-counting function is easier to work with, and π("x") can be recovered from it by Möbius inversion.
Theta functions.
The Riemann zeta function can be given formally by a divergent Mellin transform
in terms of Jacobi's theta function
However this integral does not converge for any value of "s" and so needs to be regularized: this gives the following expression for the zeta function:
Laurent series.
The Riemann zeta function is meromorphic with a single pole of order one at
"s" = 1. It can therefore be expanded as a Laurent series about "s" = 1;
the series development then is
The constants γ"n" here are called the Stieltjes constants and can be defined
by the limit
The constant term γ0 is the Euler–Mascheroni constant.
Integral.
For all formula_96 the integral relation (cf. Abel–Plana formula)
holds true, which may be used for a numerical evaluation of the zeta-function.
Rising factorial.
Another series development using the rising factorial valid for the entire complex plane is
This can be used recursively to extend the Dirichlet series definition to all complex numbers.
The Riemann zeta function also appears in a form similar to the Mellin transform in an integral over the Gauss–Kuzmin–Wirsing operator acting on "x""s"−1; that context gives rise to a series expansion in terms of the falling factorial.
Hadamard product.
On the basis of Weierstrass's factorization theorem, Hadamard gave the infinite product expansion
where the product is over the non-trivial zeros "ρ" of "ζ" and the letter "γ" again denotes the Euler–Mascheroni constant. A simpler infinite product expansion is
This form clearly displays the simple pole at "s" = 1, the trivial zeros at −2, −4, ... due to the gamma function term in the denominator, and the non-trivial zeros at "s" = "ρ". (To ensure convergence in the latter formula, the product should be taken over "matching pairs" of zeroes, i.e. the factors for a pair of zeroes of the form "ρ" and 1 − "ρ" should be combined.)
Logarithmic derivative on the critical strip.
where formula_102 is the density of zeros of "ζ" on the critical strip 0 < Re("s") < 1 ("δ" is the Dirac delta distribution, and the sum is over the nontrivial zeros "ρ" of "ζ").
Globally convergent series.
A globally convergent series for the zeta function, valid for all complex numbers "s" except formula_103 for some integer "n", was conjectured by Konrad Knopp and proven by Helmut Hasse in 1930 (cf. Euler summation):
The series only appeared in an appendix to Hasse's paper, and did not become generally known until it was rediscovered more than 60 years later (see Sondow, 1994).
Hasse also proved the globally converging series
in the same publication. However, recent research indicates that the latter series was discovered earlier, by Joseph Ser in 1926.
Peter Borwein has shown a very rapidly convergent series suitable for high precision numerical calculations. The algorithm, making use of Chebyshev polynomials, is described in the article on the Dirichlet eta function.
Series representation at positive integers via the primorial.
Here "pn#" is the primorial sequence and "Jk" is Jordan's totient function.
Applications.
The zeta function occurs in applied statistics (see Zipf's law and Zipf–Mandelbrot law).
Zeta function regularization is used as one possible means of regularization of divergent series and divergent integrals in quantum field theory. In one notable example, the Riemann
zeta-function shows up explicitly in the calculation of the Casimir effect. The zeta function is also useful for the analysis of dynamical systems.
Infinite series.
The zeta function evaluated at positive integers appears in infinite series representations of a number of constants. There are more formulas in the article Harmonic number.
Some zeta series evaluate to more complicated expressions
Generalizations.
There are a number of related zeta functions that can be considered to be generalizations of the Riemann zeta function. These include the Hurwitz zeta function
(the convergent series representation was given by Helmut Hasse in 1930, cf. Hurwitz zeta function), which coincides with the Riemann zeta function when "q" = 1 (note that the lower limit of summation in the Hurwitz zeta function is 0, not 1), the Dirichlet L-functions and the Dedekind zeta-function. For other related functions see the articles Zeta function and L-function.
The polylogarithm is given by
which coincides with the Riemann zeta function when "z" = 1.
The Lerch transcendent is given by
which coincides with the Riemann zeta function when "z" = 1 and "q" = 1 (note that the lower limit of summation in the Lerch transcendent is 0, not 1).
The Clausen function Cl"s"("θ") that can be chosen as the real or imaginary part of Li"s"("e" "iθ").
The multiple zeta functions are defined by
One can analytically continue these functions to the "n"-dimensional complex space. The special values of these functions are called multiple zeta values by number theorists and have been connected to many different branches in mathematics and physics.

</doc>
