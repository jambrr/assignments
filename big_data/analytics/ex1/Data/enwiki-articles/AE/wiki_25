<doc id="25974" url="https://en.wikipedia.org/wiki?curid=25974" title="Radiosity (computer graphics)">
Radiosity (computer graphics)

In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code "LD*E") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
Radiosity methods were first developed in about 1950 in the engineering field of heat transfer. They were later refined specifically for the problem of rendering computer graphics in 1984 by researchers at Cornell University and Hiroshima University.
Notable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and ); 3ds Max; form•Z; LightWave 3D and the Electric Image Animation System.
Visual characteristics.
The inclusion of radiosity calculations in the rendering process often lends an added element of realism to the finished scene, because of the way it mimics real-world phenomena. Consider a simple room scene.
The image on the left was rendered with a typical direct illumination renderer. There are "three types" of lighting in this scene which have been specifically chosen and placed by the artist in an attempt to create realistic lighting: spot lighting with shadows (placed outside the window to create the light shining on the floor), ambient lighting (without which any part of the room not lit directly by a light source would be totally dark), and omnidirectional lighting without shadows (to reduce the flatness of the ambient lighting).
The image on the right was rendered using a radiosity algorithm. There is only one source of light: an image of the sky placed outside the window. The difference is marked. The room glows with light. Soft shadows are visible on the floor, and subtle lighting effects are noticeable around the room. Furthermore, the red color from the carpet has bled onto the grey walls, giving them a slightly warm appearance. None of these effects were specifically chosen or designed by the artist.
Overview of the radiosity algorithm.
The surfaces of the scene to be rendered are each divided up into one or more smaller surfaces (patches).
A view factor (also known as "form factor") is computed for each pair of patches; it is a coefficient describing how well the patches can see each other. Patches that are far away from each other, or oriented at oblique angles relative to one another, will have smaller view factors. If other patches are in the way, the view factor will be reduced or zero, depending
on whether the occlusion is partial or total.
The view factors are used as coefficients in a linear system of rendering equations. Solving this system yields the radiosity, or brightness, of each patch, taking into account diffuse interreflections and soft shadows.
Progressive radiosity solves the system iteratively with intermediate radiosity values for the patch, corresponding to bounce levels. That is, after each iteration, we know how the scene looks after one light bounce, after two passes, two bounces, and so forth. This is useful for getting an interactive preview of the scene. Also, the user can stop the iterations once the image looks good enough, rather than wait for the computation to numerically converge.
Another common method for solving the radiosity equation is "shooting radiosity," which iteratively solves the radiosity equation by "shooting" light from the patch with the most error at each step. After the first pass, only those patches which are in direct line of sight of a light-emitting patch will be illuminated. After the second pass, more patches will become illuminated as the light begins to bounce around the scene. The scene continues to grow brighter and eventually reaches a steady state.
Mathematical formulation.
The basic radiosity method has its basis in the theory of thermal radiation, since radiosity relies on computing the amount of light energy transferred among surfaces. In order to simplify computations, the method assumes that all scattering is perfectly diffuse. Surfaces are typically discretized into quadrilateral or triangular elements over which a piecewise polynomial function is defined.
After this breakdown, the amount of light energy transfer can be computed by using the known reflectivity of the reflecting patch, combined with the view factor of the two patches. This dimensionless quantity is computed from the geometric orientation of two patches, and can be thought of as the fraction of the total possible emitting area of the first patch which is covered by the second.
More correctly, radiosity "B" is the energy per unit area leaving the patch surface per discrete time interval and is the combination of emitted and reflected energy:
where:
If the surfaces are approximated by a finite number of planar patches, each of which is taken to have a constant radiosity "Bi" and reflectivity "ρi", the above equation gives the discrete radiosity equation,
where "Fij" is the geometrical view factor for the radiation leaving "j" and hitting patch "i".
This equation can then be applied to each patch. The equation is monochromatic, so color radiosity rendering requires calculation for each of the required colors.
Solution methods.
The equation can formally be solved as matrix equation, to give the vector solution:
This gives the full "infinite bounce" solution for B directly. However the number of calculations to compute the matrix solution scales according to "n"3, where "n" is the number of patches. This becomes prohibitive for realistically large values of "n".
Instead, the equation can more readily be solved iteratively, by repeatedly applying the single-bounce update formula above. Formally, this is a solution of the matrix equation by Jacobi iteration. Because the reflectivities ρi are less than 1, this scheme converges quickly, typically requiring only a handful of iterations to produce a reasonable solution. Other standard iterative methods for matrix equation solutions can also be used, for example the Gauss–Seidel method, where updated values for each patch are used in the calculation as soon as they are computed, rather than all being updated synchronously at the end of each sweep. The solution can also be tweaked to iterate over each of the sending elements in turn in its main outermost loop for each update, rather than each of the receiving patches. This is known as the "shooting" variant of the algorithm, as opposed to the "gathering" variant. Using the view factor reciprocity, "A"i "F"ij = "A"j "F"ji, the update equation can also be re-written in terms of the view factor "F"ji seen by each "sending" patch "A"j:
This is sometimes known as the "power" formulation, since it is now the total transmitted power of each element that is being updated, rather than its radiosity.
The view factor "F"ij itself can be calculated in a number of ways. Early methods used a "hemicube" (an imaginary cube centered upon the first surface to which the second surface was projected, devised by Cohen and Greenberg in 1985). The surface of the hemicube was divided into pixel-like squares, for each of which a view factor can be readily calculated analytically. The full form factor could then be approximated by adding up the contribution from each of the pixel-like squares. The projection onto the hemicube, which could be adapted from standard methods for determining the visibility of polygons, also solved the problem of intervening patches partially obscuring those behind.
However all this was quite computationally expensive, because ideally form factors must be derived for every possible pair of patches, leading to a quadratic increase in computation as the number of patches increased. This can be reduced somewhat by using a binary space partitioning tree to reduce the amount of time spent determining which patches are completely hidden from others in complex scenes; but even so, the time spent to determine the form factor still typically scales as "n" log "n". New methods include adaptive integration
Sampling approaches.
The form factors "F"ij themselves are not in fact explicitly needed in either of the update equations; neither to estimate the total intensity ∑j "F"ij "B"j gathered from the whole view, nor to estimate how the power "A"j "B"j being radiated is distributed. Instead, these updates can be estimated by sampling methods, without ever having to calculate form factors explicitly. Since the mid 1990s such sampling approaches have been the methods most predominantly used for practical radiosity calculations.
The gathered intensity can be estimated by generating a set of samples in the unit circle, lifting these onto the hemisphere, and then seeing what was the radiosity of the element that a ray incoming in that direction would have originated on. The estimate for the total gathered intensity is then just the average of the radiosities discovered by each ray. Similarly, in the power formulation, power can be distributed by generating a set of rays from the radiating element in the same way, and spreading the power to be distributed equally between each element a ray hits.
This is essentially the same distribution that a path-tracing program would sample in tracing back one diffuse reflection step; or that a bidirectional ray tracing program would sample to achieve one forward diffuse reflection step when light source mapping forwards. The sampling approach therefore to some extent represents a convergence between the two techniques, the key difference remaining that the radiosity technique aims to build up a sufficiently accurate map of the radiance of all the surfaces in the scene, rather than just a representation of the current view.
Reducing computation time.
Although in its basic form radiosity is assumed to have a quadratic increase in computation time with added geometry (surfaces and patches), this need not be the case. The radiosity problem can be rephrased as a problem of rendering a texture mapped scene. In this case, the computation time increases only linearly with the number of patches (ignoring complex issues like cache use).
Following the commercial enthusiasm for radiosity-enhanced imagery, but prior to the standardization of rapid radiosity calculation, many architects and graphic artists used a technique referred to loosely as false radiosity. By darkening areas of texture maps corresponding to corners, joints and recesses, and applying them via self-illumination or diffuse mapping, a radiosity-like effect of patch interaction could be created with a standard scanline renderer (cf. ambient occlusion).
Static, pre-computed radiosity may be displayed in realtime via Lightmaps on current desktop computers with standard graphics acceleration hardware.
Advantages.
One of the advantages of the Radiosity algorithm is that it is relatively simple to explain and implement. This makes it a useful algorithm for teaching students about global illumination algorithms. A typical direct illumination renderer already contains nearly all of the algorithms (perspective transformations, texture mapping, hidden surface removal) required to implement radiosity. A strong grasp of mathematics is not required to understand or implement this algorithm.
Limitations.
Typical radiosity methods only account for light paths of the form LD*E, i.e. paths which start at a light source and make multiple diffuse bounces before reaching the eye. Although there are several approaches to integrating other illumination effects such as specularand glossy [http://www.cs.huji.ac.il/labs/cglab/papers/clustering/ reflections, radiosity-based methods are generally not used to solve the complete rendering equation.
Basic radiosity also has trouble resolving sudden changes in visibility (e.g. hard-edged shadows) because coarse, regular discretization into piecewise constant elements corresponds to a low-pass box filter of the spatial domain. Discontinuity meshing [http://www.cs.cmu.edu/~ph/discon.ps.gz] uses knowledge of visibility events to generate a more intelligent discretization.
Confusion about terminology.
Radiosity was perhaps the first rendering algorithm in widespread use which accounted for diffuse indirect lighting. Earlier rendering algorithms, such as Whitted-style ray tracing were capable of computing effects such as reflections, refractions, and shadows, but despite being highly global phenomena, these effects were not commonly referred to as "global illumination." As a consequence, the term "global illumination" became confused with "diffuse interreflection," and "Radiosity" became confused with "global illumination" in popular parlance. However, the three are distinct concepts.
The radiosity method in the current computer graphics context derives from (and is fundamentally the same as) the radiosity method in heat transfer. In this context radiosity is the total radiative flux (both reflected and re-radiated) leaving a surface, also sometimes known as radiant exitance. Calculation of Radiosity rather than surface temperatures is a key aspect of the radiosity method that permits linear matrix methods to be applied to the problem.

</doc>
<doc id="25975" url="https://en.wikipedia.org/wiki?curid=25975" title="Reign of Terror">
Reign of Terror

The Reign of Terror (6 September 1793 – 28 July 1794), also known as The Terror (), was a period of violence that occurred after the onset of the French Revolution, incited by conflict between two rival political factions, the Girondins and The Mountain, and marked by mass executions of "enemies of the revolution". The death toll ranged in the tens of thousands, with 16,594 executed by guillotine (2,639 in Paris), and another 25,000 in summary executions across France.
The guillotine (called the "National Razor") became the symbol of the revolutionary cause, strengthened by a string of executions: King Louis XVI, Marie Antoinette, the Girondins, Philippe Égalité (Louis Philippe II, Duke of Orléans), and Madame Roland, and others such as pioneering chemist Antoine Lavoisier, lost their lives under its blade. During 1794, revolutionary France was beset with conspiracies by internal and foreign enemies. Within France, the revolution was opposed by the French nobility, which had lost its inherited privileges. The Roman Catholic Church opposed the revolution, which had turned the clergy into employees of the state and required they take an oath of loyalty to the nation (through the Civil Constitution of the Clergy). In addition, the French First Republic was engaged in a series of wars with neighboring powers, and parts of France were engaging in civil war against the republican regime.
The extension of civil war and the advance of foreign armies on national territory produced a political crisis and increased the already present rivalry between the Girondins and the more radical Jacobins. The latter were eventually grouped in the parliamentary faction called the Mountain, and they had the support of the Parisian population. The French government established the Committee of Public Safety, which took its final form on 6 September 1793, in order to suppress internal counter-revolutionary activities and raise additional French military forces.
Through the Revolutionary Tribunal, the Terror's leaders exercised broad powers and used them to eliminate the internal and external enemies of the republic. The repression accelerated in June and July 1794, a period called "la Grande Terreur" (the Great Terror), and ended in the coup of 9 Thermidor Year II (27 July 1794), leading to the Thermidorian Reaction, in which several instigators of the Reign of Terror were executed, including Saint-Just and Robespierre.
Origins and causes.
After the resolution of foreign wars during 1791–93, the violence associated with the Reign of Terror increased significantly: only roughly 4% of executions had occurred before November 1793 (Brumaire, Year I), thus signalling to many that the Reign of Terror might have had additional causes. These could have included inherent issues with revolutionary ideology, and/or the need of a weapon for political repression in a time of significant foreign and civil upheaval, leading to many different interpretations by historians.
Many historians have debated the reasons the French Revolution took such a radical turn during the Reign of Terror of 1793–94. The public was frustrated that the social equality and anti-poverty measures that the revolution originally promised were not materializing. Jacques Roux's "Manifesto of the Enraged" on 25 June 1793, describes the extent to which, four years into the revolution, these goals were largely unattained by the common people. The foundation of the Terror is centered on the April 1793 creation of the Committee of Public Safety and its militant Jacobin delegates. The National Convention believed that the committee needed to rule with "near dictatorial power" and the committee was delegated new and expansive political powers to respond quickly to popular demands.
Those in power believed the Committee of Public Safety was an unfortunate, but necessary and temporary, reaction to the pressures of foreign and civil war. Historian Albert Mathiez argues that the authority of the Committee of Public Safety was based on the necessities of war, as those in power realized that deviating from the will of the people was a temporary emergency response measure in order to secure the ideals of the republic. According to Mathiez, they "touched only with trepidation and reluctance the regime established by the Constituent Assembly" so as not to interfere with the early accomplishments of the revolution.
Similar to Mathiez, Richard Cobb introduced competing circumstances of revolt and re-education within France as an explanation for the Terror. Counter-revolutionary rebellions taking place in Lyon, Brittany, Vendée, Nantes, and Marseille were threatening the revolution with royalist ideas. Cobb writes, "the revolutionaries themselves, living as if in combat… were easily persuaded that only terror and repressive force saved them from the blows of their enemies."
Terror was used in these rebellions both to execute inciters and to provide a very visible example to those who might be considering rebellion. Cobb agrees with Mathiez that the Terror was simply a response to circumstances, a necessary evil and natural defence, rather than a manifestation of violent temperaments or excessive fervour. At the same time, Cobb rejects Mathiez's Marxist interpretation that elites controlled the Reign of Terror to the significant benefit of the bourgeoisie. Instead, Cobb argues that social struggles between the classes were seldom the reason for revolutionary actions and sentiments.
Francois Furet, however, argues that circumstances could not have been the sole cause of the Reign of Terror because "the risks for the revolution were greatest" in the middle of 1793 but at that time "the activity of the Revolutionary Tribunal was relatively minimal." Widespread terror and a consequent rise in executions came after external and internal threats were vastly reduced. Therefore, Furet suggests that ideology played the crucial role in the rise of the Reign of Terror because "man's regeneration" became a central theme for the Committee of Public Safety as they were trying to instill ideals of free will and enlightened government in the public. As this ideology became more and more pervasive, violence became a significant method for dealing with counter-revolutionaries and the opposition because, for fear of being labelled a counter-revolutionary themselves, "the moderate men would have to accept, endorse and even glorify the acts of the more violent."
The Terror.
On 2 June 1793, Paris sections – encouraged by the "enragés" Jacques Roux and Jacques Hébert – took over the convention, calling for administrative and political purges, a low fixed price for bread, and a limitation of the electoral franchise to "sans-culottes" alone. With the backing of the national guard, they persuaded the convention to arrest 29 Girondist leaders, including Jacques Pierre Brissot. On 13 July the assassination of Jean-Paul Marat – a Jacobin leader and journalist known for his violent rhetoric – by Charlotte Corday resulted in a further increase in Jacobin political influence.
Georges Danton, the leader of the August 1792 uprising against the king, was removed from the committee. On 27 July Maximilien Robespierre, known in Republican circles as "the Incorruptible" for his ascetic dedication to his ideals, made his entrance, quickly becoming the most influential member of the committee as it moved to take radical measures against the revolution's domestic and foreign enemies.
The Jacobins identified themselves with the popular movement and the sans-culottes, who in turn saw popular violence as a political right. The most notorious instance of the crowd's rough justice was the prison massacres of September 1792, when around 2,000 people, including priests and nuns, were dragged from their prison cells and subjected to summary 'justice'. The Convention was determined to avoid a repeat of these brutal scenes, but that meant taking violence into their own hands as an instrument of government.
In June 1793, the sans-culottes, exasperated by the inadequacies of the government, invaded the Convention and overthrew the Girondins. In their place they endorsed the political ascendancy of the Jacobins. Thus Robespierre came to power on the back of popular street violence.
Meanwhile, on 24 June, the convention adopted the first republican constitution of France, the French Constitution of 1793. It was ratified by public referendum, but never put into force; like other laws, it was indefinitely suspended by the decree of October that the government of France would be "revolutionary until the peace".
On 25 December 1793, Robespierre stated:
The goal of the constitutional government is to conserve the republic; the aim of the revolutionary government is to found it... The revolutionary government owes to the good citizen all the protection of the nation; it owes nothing to the enemies of the people but death... These notions would be enough to explain the origin and the nature of laws that we call revolutionary ... If the revolutionary government must be more active in its march and more free in his movements than an ordinary government, is it for that less fair and legitimate? No; it is supported by the most holy of all laws: the salvation of the people.
Robespierre, a French lawyer, did not abandon his socialist convictions, but he was coming to the conclusion that the ends justified the means, and that in order to defend the Revolution against those who would destroy it, the shedding of blood was justified. The enigmatic figure of Robespierre takes us to the heart of the Revolution and throws light on both its ideals and the violence that indelibly scarred it. On 5 February 1794, Robespierre told the Convention, 
Six points.
Led by Robespierre, the National Convention, in an attempt to make their stance known to the world over, released a statement of French Foreign policy. The following six points acted to further highlight the convention’s fear of enemies of the Revolution. Because of this fear, several other legislations passed which furthered the Jacobin domination of the Revolution. This led to the consolidation, extension, and application of emergency government devices in order to maintain what the Revolution considered ‘control’.
The first point addressed stated:
The result of this was policy through which the state used violent repression to crush resistance to the government. Under control of the effectively dictatorial committee, the convention quickly enacted more legislation. On 9 September, the convention established "sans-culottes" paramilitary forces, the "revolutionary armies", to force farmers to surrender grain demanded by the government. On 17 September, the Law of Suspects was passed, which authorized the charging of counter-revolutionaries with vaguely defined "crimes against liberty". On 29 September, the convention extended price-fixing from grain and bread to other essential goods, and also fixed wages. The guillotine became the symbol of a string of executions: Louis XVI had already been guillotined before the start of the terror; Marie Antoinette, the Girondists, Philippe Égalité, Madame Roland and many others lost their lives under its blade.
The second point, the passing of the Law of Suspects, stepped political terror up to a much higher level of cruelty. Anyone who ‘by their conduct, relations, words or writings show themselves to be supporters of tyranny and federalism and enemies of freedom’ was targeted and suspected of treason. This created a mass overflow in the prison systems. As a result, the prison population of Paris increased from 1,417 to 4,525 people over a course of 3 months. This overpopulation problem caused the execution rates to rise enormously. From March to September of 1789 sixty-six people had been guillotined. By the end of the year, that number had risen to 177.
Though the Girondins and the Jacobins were both on the extreme left, and shared many of the same radical republican convictions, the Jacobins were more brutally efficient in setting up a war government. The year of Jacobin rule was the first time in history that terror became an official government policy, with the stated aim to use violence to achieve a higher political goal. The number of death sentences in Paris was 2,639, while the total number during the Terror in the whole of France (including Paris) was 16,594. The Jacobins were meticulous in maintaining a legal structure for the Terror so clear records exist for official death sentences. However, many more people were murdered without formal sentences pronounced in a court of law.
The Revolutionary Tribunal summarily condemned thousands of people to death by the guillotine, while mobs beat other victims to death. Sometimes people died for their political opinions or actions, but many for little reason beyond mere suspicion, or because some others had a stake in getting rid of them. The historian Peter Jones recalls the case of Claude-François Bertrand de Boucheporn, the last intendant of Béarn, whose “two sons had emigrated. attracted mounting suspicion. Eventually, he was tried on a charge of sending money abroad and, in 1794, executed.” 
Among people who were condemned by the revolutionary tribunals, about 8% were aristocrats, 6% clergy, 14% middle class, and 72% were workers or peasants accused of hoarding, evading the draft, desertion, or rebellion. Maximilien Robespierre, "frustrated with the progress of the revolution," saw politics through a populist lens because "any institution which does not suppose the people good, and the magistrate corruptible, is evil."
Another anti-clerical uprising was also made possible by the enactment of the Revolutionary Calendar on 24 October. Hébert's and Chaumette's atheist movement initiated an anti-religious campaign in order to dechristianise society. The program of dechristianisation waged against Catholicism, and also eventually against all forms of Christianity, included the deportation or execution of clergymen and women; the closing of churches; the rise of cults and the institution of a civic religion; the large scale destruction of religious monuments; the outlawing of public and private worship and religious education; the forced abjuration of priests of their vows and forced marriages of the clergy; the word "saint" being removed from street names; and the War in the Vendée.
The enactment of a law on 21 October 1793 made all suspected priests and all persons who harbored them liable to summary execution. The climax was reached with the celebration of the goddess Reason in Notre Dame Cathedral on 10 November. Because dissent was now regarded as counter-revolutionary, extremist "enragés" such as Hébert and moderate Montagnard "indulgents" such as Danton were guillotined in the spring of 1794. On 7 June, Robespierre, who favored deism over Hébert's atheism, and had previously condemned the Cult of Reason, recommended that the convention acknowledge the existence of his god. On the next day, the worship of the deistic Supreme Being was inaugurated as an official aspect of the revolution. Compared with Hébert's somewhat popular festivals, this austere new religion of Virtue was received with signs of hostility by the Parisian public.
Fall of Robespierre.
The repression brought thousands of suspects before the Paris Revolutionary Tribunal, whose work was expedited by the Law of 22 Prairial (10 June 1794). As a result of Robespierre's insistence on associating Terror with Virtue, his efforts to make the republic a morally united patriotic community became equated with the endless bloodshed. Finally, after 26 June's decisive military victory over Austria at the Battle of Fleurus, Robespierre was overthrown on 9 Thermidor (27 July).
One of the last groups to be executed during the terror were the Carmelite Nuns of Compiègne. The nuns were sentenced to death for refusing to give up their monastic vows. They were sent to the guillotine on 17 July 1794. The manner in which they approached their death, going freely up to the scaffold while singing the hymn "Veni Creator Spiritus", had a great impact on the public mood in Paris and helped to turn it against the terror.
The fall of Robespierre was brought about by a combination of those who wanted more power for the Committee of Public Safety (and a more radical policy than he was willing to allow) and the moderates who completely opposed the revolutionary government. They had, between them, made the Law of 22 Prairial one of the charges against him, so that, after his fall, to advocate terror would be seen as adopting the policy of a convicted enemy of the republic, putting the advocate's own head at risk. Robespierre tried to commit suicide before his execution by shooting himself, although the bullet only shattered his jaw. He was guillotined the next day.
The reign of the standing Committee of Public Safety was ended. New members were appointed the day after Robespierre's execution, and term limits were imposed (a quarter of the committee retired every three months); its powers were reduced piece by piece.
This was not an entirely or immediately conservative period; no government of the Republic envisaged a Restoration, and Marat was reburied in the Panthéon in September.
References.
Notes
Further reading

</doc>
<doc id="25976" url="https://en.wikipedia.org/wiki?curid=25976" title="Reliabilism">
Reliabilism

Reliabilism, a category of theories in the philosophical discipline of epistemology, has been advanced as a theory of knowledge, both of justification and of knowledge. Process reliabilism has been used as an argument against philosophical skepticism, such as the brain in a vat thought experiment.
Process reliabilism is a form of epistemic externalism.
A broadly reliabilist theory of knowledge is roughly as follows:
One knows that "p" ("p" stands for any proposition--e.g., that the sky is blue) if and only if "p" is true, one believes that "p" is true, and one has arrived at the belief that "p" through some "reliable process."
A broadly reliabilist theory of justified belief can be stated as follows:
One has a justified belief that "p" if, and only if, the belief is the result of a reliable process.
Moreover, a similar account can be given (and an elaborate version of this has been given by Alvin Plantinga) for such notions as 'warranted belief' or 'epistemically rational belief'.
Leading proponents of reliabilist theories of knowledge and justification have included Alvin Goldman, Marshall Swain, Kent Bach and more recently, Alvin Plantinga. Goldman's article "A Causal Theory of Knowing" ("Journal of Philosophy", v. 64 (1967), pp. 357–372) is generally credited as being the first full treatment of the theory, though D. M. Armstrong is also regarded as an important source, and (according to Hugh Mellor) Frank Ramsey was the very first to state the theory, albeit in passing.
One classical or traditional analysis of 'knowledge' is "justified true belief". In order to have a valid claim of knowledge for any proposition, one must be justified in believing "p" and "p" must be true. Since Gettier proposed his counterexamples the traditional analysis has included the further claim that knowledge must be more than justified true belief. Reliabilist theories of knowledge are sometimes presented as an alternative to that theory: rather than justification, all that is required is that the belief be the product of a reliable process. But reliabilism need not be regarded as an alternative, but instead as a further explication of the traditional analysis. On this view, those who offer reliabilist theories of justification further analyze the 'justification' part of the traditional analysis of 'knowledge' in terms of reliable processes. Not all reliabilists agree with such accounts of justification, but some do.
Objections.
Some find reliabilism of justification objectionable because it entails externalism, which is the view that one can have knowledge, or have a justified belief, despite not knowing (having "access" to) the evidence, or other circumstances, that make the belief justified. Most reliabilists maintain that a belief can be justified, or can constitute knowledge, "even if" the believer does not know about or understand the process that makes the belief reliable. In defending this view, reliabilists (and externalists generally) are apt to point to examples from simple acts of perception: if one sees a bird in the tree outside one's window and thereby gains the belief that there is a bird in that tree, one might not at all understand the cognitive processes that account for one's successful act of perception; nevertheless, it is the fact that the processes worked reliably that accounts for why one's belief is justified. In short, one finds one holds a belief about the bird, and that belief is justified if any is, but one is not acquainted at all with the processes that led to the belief that justified one's having it. Of course, internalists do not let the debate rest there; see externalism (epistemology).
Another of the most common objections to reliabilism, made first to Goldman's reliable process theory of knowledge and later to other reliabilist theories, is the so-called generality problem. For any given justified belief (or instance of knowledge), one can easily identify many different (concurrently operating) "processes" from which the belief results. My belief that there is a bird in the tree outside my window might be accorded a result of the process of forming beliefs on the basis of sense-perception, of visual sense-perception, of visual sense-perception through non-opaque surfaces in daylight, and so forth, down to a variety of different very specifically described processes. Some of these processes might be statistically reliable, while others might not. It would no doubt be better to say, in any case, that we are choosing not which process to say resulted in the belief, but instead how to describe the process, out of the many different levels of generality on which it can be accurately described.
An objection in a similar line was formulated by Stephen Stich in "The Fragmentation of Reason". Reliabilism usually considers that for generating justified beliefs a process needs to be reliable in a set of relevant possible scenarios. However, according to Stich, these scenarios are chosen in a culturally biased manner. Stich does not defend any alternative theory of knowledge or justification, but instead argues that all accounts of normative epistemic terms are culturally biased and instead only a pragmatic account can be given.
Another objection to reliabilism is called the "new evil demon problem". The evil demon problem originally motivated skepticism, but can be resuited to object to reliabilist accounts as follows: If our experiences are controlled by an evil demon, it may be the case that we believe ourselves to be doing things that we are not doing. However, these beliefs are clearly justified. Robert Brandom has called for a clarification of the role of belief in reliabilist theories. Brandom is concerned that unless the role of belief is stressed, reliabilism may attribute knowledge to things that would otherwise be considered incapable of possessing it. Brandom gives the example of a parrot that has been trained to consistently respond to red visual stimuli by saying 'that's red'. The proposition is true, the mechanism that produced it is reliable, but Brandom is reluctant to say that the parrot "knows" it is seeing red because he thinks it cannot "believe" that it is. For Brandom, beliefs pertain to concepts: without the latter there can be no former. Concepts are products of the 'game of giving and asking for reasons'. Hence, only those entities capable of reasoning, through language in a social context, can for Brandom believe and thus have knowledge. Brandom may be regarded as hybridising externalism and internalism, allowing knowledge to be accounted for by reliable external process so long as a knower possess some internal understanding of why the belief is reliable.

</doc>
<doc id="25977" url="https://en.wikipedia.org/wiki?curid=25977" title="Ideal (ring theory)">
Ideal (ring theory)

In ring theory, a branch of abstract algebra, an ideal is a special subset of a ring. Ideals generalize certain subsets of the integers, such as the even numbers or the multiples of 3. Addition and subtraction of even numbers preserves evenness, and multiplying an even number by any other integer results in another even number; these closure and absorption properties are the defining properties of an ideal. An ideal can be used to construct a quotient ring similarly to the way that, in group theory, a normal subgroup can be used to construct a quotient group.
Among the integers, the ideals correspond one-for-one with the non-negative integers: in this ring, every ideal is a principal ideal consisting of the multiples of a single non-negative number. However, in other rings, the ideals may be distinct from the ring elements, and certain properties of integers, when generalized to rings, attach more naturally to the ideals than to the elements of the ring. For instance, the prime ideals of a ring are analogous to prime numbers, and the Chinese remainder theorem can be generalized to ideals. There is a version of unique prime factorization for the ideals of a Dedekind domain (a type of ring important in number theory).
The concept of an order ideal in order theory is derived from the notion of ideal in ring theory. A fractional ideal is a generalization of an ideal, and the usual ideals are sometimes called integral ideals for clarity.
History.
Ideals were first proposed by Richard Dedekind in 1876 in the third edition of his book "Vorlesungen über Zahlentheorie" (English: "Lectures on Number Theory"). They were a generalization of the concept of ideal numbers developed by Ernst Kummer. Later the concept was expanded by David Hilbert and especially Emmy Noether.
Definitions.
For an arbitrary ring formula_1, let formula_2 be its additive group. A subset formula_3 is called a two-sided ideal (or simply an ideal) of formula_4 if it is an additive subgroup of "R" that "absorbs multiplication by elements of "R"". Formally we mean that formula_3 is an ideal if it satisfies the following conditions:
Equivalently, an ideal of "R" is a sub-"R"-bimodule of "R".
A subset formula_3 of formula_4 is called a right ideal of formula_4 if it is an additive subgroup of "R" and absorbs multiplication on the right, that is:
Equivalently, a right ideal of formula_4 is a right formula_4-submodule of formula_4.
Similarly a subset formula_3 of formula_4 is called a left ideal of formula_4 if it is an additive subgroup of "R" absorbing multiplication on the left:
Equivalently, a left ideal of formula_4 is a left formula_4-submodule of formula_4.
In all cases, the first condition can be replaced by the following well-known criterion that ensures a nonempty subset of a group is a subgroup:
The left ideals in "R" are exactly the right ideals in the opposite ring "R"o and vice versa. A two-sided ideal is a left ideal that is also a right ideal, and is often called an ideal except to emphasize that there might exist single-sided ideals. When "R" is a commutative ring, the definitions of left, right, and two-sided ideal coincide, and the term "ideal" is used alone.
Properties.
Just as normal subgroups of groups are kernels of group homomorphisms, ideals have interpretations as kernels. For a nonempty subset "A" of "R": 
If "p" is in "R", then "pR" is a right ideal and "Rp" is a left ideal of "R". These are called, respectively, the principal right and left ideals generated by "p". To remember which is which, note that right ideals are stable under right-multiplication ("IR" ⊆ "I") and left ideals are stable under left-multiplication ("RI" ⊆ "I").
The connection between cosets and ideals can be seen by switching the operation from "multiplication" to "addition".
Motivation.
Intuitively, the definition can be motivated as follows: Suppose we have a subset of elements "Z" of a ring "R" and that we would like to obtain a ring with the same structure as "R", except that the elements of "Z" should be zero (they are in some sense "negligible").
But if formula_29 and formula_30 in our new ring, then surely formula_31 should be zero too, and formula_32 as well as formula_33 should be zero for "any" element formula_34 (zero or not).
The definition of an ideal is such that the ideal "I" generated (see below) by "Z" is exactly the set of elements that are forced to become zero if "Z" becomes zero, and the quotient ring "R/I" is the desired ring where "Z" is zero, and "only" elements that are forced by "Z" to be zero are zero. The requirement that "R" and "R/I" should have the same structure (except that "I" becomes zero) is formalized by the condition that the projection from "R" to "R/I" is a (surjective) ring homomorphism.
Ideal generated by a set.
Let "R" be a (possibly not unital) ring. Any intersection of any nonempty family of left ideals of "R" is again a left ideal of "R". If "X" is any subset of "R", then the intersection of all left ideals of "R" containing "X" is a left ideal "I" of "R" containing "X", and is clearly the smallest left ideal to do so. This ideal "I" is said to be the left ideal generated by "X". Similar definitions can be created by using right ideals or two-sided ideals in place of left ideals.
If "R" has unity, then the left, right, or two-sided ideal of "R" generated by a subset "X" of "R" can be expressed internally as we will now describe. The following set is a left ideal:
Each element described would have to be in every left ideal containing "X", so this left ideal is in fact the left ideal generated by "X". The right ideal and ideal generated by "X" can also be expressed in the same way:
The former is the right ideal generated by "X", and the latter is the ideal generated by "X".
By convention, 0 is viewed as the sum of zero such terms, agreeing with the fact that the ideal of "R" generated by ∅ is {0} by the previous definition.
If a left ideal "I" of "R" has a finite subset "F" such that "I" is the left ideal generated by "F", then the left ideal "I" is said to be finitely generated. Similar terms are also applied to right ideals and two-sided ideals generated by finite subsets.
In the special case where the set "X" is just a singleton {"a"} for some "a" in "R", then the above definitions turn into the following:
These ideals are known as the left/right/two-sided principal ideals generated by "a". It is also very common to denote the two-sided ideal generated by "a" as ("a").
If "R" does not have a unit, then the internal descriptions above must be modified slightly. In addition to the finite sums of products of things in "X" with things in "R", we must allow the addition of "n"-fold sums of the form "x"+"x"+...+"x", and "n"-fold sums of the form ("−x")+("−x")+...+("−x") for every "x" in "X" and every "n" in the natural numbers. When "R" has a unit, this extra requirement becomes superfluous.
Types of ideals.
Ideals are important because they appear as kernels of ring homomorphisms and allow one to define factor rings. Different types of ideals are studied because they can be used to construct different types of factor rings.
Two other important terms using "ideal" are not always ideals of their ring. See their respective articles for details:
Ideal operations.
The sum and product of ideals are defined as follows. For formula_52 and formula_53, ideals of a ring R,
and
i.e. the product of two ideals formula_52 and formula_53 is defined to be the ideal formula_58 generated by all products of the form "ab" with "a" in formula_52 and "b" in formula_53. The product formula_58 is contained in the intersection of formula_52 and formula_53.
The sum and the intersection of ideals is again an ideal; with these two operations as join and meet, the set of all ideals of a given ring forms a complete modular lattice. Also, the union of two ideals is a subset of the sum of those two ideals, because for any element "a" inside an ideal, we can write it as "a"+0, or 0+"a", therefore, it is contained in the sum as well. However, the union of two ideals is not necessarily an ideal.
Ideals and congruence relations.
There is a bijective correspondence between ideals and congruence relations (equivalence relations that respect the ring structure) on the ring:
Given an ideal "I" of a ring "R", let "x" ~ "y" if "x" − "y" ∈ "I". Then ~ is a congruence relation on "R".
Conversely, given a congruence relation ~ on "R", let "I" = {"x" : "x" ~ 0}. Then "I" is an ideal of "R".

</doc>
<doc id="25978" url="https://en.wikipedia.org/wiki?curid=25978" title="Reversi">
Reversi

Reversi is a strategy board game for two players, played on an 8×8 uncheckered board. There are sixty-four identical game pieces called "disks" (often spelled "discs"), which are light on one side and dark on the other. Players take turns placing disks on the board with their assigned color facing up. During a play, any disks of the opponent's color that are in a straight line and bounded by the disk just placed and another disk of the current player's color are turned over to the current player's color.
The object of the game is to have the majority of disks turned to display your color when the last playable empty square is filled.
Reversi was most recently marketed by Mattel under the trademark Othello.
History.
Original version.
The game "Reversi" was invented in 1883 by either of two English men (each claiming the other a fraud), Lewis Waterman or John W. Mollett (or perhaps earlier by someone else entirely), and gained considerable popularity in England at the end of the nineteenth century. The game's first reliable mention is in the August twenty-first 1886 edition of "The Saturday Review". Later mention includes an 1895 article in "The New York Times": "Reversi is something like Go Bang, and is played with 64 pieces." In 1893, the well-known German games publisher Ravensburger started producing the game as one of its first titles. Two 18th-century continental European books dealing with a game that may or may not be Reversi are mentioned on page fourteen of the Spring 1989 "Othello Quarterly", and there has been speculation, so far without documentation, that the game has even more ancient origins.
Modern version.
The modern version of the game — the most regularly used rule-set, and the one used in international tournaments — is marketed and recognized as Othello, which was perfected by Goro Hasegawa (autonym: Satoshi Hasegawa) originated in Japan in the 1970s. There are two differences from the original game:
Hasegawa established the Japan Othello Association on March 1973, and held the first national Othello championship on April 4, 1973 in Japan. The Japanese game company Tsukuda Original launched Othello in late April, 1973 in Japan under Hasegawa’s license, which led to an immediate commercial success.
The name was selected by Hasegawa as a reference to the Shakespearean play "Othello, the Moor of Venice", referring to the conflict between the Moor Othello and Iago, and more controversially, to the unfolding drama between Othello, who is black, and Desdemona, who is white. The green color of the board is inspired by the image of the general Othello, valiantly leading his battle in a green field. It can also be likened to a jealousy competition (jealousy being the central theme in Shakespeare's play), since players engulf the pieces of the opponent, thereby turning them to their possession.
Othello was first launched in the U.S. in 1975 by Gabriel Industries and it also enjoyed commercial success there. It is said Othello game sales have exceeded $600 million and more than 40 million classic games have been sold in over 100 different countries.
Hasegawa also wrote "How to Othello (Osero No Uchikata)" in Japan in 1974, which was later translated into English and published in the U.S. in 1977 as "How to Win at Othello".
Kabushiki Kaisha Othello, which is owned by Hasegawa, registered the trademark "OTHELLO" for board games in Japan and Tsukuda Original registered the mark in the rest of the world. All intellectual property regarding Othello outside Japan is now owned by MegaHouse, a Japanese toy company that acquired PalBox, the successor to Tsukuda Original.
MegaHouse has gratefully acknowledged the late James R. Becker and Anjar Co. for their role in successfully marketing, licensing, selling, promoting, distributing, and popularizing OTHELLO branded products in the U.S. and around the world outside Japan since 1975.
Rules.
Each of the disks' two sides corresponds to one player; they are referred to here as "light" and "dark" after the sides of "Othello" pieces, but any counters with distinctive faces are suitable. The game may for example be played with a chessboard and Scrabble pieces, with one player "letters" and the other "backs".
The historical version of Reversi starts with an empty board, and the first two moves by each player are in the four central squares of the board. The players place their disks alternately with their color facing up and no captures are made. A players may choose to not play both pieces on the same diagonal, different from the standard "Othello" opening. It is also possible to play variants of Reversi and "Othello" wherein the second player's second move may or must flip one of the opposite-colored disks (as variants closest to the normal games).
For the specific game of "Othello" (as technically differing from the historical Reversi), the rules state that the game begins with four disks placed in a square in the middle of the grid, two facing white side up, two pieces with the dark side up, with same-colored disks on a diagonal with each other. Convention has initial board position such that the disks with dark side up are to the north-east and south-west (from both players' perspectives), though this is only marginally meaningful to play (where opening memorization is an issue, some players may benefit from consistency on this). If the disks with dark side up are to the north-west and south-east, the board may be rotated by 90° clockwise or counterclockwise. The dark player moves first.
Dark must place a piece with the dark side up on the board, in such a position that there exists at least one straight (horizontal, vertical, or diagonal) occupied line between the new piece and another dark piece, with one or more contiguous light pieces between them. In the below situation, dark has the following options indicated by translucent pieces:
After placing the piece, dark turns over (flips, captures) all light pieces lying on a straight line between the new piece and any anchoring dark pieces. All reversed pieces now show the dark side, and dark can use them in later moves—unless light has reversed them back in the meantime. In other words, a valid move is one where at least one piece is reversed.
If dark decided to put a piece in the topmost location (all choices are strategically equivalent at this time), one piece gets turned over, so that the board appears thus:
Now light plays. This player operates under the same rules, with the roles reversed: light lays down a light piece, causing a dark piece to flip. Possibilities at this time appear thus (indicated by transparent pieces):
Light takes the bottom left option and reverses one piece:
Players take alternate turns. If one player can not make a valid move, play passes back to the other player. When neither player can move, the game ends. This occurs when the grid has filled up or when neither player can legally place a piece in any of the remaining squares. This means the game may end before the grid is completely filled. This possibility may occur because one player has no pieces remaining on the board in that player's color. In over-the-board play this is generally scored as if the board were full (64–0).
Example where the game ends before the grid is completely filled:
The player with the most pieces on the board at the end of the game wins. An exception to this is that if a clock is employed then if one player defaults on time that player's opponent wins regardless of the board configuration, with varying methods to determine the official score where one is required.
In common practice over the internet, opponents agree upon a time-control of, typically, from one to thirty minutes per game per player. Standard time control in the World Championship is thirty minutes, and this or something close to it is common in over-the-board (as opposed to internet) tournament play generally. In time-defaulted games, where disk differential is used for tie-breaks in tournaments or for rating purposes, one common over-the-board procedure for the winner of defaulted contests to complete both sides' moves with the greater of the result thereby or one disk difference in the winner's favor being the recorded score. Games in which both players have the same number of disks their color at the end (almost always with a full-board 32–32 score) are not very common, but also not rare, and these are designated as 'ties' and scored as half of a win for each player in tournaments. The term 'draw' for such may also be heard, but is somewhat frowned upon.
What are generally referred to as "transcript sheets" are generally in use in tournament over-the-board play, with both players obligated to record their game's moves by placing the number of each move in an 8×8 grid. This both enables players to look up past games of note and tournament directors and players to resolve disputes (according to whatever specific rules are in place) where claims that an illegal move, flip or other anomaly are voiced. An alternative recording method not requiring a grid is also in use, where positions on a board are labeled left to right by letters "a" through "h" and top to bottom (far-to-near) by digits "1" through "8" (Note that this is the opposite of the chess standard, with numerals running upward away from the side (White) that has "a" through "h" left to right, and also that the perspective may be that of either player (with no fixed standard)), so that the very first move of a game may be (based upon standard starting setup) d3, c4, f5 or e6. This alternate notational scheme is used primarily in verbal discussions or where a linear representation is desirable in print, but may also be permissible as during-game transcription by either or both players.
Tournament play using ordinary sets rather than a computer interface—where this can not be an issue—have various ways of handling illegal moves and over- or underflipping (flips that should not be made but are or should be but are not). For example, permitting either player (perpetrator or its opponent) to make a correction going back some fixed number of moves (after which no remedy is available) is one procedure that has been used.
Significant variants of the game, such as where the starting position differs from standard or the objective is to have the fewest pieces one's color at the end, are sometimes—but rarely—played.
Strategic elements.
Strategic concepts in Reversi include openings (and home preparation), corners, mobility, edge play, parity, end-game play and looking ahead.
Openings.
For relatively inexperienced players the opening (early) part of a game of Reversi is generally not very easy to make sense of. A rule of thumb on the opening is that a good opening is one that leads to a good middle-game. By and large, good moves in the very earliest stages are determined by whether there is a "refutation" to a move only and few other truly general considerations aside from what exactly constitutes a refutation. The first move (by dark) is no choice at all other than for the purpose of the player's possible sense of ideal visualization. The first move by light gives three choices, and, in fact, it is generally accepted at the highest level that one of these actually may be successfully refuted, that being what is known as the "Parallel opening". The other two choices by light are called the "Diagonal opening" and the "Perpendicular opening", and these three in the order mentioned with f5 as dark's first move (See discussion on notation above) are f4, f6 and d6.
Corners.
Corner positions, once played, remain immune to flipping for the rest of the game, being termini of horizontal, vertical and diagonal lines. More generally, a piece is "stable" when, along all four axes (horizontal, vertical, and each diagonal), it is in terminal position or if from it along the axis one reaches a terminal disk passing only through disks the same color. These are not the only kinds of stable disk, however, and occupying a corner may often be a grave error if one allows one's opponent to create a "wedge" that results in him or her gathering more stable disks. This can render occupying the corner largely useless, and often much worse than that because of loss of "tempo" (Where it is an issue of running out of desirable moves and being forced to make undesirable ones, the grabbing of a corner may give the opponent not only the wedging response but also a following move which one can not respond to practically).
Mobility.
An opponent playing with reasonable strategy will not so easily relinquish the corner or any other good moves. So to achieve these good moves, a player must force its opponent to play moves that relinquish those good moves. One of the ways to achieve this involves reducing the number of moves available to the player's opponent. Ideally, this will eventually force the opponent to make an undesirable move.
Edges.
Edge pieces can anchor flips that influence moves to all regions of the board. If played poorly, this can poison later moves by causing players to flip too many pieces and open up many moves for the opponent. However, playing on edges where an opponent can not easily respond drastically reduces possible moves for that opponent.
The square immediately diagonally adjacent to the corner (called the X-square), when played in the early or middle game, typically guarantees the loss of that corner. Nevertheless, such a corner sacrifice is sometimes played for some strategic purpose (like retaining mobility). Playing to the edge squares adjacent to the corner (called the C-squares) can also be dangerous if it gives the opponent powerful forcing moves.
Parity.
Parity is one of the most important parts of the strategy. In short, the concept of parity is about getting the last move in every empty region in the end-game, and thereby increasing the number of stable disks.
The concept of parity led to a change in the perception of the game, as it led to distinct strategies for playing Black and White. It forced Black to play more aggressive moves and gave White the opportunity to stay calm and focus on keeping the parity. As a result, the opening books and midgame were focused on Black being the "attacker" and White being the "defender".
The concept of parity also controls how edge positions are played and how edges interact.
End-game.
For the end-game (the last twenty or so moves of the game) the strategies will typically change. Special techniques such as sweeping, gaining access, and the details of move order can have a large impact on the outcome of the game. Actual counting of disks in the very final stages is often critical, but sometimes in human play an inaccurate choice, for disk differential can be better than an accurate one in terms of the expected outcome (and can be essential in lost positions).
Brightwell Quotient.
Invented by the British Mathematician and three times runner-up at the World Championship and five times British Champion Graham Brightwell, this is the tie-breaker that is now used in many tournaments including the W.O.C. If two players have the same number of points in the thirteen rounds W.O.C. Swiss, the tie is resolved in favour of the player with the higher Brightwell Quotient.
The Brightwell Quotient (BQ) is calculated as follows:
Computer opponents and research.
Good Othello computer programs play very strong against human opponents. This is mostly due to difficulties in human look-ahead peculiar to Othello: The interchangeability of the disks and therefore apparent strategic meaninglessness (as opposed to chess pieces for example) makes an evaluation of different moves much harder. This can be demonstrated with blindfold games, as the memorization of the board demands much more dedication from the players than in blindfold chess.
Also the game has been particularly attractive to programmers.
Therefore the best Othello computer programs have easily defeated the best humans since 1980, when the program "The Moor" beat the reigning world champion.
In 1997, Logistello defeated the human champion Takeshi Murakami with a score of 6–0.
Analysts have estimated the number of legal positions in Othello is at most 1028, and it has a game-tree complexity of approximately 1058.
Mathematically, Othello still remains unsolved. Experts have not absolutely resolved what the outcome of a game will be where both sides use perfect play. However, analysis of thousands of high quality games (most of them computer-generated) appears to lead to a reliable conclusion (pending actual proof if true) that, on the standard 8×8 board, perfect play on both sides results in a draw.
When generalizing the game to play on an "n"×"n" board, the problem of determining if the first player has a winning move in a given position is PSPACE-complete.
On 4×4 and 6×6 boards under perfect play, the second player wins.
The first of these proofs is relatively trivial, and the second dates to around 1990.
Further reading.
Othello books to increase skill to tournament-level play:

</doc>
<doc id="25980" url="https://en.wikipedia.org/wiki?curid=25980" title="Radix sort">
Radix sort

In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines.
Most digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient. Two classifications of radix sorts are least significant digit (LSD) radix sorts and most significant digit (MSD) radix sorts. LSD radix sorts process the integer representations starting from the least digit and move towards the most significant digit. MSD radix sorts work the other way around.
LSD radix sorts typically use the following sorting order: short keys come before longer keys, and keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.
MSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as "b, c, d, e, f, g, h, i, j, ba" would be lexicographically sorted as "b, ba, c, d, e, f, g, h, i, j". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order.
Efficiency.
The topic of the efficiency of radix sort compared to other sorting algorithms is somewhat tricky and subject to quite a lot of misunderstandings. Whether radix sort is equally efficient, less efficient or more efficient than the best comparison-based algorithms depends on the details of the assumptions made. Radix sort complexity is for keys which are integers of word size . Sometimes is presented as a constant, which would make radix sort better (for sufficiently large ) than the best comparison-based sorting algorithms, which all perform comparisons to sort keys. However, in general cannot be considered a constant: if all keys are distinct, then has to be at least for a random-access machine to be able to store them in memory, which gives at best a time complexity . That would seem to make radix sort at most equally efficient as the best comparison-based sorts (and worse if keys are much longer than ).
The counter argument is that comparison-based algorithms are measured in number of comparisons, not actual time complexity.
Under some assumptions the comparisons will be constant time on average, under others they will not. Comparisons of randomly generated keys takes constant time on average, as keys differ on the very first bit in half the cases, and differ on the second bit in half of the remaining half, and so on, resulting in an average of two bits that need to be compared. In a sorting algorithm the first comparisons made satisfies the randomness condition, but as the sort progresses the keys compared are clearly not randomly chosen anymore. For example, consider a bottom-up merge sort. The first pass will compare pairs of random keys, but the last pass will compare keys that are very close in the sorting order. This makes merge sort, on this class of inputs, take time. That assumes all memory accesses cost the same, which is not a physically reasonable assumption as we scale to infinity, and not, in practice, how real computers work.
Least significant digit radix sorts.
A Least significant digit (LSD) Radix sort is a fast stable sorting algorithm which can be used to sort keys in integer representation order. Keys may be a string of characters, or numerical digits in a given 'radix'. The processing of the keys begins at the least significant digit (i.e., the rightmost digit), and proceeds to the most significant digit (i.e., the leftmost digit). The sequence in which digits are processed by an LSD radix sort is the opposite of the sequence in which digits are processed by a most significant digit (MSD) radix sort.
An LSD radix sort operates in O("nk") time, where "n" is the number of keys, and "k" is the average key length. This kind of performance for variable-length keys can be achieved by grouping all of the keys that have the same length together and separately performing an LSD radix sort on each group of keys for each length, from shortest to longest, in order to avoid processing the whole list of keys on every sorting pass.
A radix sorting algorithm was originally used to sort punched cards in several passes. A computer algorithm was invented for radix sort in 1954 at MIT by Harold H. Seward. In many large applications needing speed, the computer radix sort is an improvement on (slower) comparison sorts.
LSD radix sorts have resurfaced as an alternative to high performance comparison-based sorting algorithms (like heapsort and mergesort) that require O("n" · log "n") comparisons, where "n" is the number of items to be sorted. Comparison sorts can do no better than O("n" · log "n") execution time but offer the flexibility of being able to sort with respect to more complicated orderings than a lexicographic one; however, this ability is of little importance in many practical applications.
Definition.
Each key is first figuratively dropped into one level of buckets corresponding to the value of the rightmost digit. Each bucket preserves the original order of the keys as the keys are dropped into the bucket. There is a one-to-one correspondence between the buckets and the values that can be represented by the rightmost digit. Then, the process repeats with the next neighbouring more significant digit until there are no more digits to process. In other words:
The sort in step 2 is usually done using bucket sort or counting sort, which are efficient in this case since there are usually only a small number of digits.
An example.
Original, unsorted list:
Sorting by least significant digit (1s place) gives:
Sorting by next digit (10s place) gives:
Sorting by most significant digit (100s place) gives:
It is important to realize that each of the above steps requires just a single pass over the data, since each item can be placed in its correct bucket without having to be compared with other items.
Some radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each digit occurs is stored in an array. Consider the previous list of keys viewed in a different way:
The first counting pass starts on the least significant digit of each key, producing an array of bucket sizes:
A second counting pass on the next more significant digit of each key will produce an array of bucket sizes:
A third and final counting pass on the most significant digit of each key will produce an array of bucket sizes:
At least one LSD radix sort implementation now counts the number of times that each digit occurs in each column for all columns in a single counting pass. (See the external links section.) Other LSD radix sort implementations allocate space for buckets dynamically as the space is needed.
Iterative version using queues.
A simple version of an LSD radix sort can be achieved using queues as buckets. The following process is repeated for a number of times equal to the length of the longest key:
While this may not be the most efficient radix sort algorithm, it is relatively simple, and still quite efficient.
Most significant digit radix sorts.
A most significant digit (MSD) radix sort can be used to sort keys in lexicographic order. Unlike a least significant digit (LSD) radix sort, a most significant digit radix sort does not necessarily preserve the original order of duplicate keys. An MSD radix sort starts processing the keys from the most significant digit, leftmost digit, to the least significant digit, rightmost digit. This sequence is opposite that of least significant digit (LSD) radix sorts. An MSD radix sort stops rearranging the position of a key when the processing reaches a unique prefix of the key. Some MSD radix sorts use one level of buckets in which to group the keys. See the counting sort and pigeonhole sort articles. Other MSD radix sorts use multiple levels of buckets, which form a trie or a path in a trie. A postman's sort / postal sort is a kind of MSD radix sort.
Recursion.
A recursively subdividing MSD radix sort algorithm works as follows:
Recursive forward radix sort example.
Sort the list:
<br>170, 045, 075, 090, 002, 024, 802, 066
Sorting by least significant digit (1s place) is not needed, as there is no tens bucket with more than one number. Therefore, the now sorted zero hundreds bucket is concatenated, joined in sequence, with the one hundreds bucket and eight hundreds bucket to give:<br>002, 024, 045, 066, 075, 090, 170, 802
This example used base ten digits for the sake of readability, but of course binary digits or perhaps bytes might make more sense for a binary computer to process.
In-place MSD radix sort implementations.
Binary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.
In-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-Radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-Radix), starting from the most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting.
Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.
Stable MSD radix sort implementations.
MSD Radix Sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD Radix Sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.
Hybrid approaches.
Radix sort, such as two pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of Insertion sort is fast for small arrays, stable, in-place, and can significantly speed up Radix Sort.
Application to parallel computing.
Note that this recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.
In the top level of recursion, opportunity for parallelism is in the Counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.
Note that there are faster sorting algorithms available, for example optimal complexity O(log("n")) are those of the Three Hungarians and Richard Cole and Batcher's bitonic merge sort has an algorithmic complexity of O(log2("n")), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with "n" processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O("k"), where "k" is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fanout gate delays per cycle increasing as O(log("n")), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log("n")) PRAM sorts are all O(log2("n")) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog2("n")).
trie-based radix sort.
Another way to proceed with an MSD radix sort is to use more memory to create a trie to represent the keys and then traverse the trie to visit each key in order. A depth-first traversal of a trie starting from the root node will visit each key in order. A depth-first traversal of a trie, or any other kind of acyclic tree structure, is equivalent to traversing a maze via the right-hand rule.
A trie essentially represents a set of strings or numbers, and a radix sort which uses a trie structure is not necessarily stable, which means that the original order of duplicate keys is not necessarily preserved, because a set does not contain duplicate elements. Additional information will have to be associated with each key to indicate the population count or original order of any duplicate keys in a trie-based radix sort if keeping track of that information is important for a particular application. It may even be desirable to discard any duplicate strings as the trie creation proceeds if the goal is to find only unique strings in sorted order. Some people sort a list of strings first and then make a separate pass through the sorted list to discard duplicate strings, which can be slower than using a trie to simultaneously sort and discard duplicate strings in one pass.
One of the advantages of maintaining the trie structure is that the trie makes it possible to determine quickly if a particular key is a member of the set of keys in a time that is proportional to the length of the key, "k", in O("k") time, that is "independent" of the total number of keys. Determining set membership in a plain list, as opposed to determining set membership in a trie, requires binary search, O("k log(n)") time; linear search, O("kn") time; or some other method whose execution time is in some way dependent on the total number, "n", of all of the keys in the worst case. It is sometimes possible to determine set membership in a plain list in O("k") time, in a time that is independent of the total number of keys, such as when the list is known to be in an arithmetic sequence or some other computable sequence.
Maintaining the trie structure also makes it possible to insert new keys into the set incrementally or delete keys from the set incrementally while maintaining sorted order in O("k") time, in a time that is independent of the total number of keys. In contrast, other radix sorting algorithms must, in the worst case, re-sort the entire list of keys each time that a new key is added or deleted from an existing list, requiring O("kn") time.
Snow White analogy.
If the nodes were rooms connected by hallways, then here is how Snow White might proceed to visit all of the dwarfs if the place were dark, keeping her right hand on a wall at all times:
These series of steps serve to illustrate the path taken in the trie by Snow White via a depth-first traversal to visit the dwarfs by the ascending order of their names, Bashful, Doc, Dopey, Grumpy, Happy, Sleepy, and Sneezy. The algorithm for performing some operation on the data associated with each node of a tree first, such as printing the data, and then moving deeper into the tree is called a pre-order traversal, which is a kind of depth-first traversal. A pre-order traversal is used to process the contents of a trie in ascending order. If Snow White wanted to visit the dwarfs by the descending order of their names, then she could walk backwards while following the wall with her right hand, or, alternatively, walk forward while following the wall with her left hand. The algorithm for moving deeper into a tree first until no further descent to unvisited nodes is possible and then performing some operation on the data associated with each node is called post-order traversal, which is another kind of depth-first traversal. A post-order traversal is used to process the contents of a trie in descending order.
The root node of the trie in the diagram essentially represents a null string, an empty string, which can be useful for keeping track of the number of blank lines in a list of words. The null string can be associated with a circularly linked list with the null string initially as its only member, with the forward and backward pointers both initially pointing to the null string. The circularly linked list can then be expanded as each new key is inserted into the trie. The circularly linked list is represented in the following diagram as thick, grey, horizontally linked lines:
If a new key, other than the null string, is inserted into a leaf node of the trie, then the computer can go to the last preceding node where there was a key or a bifurcation to perform a depth-first search to find the lexicographic successor or predecessor of the inserted key for the purpose of splicing the new key into the circularly linked list. The last preceding node where there was a key or a bifurcation, a fork in the path, is a parent node in the type of trie shown here, where only unique string prefixes are represented as paths in the trie. If there is already a key associated with the parent node that would have been visited during a movement "away" from the root during a right-hand, forward-moving, depth-first traversal, then that immediately ends the depth-first search, as that key is the predecessor of the inserted key. For example, if Bashful is inserted into the trie, then the predecessor is the null string in the parent node, which is the root node in this case. In other words, if the key that is being inserted is on the leftmost branch of the parent node, then any string contained in the parent node is the lexicographic predecessor of the key that is being inserted, else the lexicographic predecessor of the key that is being inserted exists down the parent node's branch that is immediately to the left of the branch where the new key is being inserted. For example, if Grumpy were the last key inserted into the trie, then the computer would have a choice of trying to find either the predecessor, Dopey, or the successor, Happy, with a depth-first search starting from the parent node of Grumpy. With no additional information to indicate which path is longer, the computer might traverse the longer path, D, O, P. If Dopey were the last key inserted into the trie, then the depth-first search starting from the parent node of Dopey would soon find the predecessor, "Doc", because that would be the only choice.
If a new key is inserted into an internal node, then a depth-first search can be started from the internal node to find the lexicographic successor. For example, if the literal string "DO" were inserted in the node at the end of the path D, O, then a depth-first search could be started from that internal node to find the successor, "DOC", for the purpose of splicing the new string into the circularly linked list.
Forming the circularly linked list requires more memory but allows the keys to be visited more directly in either ascending or descending order via a linear traversal of the linked list rather than a depth-first traversal of the entire trie. This concept of a circularly linked trie structure is similar to the concept of a threaded binary tree. This structure will be called a circularly threaded trie.
When a trie is used to sort numbers, the number representations must all be the same length unless you are willing to perform a breadth-first traversal. When the number representations will be visited via depth-first traversal, as in the above diagram, the number representations will always be on the leaf nodes of the trie. Note how similar in concept this particular example of a trie is to the recursive forward radix sort example which involves the use of buckets instead of a trie. Performing a radix sort with the buckets is like creating a trie and then discarding the non-leaf nodes.

</doc>
<doc id="25982" url="https://en.wikipedia.org/wiki?curid=25982" title="Tug of war">
Tug of war

Tug of war (also known as war of tug, tug o' war, tug war, rope war, rope pulling, or tugging war) is a sport that directly puts two teams against each other in a test of strength: teams pull on opposite ends of a rope, with the goal being to bring the rope a certain distance in one direction against the force of the opposing team's pull.
Terminology.
The "Oxford English Dictionary" says that the phrase "tug of war" originally meant "the decisive contest; the real struggle or tussle; a severe contest for supremacy". Only in the 19th century was it used as a term for an athletic contest between two teams who haul at the opposite ends of a rope.
Origin.
The origin of tug of war are uncertain, but this sport was practised in ancient Egypt, Greece and China, where it was held in legend that the Sun and Moon played Tug of War over the light and darkness.
According to a Tang dynasty book, "The Notes of Feng", tug of war, under the name "hook pulling" (牽鉤), was used by the military commander of the State of Chu during the Spring and Autumn Period (8th century BC to 5th century BC) to train warriors. During the Tang dynasty, Emperor Xuanzong of Tang promoted large-scale tug of war games, using ropes of up to with shorter ropes attached, and more than 500 people on each end of the rope. Each side also had its own team of drummers to encourage the participants.
In ancient Greece the sport was called "helkustinda" (Greek: ἑλκυστίνδα), "efelkustinda" (ἐφελκυστίνδα) and "dielkustinda" (διελκυστίνδα), which derives from "dielkō" (διέλκω), meaning amongst others "I pull through", all deriving from the verb "helkō" (ἕλκω), "I draw, I pull". "Helkustinda" and "efelkustinda" seem to have been ordinary versions of tug of war, while "dielkustinda" had no rope, according to Julius Pollux. It is possible that the teams held hands when pulling, which would have increased difficulty, since handgrips are more difficult to sustain than a grip of a rope. Tug of war games in ancient Greece were among the most popular games used for strength and would help build strength needed for battle in full armor.
Archeological evidence shows that tug of war was also popular in India in the 12th century:
Tug of war stories about heroic champions from Scandinavia and Germany circulate Western Europe where Viking warriors pull on animal skins over open pits of fire in tests of strength and endurance, in preparation for battle and plunder.
1500 and 1600 – tug of war is popularised during tournaments in French châteaux gardens and later in Great Britain
1800 – tug of war begins a new tradition among seafaring men who were required to tug on lines to adjust sails while ships were under way and even in battle.
The Mohave people occasionally used tug-of-war matches as means of settling disputes.
As a sport.
There are tug of war clubs in many countries, and both men and women participate.
The sport was part of the Olympic Games from 1900 until 1920, but has not been included since. The sport is part of the World Games. The Tug of War International Federation (TWIF), organises World Championships for nation teams biannually, for both indoor and outdoor contests, and a similar competition for club teams.
In England the sport is catered for by the Tug of War Association (formed in 1958), and the Tug of War Federation of Great Britain (formed in 1984). In Scotland, the Scottish Tug of War Association was formed in 1980. The sport also features in Highland Games there.
Between 1976 and 1988 Tug of War was a regular event during the television series "Battle of the Network Stars". Teams of celebrities representing each major network competed in different sporting events culminating into the final event, the Tug of War. Lou Ferrigno's epic tug-o'-war performance in May 1979 is considered the greatest feat in 'Battle' history.
National organizations.
The sport is played almost in every country in the world. However, a small selection of countries have set up a national body to govern the sport. Most of these national bodies are associated then with the International governing body call TWIF which stands for The Tug of War International Federation. As of 2008 there are 53 countries associated with TWIF, among which are Scotland, Ireland, England, India, Switzerland, Belgium, Italy and the United States.
Formal rules.
Two teams of eight, whose total mass must not exceed a maximum weight as determined for the class, align themselves at the end of a rope approximately in circumference. The rope is marked with a "centre line" and two markings either side of the centre line. The teams start with the rope's centre line directly above a line marked on the ground, and once the contest (the "pull") has commenced, attempt to pull the other team such that the marking on the rope closest to their opponent crosses the centre line, or the opponents commit a foul (such as a team member sitting or falling down).
Lowering ones elbow below the knee during a 'pull' - known as 'Locking' - is a foul, as is touching the ground for extended periods of time. The rope must go under the arms; actions such as pulling the rope over the shoulders may be considered a foul. These rules apply in highly organized competitions such as the World Championships. However, in small or informal entertainment competitions, the rules are often arbitrarily interpreted and followed.
A contest may feature a moat in a neutral zone, usually of mud or softened ground, which eliminates players who cross the zone or fall into it.
Tactics.
Aside from the raw muscle power needed for tug of war, it is also a technical sport. The cooperation or "rhythm" of team members play an equally important role in victory, if not more, than their physical strength. To achieve this, a person called a "driver" is used to harmonize the team's joint traction power. He moves up and down next to his team pulling on the rope, giving orders to them when to pull and when to rest (called "hanging"). If he spots the opponents trying to pull his team away, he gives a "hang" command, each member will dig into the grass with his/her boots and movement of the rope is limited. When the opponents are played out, he shouts "pull" and rhythmically waves his hat or handkerchief for his team to pull together. Slowly but surely, the other team is forced into surrender by a runaway pull.
Injury risks.
In addition to injuries from falling and from back strains (some of which may be serious), catastrophic injuries may occur, such as finger, hand, or even arm amputations. Amputations or avulsions may result from two causes: looping or wrapping the rope around a hand or wrist, and impact from elastic recoil if the rope breaks. Amateur organizers of tugs of war may underestimate the forces generated, or overestimate the breaking strength of common ropes, and may thus be unaware of the possible consequences if a rope snaps under extreme tension. The broken ends of a rope made with a somewhat elastic polymer such as common nylon can reach high speeds, and can easily sever fingers. For this reason, specially engineered tug of war ropes exist that can safely withstand the forces generated.
Some notable tug of war accidents:
1997 arm severing incident.
On October 25, 1997, Yang Chiung-ming and Chen Ming-kuo each had their left arms severed below the shoulder during a tug-of-war event in Taipei, Taiwan. The event, held at a park along the Keelung River in Taipei in celebration of Retrocession Day (the 52nd anniversary of the end of the Japanese colonial rule in Taiwan), involved over 1,600 participants whose combined strength exerted over of force on a nylon rope that could only withstand a maximum of . The rope snapped, and the sheer rebounding force of the broken rope tore off the men's arms. Both men were taken to a nearby hospital where their arms were successfully reattached following several hours of microsurgery.

</doc>
<doc id="25983" url="https://en.wikipedia.org/wiki?curid=25983" title="Regular semantics">
Regular semantics

Regular semantics is a computing term which describes one type of guarantee provided by a data register shared by several processors in a parallel machine or in a network of computers working together. Regular semantics are defined for a variable with a single writer but multiple readers. These semantics are stronger than safe semantics but weaker than atomic semantics: they guarantee that there is a total order to the write operations which is consistent with real-time and that read operations return either the value of the last write completed before the read begins, or that of one of the writes which are concurrent with the read.
Example.
Regular semantics are weaker than linearizability. 
Consider the example shown below, where the horizontal axis represents time and the arrows represent the interval during which a read or write operation takes place. According to a regular register's definition, the third read should return 3 since the read operation is not concurrent with any write operation. On the other hand, the second read may return 2 or 3, and the first read may return either 5 or 2. The first read could return 3 and the second read could return 2. This behavior would not satisfy atomic semantics. Therefore, regular semantics is a weaker property than an atomic semantics. On the other hand, Lamport proved that a linearizable register may be implemented from registers with safe semantics, which are weaker than regular registers.
A Theorem from Regularity to Atomicity.
A single-writer multi-reader (SWMR) atomic semantics is an SWMR regular register if any of its execution history H satisfies the following property:
r1 and r2 are any two read invocations: (r1 →H r2) ⇒ ¬π(r2) →H π(r1)
Before we get into the proof,first we should know what does the new/old inversion mean.As it shown in the picture below,by looking at the execution we can see that the only difference between a regular execution and an atomic execution is when a = 0 and b = 1.In this execution,when we consider the two read invocations R.read() → a 
followed by R.read() → b, our first value(new value) is a = 0 while the second value(old value) is b=1.This is actually the main difference between atomacity and regularity.
The theorem above states that a Single writer multi-reader regular register without new or old inversion
is an atomic register. By looking at the picture we can say that as R.read() → a →H R.read() → b
and R.write(1) →H R.write(0), it is not possible to have π (R.read() → b) =R.write(1) and π (R.read() → a) = R.write(0) if the execution is atomic. 
For proving the theorem above,first we should prove that the register is safe,next we should show that the register is regular,and then at the end we should show that the register does not allow for new/old inversion which proves the atomicity. 
By the definition of the atomic register we know that a Single writer multiple reader atomic register is regular and satisfies the no new/old
inversion property. So,we only need to show that a regular register with no new/old inversion is atomic.
We know that for any two read invocations (r1 and r2) when the register is regular and there is no new/old inversion 
(r1 →H r2) ⇒sn(π(r1)) ≤ sn(π(r2)). For any execution(M)there is a total order (S)which includes the same operation invocations.We can state that S is built as follow:
we start from the total order on the write operations and we will insert the read operation as follow:
first: Read operation(r) is inserted after the associated write operation(π(r)).Second: If two read operations (r1,r2) have the same (sn(r1)=sn(r2)) then first insert the operation which starts first in the execution.
S includes all the operation invocation of M, from which it follows that S and M are equivalent. Since all the operations are ordered based on their sequence number,S is slightly a total order. Furthermore,this total order is an execution of M only adds an order on operations that are overlapping in M. If there is no overlapping between a read and write operations, there is no difference between the regularity and atomicity. Finally, we can state that S is legal since each read operation gets the last written value that comes before it in the total order. Therefore, the corresponding history is linearizable. Since this reasoning does not rely on a particular
history H, it implies that the register is atomic. 
Since atomicity (linearizability) is a local property, we can state that a set of SWMR regular registers behave atomically as soon as each of them satisfies the no new/old inversion property.

</doc>
<doc id="25984" url="https://en.wikipedia.org/wiki?curid=25984" title="Ray Kurzweil">
Ray Kurzweil

Raymond "Ray" Kurzweil ( ; born February 12, 1948) is an American author, computer scientist, inventor and futurist. Aside from futurology, he is involved in fields such as optical character recognition (OCR), text-to-speech synthesis, speech recognition technology, and electronic keyboard instruments. He has written books on health, artificial intelligence (AI), transhumanism, the technological singularity, and futurism. Kurzweil is a public advocate for the futurist and transhumanist movements, and gives public talks to share his optimistic outlook on life extension technologies and the future of nanotechnology, robotics, and biotechnology.
Kurzweil was the principal inventor of the first charge-coupled device flatbed scanner, the first omni-font optical character recognition, the first print-to-speech reading machine for the blind, the first commercial text-to-speech synthesizer, the Kurzweil K250 music synthesizer capable of simulating the sound of the grand piano and other orchestral instruments, and the first commercially marketed large-vocabulary speech recognition.
Kurzweil received the 1999 National Medal of Technology and Innovation, the United States' highest honor in technology, from President Clinton in a White House ceremony. He was the recipient of the $500,000 Lemelson-MIT Prize for 2001, the world's largest for innovation. And in 2002 he was inducted into the National Inventors Hall of Fame, established by the U.S. Patent Office. He has received twenty honorary doctorates, and honors from three U.S. presidents. Kurzweil has been described as a "restless genius" by "The Wall Street Journal" and "the ultimate thinking machine" by "Forbes". PBS included Kurzweil as one of 16 "revolutionaries who made America" along with other inventors of the past two centuries. "Inc." magazine ranked him #8 among the "most fascinating" entrepreneurs in the United States and called him "Edison's rightful heir".
Kurzweil has authored seven books, five of which have been national bestsellers. "The Age of Spiritual Machines" has been translated into 9 languages and was the #1 best-selling book on Amazon in science. Kurzweil's book "The Singularity Is Near" was a "The New York Times" bestseller, and has been the #1 book on Amazon in both science and philosophy. Kurzweil speaks widely to audiences public and private and regularly delivers keynote speeches at industry conferences like DEMO, SXSW and TED. He maintains the news website KurzweilAI.net, which has over three million readers annually.
Life, inventions, and business career.
Early life.
Ray Kurzweil grew up in the New York City borough of Queens. He was born to secular Jewish parents who had emigrated from Austria just before the onset of World War II. He was exposed via Unitarian Universalism to a diversity of religious faiths during his upbringing. His Unitarian church had the philosophy of many paths to the truth – the religious education consisted of spending six months on a single religion before moving onto the next. Kurzweil is an agnostic. His father was a musician, a noted conductor, and a music educator. His mother was a visual artist. 
Kurzweil decided he wanted to be an inventor at the age of five. As a young boy, Kurzweil had an inventory of parts from various construction toys he’d been given and old electronic gadgets he’d collected from neighbors.In his youth, Kurzweil was an avid reader of science fiction literature. At the age of eight, nine, and ten, he read the entire Tom Swift Jr. series. At the age of seven or eight, he built a robotic puppet theater and robotic game. He was involved with computers by the age of twelve (in 1960), when only a dozen computers existed in all of New York City, and built computing devices and statistical programs for the predecessor of Head Start. At the age of fourteen, Kurzweil wrote a paper detailing his theory of the neocortex. His parents were involved with the arts, and he is quoted in the documentary "Transcendent Man" as saying that the household always produced discussions about the future and technology.
Kurzweil attended Martin Van Buren High School. During class, he often held onto his class textbooks to seemingly participate, but instead, focused on his own projects which were hidden behind the book. His uncle, an engineer at Bell Labs, taught young Kurzweil the basics of computer science. In 1963, at age fifteen, he wrote his first computer program. He created a pattern-recognition software program that analyzed the works of classical composers, and then synthesized its own songs in similar styles. In 1965, he was invited to appear on the CBS television program "I've Got a Secret", where he performed a piano piece that was composed by a computer he also had built. Later that year, he won first prize in the International Science Fair for the invention; Kurzweil's submission to Westinghouse Talent Search of his first computer program alongside several other projects resulted in him being one of its national winners, which allowed him to be personally congratulated by President Lyndon B. Johnson during a White House ceremony. These activities collectively impressed upon Kurzweil the belief that nearly any problem could be overcome.
Mid-life.
While in high school, Kurzweil had corresponded with Marvin Minsky and was invited to visit him at MIT, which he did. Kurzweil also visited Frank Rosenblatt at Cornell.
He obtained a B.S. in computer science and literature in 1970 at MIT. He went to MIT to study with Marvin Minsky. He took all of the computer programming courses (eight or nine) offered at MIT in the first year and a half.
In 1968, during his sophomore year at MIT, Kurzweil started a company that used a computer program to match high school students with colleges. The program, called the Select College Consulting Program, was designed by him and compared thousands of different criteria about each college with questionnaire answers submitted by each student applicant. Around this time, he sold the company to Harcourt, Brace & World for $100,000 (roughly $670,000 in 2013 dollars) plus royalties.
In 1974, Kurzweil founded Kurzweil Computer Products, Inc. and led development of the first omni-font optical character recognition system, a computer program capable of recognizing text written in any normal font. Before that time, scanners had only been able to read text written in a few fonts. He decided that the best application of this technology would be to create a reading machine, which would allow blind people to understand written text by having a computer read it to them aloud. However, this device required the invention of two enabling technologies—the CCD flatbed scanner and the text-to-speech synthesizer. Development of these technologies was completed at other institutions such as Bell Labs, and on January 13, 1976, the finished product was unveiled during a news conference headed by him and the leaders of the National Federation of the Blind. Called the Kurzweil Reading Machine, the device covered an entire tabletop.
Kurzweil's next major business venture began in 1978, when Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. LexisNexis was one of the first customers, and bought the program to upload paper legal and news documents onto its nascent online databases.
Kurzweil sold his Kurzweil Computer Products to Lernout & Hauspie. Following the legal and bankruptcy problems of the latter, the system became a subsidiary of Xerox later known as Scansoft and now as Nuance Communications, and he functioned as a consultant for the former until 1995.
Kurzweil's next business venture was in the realm of electronic music technology. After a 1982 meeting with Stevie Wonder, in which the latter lamented the divide in capabilities and qualities between electronic synthesizers and traditional musical instruments, Kurzweil was inspired to create a new generation of music synthesizers capable of accurately duplicating the sounds of real instruments. Kurzweil Music Systems was founded in the same year, and in 1984, the Kurzweil K250 was unveiled. The machine was capable of imitating a number of instruments, and in tests musicians were unable to discern the difference between the Kurzweil K250 on piano mode from a normal grand piano. The recording and mixing abilities of the machine, coupled with its abilities to imitate different instruments, made it possible for a single user to compose and play an entire orchestral piece.
Kurzweil Music Systems was sold to South Korean musical instrument manufacturer Young Chang in 1990. As with Xerox, Kurzweil remained as a consultant for several years. Hyundai acquired Young Chang in 2006 and in January 2007 appointed Raymond Kurzweil as Chief Strategy Officer of Kurzweil Music Systems.
Later life.
Concurrent with Kurzweil Music Systems, Kurzweil created the company Kurzweil Applied Intelligence (KAI) to develop computer speech recognition systems for commercial use. The first product, which debuted in 1987, was an early speech recognition program.
Kurzweil started Kurzweil Educational Systems in 1996 to develop new pattern-recognition-based computer technologies to help people with disabilities such as blindness, dyslexia and Attention-deficit hyperactivity disorder (ADHD) in school. Products include the Kurzweil 1000 text-to-speech converter software program, which enables a computer to read electronic and scanned text aloud to blind or visually impaired users, and the Kurzweil 3000 program, which is a multifaceted electronic learning system that helps with reading, writing, and study skills.
During the 1990s, Kurzweil founded the Medical Learning Company. The company's products included an interactive computer education program for doctors and a computer-simulated patient. Around the same time, Kurzweil started KurzweilCyberArt.com—a website featuring computer programs to assist the creative art process. The site used to offer free downloads of a program called AARON—a visual art synthesizer developed by Harold Cohen—and of "Kurzweil's Cybernetic Poet", which automatically creates poetry. During this period he also started KurzweilAI.net, a website devoted towards showcasing news of scientific developments, publicizing the ideas of high-tech thinkers and critics alike, and promoting futurist-related discussion among the general population through the Mind-X forum.
In 1999, Kurzweil created a hedge fund called "FatKat" (Financial Accelerating Transactions from Kurzweil Adaptive Technologies), which began trading in 2006. He has stated that the ultimate aim is to improve the performance of FatKat's A.I. investment software program, enhancing its ability to recognize patterns in "currency fluctuations and stock-ownership trends." He predicted in his 1999 book, "The Age of Spiritual Machines," that computers will one day prove superior to the best human financial minds at making profitable investment decisions.
In June 2005, Kurzweil introduced the "Kurzweil-National Federation of the Blind Reader" (K-NFB Reader)—a pocket-sized device consisting of a digital camera and computer unit. Like the Kurzweil Reading Machine of almost 30 years before, the K-NFB Reader is designed to aid blind people by reading written text aloud. The newer machine is portable and scans text through digital camera images, while the older machine is large and scans text through flatbed scanning.
In December 2012, Kurzweil was hired by Google in a full-time position to "work on new projects involving machine learning and language processing". He was personally hired by Google co-founder Larry Page. Larry Page and Kurzweil agreed on a one-sentence job description: "to bring natural language understanding to Google".
He received a Technical Grammy on February 8, 2015, recognizing his diverse technical and creative accomplishments. For purposes of the Grammy, perhaps most notable was the aforementioned Kurzweil K250.
Postmortem life.
Kurzweil has joined the Alcor Life Extension Foundation, a cryonics company. In the event of his declared death, Kurzweil plans to be perfused with cryoprotectants, vitrified in liquid nitrogen, and stored at an Alcor facility in the hope that future medical technology will be able to repair his tissues and revive him.
Personal life.
Kurzweil married Sonya Rosenwald Fenster in 1975 and has two children. Sonya Kurzweil is a psychologist in private practice and clinical instructor in Psychology at Harvard Medical School; she is interested in the way that digital media can be integrated into the lives of children and teens.
He has a son, Ethan Kurzweil, a venture capitalist, and a daughter, Amy Kurzweil, a writer and cartoonist.
Ray Kurzweil is a cousin of writer Allen Kurzweil.
Creative approach.
Kurzweil said "I realize that most inventions fail not because the R&D department can’t get them to work, but because the timing is wrongnot all of the enabling factors are at play where they are needed. Inventing is a lot like surfing: you have to anticipate and catch the wave at just the right moment."
For the past several decades, Kurzweil's most effective and common approach to doing creative work has been conducted during his lucid dreamlike state which immediately precedes his awakening state. He claims to have constructed inventions, solved difficult problems, such as algorithmic, business strategy, organizational, and interpersonal problems, and written speeches in this state.
Books.
Kurzweil's first book, "The Age of Intelligent Machines", was published in 1990. The nonfiction work discusses the history of computer artificial intelligence (AI) and forecasts future developments. Other experts in the field of AI contribute heavily to the work in the form of essays. The Association of American Publishers' awarded it the status of "Most Outstanding Computer Science Book" of 1990.
In 1993, Kurzweil published a book on nutrition called "The 10% Solution for a Healthy Life". The book's main idea is that high levels of fat intake are the cause of many health disorders common in the U.S., and thus that cutting fat consumption down to 10% of the total calories consumed would be optimal for most people.
In 1999, Kurzweil published "The Age of Spiritual Machines", which further elucidates his theories regarding the future of technology, which themselves stem from his analysis of long-term trends in biological and technological evolution. Much emphasis is on the likely course of AI development, along with the future of computer architecture.
Kurzweil's next book, published in 2004, returned to human health and nutrition. "" was co-authored by Terry Grossman, a medical doctor and specialist in alternative medicine.
"The Singularity Is Near", published in 2005, was made into a movie starring Pauley Perrette from NCIS. In February 2007, Ptolemaic Productions acquired the rights to "The Singularity is Near", "The Age of Spiritual Machines" and "Fantastic Voyage" including the rights to film Kurzweil's life and ideas for the documentary film "Transcendent Man", which was directed by Barry Ptolemy.
"Transcend: Nine Steps to Living Well Forever", a follow-up to "Fantastic Voyage", was released on April 28, 2009.
Kurzweil's book, "How to Create a Mind: The Secret of Human Thought Revealed," was released on Nov. 13, 2012. In it Kurzweil describes his Pattern Recognition Theory of Mind, the theory that the neocortex is a hierarchical system of pattern recognizers, and argues that duplicating this architecture in machines could lead to an artificial superintelligence. In a critical review of the book, philosopher Colin McGinn refers to "the hype so blatantly brandished in its title" and asks: "He is clearly a man of many parts—but is ultimate theoretician of the mind one of them?" McGinn calls Kurzweil's claim that pattern recognition is the key to mental phenomena "obviously false" and concludes that the book is "interesting in places, fairly readable, moderately informative, but wildly overstated".
Movies.
Kurzweil wrote and co-produced a movie directed by Anthony Waller, called "The Singularity Is Near: A True Story About the Future", in 2010 based, in part, on his 2005 book "The Singularity Is Near". Part fiction, part non-fiction, he interviews 20 big thinkers like Marvin Minsky, plus there is a B-line narrative story that illustrates some of the ideas, where a computer avatar (Ramona) saves the world from self-replicating microscopic robots. In addition to his movie, an independent, feature-length documentary was made about Kurzweil, his life, and his ideas, called "Transcendent Man". Filmmakers Barry Ptolemy and Felicia Ptolemy followed Kurzweil, documenting his global speaking-tour. Premiered in 2009 at the Tribeca Film Festival, "Transcendent Man" documents Kurzweil's quest to reveal mankind's ultimate destiny and explores many of the ideas found in his New York Times bestselling book, "The Singularity Is Near", including his concept exponential growth, radical life expansion, and how we will transcend our biology. The Ptolemys documented Kurzweil's stated goal of bringing back his late father using AI. The film also features critics who argue against Kurzweil's predictions.
In 2010, an independent documentary film called "Plug & Pray" premiered at the Seattle International Film Festival, in which Kurzweil and one of his major critics, the late Joseph Weizenbaum, argue about the benefits of eternal life.
The feature-length documentary film "The Singularity" by independent filmmaker Doug Wolens (released at the end of 2012), showcasing Kurzweil, has been acclaimed as "a large-scale achievement in its documentation of futurist and counter-futurist ideas” and “the best documentary on the Singularity to date."
Kurzweil frequently comments on the application of cell-size nanotechnology to the workings of the human brain and how this could be applied to building AI. While being interviewed for a February 2009 issue of "Rolling Stone" magazine, Kurzweil expressed a desire to construct a genetic copy of his late father, Fredric Kurzweil, from DNA within his grave site. This feat would be achieved by exhumation and extraction of DNA, constructing a clone of Fredric and retrieving memories and recollections—from Ray's mind—of his father. Kurzweil kept all of his father's records, notes, and pictures in order to maintain as much of his father as he could. Ray is known for taking over 200 pills a day, meant to reprogram his biochemistry. This, according to Ray, is only a precursor to the devices at the nano scale that will eventually replace a blood-cell, self updating of specific pathogens to improve the immune system.
Views.
The Law of Accelerating Returns.
In his 1999 book "The Age of Spiritual Machines", Kurzweil proposed "The Law of Accelerating Returns", according to which the rate of change in a wide variety of evolutionary systems (including the growth of technologies) tends to increase exponentially. He gave further focus to this issue in a 2001 essay entitled "The Law of Accelerating Returns", which proposed an extension of Moore's law to a wide variety of technologies, and used this to argue in favor of Vernor Vinge's concept of a technological singularity. Kurzweil suggests that this exponential technological growth is counter-intuitive to the way our brains perceive the world—since our brains were biologically inherited from humans living in a world that was linear and local—and, as a consequence, he claims it has encouraged great skepticism in his future projections.
Stance on the future of genetics, nanotechnology, and robotics.
Kurzweil is working with the Army Science Board to develop a rapid response system to deal with the possible abuse of biotechnology. He suggests that the same technologies that are empowering us to reprogram biology away from cancer and heart disease could be used by a bioterrorist to reprogram a biological virus to be more deadly, communicable, and stealthy. However, he suggests that we have the scientific tools to successfully defend against these attacks, similar to the way we defend against computer software viruses. He has testified before Congress on the subject of nanotechnology, advocating that nanotechnology has the potential to solve serious global problems such as poverty, disease, and climate change. "Nanotech Could Give Global Warming a Big Chill".
In media appearances, Kurzweil has stressed the extreme potential dangers of nanotechnology but argues that in practice, progress cannot be stopped because that would require a totalitarian system, and any attempt to do so would drive dangerous technologies underground and deprive responsible scientists of the tools needed for defense. He suggests that the proper place of regulation is to ensure that technological progress proceeds safely and quickly, but does not deprive the world of profound benefits. He stated, "To avoid dangers such as unrestrained nanobot replication, we need relinquishment at the right level and to place our highest priority on the continuing advance of defensive technologies, staying ahead of destructive technologies. An overall strategy should include a streamlined regulatory process, a global program of monitoring for unknown or evolving biological pathogens, temporary moratoriums, raising public awareness, international cooperation, software reconnaissance, and fostering values of liberty, tolerance, and respect for knowledge and diversity."
Health and aging.
Kurzweil admits that he cared little for his health until age 35, when he was found to suffer from a glucose intolerance, an early form of type II diabetes (a major risk factor for heart disease). Kurzweil then found a doctor (Terry Grossman, M.D.) who shares his somewhat unconventional beliefs to develop an extreme regimen involving hundreds of pills, chemical intravenous treatments, red wine, and various other methods to attempt to live longer. Kurzweil was ingesting "250 supplements, eight to 10 glasses of alkaline water and 10 cups of green tea" every day and drinking several glasses of red wine a week in an effort to "reprogram" his biochemistry. Lately, he has cut down the number of supplement pills to 150.
Kurzweil has made a number of bold claims for his health regimen. In his book The Singularity Is Near, he claimed that he brought his cholesterol level down from the high 200s to 130, raised his HDL (high-density lipoprotein) from below 30 to 55, and lowered his homocysteine from an unhealthy 11 to a much safer 6.2. He also claimed that his C-reactive protein "and all of my other indexes (for heart disease, diabetes, and other conditions) are at ideal levels." He further claimed that his health regimen, including dramatically reducing his fat intake, successfully "reversed" his type 2 diabetes. (The Singularity Is Near, p. 211)
He has authored three books on the subjects of nutrition, health, and immortality: "The 10% Solution for a Healthy Life", "" and "Transcend: Nine Steps to Living Well Forever". In all, he recommends that other people emulate his health practices to the best of their abilities. Kurzweil and his current "anti-aging" doctor, Terry Grossman, now have two websites promoting their first and second book.
Kurzweil asserts that in the future, everyone will live forever. In a 2013 interview, he said that in 15 years, medical technology could add more than a year to one's remaining life expectancy for each year that passes, and we could then "outrun our own deaths". He has been an extreme advocate of SENS Research Foundation for the successful defeating of aging, and has encouraged acts of donation to hasten their rejuvenation research.
Nassim Nicholas Taleb, Lebanese American essayist, scholar and statistician, criticized Kurzweil's approach of taking multiple pills to achieve longevity in his book "Antifragile".
Kurzweil's view of the human neocortex.
According to Kurzweil, technologists will be creating synthetic neocortexes based on the operating principles of the human neocortex with the primary purpose of extending our own neocortexes. He claims to believe that the neocortex of an adult human consists of approximately 300 million pattern recognizers. He draws on the commonly accepted belief that the primary anatomical difference between humans and other primates that allowed for superior intellectual abilities was the evolution of a larger neocortex. He claims that the six-layered neocortex deals with increasing abstraction from one layer to the next. He says that at the low levels, the neocortex may seem cold and mechanical because it can only make simple decisions, but at the higher levels of the hierarchy, the neocortex is likely to be dealing with concepts like being funny, being sexy, expressing a loving sentiment, creating a poem or understanding a poem, etc. He claims to believe that these higher levels of the human neocortex were the enabling factors to permit the human development of language, technology, art, and science. He stated, "If the quantitative improvement from primates to humans with the big forehead was the enabling factor to allow for language, technology, art, and science, what kind of qualitative leap can we make with another quantitative increase? Why not go from 300 million pattern recognizers to a billion?”
Encouraging futurism and transhumanism.
Kurzweil's standing as a futurist and transhumanist has led to his involvement in several singularity-themed organizations. In December 2004, Kurzweil joined the advisory board of the Machine Intelligence Research Institute. In October 2005, Kurzweil joined the scientific advisory board of the Lifeboat Foundation. On May 13, 2006, Kurzweil was the first speaker at the Singularity Summit at Stanford University in Palo Alto, California. In May 2013, Kurzweil was the keynote speaker at the 2013 proceeding of the Research, Innovation, Start-up and Employment (RISE) international conference in Seoul, Korea Republic.
In February 2009, Kurzweil, in collaboration with Google and the NASA Ames Research Center in Mountain View, California, announced the creation of the Singularity University training center for corporate executives and government officials. The University's self-described mission is to "assemble, educate and inspire a cadre of leaders who strive to understand and facilitate the development of exponentially advancing technologies and apply, focus and guide these tools to address humanity's grand challenges". Using Vernor Vinge's Singularity concept as a foundation, the university offered its first nine-week graduate program to 40 students in June, 2009.
Predictions.
Past predictions.
Kurzweil's first book, "The Age of Intelligent Machines", presented his ideas about the future. It was written from 1986 to 1989 and published in 1990. Building on Ithiel de Sola Pool's "Technologies of Freedom" (1983), Kurzweil claims to have forecast the demise of the Soviet Union due to new technologies such as cellular phones and fax machines disempowering authoritarian governments by removing state control over the flow of information. In the book, Kurzweil also extrapolated preexisting trends in the improvement of computer chess software performance to predict that computers would beat the best human players "by the year 2000". In May 1997, chess World Champion Garry Kasparov was defeated by IBM's Deep Blue computer in a well-publicized chess match.
Perhaps most significantly, Kurzweil foresaw the explosive growth in worldwide Internet use that began in the 1990s. At the time of the publication of "The Age of Intelligent Machines", there were only 2.6 million Internet users in the world, and the medium was unreliable, difficult to use, and deficient in content. He also stated that the Internet would explode not only in the number of users but in content as well, eventually granting users access "to international networks of libraries, data bases, and information services". Additionally, Kurzweil claims to have correctly foreseen that the preferred mode of Internet access would inevitably be through wireless systems, and he was also correct to estimate that the latter would become practical for widespread use in the early 21st century.
In October of 2010, Kurzweil released his report, How My Predictions Are Faring, in PDF format, which analyzes the predictions he made in his book "The Age of Intelligent Machines" (1990), "The Age of Spiritual Machines" (1999) and "The Singularity is Near" (2005). Of the 147 total predictions, Kurzweil claims that 115 were 'entirely correct', 12 were "essentially correct", and 17 were "partially correct", and only 3 were "wrong". Adding together the "entirely" and "essentially" correct, Kurzweil's claimed accuracy rate comes to 86%
Daniel Lyons, writing in "Newsweek" magazine, criticized Kurzweil for some of his predictions that turned out to be wrong, such as the economy continuing to boom from the 1998 dot-com through 2009, a US company having a market capitalization of more than $1 trillion, a supercomputer achieving 20 petaflops, speech recognition being in widespread use and cars that would drive themselves using sensors installed in highways; all by 2009. To the charge that a 20 petaflop supercomputer was not produced in the time he predicted, Kurzweil responded that he considers Google a giant supercomputer, and that it is indeed capable of 20 petaflops.
Kurzweil's predictions for 2009 were mostly inaccurate, claims "Forbes" magazine. For example, Kurzweil predicted, "The majority of text is created using continuous speech recognition." This is not the case.
Future predictions.
In 1999, Kurzweil published a second book titled "The Age of Spiritual Machines", which goes into more depth explaining his futurist ideas. The third and final part of the book is devoted to predictions over the coming century, from 2009 through 2099. In "The Singularity Is Near" he makes fewer concrete short-term predictions, but includes many longer-term visions.
He states that with radical life extension will come radical life enhancement. He says he is confident that within 10 years we will have the option to spend some of our time in 3D virtual environments that appear just as real as real reality, but these will not yet be made possible via direct interaction with our nervous system. "If you look at video games and how we went from pong to the virtual reality we have available today, it is highly likely that immortality in essence will be possible." He claims to know that 20 to 25 years from now, we will have millions of blood-cell sized devices, known as nanobots, inside our bodies fighting against diseases, improving our memory, and cognitive abilities. Kurzweil claims to know that a machine will pass the Turing test by 2029, and that around 2045, "the pace of change will be so astonishingly quick that we won't be able to keep up, unless we enhance our own intelligence by merging with the intelligent machines we are creating". Shortly after, Kurzweil claims to know that humans will be a hybrid of biological and non-biological intelligence that becomes increasingly dominated by its non-biological component. He stresses that "AI is not an intelligent invasion from Mars. These are brain extenders that we have created to expand our own mental reach. They are part of our civilization. They are part of who we are. So over the next few decades our human-machine civilization will become increasingly dominated by its non-biological component. In "Transcendent Man" Kurzweil states "We humans are going to start linking with each other and become a metaconnection we will all be connected and all be omnipresent, plugged into this global network that is connected to billions of people, and filled with data."
In 2008, Kurzweil said in an expert panel in the National Academy of Engineering that solar power will scale up to produce all the energy needs of Earth's people in 20 years. According to Kurzweil, we only need to capture 1 part in 10,000 of the energy from the Sun that hits Earth's surface to meet all of humanity's energy needs.
Reception.
Praise.
Kurzweil was referred to as "the ultimate thinking machine" by "Forbes" and as a "restless genius" by "The Wall Street Journal". PBS included Kurzweil as one of 16 "revolutionaries who made America" along with other inventors of the past two centuries. "Inc." magazine ranked him #8 among the "most fascinating" entrepreneurs in the United States and called him "Edison's rightful heir".
Kurzweil has received many awards and honors, including:
Criticism.
Kurzweil's ideas have generated criticism within the scientific community and in the media.
Although the idea of a technological singularity is a popular concept in science fiction, some authors such as Neal Stephenson and Bruce Sterling have voiced skepticism about its real-world plausibility. Sterling expressed his views on the singularity scenario in a talk at the Long Now Foundation entitled "The Singularity: Your Future as a Black Hole". Other prominent AI thinkers and computer scientists such as Daniel Dennett, Rodney Brooks, David Gelernter and Paul Allen also criticized Kurzweil's projections.
In the cover article of the December 2010 issue of "IEEE Spectrum", John Rennie criticizes Kurzweil for several predictions that failed to become manifest by the originally predicted date. "Therein lie the frustrations of Kurzweil's brand of tech punditry. On close examination, his clearest and most successful predictions often lack originality or profundity. And most of his predictions come with so many loopholes that they border on the unfalsifiable."
Bill Joy, cofounder of Sun Microsystems, agrees with Kurzweil's timeline of future progress, but thinks that technologies such as AI, nanotechnology and advanced biotechnology will create a dystopian world. Mitch Kapor, the founder of Lotus Development Corporation, has called the notion of a technological singularity "intelligent design for the IQ 140 people...This proposition that we're heading to this point at which everything is going to be just unimaginably different—it's fundamentally, in my view, driven by a religious impulse. And all of the frantic arm-waving can't obscure that fact for me."
Some critics have argued more strongly against Kurzweil and his ideas. Cognitive scientist Douglas Hofstadter has said of Kurzweil's and Hans Moravec's books: "It's an intimate mixture of rubbish and good ideas, and it's very hard to disentangle the two, because these are smart people; they're not stupid." Biologist P. Z. Myers has criticized Kurzweil's predictions as being based on "New Age spiritualism" rather than science and says that Kurzweil does not understand basic biology. VR pioneer Jaron Lanier has even described Kurzweil's ideas as "cybernetic totalism" and has outlined his views on the culture surrounding Kurzweil's predictions in an essay for Edge.org entitled "One Half of a Manifesto".
British philosopher John Gray argues that contemporary science is what magic was for ancient civilizations. It gives a sense of hope for those who are willing to do almost anything in order to achieve eternal life. He quotes Kurzweil's Singularity as another example of a trend which has almost always been present in the history of mankind.

</doc>
<doc id="25985" url="https://en.wikipedia.org/wiki?curid=25985" title="Rutherford scattering">
Rutherford scattering

Rutherford scattering is the elastic scattering of charged particles by the Coulomb interaction. It is a physical phenomenon explained by Ernest Rutherford in 1911 that led to the development of the planetary Rutherford model of the atom and eventually the Bohr model. It is now exploited by the materials analytical technique Rutherford backscattering. Rutherford scattering was first referred to as Coulomb scattering because it relies only upon static electric (Coulomb) forces, and the minimal distance between particles is set only by this potential. The classical Rutherford scattering of alpha particles against gold nuclei is an example of "elastic scattering" because the energy and velocity of the outgoing scattered particle is the same as that with which it began.
Rutherford also later analyzed inelastic scattering when he projected alpha particles against hydrogen nuclei (protons); however this latter process is not referred to as "Rutherford scattering", although Rutherford was first to observe it. At the end of such processes, non-Coulombic forces come into play. These forces, and also energy gained from the scattering particle by the lighter target, change the scattering results in fundamental ways which suggest structural information about the target. A similar process probed the insides of nuclei in the 1960s, and is called deep inelastic scattering.
The initial discovery was made by Hans Geiger and Ernest Marsden in 1909 when they performed the gold foil experiment in collaboration with Rutherford, in which they fired a beam of alpha particles (helium nuclei) at layers of gold leaf only a few atoms thick. At the time of the experiment, the atom was thought to be analogous to a plum pudding (as proposed by J. J. Thomson), with the negative charges (the plums) found throughout a positive sphere (the pudding). If the plum-pudding model were correct, the positive "pudding", being more spread out than in the current model of a concentrated nucleus, would not be able to exert such large columbic forces, and the alpha particles should only be deflected by small angles as they pass through.
However, the intriguing results showed that around 1 in 8000 alpha particles were deflected by very large angles (over 90°), while the rest passed straight through with little or no deflection. From this, Rutherford concluded that the majority of the mass was concentrated in a minute, positively charged region (the nucleus/ central charge) surrounded by electrons. When a (positive) alpha particle approached sufficiently close to the nucleus, it was repelled strongly enough to rebound at high angles. The small size of the nucleus explained the small number of alpha particles that were repelled in this way. Rutherford showed, using the method below, that the size of the nucleus was less than about 10−14 m (how "much" less than this size, Rutherford could not tell from this experiment alone; see more below on this problem of lowest possible size).
Derivation.
The differential cross section can be derived from the equations of motion for a particle interacting with a central potential. In general, the equations of motion describing two particles interacting under a central force can be decoupled into the center of mass and the motion of the particles relative to one another. For the case of light alpha particles scattering off heavy nuclei, as in the experiment performed by Rutherford, the reduced mass is essentially the mass of the alpha particle and the nucleus off of which it scatters is essentially stationary in the lab frame.
Substituting into the Binet equation yields the equation of trajectory
where formula_2, formula_3 is the speed at infinity, and formula_4 is the impact parameter.
The general solution of the above differential equation is
and the boundary condition is
If we choose
then the deflection angle Θ can be seen from solving formula_8 as
"b" can be solved to give
To find the scattering cross section from this result consider its definition
Since the scattering angle is uniquely determined for a given formula_12 and formula_13, the number of particles scattered into an angle between formula_14 and formula_15 must be the same as the number of particles with associated impact parameters between formula_4 and formula_17. For an incident intensity formula_18, this implies the following equality
For a radially symmetric scattering potential, as in the case of the Coulomb potential, formula_20, yielding the expression for the scattering cross section
Finally, plugging in the previously derived expression for the impact parameter formula_22 we find the Rutherford scattering cross section
Details of calculating maximal nuclear size.
For head on collisions between alpha particles and the nucleus, all the kinetic energy of the alpha particle is turned into potential energy and the particle is at rest. The distance from the centre of the alpha particle to the centre of the nucleus ("b") at this point is a maximum value for the radius, if it is evident from the experiment that the particles have not hit the nucleus.
Applying the inverse-square law between the charges on the alpha particle and nucleus, one can write:
Rearranging:
For an alpha particle:
Substituting these in gives the value of about 2.7×10−14 m. (The true radius is about 7.3×10−15 m.) The true radius of the nucleus is not recovered in these experiments because the alphas do not have enough energy to penetrate to more than 27 fm of the nuclear center, as noted, when the actual radius of gold is 7.3 fm. Rutherford realized this, and also realized that actual impact of the alphas on gold causing any force-deviation from that of the 1/r coulomb potential would change the "form" of his scattering curve at high scattering angles (the smallest impact parameters) from a hyperbola to something else. This was not seen, indicating that the gold had not been "hit" so that Rutherford also knew the gold nucleus (or the sum of the gold and alpha radii) was smaller than 27 fm (2.7×10−14 m)
Extension to situations with relativistic particles and target recoil.
The extension of Rutherford-type scattering to energy regions in which the incoming particle has spin and magnetic moment, and is traveling at relativistic energies, and there is enough momentum-transfer that the struck particle recoils with some of the incoming particle's energy (so the process is inelastic rather than elastic), is called Mott scattering.

</doc>
<doc id="25986" url="https://en.wikipedia.org/wiki?curid=25986" title="Robert Langlands">
Robert Langlands

Robert Phelan Langlands (; born October 6, 1936) is a Canadian mathematician. He is best known as the founder of the Langlands program, a vast web of conjectures and results connecting representation theory and automorphic forms to the study of Galois groups in number theory. He is an emeritus professor and occupies Albert Einstein's office at the Institute for Advanced Study in Princeton.
Career.
Langlands received an undergraduate degree from the University of British Columbia in 1957, and continued on there to receive an M. Sc. in 1958. He then went to Yale University where he received a Ph.D. in 1960. His academic positions since then include the years 1960–67 at Princeton University, ending up as Associate Professor, and the years 1967–72 at Yale University. He was a Miller Research Fellow at the University of California Berkeley from 1964-65. He was appointed Hermann Weyl Professor at the Institute for Advanced Study in 1972, becoming Professor Emeritus in January 2007.
Research.
His Ph.D. thesis was on the analytical theory of semigroups, but he soon moved into representation theory, adapting the methods of Harish-Chandra to the theory of automorphic forms. His first accomplishment in this field was a formula for the dimension of certain spaces of automorphic forms, in which particular types of Harish-Chandra's discrete series appeared.
He next constructed an analytical theory of Eisenstein series for reductive groups of rank greater than one, thus extending work of Maass, Roelcke and Selberg from the early 1950s for rank one groups such as . This amounted to describing in general terms the continuous spectra of arithmetic quotients, and showing that all automorphic forms arise in terms of cusp forms and the residues of Eisenstein series induced from cusp forms on smaller subgroups. As a first application, he proved the Weil conjecture on Tamagawa numbers for the large class of arbitrary simply connected Chevalley groups defined over the rational numbers. Previously this had been known only in a few isolated cases and for certain classical groups where it could be shown by induction.
As a second application of this work, he was able to show meromorphic continuation for a large class of -functions arising in the theory of automorphic forms, not previously known to have them. These occurred in the constant terms of Eisenstein series, and meromorphicity as well as a weak functional equation were a consequence of functional equations for Eisenstein series. This work led in turn, in the winter of 1966–67, to the now well known conjectures making up what is often called the Langlands program. Very roughly speaking, they propose a huge generalization of previously known examples of reciprocity, including (a) classical class field theory, in which characters of local and arithmetic abelian Galois groups are identified with characters of local multiplicative groups and the idele quotient group, respectively; (b) earlier results of Eichler and Shimura in which the Hasse–Weil zeta functions of arithmetic quotients of the upper half plane are identified with -functions occurring in Hecke's theory of holomorphic automorphic forms. These conjectures were first posed in relatively complete form in a famous letter to Weil, written in January 1967. It was in this letter that he introduced what has since become known as the -group and along with it, the notion of functoriality.
Functoriality, the -group, the rigorous introduction of adele groups, and the consequent application of the representation theory of reductive groups over local fields changed drastically the way research in automorphic forms was carried out. Langlands's introduction of (or in cases where others had done previous work, emphasis on) these notions broke up large and to some extent intractable problems into smaller and more manageable pieces. For example, they made the infinite-dimensional representation theory of reductive groups into a major field of mathematical activity.
Functoriality is the conjecture that automorphic forms on different groups should be related in terms of their -groups. As one example of this conjecture the letter to Weil raised the possibility of solving the well known conjecture of Emil Artin regarding the behaviour of Artin's -functions, a hope partly realized in Langlands' later work on base change. In its application to Artin's conjecture, functoriality associated to every -dimensional representation of a Galois group an automorphic representation of the adelic group of . In the theory of Shimura varieties it associates automorphic representations of other groups to certain -adic Galois representations as well.
The book by Hervé Jacquet and Langlands on presented a theory of automorphic forms for the general linear group , establishing among other things the Jacquet–Langlands correspondence showing that functoriality was capable of explaining very precisely how automorphic forms for related to those for quaternion algebras. This book applied the adelic trace formula for and quaternion algebras to do this. Subsequently James Arthur, a student of Langlands while he was at Yale, successfully developed the trace formula for groups of higher rank. This has become a major tool in attacking functoriality in general, and in particular has been applied to demonstrating that the Hasse–Weil zeta functions of certain Shimura varieties are among the -functions arising from automorphic forms.
The functoriality conjecture is far from proved, but a special case (the octahedral Artin conjecture, proved by Langlands and Tunnell) was the starting point of Andrew Wiles' attack on the Taniyama–Shimura conjecture and Fermat's last theorem.
In the mid-1980s Langlands turned his attention to physics, particularly the problems of percolation and conformal invariance.
In recent years he has turned his attention back to automorphic forms, working in particular on a theme he calls `beyond endoscopy'.
In 1995 Langlands started a collaboration with Bill Casselman at the University of British Columbia with the aim of posting nearly all of his writings—including publications, preprints, as well as selected correspondence—on the Internet. The correspondence includes a copy of the original letter to Weil that introduced the -group.
Awards and honors.
Langlands has received the 1996 Wolf Prize (which he shared with Andrew Wiles), the 2005 AMS Steele Prize, the 1980 Jeffery–Williams Prize, the 1988 NAS Award in Mathematics from the National Academy of Sciences, the 2006 Nemmers Prize in Mathematics, and the 2007 Shaw Prize in Mathematical Sciences (with Richard Taylor) for his work on automorphic forms.
He was elected a Fellow of the Royal Society of London in 1981. In 2012, he became a fellow of the American Mathematical Society.
Personal life.
Langlands spent a year in Turkey in 1967–68, where his office at the Middle East Technical University was next to that of Cahit Arf's. In addition to his mathematical studies, Langlands likes to learn foreign languages, both for better understanding of foreign publications on his topic and just as a hobby. He speaks Turkish, German and Russian.

</doc>
<doc id="25987" url="https://en.wikipedia.org/wiki?curid=25987" title="Rickets">
Rickets

Rickets is defective mineralization or calcification of bones before epiphyseal closure in immature mammals due to deficiency or impaired metabolism of vitamin D, phosphorus or calcium, potentially leading to fractures and deformity. Rickets is among the most frequent childhood diseases in many developing countries. The predominant cause is a vitamin D deficiency, but lack of adequate calcium in the diet may also lead to rickets (cases of severe diarrhea and vomiting may be the cause of the deficiency). Although it can occur in adults, the majority of cases occur in children suffering from severe malnutrition, usually resulting from famine or starvation during the early stages of childhood.
Osteomalacia is a similar condition occurring in adults, generally due to a deficiency of vitamin D but occurs after epiphyseal closure.
Signs and symptoms.
Signs and symptoms of rickets include:
An X-ray or radiograph of an advanced sufferer from rickets tends to present in a classic way: bow legs 
(outward curve of long bone of the legs) and a deformed chest. Changes in the skull also occur causing a distinctive "square headed" appearance (Caput Quadratum). These deformities persist into adult life if not treated. Long-term consequences include permanent bends or disfiguration of the long bones, and a curved back.
Cause.
The primary cause of congenital rickets is Vitamin D deficiency in the mother's blood, which the baby shares. Vitamin D ensures that serum phosphate and calcium levels are sufficient to facilitate the mineralization of bone Rickets in children is similar to osteoporosis in the elderly, with brittle bones. Pre-natal care includes checking vitamin levels and ensuring that any deficiencies are supplemented. 
The role of sunlight.
Sunlight, especially ultraviolet light, lets human skin cells convert vitamin D from an inactive to active state. In the absence of vitamin D, dietary calcium is not properly absorbed, resulting in hypocalcaemia, leading to skeletal and dental deformities and neuromuscular symptoms, e.g. hyperexcitability. Foods that contain vitamin D include butter, eggs, fish liver oils, margarine, fortified milk and juice, portabella and shiitake mushrooms, and oily fishes such as tuna, herring, and salmon. A rare X-linked dominant form exists called vitamin D-resistant rickets or X-linked hypophosphatemia.
Cases have been reported in Britain in recent years of rickets in children of many social backgrounds caused by insufficient production in the body of vitamin D because the sun's ultraviolet light was not reaching the skin due to use of strong sunblock, too much "covering up" in sunlight, or not getting out into the sun. Other cases have been reported among the children of some ethnic groups in which mothers avoid exposure to the sun for religious or cultural reasons, leading to a maternal shortage of vitamin D; and people with darker skins need more sunlight to maintain vitamin D levels. The "British Medical Journal" reported in 2010 that doctors in Newcastle on Tyne saw 20 cases of rickets per year. Rickets had been a significant malaise in London, especially during the Industrial Revolution. Persistent thick fog and heavy industrial smog permeating the city blocked out significant amounts of sunlight so much so that up to 80 percent of children at one time had varying degrees of rickets in one form or the other. Diseases causing soft bones in infants, like hypophosphatasia or hypophosphatemia can also lead to rickets.
Evolutionary considerations.
Vitamin D natural selection hypotheses:
Rickets is often a result of Vitamin D3 deficiency. The Vitamin D natural selection hypothesis suggests that Vitamin D production from sunlight is a selective force for human skin color variation. The correlation between human skin color and latitude is thought to be the result of positive selection to varying levels of solar ultraviolet radiation. Northern latitudes have selection for lighter skin that allows UV rays to produce Vitamin D from 7-dehydrocholesterol. Conversely, latitudes near the equator have selection for darker skin that can block the majority of UV radiation to protect from toxic levels of Vitamin D, as well as skin cancer.
An anecdote often cited to support this hypothesis is that Arctic populations whose skin is relatively darker for their latitude, such as the Inuit, have a diet that is historically rich in vitamin D. Since these people acquire Vitamin D through their diet, there is not a positive selective force to synthesize Vitamin D from sunlight.
Environment mismatch:
Ultimately, Vitamin D deficiency arises from a mismatch between a populations previous evolutionary environment and the individual’s current environment. This risk of mismatch increases with advances in transportation methods and increases in urban population size at high latitudes.
Similar to the environmental mismatch when dark-skinned people live at high latitudes, Rickets can also occur in religious communities that require long garments with hoods and veils. These hoods and veils act as sunlight barriers that prevent individuals from synthesizing Vitamin D naturally from the sun.
In a study by Mithal et al., Vitamin D insufficiency of various countries was measured by lower 25-hydroxyvitamin D. 25(OH)D is an indicator of vitamin D insufficiency that can be easily measured. These percentages should be regarded as relative Vitamin D levels, and not as predicting evidence for development of rickets.
Asian immigrants living in Europe have an increased risk for Vitamin D deficiency. Vitamin D insufficiency was found in 40% of non-Western immigrants in the Netherlands, and in more than 80% of Turkish and Moroccan immigrants.
The Middle East, despite high rates of sun-exposure, has the highest rates of rickets worldwide . This can be explained by limited sun exposure due to cultural practices and lack of vitamin D supplementation for breast-feeding women. Up to 70% and 80% of adolescent girls in Iran and Saudi Arabia, respectively, have Vitamin D insufficiency. Socioeconomic factors that limit a Vitamin D rich diet also plays a role. 
In the United States, Vitamin D insufficiency varies dramatically by ethnicity. Among males aged 70 years and older, the prevalence of low serum 25(OH) D levels was 23% for non-Hispanic whites, 45% for Mexican Americans, and 58% for non-Hispanic blacks. Among women, the prevalence was 28.5%, 55%, and 68%, respectively.
With this evolutionary perspective in mind, parents can supplement their nutritional intake with vitamin D enhanced beverages if they feel their child is at risk for Vitamin D deficiency, or, with more assurance of benefit and no cost , enable the child to spend more time with some of their skin receiving the summer sun's rays.
Diagnosis.
Rickets may be diagnosed with the help of:
Differential diagnosis.
Infants with rickets often have bone fractures. This sometimes leads to child abuse allegations. This issue appears to be more common for solely nursing infants of black mothers, in winter in temperate climates, suffering poor nutrition and no vitamin D supplementation. People with darker skin produce less vitamin D than those with lighter skin, for the same amount of sunlight.
Treatment and prevention.
The most common treatment of rickets is the use of Vitamin D. However, surgery may be required to remove severe bone abnormalities.
Diet and sunlight.
Treatment involves increasing dietary intake of calcium, phosphates and vitamin D. Exposure to ultraviolet B light (most easily obtained when the sun is highest in the sky), cod liver oil, halibut-liver oil, and viosterol are all sources of vitamin D.
A sufficient amount of ultraviolet B light in sunlight each day and adequate supplies of calcium and phosphorus in the diet can prevent rickets. Darker-skinned people need to be exposed longer to the ultraviolet rays. The replacement of vitamin D has been proven to correct rickets using these methods of ultraviolet light therapy and medicine.
Recommendations are for 400 international units (IU) of vitamin D a day for infants and children. Children who do not get adequate amounts of vitamin D are at increased risk of rickets. Vitamin D is essential for allowing the body to uptake calcium for use in proper bone calcification and maintenance.
Supplementation.
Sufficient vitamin D levels can also be achieved through dietary supplementation and/or exposure to sunlight. Vitamin D3 (cholecalciferol) is the preferred form since it is more readily absorbed than vitamin D2. Most dermatologists recommend vitamin D supplementation as an alternative to unprotected ultraviolet exposure due to the increased risk of skin cancer associated with sun exposure. Endogenous production with full body exposure to sunlight is approximately 250 µg (10,000 IU) per day.
According to the American Academy of Pediatrics (AAP), all infants, including those who are exclusively breast-fed, may need Vitamin D supplementation until they start drinking at least of vitamin D-fortified milk or formula a day.
Epidemiology.
In developed countries, rickets is a rare disease (incidence of less than 1 in 200,000). Recently, cases of rickets have been reported among children who are fed plant-based milk substitutes and not given supplimental vitamin D.
Those at higher risk for developing rickets include:
History.
Greek physician Soranus of Ephesus, one of the chief representatives of the Methodic school of medicine who practiced in Alexandria and subsequently in Rome, reported deformation of the bones in infants as early as the first and second centuries AD. Rickets was not defined as a specific medical condition until 1645, when an English physician Daniel Whistler gave the earliest known description of the disease. In 1650 a treatise on rickets was published by Francis Glisson, a physician at Caius College, Cambridge. German pediatrician Kurt Huldschinsky successfully demonstrated in the winter of 1918–1919 how rickets could be treated with ultraviolet lamps. In 1923, American physician Harry Steenbock demonstrated that irradiation by ultraviolet light increased the vitamin D content of foods and other organic materials. Steenbock's irradiation technique was used for food stuffs, but most memorably for milk. By 1945, rickets had all but been eliminated in the United States, no cases have been recorded in Cuba, but they are there since the Special Period.
Etymology.
The word "rickets" may be from the Old English word "wrickken" ('to twist'), although because this is conjectured, several major dictionaries simply say "origin unknown". The name "rickets" is plural in form but usually singular in construction. The Greek word "rachitis" (ῥαχίτης, meaning "in or of the spine") was later adopted as the scientific term for rickets, due chiefly to the words' similarity in sound.

</doc>
<doc id="25989" url="https://en.wikipedia.org/wiki?curid=25989" title="RGB color model">
RGB color model

The RGB color model is an additive color model in which red, green, and blue light are added together in various ways to reproduce a broad array of colors. The name of the model comes from the initials of the three additive primary colors, red, green, and blue.
The main purpose of the RGB color model is for the sensing, representation, and display of images in electronic systems, such as televisions and computers, though it has also been used in conventional photography. Before the electronic age, the RGB color model already had a solid theory behind it, based in human perception of colors.
RGB is a "device-dependent" color model: different devices detect or reproduce a given RGB value differently, since the color elements (such as phosphors or dyes) and their response to the individual R, G, and B levels vary from manufacturer to manufacturer, or even in the same device over time. Thus an RGB value does not define the same "color" across devices without some kind of color management.
Typical RGB input devices are color TV and video cameras, image scanners, video games, and digital cameras. Typical RGB output devices are TV sets of various technologies (CRT, LCD, plasma, OLED, Quantum-Dots etc.), computer and mobile phone displays, video projectors, multicolor LED displays, and large screens such as JumboTron. Color printers, on the other hand, are not RGB devices, but subtractive color devices (typically CMYK color model).
This article discusses concepts common to all the different color spaces that use the RGB color model, which are used in one implementation or another in color image-producing technology.
Additive primary colors.
To form a color with RGB, three light beams (one red, one green, and one blue) must be superimposed (for example by emission from a black screen, or by reflection from a white screen). Each of the three beams is called a "component" of that color, and each of them can have an arbitrary intensity, from fully off to fully on, in the mixture.
The RGB color model is "additive" in the sense that the three light beams are added together, and their light spectra add, wavelength for wavelength, to make the final color's spectrum.
Zero intensity for each component gives the darkest color (no light, considered the "black"), and full intensity of each gives a white; the "quality" of this white depends on the nature of the primary light sources, but if they are properly balanced, the result is a neutral white matching the system's white point. When the intensities for all the components are the same, the result is a shade of gray, darker or lighter depending on the intensity. When the intensities are different, the result is a colorized hue, more or less saturated depending on the difference of the strongest and weakest of the intensities of the primary colors employed.
When one of the components has the strongest intensity, the color is a hue near this primary color (reddish, greenish, or bluish), and when two components have the same strongest intensity, then the color is a hue of a secondary color (a shade of cyan, magenta or yellow). A secondary color is formed by the sum of two primary colors of equal intensity: cyan is green+blue, magenta is red+blue, and yellow is red+green. Every secondary color is the "complement" of one primary color; when a primary and its complementary secondary color are added together, the result is white: cyan complements red, magenta complements green, and yellow complements blue.
The RGB color model itself does not define what is meant by "red", "green", and "blue" colorimetrically, and so the results of mixing them are not specified as absolute, but relative to the primary colors. When the exact chromaticities of the red, green, and blue primaries are defined, the color model then becomes an absolute color space, such as sRGB or Adobe RGB; see RGB color spaces for more details.
Physical principles for the choice of red, green, and blue.
The choice of primary colors is related to the physiology of the human eye; good primaries are stimuli that maximize the difference between the responses of the cone cells of the human retina to light of different wavelengths, and that thereby make a large color triangle.
The normal three kinds of light-sensitive photoreceptor cells in the human eye (cone cells) respond most to yellow (long wavelength or L), green (medium or M), and violet (short or S) light (peak wavelengths near 570 nm, 540 nm and 440 nm, respectively). The difference in the signals received from the three kinds allows the brain to differentiate a wide gamut of different colors, while being most sensitive (overall) to yellowish-green light and to differences between hues in the green-to-orange region.
As an example, suppose that light in the orange range of wavelengths (approximately 577 nm to 597 nm) enters the eye and strikes the retina. Light of these wavelengths would activate both the medium and long wavelength cones of the retina, but not equally—the long-wavelength cells will respond more. The difference in the response can be detected by the brain, and this difference is the basis of our perception of orange. Thus, the orange appearance of an object results from light from the object entering our eye and stimulating the different cones simultaneously but to different degrees.
Use of the three primary colors is not sufficient to reproduce "all" colors; only colors within the color triangle defined by the chromaticities of the primaries can be reproduced by additive mixing of non-negative amounts of those colors of light.
History of RGB color model theory and usage.
The RGB color model is based on the Young–Helmholtz theory of trichromatic color vision, developed by Thomas Young and Hermann Helmholtz in the early to mid nineteenth century, and on James Clerk Maxwell's color triangle that elaborated that theory (circa 1860).
Photography.
The first experiments with RGB in early color photography were made in 1861 by Maxwell himself, and involved the process of combining three color-filtered separate takes. To reproduce the color photograph, three matching projections over a screen in a dark room were necessary.
The additive RGB model and variants such as orange–green–violet were also used in the Autochrome Lumière color plates and other screen-plate technologies such as the Joly color screen and the Paget process in the early twentieth century. Color photography by taking three separate plates was used by other pioneers, such as the Russian Sergey Prokudin-Gorsky in the period 1909 through 1915. Such methods lasted until about 1960 using the expensive and extremely complex tri-color carbro Autotype process.
When employed, the reproduction of prints from three-plate photos was done by dyes or pigments using the complementary CMY model, by simply using the negative plates of the filtered takes: reverse red gives the cyan plate, and so on.
Television.
Before the development of practical electronic TV, there were patents on mechanically scanned color systems as early as 1889 in Russia. The color TV pioneer John Logie Baird demonstrated the world's first RGB color transmission in 1928, and also the world's first color broadcast in 1938, in London. In his experiments, scanning and display were done mechanically by spinning colorized wheels.
The Columbia Broadcasting System (CBS) began an experimental RGB field-sequential color system in 1940. Images were scanned electrically, but the system still used a moving part: the transparent RGB color wheel rotating at above 1,200 rpm in synchronism with the vertical scan. The camera and the cathode-ray tube (CRT) were both monochromatic. Color was provided by color wheels in the camera and the receiver.
More recently, color wheels have been used in field-sequential projection TV receivers based on the Texas Instruments monochrome DLP imager.
The modern RGB shadow mask technology for color CRT displays was patented by Werner Flechsig in Germany in 1938.
Personal computers.
Early personal computers of the late 1970s and early 1980s, such as those from Apple, Atari and Commodore, did not use RGB as their main method to manage colors, but rather composite video. IBM introduced a 16-color scheme (four bits—one bit each for red, green, blue, and intensity) with the Color Graphics Adapter (CGA) for its first IBM PC (1981), later improved with the Enhanced Graphics Adapter (EGA) in 1984. The first manufacturer of a truecolor graphic card for PCs (the TARGA) was Truevision in 1987, but it was not until the arrival of the Video Graphics Array (VGA) in 1987 that RGB became popular, mainly due to the analog signals in the connection between the adapter and the monitor which allowed a very wide range of RGB colors. Actually, it had to wait a few more years because the original VGA cards were palette-driven just like EGA, although with more freedom than VGA, but because the VGA connectors were analogue, later variants of VGA (made by various manufacturers under the informal name Super VGA) eventually added truecolor. In 1992, magazines heavily advertised truecolor Super VGA hardware.
RGB devices.
RGB and displays.
One common application of the RGB color model is the display of colors on a cathode ray tube (CRT), liquid crystal display (LCD), plasma display, or organic light emitting diode (OLED) display such as a television, a computer’s monitor, or a large scale screen. Each pixel on the screen is built by driving three small and very close but still separated RGB light sources. At common viewing distance, the separate sources are indistinguishable, which tricks the eye to see a given solid color. All the pixels together arranged in the rectangular screen surface conforms the color image.
During digital image processing each pixel can be represented in the computer memory or interface hardware (for example, a "graphics card") as binary values for the red, green, and blue color components. When properly managed, these values are converted into intensities or voltages via gamma correction to correct the inherent nonlinearity of some devices, such that the intended intensities are reproduced on the display.
The Quattron released by Sharp uses RGB color and adds yellow as a sub-pixel, supposedly allowing an increase in the number of available colors.
Video electronics.
RGB is also the term referring to a type of component video signal used in the video electronics industry. It consists of three signals—red, green, and blue—carried on three separate cables/pins. RGB signal formats are often based on modified versions of the RS-170 and RS-343 standards for monochrome video. This type of video signal is widely used in Europe since it is the best quality signal that can be carried on the standard SCART connector. This signal is known as RGBS (4 BNC/RCA terminated cables exist as well), but it is directly compatible with RGBHV used for computer monitors (usually carried on 15-pin cables terminated with 15-pin D-sub or 5 BNC connectors), which carries separate horizontal and vertical sync signals. 
Outside Europe, RGB is not very popular as a video signal format; S-Video takes that spot in most non-European regions. However, almost all computer monitors around the world use RGB.
Video framebuffer.
A framebuffer is a digital device for computers which stores data in the so-called "video memory" (comprising an array of Video RAM or similar chips). This data goes either to three digital-to-analog converters (DACs) (for analog monitors), one per primary color, or directly to digital monitors. Driven by software, the CPU (or other specialized chips) write the appropriate bytes into the video memory to define the image. Modern systems encode pixel color values by devoting eight bits to each of the R, G, and B components. RGB information can be either carried directly by the pixel bits themselves, or provided by a separate color look-up table (CLUT) if indexed color graphic modes are used.
A CLUT is a specialized RAM that stores R, G, and B values that define specific colors. Each color has its own address (index)—consider it as a descriptive reference number that provides that specific color when the image needs it. The content of the CLUT is much like a palette of colors. Image data that uses indexed color specifies addresses within the CLUT to provide the required R, G, and B values for each specific pixel, one pixel at a time. Of course, before displaying, the CLUT has to be loaded with R, G, and B values that define the palette of colors required for each image to be rendered. Some video applications store such palettes in PAL files (Microsoft AOE game, for example uses over half-a-dozen) and can combine CLUTs on screen.
This indirect scheme restricts the number of available colors in an image CLUT —typically 256-cubed (8 bits in three color channels with values of 0–255)— although each color in the RGB24 CLUT table has only 8 bits representing 256 codes for each of the R, G, and B primaries combinatorial math theory says this means that any given color can be one of 16,777,216 possible colors. However, the advantage is that an indexed-color image file can be significantly smaller than it would be with only 8 bits per pixel for each primary. 
Modern storage, however, is far less costly, greatly reducing the need to minimize image file size. By using an appropriate combination of red, green, and blue intensities, many colors can be displayed. Current typical display adapters use up to 24-bits of information for each pixel: 8-bit per component multiplied by three components (see the Digital representations section below (24bits = 2563, each primary value of 8 bits with values of 0–255). With this system, 16,777,216 (2563 or 224) discrete combinations of R, G and B values are allowed, providing millions of different (though not necessarily distinguishable) hue, saturation, and lightness shades. Increased shading has been implemented in various ways, some formats such as .png and .tga files among others using a fourth greyscale color channel as a masking layer, often called RGB32.
For images with a modest range of brightnesses from the darkest to the lightest, eight bits per primary color provides good-quality images, but extreme images require more bits per primary color as well as advanced display technology. For more information see High Dynamic Range (HDR) imaging.
Nonlinearity.
In classic cathode ray tube (CRT) devices, the brightness of a given point over the fluorescent screen due to the impact of accelerated electrons is not proportional to the voltages applied to the electron gun control grids, but to an expansive function of that voltage. The amount of this deviation is known as its gamma value (formula_1), the argument for a power law function, which closely describes this behaviour. A linear response is given by a gamma value of 1.0, but actual CRT nonlinearities have a gamma value around 2.0 to 2.5.
Similarly, the intensity of the output on TV and computer display devices is not directly proportional to the R, G, and B applied electric signals (or file data values which drive them through Digital-to-Analog Converters). On a typical standard 2.2-gamma CRT display, an input intensity RGB value of (0.5, 0.5, 0.5) only outputs about 22% of full brightness (1.0, 1.0, 1.0), instead of 50%. To obtain the correct response, a gamma correction is used in encoding the image data, and possibly further corrections as part of the color calibration process of the device. Gamma affects black-and-white TV as well as color. In standard color TV, broadcast signals are gamma corrected.
RGB and cameras.
In color television and video cameras manufactured before the 1990s, the incoming light was separated by prisms and filters into the three RGB primary colors feeding each color into a separate video camera tube (or "pickup tube"). These tubes are a type of cathode ray tube, not to be confused with that of CRT displays.
With the arrival of commercially viable charge-coupled device (CCD) technology in the 1980s, first the pickup tubes were replaced with this kind of sensors. Later, higher scale integration electronics was applied (mainly by Sony), simplifying and even removing the intermediate optics, thereby reducing the size of home video cameras and eventually leading to the development of full camcorders. Current webcams and mobile phones with cameras are the most miniaturized commercial forms of such technology.
Photographic digital cameras that use a CMOS or CCD image sensor often operate with some variation of the RGB model. In a Bayer filter arrangement, green is given twice as many detectors as red and blue (ratio 1:2:1) in order to achieve higher luminance resolution than chrominance resolution. The sensor has a grid of red, green, and blue detectors arranged so that the first row is RGRGRGRG, the next is GBGBGBGB, and that sequence is repeated in subsequent rows. For every channel, missing pixels are obtained by interpolation in the demosaicing process to build up the complete image. Also, other processes used to be applied in order to map the camera RGB measurements into a standard RGB color space as sRGB.
RGB and scanners.
In computing, an image scanner is a device that optically scans images (printed text, handwriting, or an object) and converts it to a digital image which is transferred to a computer. Among other formats, flat, drum, and film scanners exist, and most of them support RGB color. They can be considered the successors of early telephotography input devices, which were able to send consecutive scan lines as analog amplitude modulation signals through standard telephonic lines to appropriate receivers; such systems were in use in press since the 1920s to the mid-1990s. Color telephotographs were sent as three separated RGB filtered images consecutively.
Currently available scanners typically use charge-coupled device (CCD) or contact image sensor (CIS) as the image sensor, whereas older drum scanners use a photomultiplier tube as the image sensor. Early color film scanners used a halogen lamp and a three-color filter wheel, so three exposures were needed to scan a single color image. Due to heating problems, the worst of them being the potential destruction of the scanned film, this technology was later replaced by non-heating light sources such as color LEDs.
Numeric representations.
A color in the RGB color model is described by indicating how much of each of the red, green, and blue is included. The color is expressed as an RGB triplet ("r","g","b"), each component of which can vary from zero to a defined maximum value. If all the components are at zero the result is black; if all are at maximum, the result is the brightest representable white.
These ranges may be quantified in several different ways:
For example, brightest saturated red is written in the different RGB notations as:
In many environments, the component values within the ranges are not managed as linear (that is, the numbers are nonlinearly related to the intensities that they represent), as in digital cameras and TV broadcasting and receiving due to gamma correction, for example. Linear and nonlinear transformations are often dealt with via digital image processing. Representations with only 8 bits per component are considered sufficient if gamma encoding is used.
Following is the mathematical relationship between RGB space to HSI space:
formula_2
Color depth.
The RGB color model is one of the most common ways to encode color in computing, and several different binary digital representations are in use. The main characteristic of all of them is the quantization of the possible values per component (technically a "Sample (signal)" ) by using only integer numbers within some range, usually from 0 to some power of two minus one (2"n" – 1) to fit them into some bit groupings. Encodings of 1, 2, 4, 5, 8, and 16 bits per color are commonly found; the total number of bits used for an RGB color is typically called the color depth.
Geometric representation.
Since colors are usually defined by three components, not only in the RGB model, but also in other color models such as CIELAB and Y'UV, among others, then a three-dimensional volume is described by treating the component values as ordinary cartesian coordinates in a euclidean space. For the RGB model, this is represented by a cube using non-negative values within a 0–1 range, assigning black to the origin at the vertex (0, 0, 0), and with increasing intensity values running along the three axes up to white at the vertex (1, 1, 1), diagonally opposite black.
An RGB triplet ("r","g","b") represents the three-dimensional coordinate of the point of the given color within the cube or its faces or along its edges. This approach allows computations of the color similarity of two given RGB colors by simply calculating the distance between them: the shorter the distance, the higher the similarity. Out-of-gamut computations can also be performed this way.
Colors in web-page design.
The RGB color model for HTML was formally adopted as an Internet standard in HTML 3.2, though it had been in use for some time before that. Initially, the limited color depth of most video hardware led to a limited color palette of 216 RGB colors, defined by the Netscape Color Cube. With the predominance of 24-bit displays, the use of the full 16.7 million colors of the HTML RGB color code no longer poses problems for most viewers.
The web-safe color palette consists of the 216 (63) combinations of red, green, and blue where each color can take one of six values (in hexadecimal): #00, #33, #66, #99, #CC or #FF (based on the 0 to 255 range for each value discussed above). These hexadecimal values = 0, 51, 102, 153, 204, 255 in decimal, which = 0%, 20%, 40%, 60%, 80%, 100% in terms of intensity. This seems fine for splitting up 216 colors into a cube of dimension 6. However, lacking gamma correction, the perceived intensity on a standard 2.5 gamma CRT / LCD is only: 0%, 2%, 10%, 28%, 57%, 100%. See the actual web safe color palette for a visual confirmation that the majority of the colors produced are very dark, or see Xona.com Color List for a side by side comparison of proper colors next to their equivalent lacking proper gamma correction.
The syntax in CSS is:
where # equals the proportion of red, green and blue respectively. This syntax can be used after such selectors as "background-color:" or (for text) "color:".
Color management.
Proper reproduction of colors, especially in professional environments, requires color management of all the devices involved in the production process, many of them using RGB. Color management results in several transparent conversions between device-independent and device-dependent color spaces (RGB and others, as CMYK for color printing) during a typical production cycle, in order to ensure color consistency throughout the process. Along with the creative processing, such interventions on digital images can damage the color accuracy and image detail, especially where the gamut is reduced. Professional digital devices and software tools allow for 48 bpp (bits per pixel) images to be manipulated (16 bits per channel), to minimize any such damage.
ICC-compliant applications, such as Adobe Photoshop, use either the Lab color space or the CIE 1931 color space as a "Profile Connection Space" when translating between color spaces.
RGB model and luminance–chrominance formats relationship.
All luminance–chrominance formats used in the different TV and video standards such as YIQ for NTSC, YUV for PAL, YDBDR for SECAM, and YPBPR for component video use color difference signals, by which RGB color images can be encoded for broadcasting/recording and later decoded into RGB again to display them. These intermediate formats were needed for compatibility with pre-existent black-and-white TV formats. Also, those color difference signals need lower data bandwidth compared to full RGB signals.
Similarly, current high-efficiency digital color image data compression schemes such as JPEG and MPEG store RGB color internally in YCBCR format, a digital luminance-chrominance format based on YPBPR. The use of YCBCR also allows to perform lossy subsampling with the chroma channels (typically to 4:2:2 or 4:1:1 ratios), which it aids to reduce the resultant file size.

</doc>
<doc id="25991" url="https://en.wikipedia.org/wiki?curid=25991" title="Richard Garfield">
Richard Garfield

Richard Channing Garfield (born June 26, 1963, Philadelphia) is an American game designer. Garfield created ', which is considered to be the first modern collectible card game (CCG). "Magic" debuted in 1993, and its success spawned many imitations. Garfield oversaw the successful growth of "Magic" and followed it with other game designs. Garfield also designed the CCGs "Netrunner", "BattleTech", ' (originally known as "Jyhad"), "Star Wars Trading Card Game", the card game "The Great Dalmuti", and the board game "RoboRally". He also created a variation of the game "Hearts" called "Complex hearts". Garfield first became passionate about games when he played the roleplaying game "Dungeons & Dragons", so he designed Magic decks to be customizable like roleplaying characters. Garfield and "Magic "are in the Adventure Gaming Hall of Fame.
Early life and family.
Garfield was born in Philadelphia, and spent his childhood in many locations throughout the world as a result of his father's work in architecture. His family eventually settled in Oregon when he was twelve. Garfield is the great-great-grandson of U.S. President James A. Garfield, and his great-uncle invented the paper clip. He's also the nephew of Fay Jones, who, already an established artist, illustrated one Magic card for him.
While always having an interest in puzzles and games, his passion for games began when he was introduced to "Dungeons & Dragons".
Garfield designed his first game when he was 13.
Education and career.
In 1985, he received a Bachelor of Science degree in computer mathematics. He joined Bell Laboratories, then decided to continue his education and attended the University of Pennsylvania, and studied combinatorial mathematics.
While searching for a publisher for "RoboRally", which he designed in 1985, Wizards of the Coast began talking to Garfield through Mike Davis, but the game looked too expensive for a new company like Wizards to produce. Peter Adkison of Wizards of the Coast expressed interest in a fast-playing game with minimal equipment, something that would be popular at a game convention. Adkison asked Garfield to develop a game that was cheaper to produce than "RoboRally", that might be more portable and even easy to carry around to conventions; Garfield did have an idea about combining baseball cards with a card game and began turning that rough idea into a complete game over the next week. Garfield built on older prototypes of games that dated back to at least 1982, when he had created a "Cosmic Encounter"-inspired card game called "Five Magics". Garfield thus combined ideas from two previous games to invent the first trading card game, "". At first, Garfield and Adkison called the game "Manaclash", and worked on the game during Palladium's lawsuit against Wizards, protecting the game's IP under a shell company called Garfield Games. Garfield began designing "Magic" as a Penn graduate student. A group of playtesters, comprising mostly fellow Penn students, formed around the developing game.
Garfield studied under Herbert Wilf and earned a Ph.D. in combinatorial mathematics from Penn in 1993. His thesis was "The distribution of the binomial coefficients modulo p". He became a professor of mathematics at Whitman College in Walla Walla, Washington.
"Magic: The Gathering" launched in 1993. Playtesters began independently developing expansion packs, which were then passed to Garfield to edit. Garfield left academia to join Wizards of the Coast as a full-time game designer in June 1994. Garfield managed the hit game wisely, balancing player experience with business needs and allowing other designers to contribute creatively to the game. With his direction, Wizards established a robust tournament system for "Magic", something that was new to hobby gaming.
Wizards finally published Garfield's "RoboRally" in 1994. Wizards published Garfield's '-based CCF "Jyhad" in 1994, but changed the name to ' in 1995 to avoid offending Muslims. "Netrunner" (1996) was Garfield's CCG based on "Cyberpunk 2020", where he included an element that made it an entirely asymmetrical game, with the two players having different cards, abilities, and goals. Wizards published the "BattleTech Collectible Card Game" (1996), based on Garfield's design. Peter Adkison was developing a "Dungeons & Dragons" MMORPG based on a design from Garfield and Skaff Elias, but left Wizards in December 2000 after Hasbro sold the "D&D" computer rights and cancelled the project.
In 1999, Garfield was inducted into the Adventure Gaming Hall of Fame, as was "Magic". He was a primary play tester for the "Dungeons & Dragons" 3rd edition bookset, released by Wizards in 2000. He eventually left Wizards to become an independent game designer.
He still sporadically contributes to "". More recently, he has created the board games "Pecking Order" (2006) and "Rocketville" (2006). The latter was published by Avalon Hill, a subsidiary of Wizards of the Coast. He has shifted more of his attention to video games, having worked on the design and development of "Schizoid" and "Spectromancer". He has been a game designer and consultant for companies including Electronic Arts and Microsoft.
Garfield teaches a class titled "The Characteristics of Games" at the University of Washington.
Games designed.
A partial list of games designed by Garfield:

</doc>
<doc id="25994" url="https://en.wikipedia.org/wiki?curid=25994" title="Roman legion">
Roman legion

A Roman legion (from Latin "legio" "military levy, conscription", from "legere" "to choose") normally indicates the basic ancient Roman army unit recruited specifically from Roman citizens.
In reference to the early Roman Kingdom (as opposed to the Roman Republic or Empire), "the legion" means the entire Roman army. The subsequent organization of legions varied greatly over time but they were typically composed of around five thousand soldiers, divided into three lines of ten maniples during the republic, and later into ten cohorts during the early empire. Legions also included a small cavalry unit. By the third century, the legion was a much smaller unit of about 1,000 to 1,500 men, and there were more of them. In the fourth century, East Roman border guard legions ("limitanei") may have become even smaller.
For most of the Roman Imperial period, the legions formed the Roman army's elite heavy infantry, recruited exclusively from Roman citizens, while the remainder of the army consisted of auxiliaries, who provided additional infantry and the vast majority of the Roman army's cavalry. (Provincials who aspired to citizenship gained it when honorably discharged from the auxiliaries). The Roman army, for most of the Imperial period, consisted mostly of auxiliaries rather than legions.
Twelve of the legions founded before Common Era were still active until at least the fifth century, notably Legio V Macedonica, which was founded by Augustus in 43 BC and was in Egypt in the seventh century during the Islamic conquest of Egypt.
Overview of typical organization & strength.
Because legions were not standing units until the Marian reforms (c. 107 BC), and were instead created, used, and disbanded again, several hundred legions were named and numbered throughout Roman history. To date, about 50 have been identified. The Republican Legions were composed of levied men that paid for their own equipment and thus the structure of the Roman army at this time reflected the society, and at any time there would be four Consular Legions (with command divided between the two ruling Consuls) and in time of war extra Legions could be levied. Toward the end of the 2nd Century BC, Rome started to experience manpower shortages brought about by property and financial qualifications to join the army. This prompted Consul Gaius Marius to remove property qualifications and decree that all citizens, regardless of their wealth or social class, were made eligible for service in the Roman army with equipment and rewards for fulfilling years of service provided by the state. The Roman army became a volunteer, professional and standing army which extended service beyond Roman citizens but also to non-citizens that could sign on as "auxillia" (Auxiliaries) and were rewarded Roman citizenship upon completion of service and all the rights and privileges that entailed. In the time of Augustus, there were nearly 50 upon his succession but this was reduced to about 25–35 permanent standing legions and this remained the figure for most of the Empire's history.
A legion consisted of several cohorts of heavy infantry known as legionaries. It was almost always accompanied by one or more attached units of auxiliaries, who were not Roman citizens and provided cavalry, ranged troops and skirmishers to complement the legion's heavy infantry. The recruitment of non-citizens was rare but appears to have occurred in times of great need; For example, Caesar appears to have recruited the Legio V Alaudae mostly from non-citizen Gauls.
The size of a typical legion varied throughout the history of ancient Rome, with complements of 4,200 legionaries and 300 equites (drawn from the wealthier classes – in early Rome all troops provided their own equipment) in the republican period of Rome, (the infantry were split into 10 cohorts each of four maniples of 120 legionaries), to 5,200 men plus 120 auxiliaries in the imperial period (split into 10 cohorts, nine of 480 men each, plus the first cohort holding 800 men).
History.
Roman kings (to c.500 BC).
In the period before the raising of the "legio" and the early years of the Roman Kingdom and the Republic, forces are described as being organized into "centuries" of roughly one hundred men. These centuries were grouped together as required and answered to the leader who had hired or raised them. Such independent organization persisted until the 2nd century BC amongst light infantry and cavalry, but was discarded completely in later periods with the supporting role taken instead by allied troops. The roles of century leader (later formalised as a centurion), second in command and standard bearer are referenced in this early period.
Much Roman history of the era is shrouded in legend, but it is believed that during the reign of Servius Tullius, the census (from Latin: "censeō" – accounting of the people) was introduced. With this all Roman able-bodied, property-owning male citizens were divided into five classes for military service based on their wealth and then organised into centuries as sub-units of the greater Roman army or "legio" (multitude). Joining the army was both a duty and a distinguishing mark of Roman citizenship; during the entire pre-Marian period the wealthiest land owners performed the most years of military service. These individuals would have had the most to lose should the state have fallen.
The first and wealthiest common class was armed in the fashion of the hoplite with spear, sword, helmet, breast plate and round shield (called "clipeus" in Latin, similar to the Greek "aspis", also called "hoplon"); there were 82 centuries of these of which two were trumpeters. Roman soldiers had to purchase their own equipment. The second and third class also acted as spearmen but were less heavily armoured and carried a larger oval or rectangular shield. The fourth class could afford no armour; perhaps bearing a small shield and armed with spear and javelin. All three of the latter classes made up about 26 centuries. The fifth and final class was composed only of slingers. There were 32 centuries raised from this class, two of which were designated engineers. The army officers as well as the cavalry were drawn from leading citizens who enrolled as equestrians ("equites"). The equites were later placed in smaller groups of 30 that were commanded by decurions (which means commander of ten). There were 18 centuries of equites.
Until the 4th century BC, the massive Greek phalanx was the mode of battle. Roman soldiers would have thus looked much like Greek "hoplites". Tactics were no different from those of the early Greeks and battles were joined on flat terrain. Spearmen would deploy in tightly packed rows to form a shield wall with their spears pointing forwards. They charged the enemy supported by javelin throwers and slingers; the cavalry pursued the enemy, sometimes dismounting to support infantry in dire situations. The phalanx was a cumbersome military unit to manoeuvre and was easily defeated by mountain tribes, such as the Volsci or Samnites, in rough terrain.
Early civilian authorities called "praetors" doubled as military leaders during the summer war season. A declaration of war included a religious ceremony ending with the throwing of a ceremonial javelin into the enemy's territory to mark the start of hostilities.
Roman Republic (509–107 BC).
At some point, possibly in the beginning of the Roman Republic after the kings were overthrown, the "legio" was subdivided into two separate legions, each one ascribed to one of the two consuls. In the first years of the Republic, when warfare was mostly concentrated on raiding, it is uncertain if the full manpower of the legions was summoned at any one time. In 494 BC, when three foreign threats emerged, the dictator Manius Valerius Maximus raised ten legions which Livy says was a greater number than had been raised previously at any one time.
Also, some warfare was still conducted by Roman forces outside the legionary structure, the most famous example being the campaign in 479 BC by the clan army of gens Fabia against the Etruscan city of Veii (in which the clan was annihilated). Legions became more formally organized in the 4th century BC, as Roman warfare evolved to more frequent and planned operations, and the consular army was raised to two legions each.
In the Republic, legions had an ephemeral existence. Except for Legio I to IV, which were the consular armies (two per consul), other units were levied by campaign. Rome's Italian allies were required to provide approximately ten cohorts (auxilia were not organized into legions) to support each Roman Legion.
In the middle of the Republic, legions were composed of the following units:
Each of these three lines was subdivided into (usually 10) chief tactical units called maniples. A maniple consisted of two centuries and was commanded by the senior of the two centurions. At this time, each century of hastati and principes consisted of 60 men; a century of triarii was 30 men. These 3,000 men (twenty maniples of 120 men, and ten maniples of 60 men), together with about 1,200 velites and 300 cavalry gave the mid Republican ("manipular") legion a nominal strength of about 4,500 men.
Late Republic (107–30 BC).
"See also List of Roman legions for details of notable late Republican legions"<br>
"See also Sub-Units of the Roman legion"
The Marian reforms (of Gaius Marius) enlarged the centuries to 80 men, and grouped them into six-century "cohorts" (rather than two-century maniples). Each century had its own standard and was made up of ten units ("contubernia") of eight men who shared a tent, a millstone, a mule and cooking pot.
Following the reforms of the general Marius in the 2nd century BC, the legions took on the second, narrower meaning that is familiar in the popular imagination as close-order citizen heavy infantry.
At the end of the 2nd century BC, Gaius Marius reformed the previously ephemeral legions as a professional force drawing from the poorest classes, enabling Rome to field larger armies and providing employment for jobless citizens of the city of Rome. However, this put the loyalty of the soldiers in the hands of their general rather than the State of Rome itself. This development ultimately enabled Julius Caesar to cross the Rubicon with an army loyal to him personally and effectively end the Republic.
The legions of the late Republic and early Empire are often called "Marian" legions. Following the Battle of Vercellae in 101 BC, Marius granted all Italian soldiers Roman citizenship. He justified this action to the Senate by saying that in the din of battle he could not distinguish Roman from ally. This effectively eliminated the notion of allied legions; henceforth all Italian legions would be regarded as Roman legions, and full Roman citizenship was open to all the regions of Italy. At the same time, the three different types of heavy infantry were replaced by a single, standard type based on the "Principes": armed with two heavy javelins called "pila" (singular "pilum"), the short sword called "gladius", chain mail ("lorica hamata"), helmet and rectangular shield ("scutum").
The role of allied legions would eventually be taken up by contingents of allied auxiliary troops, called "Auxilia". "Auxilia" contained specialist units, engineers and pioneers, artillerymen and craftsmen, service and support personnel and irregular units made up of non-citizens, mercenaries and local militia. These were usually formed into complete units such as light cavalry, light infantry or "velites", and labourers. There was also a reconnaissance squad of 10 or more light mounted infantry called "speculatores" who could also serve as messengers or even as an early form of military intelligence service.
As part of the Marian reforms, the legions' internal organization was standardized. Each legion was divided into "cohorts". Prior to this, cohorts had been temporary administrative units or tactical task forces of several maniples, even more transitory than the legions themselves. Now the cohorts were ten permanent units, composed of 6 centuries and in the case of the first cohort 12 centuries each led by a centurion assisted by an "optio". The cohorts came to form the basic tactical unit of the legions. Ranking within the legion was based on length of service, with the senior Centurion commanding the first century of the first cohort; he was called the "primus pilus" (First Spear), and reported directly to the superior officers (legates and tribuni). All career soldiers could be promoted to the higher ranks in recognition of exceptional acts of bravery or valour. A newly promoted junior Centurion would be assigned to the sixth century of the tenth cohort and slowly progressed through the ranks from there.
Every legion had a large baggage train, which included 640 mules (1 mule for every 8 legionaries) just for the soldiers' equipment. To keep these baggage trains from becoming too large and slow, Marius had each infantryman carry as much of his own equipment as he could, including his own armour, weapons and 15 days' rations, for about 25–30 kg (50–60 pounds) of load total. To make this easier, he issued each legionary a cross stick to carry their loads on their shoulders. The soldiers were nicknamed "Marius' Mules" because of the amount of gear they had to carry themselves. This arrangement allowed for the possibility for the supply train to become temporarily detached from the main body of the legion, thus greatly increasing the army's speed when needed.
A typical legion of this period had 5,120 legionaries as well as a large number of camp followers, servants and slaves. Legions could contain as many as 6,000 fighting men when including the auxiliaries, although much later in Roman history the number was reduced to 1,040 to allow for greater mobility. Numbers would also vary depending on casualties suffered during a campaign; Julius Caesar's legions during his campaign in Gaul often only had around 3,500 men.
Tactics were not very different from the past, but their effectiveness was largely improved because of the professional training of the soldiers.
After the Marian reforms, and throughout the history of Rome's Late Republic, the legions played an important political role. By the 1st century BC, the threat of the legions under a demagogue was recognized. Governors were not allowed to leave their provinces with their legions. When Julius Caesar broke this rule, leaving his province of Gaul and crossing the Rubicon into Italy, he precipitated a constitutional crisis. This crisis and the civil wars which followed brought an end to the Republic and led to the foundation of the Empire under Augustus in 27 BC.
Early Empire (27 BC–AD 200).
"See also Directory of Roman legions of the early Empire"<br>
"See also Sub-Units of the Roman legion"
Generals, during the recent Republican civil wars, had formed their own legions and numbered them as they wished. During this time, there was a high incidence of "Gemina" (twin) legions, where two legions were consolidated into a single organization (and was later made official and put under a legatus and six duces). At the end of the civil war against Mark Antony, Augustus was left with around fifty legions, with several double counts (multiple Legio Xs for instance). For political and economic reasons, Augustus reduced the number of legions to 28 (which diminished to 25 after the Battle of Teutoburg Forest, in which 3 legions were completely destroyed by the Germanics).
Beside streamlining the army, Augustus also regulated the soldiers' pay. At the same time, he greatly increased the number of auxiliaries to the point where they were equal in number to the legionaries. He also created the Praetorian Guard along with a permanent navy where served the "liberti", or freed slaves.
Augustus' military policies proved sound and cost effective, and were generally followed by his successors. These emperors would carefully add new legions, as circumstances required or permitted, until the strength of the standing army stood at around 30 legions (hence the wry remark of the philosopher Favorinus that "It is ill arguing with the master of 30 legions"). With each legion having 5,120 legionaries usually supported by an equal number of auxiliary troops, the total force available to a legion commander during the Pax Romana probably ranged from 11,000 downwards, with the more prestigious legions and those stationed on hostile borders or in restive provinces tending to have more auxiliaries. Some legions may have even been reinforced at times with units making the associated force near 15,000–16,000 or about the size of a modern division.
Throughout the imperial era, the legions played an important political role. Their actions could secure the empire for a usurper or take it away. For example, the defeat of Vitellius in the Year of the Four Emperors was decided when the Danubian legions chose to support Vespasian.
In the empire, the legion was standardized, with symbols and an individual history where men were proud to serve. The legion was commanded by a "legatus" or "legate". Aged around thirty, he would usually be a senator on a three-year appointment. Immediately subordinate to the legate would be six elected "military tribunes" — five would be staff officers and the remaining one would be a noble heading for the Senate (originally this tribune commanded the legion). There would also be a group of officers for the medical staff, the engineers, record-keepers, the "praefectus castrorum" (commander of the camp) and other specialists such as priests and musicians.
Late Empire (from 200).
In the Later Roman Empire, the number of legions was increased and the Roman Army expanded. There is no evidence to suggest that legions changed in form before the Tetrarchy, although there is evidence that they were smaller than the paper strengths usually quoted. The final form of the legion originated with the elite "legiones palatinae" created by Diocletian and the Tetrarchs. These were infantry units of around 1,000 men rather than the 5,000, including cavalry, of the old Legions. The earliest "legiones palatinae" were the "Lanciarii", "Joviani", "Herculiani" and "Divitenses".
The 4th century saw a very large number of new, small legions created, a process which began under Constantine II. In addition to the elite "palatini", other legions called "comitatenses" and "pseudocomitatenses", along with the "auxilia palatina", provided the infantry of late Roman armies. The Notitia Dignitatum lists 25 "legiones palatinae", 70 "legiones comitatenses", 47 "legiones pseudocomitatenses" and 111 "auxilia palatina" in the field armies, and a further 47 "legiones" in the frontier armies. Legion names such as "Honoriani" and "Gratianenses" found in the Notitia suggest that the process of creating new legions continued through the 4th century rather than being a single event. The names also suggest that many new legions were formed from "vexillationes" or from old legions. In addition, there were 24 vexillationes palatini, 73 vexillationes comitatenses; 305 other units in the Eastern limitanei and 181 in the Western limitanei.
According to the late Roman writer Vegetius' "De Re Militari", each century had a ballista and each cohort had an onager, giving the legion a formidable siege train of 59 Ballistae and 10 Onagers, each manned by 10 "libritors" (artillerymen) and mounted on wagons drawn by oxen or mules. In addition to attacking cities and fortifications, these would be used to help defend Roman forts and fortified camps (castra) as well. They would even be employed on occasion, especially in the later Empire, as field artillery during battles or in support of river crossings.
Despite a number of reforms, the Legion system survived the fall of the Western Roman Empire, and was continued in the Eastern Roman Empire until around 7th century, when reforms begun by Emperor Heraclius to counter the increasing need for soldiers around the Empire resulted in the Theme system. Despite this, the Eastern Roman/Byzantine armies continued to be influenced by the earlier Roman legions, and were maintained with similar level of discipline, strategic prowess, and organization.
Legionary ranks.
Aside from the rank and file legionary (who received the base wage of 10 asses a day or 225 denarii a year), the following list describes the system of officers which developed within the legions from the Marian reforms (104 BC) until the military reforms of Diocletian (c. 290).
Centurions.
The rank of centurion was an officer grade that included many ranks, meaning centurions had very good prospects for promotion. The most senior centurion in a legion was known as the "primus pilus" (first file or spear), who directly commanded the first century of the first cohort and commanded the whole first cohort when in battle. Within the second to tenth cohorts, the commander of each cohort's first century was known as a "pilus prior" and was in command of his entire cohort when in battle. The seniority of the pilus prior centurions was followed by the five other century commanders of the first cohort, who were known as "primi ordines".
In modern military terms, an ordinary centurion was approximately equivalent to a Warrant Officer that had a junior officer's commission, whereas the most senior centurion was closer to the equivalent to the rank of a full Captain.
The equestrian, or military tribunes held positions equivalent to the rank of Major, while the Senatorial Tribune and the Camp Prefect were the equivalent of a Lt. Colonel.
The Legion Legate was the equivalent of full Brigadier with the Imperial Legate holding the rank of General.
The six centuries of a normal cohort, were, in order of precedence:
The centuries took their titles from the old use of the legion drawn up in three lines of battle using three classes of soldier. (Each century would then hold a cross-section of this theoretical line, although these century titles were now essentially nominal.) Each of the three lines is then sub-divided within the century into a more forward and a more rear century.
Pay.
From the time of Gaius Marius onwards, legionaries received 225 "denarii" a year (equal to 900 "Sestertii"); this basic rate remained unchanged until Domitian, who increased it to 300 denarii. In spite of the steady inflation during the 2nd century, there was no further rise until the time of Septimius Severus, who increased it to 500 denarii a year. However, the soldiers did not receive all the money in cash, as the state deducted their pay with a clothing and food tax. To this wage, a legionary on active campaign would hope to add the booty of war, from the bodies of their enemies and as plunder from enemy settlements. Slaves could also be claimed from the prisoners of war and divided amongst the legion for later selling, which would bring in a sizeable supplement to their regular pay.
All legionary soldiers would also receive a "praemia" on the completion of their term of service: a sizeable sum of money (3000 denarii from the time of Augustus) and/or a plot of good farmland (good land was in much demand); farmland given to veterans often helped in establishing control of the frontier regions and over rebellious provinces. Later, under Caracalla, the "praemia" increased to 5,000 denarii.
Symbols.
From 104 BC onwards, each legion used an aquila (eagle) as its standard symbol. The symbol was carried by an officer known as aquilifer, and its loss was considered to be a very serious embarrassment, and often led to the disbanding of the legion itself. Normally, this was because any legion incapable of regaining its eagle in battle was so severely mauled that it was no longer effective in combat.
In "Gallic War" (Bk IV, Para. 25), Julius Caesar describes an incident at the start of his first invasion of Britain in 55 BC that illustrated how fear for the safety of the eagle could drive Roman soldiers. When Caesar's troops hesitated to leave their ships for fear of the Britons, the aquilifer of the tenth legion threw himself overboard and, carrying the eagle, advanced alone against the enemy. His comrades, fearing disgrace, 'with one accord, leapt down from the ship' and were followed by troops from the other ships.
With the birth of the Roman Empire, the legions created a bond with their leader, the emperor himself. Each legion had another officer, called imaginifer, whose role was to carry a pike with the "imago" (image, sculpture) of the emperor as "pontifex maximus".
Each legion, furthermore, had a "vexillifer" who carried a "vexillum" or "signum", with the legion name and emblem depicted on it, unique to the legion. It was common for a legion to detach some sub-units from the main camp to strengthen other corps. In these cases, the detached subunits carried only the vexillum, and not the aquila, and were called, therefore, "vexillationes". A miniature vexillum, mounted on a silver base, was sometimes awarded to officers as a recognition of their service upon retirement or reassignment.
Civilians could also be rewarded for their assistance to the Roman legions. In return for outstanding service, a citizen was given an arrow without a head. This was considered a great honour and would bring the recipient much prestige.
Discipline.
The military discipline of the legions was quite harsh. Regulations were strictly enforced, and a broad array of punishments could be inflicted upon a legionary who broke them. Many legionaries became devotees in the cult of the minor goddess Disciplina, whose virtues of frugality, severity and loyalty were central to their code of conduct and way of life.
Factors in the legion's success.
Examples of ideas that were copied and adapted include weapons like the gladius (Iberians) and warship design (cf. Carthaginians' quinquereme), as well as military units, such as heavy mounted cavalry and mounted archers (Parthians and Numidians).

</doc>
<doc id="25995" url="https://en.wikipedia.org/wiki?curid=25995" title="Reciprocating engine">
Reciprocating engine

[[Image:Four stroke engine diagram.jpg|thumbnail|right|Internal combustion piston engine <br>Components of a typical, four stroke cycle, internal combustion piston engine. <br>
E - Exhaust camshaft<br>
I - Intake camshaft<br>
S - Spark plug<br>
V - Valves<br>
P - Piston<br>
R - Connecting rod<br>
C - Crankshaft<br>
W - Water jacket for coolant flow]]
A reciprocating engine, also often known as a piston engine, is a heat engine (usually, although there are also pneumatic and hydraulic reciprocating engines) that uses one or more reciprocating pistons to convert pressure into a rotating motion. This article describes the common features of all types. The main types are: the internal combustion engine, used extensively in motor vehicles; the steam engine, the mainstay of the Industrial Revolution; and the niche application Stirling engine. Internal Combustion engines are further classified in two ways: either a spark-ignition (SI) engine, where the spark plug initiates the combustion; or a compression-ignition (CI) engine, where the air within the cylinder is compressed, thus heating it, so that the heated air ignites fuel that is injected then or earlier.
Common features in all types.
There may be one or more pistons. Each piston is inside a cylinder, into which a gas is introduced, either already under pressure (e.g. steam engine), or heated inside the cylinder either by ignition of a fuel air mixture (internal combustion engine) or by contact with a hot heat exchanger in the cylinder (Stirling engine). The hot gases expand, pushing the piston to the bottom of the cylinder. This position is also known as the Bottom Dead Center (BDC), or where the piston forms the largest volume in the cylinder. The piston is returned to the cylinder top (Top Dead Centre) (TDC) by a flywheel, the power from other pistons connected to the same shaft or (in a double acting cylinder) by the same process acting on the other side of the piston. This is where the piston forms the smallest volume in the cylinder. In most types the expanded or "exhausted" gases are removed from the cylinder by this stroke. The exception is the Stirling engine, which repeatedly heats and cools the same sealed quantity of gas. The stroke is simply the distance between the TDC and the BDC, or the greatest distance that the piston can travel in one direction.
In some designs the piston may be powered in both directions in the cylinder, in which case it is said to be double-acting.
In most types, the linear movement of the piston is converted to a rotating movement via a connecting rod and a crankshaft or by a swashplate or other suitable mechanism. A flywheel is often used to ensure smooth rotation or to store energy to carry the engine through an un-powered part of the cycle. The more cylinders a reciprocating engine has, generally, the more vibration-free (smoothly) it can operate. The power of a reciprocating engine is proportional to the volume of the combined pistons' displacement.
A seal must be made between the sliding piston and the walls of the cylinder so that the high pressure gas above the piston does not leak past it and reduce the efficiency of the engine. This seal is usually provided by one or more piston rings. These are rings made of a hard metal, and are sprung into a circular groove in the piston head. The rings fit tightly in the groove and press against the cylinder wall to form a seal.
It is common to classify such engines by the number and alignment of cylinders and total volume of displacement of gas by the pistons moving in the cylinders usually measured in cubic centimetres (cm³ or cc) or litres (l) or (L) (US: liter). For example, for internal combustion engines, single and two-cylinder designs are common in smaller vehicles such as motorcycles, while automobiles typically have between four and eight, and locomotives, and ships may have a dozen cylinders or more. Cylinder capacities may range from 10 cm³ or less in model engines up to several thousand cubic centimetres in ships' engines.
The compression ratio affects the performance in most types of reciprocating engine. It is the ratio between the volume of the cylinder, when the piston is at the bottom of its stroke, and the volume when the piston is at the top of its stroke.
The bore/stroke ratio is the ratio of the diameter of the piston, or "bore", to the length of travel within the cylinder, or "stroke". If this is around 1 the engine is said to be "square", if it is greater than 1, i.e. the bore is larger than the stroke, it is "oversquare". If it is less than 1, i.e. the stroke is larger than the bore, it is "undersquare".
Cylinders may be aligned in line, in a V configuration, horizontally opposite each other, or radially around the crankshaft. Opposed-piston engines put two pistons working at opposite ends of the same cylinder and this has been extended into triangular arrangements such as the Napier Deltic. Some designs have set the cylinders in motion around the shaft, such as the Rotary engine.
[[Image:BetaStirlingTG4web.svg|thumb|250px|Stirling piston engine <br>Rhombic Drive – Beta Stirling Engine Design, showing the second displacer piston (green) within the cylinder, which shunts the working gas between the hot and cold ends, but produces no power itself.<br>
Pink – Hot cylinder wall<br>
Dark grey – Cold cylinder wall <br>
Green – Displacer piston<br>
Dark blue – Power piston <br>
Light blue – Flywheels ]]
In steam engines and internal combustion engines, valves are required to allow the entry and exit of gasses at the correct times in the piston's cycle. These are worked by cams, eccentrics or cranks driven by the shaft of the engine. Early designs used the D slide valve but this has been largely superseded by Piston valve or Poppet valve designs. In steam engines the point in the piston cycle at which the steam inlet valve closes is called the cutoff and this can often be controlled to adjust the torque supplied by the engine and improve efficiency. In some steam engines, the action of the valves can be replaced by an oscillating cylinder.
Internal combustion engines operate through a sequence of strokes that admit and remove gases to and from the cylinder. These operations are repeated cyclically and an engine is said to be 2-stroke, 4-stroke or 6-stroke depending on the number of strokes it takes to complete a cycle.
In some steam engines, the cylinders may be of varying size with the smallest bore cylinder working the highest pressure steam. This is then fed through one or more, increasingly larger bore cylinders successively, to extract power from the steam at increasingly lower pressures. These engines are called Compound engines.
Aside from looking at the power that the engine can produce, the Mean Effective Pressure (MEP), can also be used in comparing the power output and performance of reciprocating engines of the same size. The mean effective pressure is the fictitious pressure which would produce the same amount of net work that was produced during the power stroke cycle. This is shown by:
Wnet = MEP x Piston Area x Stroke = MEP x Displacement Volume and therefore: MEP = Wnet/Displacement Volume
Whichever engine with the larger value of MEP produces more net work per cycle and performs more efficiently.
History.
An early known example of rotary to reciprocating motion can be found in a number of Roman saw mills (dating to the 3rd to 6th century AD) in which a crank and connecting rod mechanism converted the rotary motion of the waterwheel into the linear movement of the saw blades.
The reciprocating engine developed in Europe during the 18th century, first as the atmospheric engine then later as the steam engine. These were followed by the Stirling engine and internal combustion engine in the 19th century. Today the most common form of reciprocating engine is the internal combustion engine running on the combustion of petrol, diesel, Liquefied petroleum gas (LPG) or compressed natural gas (CNG) and used to power motor vehicles and engine power plants.
One notable reciprocating engine from the WWII Era was the 28-cylinder, Pratt & Whitney R-4360 "Wasp Major" radial engine. It powered the last generation of large piston-engined planes before jet engines and turboprops took over from 1944 onward. It had a total engine capacity of , and a high power-to-weight ratio.
The largest reciprocating engine in production at present, but not the largest ever built, is the Wärtsilä-Sulzer RTA96-C turbocharged two-stroke diesel engine of 2006 built by Wärtsilä. It is used to power the largest modern container ships such as the Emma Mærsk. It is five stories high (), long, and weighs over in its largest 14 cylinders version producing more than 84.42 MW (114,800 bhp). Each cylinder has a capacity of , making a total capacity of for the largest versions.
Engine capacity.
For piston engines, an engine's capacity is the engine displacement, in other words the volume swept by all the pistons of an engine in a single movement. It is generally measured in litres (l) or cubic inches (c.i.d. "or" cu in "or" in³) for larger engines, and cubic centimetres (abbreviated cc) for smaller engines. All else being equal, engines with greater capacities are more powerful and consumption of fuel increases accordingly, although power and fuel consumption are affected by many factors outside of engine displacement.
Other modern non-internal combustion types.
Reciprocating engines that are powered by compressed air, steam or other hot gases are still used in some applications such as to drive many modern torpedoes or as pollution-free motive power. Most steam-driven applications use steam turbines, which are more efficient than piston engines.
The French-designed FlowAIR vehicles use compressed air stored in a cylinder to drive a reciprocating engine in a pollution-free urban vehicle.
Torpedoes may use a working gas produced by high test peroxide or Otto fuel II, which pressurise without combustion. The Mark 46 torpedo, for example, can travel underwater at fuelled by Otto fuel without oxidant.
Reciprocating quantum heat engine.
Quantum heat engines are devices that generate power from heat that flows from a hot to a cold reservoir.
The mechanism of operation of the engine can be described by the laws of quantum mechanics. 
Quantum refrigerators are devices that consume power with the purpose to pump heat from a cold to a hot reservoir.
In a reciprocating quantum heat engine the working medium is a quantum system such as spin systems or an harmonic oscillator.
The Carnot cycle and Otto cycle are the ones most studied.
The quantum versions obey the laws of thermodynamics. In addition these models can justify the assumptions of
endoreversible thermodynamics.
A theoretical study has shown that it is possible and practical to build a reciprocating engine that is composed of a single oscillating atom. This is an area for future research and could have applications in nanotechnology.
Miscellaneous engines.
There are a large number of unusual varieties of piston engines that have various claimed advantages, many of which see little if any current use:

</doc>
<doc id="25996" url="https://en.wikipedia.org/wiki?curid=25996" title="Register">
Register

Register or registration may refer to:

</doc>
<doc id="25998" url="https://en.wikipedia.org/wiki?curid=25998" title="Radical feminism">
Radical feminism

Radical feminism is a perspective within feminism that calls for a radical reordering of society in which male supremacy is eliminated in all social and economic contexts. Radical feminists seek to abolish patriarchy by challenging existing social norms and institutions, rather than through a purely political process. This includes challenging the notion of traditional gender roles, opposing the sexual objectification of women, and raising public awareness about such issues as rape and violence against women.
Early radical feminism, arising within second-wave feminism in the 1960s, typically viewed patriarchy as a "transhistorical phenomenon" prior to or deeper than other sources of oppression, "not only the oldest and most universal form of domination but the primary form" and the model for all others. Later politics derived from radical feminism ranged from cultural feminism to more syncretic politics that placed issues of class, economics, etc. on a par with patriarchy as sources of oppression. Radical feminists locate the root cause of women's oppression in patriarchal gender relations, as opposed to legal systems (as in liberal feminism) or class conflict (as in anarchist feminism, socialist feminism, and Marxist feminism).
Theory and ideology.
Radical feminists assert that society is a patriarchy in which the class of men are the oppressors of the class of women. They posit that because of patriarchy, women have come to be viewed as the "other" to the male norm and as such have been systematically oppressed and marginalized. They furthermore assert that men as a class, benefit from the oppression of women. Radical feminists seek to abolish patriarchy, and believe that the way to do this and to deal with oppression of any kind is to address the underlying causes of it through revolution.
While some radical feminists propose that the oppression of women is the most fundamental form of oppression, one that cuts across boundaries of all other forms of oppression, others acknowledge the simultaneous and intersecting effect of other independent categories of oppression. These other categories of oppression may include, but are not limited to, oppression based on race, social class, perceived attractiveness, sexual orientation, and ability.
Patriarchal theory is not generally defined as a belief that all men always benefit from the oppression of all women. Rather, patriarchal theory maintains that the primary element of patriarchy is a relationship of dominance, where one party is dominant and exploits the other party for the benefit of the former. Radical feminists believe that men (as a class) use social systems and other methods of control to keep women (and non-dominant men) suppressed. Radical feminists also believe that eliminating patriarchy, and other systems which perpetuate the domination of one group over another, will liberate everyone from an unjust society.
Some radical feminists calledEller, Cynthia, "The Myth of Matriarchal Prehistory: Why an Invented Past Won't Give Women a Future" (Boston, Mass.: Beacon Press, 2000 (ISBN 0-8070-6792-X)), p. 3.</ref> for women to govern women and men, among them Phyllis Chesler,Douglas, Carol Anne, "Women and Madness", in "off our backs", vol. 36, no. 2, Jul. 1, 2006, p. 71, col. 1 ("Review") (ISSN 0030-0071).Spender, Dale, "For the Record: The Making and Meaning of Feminist Knowledge" (London: The Women's Press, 1985 (ISBN 0-7043-2862-3)), p. 151 and see reply from Phyllis Chesler to author at p. 214.</ref> Monique Wittig (in fiction),Moi, Toril, "Sexual/Textual Politics: Feminist Literary Theory" (London: Routledge, 2d ed., 2002 (ISBN 0-415-28012-5)), p. 78.Auerbach, Nina, "Communities of Women: An Idea in Fiction" (Cambridge, Mass.: Harvard Univ. Press, 1978 (ISBN 0-674-15168-2)), p. 186.Porter, Laurence M., "Feminist Fantasy and Open Structure in Monique Wittig's Les Guérillères", in Morse, Donald E., Marshall B. Tymn, & Csilla Bertha, eds., "The Celebration of the Fantastic: Selected Papers from the Tenth Anniversary International Conference on the Fantastic in the Arts" (Westport, Conn.: Greenwood Press, 1992 (ISBN 0-313-27814-8)), p. 267.Zerilli, Linda M. G., "Feminism and the Abyss of Freedom", "op. cit.", p. 80 n. 51, quoting Porter, Laurence M., "Feminist Fantasy and Open Structure in Monique Wittig's Les Guérillères", "op. cit.", p. [261.</ref> Mary Daly, Jill Johnston,Franklin, Kris, & Sara E. Chinn, "Lesbians, Legal Theory and Other Superheroes", in "Review of Law & Social Change", vol. XXV, 1999, pp. 310–311, as accessed Oct. 21, 2010 (citing in n. 45 "Lesbian Nation", p. 15)).Ross, Becki L., "The House That Jill Built: A Lesbian Nation in Formation" (Toronto: Univ. of Toronto Press, pbk. 1995 (ISBN 0-8020-7479-0)), "passim", esp. pp. 8 & 15–16 & also pp. 19, 71, 111, 204, 205, 212, 219, & 231.Ross, Becki L., "The House That Jill Built", "op. cit.", p. 204 & n. 18, citing McCoy, Sherry, & Maureen Hicks, "A Psychological Retrospective on Power in the Contemporary Lesbian-Feminist Community", in "Frontiers", vol. 4, no. 3 (1979), p. 67.</ref> and Robin Morgan.
Redstockings co-founder Ellen Willis wrote in 1984 that radical feminists "got sexual politics recognized as a public issue," "created the vocabulary... with which the second wave of feminism entered popular culture," "sparked the drive to legalize abortion", "were the first to demand total equality in the so-called private sphere" ("housework and child care ... emotional and sexual needs"), and "created the atmosphere of urgency" that almost led to the passage of the Equal Rights Amendment. The influence of radical feminism can be seen in the adoption of these issues by the National Organization for Women (NOW), a feminist group that had previously been focused almost entirely on economic issues.
Movement.
Roots.
The ideology of radical feminism in the United States developed as a component of the women's liberation movement. It grew largely due to the influence of the civil rights movement, that had gained momentum in the 1960s, and many of the women who took up the cause of radical feminism had previous experience with radical protest in the struggle against racism. Chronologically, it can be seen within the context of second wave feminism that started in the early 1960s. The primary players and the pioneers of this second wave of feminism included Shulamith Firestone, Kathie Sarachild, Ti-Grace Atkinson, Carol Hanisch, and Judith Brown. Many local women's groups in the late sixties, such as the UCLA Women's Liberation Front (WLF), offered diplomatic statements of radical feminism's ideologies. UCLA's WLF co-founder Devra Weber recalls, "'... the radical feminists were opposed to patriarchy, but not necessarily capitalism. In our group at least, they opposed so-called male dominated national liberation struggles'".
These women helped secure the bridge that translated radical protest for racial equality over to the struggle for women's rights; by witnessing the discrimination and oppression to which the black population was subjected, they were able to gain strength and motivation to do the same for their fellow women. They took up the cause and advocated for a variety of women's issues, including abortion, the Equal Rights Amendment, access to credit, and equal pay. They failed to stir up enough interest among most of the women's fringe groups of society. Most women of color (who were predominantly working-class) did not participate in the formation of the radical feminist movement because it did not address many issues that were relevant to those from a working-class background. But for those who felt compelled enough to stand up for the cause, radical action was needed, and so they took to the streets and formed consciousness raising groups to rally support for the cause and recruit people who would be willing to fight for it. Later on, Second Wave radical feminism saw greater numbers of black feminists and other women of color participating.
In the 1960s, radical feminism emerged simultaneously within liberal feminist and working class feminist discussions, first in the United States, then in the United Kingdom and Australia. Those involved had gradually come to believe that it was not only the middle-class nuclear family which oppressed women, but that it was also social movements and organizations that claimed to stand for human liberation, notably the counterculture, the New Left, and Marxist political parties, all of which they considered to be male-dominated and male-oriented. Women in countercultural groups related that the gender relations present in such groups were very much those of mainstream culture.
In the United States, radical feminism developed as a response to some of the perceived failings of both New Left organizations such as the Students for a Democratic Society (SDS) and feminist organizations such as NOW. Initially concentrated in big cities like New York, Chicago, Boston, Washington, DC, and on the West Coast, radical feminist groups spread across the country rapidly from 1968 to 1972.
In the United Kingdom, feminism developed out of discussions within community based radical women's organizations and discussions by women within the Trotskyist left. Radical feminism was imported into the UK by American radical feminists and seized on by British radical women as offering an exciting new theory to replace Trotskyism. As the 1970s progressed, British feminists split into two major schools of thought: socialist and radical. In 1977, another split occurred, with a third grouping calling itself "revolutionary feminism" breaking away from the other two.
Australian radical feminism developed slightly later, during an extended period of social radicalization, largely as an expression of that radicalization.
Radical feminists introduced the use of consciousness raising (CR) groups. These groups brought together intellectuals, workers, and middle class women in developed Western countries to discuss their experiences. During these discussions, women noted a shared and repressive system regardless of their political affiliation or social class. Based on these discussions, the women drew the conclusion that ending of patriarchy was the most necessary step towards a truly free society. These consciousness-raising sessions allowed early radical feminists to develop a political ideology based on common experiences women faced with male supremacy. Consciousness raising was extensively used in chapter sub-units of the National Organization for Women (NOW) during the 1970s. The feminism that emerged from these discussions stood first and foremost for the liberation of women, as women, from the oppression of men in their own lives, as well as men in power. Radical feminism claimed that a totalizing ideology and social formation—"patriarchy" (government or rule by fathers)—dominated women in the interests of men.
Within groups such as New York Radical Women (1967–1969; no relation to the present-day socialist feminist organization Radical Women), which Ellen Willis characterized as "the first women's liberation group in New York City", a radical feminist ideology began to emerge that declared that "the personal is political" and "sisterhood is powerful", formulations that arose from these consciousness-raising sessions. New York Radical Women fell apart in early 1969 in what came to be known as the "politico-feminist split" with the "politicos" seeing capitalism as the source of women's oppression, while the "feminists" saw male supremacy as "a set of material, institutionalized relations, not just bad attitudes." The feminist side of the split, which soon began referring to itself as "radical feminists", soon constituted the basis of a new organization, Redstockings. At the same time, Ti-Grace Atkinson led "a radical split-off from NOW", which became known as The Feminists. A third major stance would be articulated by the New York Radical Feminists, founded later in 1969 by Shulamith Firestone (who broke from the Redstockings) and Anne Koedt.
During this period, the movement produced "a prodigious output of leaflets, pamphlets, journals, magazine articles, newspaper and radio and TV interviews." Many important feminist works, such as Koedt's essay "The Myth of the Vaginal Orgasm" (1970) and Kate Millet's book "Sexual Politics" (1970), emerged during this time and in this milieu.
Ideology emerges and diverges.
At the beginning of this period, "heterosexuality was more or less an unchallenged assumption." Among radical feminists, the view became widely held that, thus far, the sexual freedoms gained in the sexual revolution of the 1960s, in particular, the decreasing emphasis on monogamy, had been largely gained by men at women's expense. This assumption of heterosexuality would soon be challenged by the rise of political lesbianism, closely associated with Atkinson and The Feminists. The belief that the sexual revolution was a victory of men over women would eventually lead to the women's anti-pornography movement of the late 1970s.
Redstockings and The Feminists were both radical feminist organizations, but held rather distinct views. Most members of Redstockings held to a materialist and anti-psychologistic view. They viewed men's oppression of women as ongoing and deliberate, holding individual men responsible for this oppression, viewing institutions and systems (including the family) as mere vehicles of conscious male intent, and rejecting psychologistic explanations of female submissiveness as blaming women for collaboration in their own oppression. They held to a view—which Willis would later describe as "neo-Maoist"—that it would be possible to unite all or virtually all women, as a class, to confront this oppression by personally confronting men.
The Feminists held a more idealistic, psychologistic, and utopian philosophy, with a greater emphasis on "sex roles", seeing sexism as rooted in "complementary patterns of male and female behavior". They placed more emphasis on institutions, seeing marriage, family, prostitution, and heterosexuality as all existing to perpetuate the "sex-role system". They saw all of these as institutions to be destroyed. Within the group, there were further disagreements, such as Koedt's viewing the institution of "normal" sexual intercourse as being focused mainly on male sexual or erotic pleasure, while Atkinson viewed it mainly in terms of reproduction. In contrast to the Redstockings, The Feminists generally considered genitally focused sexuality to be inherently male. Ellen Willis would later write that insofar as the Redstockings considered abandoning heterosexual activity, they saw it as a "bitter price" they "might have to pay for militance", whereas The Feminists embraced separatist feminism as a strategy.
The New York Radical Feminists (NYRF) took a more psychologistic (and even biologically determinist) line. They argued that men dominated women not so much for material benefits as for the ego satisfaction intrinsic in domination. Similarly, they rejected the Redstockings view that women submitted only out of necessity or The Feminists' implicit view that they submitted out of cowardice, but instead argued that social conditioning simply led most women to accept a submissive role as "right and natural".
Action.
Radical feminism was not and is not only a movement of ideology and theory. Radical feminists also take direct action. In 1968, they protested against the Miss America pageant by throwing high heels and other feminine accoutrements into a garbage bin, to represent freedom. In 1970, they also staged a sit-in at the "Ladies' Home Journal". In addition, they held speakouts about topics such as rape.
Radical egalitarianism.
Because of their commitment to radical egalitarianism, most early radical feminist groups operated initially without any formal internal structure. When informal leadership developed, it was often resented. Many groups ended up expending more effort debating their own internal operations than dealing with external matters, seeking to "perfect a perfect society in microcosm" rather than focus on the larger world. Resentment of leadership was compounded by the view that all "class striving" was "male-identified". In the extreme, exemplified by The Feminists, the upshot, according to Ellen Willis, was "unworkable, mechanistic demands for an absolutely random division of labor, taking no account of differences in skill, experience, or even inclination". "The result," writes Willis, "was not democracy but paralysis." When The Feminists began to select randomly who could talk to the press, Ti-Grace Atkinson quit the organization she had founded.
Social organization and aims in the U.S. and Australia.
Radical feminists have generally formed small activist or community associations around either consciousness raising or concrete aims. Many radical feminists in Australia participated in a series of squats to establish various women's centers, and this form of action was common in the late 1970s and early 1980s. By the mid-1980s many of the original consciousness raising groups had dissolved, and radical feminism was more and more associated with loosely organized university collectives. Radical feminism can still be seen, particularly within student activism and among working class women.
In Australia, many feminist social organizations accepted government funding during the 1980s, and the election of a conservative government in 1996 crippled these organizations.
While radical feminists aim to dismantle patriarchal society in a historical sense, their immediate aims are generally concrete. Some common demands include:
Other nations.
The movement also arose in Israel among Jews.
Views on the sex industry.
Radical feminists have written about a wide range of issues regarding the sex industry – which they tend to oppose – including but not limited to: harm to women during the production of pornography, the social harm from consumption of pornography, the coercion and poverty that leads women to become prostitutes, the long-term effects of prostitution, the raced and classed nature of prostitution, and male dominance over women in prostitution and pornography.
Views on prostitution.
Radical feminists argue that, in most cases, prostitution is not a conscious and calculated choice. They say that most women who become prostitutes do so because they were forced or coerced by a pimp or by human trafficking, or, when it is an independent decision, it is generally the result of extreme poverty and lack of opportunity, or of serious underlying problems, such as drug addiction, past trauma (such as child sexual abuse) and other unfortunate circumstances. 
Radical feminists point out that women from the lowest socioeconomic classes—impoverished women, women with a low level of education, women from the most disadvantaged racial and ethnic minorities—are overrepresented in prostitution all over the world. "If prostitution is a free choice, why are the women with the fewest choices the ones most often found doing it?" (MacKinnon, 1993). A large percentage of prostitutes polled in one study of 475 people involved in prostitution reported that they were in a difficult period of their lives and most wanted to leave the occupation.
Catharine MacKinnon argues that "In prostitution, women have sex with men they would never otherwise have sex with. The money thus acts as a form of force, not as a measure of consent. It acts like physical force does in rape."
They believe no person can be said to truly consent to their own oppression and no people should have the right to consent to the oppression of others. In the words of Kathleen Barry, consent is not a “good divining rod as to the existence of oppression, and consent to violation is a fact of oppression. Oppression cannot effectively be gauged according to the degree of “consent,” since even in slavery there was some consent, if consent is defined as inability to see, or feel any alternative.”
Andrea Dworkin stated her opinions as: "Prostitution in and of itself is an abuse of a woman's body. Those of us who say this are accused of being simple-minded. But prostitution is very simple. (...) In prostitution, no woman stays whole. It is impossible to use a human body in the way women's bodies are used in prostitution and to have a whole human being at the end of it, or in the middle of it, or close to the beginning of it. It's impossible. And no woman gets whole again later, after.”
Radical feminist thinking has analyzed prostitution as a cornerstone of patriarchal domination and sexual subjugation of women that impacts negatively not only on the women and girls in prostitution but on all women as a group because prostitution continually affirms and reinforces patriarchal definitions of women as having a primary function to serve men sexually. They claim it is crucial that society does not replace one patriarchal view on female sexuality - e.g., that women should not have sex outside marriage/a relationship and that casual sex is shameful for a woman, etc. - with another similarly oppressive and patriarchal view - acceptance of prostitution, a sexual practice which is based on a highly patriarchal construct of sexuality: that the sexual pleasure of a woman is irrelevant, that her only role during sex is to submit to the man’s sexual demands and to do what he tells her, that sex should be controlled by the man and that the woman’s response and satisfaction are irrelevant. These feminists argue that sexual liberation for women cannot be achieved as long as we normalize unequal sexual practices where a man dominates a woman.
They see prostitution as a form of male dominance, as it puts the woman in a subordinate position, reducing her to a mere instrument of sexual pleasure for the client. These feminists believe that many clients use the services of prostitutes because they enjoy the "power trip" they derive from the act and the control they have over the woman during the sexual activity. Catharine MacKinnon argues that prostitution "isn't sex only, it’s you do what I say, sex."
Radical feminists strongly object to the patriarchal ideology which has been one of the justifications for the existence of prostitution throughout history (and which they say continues to justify it in many cultures), that is, that prostitution is a "necessary evil", as men cannot control themselves, and thus it is "necessary" that a small number of women be "sacrificed" to be used and abused by men, in order to protect "chaste" women from rape and harassment. These feminists see prostitution as a form of slavery, and say that, far from decreasing rape rates, prostitution leads to a sharp "increase" in sexual violence against women, by sending the message that it is acceptable for a man to treat a woman as a sexual instrument over which he has total control. Melissa Farley argues that Nevada's high rape rate is connected to legal prostitution because Nevada is the only US state which allows legal brothels and is ranked 4th out of the 50 U.S. states for sexual assault crimes, saying, "Nevada's rape rate is higher than the U.S. average and way higher than the rape rate in California, New York and New Jersey. Why is this? Legal prostitution creates an atmosphere in this state in which women are not humans equal to them, are disrespected by men, and which then sets the stage of increased violence against women."
Indigenous women the world over are particularly targeted for prostitution. In Canada, New Zealand, Mexico, and Taiwan, studies have shown that indigenous women are at the bottom of the race and class hierarchy of prostitution, often subjected to the worst conditions, most violent demands and sold at the lowest price. It is common for indigenous women to be over-represented in prostitution when compared with their total population. This is as a result of the combined forces of colonialism, physical displacement from ancestral lands, destruction of indigenous social and cultural order, misogyny, globalization/neoliberalism, race discrimination and extremely high levels of violence perpetrated against them.
Views on pornography.
Radical feminists, notably Catherine MacKinnon, charge that the production of pornography entails physical, psychological, and/or economic coercion of the women who perform and model in it. This is said to be true even when the women are being presented as enjoying themselves. It is also argued that much of what is shown in pornography is abusive by its very nature. Gail Dines holds that pornography, exemplified by gonzo pornography, is becoming increasingly violent and that women who perform in pornography are brutalized in the process of its production.
Radical feminists point to the testimony of well known participants in pornography, such as Traci Lords and Linda Boreman, and argue that most female performers are coerced into pornography, either by somebody else, or by an unfortunate set of circumstances. The feminist anti-pornography movement was galvanized by the publication of "Ordeal", in which Linda Boreman (who under the name of "Linda Lovelace" had starred in "Deep Throat") stated that she had been beaten, raped, and pimped by her husband Chuck Traynor, and that Traynor had forced her at gunpoint to make scenes in "Deep Throat", as well as forcing her, by use of both physical violence against Boreman as well as emotional abuse and outright threats of violence, to make other pornographic films. Dworkin, MacKinnon, and Women Against Pornography issued public statements of support for Boreman, and worked with her in public appearances and speeches.
Radical feminists hold the view that pornography contributes to sexism, arguing that in pornographic performances the actresses are reduced to mere receptacles—objects—for sexual use and abuse by men. They argue that the narrative is usually formed around men's pleasure as the only goal of sexual activity, and that the women are shown in a subordinate role. Some opponents believe pornographic films tend to show women as being extremely passive, or that the acts which are performed on the women are typically abusive and solely for the pleasure of their sex partner. On-face ejaculation and anal sex are increasingly popular among men, following trends in porn. MacKinnon and Dworkin defined pornography as "the graphic sexually explicit subordination of women through pictures or words".
Radical feminists say that consumption of pornography is a cause of rape and other forms of violence against women. Robin Morgan summarizes this idea with her often-quoted statement, "Pornography is the theory, and rape is the practice."
Radical feminists charge that pornography eroticizes the domination, humiliation, and coercion of women, and reinforces sexual and cultural attitudes that are complicit in rape and sexual harassment. MacKinnon argued that pornography leads to an increase in sexual violence against women through fostering rape myths. Such rape myths include the belief that women really want to be raped and that they mean yes when they say no. Additionally, according to MacKinnon, pornography desensitizes viewers to violence against women, and this leads to a progressive need to see more violence in order to become sexually aroused, an effect she claims is well documented.
German radical feminist Alice Schwarzer is one proponent of the point of view according to which pornography gives a distorted view of men and women's bodies, as well as the actual sexual act, often showing the performers with synthetic implants or exaggerated expressions of pleasure, as well as fetishes that are not the norm, such as watersports, being presented as popular and normal.
Radical lesbian feminism.
Radical lesbians are distinguished from other radical feminists through their ideological roots in political lesbianism. Radical lesbians see lesbianism as an act of resistance against the political institution of heterosexuality, which they view as violent and oppressive towards women.
Views on transgender issues.
Since the 1970s, there has been an ongoing debate among radical feminists about the role of transgender identities in society. Many radical feminists, as well as groups such as Radical Women, have supported transgender rights and trans-inclusivity. Others, such as Janice Raymond, Germaine Greer, Sheila Jeffreys, Julie Bindel, and Robert Jensen, have accused the transgender movement of perpetuating patriarchal gender norms and characterized it as incompatible with radical feminist ideology.
Opposition.
In 1978 the Lesbian Organization of Toronto voted to become womyn-born womyn only and wrote: 
A woman's voice was almost never heard as a woman's voice – it was always filtered through men's voices. So here a guy comes along saying, "I'm going to be a girl now and speak for girls." And we thought, "No you're not." A person cannot just join the oppressed by fiat.
In 1979 American lesbian radical feminist activist Janice Raymond released the book "", which looked at the role of transsexuality – particularly psychological and surgical approaches to it – in reinforcing traditional gender stereotypes, the ways in which the "medical-psychiatric complex" has medicalized "gender identity", and the social and political context that helped spawn transsexual treatment and surgery as normal and therapeutic medicine. "The Transsexual Empire" maintains that transsexuality is based on the "patriarchal myths" of "male mothering", and "making of woman according to man's image". Raymond claimed this was done in order "to colonize feminist identification, culture, politics and sexuality", adding: "All transsexuals rape women's bodies by reducing the real female form to an artifact, appropriating this body for themselves ... Transsexuals merely cut off the most obvious means of invading women, so that they seem non-invasive."
In 1999, Germaine Greer published a sequel to "The Female Eunuch", the book "The Whole Woman". One chapter was titled "Pantomime Dames", wherein she states her opposition to accepting transsexuals as women: 
Governments that consist of very few women have hurried to recognise as women men who believe that they are women and have had themselves castrated to prove it, because they see women not as another sex but as a non-sex. No so-called sex-change has ever begged for a uterus-and-ovaries transplant; if uterus-and-ovaries transplants were made mandatory for wannabe women they would disappear overnight. The insistence that man-made women be accepted as women is the institutional expression of the mistaken conviction that women are defective males.
Sheila Jeffreys argues that gender is not immutable and thus does not warrant radical medical intervention, considers detransitioners to be evidence of this, and describes sex reassignment surgery as "mutilation". Jeffreys also argues that "the vast majority of transsexuals still subscribe to the traditional stereotype of women" and that by transitioning medically and socially, trans women are "constructing a conservative fantasy of what women should be. They are inventing an essence of womanhood which is deeply insulting and restrictive". Throughout "Gender Hurts: A Feminist Analysis of the Politics of Transgenderism", co-written with Lorene Gottschalk, Jeffreys insists on using male pronouns to refer to trans women arguing that "use by men of feminine pronouns conceals the masculine privilege bestowed upon them by virtue of having been placed in and brought up in the male sex caste". Julie Bindel said "I don't have a problem with men disposing of their genitals, but it does not make them women, in the same way that shoving a bit of vacuum hose down your 501s does not make you a man." As of 2009 Bindel maintained that "people should question the basis of the diagnosis of male psychiatrists, at a time when gender polarisation and homophobia work hand-in-hand." She argues that "Iran carries out the highest number of sex change surgeries in the world" because "surgery is an attempt to keep gender stereotypes intact" and that "the idea that certain distinct behaviours are appropriate for males and females underlies feminist criticism of the phenomenon of 'transgenderism'."
Robert Jensen has outlined feminist and ecological concerns about transgender ideology, and connected that ideology to a larger cultural fear of the feminist critique of patriarchy.
Radical feminists have sometimes advocated for the exclusion of trans women from feminist events, a source of much controversy. Lisa Vogel, the Michfest event organizer claimed that protesters from Camp Trans responded to this controversy with vandalism. They argue that trans women cannot be counted as women because they were not born biologically female. Such radical feminists hold that trans women have enjoyed male privilege by virtue of being assigned male at birth and their insistence on acceptance is a type of male entitlement. Radical feminists reject the notion of a female brain. They believe that the differences in behavior between men and women are a result of different socialization and believe that - in the words of Lierre Keith - femininity is "ritualized submission". In this view, gender is less an identity than a caste position and transgender is an obstacle to gender abolition. These views are not widely held by broader feminist movement, are rejected by many trans women, and are often labeled transphobic.
The term TERF (trans-exclusionary radical feminist) has been used to refer to radical feminists who hold such views, and radical feminists holding these views have been described as members of a hate group who are allegedly "masquerading as feminists". The term is considered a slur by those at whom it is directed.
Criticism.
During the early years, some radical feminists were criticized for emphasizing sex-based discrimination at the expense of race- and class-based discrimination, for being unwilling to work with men to effect change through political channels, and for reinforcing gender essentialism (the idea that men and women are inherently different).
According to Ellen Willis' 1984 essay "Radical Feminism and Feminist Radicalism", within the New Left, radical feminists were accused of being "bourgeois", "antileft", or even "apolitical", whereas they saw themselves as further "radicalizing the left by expanding the definition of radical". Early radical feminists tended to be white and middle class. Willis hypothesized that this was, at least in part, because "most black and working-class women could not accept the abstraction of feminist issues from race and class issues"; the resulting narrow demographic base, in turn, limited the validity of generalizations based on radical feminists' personal experiences of gender relations. Many early radical feminists broke political ties with "male-dominated left groups", or would work with them only in "ad hoc" coalitions.
Also, Willis, although very much a part of early radical feminism and continuing to hold that it played a necessary role in placing feminism on the political agenda, later criticized its inability "to integrate a feminist perspective with an overall radical politics," while viewing this limitation as inevitable in the historical context of the times. In part this limitation arose from the fact that consciousness raising, as "the primary method of understanding women's condition" in the movement at this time and its "most successful organizing tool", led to an emphasis on personal experience that concealed "prior political and philosophical assumptions".

</doc>
<doc id="26000" url="https://en.wikipedia.org/wiki?curid=26000" title="Ray tracing (graphics)">
Ray tracing (graphics)

In computer graphics, ray tracing is a technique for generating an image by tracing the path of light through pixels in an image plane and simulating the effects of its encounters with virtual objects. The technique is capable of producing a very high degree of visual realism, usually higher than that of typical scanline rendering methods, but at a greater computational cost. This makes ray tracing best suited for applications where the image can be rendered slowly ahead of time, such as in still images and film and television visual effects, and more poorly suited for real-time applications like video games where speed is critical. Ray tracing is capable of simulating a wide variety of optical effects, such as reflection and refraction, scattering, and dispersion phenomena (such as chromatic aberration).
Algorithm overview.
Optical ray tracing describes a method for producing visual images constructed in 3D computer graphics environments, with more photorealism than either ray casting or scanline rendering techniques. It works by tracing a path from an imaginary eye through each pixel in a virtual screen, and calculating the color of the object visible through it.
Scenes in ray tracing are described mathematically by a programmer or by a visual artist (typically using intermediary tools). Scenes may also incorporate data from images and models captured by means such as digital photography.
Typically, each ray must be tested for intersection with some subset of all the objects in the scene. Once the nearest object has been identified, the algorithm will estimate the incoming light at the point of intersection, examine the material properties of the object, and combine this information to calculate the final color of the pixel. Certain illumination algorithms and reflective or translucent materials may require more rays to be re-cast into the scene.
It may at first seem counterintuitive or "backwards" to send rays "away" from the camera, rather than "into" it (as actual light does in reality), but doing so is many orders of magnitude more efficient. Since the overwhelming majority of light rays from a given light source do not make it directly into the viewer's eye, a "forward" simulation could potentially waste a tremendous amount of computation on light paths that are never recorded.
Therefore, the shortcut taken in raytracing is to presuppose that a given ray intersects the view frame. After either a maximum number of reflections or a ray traveling a certain distance without intersection, the ray ceases to travel and the pixel's value is updated.
Detailed description of ray tracing computer algorithm and its genesis.
What happens in nature.
In nature, a light source emits a ray of light which travels, eventually, to a surface that interrupts its progress. One can think of this "ray" as a stream of photons traveling along the same path. In a perfect vacuum this ray will be a straight line (ignoring relativistic effects). Any combination of four things might happen with this light ray: absorption, reflection, refraction and fluorescence. A surface may absorb part of the light ray, resulting in a loss of intensity of the reflected and/or refracted light. It might also reflect all or part of the light ray, in one or more directions. If the surface has any transparent or translucent properties, it refracts a portion of the light beam into itself in a different direction while absorbing some (or all) of the spectrum (and possibly altering the color). Less commonly, a surface may absorb some portion of the light and fluorescently re-emit the light at a longer wavelength color in a random direction, though this is rare enough that it can be discounted from most rendering applications. Between absorption, reflection, refraction and fluorescence, all of the incoming light must be accounted for, and no more. A surface cannot, for instance, reflect 66% of an incoming light ray, and refract 50%, since the two would add up to be 116%. From here, the reflected and/or refracted rays may strike other surfaces, where their absorptive, refractive, reflective and fluorescent properties again affect the progress of the incoming rays. Some of these rays travel in such a way that they hit our eye, causing us to see the scene and so contribute to the final rendered image.
Ray casting algorithm.
The first ray tracing algorithm used for rendering was presented by Arthur Appel in 1968. This algorithm has since been termed "ray casting". The idea behind ray casting is to shoot rays from the eye, one per pixel, and find the closest object blocking the path of that ray. Think of an image as a screen-door, with each square in the screen being a pixel. This is then the object the eye sees through that pixel. Using the material properties and the effect of the lights in the scene, this algorithm can determine the shading of this object. The simplifying assumption is made that if a surface faces a light, the light will reach that surface and not be blocked or in shadow. The shading of the surface is computed using traditional 3D computer graphics shading models. One important advantage ray casting offered over older scanline algorithms was its ability to easily deal with non-planar surfaces and solids, such as cones and spheres. If a mathematical surface can be intersected by a ray, it can be rendered using ray casting. Elaborate objects can be created by using solid modeling techniques and easily rendered.
Recursive ray tracing algorithm.
The next important research breakthrough came from Turner Whitted in 1979. Previous algorithms traced rays from the eye into the scene until they hit an object, but determined the ray color without recursively tracing more rays. Whitted continued the process. When a ray hits a surface, it can generate up to three new types of rays: reflection, refraction, and shadow. A reflection ray is traced in the mirror-reflection direction. The closest object it intersects is what will be seen in the reflection. Refraction rays traveling through transparent material work similarly, with the addition that a refractive ray could be entering or exiting a material. A shadow ray is traced toward each light. If any opaque object is found between the surface and the light, the surface is in shadow and the light does not illuminate it. This recursive ray tracing added more realism to ray traced images.
Advantages over other rendering methods.
Ray tracing's popularity stems from its basis in a realistic simulation of lighting over other rendering methods (such as scanline rendering or ray casting). Effects such as reflections and shadows, which are difficult to simulate using other algorithms, are a natural result of the ray tracing algorithm. The computational independence of each ray makes ray tracing amenable to parallelization.
Disadvantages.
A serious disadvantage of ray tracing is performance. Scanline algorithms and other algorithms use data coherence to share computations between pixels, while ray tracing normally starts the process anew, treating each eye ray separately. However, this separation offers other advantages, such as the ability to shoot more rays as needed to perform spatial anti-aliasing and improve image quality where needed.
Although it does handle interreflection and optical effects such as refraction accurately, traditional ray tracing is also not necessarily photorealistic. True photorealism occurs when the rendering equation is closely approximated or fully implemented. Implementing the rendering equation gives true photorealism, as the equation describes every physical effect of light flow. However, this is usually infeasible given the computing resources required.
The realism of all rendering methods can be evaluated as an approximation to the equation. Ray tracing, if it is limited to Whitted's algorithm, is not necessarily the most realistic. Methods that trace rays, but include additional techniques (photon mapping, path tracing), give far more accurate simulation of real-world lighting.
It is also possible to approximate the equation using ray casting in a different way than what is traditionally considered to be "ray tracing". For performance, rays can be clustered according to their direction, with rasterization hardware and depth peeling used to efficiently sum the rays.
Reversed direction of traversal of scene by the rays.
The process of shooting rays from the eye to the light source to render an image is sometimes called "backwards ray tracing", since it is the opposite direction photons actually travel. However, there is confusion with this terminology. Early ray tracing was always done from the eye, and early researchers such as James Arvo used the term "backwards ray tracing" to mean shooting rays from the lights and gathering the results. Therefore, it is clearer to distinguish "eye-based" versus "light-based" ray tracing.
While the direct illumination is generally best sampled using eye-based ray tracing, certain indirect effects can benefit from rays generated from the lights. Caustics are bright patterns caused by the focusing of light off a wide reflective region onto a narrow area of (near-)diffuse surface. An algorithm that casts rays directly from lights onto reflective objects, tracing their paths to the eye, will better sample this phenomenon. This integration of eye-based and light-based rays is often expressed as bidirectional path tracing, in which paths are traced from both the eye and lights, and the paths subsequently joined by a connecting ray after some length.
Photon mapping is another method that uses both light-based and eye-based ray tracing; in an initial pass, energetic photons are traced along rays from the light source so as to compute an estimate of radiant flux as a function of 3-dimensional space (the eponymous photon map itself). In a subsequent pass, rays are traced from the eye into the scene to determine the visible surfaces, and the photon map is used to estimate the illumination at the visible surface points. The advantage of photon mapping versus bidirectional path tracing is the ability to achieve significant reuse of photons, reducing computation, at the cost of statistical bias.
An additional problem occurs when light must pass through a very narrow aperture to illuminate the scene (consider a darkened room, with a door slightly ajar leading to a brightly lit room), or a scene in which most points do not have direct line-of-sight to any light source (such as with ceiling-directed light fixtures or torchieres). In such cases, only a very small subset of paths will transport energy; Metropolis light transport is a method which begins with a random search of the path space, and when energetic paths are found, reuses this information by exploring the nearby space of rays.
To the right is an image showing a simple example of a path of rays recursively generated from the camera (or eye) to the light source using the above algorithm. A diffuse surface reflects light in all directions.
First, a ray is created at an eyepoint and traced through a pixel and into the scene, where it hits a diffuse surface. From that surface the algorithm recursively generates a reflection ray, which is traced through the scene, where it hits another diffuse surface. Finally, another reflection ray is generated and traced through the scene, where it hits the light source and is absorbed. The color of the pixel now depends on the colors of the first and second diffuse surface and the color of the light emitted from the light source. For example, if the light source emitted white light and the two diffuse surfaces were blue, then the resulting color of the pixel is blue.
Example.
As a demonstration of the principles involved in raytracing, let us consider how one would find the intersection between a ray and a sphere. This is merely the math behind the line–sphere intersection and the subsequent determination of the colour of the pixel being calculated. There is, of course, far more to the general process of raytracing, but this demonstrates an example of the algorithms used.
In vector notation, the equation of a sphere with center formula_1 and radius formula_2 is
Any point on a ray starting from point formula_4 with direction formula_5 (here formula_5 is a unit vector) can be written as
where formula_8 is its distance between formula_9 and formula_4. In our problem, we know formula_1, formula_12, formula_4 (e.g. the position of a light source) and formula_5, and we need to find formula_8. Therefore, we substitute for formula_9:
Let formula_18 for simplicity; then
Knowing that d is a unit vector allows us this minor simplification:
This quadratic equation has solutions
The two values of formula_24 found by solving this equation are the two ones such that formula_25 are the points where the ray intersects the sphere.
Any value which is negative does not lie on the ray, but rather in the opposite half-line (i.e. the one starting from formula_4 with opposite direction).
If the quantity under the square root ( the discriminant ) is negative, then the ray does not intersect the sphere.
Let us suppose now that there is at least a positive solution, and let formula_24 be the minimal one. In addition, let us suppose that the sphere is the nearest object on our scene intersecting our ray, and that it is made of a reflective material. We need to find in which direction the light ray is reflected. The laws of reflection state that the angle of reflection is equal and opposite to the angle of incidence between the incident ray and the normal to the sphere.
The normal to the sphere is simply
where formula_29 is the intersection point found before. The reflection direction can be found by a reflection of formula_5 with respect to formula_31, that is
Thus the reflected ray has equation
Now we only need to compute the intersection of the latter ray with our field of view, to get the pixel which our reflected light ray will hit. Lastly, this pixel is set to an appropriate color, taking into account how the color of the original light source and the one of the sphere are combined by the reflection.
Adaptive depth control.
This means that we stop generating reflected/transmitted rays when the computed intensity becomes less than a certain threshold. You must always set a certain maximum depth or else the program would generate an infinite number of rays. But it is not always necessary to go to the maximum depth if the surfaces are not highly reflective. To test for this the ray tracer must compute and keep the product of the global and reflection coefficients as the rays are traced.
Example: let Kr = 0.5 for a set of surfaces. Then from the first surface the maximum contribution is 0.5, for the reflection from the second: 0.5 * 0.5 = 0.25, the third: 0.25 * 0.5 = 0.125, the fourth: 0.125 * 0.5 = 0.0625, the fifth: 0.0625 * 0.5 = 0.03125, etc. In addition we might implement a distance attenuation factor such as 1/D2, which would also decrease the intensity contribution.
For a transmitted ray we could do something similar but in that case the distance traveled through the object would cause even faster intensity decrease. As an example of this, Hall & Greenbergfound that even for a very reflective scene, using this with a maximum depth of 15 resulted in an average ray tree depth of 1.7.
Bounding volumes.
We enclose groups of objects in sets of hierarchical bounding volumes and first test for intersection with the bounding volume, and then only if there is an intersection, against the objects enclosed by the volume.
Bounding volumes should be easy to test for intersection, for example a sphere or box (slab). The best bounding volume will be determined by the shape of the underlying object or objects. For example, if the objects are long and thin then a sphere will enclose mainly empty space and a box is much better. Boxes are also easier for hierarchical bounding volumes.
Note that using a hierarchical system like this (assuming it is done carefully) changes the intersection computational time from a linear dependence on the number of objects to something between linear and a logarithmic dependence. This is because, for a perfect case, each intersection test would divide the possibilities by two, and we would have a binary tree type structure. Spatial subdivision methods, discussed below, try to achieve this.
Kay & Kajiya give a list of desired properties for hierarchical bounding volumes:
In real time.
The first implementation of a "real-time" ray-tracer was credited at the 2005 SIGGRAPH computer graphics conference as the REMRT/RT tools developed in 1986 by Mike Muuss for the BRL-CAD solid modeling system. Initially published in 1987 at USENIX, the BRL-CAD ray-tracer is the first known implementation of a parallel network distributed ray-tracing system that achieved several frames per second in rendering performance. This performance was attained by means of the highly optimized yet platform independent LIBRT ray-tracing engine in BRL-CAD and by using solid implicit CSG geometry on several shared memory parallel machines over a commodity network. BRL-CAD's ray-tracer, including REMRT/RT tools, continue to be available and developed today as Open source software.
Since then, there have been considerable efforts and research towards implementing ray tracing in real time speeds for a variety of purposes on stand-alone desktop configurations. These purposes include interactive 3D graphics applications such as demoscene productions, computer and video games, and image rendering. Some real-time software 3D engines based on ray tracing have been developed by hobbyist demo programmers since the late 1990s.
The OpenRT project includes a highly optimized software core for ray tracing along with an OpenGL-like API in order to offer an alternative to the current rasterisation based approach for interactive 3D graphics. Ray tracing hardware, such as the experimental Ray Processing Unit developed at the Saarland University, has been designed to accelerate some of the computationally intensive operations of ray tracing. On March 16, 2007, the University of Saarland revealed an implementation of a high-performance ray tracing engine that allowed computer games to be rendered via ray tracing without intensive resource usage.
On June 12, 2008 Intel demonstrated a special version of ', titled ', using ray tracing for rendering, running in basic HD (720p) resolution. ETQW operated at 14-29 frames per second. The demonstration ran on a 16-core (4 socket, 4 core) Xeon Tigerton system running at 2.93 GHz.
At SIGGRAPH 2009, Nvidia announced OptiX, a free API for real-time ray tracing on Nvidia GPUs. The API exposes seven programmable entry points within the ray tracing pipeline, allowing for custom cameras, ray-primitive intersections, shaders, shadowing, etc. This flexibility enables bidirectional path tracing, Metropolis light transport, and many other rendering algorithms that cannot be implemented with tail recursion. Nvidia has shipped over 350,000,000 OptiX capable GPUs as of April 2013. OptiX-based renderers are used in Adobe AfterEffects, Bunkspeed Shot, Autodesk Maya, 3ds max, and many other renderers.
Imagination Technologies offers a free API called OpenRL which accelerates tail recursive ray tracing-based rendering algorithms and, together with their proprietary ray tracing hardware, works with Autodesk Maya to provide what 3D World calls "real-time raytracing to the everyday artist".
Computational Complexity.
Various complexity results have been proven for certain formulations of the ray tracing problem. In particular, if the decision version of the ray tracing problem is defined as follows - given a light ray's initial position and direction and some fixed point, does the ray eventually reach that point, then the referenced paper proves the following results:

</doc>
<doc id="26002" url="https://en.wikipedia.org/wiki?curid=26002" title="Ron Carter">
Ron Carter

Ron Carter (born Ronald Levin Carter, May 4, 1937) is an American jazz double bassist. His appearances on over 1100 recording sessions make him the second most-recorded jazz bassist in history, after Milt Hinton. Carter is also a cellist who has recorded numerous times on that instrument. Some of his studio albums as a leader include: "Blues Farm" (1973); "All Blues" (1973); "Spanish Blue" (1974); "Anything Goes" (1975); "Yellow & Green" (1976); "Pastels" (1976); "Piccolo" (1977); "Third Plane" (1977); "Peg Leg" (1978); and "A Song for You" (1978).
He was a member of the Miles Davis Quintet in the early 1960s, which also included Herbie Hancock, Wayne Shorter and drummer Tony Williams. Carter joined Davis's group in 1963, appearing on the album "Seven Steps to Heaven" and the follow-up "E.S.P.". Carter also performed on some of Hancock, Williams and Shorter's recordings during the sixties for Blue Note Records. He was a sideman on many Blue Note recordings of the era, playing with Sam Rivers, Freddie Hubbard, Duke Pearson, Lee Morgan, McCoy Tyner, Andrew Hill, Horace Silver and many others. He was elected to the Down Beat Jazz Hall of Fame in 2012. In 1993, he won a Grammy Award for Best Jazz Instrumental Group and another Grammy in 1998 for "an instrumental composition for the film" "Round Midnight".
Early life.
Carter was born in Ferndale, Michigan. He started to play cello at the age of 10, but when his family moved to Detroit, he ran into difficulties performing on cello due the racial stereotyping of classical musicians, the vast majority of whom where caucasian at that time. Carter switched to playing double bass. He attended Cass Technical High School in Detroit, and, later, the Eastman School of Music in Rochester, New York, where he played in its Philharmonic Orchestra. He finished his bachelor's degree at Eastman in 1959, and in 1961 a master's degree in double bass performance from the Manhattan School of Music in New York City.
His first jobs as a jazz musician were playing bass with Jaki Byard and Chico Hamilton. His first records were made with Eric Dolphy (another former member of Hamilton's group) and Don Ellis, in 1960. His own first date as leader, "Where?", with Eric Dolphy, Charlie Persip, Mal Waldron, George Duvivier, and a date also with Dolphy called "Out There" with George Duvivier and Roy Haynes and Carter on cello; its advanced harmonies and concepts were in step with the third stream movement.
Career.
1960s-1980s.
Carter came to fame via the second great Miles Davis Quintet in the early 1960s, which also included Herbie Hancock, Wayne Shorter and drummer Tony Williams. Carter joined Davis's group in 1963, appearing on the album "Seven Steps to Heaven" and the follow-up "E.S.P.", the latter being the first album to feature only the full quintet. It also featured three of Carter's compositions (the only time he contributed compositions to Davis's group). He stayed with Davis until 1968 (when he was replaced by Dave Holland), and participated in a couple of studio sessions with Davis in 1969 and 1970. Although he played electric bass occasionally during this era of early jazz-rock fusion, he has subsequently stopped playing that instrument, and in the 2000s plays only double bass.
Carter also performed on some of Hancock, Williams and Shorter's recordings during the sixties for Blue Note Records. He was a sideman on many Blue Note recordings of the era, playing with Sam Rivers, Freddie Hubbard, Duke Pearson, Lee Morgan, McCoy Tyner, Andrew Hill, Horace Silver and many others.
After leaving Davis, Carter was for several years a mainstay of CTI Records, making albums under his own name and also appearing on many of the label's records with a diverse range of other musicians. Notable musical partnerships in the 1970s and 1980s included Joe Henderson, Houston Person, Hank Jones and Cedar Walton. During the 1970s he was a member of the New York Jazz Quartet. In 1986, Carter played double bass on "Big Man on Mulberry Street" on Billy Joel's album "The Bridge".
1990s-2000s.
In 1993, he won a Grammy Award for Best Jazz Instrumental Group and another Grammy in 1998 for "an instrumental composition for the film" "Round Midnight".
Carter is a Distinguished Professor Emeritus of the Music Department of The City College of New York, having taught there for twenty years, and received an honorary Doctorate from the Berklee College of Music in Spring 2005. He joined the faculty of the Juilliard School in New York City in 2008, teaching bass in the school's Jazz Studies program. Carter made an appearance in Robert Altman's 1996 film, "Kansas City". The end credits feature him and fellow bassist Christian McBride duetting on "Solitude".
Carter sits on the Advisory Committee of the Board of Directors of The Jazz Foundation of America and on the Honorary Founder's Committee. Carter has worked with the Jazz Foundation since its inception to save the homes and the lives of America's elderly jazz and blues musicians including musicians that survived Hurricane Katrina.
Carter appeared as himself in an episode of the HBO series "Treme" entitled "What Is New Orleans." Carter's authorized biography, "Ron Carter: Finding the Right Notes", by Dan Ouellette, was published by ArtistShare in 2008. In 2013, Carter was one of four judges at Jazz at Lincoln Center's 18th Annual Essentially Ellington competition and festival.
Discography.
As sideman.
With Pepper Adams
With Toshiko Akiyoshi
With Geri Allen
With Gene Ammons
With Roy Ayers
With Chet Baker
With Gato Barbieri
With Joey Baron
With Gary Bartz
With George Benson
With Bob Brookmeyer
With Ray Bryant
With Kenny Burrell
With Henry Butler
With Jaki Byard
With Donald Byrd
With Billy Cobham
With Alice Coltrane
With Harry Connick, Jr.
With Chick Corea
With Hank Crawford
With Tadd Dameron
With Miles Davis
With Eli Degibri
With Paul Desmond
With Eric Dolphy
With Lou Donaldson
With Charles Earland
With Don Ellis
With Art Farmer
With Roberta Flack
With Bill Frisell
With Johnny Frigo
With Red Garland
With Stan Getz
With Astrud Gilberto
With Giorgio
With Benny Golson
With Johnny Griffin
With Jim Hall
With Chico Hamilton
With Johnny Hammond
With Herbie Hancock
With Barry Harris
With Eddie Harris
With Gene Harris
With Coleman Hawkins
With Joe Henderson
With Andrew Hill
With Johnny Hodges
With Freddie Hubbard
With Bobby Hutcherson
With Jackie and Roy
With Milt Jackson
With Antonio Carlos Jobim
With Billy Joel
With Hank Jones
With Ivan "Boogaloo Joe" Jones
With Quincy Jones
With Sam Jones
With Steve Kuhn and Gary McFarland
With Yusef Lateef
With Hubert Laws
With Johnny Lytle
With Junior Mance
With Herbie Mann
With Arif Mardin
With Les McCann
With Howard McGhee
With Charles McPherson
With Meeco
With Helen Merrill
With Wes Montgomery
With James Moody
With Airto Moreira
With Idris Muhammad
With Oliver Nelson
With David "Fathead" Newman
With the New York Jazz Quartet
With Hermeto Pascoal
With Rosa Passos
With Duke Pearson
With Houston Person
With Austin Peralta
With Pony Poindexter
With Sam Rivers
With Shirley Scott
With Gil Scott-Heron
With Don Sebesky
With Bud Shank
With Marlena Shaw
With Woody Shaw
With Wayne Shorter
With Horace Silver
With Paul Simon
With Grace Slick
With Jimmy Smith
With Phoebe Snow
With Sonny Stitt
With Ed Summerlin
With Gábor Szabó
With Livingston Taylor
With Buddy Terry
With Ed Thigpen
With Bobby Timmons
With Charles Tolliver
With A Tribe Called Quest
With Stanley Turrentine
With McCoy Tyner
With Mal Waldron
With Cedar Walton
With Grover Washington Jr.
With Randy Weston
With Kai Winding
With Leo Wright

</doc>
<doc id="26003" url="https://en.wikipedia.org/wiki?curid=26003" title="Radian">
Radian

The radian is the standard unit of angular measure, used in many areas of mathematics. An angle's measurement in radians is numerically equal to the length of a corresponding arc of a unit circle; one radian is just under 57.3 degrees (when the arc length is equal to the radius). The unit was formerly an SI supplementary unit, but this category was abolished in 1995 and the radian is now considered an SI derived unit.
Separately, the SI unit of solid angle measurement is the steradian.
The radian is represented by the symbol rad (Unicode-encoded as ). An alternative symbol is c, the superscript letter c, for "circular measure", or the letter r, but both of those symbols are infrequently used as it can be easily mistaken for a degree symbol (°) or a radius (r). So for example, a value of 1.2 radians could be written as 1.2 rad, 1.2 r, 1.2, or 1.2.
Definition.
Radian describes the plane angle subtended by a circular arc as the length of the arc divided by the radius of the arc. One radian is the angle subtended at the center of a circle by an arc that is equal in length to the radius of the circle. More generally, the magnitude in radians of such a subtended angle is equal to the ratio of the arc length to the radius of the circle; that is, "θ" = "s" /"r", where "θ" is the subtended angle in radians, "s" is arc length, and "r" is radius. Conversely, the length of the enclosed arc is equal to the radius multiplied by the magnitude of the angle in radians; that is, "s" = "rθ".
As the ratio of two lengths, the radian is a "pure number" that needs no unit symbol, and in mathematical writing the symbol "rad" is almost always omitted. When quantifying an angle in the absence of any symbol, radians are assumed, and when degrees are meant the symbol ° is used.
It follows that the magnitude in radians of one complete revolution (360 degrees) is the length of the entire circumference divided by the radius, or 2π"r" /"r", or 2π. Thus 2π radians is equal to 360 degrees, meaning that one radian is equal to 180/π degrees.
History.
The concept of radian measure, as opposed to the degree of an angle, is normally credited to Roger Cotes in 1714. He described the radian in everything but name, and he recognized its naturalness as a unit of angular measure. The idea of measuring angles by the length of the arc was already in use by other mathematicians. For example, al-Kashi (c. 1400) used so-called "diameter parts" as units where one diameter part was radian and they also used sexagesimal subunits of the diameter part.
The term "radian" first appeared in print on 5 June 1873, in examination questions set by James Thomson (brother of Lord Kelvin) at Queen's College, Belfast. He had used the term as early as 1871, while in 1869, Thomas Muir, then of the University of St Andrews, vacillated between the terms "rad", "radial", and "radian". In 1874, after a consultation with James Thomson, Muir adopted "radian".
Conversions.
Conversion between radians and degrees.
As stated, one radian is equal to 180/π degrees. Thus, to convert from radians to degrees, multiply by 180/π.
For example:
Conversely, to convert from degrees to radians, multiply by π/180.
For example:
formula_7
Radians can be converted to turns (complete revolutions) by dividing the number of radians by 2π.
Radian to degree conversion derivation.
The length of circumference of a circle is given by formula_8, where formula_9 is the radius of the circle.
So the following equivalent relation is true:
formula_10a formula_11 sweep is needed to draw a full circle
By the definition of radian, a full circle represents:
Combining both the above relations:
Conversion between radians and gradians.
formula_17 radians are equal to one turn, which is by definition 400 gradians (400 gons or 400g). So, to convert from radians to gradians multiply by formula_18, and to convert from gradians to radians multiply by formula_19. For example,
Advantages of measuring in radians.
In calculus and most other branches of mathematics beyond practical geometry, angles are universally measured in radians. This is because radians have a mathematical "naturalness" that leads to a more elegant formulation of a number of important results.
Most notably, results in analysis involving trigonometric functions are simple and elegant when the functions' arguments are expressed in radians. For example, the use of radians leads to the simple limit formula
which is the basis of many other identities in mathematics, including
Because of these and other properties, the trigonometric functions appear in solutions to mathematical problems that are not obviously related to the functions' geometrical meanings (for example, the solutions to the differential equation formula_25, the evaluation of the integral formula_26, and so on). In all such cases it is found that the arguments to the functions are most naturally written in the form that corresponds, in geometrical contexts, to the radian measurement of angles.
The trigonometric functions also have simple and elegant series expansions when radians are used; for example, the following Taylor series for sin "x" :
If "x" were expressed in degrees then the series would contain messy factors involving powers of π/180: if "x" is the number of degrees, the number of radians is "y" = π"x" /180, so
Mathematically important relationships between the sine and cosine functions and the exponential function (see, for example, Euler's formula) are, again, elegant when the functions' arguments are in radians and messy otherwise.
Dimensional analysis.
Although the radian is a unit of measure, it is a dimensionless quantity. This can be seen from the definition given earlier: the angle subtended at the centre of a circle, measured in radians, is equal to the ratio of the length of the enclosed arc to the length of the circle's radius. Since the units of measurement cancel, this ratio is dimensionless.
Although polar and spherical coordinates use radians to describe coordinates in two and three dimensions, the unit is derived from the radius coordinate, so the angle measure is still dimensionless.
Use in physics.
The radian is widely used in physics when angular measurements are required. For example, angular velocity is typically measured in radians per second (rad/s). One revolution per second is equal to 2π radians per second.
Similarly, angular acceleration is often measured in radians per second per second (rad/s2).
For the purpose of dimensional analysis, the units of angular velocity and angular acceleration are s−1 and s−2 respectively.
Likewise, the phase difference of two waves can also be measured in radians. For example, if the phase difference of two waves is (k·2π) radians, where k is an integer, they are considered in phase, whilst if the phase difference of two waves is (k·2π + π), where k is an integer, they are considered in antiphase.
Multiples of radian units.
Metric prefixes have limited use with radians, and none in mathematics.
A milliradian (mrad) is a thousandth of a radian and a microradian (μrad) is a millionth of a radian, i.e. 103 mrad = 106 μrad = 1 rad.
There are 2π × 1000 milliradians (≈ 6283.185 mrad) in a circle. So a trigonometric milliradian is just under of a circle. This “real” trigonometric unit of angular measurement of a circle is in use by telescopic sight manufacturers using (stadiametric) rangefinding in reticles.
The divergence of laser beams is also usually measured in milliradians.
An approximation of the trigonometric milliradian (0.001 rad), known as the (angular) mil, is used by NATO and other military organizations in gunnery and targeting. Each angular mil represents of a circle and is % smaller than the trigonometric milliradian. For the small angles typically found in targeting work, the convenience of using the number 6400 in calculation outweighs the small mathematical errors it introduces. In the past, other gunnery systems have used different approximations to ; for example Sweden used the "streck" and the USSR used .
Being based on the milliradian, the NATO mil subtends roughly 1 m at a range of 1000 m (at such small angles, the curvature is negligible).
Smaller units like microradians (μrad) and nanoradians (nrad) are used in astronomy, and can also be used to measure the beam quality of lasers with ultra-low divergence. More common is arc second, which is radians (around 4.8481 microradians). Similarly, the prefixes smaller than milli- are potentially useful in measuring extremely small angles.

</doc>
<doc id="26004" url="https://en.wikipedia.org/wiki?curid=26004" title="Redshift (disambiguation)">
Redshift (disambiguation)

Redshift is a phenomenon in physics, especially astrophysics
Redshift or red shift can also refer to:

</doc>
<doc id="26006" url="https://en.wikipedia.org/wiki?curid=26006" title="Rom">
Rom

Rom, ROM, and variants, may refer to:

</doc>
<doc id="26007" url="https://en.wikipedia.org/wiki?curid=26007" title="Mass racial violence in the United States">
Mass racial violence in the United States

Mass racial violence in the United States, also called race riots, can include such disparate events as:
Anti-immigrant and anti-Catholic violence.
Riots defined by "race" have taken place between ethnic groups in the United States since as early as the pre-Revolution era of the 18th century. During the early-to-mid- 19th centuries, violent rioting occurred between Protestant "Nativists" and recently arrived Irish Catholic immigrants. These reached heights during the peak of immigration in the 1840s and 1850s in cities including New York, Philadelphia, and Boston. During the early 20th century, riots were common against Irish and French-Canadian immigrants in Providence, Rhode Island.
The San Francisco Vigilance Movements of 1851 and 1856 are often described by sympathetic historians as responses to rampant crime and government corruption. But, recent historians have noted that the vigilantes had a nativist bias; they systematically attacked first Irish immigrants, and later Mexicans, Chileans who came as miners during the California Gold Rush, and Chinese immigrants. During the early 20th century, racial or ethnic violence was directed by whites against Filipinos, Japanese and Armenians in California, who had arrived in waves of immigration.
During the late 19th century and early 20th century, Italian Americans were subject to racial violence. In 1891, eleven Italians were lynched by a mob of thousands in New Orleans. In the 1890s a total of twenty Italians were lynched in the South. Anti-Polish violence also occurred in the same time period.
Nineteenth-century events.
Like lynchings, race riots often had their roots in economic tensions or in white defense of the color line. 
In 1887, for example, ten thousand workers at sugar plantations in Louisiana, organized by the Knights of Labor, went on strike for an increase in their pay to $1.25 a day. Most of the workers were black, but some were white, infuriating Governor Samuel Douglas McEnery, who declared that "God Almighty has himself drawn the color line." The militia was called in, but withdrawn to give free rein to a lynch mob in Thibodaux. The mob killed between 20 and 300 blacks. A black newspaper described the scene:
In 1891, a mob lynched Joe Coe, a black worker in Omaha, Nebraska suspected of attacking a young white woman from South Omaha. Approximately 10,000 white people, mostly ethnic immigrants from South Omaha, reportedly swarmed the courthouse and took Coe from his jail cell, beating and then lynching him. Reportedly 6,000 people visited Coe's corpse during a public exhibition at which pieces of the lynching rope were sold as souvenirs. This was a period when even officially sanctioned executions, such as hangings, were regularly conducted in public.
Twentieth-century events.
Labor and immigrant conflict was a source of tensions that catalyzed into the East St. Louis riot of 1917. White rioters, many of them ethnic immigrants, killed an estimated 100 black residents of East St. Louis, after black residents had killed two white policemen, mistaking the car they were riding in for a previous car of white occupants who drove through a black neighborhood and fired randomly into a crowd of blacks. White-on-Black race riots include the Atlanta riots (1906), the Omaha and Chicago riots (1919), part of a series of riots in the volatile post-World War I environment, and the Tulsa riots (1921).
The Chicago race riot of 1919 grew out of tensions on the Southside, where Irish descendants and African Americans competed for jobs at the stockyards, and where both were crowded into substandard housing. The Irish descendants had been in the city longer, and were organized around athletic and political clubs.
A young black Chicagoan, Eugene Williams, paddled a raft near a Southside Lake Michigan beach into "white territory", and drowned after being hit by a rock thrown by a young white man. Witnesses pointed out the killer to a policeman, who refused to make an arrest. An indignant black mob attacked the officer. Violence broke out across the city. White mobs, many of them organized around Irish athletic clubs, began pulling black people off trolley cars, attacking black businesses, and beating victims with baseball bats and iron bars. Having learned from the East St. Louis riot, the city closed down the street car system, but the rioting continued. A total of 23 blacks and 15 whites were killed.
The 1921 Tulsa race riot grew out of economic competition, as the black Greenwood area was compared to Wall Street, and filled with independent businesses. In the immediate event, blacks resisted whites who tried to lynch 19-year-old Dick Rowland, who worked at shoeshines. Thirty-nine people (26 black, 13 white) were confirmed killed. An early 21st century investigation of these events has suggested that the number of casualties could be much higher. White mobs set fire to the black Greenwood district, destroying 1,256 homes and as many as 200 businesses. Fires leveled 35 blocks of residential and commercial neighborhood. Black people were rounded up by the Oklahoma National Guard and put into several internment centers, including a baseball stadium. White rioters in airplanes shot at black refugees and dropped improvised kerosene bombs and dynamite on them.
By the 1960s, decades of racial, economic, and political forces, which generated inner city poverty, resulted in "race riots" within minority areas in cities across the United States. The beating and rumored death of cab driver John Smith by police, sparked the 1967 Newark riots. This event became, per capita, one of the deadliest civil disturbances of the 1960s. The long and short term causes of the Newark riots are explored in depth in the documentary film "Revolution '67" and many news reports of the times. The riots in Newark spread across the United States in most major cities and over 100 deaths were reported. Many inner city neighborhoods in these cities were destroyed. The assassinations of Rev. Martin Luther King, Jr. in Memphis, Tennessee and later of Robert Kennedy in Los Angeles in 1968 also led to nationwide rioting across the country with similar mass deaths. During the same time period, and since then, violent acts committed against African-American churches and their members have been commonplace.
The 1980s and '90s saw a number of riots tied to longstanding racial tensions between police and minority communities. The 1980 Miami riots occurred following the death of an African-American motorist at the hands of four white Miami-Dade Police officers who were subsequently acquitted on charges of manslaughter and evidence tampering. Similarly, the six-day 1992 Los Angeles riots erupted after the acquittal of four white LAPD officers who had been filmed beating Rodney King, an African-American motorist. Khalil Gibran Muhammad, the Director of the Harlem-based Schomburg Center for Research in Black Culture has identified over 100 instances of mass racial violence in the United States since 1935 and has noted that almost every instance was precipitated by a police incident.
Twenty-first-century events.
The trend from the late 20th century continued with unrest due to perceived unfair policing of minority communities. The Cincinnati riots of 2001 were caused by the killing of 19-year-old African-American Timothy Thomas by white police officer Stephen Roach, who was subsequently acquitted on charges of negligent homicide. The 2014 Ferguson unrest occurred against a backdrop of racial tension between police and the black community of Ferguson, Missouri in the wake of the shooting of Michael Brown; similar incidents elsewhere such as the shooting of Trayvon Martin sparked smaller and isolated protests. According to the Associated Press' annual poll of United States news directors and editors, the top news story of 2014 was police killings of unarmed blacks—including the shooting of Michael Brown—as well as their investigations and the protests in their aftermath.

</doc>
<doc id="26010" url="https://en.wikipedia.org/wiki?curid=26010" title="Robert Johnson">
Robert Johnson

Robert Leroy Johnson (May 8, 1911 – August 16, 1938) was an American blues singer-songwriter and musician. His landmark recordings in 1936 and 1937 display a combination of singing, guitar skills, and songwriting talent that has influenced later generations of musicians. Johnson's shadowy and poorly documented life and death at age 27 have given rise to much legend, including the Faustian myth that he sold his soul to the devil at a crossroads to achieve success. As an itinerant performer who played mostly on street corners, in juke joints, and at Saturday night dances, Johnson had little commercial success or public recognition in his lifetime.
It was only after the reissue of his recordings in 1961, on the LP "King of the Delta Blues Singers", that his work reached a wider audience. Johnson is now recognized as a master of the blues, particularly of the Mississippi Delta blues style. He is credited by many rock musicians as an important influence; Eric Clapton has called Johnson "the most important blues singer that ever lived." Johnson was inducted into the Rock and Roll Hall of Fame as an early influence in its first induction ceremony, in 1986. In 2010, David Fricke ranked Johnson fifth in "Rolling Stone" magazine's "100 Greatest Guitarists of All Time".
Life and career.
Early life.
Johnson was born in Hazlehurst, Mississippi, possibly on May 8, 1911, to Julia Major Dodds (born October 1874) and Noah Johnson (born December 1884). Julia was married to Charles Dodds (born February 1865), a relatively prosperous landowner and furniture maker, with whom she had ten children. Charles Dodds had been forced by a mob to leave Hazlehurst following a dispute with white landowners. Julia left Hazlehurst with baby Robert but after some two years sent him to live in Memphis with her husband, who had changed his name to Charles Spencer.
About 1919, Robert rejoined his mother in the Mississippi Delta area around Tunica and Robinsonville, Mississippi. Julia's new husband, known as Dusty Willis, was 24 years her junior. Robert was remembered by some residents as "Little Robert Dusty", but he was registered at Tunica's Indian Creek School as Robert Spencer. In the 1920 census he is listed as Robert Spencer, living in Lucas, Arkansas, with Will and Julia Willis. Robert was at school in 1924 and 1927 and the quality of his signature on his marriage certificate suggests that he was relatively well educated for a boy of his background. One school friend, Willie Coffee, was located and filmed, recalling that Robert was already noted for playing the harmonica and jaw harp. He also remembered that Robert was absent for long periods, which suggests that he may have been living and studying in Memphis.
After school, Robert adopted the surname of his natural father, signing himself as Robert Johnson on the certificate of his marriage to sixteen-year-old Virginia Travis in February 1929. She died in childbirth shortly after. Surviving relatives of Virginia told the blues researcher Robert "Mack" McCormick that this was a divine punishment for Robert's decision to sing secular songs, known as "selling your soul to the Devil". McCormick believes that Johnson himself accepted the phrase as a description of his resolve to abandon the settled life of a husband and farmer to become a full-time blues musician.
Around this time, the blues musician Son House moved to Robinsonville, where his musical partner Willie Brown lived. Late in life, House remembered Johnson as a "little boy" who was a competent harmonica player but an embarrassingly bad guitarist. Soon after, Johnson left Robinsonville for the area around Martinsville, close to his birthplace, possibly searching for his natural father. Here he perfected the guitar style of House and learned other styles from Isaiah "Ike" Zinnerman. Zinnerman was rumored to have learned supernaturally to play guitar by visiting graveyards at midnight.
When Johnson next appeared in Robinsonville, he seemed to have miraculously acquired a guitar technique. House was interviewed at a time when the legend of Johnson's pact with the devil was well known among blues researchers. He was asked whether he attributed Johnson's technique to this pact, and his equivocal answers have been taken as confirmation.
While living in Martinsville, Johnson fathered a child with Vergie Mae Smith. He married Caletta Craft in May 1931. In 1932, the couple moved to Clarksdale, Mississippi, in the Delta. Here Caletta died in childbirth, and Johnson left for a career as a "walking" or itinerant musician.
Itinerant musician.
From 1932 until his death in 1938, Johnson moved frequently between large cities like Memphis, Tennessee, and Helena, Arkansas, and the smaller towns of the Mississippi Delta and neighboring regions of Mississippi and Arkansas. On occasion, he traveled much farther. The blues musician Johnny Shines accompanied him to Chicago, Texas, New York, Canada, Kentucky, and Indiana. Henry Townsend shared a musical engagement with him in St. Louis. In many places he stayed with members of his large extended family or with women friends. He did not marry again but formed some long-term relationships with women to whom he would return periodically. One was Estella Coleman, the mother of the blues musician Robert Lockwood, Jr. In other places he stayed with a woman he seduced at his first performance. In each location, Johnson's hosts were largely ignorant of his life elsewhere. He used different names in different places, employing at least eight distinct surnames.
Biographers have looked for consistency from musicians who knew Johnson in different contexts: Shines, who traveled extensively with him; Lockwood, who knew him as his mother's partner; David "Honeyboy" Edwards, whose cousin Willie Mae Powell had a relationship with Johnson. From a mass of partial, conflicting, and inconsistent eyewitness accounts, biographers have attempted to summarize Johnson's character. "He was well mannered, he was soft spoken, he was indecipherable". "As for his character, everyone seems to agree that, while he was pleasant and outgoing in public, in private he was reserved and liked to go his own way". "Musicians who knew Johnson testified that he was a nice guy and fairly average—except, of course, for his musical talent, his weakness for whiskey and women, and his commitment to the road."
When Johnson arrived in a new town, he would play for tips on street corners or in front of the local barbershop or a restaurant. Musical associates have said that in live performances Johnson often did not focus on his dark and complex original compositions, but instead pleased audiences by performing more well-known pop standards of the day – and not necessarily blues. With an ability to pick up tunes at first hearing, Johnson had no trouble giving his audiences what they wanted, and certain of his contemporaries later remarked on Johnson's interest in jazz and country music. Johnson also had an uncanny ability to establish a rapport with his audience; in every town in which he stopped, Johnson would establish ties to the local community that would serve him well when he passed through again a month or a year later.
Fellow musician Shines was 17 when he met Johnson in 1933. He estimated Johnson was maybe a year older than himself. In Samuel Charters' "Robert Johnson", Shines describes Johnson:
During this time Johnson established what would be a relatively long-term relationship with Estella Coleman, a woman about 15 years his senior and the mother of musician Robert Lockwood, Jr. Johnson reportedly cultivated a woman to look after him in each town he played in. He supposedly asked homely young women living in the country with their families whether he could go home with them, and in most cases he was accepted, until a boyfriend arrived or Johnson was ready to move on.
In 1941, Alan Lomax learned from Muddy Waters that Johnson had performed in the Clarksdale, Mississippi area. By 1959, historian Samuel Charters could add only that Will Shade of the Memphis Jug Band remembered Johnson had once briefly played with him in West Memphis, Arkansas. In the last year of his life, Johnson is believed to have traveled to St. Louis and possibly Illinois, and then to some states in the East. In 1938, Columbia Records producer John H. Hammond, who owned some of Johnson's records, had record producer Don Law seek out Johnson to book him for the first "From Spirituals to Swing" concert at Carnegie Hall in New York. On learning of Johnson's death, Hammond replaced him with Big Bill Broonzy, but still played two of Johnson's records from the stage.
Recording sessions.
In Jackson, Mississippi, around 1936, Johnson sought out H. C. Speir, who ran a general store and also acted as a talent scout. Speir put Johnson in touch with Ernie Oertle, who, as a salesman for the ARC group of labels, introduced Johnson to Don Law to record his first sessions in San Antonio, Texas. The recording session was held on November 23, 1936, in room 414 of the Gunter Hotel in San Antonio, which Brunswick Records had set up to be a temporary recording studio. In the ensuing three-day session, Johnson played 16 selections, and recorded alternate takes for most of these. Johnson reportedly performed facing the wall, which has been cited as evidence he was a shy man and reserved performer. This conclusion was played up in the inaccurate liner notes of the 1961 album "King of the Delta Blues Singers". Slide guitarist Ry Cooder speculates that Johnson played facing a corner to enhance the sound of the guitar, a technique he calls "corner loading".
Among the songs Johnson recorded in San Antonio were "Come On In My Kitchen", "Kind Hearted Woman Blues", "I Believe I'll Dust My Broom" and "Cross Road Blues". The first to be released were "Terraplane Blues" and "Last Fair Deal Gone Down", probably the only recordings of his that he would live to hear. "Terraplane Blues" became a moderate regional hit, selling 5,000 copies.
His first recorded song, "Kind Hearted Woman Blues", was part of a cycle of spin-offs and response songs that began with Leroy Carr's "Mean Mistreater Mama" (1934). According to Wald, it was "the most musically complex in the cycle" and stood apart from most rural blues as a through-composed lyric, rather than an arbitrary collection of more-or-less unrelated verses. In contrast to most Delta players, Johnson had absorbed the idea of fitting a composed song into the three minutes of a 78-rpm side. Most of Johnson's "somber and introspective" songs and performances come from his second recording session.
In 1937, Johnson traveled to Dallas, Texas, for another recording session with Don Law in a makeshift studio at the Vitagraph (Warner Brothers) Building, at 508 Park Avenue, where Brunswick Record Corporation was located on the third floor. Eleven records from this session would be released within the following year. Johnson did two takes of most of these songs and recordings of those takes survived. Because of this, there is more opportunity to compare different performances of a single song by Johnson than for any other blues performer of his time and place. Johnson recorded almost half of the 29 songs that make up his entire discography in Dallas.
Death.
Johnson died on August 16, 1938, at the age of 27, near Greenwood, Mississippi of unknown causes. Several differing accounts have described the events preceding his death. Johnson had been playing for a few weeks at a country dance in a town about from Greenwood. According to one theory, Johnson was murdered by the jealous husband of a woman with whom he had flirted. In an account by fellow blues legend Sonny Boy Williamson, Johnson had been flirting with a married woman at a dance, where she gave him a bottle of whiskey poisoned by her husband. When Johnson took the bottle, Williamson knocked it out of his hand, admonishing him to never drink from a bottle that he had not personally seen opened. Johnson replied, "Don't ever knock a bottle out of my hand." Soon after, he was offered another (poisoned) bottle and accepted it. Johnson is reported to have begun feeling ill the evening after and had to be helped back to his room in the early morning hours. Over the next three days his condition steadily worsened and witnesses reported that he died in a convulsive state of severe pain. Musicologist Robert "Mack" McCormick claimed to have tracked down the man who murdered Johnson and to have obtained a confession from him in a personal interview, but he has declined to reveal the man's name.
While strychnine has been suggested as the poison that killed Johnson, at least one scholar has disputed the notion. Tom Graves, in his book "Crossroads: The Life and Afterlife of Blues Legend Robert Johnson", relies on expert testimony from toxicologists to argue that strychnine has such a distinctive odor and taste that it cannot be disguised, even in strong liquor. Graves also claims that a significant amount of strychnine would have to be consumed in one sitting to be fatal, and that death from the poison would occur within hours, not days. Johnson's contemporary David "Honeyboy" Edwards similarly noted that the poison could not have been strychnine, since Johnson would have died much more rapidly, instead of suffering for three days.
LeFlore County registrar Cornelia Jordan, after conducting an investigation into Johnson's death for the state director of Vital Statistics, R.N. Whitfield, wrote on Johnson's death certificate: I talked with the white man on whose place this negro died and I also talked with a negro woman on the place. The plantation owner said the negro man, seemingly about 26 years old, came from Tunica two or three weeks before he died to play banjo at a negro dance given there on the plantation. He stayed in the house with some of the negroes saying he wanted to pick cotton. The white man did not have a doctor for this negro as he had not worked for him. He was buried in a homemade coffin furnished by the county. The plantation owner said it was his opinion that the man died of syphilis.
Gravesite.
The exact location of his grave is officially unknown; three different markers have been erected at possible sites in church cemeteries burial outside Greenwood.
An interviewee in the documentary "The Search for Robert Johnson" (1991) suggests that owing to poverty and lack of transportation Johnson is most likely to have been buried in a pauper's grave (or "potter's field") very near where he died.
Devil legend.
According to legend, as a young man living on a plantation in rural Mississippi, Johnson had a tremendous desire to become a great blues musician. He was instructed to take his guitar to a crossroad near Dockery Plantation at midnight. There he was met by a large black man (actually the Devil) who took the guitar and tuned it. The Devil played a few songs and then returned the guitar to Johnson, giving him mastery of the instrument. This was in effect, a deal with the Devil mirroring the legend of Faust. In exchange for his soul, Johnson was able to create the blues for which he became famous.
Various accounts.
This legend was developed over time and has been chronicled by Gayle Dean Wardlow, Edward Komara and Elijah Wald, who sees the legend as largely dating from Johnson's rediscovery by white fans more than two decades after his death. Son House once told the story to Pete Welding as an explanation of Johnson's astonishingly rapid mastery of the guitar. Welding reported it as a serious belief in a widely read article in "Down Beat" in 1966. Other interviewers failed to elicit any confirmation from House and there were fully two years between House's observation of Johnson as first a novice and then a master.
Further details were absorbed from the imaginative retellings by Greil Marcus and Robert Palmer. Most significantly, the detail was added that Johnson received his gift from a large black man at a crossroads. There is dispute as to how and when the crossroads detail was attached to the Robert Johnson story. All the published evidence, including a full chapter on the subject in the biography "Crossroads" by Tom Graves, suggests an origin in the story of blues musician Tommy Johnson. This story was collected from his musical associate Ishman Bracey and his elder brother Ledell in the 1960s. One version of Ledell Johnson's account was published in David Evans's 1971 biography of Tommy Johnson, and was repeated in print in 1982 alongside House's story in the widely read "Searching for Robert Johnson."
In another version, Ledell placed the meeting not at a crossroads but in a graveyard. This resembles the story told to Steve LaVere that Ike Zinnerman of Hazlehurst, Mississippi learned to play the guitar at midnight while sitting on tombstones. Zinnerman is believed to have influenced the playing of the young Robert Johnson. Recent research by blues scholar Bruce Conforth, in "Living Blues" magazine, makes the story clearer. Johnson and Ike Zimmerman did practice in a graveyard at night, because it was quiet and no one would disturb them, but it was not the Hazlehurst cemetery as had been believed. Zimmerman (his actual name as it was reportedly spelled on census records for the family going back into the early 1800s, his social security card, social security death notice, funeral program, and by his daughters) was not from Hazlehurst but nearby Beauregard, Mississippi. And he didn't practice in one graveyard, but in several in the area. Johnson spent about a year living with and learning from Zimmerman, who ultimately accompanied Johnson back to the Delta to look after him.
While Dockery, Hazlehurst and Beauregard have each been claimed as the locations of the mythical crossroads, there are also tourist attractions claiming to be "The Crossroads" in both Clarksdale and Memphis. Local residents of Rosedale, Mississippi, claim Johnson sold his soul to the devil at the intersection of Highways 1 and 8 in their town, while the 1986 movie "Crossroads" was filmed in Beulah, Mississippi. Blues historian Steve Cheseborough writes that it may be impossible to discover the exact location of the mythical crossroads, because "Robert Johnson was a rambling guy".
Interpretations.
Some scholars have argued that the devil in these songs may refer not only to the Christian figure of Satan but also to the African trickster god Legba, himself associated with crossroads. Folklorist Harry M. Hyatt wrote that, during his research in the South from 1935–1939, when African-Americans born in the 19th or early-20th century said they or anyone else had "sold their soul to the devil at the crossroads," they had a different meaning in mind. Hyatt claimed there was evidence indicating African religious retentions surrounding Legba and the making of a "deal" (not selling the soul in the same sense as in the Faustian tradition cited by Graves) with this so-called "devil" at the crossroads.
This view that the devil in Johnson's songs is derived from an African deity was disputed by the blues scholar David Evans in an essay published in 1999, "Demythologizing the Blues":
The musicologist Alan Lomax dismissed the myth by stating "In fact, every blues fiddler, banjo picker, harp blower, piano strummer and guitar framer was, in the opinion of both himself and his peers, a child of the Devil, a consequence of the black view of the European dance embrace as sinful in the extreme".
Musical style.
Johnson is considered a master of the blues, particularly of the Delta blues style. Keith Richards, of the Rolling Stones, said in 1990, "You want to know how good the blues can get? Well, this is it." But according to Elijah Wald, in his book "Escaping the Delta", Johnson in his own time was most respected for his ability to play in a wide range of styles, from raw country slide guitar to jazz and pop licks, and for his ability to pick up guitar parts almost instantly upon hearing a song. His first recorded song, "Kind Hearted Woman Blues," in contrast to the prevailing Delta style of the time, more resembled the style of Chicago or St. Louis, with "a full-fledged, abundantly varied musical arrangement." Unusual for a Delta player of the time, a recording exhibits what Johnson could do entirely outside of a blues style. "They're Red Hot", from his first recording session, shows that he was also comfortable with an "uptown" swing or ragtime sound similar to that of the Harlem Hamfats, but as Wald remarked, "no record company was heading to Mississippi in search of a down-home Ink Spots ... e could undoubtedly have come up with a lot more songs in this style if the producers had wanted them." Myers adds:
Voice.
An important aspect of Johnson's singing was his use of microtonality. These subtle inflections of pitch help explain why his singing conveys such powerful emotion. Eric Clapton described Johnson's music as "the most powerful cry that I think you can find in the human voice." In two takes of "Me and the Devil Blues" he shows a high degree of precision in the complex vocal delivery of the last verse: "The range of tone he can pack into a few lines is astonishing." The song's "hip humor and sophistication" is often overlooked. "enerations of blues writers in search of wild Delta primitivism," wrote Wald, have been inclined to overlook or undervalue aspects that show Johnson as a polished professional performer.
Johnson is also known for using the guitar as "the other vocalist in the song", a technique later perfected by B. B. King and his personified guitar named Lucille': "In Africa and in Afro-American tradition, there is the tradition of the talking instrument, beginning with the drums ... the one-strand and then the six-strings with bottleneck-style performance; it becomes a competing voice ...or a complementary voice ... in the performance."
Bob Dylan wrote that "When Johnson started singing, he seemed like a guy who could have sprung from the head of Zeus in full armor. I immediately differentiated between him and anyone else I had ever heard. The songs weren't customary blues songs. They were so utterly fluid. At first they went by quick, too quick to even get. They jumped all over the place in range and subject matter, short punchy verses that resulted in some panoramic story-fires of mankind blasting off the surface of this spinning piece of plastic."
Instrument.
Johnson mastered the guitar, being considered today one of the all-time greats on the instrument. His approach was highly complex and extremely advanced musically. When Keith Richards was first introduced to Johnson's music by his bandmate Brian Jones, he asked, "Who is the other guy playing with him?", not realizing it was Johnson playing one guitar. "I was hearing two guitars, and it took a long time to actually realise he was doing it all by himself," said Richards, who later stated that "Robert Johnson was like an orchestra all by himself." "As for his guitar technique, it's politely reedy but ambitiously eclectic—moving effortlessly from hen-picking and bottleneck slides to a full deck of chucka-chucka rhythm figures."
Lyrics.
In "The Story with Dick Gordon", Bill Ferris, of American Public Media, said, "Robert Johnson I think of in the same way I think of the British Romantic poets, Keats and Shelley, who burned out early, who were geniuses at wordsmithing poetry ... The Blues, if anything, are deeply sexual. You know, 'my car doesn't run, I'm gonna check my oil' ... 'if you don't like my apples, don't shake my tree'. Every verse has sexuality associated with it."
Influences.
Johnson fused approaches specific to Delta blues to those from the broader music world. The slide guitar work on "Rambling on My Mind" is pure Delta and Johnson's vocal there has "a touch of ... Son House rawness," but the train imitation on the bridge is not at all typical of Delta blues, and is more like something out of minstrel show music or vaudeville. Johnson did record versions of "Preaching the Blues" and "Walking Blues" in the older bluesman's vocal and guitar style (House's chronology has been questioned by Guralnick). As with the first take of "Come On In My Kitchen," the influence of Skip James is evident in James's "Devil Got My Woman", but the lyrics rise to the level of first-rate poetry, and Johnson sings with a strained voice found nowhere else in his recorded output.
The sad, romantic "Love in Vain" successfully blends several of Johnson's disparate influences. The form, including the wordless last verse, follows Leroy Carr's last hit "When the Sun Goes Down"; the words of the last sung verse come directly from a song Blind Lemon Jefferson recorded in 1926. Johnson's last recording, "Milkcow's Calf Blues" is his most direct tribute to Kokomo Arnold, who wrote "Milkcow Blues" and who influenced Johnson's vocal style.
"From Four Until Late" shows Johnson's mastery of a blues style not usually associated with the Delta. He croons the lyrics in a manner reminiscent of Lonnie Johnson, and his guitar style is more that of a ragtime-influenced player like Blind Blake. Lonnie Johnson's influence on Robert Johnson is even clearer in two other departures from the usual Delta style: "Malted Milk" and "Drunken Hearted Man". Both copy the arrangement of Lonnie Johnson's "Life Saver Blues". The two takes of "Me and the Devil Blues" show the influence of Peetie Wheatstraw, calling into question the interpretation of this piece as "the spontaneous heart-cry of a demon-driven folk artist."
Legacy.
Johnson has had enormous impact on music and musicians—but outside his own time and place and even the genre for which he was famous. His influence on contemporaries was much smaller, due in part to the fact that he was an itinerant performer—playing mostly on street corners, in juke joints, and at Saturday night dances—who worked in a then undervalued style of music. He also died young after recording only a handful of songs. Johnson, though well-traveled and admired in his performances, was little noted in his lifetime, his records even less so. "Terraplane Blues", sometimes described as Johnson's only hit record, outsold his others, but was still only a minor success.
If one had asked black blues fans about Robert Johnson in the first twenty years after his death, writes Elijah Wald, "the response in the vast majority of cases would have been a puzzled 'Robert who?'" This lack of recognition extended to black musicians: "As far as the evolution of black music goes, Robert Johnson was an extremely minor figure, and very little that happened in the decades following his death would have been affected if he had never played a note". Columbia Records released the album "King of the Delta Blues Singers", a compilation of Johnson's recordings, in 1961, which introduced his work to a much wider audience—fame and recognition he only received long after his death.
Rock and roll.
Johnson's greatest influence has been on genres of music that developed after his death: rock and roll and rock. The Rock and Roll Hall of Fame included four of his songs in a set of 500 they deemed to have shaped the genre:
Johnson recorded these songs a decade and a half before the advent of rock and roll, dying a year or two later. The Museum inducted him as an early influence in their first induction ceremony in 1986, almost a half century after his death. Marc Meyers, of the "Wall Street Journal", wrote that "His 'Stop Breakin' Down Blues' from 1937 is so far ahead of its time that the song could easily have been a rock demo cut in 1954."
Rock music and related genres.
Many of the artists who claim to have been influenced by Johnson the most, injecting his revolutionary stylings into their work and recording tribute songs and collections, are prominent rock musicians from the United Kingdom. His impact on these musicians—who contributed to and helped to define rock and roll and rock music—came from the compilation of his works released in 1961 by Columbia Records ("King of the Delta Blues Singers").
Brian Jones, of the Rolling Stones, introduced his bandmate Keith Richards to his first Robert Johnson album. The blues master's recordings would have as much impact on him as on Mick Jagger. The group performed his "Walkin' Blues" at the Rock and Roll Circus in 1968. They arranged their own version of "Love in Vain" for their album "Let It Bleed" and recorded "Stop Breakin' Down Blues" for "Exile on Main Street". Mick Jagger, in the role of Turner in the 1970 film "Performance", performed excerpts from "Come On In My Kitchen" and "Me and the Devil Blues."
Alexis Korner, who has been called "the Founding Father of British Blues", co-wrote and recorded a song entitled "Robert Johnson" for "The Party Album" released in 1978. Other examples of the influence he had on English blues and blues-rock musicians and musical groups include:
Sam Dunn's documentary "Metal Evolution" cites Johnson as the "great grandfather to all things heavy metal", with members of the bands Rush and Slipknot agreeing that he played a major role in the development of rock music.
Bob Dylan wrote of Johnson in his 2004 autobiography "", "If I hadn't heard the Robert Johnson record when I did, there probably would have been hundreds of lines of mine that would have been shut down—that I wouldn't have felt free enough or upraised enough to write."
Guitar technique.
Johnson's revolutionary guitar playing has led contemporary experts, assessing his talents through the handful of old recordings available, to rate him among the greatest guitar players of all time:
Musicians who proclaim Johnson's profound impact on them—including Keith Richards, Jimi Hendrix, and Eric Clapton—all rated in the top ten with him on each of these lists. The boogie bass line he fashioned for "I Believe I'll Dust My Broom" has now passed into the standard guitar repertoire. At the time it was completely new, a guitarist's version of something people would otherwise have heard only from a piano.
Lifetime achievement.
"The Complete Recordings", a double-disc box set released by Sony/Columbia Legacy on August 28, 1990, containing almost everything Robert Johnson ever recorded, with all 29 recordings (and 12 alternate takes) won a Grammy Award for “Best Historical Album” that year. In 2006 he was awarded a Grammy Lifetime Achievement Award (accepted by his son Claud).
Use in advertisements.
Johnson's recordings, such as "Sweet Home Chicago", have been used by companies and nonprofit organizations for marketing purposes. "Sweet Home Chicago" is played at many events in Chicago.
Problems of biography.
Very little of Johnson's early life is known with certainty. Two marriage licenses for Johnson have been located in county records offices. The ages given in these certificates point to different birth dates, as do the entries showing his attendance at Indian Creek School, in Tunica, Mississippi. That he was not listed among his mother's children in the 1910 census casts further doubt on these dates. Carrie Thompson claimed that her mother, who was also Robert's mother, remembered his birth date as May 8, 1911. The 1920 census gives his age as 7, suggesting he was born in 1912 or 1913. Five significant dates from his career are documented: Monday, Thursday and Friday, November 23, 26, and 27, 1936, at a recording session in San Antonio, Texas., and Saturday and Sunday, June 19 and 20, 1937, at a recording session in Dallas. His death certificate, discovered in 1968, lists the date and location of his death.
Johnson's records were admired by record collectors from the time of their first release and efforts were made to discover his biography, with virtually no success. Blues researcher Mack McCormick began researching his family background, but was never ready to publish. McCormick's research eventually became as much a legend as Johnson himself. In 1982, McCormick permitted Peter Guralnick to publish a summary in "Living Blues" (1982), later reprinted in book form as "Searching for Robert Johnson". Later research has sought to confirm this account or to add minor details. A revised summary acknowledging major informants was written by Stephen LaVere for the booklet accompanying the compilation album "Robert Johnson, The Complete Recordings" (1990), and is maintained with updates at the Delta Haze website. The documentary film "The Search for Robert Johnson" contains accounts by McCormick and Wardlow of what informants have told them: long interviews of David Honeyboy Edwards and Johnny Shines and short interviews of surviving friends and family. These published biographical sketches achieve coherent narratives, partly by ignoring reminiscences and hearsay accounts which contradict or conflict with other accounts.
A relatively full account of Johnson's brief musical career emerged in the 1960s, largely from accounts by Son House, Johnny Shines, David Honeyboy Edwards and Robert Lockwood. In 1961, the sleeve notes to the album "King of the Delta Blues Singers" included reminiscences of Don Law who had recorded Johnson in 1936. Law added to the mystique surrounding Johnson, representing him as very young and extraordinarily shy.
Photographs.
The two confirmed images of Johnson were located in 1973, in the possession of his half-sister Carrie Thompson, but were not widely published until the late 1980s. A third photo, purporting to show Johnson posing with the blues musician Johnny Shines, was published in the November 2008 issue of "Vanity Fair" magazine. It was declared authentic by the forensic artist Lois Gibson and by Johnson's estate in 2013. The authenticity of the third photo has been disputed by some music historians, including Elijah Wald and Gayle Dean Wardlow, who considered that the clothing suggests a date after Johnson's death and that the photograph may have been reversed and retouched. In December 2015 a fourth photograph was published, purportedly showing Johnson, his wife Calletta Craft, Estella Coleman, and Robert Lockwood Jr. This photograph was also declared authentic by Lois Gibson, but her identification of Johnson has been dismissed by other facial recognition experts and blues historians. In his book "Searching for Robert Johnson", Peter Guralnick stated that the blues archivist Mack McCormick showed him a photograph of Johnson with his nephew Louis, probably taken at the same time as the famous "pinstripe suit" photograph, showing Louis dressed in his United States Navy uniform. This photograph has never been made public.
Playback speed hypothesis.
In "The Guardian"'s music blog from May 2010, Jon Wilde speculated that Johnson's recordings may have been "accidentally speeded up when first committed to 78 records, or else were deliberately speeded up to make them sound more exciting." He does not give a source for this statement. Biographer Elijah Wald and other musicologists dispute this hypothesis on various grounds, including that Johnson's extant recordings were made on five different days, spread across two years at two different studios, making uniform speed changes or malfunctions highly improbable. In addition, fellow musicians, contemporaries and family who worked with or witnessed Johnson perform spoke of his recordings for more than 70 years preceding Wilde's hypothesis without ever suggesting that the speed of his performances had been altered.
Descendants.
Johnson left no will. In 1998, the Mississippi Supreme Court ruled that Claud Johnson, a retired truck driver living in Crystal Springs, Mississippi, was the son of Robert Johnson and his sole heir. The court heard that he had been born to Virgie Jane Smith (later Virgie Jane Cain), who had a relationship with Robert Johnson in 1931. The relationship was attested to by a friend, Eula Mae Williams, but other relatives descended from Robert Johnson's half-sister, Carrie Harris Thompson, contested Claud Johnson's claim. The effect of the judgment was to allow Claud Johnson to receive over $1 million in royalties. Claud Johnson died, aged 83, on June 30, 2015, leaving six children.
Discography.
Eleven 78-rpm records by Johnson were released by Vocalion Records during his lifetime. A twelfth was issued posthumously. Johnson's estate hold the copyrights to his songs.
"The Complete Recordings", a two-disc set, released on August 28, 1990, contains almost everything Johnson recorded, with all 29 recordings, and 12 alternate takes. (Another alternate take of "Traveling Riverside Blues" which was released by Sony on the CD "King of the Delta Blues Singers" and was included in early printings of the paperback edition of Elijah Wald's "Escaping the Delta".)
To celebrate Johnson's 100th birthday, May 8, 2011, Sony Legacy released "Robert Johnson: The Centennial Collection", a re-mastered 2-CD set of all 42 of his recordings and two brief fragments, one of Johnson practicing a guitar figure and the other of Johnson saying, presumably to engineer Don Law, "I wanna go on with our next one myself." Reviewers commented that the sound quality of the 2011 release was a substantial improvement on the 1990 release.
Awards and recognitions.
National Recording Registry.
The National Recording Preservation Board added "The Complete Recordings" to the National Recording Registry of the Library of Congress in 2003. The board annually selects songs that are "culturally, historically, or aesthetically significant" for inclusion in the Registry.
Rock and Roll Hall of Fame.
The Rock and Roll Hall of Fame included four songs by Johnson in its list of the "500 Songs That Shaped Rock and Roll". A memorial to him reads, "Robert Johnson stands at the crossroads of American music, much as a popular folk legend has it he once stood at Mississippi crossroads and sold his soul to the devil in exchange for guitar-playing prowess.
Honors and inductions.
On September 17, 1994, the U.S. Post Office issued a Robert Johnson 29-cent commemorative postage stamp.
Tribute albums.
Tribute albums to Robert Johnson include:

</doc>
<doc id="26011" url="https://en.wikipedia.org/wiki?curid=26011" title="Receptive aphasia">
Receptive aphasia

Receptive aphasia, also known as Wernicke's aphasia, fluent aphasia, or sensory aphasia, is a type of aphasia in which an individual is unable to understand language in its written or spoken form. Even though they can speak using grammar, syntax, rate, and intonation, they typically have difficulty expressing themselves meaningfully through speech. Receptive aphasia was named after Carl Wernicke who recognized this condition. People with receptive aphasia are typically unaware of how they are speaking and do not realize their speech may lack meaning. This is due to the fact that they have poor comprehension skills and do not understand their own speech. They typically remain unaware of even their most profound language deficits. When experienced with Broca's aphasia, the patient displays global aphasia.
Like many acquired language disorders, receptive aphasia can be experienced in many different ways and to many different degrees. While the typical case shows severely disturbed language comprehension, many individuals are still able to maintain conversations. Many may only experience difficulties with things such as accents and fast speech with the occasional speech error and can often carry out simple commands. Not all individuals show a complete loss of language comprehension. A common symptom of receptive aphasia is misinterpreting the meaning of words, gestures, pictures. For example, a patient with receptive aphasia may take the expression "it's raining cats and dogs" literally instead of figuratively. What is described here is what is referred to as a "textbook" example with the typical, fully expressed symptoms.
Receptive aphasia is not to be confused with Wernicke-Korsakoff syndrome or expressive aphasia.
Presentation.
Receptive aphasia results from damage to Wernicke's area located posterior to the lateral sulcus in the left hemisphere of the brain. This area is adjacent to the auditory cortex. The damage is most often the result of a stroke, although damage to Wernicke's area through blunt force trauma from accidents is another possible cause. While the onset of the disorder is typically very sudden, it is possible for the symptoms to begin gradually, with nonsensical utterances and word-finding issues appearing in the individual's speech.
With receptive aphasia an individual primarily loses their ability to comprehend language. This typically takes the form of both an inability to understand speech as well as written text. They also lose the ability to understand their own spoken language. This inability to understand language is usually accompanied with symptoms of Anosognosia: the individual is unaware of the disorder. When attempting to communicate with others they often rely on situational cues in order to maintain the conversation. Individuals typically retain almost all of their cognitive abilities outside of those related to understanding language. Receptive aphasia, unlike expressive aphasia often occurs without any motor deficits.
Individuals with receptive aphasia often display symptoms of Anomia (word-finding issues) and Paraphasia, perhaps because of their difficulty understanding their own speech, . A person with receptive aphasia speaks with normal prosody and intonation but uses random words, invents words, leaves out key words, substitutes words or verb tenses, pronouns, or prepositions, and utters sentences that do not make sense. Their expressive language is often devoid of any meaning. Other symptoms can include a loss of verbal pragmatic skills and conversational turn-taking. A person with receptive aphasia is usually unaware of how much they are speaking, so they may continue to talk even when they should pause to allow others to speak; this is often referred to as "press of speech." Despite their difficulties in forming sentences that make sense, they do speak fluently. Sentences are typically grammatically correct. This fluent, although nonsensical speech, is often referred to as "word salad". The juxtaposition between fluent speech and the lack of meaning is characteristic of receptive aphasia. Patients also display logorrhoea, a nonstop output of words during spontaneous speech. The rate of speech errors produced is variable, with some patients showing only 10% of productions being errors and others showing up to 80% of speech production being incorrect.
Below is an example of an interaction between an individual with severe receptive aphasia and a clinician: 
Below is an example of spontaneous speech from the same individual showing logorrhea symptoms:
As a result, these individuals often display logorrhea, a nonstop output of speech.
Words that are affective and more associated with emotions are retained. 
The ability to utter profanity is therefore unaffected due to the typical association of profanity with emotional outbursts and emphasis, rather than with the meaning of the word itself.
If Wernicke's area is damaged in the non-dominant hemisphere, the syndrome resulting will be sensory dysprosody—the inability to perceive the pitch, rhythm, and emotional tone of speech. This difference in impairment between left and right hemisphere lesions is caused by functional asymmetry; the designation of specific brain functions to either the left or right hemisphere. The main lateralized speech functions of the right hemisphere are the regulation, perception, and production of melodic speech—thereby causing right-hemisphere lesions to impair prosody. Furthermore, dysprosody also impairs the ability to interpret melodies, the ability to differentiate between male and female voices, and the ability to recognize familiar noises. Dysprosodic patients also experience exaggerated pathological emotional outbursts, despite showing less emotion through facial expressions. Dysprosody is believed to be an impairment to the ability to encode non-verbal information, as opposed to a cognitive deficit or an inability to experience emotion.
Patients who communicated using sign language before the onset of the aphasia experience analogous symptoms.
Causes.
Receptive aphasia is traditionally associated with neurological damage to Wernicke's area in the brain, (Brodmann area 22, in the posterior part of the superior temporal gyrus of the dominant hemisphere). Since Wernicke's area is responsible for "reading, thinking of what to write, and processing information", that is where we see many of the deficits associated with damage to this area. However, the key deficits of receptive aphasia do not come from damage to Wernicke's area; instead, most of the core difficulties are proposed to come from damage to the medial temporal lobe and underlying white matter. Receptive aphasia results from damage in the posterior one-third of the superior gyrus of the temporal lobe of the left hemisphere. Damage in this area not only destroys local language regions but also cuts off most of the occipital, temporal, and parietal regions from the core language region. While Wernicke's area is the site of language recognition, perception, interpretations, and understanding, that does not mean that it is wholly responsible for the comprehension of semantic meaning. Attempts to localize "receptive" language modules are generally futile, given the full range of interpretations of "receptive functionality". As Dennis C. Tanner states, "Although there may be … areas important in perceiving vowels and consonants, pinpointing the brain cells … understanding the implications of a Robert Frost poem is absurd." The reason for this is due to the many levels of semantic understanding—auditory perception, speech discrimination, denotative extraction, and dynamic symbolism. The process of full-depth semantic decoding has the capacity to engage virtually the whole brain. Therefore, it is more apt to think of Wernicke's area as an important conduit for a larger, all-encompassing process.
Assessment.
Receptive aphasia can be difficult to diagnose as the symptoms can be mistaken as a confused state due to stroke or blunt force trauma. In order for receptive aphasia to be diagnosed a complete language examination, especially of the auditory system, must be done. There are various diagnostic tests and measures used to determine whether a patient should be diagnosed with receptive aphasia.
Some examples of these assessments can be seen below: 
During assessment clinicians evaluate the patient's initial functioning and performance on the above tasks to form a baseline for treatment. This baseline can help them decide what type of treatment they can use and compare client's future progress with their initial abilities.
Treatment.
Many sources agree that patients often don't seek treatment due to their anosognosia and therefore lack of awareness that they could benefit from therapy. This apparent lack of concern surrounding their symptoms needs to be addressed before treatment can be initiated. In order for the treatment to be helpful, patients need to be cooperative and engaged in their therapy. Because each case of receptive aphasia presents itself differently, the treatment options are varied and use multiple techniques. Speech-language pathologists work to create therapeutic programs that are functional and effective for receptive aphasia patients. The patient's likelihood and prognosis of recovery is dependent upon their severity of symptoms and whether they maintained any auditory comprehension abilities. Across the range of symptoms in receptive aphasia, a patient's auditory comprehension deficits and poor self-monitoring must be initially addressed so that the patient can participate in language-based activities.
Furthermore, the type of auditory deficit found in a case of Wernicke's aphasia has been shown to be responsive to treatment of short term memory and working memory. Short term memory is used for verbal comprehension when a sentence must be mentally "repeated" in order to fully understand its content. Working memory is for a "second pass", after meaning is encoded. This is done in order to identify pronouns, to "act out" the meaning of the sentence mentally, to check that action for plausibility against long term memory, and to check meaning against syntax. After treatment to increase both short term memory and working memory, a patient with an auditory deficit characteristic of Wernicke's aphasia was able to recall far more words than previously possible, and began to include verbs in her recollections when prior to treatment she had included virtually no verbs. This variety of treatment focused on increasing memory in order to aid sentence comprehension appears promising.
Comprehension training.
Comprehension deficits as well as issues of pressure of speech can be improved through comprehension training. Comprehension training confronts the issues of pressure of speech by redirecting patients attention to listening rather than speaking. In this training the clinician will give the patient short instructions with contextual cues (such as facial expressions and gestures) and remind the patient to stop speaking while listening to these instructions through the use of these cues. The comprehension tasks used in this training involve listening to short, context-dependent instructions given by a clinician and initially responding by pointing to an object or picture. These tasks become gradually more difficult as therapy continues. The main goal of this therapy is to enhance patient's attention towards incoming information while simultaneously slowing and monitoring his or her own speech output. Therapists using this contextual approach have found they can increase speech comprehension from 2% to 90%.
Schuell's stimulation.
Schuell's stimulation is a well-known treatment, and is the most effective treatment. This treatment involves introducing the patient to strong, controlled, and intensive auditory stimulation. This immersion into intensive auditory stimulation is believed to increase neuronal firing causing an increase in neural activation. This neural activation is used as a facilitator to increase brain reorganization and therefore recovery of language in the patient.
Redistribution of brain activation allows uninjured parts of the brain, such as the frontal and right hemisphere to compensate for the injuries found in Wernicke's area. On comprehension tasks, the average person shows activation in Broca and Wernicke's areas in the left hemisphere of the brain with little activation in the right hemisphere. In contrast, a patient with receptive aphasia shows activation in the right hemisphere of the brain, providing evidence that neuroplasticity plays a role in recovery.
Social approach.
The social approach involves a collaborative effort on behalf of patients and clinicians to determine goals for therapy and also determining the most important functional outcomes that could improve the patient's everyday life. A conversational approach which is thought to provide opportunities for development and use of strategies to overcome barriers to communication.The main goals of this treatment are to improve the patient's conversational confidence and skills using conversational coaching, supported conversations, and partner training. 
Successful treatment incorporates these various treatment programs and approaches to facilitate patient's learning. In order to improve self-monitoring speech language pathologists will slow their own rate of speech, pausing between meaningful segments and encourage patients to do the same, slowing down their own speech, listening to themselves speak and monitoring their speech output.
It is also important to include patient's families in treatment programs so they can have speaking partners where they communicate the most, at home. Clinicians can teach family members how to support one another and adjust their speaking patterns to further facilitate their loved one's treatment and rehabilitation.
Luria's theory.
Luria proposed that this type of aphasia has three characteristics.

</doc>
<doc id="26022" url="https://en.wikipedia.org/wiki?curid=26022" title="Rehoboam">
Rehoboam

Rehoboam (pronounced ; ; ; ) was an Israelite king mentioned in the Hebrew Bible. According to I Kings and II Chronicles, he was initially king of the United Monarchy of Israel, but after the ten northern tribes of Israel rebelled in 932/931 BC to form the independent Kingdom of Israel, he remained as king of only the Kingdom of Judah, or southern kingdom. He was a son of Solomon and a grandson of David. His mother was Naamah the Ammonite.
One episode which the Bible places during the reign of Rehoboam, and which is confirmed by the records from the Bubastite Portal in Karnak and other archaeological find—without the specific mention of the name Rehoboam—is the Egyptian invasion of Palestine by the Egyptian pharaoh Shoshenq I, who is identified by many with the biblical King Shishak. One of the most difficult issues in identifying Shishak with Shoshenq I is the biblical statement that "King Shishak of Egypt attacked Jerusalem. He seized the treasures of the Lord's temple and the royal palace" (), making this Shoshenq's biggest prize, whereas the Bubastite Portal lists do not include Jerusalem or any city from central Judea among the surviving names in the list of Shoshenq's conquests.
Biblical background.
Solomon's wisdom and power were not sufficient to prevent the rebellion of several of his border cities. Damascus under Rezon secured its independence of Solomon; and Jeroboam, a superintendent of works, his ambition stirred by the words of the prophet Ahijah (I Kings xi. 29-40), fled to Egypt. Thus before the death of Solomon the apparently unified kingdom of David began to disintegrate. With Damascus independent and a powerful man of Ephraim, the most prominent of the Ten Tribes, awaiting his opportunity, the future of Solomon's kingdom became dubious.
Biblical narrative.
Conventional Bible chronology dates the start of Rehoboam's reign to the mid 10th century BC. His reign is described in and and in In the Hebrew Bible, Rehoboam was 41 years old when he ascended the throne.
The assembly for the coronation of Solomon's successor, Rehoboam, was called at Shechem, the one sacredly historic city within the territory of the Ten Tribes. Before the coronation took place the assembly requested certain reforms in the policy followed by Rehoboam's father, Solomon. The reforms requested would materially reduce the royal exchequer and hence its power to continue the magnificence of Solomon's court. The older men counseled Rehoboam at least to speak to the people in a civil manner (it is not clear whether they counseled him to accept the demands). However, the new king sought the advice from the people he had grown up with, who advised the king to show no weakness to the people, and to tax them even more, which Rehoboam did. He proclaimed to the people, 
Although the ostensible reason was the heavy burden laid upon Israel because of Solomon's great outlay for buildings and for luxury of all kinds, the other reasons include the historical opposition between the north and the south. The two sections had acted independently until David, by his victories, succeeded in uniting all the tribes, though the Ephraimitic jealousy was ever ready to develop into open revolt. Religious considerations were also operative. The building of the Temple was a severe blow for the various sanctuaries scattered through the land, and the priests of the high places probably supported the revolt. Josephus (Ant., VIII., viii. 3) makes the rebels exclaim: " We leave to Rehoboam the Temple his father built."
Jeroboam and the people rebelled, with the ten northern tribes breaking away and forming a separate kingdom. The new breakaway kingdom continued to be called Kingdom of Israel, and was also known as Samaria, or Ephraim or the northern Kingdom. The realm Rehoboam was left with was called Kingdom of Judah.
Civil war.
Rehoboam went to war against the new Kingdom of Israel with a force of 180,000 soldiers. However, he was advised against fighting his brethren, and so returned to Jerusalem. The text reports that Israel and Judah were in a state of war throughout his 17-year reign.
Egyptian invasion.
In the 5th year of Rehoboam's reign Shishaq, king of Egypt, brought a huge army and took many cities. According to Joshua, son of Nadav, the mention in 2 Chron. 11, 6 sqq., that Rehoboam built fifteen fortified cities, indicates that the attack was not unexpected. The account in Chronicles states that Shishaq marched with 1,200 chariots, 60,000 horsemen and troops who came with him from Egypt: Libyans, Sukkites, and Kushites. Shishaq's armies captured all of the fortified towns leading to Jerusalem between Gezer and Gibeon. When they laid siege to Jerusalem, Rehoboam gave Shishaq all of the treasures out of the temple as a tribute. The Egyptian campaign cut off trade with south Arabia via Elath and the Negev that had been established during Solomon's reign. Judah became a vassal state of Egypt.
An account of this invasion from the Egyptian perspective can be found in the Shishaq Relief at the Bubastis Portal near the Temple of Amun at Karnak.
Succession.
Rehoboam had 18 wives and 60 concubines. They bore him 28 sons and 60 daughters. His wives included Mahalath, the daughter of Jerimoth the son of David, and Abihail, the daughter of Eliab the son of Jesse. His sons with Abihail were Jeush, Shemariah, and Zaham. After Abihail he married his cousin Maacah, daughter of Absalom, David's son. His sons with Maacah were Abijah, Attai, Ziza, and Shelomith. The names of his other wives, sons and all his daughters are not given.
Rehoboam reigned for 17 years. When he died he was buried beside his ancestors in Jerusalem. He was succeeded by his son Abijah.
Biblical chronology.
Using the information in Kings and Chronicles Edwin Thiele has calculated the date for the division of the kingdom is 931–930 BC. Thiele noticed that for the first seven kings of Israel (ignoring Zimri's inconsequential seven-day reign), the synchronisms to Judean kings fell progressively behind by one year for each king. Thiele saw this as evidence that the northern kingdom was measuring the years by a non-accession system (first partial year of reign was counted as year one), whereas the southern kingdom was using the accession method (it was counted as year zero). Once this was understood, the various reign lengths and cross-synchronisms for these kings was worked out, and the sum of reigns for both kingdoms produced 931/930 BC for the division of the kingdom when working backwards from the Battle of Qarqar in 853 BC.

</doc>
<doc id="26023" url="https://en.wikipedia.org/wiki?curid=26023" title="RS-232">
RS-232

In telecommunications, RS-232 is a standard for serial communication transmission of data. It formally defines the signals connecting between a "DTE" ("data terminal equipment") such as a computer terminal, and a "DCE" ("data circuit-terminating equipment" or "data communication equipment"), such as a modem. The RS-232 standard is commonly used in computer serial ports. The standard defines the electrical characteristics and timing of signals, the meaning of signals, and the physical size and pinout of connectors. The current version of the standard is "TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange", issued in 1997.
An RS-232 serial port was once a standard feature of a personal computer, used for connections to modems, printers, mice, data storage, uninterruptible power supplies, and other peripheral devices. However, RS-232 is hampered by low transmission speed, large voltage swing, and large standard connectors. In modern personal computers, USB has displaced RS-232 from most of its peripheral interface roles. Many computers do not come equipped with RS-232 ports and must use either an external USB-to-RS-232 converter or an internal expansion card with one or more serial ports to connect to RS-232 peripherals. Nevertheless, RS-232 devices are still used, especially in industrial machines, networking equipment, and scientific instruments.
Scope of the standard.
The Electronic Industries Association (EIA) standard RS-232-C as of 1969 defines:
The standard does not define such elements as the character encoding, the framing of characters, or error detection protocols. The character format and transmission bit rate are set by the serial port hardware which may also contain circuits to convert the internal logic levels to RS-232 compatible signal levels. The standard does not define bit rates for transmission, except that it says it is intended for bit rates lower than 20,000 bits per second.
History.
RS-232 was first introduced in 1962 by the "Radio Sector" of the EIA. The original DTEs were electromechanical teletypewriters, and the original DCEs were (usually) modems. When electronic terminals (smart and dumb) began to be used, they were often designed to be interchangeable with teletypewriters, and so supported RS-232. The C revision of the standard was issued in 1969 in part to accommodate the electrical characteristics of these devices.
Because the standard did not foresee the requirements of devices such as computers, printers, test instruments, POS terminals, and so on, designers implementing an RS-232 compatible interface on their equipment often interpreted the standard idiosyncratically. The resulting common problems were non-standard pin assignment of circuits on connectors, and incorrect or missing control signals. The lack of adherence to the standards produced a thriving industry of breakout boxes, patch boxes, test equipment, books, and other aids for the connection of disparate equipment. A common deviation from the standard was to drive the signals at a reduced voltage. Some manufacturers therefore built transmitters that supplied +5 V and −5 V and labeled them as "RS-232 compatible".
Later personal computers (and other devices) started to make use of the standard so that they could connect to existing equipment. For many years, an RS-232-compatible port was a standard feature for serial communications, such as modem connections, on many computers. It remained in widespread use into the late 1990s. In personal computer peripherals, it has largely been supplanted by other interface standards, such as USB. RS-232 is still used to connect older designs of peripherals, industrial equipment (such as PLCs), console ports, and special purpose equipment.
The standard has been renamed several times during its history as the sponsoring organization changed its name, and has been variously known as EIA RS-232, EIA 232, and, most recently as TIA 232. The standard continued to be revised and updated by the Electronic Industries Alliance and since 1988 by the Telecommunications Industry Association (TIA). Revision C was issued in a document dated August 1969. Revision D was issued in 1986. The current revision is "TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange", issued in 1997. Changes since Revision C have been in timing and details intended to improve harmonization with the CCITT standard V.24, but equipment built to the current standard will interoperate with older versions.
Related ITU-T standards include V.24 (circuit identification) and V.28 (signal voltage and timing characteristics).
In revision D of EIA-232, the D-subminiature connector was formally included as part of the standard (it was only referenced in the appendix of RS-232 C). The voltage range was extended to ±25 volts, and the circuit capacitance limit was expressly stated as 2500 pF. Revision E of EIA 232 introduced a new, smaller, standard D-shell 26-pin "Alt A" connector, and made other changes to improve compatibility with CCITT standards V.24, V.28 and ISO 2110.
Limitations of the standard.
Because RS-232 is used beyond the original purpose of interconnecting a terminal with a modem, successor standards have been developed to address the limitations. Issues with the RS-232 standard include:
Role in modern personal computers.
In the book "PC 97 Hardware Design Guide", Microsoft deprecated support for the RS-232 compatible serial port of the original IBM PC design. Today, RS-232 has mostly been replaced in personal computers by USB for local communications. Compared with RS-232, USB is faster, uses lower voltages, and has connectors that are simpler to connect and use. However, USB is limited by standard to no more than 5 meters of cable, thus favoring RS-232 when longer distances are needed. Both standards have software support in popular operating systems.
USB is designed to make it easy for device drivers to communicate with hardware. USB is more complex than the RS-232 standard because it includes a protocol for transferring data to devices. This requires more software to support the protocol used. There is no direct analog to the terminal emulator programs that let users communicate directly with serial ports.
Serial ports of personal computers are also sometimes used to directly control various hardware devices, such as relays or lamps. Personal computers may use a serial port to interface to devices such as uninterruptible power supplies. In some cases, serial data is not exchanged, but the control lines are used to signal conditions such as loss of power or low battery alarms. An application program can detect or change the state of RS-232 control lines in the registers of the serial hardware using only a few input/output instructions; by contrast, a USB interface requires software to decode the serial data.
Devices that convert between USB and RS-232 do not work with all software or on all personal computers.
In fields such as laboratory automation or surveying, RS-232 devices may continue to be used. PLCs, VFDs, servo drives, and CNC equipment are programmable via RS-232. Some manufacturers have responded to this demand: Toshiba re-introduced the DE-9M connector on the Tecra laptop.
Serial ports with RS-232 are also commonly used to communicate to headless systems such as servers, where no monitor or keyboard is installed, during boot when operating system is not running yet and therefore no network connection is possible. An RS-232 serial port can communicate to some embedded systems such as routers as an alternative to network mode of monitoring.
Physical interface.
In RS-232, user data is sent as a time-series of bits. Both synchronous and asynchronous transmissions are supported by the standard. In addition to the data circuits, the standard defines a number of control circuits used to manage the connection between the DTE and DCE. Each data or control circuit only operates in one direction, that is, signaling from a DTE to the attached DCE or the reverse. Because transmit data and receive data are separate circuits, the interface can operate in a full duplex manner, supporting concurrent data flow in both directions. The standard does not define character framing within the data stream, or character encoding.
Voltage levels.
The RS-232 standard defines the voltage levels that correspond to logical one and logical zero levels for the data transmission and the control signal lines. Valid signals are either in the range of +3 to +15 volts or the range −3 to −15 volts with respect to the "Common Ground" (GND) pin; consequently, the range between −3 to +3 volts is not a valid RS-232 level. For data transmission lines (TxD, RxD, and their secondary channel equivalents), logic one is defined as a negative voltage, the signal condition is called "mark". Logic zero is positive and the signal condition is termed "space". Control signals have the opposite polarity: the asserted or active state is positive voltage and the deasserted or inactive state is negative voltage. Examples of control lines include request to send (RTS), clear to send (CTS), data terminal ready (DTR), and data set ready (DSR).
The standard specifies a maximum open-circuit voltage of 25 volts: signal levels of ±5 V, ±10 V, ±12 V, and ±15 V are all commonly seen depending on the voltages available to the line driver circuit. Some RS-232 driver chips have inbuilt circuitry to produce the required voltages from a 3 or 5 volt supply. RS-232 drivers and receivers must be able to withstand indefinite short circuit to ground or to any voltage level up to ±25 volts. The slew rate, or how fast the signal changes between levels, is also controlled.
Because the voltage levels are higher than logic levels typically used by integrated circuits, special intervening driver circuits are required to translate logic levels. These also protect the device's internal circuitry from short circuits or transients that may appear on the RS-232 interface, and provide sufficient current to comply with the slew rate requirements for data transmission.
Because both ends of the RS-232 circuit depend on the ground pin being zero volts, problems will occur when connecting machinery and computers where the voltage between the ground pin on one end, and the ground pin on the other is not zero. This may also cause a hazardous ground loop. Use of a common ground limits RS-232 to applications with relatively short cables. If the two devices are far enough apart or on separate power systems, the local ground connections at either end of the cable will have differing voltages; this difference will reduce the noise margin of the signals. Balanced, differential serial connections such as RS-422, RS-485, and USB can tolerate larger ground voltage differences because of the differential signaling.
Unused interface signals terminated to ground will have an undefined logic state. Where it is necessary to permanently set a control signal to a defined state, it must be connected to a voltage source that asserts the logic 1 or logic 0 level, for example with a pullup resistor. Some devices provide test voltages on their interface connectors for this purpose.
Connectors.
RS-232 devices may be classified as Data Terminal Equipment (DTE) or Data Circuit-terminating Equipment (DCE); this defines at each device which wires will be sending and receiving each signal. According to the standard, male connectors have DTE pin functions, and female connectors have DCE pin functions. Other devices may have any combination of connector gender and pin definitions. Many terminals were manufactured with female connectors but were sold with a cable with male connectors at each end; the terminal with its cable satisfied the recommendations in the standard.
The standard recommends the D-subminiature 25-pin connector, but does not make it mandatory. Most devices only implement or use a few of the twenty signals specified in the standard, so connectors and cables with fewer pins are sufficient for most connections, more compact, and less expensive. Personal computer manufacturers replaced the DB-25M connector with the smaller DE-9M connector. This connector, with a different pinout (see Serial port § Pinouts), is prevalent for personal computers and associated devices.
Presence of a 25-pin D-sub connector does not necessarily indicate an RS-232-C compliant interface. For example, on the original IBM PC, a male D-sub was an RS-232-C DTE port (with a non-standard current loop interface on reserved pins), but the female D-sub connector on the same PC model was used for the parallel "Centronics" printer port. Some personal computers put non-standard voltages or signals on some pins of their serial ports.
Cables.
The standard does not define a maximum cable length, but instead defines the maximum capacitance that a compliant drive circuit must tolerate. A widely used rule of thumb indicates that cables more than long will have too much capacitance, unless special cables are used. By using low-capacitance cables, full speed communication can be maintained over larger distances up to about . For longer distances, other signal standards are better suited to maintain high speed.
Since the standard definitions are not always correctly applied, it is often necessary to consult documentation, test connections with a breakout box, or use trial and error to find a cable that works when interconnecting two devices. Connecting a fully standard-compliant DCE device and DTE device would use a cable that connects identical pin numbers in each connector (a so-called "straight cable"). "Gender changers" are available to solve gender mismatches between cables and connectors. Connecting devices with different types of connectors requires a cable that connects the corresponding pins according to the table above. Cables with 9 pins on one end and 25 on the other are common. Manufacturers of equipment with 8P8C connectors usually provide a cable with either a DB-25 or DE-9 connector (or sometimes interchangeable connectors so they can work with multiple devices). Poor-quality cables can cause false signals by crosstalk between data and control lines (such as Ring Indicator).
If a given cable will not allow a data connection, especially if a gender changer is in use, a null modem cable may be necessary. Gender changers and null modem cables are not mentioned in the standard, so there is no officially sanctioned design for them.
3-wire and 5-wire RS-232.
A minimal "3-wire" RS-232 connection consisting only of transmit data, receive data, and ground, is commonly used when the full facilities of RS-232 are not required. Even a two-wire connection (data and ground) can be used if the data flow is one way (for example, a digital postal scale that periodically sends a weight reading, or a GPS receiver that periodically sends position, if no configuration via RS-232 is necessary). When only hardware flow control is required in addition to two-way data, the RTS and CTS lines are added in a 5-wire version.
Data and control signals.
The following table lists commonly used RS-232 signals (called "circuits" in the specifications) and their pin assignments on the recommended DB-25 connectors. (See Serial port § pinouts) for other commonly used connectors not defined by the standard.)
The signals are named from the standpoint of the DTE. The ground pin is a common return for the other connections, and establishes the "zero" voltage to which voltages on the other pins are referenced. The DB-25 connector includes a second "protective ground" on pin 1; this is connected to equipment frame ground.
Ring Indicator.
"Ring Indicator" (RI), is a signal sent from the DCE to the DTE device. It indicates to the terminal device that the phone line is ringing. In many computer serial ports, a hardware interrupt is generated when the RI signal changes state. Having support for this hardware interrupt means that a program or operating system can be informed of a change in state of the RI pin, without requiring the software to constantly "poll" the state of the pin. RI does not correspond to another signal that carries similar information the opposite way.
On an external modem the status of the Ring Indicator pin is often coupled to the "AA" (auto answer) light, which flashes if the RI signal has detected a ring. The asserted RI signal follows the ringing pattern closely, which can permit software to detect distinctive ring patterns.
The Ring Indicator signal is used by some older uninterruptible power supplies (UPSs) to signal a power failure state to the computer.
Certain personal computers can be configured for wake-on-ring, allowing a computer that is suspended to answer a phone call.
RTS, CTS, and RTR.
The RTS and CTS signals were originally defined for use with half-duplex (one direction at a time) modems that disable their transmitters when not required, and must transmit a synchronization preamble to the receiver when they are re-enabled. The DTE asserts RTS to indicate a desire to transmit to the DCE, and in response the DCE asserts CTS to grant permission, once synchronization with the DCE at the far end is achieved. Such modems are no longer in common use. There is no corresponding signal that the DTE could use to temporarily halt incoming data from the DCE. Thus RS-232's use of the RTS and CTS signals, per the older versions of the standard, is asymmetric.
This scheme is also employed in present-day RS-232 to RS-485 converters. RS-485 is a multiple-access bus on which only one device can transmit at a time, a concept that is not provided for in RS-232. The RS-232 device asserts RTS to tell the converter to take control of the RS-485 bus so that the converter, and thus the RS-232 device, can send data onto the bus.
Modern communications environments use full-duplex (both directions simultaneously) modems. In that environment, DTEs have no reason to deassert RTS. However, due to the possibility of changing line quality, delays in processing of data, etc., there is a need for symmetric, bidirectional flow control.
A symmetric alternative providing flow control in both directions was developed and marketed in the late 1980s by various equipment manufacturers. It redefined the RTS signal to mean that the DTE is ready to receive data from the DCE. This scheme was eventually codified in version RS-232-E (actually TIA-232-E by that time) by defining a new signal, "RTR (Ready to Receive)," which is CCITT V.24 circuit 133. TIA-232-E and the corresponding international standards were updated to show that circuit 133, when implemented, shares the same pin as RTS (Request to Send), and that when 133 is in use, RTS is assumed by the DCE to be asserted at all times.
In this scheme, commonly called "RTS/CTS flow control" or "RTS/CTS handshaking" (though the technically correct name would be "RTR/CTS"), the DTE asserts RTR whenever it is ready to receive data from the DCE, and the DCE asserts CTS whenever it is ready to receive data from the DTE. Unlike the original use of RTS and CTS with half-duplex modems, these two signals operate independently from one another. This is an example of hardware flow control. However, "hardware flow control" in the description of the options available on an RS-232-equipped device does not always mean RTS/CTS handshaking.
Equipment using this protocol must be prepared to buffer some extra data, since a transmission may have begun just before the control line state change.
Seldom-used features.
The EIA-232 standard specifies connections for several features that are not used in most implementations. Their use requires 25-pin connectors and cables.
Signal rate selection.
The DTE or DCE can specify use of a "high" or "low" signaling rate. The rates as well as which device will select the rate must be configured in both the DTE and DCE. The prearranged device selects the high rate by setting pin 23 to ON.
Loopback testing.
Many DCE devices have a loopback capability used for testing. When enabled, signals are echoed back to the sender rather than being sent on to the receiver. If supported, the DTE can signal the local DCE (the one it is connected to) to enter loopback mode by setting pin 18 to ON, or the remote DCE (the one the local DCE is connected to) to enter loopback mode by setting pin 21 to ON. The latter tests the communications link as well as both DCE's. When the DCE is in test mode it signals the DTE by setting pin 25 to ON.
A commonly used version of loopback testing does not involve any special capability of either end. A hardware loopback is simply a wire connecting complementary pins together in the same connector (see "loopback").
Loopback testing is often performed with a specialized DTE called a bit error rate tester (or BERT).
Timing signals.
Some synchronous devices provide a clock signal to synchronize data transmission, especially at higher data rates. Two timing signals are provided by the DCE on pins 15 and 17. Pin 15 is the transmitter clock, or send timing (ST); the DTE puts the next bit on the data line (pin 2) when this clock transitions from OFF to ON (so it is stable during the ON to OFF transition when the DCE registers the bit). Pin 17 is the receiver clock, or receive timing (RT); the DTE reads the next bit from the data line (pin 3) when this clock transitions from ON to OFF.
Alternatively, the DTE can provide a clock signal, called transmitter timing (TT), on pin 24 for transmitted data. Data is changed when the clock transitions from OFF to ON, and read during the ON to OFF transition. TT can be used to overcome the issue where ST must traverse a cable of unknown length and delay, clock a bit out of the DTE after another unknown delay, and return it to the DCE over the same unknown cable delay. Since the relation between the transmitted bit and TT can be fixed in the DTE design, and since both signals traverse the same cable length, using TT eliminates the issue. TT may be generated by looping ST back with an appropriate phase change to align it with the transmitted data. ST loop back to TT lets the DTE use the DCE as the frequency reference, and correct the clock to data timing.
Synchronous clocking is required for such protocols as SDLC, HDLC, and X.25.
Secondary channel.
A secondary data channel, identical in capability to the primary channel, can optionally be implemented by the DTE and DCE devices. Pin assignments are as follows:
Related standards.
Other serial signaling standards may not interoperate with standard-compliant RS-232 ports. For example, using the TTL levels of near +5 and 0 V puts the mark level in the undefined area of the standard. Such levels are sometimes used with GPS receivers and depth finders.
A 20 mA current loop uses the absence of 20 mA current for high, and the presence of current in the loop for low; this signaling method is often used for long-distance and optically isolated links. Connection of a current-loop device to a compliant RS-232 port requires a level translator. Current-loop devices can supply voltages in excess of the withstand voltage limits of a compliant device. The original IBM PC serial port card implemented a 20 mA current-loop interface, which was never emulated by other suppliers of plug-compatible equipment.
Other serial interfaces similar to RS-232:
Development tools.
When developing or troubleshooting systems using RS-232, close examination of hardware signals can be important to find problems. A simple indicator device uses LEDs to show the high/low state of data or control pins. A serial line analyzer is a device similar to a logic analyzer but specialized for RS-232's voltage levels, connectors, and, where used, clock signals. The serial line analyzer can collect, store, and display the data and control signals, allowing developers to view them in detail. Some simply display the signals as waveforms; more elaborate versions include the ability to decode characters in ASCII or other common codes and to interpret common protocols used over RS-232 such as SDLC, HDLC, DDCMP, and X.25. Serial line analyzers are available as standalone units, as software and interface cables for general-purpose logic analyzers and oscilloscopes, and as programs that run on common personal computers and devices.

</doc>
<doc id="26024" url="https://en.wikipedia.org/wiki?curid=26024" title="RMS">
RMS

RMS may refer to:

</doc>
<doc id="26028" url="https://en.wikipedia.org/wiki?curid=26028" title="Relationship">
Relationship

Relationship(s) or relation(s) may refer to

</doc>
<doc id="26032" url="https://en.wikipedia.org/wiki?curid=26032" title="Running">
Running

Running is a method of terrestrial locomotion allowing humans and other animals to move rapidly on foot. Running is a type of gait characterized by an aerial phase in which all feet are above the ground (though there are exceptions). This is in contrast to walking, where one foot is always in contact with the ground, the legs are kept mostly straight and the center of gravity vaults over the stance leg or legs in an inverted pendulum fashion. A characteristic feature of a running body from the viewpoint of spring-mass mechanics is that changes in kinetic and potential energy within a stride occur simultaneously, with energy storage accomplished by springy tendons and passive muscle elasticity. The term running can refer to any of a variety of speeds ranging from jogging to sprinting.
It is assumed that the ancestors of mankind developed the ability to run for long distances about 2.6 million years ago, probably in order to hunt animals. Competitive running grew out of religious festivals in various areas. Records of competitive racing date back to the Tailteann Games in Ireland in 1829 BCE, while the first recorded Olympic Games took place in 776 BCE. Running has been described as the world's most accessible sport.
History.
It is thought that human running evolved at least four and a half million years ago out of the ability of the ape-like Australopithecus, an early ancestor of humans, to walk upright on two legs.
The theory proposed considered to be the most likely evolution of running is of early humans' developing as endurance runners from the practice of persistence hunting of animals, the activity of following and chasing until a prey is too exhausted to flee, succumbing to "chase myopathy" (Sears 2001), and that human features such as the nuchal ligament, abundant sweat glands, the Achilles tendons, big knee joints and muscular glutei maximi, were changes caused by this type of activity (Bramble & Lieberman 2004, et al.). The theory as first proposed used comparitative physiological evidence and the natural habits of animals when running, indicating the likelihood of this activity as a successful hunting method. Further evidence from observation of modern-day hunting practice also indicated this likelihood (Carrier et al. 1984).
Competitive running grew out of religious festivals in various areas such as Greece, Egypt, Asia, and the East African Rift in Africa. The Tailteann Games, an Irish sporting festival in honour of the goddess Tailtiu, dates back to 1829 BCE, and is one of the earliest records of competitive running. The origins of the Olympics and Marathon running are shrouded by myth and legend, though the first recorded games took place in 776 BCE. Running in Ancient Greece can be traced back to these games of 776 BCE.
Running kinematic description.
Running gait can be divided into two phases in regard to the lower extremity: stance and swing. These can be further divided into absorption, propulsion, initial swing and terminal swing. Due to the continuous nature of running gait, no certain point is assumed to be the beginning. However, for simplicity it will be assumed that absorption and footstrike mark the beginning of the running cycle in a body already in motion.
Footstrike.
Footstrike occurs when a plantar portion of the foot makes initial contact with the ground. Common footstrike types include forefoot, midfoot and heel strike types. These are characterized by initial contact of the ball of the foot, ball and heel of the foot simultaneously and heel of the foot respectively. During this time the hip joint is undergoing extension from being in maximal flexion from the previous swing phase. For proper force absorption, the knee joint should be flexed upon footstrike and the ankle should be slightly in front of the body. Footstrike begins the absorption phase as forces from initial contact are attenuated throughout the lower extremity. Absorption of forces continues as the body moves from footstrike to midstance due to vertical propulsion from the toe-off during a previous gait cycle.
Midstance.
Midstance is defined as the time at which the lower extremity limb of focus is in knee flexion directly underneath the trunk, pelvis and hips. It is at this point that propulsion begins to occur as the hips undergo hip extension, the knee joint undergoes extension and the ankle undergoes plantar flexion. Propulsion continues until the leg is extended behind the body and toe off occurs. This involves maximal hip extension, knee extension and plantar flexion for the subject, resulting in the body being pushed forward from this motion and the ankle/foot leaves the ground as initial swing begins.
Propulsion phase.
Most recent research, particularly regarding the footstrike debate, has focused solely on the absorption phases for injury identification and prevention purposes. The propulsion phase of running involves the movement beginning at midstance until toe off. From a full stride length model however, components of the terminal swing and footstrike can aid in propulsion.
Set up for propulsion begins at the end of terminal swing as the hip joint flexes, creating the maximal range of motion for the hip extensors to accelerate through and produce force. As the hip extensors change from reciporatory inhibitors to primary muscle movers, the lower extremity is brought back toward the ground, although aided greatly by the stretch reflex and gravity. Footstrike and absorption phases occur next with two types of outcomes. This phase can be only a continuation of momentum from the stretch reflex reaction to hip flexion, gravity and light hip extension with a heel strike, which does little to provide force absorption through the ankle joint. With a mid/forefoot strike, loading of the gastro-soleus complex from shock absorption will serve to aid in plantar flexion from midstance to toe-off.
As the lower extremity enters midstance, true propulsion begins. The hip extensors continue contracting along with help from the acceleration of gravity and the stretch reflex left over from maximal hip flexion during the terminal swing phase. Hip extension pulls the ground underneath the body, thereby pulling the runner forward. During midstance, the knee should be in some degree of knee flexion due to elastic loading from the absorption and footstrike phases to preserve forward momentum. The ankle joint is in dorsiflexion at this point underneath the body, either elastically loaded from a mid/forefoot strike or preparing for stand-alone concentric plantar flexion.
All three joints perform the final propulsive movements during toe-off. The plantar flexors plantar flex, pushing off from the ground and returning from dorsiflexion in midstance. This can either occur by releasing the elastic load from an earlier mid/forefoot strike or concentrically contracting from a heel strike. With a forefoot strike, both the ankle and knee joints will release their stored elastic energy from the footstrike/absorption phase. The quadriceps group/knee extensors go into full knee extension, pushing the body off of the ground. At the same time, the knee flexors and stretch reflex pull the knee back into flexion, adding to a pulling motion on the ground and beginning the initial swing phase. The hip extensors extend to maximum, adding the forces pulling and pushing off of the ground. The movement and momentum generated by the hip extensors also contributes to knee flexion and the beginning of the initial swing phase.
Swing phase.
Initial swing is the response of both stretch reflexes and concentric movements to the propulsion movements of the body. Hip flexion and knee flexion occur beginning the return of the limb to the starting position and setting up for another footstrike. Initial swing ends at midswing, when the limb is again directly underneath the trunk, pelvis and hip with the knee joint flexed and hip flexion continuing. Terminal swing then begins as hip flexion continues to the point of activation of the stretch reflex of the hip extensors. The knee begins to extend slightly as it swings to the anterior portion of the body. The foot then makes contact with the ground with footstrike, completing the running cycle of one side of the lower extremity.
Each limb of the lower extremity works opposite to the other. When one side is in toe-off/propulsion, the other side is in the swing/recovery phase preparing for footstrike. Following toe-off and the beginning of the initial swing of one side, there is a flight phase where neither extremity is in contact with the ground due to the opposite side finishing terminal swing. As the footstrike of one side occurs, initial swing continues. The opposing limbs meet with one in midstance and midswing, beginning the propulsion and terminal swing phases.
Upper extremity function.
Upper extremity function serves mainly in providing balance in conjunction with the opposing side of the lower extremity. The movement of each leg is paired with the opposite arm which serves to counterbalance the body, particularly during the stance phase. The arms move most effectively (as seen in elite athletes) with the elbow joint at an approximately 90 degrees or less, the hands swinging from the hips up to mid chest level with the opposite leg, the Humerus moving from being parallel with the trunk to approximately 45 degrees shoulder extension (never passing the trunk in flexion) and with as little movement in the transverse plane as possible. The trunk also rotates in conjunction with arm swing. It mainly serves as a balance point from which the limbs are anchored. Thus trunk motion should remain mostly stable with little motion except for slight rotation as excessive movement would contribute to transverse motion and wasted energy. 
Mechanics of Propulsion
Footstrike debate.
Recent research into various forms of running has focused on the differences in the potential injury risks and shock absorption capabilities between heel and mid/forefoot footstrikes. It has been shown that heel striking is generally associated with higher rates of injury and impact due to inefficient shock absorption and inefficient biomechanical compensations for these forces. This is due to forces from a heel strike traveling through bones for shock absorption rather than being absorbed by muscles. Since bones cannot disperse forces easily, the forces transmitted to other parts of the body, including ligaments, joints and bones in the rest of the lower extremity all the way up to the lower back. This causes the body to use abnormal compensatory motions in an attempt to avoid serious bone injuries. These compensations include internal rotation of the tibia, knee and hip joints. Excessive amounts of compensation over time have been linked to higher risk of injuries in those joints as well as the muscles involved in those motions. Conversely, a mid/forefoot strike has been associated with greater efficiency and lower injury risk due to the triceps surae being used as a lever system to absorb forces with the muscles eccentrically rather than through the bone. Landing with a mid/forefoot strike has also been shown to not only properly attenuate shock but allows the triceps surae to aid in propulsion via reflexive plantarflexion after stretching to absorb ground contact forces. Thus a mid/forefoot strike may aid in propulsion.
However, even among elite athletes there are variations in self selected footstrike types. This is especially true in longer distance events, where there is a prevalence of heel strikers. There does tend however to be a greater percentage of mid/forefoot striking runners in the elite fields, particularly in the faster racers and the winning individuals or groups. While one could attribute the faster speeds of elite runners compared to recreational runners with similar footstrikes to physiological differences, the hip and joints have been left out of the equation for proper propulsion. This brings up the question as to how heel striking elite distance runners are able to keep up such high paces with a supposedly inefficient and injurious footstrike technique.
Stride length, hip and knee function.
Biomechanical factors associated with elite runners include increased hip function, use and stride length over recreational runners. An increase in running speeds causes increased ground reaction forces and elite distance runners must compensate for this to maintain their pace over long distances.
These forces are attenuated through increased stride length via increased hip flexion and extension through decreased ground contact time and more force being used in propulsion. With increased propulsion in the horizontal plane, less impact occurs from decreased force in the vertical plane. Increased hip flexion allows for increased use of the hip extensors through midstance and toe-off, allowing for more force production.
The difference even between world class and national level distance runners has been associated with more efficient hip joint function. The increase in velocity likely comes from the increased range of motion in hip flexion and extension, allowing for greater acceleration and velocity. The hip extensors and hip extension have been linked to more powerful knee extension during toe-off, which contributes to propulsion.
Stride length must be properly increased with some degree of knee flexion maintained through the terminal swing phases, as excessive knee extension during this phase along with footstrike has been associated with higher impact forces due to braking and an increased prevalence of heel striking. Elite runners tend to exhibit some degree of knee flexion at footstrike and midstance, which first serves to eccentrically absorb impact forces in the quadriceps muscle group. Secondly it allows for the knee joint to concentrically contract and provides major aid in propulsion during toe-off as the quadriceps group is capable of produce large amounts of force.
Recreational runners have been shown to increase stride length through increased knee extension rather than increased hip flexion as exhibited by elite runners, which serves instead to provide an intense breaking motion with each step and decrease the rate and efficiency of knee extension during toe-off, slowing down speed. Knee extension however contributes to additional stride length and propulsion during toe-off and is seen more frequently in elite runners as well.
Elements of good running technique.
Upright posture and a slight forward lean.
Leaning forward places a runner's center of mass on the front part of the foot, which avoids landing on the heel and facilitates the use of the spring mechanism of the foot. It also makes it easier for the runner to avoid landing the foot in front of the center of mass and the resultant braking effect. While upright posture is essential, a runner should maintain a relaxed frame and use his/her core to keep posture upright and stable. This helps prevent injury as long as the body is neither rigid nor tense. The most common running mistakes are tilting the chin up and scrunching shoulders.
Stride rate and types.
Exercise physiologists have found that the stride rates are extremely consistent across professional runners, between 185 and 200 steps per minute. The main difference between long- and short-distance runners is the length of stride rather than the rate of stride.
During running, the speed at which the runner moves may be calculated by multiplying the cadence (steps per second) by the stride length. Running is often measured in terms of pace in minutes per mile or kilometer. Fast stride rates coincide with the rate one pumps one's arms. The faster one's arms move up and down, parallel with the body, the faster the rate of stride. Different types of stride are necessary for different types of running. When sprinting, runners stay on their toes bringing their legs up, using shorter and faster strides. Long distance runners tend to have more relaxed strides that vary.
Running injuries.
Many injuries are associated with running because of its high-impact nature. Change in running volume may lead to development of patellofemoral pain syndrome, iliotibial band syndrome, patellar tendinopathy, plica syndrome, and medial tibial stress syndrome. Change in running pace may cause Achilles Tendinitis, gastrocnemius injuries, and plantar fasciitis. Repetitive stress on the same tissues without enough time for recovery or running with improper form can lead to many of the above. Runners generally attempt to minimize these injuries by warming up before exercise, focusing on proper running form, performing strength training exercises, eating a well balanced diet, allowing time for recovery, and "icing" (applying ice to sore muscles or taking an ice bath).
Another common, running-related injury is chafing, caused by repetitive rubbing of one piece of skin against another, or against an article of clothing. One common location for chafe to occur is the runner's upper thighs. The skin feels coarse and develops a rash-like look. A variety of deodorants and special anti-chafing creams are available to treat such problems. Chafe is also likely to occur on the nipple.
Some runners may experience injuries when running on concrete surfaces. The problem with running on concrete is that the body adjusts to this flat surface running and some of the muscles will become weaker, along with the added impact of running on a harder surface. Therefore, it is advised to change terrain occasionally – such as trail, beach, or grass running. This is more unstable ground and allows the legs to strengthen different muscles. Runners should be wary of twisting their ankles on such terrain. Running downhill also increases knee stress and should therefore be avoided. Reducing the frequency and duration can also prevent injury.
Barefoot running has been promoted as a means of reducing running related injuries, but this remains controversial and a majority of professionals advocate the wearing of appropriate shoes as the best method for avoiding injury. However, a study in 2013 concluded that wearing neutral shoes is not associated with increased injuries.
Benefits of running.
While there exists potential for injury while running (just as there is in any sport), there are many benefits. Some of these benefits include potential weight loss, improved cardiovascular and respiratory health (reducing the risk of cardiovascular and respiratory diseases), improved cardiovascular fitness, reduced total blood cholesterol, strengthening of bones (and potentially increased bone density), possible strengthening of the immune system and an improved self-esteem and emotional state. Running, like all forms of regular exercise, can effectively slow or reverse the effects of aging.
Running can assist people in losing weight, staying in shape and improving body composition. Running increases your metabolism. Different speeds and distances are appropriate for different individual health and fitness levels. For new runners, it takes time to get into shape. The key is consistency and a slow increase in speed and distance. While running, it is best to pay attention to how one's body feels. If a runner is gasping for breath or feels exhausted while running, it may be beneficial to slow down or try a shorter distance for a few weeks. If a runner feels that the pace or distance is no longer challenging, then the runner may want to speed up or run farther.
Running can also have psychological benefits, as many participants in the sport report feeling an elated, euphoric state, often referred to as a "runner's high". Running is frequently recommended as therapy for people with clinical depression and people coping with addiction. A possible benefit may be the enjoyment of nature and scenery, which also improves psychological well-being (see Ecopsychology § Practical benefits).
In animal models, running has been shown to increase the number of newly born neurons within the brain. This finding could have significant implications in aging as well as learning and memory.
Whereby an optimal amount of vigorous aerobic exercise such as running might bring benefits related to lower cardiovascular disease and life extension, it should be noted that in an excessive dose (e.g., marathons) it might have an opposite effect associated with cardiotoxicity.
Running events.
Running is both a competition and a type of training for sports that have running or endurance components. As a sport, it is split into events divided by distance and sometimes includes permutations such as the obstacles in steeplechase and hurdles. Running races are contests to determine which of the competitors is able to run a certain distance in the shortest time. Today, competitive running events make up the core of the sport of athletics. Events are usually grouped into several classes, each requiring substantially different athletic strengths and involving different tactics, training methods, and types of competitors.
Running competitions have probably existed for most of humanity's history and were a key part of the ancient Olympic Games as well as the modern Olympics. The activity of running went through a period of widespread popularity in the United States during the running boom of the 1970s. Over the next two decades, as many as 25 million Americans were doing some form of running or jogging – accounting for roughly one tenth of the population. Today, road racing is a popular sport among non-professional athletes, who included over 7.7 million people in America alone in 2002.
Limits of speed.
Footspeed, or sprint speed, is the maximum speed at which a human can run. It is affected by many factors, varies greatly throughout the population, and is important in athletics and many sports.
The fastest human footspeed on record is 44.7 km/h (12.4 m/s, 27.8 mph), seen during a 100-meter sprint (average speed between the 60th and the 80th meter) by Usain Bolt.
Events by type.
Track running events are individual or relay events with athletes racing over specified distances on an oval running track. The events are categorised as sprints, middle and long-distance, and hurdling.
Road running takes place on a measured course over an established road (as opposed to track and cross country running). These events normally range from distances of 5 kilometers to longer distances such as half marathons and marathons, and they may involve large numbers of runners or wheelchair entrants.
Cross country running takes place over open or rough terrain. The courses used at these events may include grass, mud, woodlands, hills, flat ground and water. It is a popular participatory sport, and is one of the events which, along with track and field, road running, and racewalking, makes up the umbrella sport of athletics.
Events by distance.
Sprints.
Sprints are short running events in athletics and track and field. Races over short distances are among the oldest running competitions. The first 13 editions of the Ancient Olympic Games featured only one event – the stadion race, which was a race from one end of the stadium to the other. There are three sprinting events which are currently held at the Olympics and outdoor World Championships: the 100 metres, 200 metres, and 400 metres. These events have their roots in races of imperial measurements which were later altered to metric: the 100 m evolved from the 100 yard dash, the 200 m distances came from the furlong (or 1/8 of a mile), and the 400 m was the successor to the 440 yard dash or quarter-mile race.
At the professional level, sprinters begin the race by assuming a crouching position in the starting blocks before leaning forward and gradually moving into an upright position as the race progresses and momentum is gained. Athletes remain in the same lane on the running track throughout all sprinting events, with the sole exception of the 400 m indoors. Races up to 100 m are largely focused upon acceleration to an athlete's maximum speed. All sprints beyond this distance increasingly incorporate an element of endurance. Human physiology dictates that a runner's near-top speed cannot be maintained for more than thirty seconds or so as lactic acid builds up and leg muscles begin to be deprived of oxygen.
The 60 metres is a common indoor event and it an indoor world championship event. Other less-common events include the 50 metres, 55 metres, 300 metres and 500 metres which are used in some high and collegiate competitions in the United States. The 150 metres, though rarely competed, has a star-studded history: Pietro Mennea set a world best in 1983, Olympic champions Michael Johnson and Donovan Bailey went head-to-head over the distance in 1997, and Usain Bolt improved Mennea's record in 2009.
Middle distance.
Middle distance running events are track races longer than sprints up to 3000 metres. The standard middle distances are the 800 metres, 1500 metres and mile run, although the 3000 metres may also be classified as a middle distance event. The 880 yard run, or half mile, was the forebear to the 800 m distance and it has its roots in competitions in the United Kingdom in the 1830s. The 1500 m came about as a result of running three laps of a 500 m track, which was commonplace in continental Europe in the 1900s.
Long distance.
Examples of longer-distance running events are long distance track races, marathons, ultramarathons, and multiday races.

</doc>
<doc id="26034" url="https://en.wikipedia.org/wiki?curid=26034" title="René Magritte">
René Magritte

René François Ghislain Magritte (; 21 November 1898 – 15 August 1967) was a Belgian surrealist artist. He became well known for a number of witty and thought-provoking images. Often depicting ordinary objects in an unusual context, his work is known for challenging observers' preconditioned perceptions of reality. His imagery has influenced pop, minimalist and conceptual art.
Early life.
René Magritte was born in Lessines, in the province of Hainaut, Belgium, in 1898. He was the oldest son of Léopold Magritte, a tailor and textile merchant, and Régina (née Bertinchamps), who was a milliner before she got married. Little is known about Magritte's early life. He began lessons in drawing in 1910. On 12 March 1912, his mother committed suicide by drowning herself in the River Sambre. This was not her first attempt at taking her own life; she had made many over a number of years, driving her husband Léopold to lock her into her bedroom. One day she escaped, and was missing for days. Her body was later discovered a mile or so down the nearby river.
According to a legend, 13-year-old Magritte was present when her body was retrieved from the water, but recent research has discredited this story, which may have originated with the family nurse. Supposedly, when his mother was found, her dress was covering her face, an image that has been suggested as the source of several of Magritte's paintings in 1927–1928 of people with cloth obscuring their faces, including "Les Amants".
Career.
Magritte's earliest paintings, which date from about 1915, were Impressionistic in style. From 1916 to 1918, he studied at the Académie Royale des Beaux-Arts in Brussels, under Constant Montald, but found the instruction uninspiring. The paintings he produced during the years 1918–1924 were influenced by Futurism and by the figurative Cubism of Metzinger. 
In 1922, Magritte married Georgette Berger, whom he had met as a child in 1913. From December 1920 until September 1921, Magritte served in the Belgian infantry in the Flemish town of Beverlo near Leopoldsburg. In 1922–23, he worked as a draughtsman in a wallpaper factory, and was a poster and advertisement designer until 1926, when a contract with Galerie 'Le Centaure' in Brussels made it possible for him to paint full-time. In 1926, Magritte produced his first surreal painting, "The Lost Jockey" ("Le jockey perdu"), and held his first exhibition in Brussels in 1927. Critics heaped abuse on the exhibition. Depressed by the failure, he moved to Paris where he became friends with André Breton, and became involved in the surrealist group. The illusionistic, dream-like quality is characteristic of Magritte's version of Surrealism. He became a leading member of the movement after leaving his native Belgium in 1927 for Paris, where he stayed for three years.
Galerie 'Le Centaure' closed at the end of 1929, ending Magritte's contract income. Having made little impact in Paris, Magritte returned to Brussels in 1930 and resumed working in advertising. He and his brother, Paul, formed an agency which earned him a living wage.
During the early stages of his career, the British surrealist patron Edward James allowed Magritte to stay rent free in his London home and paint. James is featured in two of Magritte's works, "Le Principe du Plaisir (The Pleasure Principle)" and "La Reproduction Interdite", a painting also known as "Not to be Reproduced".
During the of Belgium in World War II he remained in Brussels, which led to a break with Breton. He briefly adopted a colorful, painterly style in 1943–44, an interlude known as his "Renoir Period", as a reaction to his feelings of alienation and abandonment that came with living in German-occupied Belgium. In 1946, renouncing the violence and pessimism of his earlier work, he joined several other Belgian artists in signing the manifesto "Surrealism in Full Sunlight". During 1947–48, Magritte's "Vache Period", he painted in a provocative and crude Fauve style. During this time, Magritte supported himself through the production of fake Picassos, Braques and Chiricos—a fraudulent repertoire he was later to expand into the printing of forged banknotes during the lean postwar period. This venture was undertaken alongside his brother Paul Magritte and fellow Surrealist and 'surrogate son' Marcel Mariën, to whom had fallen the task of selling the forgeries. At the end of 1948, he returned to the style and themes of his prewar surrealistic art.
His work was exhibited in the United States in New York in 1936 and again in that city in two retrospective exhibitions, one at the Museum of Modern Art in 1965, and the other at the Metropolitan Museum of Art in 1992.
Politically, Magritte stood to the left, and retained close ties to the Communist Party, even in the post-war years. However, he was critical of the functionalist cultural policy of the communist left, stating that "Class consciousness is as necessary as bread; but that does not mean that workers must be condemned to bread and water and that wanting chicken and champagne would be harmful. (...) For the Communist painter, the justification of artistic activity is to create pictures that can represent mental luxury". While remaining committed to the political left, he thus advocated a certain autonomy of art. On his religious views, Magritte was an agnostic.
Popular interest in Magritte's work rose considerably in the 1960s, and his imagery has influenced pop, minimalist and conceptual art. In 2005 he came 9th in the Walloon version of "De Grootste Belg" ("The Greatest Belgian"); in the Flemish version he was 18th.
Personal life.
Magritte married Georgette Berger in June 1922. Georgette was daughter of a butcher in Charleroi and had first met Magritte when she was only 13. They met again in Brussels in 1920 and Georgette subsequently became Magritte's model and muse. In 1936 Magritte's marriage got into trouble when he met a young artist, Sheila Legg, and began an affair. Magritte arranged for his friend, Paul Colinet, to entertain and distract Georgette, but this led to an affair between his wife and Colinet. Magritte and his wife did not reconcile until 1940.
Magritte died of pancreatic cancer on 15 August 1967, aged 68, and was interred in Schaerbeek Cemetery, Evere, Brussels.
Philosophical and artistic gestures.
Magritte's work frequently displays a collection of ordinary objects in an unusual context, giving new meanings to familiar things. The use of objects as other than what they seem is typified in his painting, 'The Treachery of Images' ("La trahison des images"), which shows a pipe that looks as though it is a model for a tobacco store advertisement. Magritte painted below the pipe ""Ceci n'est pas une pipe"" ("This is not a pipe"), which seems a contradiction, but is actually true: the painting is not a pipe, it is an "image" of a pipe. It does not "satisfy emotionally"—when Magritte was once asked about this image, he replied that of course it was not a pipe, just try to fill it with tobacco.
Magritte used the same approach in a painting of an apple: he painted the fruit and then used an internal caption or framing device to deny that the item was an apple. In these ""Ceci n'est pas"" works, Magritte points out that no matter how naturalistically we depict an object, we never do catch the item itself.
Among Magritte's works are a number of surrealist versions of other famous paintings. Elsewhere, Magritte challenges the difficulty of artwork to convey meaning with a recurring motif of an easel, as in his "The Human Condition" series (1933, 1935) or "The Promenades of Euclid" (1955) (wherein the spires of a castle are "painted" upon the ordinary streets which the canvas overlooks). In a letter to André Breton, he wrote of "The Human Condition" that it was irrelevant if the scene behind the easel differed from what was depicted upon it, "but the main thing was to eliminate the difference between a view seen from outside and from inside a room." The windows in some of these pictures are framed with heavy drapes, suggesting a theatrical motif.
Magritte's style of surrealism is more representational than the "automatic" style of artists such as Joan Miró. Magritte's use of ordinary objects in unfamiliar spaces is joined to his desire to create poetic imagery. He described the act of painting as "the art of putting colors side by side in such a way that their real aspect is effaced, so that familiar objects—the sky, people, trees, mountains, furniture, the stars, solid structures, graffiti—become united in a single poetically disciplined image. The poetry of this image dispenses with any symbolic significance, old or new."
René Magritte described his paintings as "visible images which conceal nothing; they evoke mystery and, indeed, when one sees one of my pictures, one asks oneself this simple question, 'What does that mean?'. It does not mean anything, because mystery means nothing either, it is unknowable."
Magritte's constant play with reality and illusion has been attributed to the early death of his mother. Psychoanalysts who have examined bereaved children have hypothesized that Magritte's back and forth play with reality and illusion reflects his "constant shifting back and forth from what he wishes—'mother is alive'—to what he knows—'mother is dead' ".
Artists influenced by Magritte.
Contemporary artists have been greatly influenced by René Magritte's stimulating examination of the fickleness of images. Some artists who have been influenced by Magritte's works include John Baldessari, Ed Ruscha, Andy Warhol, Jasper Johns, Jan Verdoodt, Martin Kippenberger, Duane Michals and Storm Thorgerson. Some of the artists' works integrate direct references and others offer contemporary viewpoints on his abstract fixations.
Magritte's use of simple graphic and everyday imagery has been compared to that of the Pop artists. His influence in the development of Pop art has been widely recognized, although Magritte himself discounted the connection. He considered the Pop artists' representation of "the world as it is" as "their error", and contrasted their attention to the transitory with his concern for "the feeling for the real, insofar as it is permanent." The 2006–2007 LACMA exhibition "Magritte and Contemporary Art: The Treachery of Images" examined the relationship between Magritte and contemporary art.
In popular culture.
The 1960s brought a great increase in public awareness of Magritte's work. Thanks to his "sound knowledge of how to present objects in a manner both suggestive and questioning", his works have been frequently adapted or plagiarized in advertisements, posters, book covers and the like. Examples include album covers such as "Beck-Ola" by The Jeff Beck Group (reproducing Magritte's "The Listening Room"), Alan Hull's 1972 album Pipedream which used "The Philosopher's Lamp", Jackson Browne's 1974 album "Late for the Sky", with artwork inspired by "The Empire of Light", Oregon's album "Oregon" referring to "Carte Blanche", and the Firesign Theatre's album "Just Folks... A Firesign Chat" based on "The Mysteries of the Horizon". Styx's album cover of "The Grand Illusion" incorporated a special adaptation of the painting, "The Blank Check", by Magritte as well.
The band Punch Brothers recently used The Lovers as the cover of their 2015 album The Phosphorescent Blues.
Paul Simon's song "Rene and Georgette Magritte with Their Dog after the War", inspired by a photograph of Magritte by Lothar Wolleh, appears on the 1983 album "Hearts and Bones".
John Cale wrote a song titled "Magritte". The song appears on the 2003 album "HoboSapiens".
Tom Stoppard has written a surrealist play called "After Magritte".
John Berger scripted the book "Ways of Seeing" using images and ideologies regarding Magritte. Douglas Hofstadter's book "Gödel, Escher, Bach" uses Magritte works for many of its illustrations. "The Treachery of Images" was used in a major plot in L. J. Smith's "The Forbidden Game."
Magritte's imagery has inspired filmmakers ranging from the surrealist Marcel Mariën to mainstream directors such as Jean-Luc Godard, Alain Robbe-Grillet, Bernardo Bertolucci, Nicolas Roeg, John Boorman and Terry Gilliam.
According to Ellen Burstyn, in the 1998 documentary "The Fear of God: 25 Years of "The Exorcist"", the iconic poster shot for the film "The Exorcist" was inspired by Magritte's "L'Empire des Lumières".
In the 1992 movie "Toys", Magritte's work was influential in the entire movie but specifically in a break-in scene, featuring Robin Williams and Joan Cusack in a music video hoax. Many of Magritte's works were used directly in that scene.
The cover art for Jesse Jagz's third studio album "" (2014) was inspired by Magritte's artistic works. 
Magritte Museum.
The Magritte Museum opened to the public on 30 May 2009 in Brussels. Housed in the five-level neo-classical Hotel Altenloh, on the Place Royale, it displays some 200 original Magritte paintings, drawings and sculptures including "The Return", "Scheherazade" and "The Empire of Lights". This multidisciplinary permanent installation is the biggest Magritte archive anywhere and most of the work is directly from the collection of the artist's widow, Georgette Magritte, and from Irene Hamoir Scutenaire, who was his primary collector. Additionally, the museum includes Magritte's experiments with photography from 1920 on and the short surrealist films he made from 1956 on.
Another museum is located at 135 Rue Esseghem in Brussels in Magritte's former home, where he lived with his wife from 1930 to 1954. A painting, "Olympia" (1948), a nude portrait of Magritte's wife by Magritte, was stolen from this museum on the morning of 24 September 2009 by two armed men. The stolen work is said to be worth about US$1.1 million. "Olympia" was returned to the museum early January 2012. The thieves handed back the painting because they were unable to sell it on the black market due to its fame.

</doc>
<doc id="26035" url="https://en.wikipedia.org/wiki?curid=26035" title="Rudolf Diesel">
Rudolf Diesel

Rudolf Christian Karl Diesel (; 18 March 185829 September 1913) was a German inventor and mechanical engineer, famous for the invention of the diesel engine and his mysterious death. Diesel was the subject of the 1942 film "Diesel".
Early life and education.
Diesel was born in Paris, France in 1858 the second of three children of Elise (née Strobel) and Theodor Diesel. His parents were Bavarian immigrants living in Paris. Theodor Diesel, a bookbinder by trade, left his home town of Augsburg, Bavaria, in 1848. He met his wife, a daughter of a Nuremberg merchant, in Paris in 1855 and became a leather goods manufacturer there.
Rudolf Diesel spent his early childhood in France, but at the outbreak of the Franco-Prussian War in 1870, his family (as were many other Germans) was forced to leave. They settled in London, England. Before the war's end, however, Diesel's mother sent 12-year-old Rudolf to Augsburg to live with his aunt and uncle, Barbara and Christoph Barnickel, to become fluent in German and to visit the "Königliche Kreis-Gewerbsschule" (Royal County Trade School), where his uncle taught mathematics.
At the age of 14, Diesel wrote a letter to his parents saying that he wanted to become an engineer. After finishing his basic education at the top of his class in 1873, he enrolled at the newly-founded Industrial School of Augsburg. Two years later, he received a merit scholarship from the Royal Bavarian Polytechnic of Munich, which he accepted against the wishes of his parents, who would rather have seen him start to work.
Career.
One of Diesel's professors in Munich was Carl von Linde. Diesel was unable to graduate with his class in July 1879 because he fell ill with typhoid. While waiting for the next examination date, he gained practical engineering experience at the "Gebrüder Sulzer Maschinenfabrik" (Sulzer Brothers Machine Works) in Winterthur, Switzerland. Diesel graduated in January 1880 with highest academic honours and returned to Paris, where he assisted his former Munich professor, Carl von Linde, with the design and construction of a modern refrigeration and ice plant. Diesel became the director of the plant one year later.
In 1883, Diesel married Martha Flasche, and continued to work for Linde, gaining numerous patents in both Germany and France.
In early 1890, Diesel moved to Berlin with his wife and children, Rudolf Jr, Heddy, and Eugen, to assume management of Linde's corporate research and development department and to join several other corporate boards there. As he was not allowed to use the patents he developed while an employee of Linde's for his own purposes, he expanded beyond the field of refrigeration. He first worked with steam, his research into thermal efficiency and fuel efficiency leading him to build a steam engine using ammonia vapour. During tests, however, the engine exploded and almost killed him. He spent many months in a hospital, followed by health and eyesight problems.
He then began designing an engine based on the Carnot cycle, and in 1893, soon after Karl Benz was granted a patent for his invention of the motor car in 1886, Diesel published a treatise entitled "Theorie und Konstruktion eines rationellen Wärmemotors zum Ersatz der Dampfmaschine und der heute bekannten Verbrennungsmotoren" "and Construction of a Rational Heat-engine to Replace the Steam Engine and Combustion Engines Known Today" and formed the basis for his work on and invention of the diesel engine.
Diesel understood thermodynamics and the theoretical and practical constraints on fuel efficiency. He knew that as much as 90% of the energy available in the fuel is wasted in a steam engine. His work in engine design was driven by the goal of much higher efficiency ratios. After experimenting with a Carnot Cycle engine, he developed his own approach. Eventually, he obtained a patent for his design for a compression-ignition engine. In his engine, fuel was injected at the end of compression and the fuel was ignited by the high temperature resulting from compression. From 1893 to 1897, Heinrich von Buz, director of MAN AG in Augsburg, gave Rudolf Diesel the opportunity to test and develop his ideas. Rudolf Diesel obtained patents for his design in Germany and other countries, including the U.S. ( and ).
He was inducted into the Automotive Hall of Fame in 1978.
Disappearance and death.
In the evening of September 29, 1913, Diesel boarded the post office steamer "Dresden" in Antwerp on his way to a meeting of the Consolidated Diesel Manufacturing company in London. He took dinner on board the ship and then retired to his cabin at about 10 p.m., leaving word to be called the next morning at 6:15 a.m.; but he was never seen alive again. In the morning his cabin was empty and his bed had not been slept in, although his nightshirt was neatly laid out and his watch had been left where it could be seen from the bed. His hat and overcoat were discovered neatly folded beneath the afterdeck railing.
Ten days later, the crew of the Dutch boat "Coertsen" came upon the corpse of a man floating in the North Sea near Norway. The body was in such an advanced state of decomposition that it was unrecognisable, and they did not bring it aboard. Instead, the crew retrieved personal items (pill case, wallet, I.D. card, pocket knife, eyeglass case) from the clothing of the dead man, and returned the body to the sea. On October 11, 1913 it was reported that Diesel's body was found at the mouth of the Scheldt by a boatman but he was forced to throw it overboard because of heavy weather.
There are various theories to explain Diesel's death. His biographers, such as Grosser in 1978, present a case for suicide, and clearly consider it most likely. Conspiracy theories suggest that various people's business or military interests may have provided motives for murder, however evidence is limited for all explanations.
Shortly after Diesel's disappearance, his wife Martha opened a bag that her husband had given to her just before his ill-fated voyage, with directions that it should not be opened until the following week. She discovered 200,000 German marks in cash and a number of financial statements indicating that their bank accounts were virtually empty. In a diary Diesel brought with him on the ship, for the date 29 September 1913, a cross was drawn, indicating death. 
Legacy.
After Diesel's death, the diesel engine underwent much development and became a very important replacement for the steam piston engine in many applications. Because the diesel engine required a heavier, more robust construction than a gasoline engine, it was not widely used in aviation (but see aircraft diesel engine). The diesel engine became widespread in many other applications, however, such as stationary engines, agricultural machines, submarines, ships, and much later, locomotives, trucks, and in modern automobiles.
Diesel engines are most often found in applications where a high torque requirement and low RPM requirement exist. Because of their generally more robust construction and high torque, diesel engines have also become the workhorses of the trucking industry. Recently, diesel engines that have overcome their weight penalty have been designed, certified, and flown in light aircraft. These engines are designed to run on either diesel fuel or more commonly jet fuel.
The diesel engine has the benefit of running more fuel-efficiently than gasoline engines due to much higher compression ratios and longer duration of combustion, which means the temperature rises more slowly, allowing more heat to be converted to mechanical work. Diesel was interested in using coal dust or vegetable oil as fuel, and in fact, his engine was run on peanut oil.
Although these fuels were not immediately popular, during 2008 rises in fuel prices, coupled with concerns about oil reserves, have led to more widespread use of vegetable oil and biodiesel. The primary source of fuel remains what became known as diesel fuel, an oil by-product derived from refinement of petroleum, which is safer to store than gasoline (its flash point is approximately 175 degrees higher) and will not explode.
Use of vegetable oils as diesel engine fuel.
In the foreword to a book titled "Diesel Engines for Land and Marine Work", Rudolf Diesel states, "In 1900 a small Diesel engine was exhibited by the Otto company which, on the suggestion of the French Government, was run on Arachide oil, and operated so well that very few people were aware of the fact. The motor was built for ordinary oils, and without any modification was run on vegetable oil." Diesel went on to say that "I have recently repeated these experiments on a large scale with full success and entire confirmation of the results formerly obtained."
Patent dispute with Herbert Akroyd Stuart.
Akroyd-Stuart's "compression ignition" engine (as opposed to "spark-ignition") was patented two years earlier than Diesel's similar engine; Diesel's patentable idea was to increase the pressure. Due to the lower pressures used, the hot bulb engine, with an internal pressure of about , as opposed to the diesel engine's c. , had only about a 12% thermal efficiency versus more than 50% for some large Diesels. Details of the claim, that a patent submitted by Herbert Akroyd Stuart pre-dated that of Rudolf Diesel, may be found under the name of that inventor.
The high compression and thermal efficiency is what distinguishes the patent granted to Diesel from a hot bulb engine patent.

</doc>
