<doc id="28892" url="https://en.wikipedia.org/wiki?curid=28892" title="Skara Brae">
Skara Brae

Skara Brae is a stone-built Neolithic settlement, located on the Bay of Skaill on the west coast of Mainland, the largest island in the Orkney archipelago of Scotland. It consists of eight clustered houses, and was occupied from roughly 3180 BC–2500 BC. Europe's most complete Neolithic village, Skara Brae gained UNESCO World Heritage Site status as one of four sites making up "The Heart of Neolithic Orkney." Older than Stonehenge and the Great Pyramids, it has been called the "Scottish Pompeii" because of its excellent preservation.
Discovery and early exploration.
In the winter of 1850, a severe storm hit Scotland, causing widespread damage and over 200 deaths. In the Bay of Skaill, the storm stripped the earth from a large irregular knoll known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after four houses were uncovered, the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts. In 1924, another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated. The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.
Neolithic lifestyle.
Skara Brae's people were makers and users of grooved ware, a distinctive style of pottery that appeared in northern Scotland not long before the establishment of the village. The houses used earth sheltering, being sunk into the ground. They were sunk into mounds of pre-existing prehistoric domestic waste known as middens. The midden provided the houses with a stability and also acted as insulation against Orkney's harsh winter climate. On average, the houses measure in size with a large square room containing a stone hearth used for heating and cooking. Given the number of homes, it seems likely that no more than fifty people lived in Skara Brae at any given time.
It is by no means clear what material the inhabitants burned in their hearths. Gordon Childe was sure that the fuel was peat, but a detailed analysis of vegetation patterns and trends suggests that climatic conditions conducive to the development of thick beds of peat did not develop in this part of Orkney until after Skara Brae was abandoned. Other possible fuels include driftwood and animal dung. There is evidence that dried seaweed may have been used significantly. At some sites in Orkney, investigators have found a glassy, slag-like material called "Kelp" or "Cramp" that may be residual burnt seaweed.
The dwellings contain a number of stone-built pieces of furniture, including cupboards, dressers, seats, and storage boxes. Each dwelling was entered through a low doorway that had a stone slab door that could be closed "by a bar that slid in bar-holes cut in the stone door jambs". A sophisticated drainage system was incorporated into the village's design. It included a primitive form of toilet in each dwelling.
Seven of the houses have similar furniture, with the beds and dresser in the same places in each house. The dresser stands against the wall opposite the door, and was the first thing seen by anyone entering the dwelling. Each of these houses had the larger bed on the right side of the doorway and the smaller on the left. Lloyd Laing noted that this pattern accorded with Hebridean custom up to the early 20th century suggesting that the husband's bed was the larger and the wife's was the smaller. The discovery of beads and paint-pots in some of the smaller beds may support this interpretation. Additional support may come from the recognition that stone boxes lie to the left of most doorways, forcing the person entering the house to turn to the right-hand, 'male', side of the dwelling. At the front of each bed lie the stumps of stone pillars that may have supported a canopy of fur; another link with recent Hebridean style.
One house, called House 8, has no storage boxes or dresser. It has been divided into something resembling small cubicles. When this house was excavated, fragments of stone, bone and antler were found. It is possible that this building was used as a house to make simple tools such as bone needles or flint axes. The presence of heat-damaged volcanic rocks and what appears to be a flue, support this interpretation. House 8 is distinctive in other ways as well. It is a stand-alone structure not surrounded by midden, instead it is above ground and has walls over thick. It has a "porch" protecting the entrance.
The site provided the earliest known record of the human flea "Pulex irritans" in Europe.
The Grooved Ware People who built Skara Brae were primarily pastoralists who raised cattle and sheep. Childe originally believed that the inhabitants did not practice agriculture, but excavations in 1972 unearthed seed grains from a midden suggesting that barley was cultivated. Fish bones and shells are common in the middens indicating that dwellers ate seafood. Limpet shells are common and may have been fish-bait that was kept in stone boxes in the homes. The boxes were formed from thin slabs with joints carefully sealed with clay to render them waterproof.
This pastoral lifestyle is in sharp contrast to some of the more exotic interpretations of the culture of the Skara Brae people. Euan MacKie suggested that Skara Brae might be the home of a privileged theocratic class of wise men who engaged in astronomical and magical ceremonies at nearby Ring of Brodgar and the Standing Stones of Stenness. Graham and Anna Ritchie cast doubt on this interpretation noting that there is no archaeological evidence for this claim, although a Neolithic "low road" that goes from Skara Brae passes near both these sites and ends at the chambered tomb of Maeshowe. Low roads connect Neolithic ceremonial sites throughout Britain. 
Dating and abandonment.
Originally, Childe believed that the settlement dated from around 500 BC. This interpretation was coming under increasing challenge by the time new excavations in 1972–73 settled the question. Radiocarbon results obtained from samples collected during these excavations indicate that occupation of Skara Brae began about 3180 BC with occupation continuing for about six hundred years. Around 2500 BC, after the climate changed, becoming much colder and wetter, the settlement may have been abandoned by its inhabitants. There are many theories as to why the people of Skara Brae left; particularly popular interpretations involve a major storm. Evan Hadingham combined evidence from found objects with the storm scenario to imagine a dramatic end to the settlement:
Anna Ritchie strongly disagrees with catastrophic interpretations of the village's abandonment:
The site was farther from the sea than it is today, and it is possible that Skara Brae was built adjacent to a freshwater lagoon protected by dunes. Although the visible buildings give an impression of an organic whole, it is certain that an unknown quantity of additional structures had already been lost to sea erosion before the site's rediscovery and subsequent protection by a seawall. Uncovered remains are known to exist immediately adjacent to the ancient monument in areas presently covered by fields, and others, of uncertain date, can be seen eroding out of the cliff edge a little to the south of the enclosed area.
Artefacts.
A number of enigmatic carved stone balls have been found at the site and some are on display in the museum. Similar objects have been found throughout northern Scotland. The spiral ornamentation on some of these "balls" has been stylistically linked to objects found in the Boyne Valley in Ireland. Similar symbols have been found carved into stone lintels and bed posts. These symbols, sometimes referred to as "runic writings", have been subjected to controversial translations. For example, Castleden suggested that "colons" found punctuating vertical and diagonal symbols may represent separations between words.
Lumps of red ochre found here and at other Neolithic sites have been interpreted as evidence that body painting may have been practised. Nodules of haematite with highly polished surfaces have been found as well; the shiny surfaces suggest that the nodules were used to finish leather.
Other artefacts excavated on site made of animal, fish, bird, and whalebone, whale and walrus ivory, and killer whale teeth included awls, needles, knives, beads, adzes, shovels, small bowls and, most remarkably, ivory pins up to long. These pins are very similar to examples found in passage graves in the Boyne Valley, another piece of evidence suggesting a linkage between the two cultures. So-called Skaill knives were commonly used tools in Skara Brae; these consist of large flakes knocked off sandstone cobbles. Skaill knives have been found throughout Orkney and Shetland.
The 1972 excavations reached layers that had remained waterlogged and had preserved items that otherwise would have been destroyed. These include a twisted skein of heather, one of a very few known examples of Neolithic rope, and a wooden handle.
Related sites in Orkney.
A comparable, though smaller, site exists at Rinyo on Rousay. Unusually, no Maeshowe-type tombs have been found on Rousay and although there are a large number of Orkney–Cromarty chambered cairns, these were built by Unstan ware people.
Knap of Howar on the Orkney island of Papa Westray, is a well preserved Neolithic farmstead. Dating from 3500 BC to 3100 BC, it is similar in design to Skara Brae, but from an earlier period, and it is thought to be the oldest preserved standing building in northern Europe.
There is also a site currently under excavation at Links of Noltland on Westray that appears to have similarities to Skara Brae.
World Heritage status.
"The Heart of Neolithic Orkney" was inscribed as a World Heritage site in December 1999. In addition to Skara Brae the site includes Maeshowe, the Ring of Brodgar, the Standing Stones of Stenness and other nearby sites. It is managed by Historic Scotland, whose 'Statement of Significance' for the site begins:

</doc>
<doc id="28893" url="https://en.wikipedia.org/wiki?curid=28893" title="Sinners in the Hands of an Angry God">
Sinners in the Hands of an Angry God

"Sinners in the Hands of an Angry God" is a sermon written by British Colonial Christian theologian Jonathan Edwards, preached to his own congregation in Northampton, Massachusetts to unknown effect, and again on July 8, 1741 in Enfield, Connecticut. Like Edwards' other works, it combines vivid imagery of Hell with observations of the world and citations of the scripture. It is Edwards' most famous written work, is a fitting representation of his preaching style, and is widely studied by Christians and historians, providing a glimpse into the theology of the Great Awakening of c. 1730–1755.
This is a typical sermon of the Great Awakening, emphasizing the belief that Hell is a real place. Edwards hoped that the imagery and language of his sermon would awaken audiences to the horrific reality that he believed awaited them should they continue life without devotion to Christ. The underlying point is that God has given humanity a chance to rectify their sins. Edwards says that it is the will of God that keeps wicked men from the depths of Hell. This act of restraint has given humanity a chance to mend their ways and return to Christ.
Doctrine.
""There is nothing that keeps wicked men at any one moment out of hell, but the mere pleasure of God.""
Most of the sermon's text consists of ten "considerations":
Purpose.
One church in Enfield, Connecticut had been largely unaffected during the Great Awakening of New England. Edwards was invited by the pastor of the church to preach to them. Edwards's aim was to teach his listeners about the horrors of hell, the dangers of sin and the terrors of being lost. Edwards described the shaky position of those who do not follow Christ's urgent call to receive forgiveness.
Application.
In the final section of "Sinners in the Hands of an Angry God," Edwards shows his theological argument throughout scripture and biblical history. Invoking stories and examples throughout the whole Bible. Edwards ends the sermon with one final appeal, "Therefore let everyone that is out of Christ, now awake and fly from the wrath to come." According to Edwards, only by returning to Christ can one escape the stark fate he outlines.
Effect and legacy.
Jonathan Edwards was interrupted many times before finishing the sermon by people moaning and crying out, "What shall I do to be saved?" Although the sermon has received criticism, Edwards' words have endured and are still read to this day. Edwards' sermon continues to be the leading example of a Great Awakening sermon and is still used in religious and academic studies.

</doc>
<doc id="28894" url="https://en.wikipedia.org/wiki?curid=28894" title="Scottish Highlands">
Scottish Highlands

The Highlands (, "the place of the Gaels"; ) are a historic region of Scotland. The region became culturally distinguishable from the Lowlands from the later Middle Ages into the modern period, when Lowland Scots replaced Scottish Gaelic throughout most of the Lowlands. The term is also used for the area north and west of the Highland Boundary Fault, although the exact boundaries are not clearly defined, particularly to the east. The Great Glen divides the Grampian Mountains to the southeast from the Northwest Highlands. The Scottish Gaelic name of "A' Ghàidhealtachd" literally means "the place of the Gaels" and traditionally, from a Gaelic-speaking point of view, includes both the Western Isles and the Highlands.
The area is very sparsely populated, with many mountain ranges dominating the region, and includes the highest mountain in the British Isles, Ben Nevis. Before the 19th century the Highlands was home to a much larger population, but due to a combination of factors including the outlawing of the traditional Highland way of life following the Jacobite Rising of 1745, the infamous Highland Clearances, and mass migration to urban areas during the Industrial Revolution, the area is now one of the most sparsely populated in Europe. At 9.1 per km2 in 2012, the population density in the Highlands and Islands is less than 1/7th of Scotland's as a whole, comparable with that of Bolivia, Chad and Russia.
The Highland Council is the administrative body for much of the Highlands, with its administrative centre at Inverness. However, the Highlands also includes parts of the council areas of Aberdeenshire, Angus, Argyll and Bute, Moray, Perth and Kinross, and Stirling. Although the Isle of Arran administratively belongs to North Ayrshire, its northern part is generally regarded as part of the Highlands.
The latest Census figures released by the National Register of Scotland, show that the Highlands' population has risen by 23,000 between 2001 and 2011 to 232,000.
The Scottish highlands is the only region in the UK to have the Taiga biome as it features concentrated populations of Scots pine.
History.
Culture.
Between the 15th century and the 20th century, the area differed from most of the Lowlands in terms of language. In Scottish Gaelic, the region is known as the "Gàidhealtachd", because it was traditionally the Gaelic-speaking part of Scotland, although the language is now largely confined to the Outer Hebrides. The terms are sometimes used interchangeably but have different meanings in their respective languages. Scottish English (in its Highland form) is the predominant language of the area today, though Highland English has been influenced by Gaelic speech to a significant extent. Historically, the "Highland line" distinguished the two Scottish cultures. While the Highland line broadly followed the geography of the Grampians in the south, it continued in the north, cutting off the north-eastern areas, that is Caithness, Orkney and Shetland, from the more Gaelic Highlands and Hebrides.
In the aftermath of the Jacobite risings, the British government enacted a series of laws to try to speed up the destruction of the clan system, including bans on the bearing of arms and the wearing of tartan, and limitations on the activities of the Episcopalian Church. Most of this legislation was repealed by the end of the 18th century as the Jacobite threat subsided. There was soon a rehabilitation of Highland culture. Tartan was adopted for Highland regiments in the British Army, which poor Highlanders joined in large numbers in the era of the Revolutionary and Napoleonic wars (1790–1815). Tartan had largely been abandoned by the ordinary people of the region, but in the 1820s, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe. The international craze for tartan, and for idealising a romanticised Highlands, was set off by the Ossian cycle, and further popularised by the works of Walter Scott. His "staging" of the visit of King George IV to Scotland in 1822 and the king's wearing of tartan resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish woollen industry. Individual clan tartans were largely designated in this period and they became a major symbol of Scottish identity. This "Highlandism", by which all of Scotland was identified with the culture of the Highlands, was cemented by Queen Victoria's interest in the country, her adoption of Balmoral as a major royal retreat, and her interest in "tartenry".
Economy.
The Highlands before 1800 were very poor and traditional, and were not much affected by the uplift of the Scottish Enlightenment or the Industrial Revolution that was sweeping the Lowlands of Scotland. The period of the Napoleonic wars brought prosperity, optimism, and economic growth to the Highlands. The economy grew thanks to wages paid in industries such as kelping (in which kelp was burned for the useful chemicals obtained from the ashes), fisheries, and weaving, as well as large-scale infrastructure spending such as the Caledonian Canal project. On the East Coast, farmlands were improved, and high prices for cattle brought money to the area. Service in the Army was also attractive to young men from the Highlands, who sent pay home and retired there with their army pensions. This prosperity ended after 1815, and long-term negative factors began to undermine the economic position of the poor tenant farmers, who typically rented a few acres, and were known as crofters. Landowners were increasingly market-oriented in the century after 1750, and this tended to dissolve the traditional social and economic structure of the North-West Highlands and the Hebrides, causing great disruption for the crofters. The Highland Clearances and the end of the township system followed changes in land ownership and tenancy and the replacement of cattle by sheep. The Great Irish Famine of the 1840s was caused by a plant disease that reached the Highlands in 1846, causing great distress. In a complex form of chain migration, many Highlanders emigrated. Clan leaders would designate which young people should emigrate, where to, and in which order. The first arrivals would prepare the way for their kinsmen who continued to arrive in the chain migration.
The unequal concentration of land ownership remained an emotional and controversial subject, of enormous importance to the Highland economy, and eventually became a cornerstone of liberal radicalism. The poor crofters were politically powerless, and many of them turned to religion. They embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800. Most joined the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. The religious change energised the crofters and separated them from the landlords; it helped prepare them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League.
Violence erupted, starting on the Isle of Skye, when Highland landlords cleared their lands for sheep and deer parks. It was quietened when the government stepped in, passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. This contrasted with the Irish Land War under way at the same time, where the Irish were intensely politicised through roots in Irish nationalism, while political dimensions were limited. In 1885 three Independent Crofter candidates were elected to Parliament, which listened to their pleas. The results included explicit security for the Scottish smallholders; the legal right to bequeath tenancies to descendants; and the creation of a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained their votes.
Religion.
The Scottish Reformation achieved partial success in the Highlands. Roman Catholicism remained strong in some areas, owing to remote locations and the efforts of Franciscan missionaries from Ireland, who regularly came to celebrate Mass. Although the presence of Roman Catholicism has faded, there remain significant Catholic strongholds within the Highlands and Islands such as Moidart and Morar on the mainland and South Uist and Barra in the southern Outer Hebrides. 
The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw somewhat greater success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society after the Battle of Culloden in 1746. In the 19th century, the evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly, appealing much more strongly than did the established church.
For the most part, however, the Highlands are considered predominantly Protestant, loyal to the Church of Scotland. In contrast to the Catholic southern islands, the northern Outer Hebrides islands (Lewis, Harris and North Uist) have an exceptionally high proportion of their population belonging to the Protestant Free Church of Scotland or the Free Presbyterian Church of Scotland. The Outer Hebrides have been described as the last bastion of Calvinism in Britain and the Sabbath remains widely observed. Inverness and the surrounding area has a majority Protestant population, with most locals belonging to either The Kirk or the Free Church of Scotland. The church maintains a noticeable presence within the area, with church attendance notably higher than in other Scottish cities. Religion continues to play an important role in Highland culture, with Sabbath observance still widely practised, particularly in the Hebrides.
Historical geography.
In traditional Scottish geography, the Highlands refers to that part of Scotland north-west of the Highland Boundary Fault, which crosses mainland Scotland in a near-straight line from Helensburgh to Stonehaven. However the flat coastal lands that occupy parts of the counties of Nairnshire, Morayshire, Banffshire and Aberdeenshire are often excluded as they do not share the distinctive geographical and cultural features of the rest of the Highlands. The north-east of Caithness, as well as Orkney and Shetland, are also often excluded from the Highlands, although the Hebrides are usually included. The Highland area, as so defined, differed from the Lowlands in language and tradition, having preserved Gaelic speech and customs centuries after the anglicisation of the latter; this led to a growing perception of a divide, with the cultural distinction between Highlander and Lowlander first noted towards the end of the 14th century. In Aberdeenshire, the boundary between the Highlands and the Lowlands is not well defined. There is a stone beside the A93 road near the village of Dinnet on Royal Deeside which states 'You are now in the Highlands', although there are areas of Highland character to the east of this point.
A much wider definition of the Highlands is that used by the Scotch Whisky industry. Highland Single Malts are produced at distilleries north of an imaginary line between Dundee and Greenock, thus including all of Aberdeenshire and Angus.
Inverness is traditionally regarded as the capital of the Highlands, although less so in the Highland parts of Aberdeenshire, Angus, Perthshire and Stirlingshire which look more to Aberdeen, Perth, Dundee and Stirling as their commercial centres. Under some of the wider definitions in use, Aberdeen could be considered the largest city in the Highlands, although it does not share the recent Gaelic cultural history typical of the Highlands proper.
Highland Council area.
The Highland Council area, created as one of the local government regions of Scotland, has been a unitary council area since 1996. The council area excludes a large area of the southern and eastern Highlands, and the Western Isles, but includes Caithness. "Highlands" is sometimes used, however, as a name for the council area, as in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", is also used to refer to the area covered by the fire and rescue service. This area consists of the Highland council area and the island council areas of Orkney, Shetland and the Western Isles.
Highland Council signs in the Pass of Drumochter, between Glen Garry and Dalwhinnie, say "Welcome to the Highlands".
Highlands and Islands.
Much of the Highlands area overlaps the Highlands and Islands area. An electoral region called "Highlands and Islands" is used in elections to the Scottish Parliament: this area includes Orkney and Shetland, as well as the Highland Council local government area, the Western Isles and most of the Argyll and Bute and Moray local government areas. "Highlands and Islands" has, however, different meanings in different contexts. It means Highland (the local government area), Orkney, Shetland, and the Western Isles in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", refers to the same area as that covered by the fire and rescue service.
Historical crossings.
There have been trackways from the Lowlands to the Highlands since prehistoric times. Many traverse the Mounth, a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven. The most well-known and historically important trackways are the Causey Mounth, Elsick Mounth, Cryne Corse Mounth and Cairnamounth.
Courier delivery.
Although most of the Highlands is geographically on the British mainland, it is somewhat less accessible than the rest of Britain; thus most UK couriers categorise it separately, alongside Northern Ireland, the Isle of Man, and other offshore islands. They thus charge additional fees for delivery to the Highlands, or exclude the area entirely. Whilst the physical remoteness from the largest population centres inevitably leads to higher transit cost, there is confusion and consternation over the scale of the fees charged and the effectiveness of their communication, and the use of the word Mainland in their justification. Since the charges are often based on postcode areas, many far less remote areas, including some which are traditionally considered part of the lowlands, are also subject to these charges. Royal Mail is the only delivery network bound by a Universal Service Obligation to charge a uniform tariff across the UK. This, however, applies only to mail items and not larger packages which are dealt with by its Parcelforce division.
Geology.
The Highlands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland is largely composed of ancient rocks from the Cambrian and Precambrian periods which were uplifted during the later Caledonian Orogeny. Smaller formations of Lewisian gneiss in the northwest are up to 3 billion years old. The overlying rocks of the Torridonian sandstone form mountains in the Torridon Hills such as Liathach and Beinn Eighe in Wester Ross.
These foundations are interspersed with many igneous intrusions of a more recent age, the remnants of which have formed mountain massifs such as the Cairngorms and the Cuillin of Skye. A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast and partially down the Highland Boundary Fault. The Jurassic beds found in isolated locations on Skye and Applecross reflect the complex underlying geology. They are the original source of much North Sea oil. The Great Glen is a transform fault which divides the Grampian Mountains to the southeast from the Northwest Highlands.
The entire region was covered by ice sheets during the Pleistocene ice ages, save perhaps for a few nunataks. The complex geomorphology includes incised valleys and lochs carved by the action of mountain streams and ice, and a topography of irregularly distributed mountains whose summits have similar heights above sea-level, but whose bases depend upon the amount of denudation to which the plateau has been subjected in various places.

</doc>
<doc id="28896" url="https://en.wikipedia.org/wiki?curid=28896" title="Scotch whisky">
Scotch whisky

Scotch whisky, often simply called Scotch, is malt whisky or grain whisky made in Scotland. Scotch whisky must be made in a manner specified by law.
All Scotch whisky was originally made from malted barley. Commercial distilleries began introducing whisky made from wheat and rye in the late 18th century. Scotch whisky is divided into five distinct categories: single malt Scotch whisky, single grain Scotch whisky, blended malt Scotch whisky (formerly called "vatted malt" or "pure malt"), blended grain Scotch whisky, and blended Scotch whisky.
All Scotch whisky must be aged in oak barrels for at least three years. Any age statement on a bottle of Scotch whisky, expressed in numerical form, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed-age whisky.
The first written mention of Scotch whisky is in the Exchequer Rolls of Scotland, 1495. A friar named John Cor was the distiller at Lindores Abbey in the Kingdom of Fife.
Many Scotch whisky drinkers will refer to a unit for drinking as a dram.
Regulations and labelling.
Legal definition.
As of 23 November 2009, the Scotch Whisky Regulations 2009 (SWR) define and regulate the production, labelling, packaging as well as the advertising of Scotch whisky in the United Kingdom. They replace previous regulations that focused solely on production. International trade agreements have the effect of making some provisions of the SWR apply in various other countries as well as in the UK. The SWR define "Scotch whisky" as whisky that is:
Labelling.
A Scotch whisky label comprises several elements that indicate aspects of production, age, bottling, and ownership. Some of these elements are regulated by the SWR, and some reflect tradition and marketing. The spelling of the term "whisky" is often debated by journalists and consumers. Scottish, Australian and Canadian whiskies use "whisky", Irish whiskies use "whiskey", while American and other styles vary in their spelling of the term.
The label always features a declaration of the malt or grain whiskies used. A single malt Scotch whisky is one that is entirely produced from malt in one distillery. One may also encounter the term "single cask", signifying the bottling comes entirely from one cask. The term "blended malt" signifies that single malt whisky from different distilleries are blended in the bottle. The Cardhu distillery also began using the term "pure malt" for the same purpose, causing a controversy in the process over clarity in labelling – the Glenfiddich distillery was using the term to describe some single malt bottlings. As a result, the Scotch Whisky Association declared that a mixture of single malt whiskies must be labelled a "blended malt". The use of the former terms "vatted malt" and "pure malt" is prohibited. The term "blended malt" is still debated, as some bottlers maintain that consumers confuse the term with "blended Scotch whisky", which contains some proportion of grain whisky.
The brand name featured on the label is usually the same as the distillery name (for example, the Talisker Distillery labels its whiskies with the Talisker name). Indeed, the SWR prohibit bottlers from using a distillery name when the whisky was not made there. A bottler name may also be listed, sometimes independent of the distillery. In addition to requiring that Scotch whisky be distilled in Scotland, the SWR require that it also be bottled and labelled in Scotland. Labels may also indicate the region of the distillery (for example, Islay or Speyside).
Alcoholic strength is expressed on the label with "Alcohol By Volume" ("ABV") or sometimes simply "Vol". Typically, bottled whisky is between 40% and 46% ABV. Whisky is considerably stronger when first emerging from the cask—normally 60–63% ABV. Water is then added to create the desired bottling strength. If the whisky is not diluted before bottling, it can be labelled as cask strength.
A whisky's age may be listed on the bottle providing a guarantee of the youngest whisky used. An age statement on the bottle, in the form of a number, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old. In the early 21st century, such "No age statement" whiskies became more common, as distilleries responded to the depletion of aged stocks caused by improved sales.
Labels may also carry various declarations of filtration techniques or final maturation processes. A Scotch whisky labelled as "natural" or "non-chill-filtered" has not been through a filtration process during bottling that removes compounds that some consumers see as desirable. Whisky is aged in various types of casks—and often in used sherry or port casks—during distinct portions of the maturation process, and will take on characteristics, flavour and aromas from such casks. Special casks are sometimes used at the end of the maturation process, and such whiskies may be labelled as "wood finished", "sherry/port finished", and so on.
Types.
There are two basic types of Scotch whisky, from which all blends are made:
Excluded from the definition of "single grain Scotch whisky" is any spirit that qualifies as a single malt Scotch whisky or as a blended Scotch whisky. The latter exclusion is to ensure that a blended Scotch whisky produced from single malt(s) and single grain(s) distilled at the same distillery does not also qualify as single grain Scotch whisky.
Three types of blends are defined for Scotch whisky:
The five Scotch whisky definitions are structured in such a way that the categories are mutually exclusive. The 2009 regulations changed the formal definition of blended Scotch whisky to achieve this result, but in a way that reflected traditional and current practice: before the 2009 SWR, any combination of Scotch whiskies qualified as a blended Scotch whisky, including for example a blend of single malt Scotch whiskies.
As was the case under the Scotch Whisky Act 1988, regulation 5 of the SWR 2009 stipulates that the only whisky that may be manufactured in Scotland is Scotch whisky. The definition of "manufacture" is "keeping for the purpose of maturation; and keeping, or using, for the purpose of blending, except for domestic blending for domestic consumption". This provision prevents the existence of two "grades" of whisky originating from Scotland, one “Scotch whisky” and the other, a "whisky – product of Scotland" that complies with the generic EU standard for whisky. According to the Scotch Whisky Association, allowing non-Scotch whisky production in Scotland would make it difficult to protect Scotch whisky as a distinctive product.
Single grain.
The majority of grain whisky produced in Scotland goes to make blended Scotch whisky. The average blended whisky is 60%–85% grain whisky.
Some higher-quality grain whisky from a single distillery is bottled as single grain whisky.
Blended malt.
Blended malt whisky—formerly called "vatted malt" or "pure malt" (terms that are now prohibited in the SWR 2009)—is one of the least common types of Scotch: a blend of single malts from more than one distillery (possibly with differing ages). Blended malts contain only malt whiskies—no grain whiskies—and are usually distinguished from other types of whisky by the absence of the word "single" before "malt" on the bottle, and the absence of a distillery name. The age of the vat is that of the youngest of the original ingredients. For example, a blended malt marked "8 years old" may include older whiskies, with the youngest constituent being eight years old. Johnnie Walker Green Label and Monkey Shoulder are examples of blended malt whisky. Starting from November 2011, no Scotch whisky could be labelled as a vatted malt or pure malt, the SWR requiring them to be labelled blended malt instead.
Blended.
Blended Scotch whisky constitutes about 90% of the whisky produced in Scotland. Blended Scotch whiskies contain both malt whisky and grain whisky. Producers combine the various malts and grain whiskies to produce a consistent brand style. Notable blended Scotch whisky brands include Bells, Dewar's, Johnnie Walker, Whyte and Mackay, Cutty Sark, J&B, The Famous Grouse, Ballantine's and Chivas Regal.
Independent bottlers.
Most malt distilleries sell a significant amount of whisky by the cask for blending, and sometimes to private buyers as well. Whisky from such casks is sometimes bottled as a single malt by "independent bottling" firms such as Duncan Taylor, Master of Malt, Gordon & MacPhail, Cadenhead's, The Scotch Malt Whisky Society, Murray McDavid, Berry Brothers & Rudd, Douglas Laing, and others. These are usually labelled with the distillery's name, but not using the distillery's trademarked logos or typefaces. An "official bottling" (or "proprietary bottling"), by comparison, is from the distillery (or its owner). Many independent bottlings are from single casks, and they may sometimes be very different from an official bottling.
For a variety of reasons, some independent brands do not identify which facility distilled the whisky in the bottle. They may instead identify only the general geographical area of the source, or they simply market the product using their own brand name without identifying their source. This may, in some cases, be simply to give the independent bottling company the flexibility to purchase from multiple distillers without changing their labels.
History.
According to the Scotch Whisky Association, Scotch whisky evolved from a Scottish drink called "uisge beatha", which means "water of life". The earliest record of distillation in Scotland occurred as long ago as 1494, as documented in the "Exchequer Rolls", which were records of royal income and expenditure. The quote above records eight bolls of malt given to Friar John Cor wherewith to make aqua vitae over the previous year. This would be enough for 1,500 bottles, which suggests that distillation was well-established by the late 15th century.
Whisky production was first taxed in 1644, causing a rise in illicit whisky distilling in the country. Around 1780, there were about eight legal distilleries and 400 illegal ones. In 1823, Parliament eased restrictions on licensed distilleries with the "Excise Act", while at the same time making it harder for the illegal stills to operate, thereby ushering in the modern era of Scotch production. Two events helped to increase whisky's popularity: first, the introduction in 1831 of the column still; the whisky produced with this process was generally less expensive to produce and also less intense and smoother, because a column still can perform the equivalent of multiple distillation steps in a continuous distillation process. Second, the "phylloxera" bug destroyed wine and cognac production in France in 1880.
Regions.
Scotland was traditionally divided into four regions: The Highlands, The Lowlands, The Isle of Islay, and Campbeltown. Due to the large number of distilleries found there, the Speyside region is now also recognized by the Scotch Whisky Association (SWA) as a distinct region. The whisky-producing islands other than Islay are not recognised as a distinct region by the SWA, which groups them into the Highlands region.
Although only five regions are specifically described, any Scottish locale may be used to describe a whisky if it is distilled entirely within that place; for example a single malt whisky distilled on Orkney could be described as "Orkney Single Malt Scotch Whisky" instead of as a Highland whisky.
References.
Notes
Cited sources
Other sources

</doc>
<doc id="28897" url="https://en.wikipedia.org/wiki?curid=28897" title="Special drawing rights">
Special drawing rights

Special drawing rights (ISO 4217 currency code XDR,
also abbreviated SDR) are supplementary foreign exchange reserve assets defined and maintained by the International Monetary Fund (IMF). The XDR is the unit of account for the IMF, and is not a currency "per se". XDRs instead represent a claim to currency held by IMF member countries for which they may be exchanged. The XDR was created in 1969 to supplement a shortfall of preferred foreign exchange reserve assets, namely gold and the U.S. dollar. 
XDRs are allocated to countries by the IMF. Private parties do not hold or use them. The amount of XDRs in existence was around XDR 21.4 billion in August 2009. During the global financial crisis of 2009, an additional XDR 182.6 billion were allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". By October 2014, the amount of XDRs in existence was XDR 204 billion.
The value of the XDR is based on a basket of key international currencies reviewed by IMF every five years. The weights assigned to each currency in the XDR basket are adjusted to take into account their current prominence in terms of international trade and national foreign exchange reserves. In the review conducted in November 2015, the IMF decided that the Renminbi (Chinese Yuan) will be added to the basket effective October 1, 2016. From that date, the XDR basket will consist of the following five currencies: U.S. dollar 41.73%, Euro 30.93%, Chinese yuan 10.92%, Japanese yen 8.33%, British pound 8.09%.
Name.
While the ISO 4217 currency code for Special Drawing Rights is XDR, they are often referred to by their acronym SDR. Both refer to the name "Special Drawing Rights".
Intentionally innocuous and free of connotations because of disagreements over the nature of this new reserve asset during its creation, the name derives from a debate about its primary function—money or credit. While the name would offend neither side, it can be argued that prior to 1981 the XDR was a debt security and so a form of credit. Member countries receiving XDR allocations were required by the reconstitution provision of the XDR articles to hold a prescribed number of XDRs. If a state used any of its allotment, it was expected to rebuild its XDR holdings. As the reconstitution provisions were abrogated in 1981, the XDR now functions less like credit than previously. Countries are still expected to maintain their XDR holdings at a certain level, but penalties for holding fewer than the allocated amount are now less onerous.
The name may actually derive from an early proposal for IMF "reserve drawing rights". The word "reserve" was later replaced with "special" because the idea that the IMF was creating a foreign exchange reserve asset was contentious.
History.
Special drawing rights were created by the IMF in 1969 and were intended to be an asset held in foreign exchange reserves under the Bretton Woods system of fixed exchange rates. 1 XDR was initially defined as US$1, equal to 0.888671 g of gold. After the collapse of that system in the early 1970s the XDR has taken on a less important role. Acting as the unit of account for the IMF has been its primary purpose since 1972.
The IMF itself calls the current role of the XDR "insignificant". Developed countries, who hold the greatest number of XDRs, are unlikely to use them for any purpose. The only actual users of XDRs may be those developing countries that see them as "a rather cheap line of credit".
One reason XDRs may not see much use as foreign exchange reserve assets is that they must be exchanged into a currency before use. This is due in part to the fact private parties do not hold XDRs: they are only used and held by IMF member countries, the IMF itself, and a select few organizations licensed to do so by the IMF. Basic functions of foreign exchange reserves, such as market intervention and liquidity provision, as well as some less prosaic ones, such as maintaining export competitiveness via favorable exchange rates, cannot be accomplished directly using XDRs. This fact has led the IMF to label the XDR as an "imperfect reserve asset".
Another reason they may see little use is that the number of XDRs in existence is relatively few. As of January 2011, XDRs represented less than 4% of global foreign exchange reserve assets. To function well a foreign exchange reserve asset must have sufficient liquidity, but XDRs, because of their small number, may be perceived to be an illiquid asset. The IMF says, "expanding the volume of official XDRs is a prerequisite for them to play a more meaningful role as a substitute reserve asset."
Alternative to U.S. dollar.
The XDR comes to prominence when the U.S. dollar is weak or otherwise unsuitable to be a foreign exchange reserve asset. This usually manifests itself as an allocation of XDRs to IMF member countries. Distrust of the U.S. dollar is not the only stated reason allocations have been made, however.
One of its first roles was to alleviate an expected shortfall of U.S. dollars c. 1970. At this time, the United States had a conservative monetary policy and did not want to increase the total amount of U.S. dollars in existence. If the United States had continued down this path, the dollar would have become a less attractive foreign exchange reserve asset: it would not have had the necessary liquidity to serve this function. Soon after XDR allocations began, the United States reversed its former policy and provided sufficient liquidity. In the process a potential role for the XDR was removed. During this first round of allocations, 9.3 billion XDRs were distributed to IMF member countries.
The XDR resurfaced in 1978 when many countries were wary of taking on more foreign exchange reserve assets denominated in U.S. dollars. This suspicion of the dollar precipitated an allocation of 12 billion XDRs over a period of four years.
Concomitant with the financial crisis of 2007–2008, the third round of XDR allocations occurred in the years 2009 and 2011. The IMF recognized the financial crisis as the cause for distributing the large majority of these third-round allotments, but some allocations were couched as distributing XDRs to countries that had never received any and others as a re-balancing of IMF quotas, which determine how many XDRs a country is allotted, to better represent the economic strength of emerging markets.
During this time China, a country with large holdings of U.S. dollar foreign exchange reserves, voiced its displeasure at the current international monetary system, and promoted measures that would allow the XDR to "fully satisfy the member countries' demand for a reserve currency". These comments, made by a chairman of the People's Bank of China, Zhou Xiaochuan, drew media attention, and the IMF showed some support for China's stance. It produced a paper exploring ways the substance and function of the XDR could be increased. China has also suggested the creation of a substitution account to allow exchange of U.S. dollars into XDRs. When substitution was proposed before, in 1978, the United States appeared reluctant to allow such a mechanism to become operational.
Use by developing countries.
In 2001, the UN suggested allocating XDRs to developing countries for use by them as cost-free alternatives to building foreign exchange reserves through borrowing or running current account surpluses. In 2009, an XDR allocation was made to countries that had joined the IMF after the 1979–1981 round of allocations was complete (and so had never been allocated any). First proposed in 1997, many of the beneficiaries of this 2009 allocation were developing countries.
Value definition.
The value of the XDR is determined by the value of several currencies important to the world’s trading and financial systems. Initially its value was fixed, so that 1 XDR = 1 U.S. dollar, but this was abandoned in favor of a currency basket after the 1973 collapse of the Bretton Woods system of fixed exchange rates. From July 1974 to December 1980, the XDR consisted of a basket of 16 currencies. From January 1981 until the birth of the euro, the basket was updated to include U.S. dollar, the Deutsche mark, the French franc, the British pound and the Japanese yen as the constituents. When the euro was introduced in January 1999, it replaced the mark and franc; the basket was then composed of the U.S. dollar, the euro, the British pound and the Japanese yen. Beginning on October 1, 2016 the SDR will include the Chinese Renminbi in the currency basket.
This basket is re-evaluated every five years, and the currencies included as well as the weights given to them can then change. A currency's importance is currently measured by the degree to which it is used as a foreign exchange reserve asset and the amount of exports sold in that currency. 
Because of fluctuating exchange rates, the relative value of each currency varies continuously and so does the value of the XDR. The IMF fixes the value of one XDR in terms of U.S. dollars every day. The latest U.S. dollar valuation of the XDR is published on the IMF website.
Interest rate.
Special drawing rights carry a weekly determined interest rate, but no party pays interest if an IMF member country maintains the amount of XDRs allocated to it. Based on "a weighted average of representative interest rates on short-term debt in the money markets of the XDR basket currencies", interest is paid by an IMF member country if it holds fewer XDRs than it was allocated, and interest is paid to a member country if it holds more XDRs than it was allocated.
Allocations.
Special drawing rights are allocated to member countries by the IMF. A country's IMF quota, the maximum amount of financial resources that it is obligated to contribute to the fund, determines its allotment of XDRs. Any new allocations must be voted on in the XDR Department of the IMF and pass with an 85% majority. All IMF member countries are represented in the XDR Department, but this is not a one country, one vote system; voting power is determined by a member country's IMF quota. For example, the United States has 16.7% of the vote as of March 2, 2011.
Allocations are not made on a regular basis and have only occurred on several occasions. The first round took place because of a situation that was soon reversed, the possibility of an insufficient amount of U.S. dollars because of U.S. reluctance to run the deficit necessary to supply future demand. Extraordinary circumstances have, likewise, led to the other XDR allocation events.
Exchange.
In order to use its XDRs, a country must find a willing party to buy them. The IMF acts as an intermediary in this voluntary exchange; it also has the authority under the designation mechanism to ask member countries with strong foreign exchange reserves to purchase XDRs from those with weak reserves. The maximum obligation any country has under this mechanism is currency equal to twice the amount of its XDR allocation. As of 2015, XDRs may only be exchanged for Euro, Japanese yen, UK pounds, or U.S. dollars. The IMF says exchanging XDRs can take "several days".
It is not, however, the IMF that pays out foreign currency in exchange for XDRs: the claim to currency that XDRs represent is not a claim on the IMF.
Other uses.
Unit of account.
Some international organizations use the XDR as a unit of account. The IMF says using the XDR in this way "help cope with exchange rate volatility". As of 2001, organizations that use the XDR as a unit of account, besides the IMF itself, include: African Development Bank, Arab Monetary Fund, Asian Development Bank, Bank for International Settlements, Common Fund for Commodities, East African Development Bank, Economic Community of West African States, International Center for Settlement of Investment Disputes, International Fund for Agricultural Development, and Islamic Development Bank. It is not only international organizations that use the XDR in this way. JETRO uses XDRs to price foreign aid. In addition, charges, liabilities, and fees prescribed by some international treaties are denominated in XDRs. In 2003, the Bank for International Settlements ceased to use the gold franc as their currency, in favour of XDR.
Use in international law.
In some international treaties and agreements, XDRs are used to value penalties, charges or prices. For example, the Convention on Limitation of Liability for Maritime Claims caps personal liability for damages to ships at XDR 330,000. The Montreal Convention and other treaties also use XDRs in this way.
Currency peg.
According to the IMF, "the SDR may not be any country’s optimal basket", but a few countries do peg their currencies to the XDR. One possible benefit to nations with XDR pegs is that they may be perceived to be more transparent. As of 2000, the number of countries that did so was four. This is a substantial decrease from 1983, when 14 countries had XDR pegs. As of 2007 and 2010, Syria pegs its pound to the XDR.

</doc>
<doc id="28898" url="https://en.wikipedia.org/wiki?curid=28898" title="Special Operations Executive">
Special Operations Executive

The Special Operations Executive (SOE) was a British World War II organisation. Following Cabinet approval, it was officially formed by Minister of Economic Warfare Hugh Dalton on 22 July 1940, to conduct espionage, sabotage and reconnaissance in occupied Europe (and later, in occupied Southeast Asia also) against the Axis powers, and to aid local resistance movements.
It was initially also involved in the formation of the Auxiliary Units, a top secret "stay-behind" resistance organisation which would have been activated in the event of a German invasion of Britain.
Few people were aware of SOE's existence. To those who were part of it or liaised with it, it was sometimes referred to as "the Baker Street Irregulars", after the location of its London headquarters. It was also known as "Churchill's Secret Army" or the "Ministry of Ungentlemanly Warfare". Its various branches, and sometimes the organisation as a whole, were concealed for security purposes behind names such as the "Joint Technical Board" or the "Inter-Service Research Bureau", or fictitious branches of the Air Ministry, Admiralty or War Office.
SOE operated in all countries or former countries occupied by or attacked by the Axis forces, except where demarcation lines were agreed with Britain's principal Allies (the Soviet Union and the United States). It also made use of neutral territory on occasion, or made plans and preparations in case neutral countries were attacked by the Axis. The organisation directly employed or controlled just over 13,000 people, about 3,200 of whom were women.
After the war, the organisation was officially dissolved on 15 January 1946. A memorial to SOE's agents was unveiled on the Albert Embankment by Lambeth Palace in London in October 2009.
History.
Origins.
The organisation was formed from the merger of three existing secret departments, which had been formed shortly before the outbreak of the Second World War. Immediately after Germany annexed Austria (the "Anschluss") in March 1938, the Foreign Office created a propaganda organisation known as Department EH (after Electra House, its headquarters), run by Canadian newspaper magnate Sir Campbell Stuart. Later that month, the Secret Intelligence Service (SIS, also known as MI6) formed a section known as Section D, under Major Lawrence Grand RE, to investigate the use of sabotage, propaganda and other irregular means to weaken an enemy. In the autumn of the same year, the War Office expanded an existing research department known as GS (R) and appointed Major J. C. Holland RE as its head to conduct research into guerrilla warfare. GS (R) was renamed MI(R) in early 1939.
These three departments worked with few resources until the outbreak of war. There was much overlap between their activities and Section D and EH duplicated much of each other's work. On the other hand, the heads of Section D and MI(R) knew each other and shared information. They agreed a rough division of their activities; MI(R) researched irregular operations which could be undertaken by regular uniformed troops, while Section D dealt with truly undercover work.
During the early months of the war, Section D was based first at St Ermin's Hotel in Westminster and then the Metropole Hotel near Trafalgar Square. The Section attempted unsuccessfully to sabotage deliveries of vital strategic materials to Germany from neutral countries by mining the Iron Gate on the River Danube. MI(R) meanwhile produced pamphlets and technical handbooks for guerrilla leaders. MI(R) was also involved in the formation of the Independent Companies, autonomous units intended to carry out sabotage and guerrilla operations behind enemy lines in the Norwegian Campaign; and the Auxiliary Units, stay-behind commando units based around the Home Guard which would act in the event of an Axis invasion of Britain, as seemed possible in the early years of the war.
Formation.
On 13 June 1940, at the instigation of newly appointed Prime Minister Winston Churchill, Lord Hankey (who held the Cabinet post of Chancellor of the Duchy of Lancaster) persuaded Section D and MI R that their operations should be coordinated. On 1 July, a Cabinet level meeting arranged the formation of a single sabotage organisation. On 16 July, Hugh Dalton, the Minister of Economic Warfare, was appointed to take political responsibility for the new organisation, which was formally created on 22 July. Dalton used the Irish Republican Army (IRA) during the Irish war of Independence as a model for the organisation. Churchill ordered SOE to "set Europe ablaze". Majors Grand and Holland both returned to service in the regular army and Campbell Stuart left the organisation.
One department of MI R, MI R(C), which was involved in the development of weapons for irregular warfare, was not integrated into SOE but became an independent body codenamed MD1. It was nicknamed "Churchill's Toyshop" from the Prime Minister's close interest in it and his enthusiastic support.
Leadership.
The Director of SOE was usually referred to by the initials "CD". The first Director to be appointed was Sir Frank Nelson, a former head of a trading firm in India, a back bench Conservative Member of Parliament and Consul in Basel, Switzerland.
Dalton was replaced as Minister of Economic Warfare by Lord Selborne in February 1942. Selborne in turn retired Nelson, who had suffered ill health as a result of his hard work, and appointed Sir Charles Hambro, head of the English banking firm Hambro's to replace him. Hambro had been a close friend of Churchill before the war and had won the Military Cross in the First World War. Selborne also transferred Gladwyn Jebb, the senior civil servant who had run the Ministry's day-to-day dealings with SOE, back to the Foreign Office.
Selborne and Hambro cooperated closely until August 1943, when they fell out over the question of whether SOE should remain a separate body or coordinate its operations with those of the British Army in several theatres of war. Hambro felt that any loss of autonomy would cause a number of problems for SOE in the future. At the same time, Hambro was found to have failed to pass on vital information to Selborne. He was dismissed as Director, and became head of a raw materials purchasing commission in Washington, D.C., which was involved in the exchange of nuclear information.
As part of the subsequent closer ties between the Imperial General Staff and SOE (although SOE had no representation on the Chiefs of Staff Committee), Hambro's replacement as Director from September 1943 was the former Deputy Director, Major General Colin Gubbins. Gubbins had wide experience of commando and clandestine operations and had played a major part in MI R's early operations. He also put in practice many of the lessons he learned from the IRA during the Irish War of Independence.
Organisation.
The organization of SOE continually evolved and changed during the war. Initially, it consisted of three broad departments: SO1, which dealt with propaganda; SO2 (Operations); and SO3 (Research). SO3 was quickly overloaded with paperwork and was merged into SO2. In August 1941, following quarrels between the Ministry of Economic Warfare and the Ministry of Information over their relative responsibilities, SO1 was removed from SOE and became an independent organisation, the Political Warfare Executive.
Thereafter there was a single, broad "Operations" department which controlled the Sections operating into enemy and sometimes neutral territory, and the selection and training of agents. Sections were assigned to a single country. Some enemy-occupied countries had two or more sections assigned to deal with politically disparate resistance movements. (France had no less than six).
Four departments and some smaller groups were controlled by the Director of Scientific Research, Professor Dudley Maurice Newitt, and were concerned with the development or acquisition and production of special equipment. A few other sections were involved with finance, security, economic research and administration, although SOE had no central registry or filing system. When Gubbins was appointed Director, he formalised some of the administrative practices which had grown in an "ad hoc" fashion and appointed an Establishment Officer to oversee the manpower and other requirements of the various departments.
The Director of SOE had either a Deputy from the Army, or (once Gubbins became Director) an army officer as Chief of Staff. The main controlling body of SOE was its Council, consisting of around fifteen heads of departments or sections. About half of the Council were from the armed forces (although some were specialists who were only commissioned after the outbreak of war), the rest were various civil servants, lawyers, or business or industrial experts. Most of the members of the Council, and the senior officers and functionaries of SOE generally, were recruited by word of mouth among public school alumni and Oxbridge graduates, although this did not notably affect SOE's political complexion.
Several subsidiary SOE headquarters and stations were set up to manage operations which were too distant for London to control directly. SOE's operations in the Middle East and Balkans were controlled from a headquarters in Cairo, which was notorious for poor security, infighting and conflicts with other agencies. It finally became known in April 1944 as Special Operations (Mediterranean), or SO(M). Shortly after the Allied landings in North Africa, a station codenamed ""Massingham"" was established near Algiers in late 1942, which operated into Southern France. Following the Allied invasion of Italy, personnel from "Massingham" established forward stations in Brindisi and near Naples. A subsidiary headquarters initially known as "Force 133" was later set up in Bari in Southern Italy, under the Cairo headquarters, to control operations in the Balkans and Northern Italy.
An SOE station, which was first called the "India Mission", and was subsequently known as "GS I(k)" was set up in India late in 1940. It subsequently moved to Ceylon so as to be closer to the headquarters of the Allied South East Asia Command and became known as Force 136. A "Singapore Mission" was set up at the same time as the India Mission but was unable to overcome official opposition to its attempts to form resistance movements in Malaya before the Japanese overran Singapore. Force 136 took over its surviving staff and operations.
There was also a branch office in New York, formally titled British Security Coordination, and headed by the Canadian businessman Sir William Stephenson. This branch office, located at Room 3603, 630 Fifth Avenue, Rockefeller Center, coordinated the work of SOE, SIS and MI5 with the American F.B.I. and Office of Strategic Services.
Aims.
As with its leadership and organisation, the aims and objectives of SOE changed throughout the war. SOE occasionally carried out operations with direct military objectives, such as Operation Harling, originally designed to cut one of the Axis supply lines to their troops fighting in North Africa. They also carried out some high-profile operations aimed mainly at the morale both of the Axis and occupied nations, such as Operation Anthropoid, the assassination in Prague of Reinhard Heydrich.
Dalton's early enthusiasm for fomenting widespread strikes, civil disobedience and nuisance sabotage in Axis-occupied areas had to be curbed. Thereafter, there were two main aims, often mutually incompatible; sabotage of the Axis war effort, and the creation of secret armies which would rise up to assist the liberation of their countries when Allied troops arrived or were about to do so. It was recognised that acts of sabotage would bring about reprisals and increased Axis security measures which would hamper the creation of underground armies. As the tide of war turned in the Allies' favour, these underground armies became more important. 
Relationships.
SOE cooperated fairly well with Combined Operations Headquarters during the middle years of the war, usually on technical matters as SOE's equipment was readily adopted by commandos and other raiders. This support was lost when Vice Admiral Louis Mountbatten left Combined Operations, though by this time SOE had its own transport and had no need to rely on Combined Operations for resources. On the other hand, the Admiralty objected to SOE developing its own underwater vessels, and the duplication of effort this involved. The Royal Air Force, and in particular RAF Bomber Command under "Bomber" Harris also objected to aircraft being allocated to SOE.
SOE's relationships with the Foreign Office were difficult on several occasions, as various governments in exile protested at operations taking place without their knowledge or approval, which resulted in Axis reprisals against civilian populations. SOE nevertheless generally adhered to the rule, ""No bangs without Foreign Office approval."" There was also tension between SOE and SIS, which the Foreign Office controlled. Where SIS preferred placid conditions in which it could gather intelligence and work through influential persons or authorities, SOE was intended to create unrest and turbulence, and often backed anti-establishment organisations, such as the Communists, in several countries. At one stage, SIS actively hindered SOE's attempts to infiltrate agents into enemy-occupied France.
Towards the end of the war, as Allied forces began to liberate territories occupied by the Axis and in which SOE had established resistance forces, SOE also liaised with and to some extent came under the control of the Allied theatre commands. Relationships with Supreme Headquarters Allied Expeditionary Force in north-west Europe (whose commander was General Dwight D. Eisenhower) and South East Asia Command (whose commander was Admiral Louis Mountbatten, already well known to SOE) were generally excellent. However, there were continued difficulties with the Commanders in Chief in the Mediterranean, partly because of the complaints over impropriety at SOE's Cairo headquarters during 1941 and partly because both the supreme command in the Mediterranean and SOE's establishments were split in 1942 and 1943, leading to divisions of responsibility and authority.
Dissolution.
Towards the end of the war (in late 1944), Lord Selborne advocated keeping SOE, or a similar body, in being and that it would report to the Ministry of Defence. The Foreign Secretary, Anthony Eden, insisted that his ministry, already responsible for the SIS, should control SOE or its successors. The Joint Intelligence Committee, which had a broad coordinating role over Britain's intelligence services and operations, took the view that SOE was a more effective organisation than the SIS but that it was unwise to perform special operations outside the control of the Chiefs of Staff.
The debate continued for several months until on 22 May 1945, Selborne wrote:
Churchill took no immediate decision, and after he lost the general election on 5 July 1945, the matter was dealt with by the Labour Prime Minister, Clement Attlee. Selborne told Attlee that SOE still possessed a worldwide network of clandestine radio networks and sympathisers. Attlee replied that he had no wish to own a British Comintern, and closed Selborne's network down at 48 hours' notice.
SOE was dissolved officially on 15 January 1946. Some of its senior staff moved easily into financial services in the City of London, although some of them had not lost their undercover mentality and did little for the City's name. Most of SOE's other personnel reverted to their peacetime occupations or regular service in the armed forces, but 280 of them were taken into the ""Special Operations Branch"" of MI6. Some of these had served as agents in the field, but MI6 was most interested in SOE's training and research staff. Sir Stewart Menzies, the head of MI6 (who was generally known simply as "C") soon decided that a separate branch was unsound, and merged it into the general body of MI6.
Gubbins was not given further employment by the Army, but he later founded the Special Forces Club for former members of SOE and similar organisations.
Locations.
SOE maintained a large number of training, research and development or administrative centres. It was a joke that ""SOE"" stood for ""Stately 'omes of England"", after the large number of country houses and estates it requisitioned and used.
After working from temporary offices in Central London, the headquarters of SOE was moved on 31 October 1940 into 64 Baker Street (hence the nickname ""the Baker Street Irregulars""). Ultimately, SOE occupied much of the western side of Baker Street.
The establishments connected with experimentation and production of equipment were mainly concentrated in Hertfordshire and were designated by roman numbers. The main weapons and devices research establishments were "The Firs", the home of MD1 near Aylesbury in Buckinghamshire, and Station IX at The Frythe, a former hotel outside Welwyn Garden City where, under the cover name of ISRB (Inter Services Research Bureau), SOE developed radios, weapons, explosive devices and booby traps. Station XII at Aston House near Stevenage in Hertfordshire originally conducted research and development but later became a production, storage and distribution centre for devices already developed.
Station XV, at the Thatched Barn near Borehamwood, was devoted to camouflage, which usually meant equipping agents with authentic local clothing and personal effects. Various sub-stations in London, and Station XIV near Roydon in Essex which specialised in forgery of identity papers, ration books and so on, were also involved in this task. Station XV and other camouflage sections also devised methods of hiding weapons, explosives or radios in innocuous-seeming items.
The training establishments and properties used by country sections were widely distributed and were designated by Arabic numbers. The initial training centres of the SOE were at country houses such as Wanborough Manor, Guildford. Agents destined to serve in the field underwent commando training at Arisaig in Scotland, where they were taught armed and unarmed combat skills by William E. Fairbairn and Eric A. Sykes, former Inspectors in the Shanghai Municipal Police. They then attended courses in security and ""tradecraft"" at Group B schools around Beaulieu in Hampshire. Finally, they received specialist training in skills such as demolition techniques or Morse code telegraphy at various country houses in England and parachute training (if necessary) by STS 51 and 51a situated near Altrincham, Cheshire with the assistance of No.1 Parachute Training School RAF, at RAF Ringway (which later became Manchester Airport).
A commando training centre similar to Arisaig and run by Fairbairn was later set up at Oshawa, for Canadian members of SOE and members of the newly created American organisation, the Office of Strategic Services.
Agents.
A variety of people from all classes and pre-war occupations served SOE in the field. The backgrounds of agents in F Section, for example, ranged from Indian royalty (Noor Inayat Khan) to working class, with some even reputedly from the criminal underworld.
In most cases, the primary quality required of an agent was a deep knowledge of the country in which he or she was to operate, and especially its language, if the agent was to pass as a native of the country. Dual nationality was often a prized attribute. This was particularly so of France. In other cases, especially in the Balkans, a lesser degree of fluency was required as the resistance groups concerned were already in open rebellion and a clandestine existence was unnecessary. A flair for diplomacy combined with a taste for rough soldiering was more necessary. Some regular army officers proved adept as envoys, although others (such as the former diplomat Fitzroy Maclean or the classicist Christopher Woodhouse) were commissioned only during wartime.
Exiled or escaped members of the armed forces of some occupied countries were obvious sources of agents. This was particularly true of Norway and the Netherlands. In other cases (such as Frenchmen owing loyalty to Charles de Gaulle and especially the Poles), the agents' first loyalty was to their leaders or governments in exile, and they treated SOE only as a means to an end. This could occasionally lead to mistrust and strained relations in Britain.
The organisation was prepared to ignore almost any contemporary social convention in its fight against the Axis. It employed known homosexuals, people with criminal records (some of whom taught skills such as lock-picking) or bad conduct records in the armed forces, Communists and anti-British nationalists. Although some of these might have been considered a security risk, there is practically no known case of an SOE agent wholeheartedly going over to the enemy. However, there were cases such as that of Henri Déricourt, in which the conduct of agents was questionable but it was impossible to establish whether they were acting under secret orders from SOE or MI6.
SOE was also far ahead of contemporary attitudes in its use of women in armed combat. Although women were first considered only as couriers in the field or as wireless operators or administrative staff in Britain, those sent into the field were trained to use weapons and in unarmed combat. Most were commissioned into either the First Aid Nursing Yeomanry (FANY) or the Women's Auxiliary Air Force. Some (such as Pearl Witherington) became the organisers of resistance networks. Others such as Odette Hallowes or Violette Szabo were decorated for bravery, posthumously in Szabo's case. Of SOE's 55 female agents, thirteen were killed in action or died in Nazi concentration camps.
Communications.
Radio.
Most of the resistance networks which SOE formed or liaised with were controlled by radio directly from Britain or one of SOE's subsidiary headquarters. All resistance circuits contained at least one wireless operator, and all drops or landings were arranged by radio, except for some early exploratory missions sent "blind" into enemy-occupied territory.
At first, SOE's radio traffic went through the SIS-controlled radio station at Bletchley Park. From 1 June 1942 SOE used its own transmitting and receiving stations at Grendon Underwood and Poundon nearby, as the location and topography were suitable. Teleprinters linked the radio stations with SOE's HQ in Baker Street. Operators in the Balkans worked to radio stations in Cairo.
SOE was highly dependent upon the security of radio transmissions. There were three factors involved in this: the physical qualities and capabilities of the radio sets, the security of the transmission procedures and the provision of proper ciphers.
SOE's first radios were supplied by SIS. They were large, clumsy and required large amounts of power. SOE acquired a few, much more suitable, sets from the Poles in exile, but eventually designed and manufactured their own, such as the Paraset. The A Mk III, with its batteries and accessories, weighed only , and could fit into a small attache case, although the B Mk II, otherwise known as the B2, which weighed , was required to work over ranges greater than about .
Operating procedures were insecure at first. Operators were forced to transmit verbose messages on fixed frequencies and at fixed times and intervals. This allowed German direction finding teams time to triangulate their positions. After several operators were captured or killed, procedures were made more flexible and secure. The SOE wireless operators were also known as "The Pianists".
As with their first radio sets, SOE's first ciphers were inherited from SIS. Leo Marks, SOE's chief cryptographer, was responsible for the development of better codes to replace the insecure poem codes. Eventually, SOE settled on single use ciphers, printed on silk. Unlike paper, which would be given away by rustling, silk would not be detected by a casual search if it was concealed in the lining of clothing.
British Broadcasting Corporation.
The BBC also played its part in communications with agents or groups in the field. During the war, it broadcast to almost all Axis-occupied countries, and was avidly listened to, even at risk of arrest. The BBC included various "personal messages" in its broadcasts, which could include lines of poetry or apparently nonsensical items. They could be used to announce the safe arrival of an agent or message in London for example, or could be instructions to carry out operations on a given date. These were used for example to mobilize the resistance groups in the hours before Operation Overlord.
Other methods.
In the field, agents could sometimes make use of the postal services, though these were slow, not always reliable and letters were almost certain to be opened and read by the Axis security services. In training, agents were taught to use a variety of easily available substances to make invisible ink, though most of these could be detected by a cursory examination, or to hide coded messages in apparently innocent letters. The telephone services were even more certain to be intercepted and listened to by the enemy, and could be used only with great care.
The most secure method of communication in the field was by courier. In the earlier part of the war, most women sent as agents in the field were employed as couriers, on the assumption that they would be less likely to be suspected of illicit activities.
Equipment.
Weapons.
Although SOE used some silenced assassination weapons such as the De Lisle carbine and the Welrod (specifically developed for SOE at Station IX), it took the view that weapons issued to resisters should not require extensive training in their use, or need careful maintenance. The crude and cheap Sten was a favourite. For issue to large forces such as the Yugoslav Partisans, SOE used captured German or Italian weapons. These were available in large quantities after the Tunisian and Sicilian campaigns and the surrender of Italy, and the partisans could acquire ammunition for these weapons (and the Sten) from enemy sources.
SOE also adhered to the principle that resistance fighters would be handicapped rather than helped by heavy equipment such as mortars or anti-tank guns. These were awkward to transport, almost impossible to conceal and required skilled and highly trained operators. Later in the war however, when resistance groups staged open rebellions against enemy occupation, some heavy weapons were dispatched, for example to the Maquis du Vercors.
Most SOE agents received training on captured enemy weapons before being sent into enemy-occupied territory. Ordinary SOE agents were also armed with handguns acquired abroad, such as, from 1941, a variety of US pistols, and a large quantity of the Spanish Llama .38 ACP in 1944. Such was SOE's demand for weapons, a consignment of 8,000 Ballester–Molina .45 calibre weapons was purchased from Argentina, apparently with the mediation of USA.
SOE agents were issued with the Fairbairn–Sykes fighting knife also issued to Commandos. For specialised operations or use in extreme circumstances, SOE issued small fighting knives which could be concealed in the heel of a hard leather shoe or behind a coat lapel. Given the likely fate of agents captured by the Gestapo, SOE also disguised suicide pills as coat buttons.
Sabotage.
SOE developed a wide range of explosive devices for sabotage, such as limpet mines, shaped charges and time fuses. These were also used by commando units. SOE pioneered the use of plastic explosive. (The term "plastique" comes from plastic explosive packaged by SOE and originally destined for France but taken to the United States instead.) Plastic explosive could be shaped and cut to perform almost any demolition task. It was also inert and required a powerful detonator to cause it to explode, and was therefore safe to transport and store. It was used in everything from car bombs, to exploding rats designed to destroy coal-fired boilers.
Other, more subtle sabotage methods included lubricants laced with grinding materials, intended for introduction into vehicle oil systems, railway wagon axle boxes, etc., incendiaries disguised as innocuous objects, explosive material concealed in coal piles to destroy locomotives, and land mines disguised as cow or elephant dung. On the other hand, some sabotage methods were extremely simple but effective, such as using sledgehammers to crack cast-iron mountings for machinery.
Submarines.
Station IX developed several miniature submersible craft. The Welman submarine and "Sleeping Beauty" were offensive weapons, intended to place explosive charges on or adjacent to enemy vessels at anchor. The Welman was used once or twice in action, but without success. The Welfreighter was intended to deliver stores to beaches or inlets, but it too was unsuccessful.
A sea trials unit was set up in West Wales at Goodwick, by Fishguard (station IXa) where these craft were tested. In late 1944 craft were dispatched to Australia to the Allied Intelligence Bureau (SRD), for tropical testing.
Other.
SOE also revived some medieval devices, such as the caltrop, which could be used to burst the tyres of vehicles or injure foot soldiers and crossbows powered by multiple rubber bands to shoot incendiary bolts. There were two types, known as ""Big Joe"" and ""Lil Joe"" respectively. They had tubular alloy skeleton stocks and were designed to be collapsible for ease of concealment.
An important section of SOE was the Operation Research and Trials Section, which was formally established in August 1943. The section had the responsibility both for issuing formal requirements and specifications to the relevant development and production sections, and for testing prototypes of the devices produced under conditions which closely matched those to be expected in the field. Over the twelve-month period from 1 November 1943 to 1 November 1944 for example, the section tested 78 devices. Some of these were weapons such as the Sleeve gun, or fuses or adhesion devices to be used in sabotage, others again were utility objects such as waterproof containers for stores to be dropped by parachute or night glasses (lightweight binoculars with plastic lenses). Of the devices tested, 47% were accepted for use with little or no modification, 31% were accepted only after considerable modification and the remaining 22% were rejected.
Before SOE's research and development procedures were formalised in 1943, a variety of more or less useful devices were developed. Some of the more imaginative devices invented by SOE included exploding pens with enough explosive power to blast a hole in the bearer's body, or guns concealed in tobacco pipes, though there is no record of any of these being used in action.
Transport.
The continent of Europe was largely closed to normal travel. Although it was possible in some cases to cross frontiers from neutral countries such as Spain or Sweden, this was slow and there were issues over violating these countries' neutrality. SOE had to rely largely on its own air or sea transport for movement of people, arms and equipment.
Air.
SOE was engaged in disputes with the RAF from its early days. In January 1941, an intended ambush (Operation Savanna) against the aircrew of a German "pathfinder" air group near Vannes in Brittany was thwarted when Air Vice Marshal Charles Portal, the Chief of the Air Staff, objected on moral grounds to parachuting what he regarded as assassins. Although Portal's objections were later overcome (and "Savanna" was mounted, unsuccessfully), Air Marshal Harris (""Bomber Harris""), the Commander-in-Chief of Bomber Command, resented the diversion of bombers to SOE purposes (or indeed any purposes other than the offensive against German cities). He too was overruled and by April 1942, SOE had the services of 138 and 161 squadrons at RAF Tempsford.
The aircraft used by SOE included the Westland Lysander, which could carry up to three passengers and two panniers loaded with stores, and had an effective range of . It could use rough landing strips only in length, or even less. Lysanders were used to transport 101 agents to and 128 agents from Nazi-occupied Europe. The Lockheed Hudson had a range greater and could carry more passengers (ten or more), but required landing strips twice as long as those needed for the Lysander.
To deliver agents and stores by parachute, SOE could use several aircraft originally designed as bombers: the Armstrong Whitworth Whitley until November 1942, the Handley Page Halifax and the Short Stirling. The Stirling could carry a particularly large load, but only the Halifax had the range to reach dropping zones in eastern Poland (and even then, only from bases in Southern Italy). Later in the war, SOE also used the American-supplied Douglas Dakota, which was often landed at airfields in territory held by partisans in the Balkans.
Stores were usually parachuted in cylindrical containers. The "C" type was long and when fully loaded could weigh up to . The "H" type was the same size overall but could be broken down into five smaller sections. This made it easier to carry and conceal but made it impossible to carry long loads such as rifles. Some inert stores such as boots and blankets were "free-dropped" i.e. simply thrown out of the aircraft bundled together without a parachute, often to the hazard of any receiving committee on the ground.
Station IX developed a miniature folding motorbike (the "Welbike") for use by parachutists, though this was noisy and conspicuous, and was of little use on rough ground.
Locating and homing equipment.
Some devices used by SOE were designed specifically to guide aircraft to landing strips and dropping zones. Such sites could be marked by an agent on the ground with bonfires or bicycle lamps, but this required good visibility, as the pilot of an aircraft had not only to spot the ground signals, but also to navigate by visible landmarks to correct dead reckoning. Many landings or drops were thwarted by bad weather. To overcome these problems, SOE and Allied airborne forces used the Rebecca/Eureka transponding radar, which enabled an aircraft to home in on a point on the ground even in thick weather. It was however difficult to carry or conceal. SOE also developed the S-Phone, which allowed a pilot or radio operator aboard an aircraft to communicate by voice with the "reception committee". Sound quality was good enough for voices to be recognisable, so that a mission could be aborted if there was any doubt of an agent's identity.
Sea.
SOE also experienced difficulties with the Royal Navy, who were usually unwilling to allow SOE to use its submarines or motor torpedo boats to deliver agents or equipment. Submarines were regarded as too valuable to risk within range of enemy coastal defences, and MTBs were in any case often too noisy and conspicuous for clandestine landings. However, SOE often used clandestine craft such as local fishing boats or caiques and eventually ran quite large fleets of these, from the Helford estuary, Algiers, the Shetland Islands (a service termed the Shetland Bus), Ceylon etc.
Operations.
France.
SOE's operations were usually mounted in order to feel out resistance groups willing to work with the Allies in preparation for invasion. In France, personnel were directed by two London-based country sections. F Section was under British control, while RF Section was linked to General de Gaulle's Free French government in exile. Most native French agents served in RF. There were also two smaller sections: EU/P Section, which dealt with the Polish community in France, and the DF Section which was responsible for establishing escape routes. During the latter part of 1942 another section known as AMF was established in Algiers, to operate into Southern France.
On 5 May 1941, Georges Bégué (1911–1993) became the first SOE agent dropped into German occupied France. He then set up radio communications and met the next agents parachuted into France. Between Bégué's first drop in May 1941 and August 1944, more than four hundred F Section agents were sent into occupied France. They served in a variety of functions including arms and sabotage instructors, couriers, circuit organisers, liaison officers and radio operators. RF sent about the same number; AMF sent 600 (although not all of these belonged to SOE). EU/P and DF sent a few dozen agents each.
SOE included a number of women (who were often commissioned into women's branches of the armed forces such as the First Aid Nursing Yeomanry). F Section alone sent 39 female agents into the field, of whom 13 did not return. The Valençay SOE Memorial was unveiled at Valençay in the Indre "département" of France on 6 May 1991, marking the fiftieth anniversary of the despatch of F Section's first agent to France. The memorial's roll of honour lists the names of the 91 men and 13 women members of the SOE who gave their lives for France's freedom.
To support the Allied invasion of France on D Day in June 1944, three-man parties were dropped into various parts of France as part of Operation Jedburgh, to coordinate widespread overt (as opposed to clandestine) acts of resistance. A total of 100 men were eventually dropped, together with 6,000 tons of military stores (4,000 tons had been dropped during the years before D-Day). At the same time, all the various sections operating in France (except EU/P) were nominally placed under a London-based HQ titled État-major des Forces Françaises de l'Intérieur (EMFFI).
Poland.
SOE did not need to instigate Polish resistance, because unlike the Vichy French the Poles overwhelmingly refused to collaborate with the Nazis. Early in the war the Poles established the Polish Home Army, led by a clandestine resistance government known as the Polish Secret State. Nevertheless, there were many Polish members of SOE and much cooperation between the SOE and the Polish resistance.
SOE assisted the Polish government in exile with training facilities and logistical support for its 605 special forces operatives known as the Cichociemni, or ""The Dark and Silent"". Members of the unit, which was based in Audley End House, Essex, were rigorously trained before being parachuted into occupied Poland. Because of the distance involved in air travel to Poland, customised aircraft with extra fuel capacity were used in Polish operations such as Operation Wildhorn III. Sue Ryder chose the title Baroness Ryder of Warsaw in honour of these operations.
Secret Intelligence Service member Krystyna Skarbek ("nom de guerre" Christine Granville) was a founder member of SOE and helped establish a cell of Polish spies in Central Europe. She ran several operations in Poland, Egypt, Hungary (with Andrzej Kowerski) and France, often using the staunchly anti-Nazi Polish expatriate community as a secure international network. Non-official cover agents Elzbieta Zawacka and Jan Nowak-Jezioranski perfected the Gibraltar courier route out of occupied Europe. Maciej Kalenkiewicz was parachuted into occupied Poland, only to be killed by the Soviets. A Polish agent was integral to SOE's Operation Foxley, the plan to assassinate Hitler.
Thanks to cooperation between SOE and the Polish Home Army, the Poles were able to deliver the first Allied intelligence on the Holocaust to London in June 1942. Witold Pilecki of the Polish Home Army designed a joint operation with SOE to liberate Auschwitz, but the British rejected it as infeasible. Joint Anglo-Polish operations provided London with vital intelligence on the V-2 rocket, German troops movements on the Eastern Front, and the Soviet repressions of Polish citizens.
RAF 'Special Duties Flights' were sent to Poland to assist the Warsaw Uprising against the Nazis. The rebellion was defeated with a loss of 200,000 casualties (mostly German executions of Polish civilians) after the nearby Red Army refused military assistance to the Polish Home Army. RAF Special Duties Flights were refused landing rights at Soviet-held airfields near Warsaw, even when requiring emergency landings after battle damage. These flights were also attacked by Soviet fighters, despite the U.S.S.R.'s officially Allied status.
Germany.
Due to the dangers and lack of friendly population few operations were conducted in Germany itself. The German and Austrian section of SOE was run by Lieutenant Colonel Ronald Thornley for most of the war, and was mainly involved with black propaganda and administrative sabotage in collaboration with the German section of the Political Warfare Executive. After D-Day, the section was re-organised and enlarged with Major General Gerald Templer heading the Directorate, with Thornley as his deputy.
Several major operations were planned, including Operation "Foxley", a plan to assassinate Hitler, and Operation "Periwig", an ingenious plan to simulate the existence of a large-scale anti-Nazi resistance movement within Germany. "Foxley" was never carried out but "Periwig" went ahead despite restrictions placed on it by SIS and SHAEF. Several German prisoners of war were trained as agents, briefed to make contact with the anti-Nazi resistance and to conduct sabotage. They were then parachuted into Germany in the hope that they would either hand themselves in to the "Gestapo" or be captured by them, and reveal their supposed mission. Fake coded wireless transmissions were broadcast to Germany and various pieces of agent paraphernalia such as code books and wireless receivers were allowed to fall into the hands of the German authorities.
The Netherlands.
Section N of SOE ran operations in the Netherlands. They committed some of SOE's worst blunders in security, which allowed the Germans to capture many agents and much sabotage material, in what the Germans called the 'Englandspiel'. SOE apparently ignored the absence of security checks in radio transmissions, and other warnings from their chief cryptographer, Leo Marks, that the Germans were running the supposed resistance networks. A total of 50 agents were caught and brought to Camp Haaren in the South of the Netherlands.
Five captured men managed to escape from the camp. Two of them, Pieter Dourlein and Ben Ubbink, escaped on 29 August 1943 and found their way to Switzerland. There, the Netherlands Embassy sent messages over their controlled sets to England that SOE Netherlands was compromised. SOE set up new networks, which continued to operate until the Netherlands were liberated at the end of the war.
Belgium.
Section T established some effective networks in Belgium, in part orchestrated by fashion designer Hardy Amies, who rose to the rank of Lieutenant Colonel. Amies adapted names of fashion accessories for use as code words, while managing some of the most murderous and ruthless agents in the field.
In the aftermath of the Battle of Normandy, British armoured forces liberated the country in less than a week, giving the resistance little time to stage an uprising. They did assist British forces to bypass German rearguards, and this allowed the Allies to capture the vital docks at Antwerp intact (although a protracted and bloody Battle of the Scheldt was later fought to clear the Scheldt estuary before the Allies could use the port).
After Brussels was liberated, Amies outraged his superiors by setting up a "Vogue" photo-shoot in Belgium. In 1946, he was Knighted in Belgium for his service with SOE, being a Named Officier de l'Ordre de la Couronne.
Italy.
As both an enemy country, and supposedly a monolithic fascist state with no organised opposition which SOE could use, SOE made little effort in Italy before mid-1943, when Mussolini's government collapsed and Allied forces already occupied Sicily. In April 1941, in a mission codenamed "Yak", Peter Fleming attempted to recruit agents from among the many thousands of Italian prisoners of war captured in the Western Desert Campaign. He met with no response. Attempts to search among Italian immigrants in the United States, Britain and Canada for agents to be sent to Italy had similarly poor results.
During the first three years of war, the most important "episode" of the collaboration between SOE and Italian anti-fascism was a project of an anti-fascist uprising in Sardinia, which the SOE supported at some stage but did not receive approval from the Foreign Office.
In the aftermath of the Italian collapse, SOE (in Italy renamed N. 1 Special Force) helped build a large resistance organisation in the cities of Northern Italy, and in the Alps. Italian partisans harassed German forces in Italy throughout the autumn and winter of 1944, and in the Spring 1945 offensive in Italy they captured Genoa and other cities unaided by Allied forces. SOE helped the Italian Resistance send British missions to the partisan formations and supply war material to the bands of patriots, a supply made without political prejudices, and which also helped the Communist formations (Brigate Garibaldi).
Late in 1943, SOE established a base at Bari in Southern Italy, from which they operated their networks and agents in the Balkans. This organisation had the codename ""Force 133"". This later became ""Force 266"", reserving 133 for operations run from Cairo rather than the heel of Italy. Flights from Brindisi were run to the Balkans and Poland, particularly once control had been wrested from SOE's Cairo headquarters and was exercised directly by Gubbins. SOE established a new packing station for the parachute containers close to Brindisi Air base, along the lines of those created at Saffron Walden. This was ME 54, a factory employing hundreds, the American (OSS) side of which was known as "Paradise Camp".
Yugoslavia.
In the aftermath of the German invasion in 1941, the Kingdom of Yugoslavia fragmented. In Croatia, there was a substantial pro-Axis movement, the Ustaše. In Croatia as well as the remainder of Yugoslavia, two resistance movements formed; the royalist Chetniks under Draža Mihailović, and the Communist Partisans under Josip Broz Tito.
Mihailović was the first to attempt to contact the Allies, and SOE despatched a party on 20 September 1941 under Major ""Marko"" Hudson. Hudson also encountered Tito's forces. Through the royalist government in exile, SOE at first supported the Chetniks. Eventually, however, due to reports that the Chetniks were less effective and even collaborating with German and Italian forces on occasion, British support was redirected to the Partisans, even before the Tehran Conference in 1943.
Although relations were often touchy throughout the war, it can be argued that SOE's unstinting support was a factor in Yugoslavia's maintaining a neutral stance during the Cold War. However, accounts vary dramatically between all historical works on the ""Chetnik controversy"".
Hungary.
SOE was unable to establish links or contacts in Hungary before the regime of Miklós Horthy aligned itself with the Axis Powers. Distance and lack of such contacts prevented any effort being made by SOE until the Hungarians themselves dispatched a diplomat (László Veress) in a clandestine attempt to contact the Western Allies. SOE facilitated his return, with some radio sets. Before the Allied governments could agree terms, Hungary was placed under German military occupation and Veress was forced to flee the country.
Two missions subsequently dropped "blind" i.e. without prior arrangement for a reception party, failed. So too did an attempt by Basil Davidson to incite a partisan movement in Hungary, after he made his way there from northeastern Yugoslavia.
Greece.
Greece was overrun by the Axis after a desperate defence lasting several months. In the aftermath, SIS and another intelligence organisation, SIME, discouraged attempts at sabotage or resistance as this might imperil relations with Turkey, although SOE maintained contacts with resistance groups in Crete. When an agent, "Odysseus", a former tobacco-smuggler, attempted to contact potential resistance groups in Greece, he reported that no group was prepared to cooperate with the monarchist government in exile in Cairo.
In late 1942, at the army's instigation, SOE mounted its first operation, codenamed Operation "Harling", into Greece in an attempt to disrupt the railway which was being used to move materials to the German Panzer Army Africa. A party under Colonel (later Brigadier) Eddie Myers, assisted by Christopher Woodhouse, was parachuted into Greece and discovered two guerrilla groups operating in the mountains: the pro-Communist ELAS and the republican EDES. On 25 November 1942, Myers's party blew up one of the spans of the railway viaduct at Gorgopotamos, supported by 150 Greek partisans from these two organisations who engaged Italians guarding the viaduct. This cut the railway linking Thessaloniki with Athens and Piraeus.
Relations between the resistance groups and the British soured. When the British needed once again to disrupt the railway across Greece as part of the deception operations preceding Operation "Husky", the Allied invasion of Sicily, the resistance groups refused to take part, rightly fearing German reprisals against civilians. Instead, a six-man commando party from the British and New Zealand armies, led by New Zealander Lieutenant Colonel Cecil Edward Barnes a civil engineer, carried out the destruction of the Asopos viaduct on 21 June 1943. Two attempts by Mike Cumberlege to make the Corinth Canal unnavigable ended in failure. 
EDES received most aid from SOE, but ELAS secured many weapons when Italy collapsed and Italian military forces in Greece dissolved. ELAS and EDES fought a vicious civil war in 1943 until SOE brokered an uneasy armistice (the Plaka agreement).
A lesser known, but important function of the SOE in Greece was to inform the Cairo headquarters of the movement of the German military aircraft that were serviced and repaired at the two former Greek military aircraft facilities in and around Athens.
Eventually, the British Army occupied Athens and Piraeus in the aftermath of the German withdrawal, and fought a street-by-street battle to drive ELAS from these cities and impose an interim government under Archbishop Damaskinos. SOE's last act was to evacuate several hundred disarmed EDES fighters to Corfu, preventing their massacre by ELAS.
Crete.
In Crete there were several resistance groups and Allied stay-behind parties after the Germans occupied the island in the Battle of Crete. SOE's operations on Crete involved figures such as Patrick Leigh Fermor, John Lewis, Harry Rudolph Fox Burr, Tom Dunbabin, Sandy Rendel, John Houseman, Xan Fielding and Bill Stanley Moss. Some of the most famous moments included the abduction of General Heinrich Kreipe led by Leigh Fermor and Moss - subsequently portrayed in the film "Ill Met by Moonlight", and the sabotage of Damasta led by Moss.
Albania.
Albania had been under Italian influence since 1923, and was occupied by the Italian Army in 1939. In 1943, a small liaison party entered Albania from northwestern Greece. SOE agents who entered Albania then or later included Julian Amery, Anthony Quayle, David Smiley and Neil "Billy" McLean. They discovered another internecine war between the Communist partisans under Enver Hoxha, and the republican Balli Kombëtar. As the latter had collaborated with the Italian occupiers, Hoxha gained Allied support.
SOE's envoy to Albania, Brigadier Edmund "Trotsky" Davies, was captured by the Germans early in 1944. Some SOE officers warned that Hoxha's aim was primacy after the war, rather than fighting Germans. They were ignored, but Albania was never a major factor in the effort against the Germans.
Czechoslovakia.
SOE sent many missions into the Czech areas of the so-called Protectorate of Bohemia and Moravia, and later into Slovakia. The most famous mission was Operation Anthropoid, the assassination of SS-Obergruppenführer Reinhard Heydrich in Prague. From 1942 to 1943 the Czechoslovaks had their own Special Training School (STS) at Chicheley Hall in Buckinghamshire. In 1944, SOE sent men to support the Slovak National Uprising.
Norway.
In March 1941 a group performing commando raids in Norway, Norwegian Independent Company 1 (NOR.I.C.1) was organised under leadership of Captain Martin Linge. Their initial raid in 1941 was Operation Archery, the best known raid was probably the Norwegian heavy water sabotage. Communication lines with London were gradually improved so that by 1945, 64 radio operators were spread throughout Norway.
Denmark.
Most of the actions conducted by the Danish resistance were railway sabotage to hinder German troop and material movements from and to Norway. However, there were examples of sabotage on a much larger scale especially by BOPA. In all over 1,000 operations were conducted from 1942 and onwards.
In October 1943 the Danish resistance also saved nearly all of the Danish Jews from certain death in German concentration camps. This was a massive overnight operation and is to this day recognised among Jews as one of the most significant displays of public defiance against the Germans.
The Danish resistance assisted SOE in its activities in neutral Sweden. For example, SOE was able to obtain several shiploads of vital ball-bearings which had been interned in Swedish ports. The Danes also pioneered several secure communications methods; for example, a burst transmitter/receiver which transcribed Morse code onto a paper tape faster than a human operator could handle.
Romania.
In 1943 an SOE delegation was parachuted into Romania to instigate resistance against the Nazi occupation at ""any cost"" (Operation Autonomous). The delegation, including Colonel Gardyne de Chastelain, Captain Silviu Meţianu and Ivor Porter, was captured by the Romanian Gendarmerie and held until the night of King Michael's Coup on 23 August 1944.
Other operations in Europe.
Through cooperation with the Special Operations Executive and the British intelligence service, a group of Jewish volunteers from Palestine were sent on missions to several countries in Nazi-occupied Europe from 1943 to 1945.
Abyssinia.
Abyssinia was the scene of some of SOE's earliest and most successful efforts. SOE organised a force of Ethiopian irregulars under Orde Charles Wingate in support of the exiled Emperor Haile Selassie. This force (named Gideon Force by Wingate) caused heavy casualties to the Italian occupation forces, and contributed to the successful British campaign there. Wingate was to use his experience to create the Chindits in Burma.
Southeast Asia.
As early as 1940, SOE was preparing plans for operations in Southeast Asia. As in Europe, after initial Allied military disasters, SOE built up indigenous resistance organisations and guerrilla armies in enemy (Japanese) occupied territory. SOE also launched ""Operation Remorse"" (1944–45), which was ultimately aimed at protecting the economic and political status of Hong Kong. Force 136 engaged in covert trading of goods and currencies in China. Its agents proved remarkably successful, raising £77m through their activities, which were used to provide assistance for Allied prisoners of war and, more controversially, to buy influence locally in order to facilitate a smooth return to pre-war conditions.
Later analysis and commentaries.
The mode of warfare encouraged and promoted by SOE is considered by several modern commentators to have established the modern model that many alleged terrorist organisations emulate.
Two opposed views were quoted by Tony Geraghty in "The Irish War: The Hidden Conflict Between the IRA and British Intelligence". M. R. D. Foot, who wrote several official histories of SOE wrote:
However the British military historian John Keegan wrote:

</doc>
<doc id="28899" url="https://en.wikipedia.org/wiki?curid=28899" title="System request">
System request

System request (often abbreviated SysRq or Sys Req) is a key on keyboards for PCs that has no standard use.
History.
Introduced by IBM with the PC/AT, it was intended to be available as a special key to directly invoke low-level operating system functions with no possibility of conflicting with any existing software. A special BIOS routine — software interrupt 0x15, subfunction 0x85 — was added to signal the OS when SysRq was pushed or released. Unlike most keys, when it is pressed nothing is stored in the keyboard buffer.
The specific low level function that the SysRq key was meant for was to switch between operating systems. When the original IBM-PC was created in 1980, there were three leading competing operating systems: PC DOS, CP/M-86, and UCSD p-System, while Xenix was added in 1983-1984. The SysRq key was added so that multiple operating systems could be run on the same computer, making use of the capabilities of the 286 chip in the PC/AT.
A special key was needed because most software of the day operated at a low level, often bypassing the OS entirely, and typically made use of many hotkey combinations. The use of Terminate and Stay Resident (TSR) programs further complicated matters. To implement a task switching or multitasking environment, it was thought that a special, separate key was needed. This is similar to the way “Control-Alt-Delete” is used under Windows NT.
On 84-key keyboards (except the 84-key IBM Model M space saver keyboard), SysRq was a key of its own. On the later 101-key keyboard, it shares a physical key with the Print Screen key function. One must hold down the Alt key while pressing this “dual-function” key to invoke SysRq.
The default BIOS keyboard routines simply ignore SysRq and return without taking action. So did the MS-DOS input routines. The keyboard routines in libraries supplied with many high-level languages followed suit. Although it is still included on most PC keyboards manufactured, and though it is used by some debugging software, the key is of no use for the vast majority of users.
On the Hyundai/Hynix Super-16 computer, pressing will hard boot the system (it will reboot when is unresponsive, and it will invoke startup memory tests that are bypassed on soft-boot).
Modern uses.
In Linux, the kernel can be configured to provide functions for system debugging and crash recovery. This use is known as the “Magic SysRq key”.
Microsoft has also used SysRq for various OS- and application-level debuggers. In the CodeView debugger, it was sometimes used to break into the debugging during program execution. For the Windows NT remote kernel debugger, it can be used to force the system into the debugger.
In embedded systems, SysRq key is usually used to assert low-level on RESET# signal.
Similar keys.
IBM 3270-type console keyboards of the IBM System/370 mainframe computer, created in 1970, had an operator interrupt key that was used to cause the operating system such as VM/370 or MVS to allow the console to give input to the operating system.

</doc>
<doc id="28900" url="https://en.wikipedia.org/wiki?curid=28900" title="Split infinitive">
Split infinitive

In the English language, a split infinitive or cleft infinitive is a grammatical construction in which a word or phrase comes between the "to" and the bare infinitive of the "to" form of the infinitive verb. Usually an adverb or adverbial phrase comes between them.
A well-known example occurs in the opening sequence of the "Star Trek" television series: "to "boldly" go where no man has gone before"; the adverb "boldly" is said to split the infinitive "to go". Sometimes more than one word splits the infinitive, as in: "The population is expected to "more than" double in the next ten years".
In the 19th century, some grammatical authorities sought to introduce a prescriptive rule against it. The construction is still the subject of disagreement, though modern English usage guides have dropped the objection to the split infinitive.
History of the construction.
Middle English.
In Old English, infinitives were single words ending in "-n" or "-an" (compare modern Dutch and German "-n", "-en"). Gerunds were formed using "to" followed by a verbal noun in the dative case, which ended in "-anne" or "-enne" (e.g. "tō cumenne" = "coming, to come"). In Middle English, the bare infinitive and the gerund coalesced into the same form ending in "-(e)n" (e.g. "comen" "come"; "to comen" "to come"). The "to" infinitive was not split in Old or Early Middle English.
The first known example of a split infinitive in English, in which a pronoun rather than an adverb splits the infinitive, is in Layamon's "Brut" (early 13th century):
This may be a poetic inversion for the sake of meter, and therefore says little about whether Layamon would have felt the construction to be syntactically natural. However, no such reservation applies to the following prose example from John Wycliffe (14th century), who was fond of splitting infinitives:
Modern English.
After its rise in Middle English, the construction became rare in the 15th and 16th centuries. William Shakespeare used it once, or perhaps twice, which appears to be a syntactical inversion for the sake of rhyme:
Edmund Spenser, John Dryden, Alexander Pope, and the King James Version of the Bible used none, and they are very rare in the writing of Samuel Johnson. John Donne used them several times, though, and Samuel Pepys also used at least one. No reason for the near disappearance of the split infinitive is known; in particular, no prohibition is recorded.
Split infinitives reappeared in the 18th century and became more common in the 19th.
Daniel Defoe, Benjamin Franklin, William Wordsworth, Abraham Lincoln, George Eliot, Henry James, and Willa Cather are among the writers who used them. Examples in the poems of Robert Burns attest its presence also in 18th century Scots:
In colloquial speech the construction came to enjoy widespread use. Today, according to the "American Heritage Book of English Usage", "people split infinitives all the time without giving it a thought." In corpora of contemporary spoken English, some adverbs such as "always" and "completely" appear more often in the split position than the unsplit.
Theories of origins.
Although it is difficult to say why the construction developed in Middle English, or why it revived so powerfully in Modern English, a number of theories have been postulated.
Analogy.
Traditional grammarians have suggested that the construction appeared because people frequently place adverbs before finite verbs. George Curme writes: "If the adverb should immediately precede the finite verb, we feel that it should immediately precede also the infinitive…" Thus if one says:
one may, by analogy, wish to say:
This is supported by the fact that split infinitives are often used as echoes, as in the following exchange, in which the riposte parodies the slightly odd collocation in the original sentence:
Here is an example of an adverb being transferred into split infinitive position from a parallel position in a different construction.
Transformational grammar.
Transformational grammarians have attributed the construction to a re-analysis of the role of "to".
Types.
In the modern language, splitting usually involves a single adverb coming between the verb and its marker. Very frequently, this is an emphatic adverb, for example:
Sometimes it is a negation, as in the self-referential joke:
However, in modern colloquial English almost any adverb may be found in this syntactic position, especially when the adverb and the verb form a close syntactic unit (really-pull, not-split).
Compound split infinitives, i.e., infinitives split by more than one word, usually involve a pair of adverbs or a multi-word adverbial:
Examples of non-adverbial elements participating in the split-infinitive construction seem rarer in Modern English than in Middle English. The pronoun "all" commonly appears in this position:
and may even be combined with an adverb:
This is an extension of the subject pronoun ("you all"). However an object pronoun, as in the Layamon example above, would be unusual in modern English, perhaps because this might cause a listener to misunderstand the "to" as a preposition:
While, structurally, acceptable as 'poetic' formulation, this would result in a garden path sentence - particularly evident if the indirect object is omitted:
Other parts of speech would be very unusual in this position. However, in verse, poetic inversion for the sake of meter or of bringing a rhyme word to the end of a line often results in abnormal syntax, as with Shakespeare's split infinitive ("to pitied be", cited above), in fact an inverted passive construction in which the infinitive is split by a past participle. Presumably, this would not have occurred in a prose text by the same author.
History of the term.
It was not until the very end of the 19th century that terminology emerged to describe the construction. According to the main etymological dictionaries, the earliest use of the term "split infinitive" on record dates from 1897, with "infinitive-splitting" and "infinitive-splitter" following in 1926 and 1927 respectively. The now rare "cleft infinitive" is slightly older, attested from 1893. The term "compound split infinitive" is not found in these dictionaries and appears to be very recent.
This terminology implies analysing the full infinitive as a two-word infinitive, which not all grammarians accept. As one who used "infinitive" to mean the single-word verb, Otto Jespersen challenged the epithet: "'To' is no more an essential part of an infinitive than the definite article is an essential part of a nominative, and no one would think of calling 'the good man' a split nominative." However, no alternative terminology has been proposed.
History of the controversy.
Although it is sometimes reported that a prohibition on split infinitives goes back to Renaissance times, and frequently the 18th century scholar Robert Lowth is cited as the originator of the prescriptive rule, such a rule is not to be found in Lowth's writing and is not known to appear in any other text prior to the mid-19th century.
Possibly the earliest comment against split infinitives was by an anonymous American in 1834:
The practice of separating the prefix of the infinitive mode from the verb, by the intervention of an adverb, is not unfrequent among uneducated persons ... I am not conscious, that any rule has been heretofore given in relation to this point ... The practice, however, of not separating the particle from its verb, is so general and uniform among good authors, and the exceptions are so rare, that the rule which I am about to propose will, I believe, prove to be as accurate as most rules, and may be found beneficial to inexperienced writers. It is this :—"The particle," TO, "which comes before the verb in the infinitive mode, must not be separated from it by the intervention of an adverb or any other word or phrase; but the adverb should immediately precede the particle, or immediately follow the verb."
In 1840, Richard Taylor also condemned split infinitives as a "disagreeable affectation", and in 1859, Solomon Barrett, Jr., called them "a common fault". However, the issue seems not to have attracted wider public attention until Henry Alford addressed it in his "Plea for the Queen's English" in 1864:
A correspondent states as his own usage, and defends, the insertion of an adverb between the sign of the infinitive mood and the verb. He gives as an instance, ""to scientifically illustrate"". But surely this is a practice entirely unknown to English speakers and writers. It seems to me, that we ever regard the "to" of the infinitive as inseparable from its verb. And, when we have already a choice between two forms of expression, "scientifically to illustrate" and "to illustrate scientifically," there seems no good reason for flying in the face of common usage.
Others followed, among them Bache, 1869 ("The "to" of the infinitive mood is inseparable from the verb"); William B. Hodgson, 1889; and Raub, 1897 ("The sign "to" must not be separated from the remaining part of the infinitive by an intervening word").
Even as these authorities were condemning the split infinitive, others were endorsing it: Brown, 1851 (saying some grammarians had criticized it and it was less elegant than other adverb placements but sometimes clearer); Hall, 1882; Onions, 1904; Jespersen, 1905; and Fowler and Fowler, 1906. Despite the defence by some grammarians, by the beginning of the 20th century the prohibition was firmly established in the press. In the 1907 edition of "The King's English", the Fowler brothers wrote:
The 'split' infinitive has taken such hold upon the consciences of journalists that, instead of warning the novice against splitting his infinitives, we must warn him against the curious superstition that the splitting or not splitting makes the difference between a good and a bad writer.
In large parts of the school system, the construction was opposed with ruthless vigour. A correspondent to the BBC on a programme about English grammar in 1983 remarked:
One reason why the older generation feel so strongly about English grammar is that we were severely punished if we didn't obey the rules! One split infinitive, one whack; two split infinitives, two whacks; and so on.
As a result, the debate took on a degree of passion which the bare facts of the matter never warranted. There was frequent skirmishing between the splitters and anti-splitters until the 1960s. George Bernard Shaw wrote letters to newspapers supporting writers who used the split infinitive, and Raymond Chandler complained to the editor of "The Atlantic Monthly" about a proofreader who changed Chandler's split infinitives:
By the way, would you convey my compliments to the purist who reads your proofs and tell him or her that I write in a sort of broken-down patois which is something like the way a Swiss-waiter talks, and that when I split an infinitive, God damn it, I split it so it will remain split, and when I interrupt the velvety smoothness of my more or less literate syntax with a few sudden words of barroom vernacular, this is done with the eyes wide open and the mind relaxed and attentive. The method may not be perfect, but it is all I have.
Post-1960 authorities show a strong tendency to accept the split infinitive. Follett, in "Modern American Usage" (1966) writes: "The split infinitive has its place in good composition. It should be used when it is expressive and well led up to." Fowler (Gower's revised second edition, 1983) offers the following example of the consequences of refusal to split infinitives: "The greatest difficulty about assessing the economic achievements of the Soviet Union is that its spokesmen try "absurdly to exaggerate" them; in consequence the visitor may tend "badly to underrate" them" (italics added). This question results: "Has dread of the split infinitive led the writer to attach the adverbs ['absurdly' and 'badly'] to the wrong verbs, and would he not have done better "to boldly split" both infinitives, since he cannot put the adverbs after them without spoiling his rhythm" (italics added)? Bernstein (1985) argues that, although infinitives should not always be split, they should be split where doing so improves the sentence: "The natural position for a modifier is before the word it modifies. Thus the natural position for an adverb modifying an infinitive should be just ... "after" the to" (italics added). Bernstein continues: "Curme's contention that the split infinitive is often an improvement ... cannot be disputed." Heffernan and Lincoln, in their modern English composition textbook, agree with the above authors. Some sentences, they write, "are weakened by ... cumbersome splitting", but in other sentences "an infinitive may be split by a one-word modifier that would be awkward in any other position."
Principal objections to the split infinitive.
Objections to the split infinitive fall into three categories, of which only the first is accorded any credence by linguists.
The descriptivist objection.
Like most linguistic prescription, disapproval of the split infinitive was originally based on the descriptive observation that it was not in fact a feature of the prestige form of English which those proscribing it wished to champion. This is made explicit in the anonymous 1834 text, the first known statement of the position, and in Alford's objection in 1864, the first truly influential objection to the construction, both cited above. The descriptivist objection involves a person whose idiolect does not have the construction advising against its use on the grounds that it is not the norm: thus, many English speakers avoid split infinitives not because they follow a prescriptive rule, but simply because it was not part of the language that they learned as children. However, as the construction grows in popularity, the strength of the descriptivist objection is progressively reduced.
Many of those who avoid split infinitives differentiate according to type and register. Infinitives split by multi-word phrases ("compound split infinitives") and those split by pronouns are demonstrably less usual than the straightforward example of an infinitive split by an adverb. Likewise, split infinitives are far more common in speech and informal writing than in academic writing. Thus, while an outright rejection of the split infinitive is no longer sustainable on descriptive grounds (as it was in 1834), the advice to avoid it in formal settings, and to avoid some types in particular, remains a tenable position. The prescriptive rule of thumb draws on the descriptive observation that certain split infinitives are not usual in certain situations. For examples of modern linguists using such arguments, see the current views section below.
The argument from the full infinitive.
A second argument is summed up by Alford's statement "It seems to me that we ever regard the "to" of the infinitive as inseparable from its verb."
The "to" in the infinitive construction, which is found throughout the Germanic languages, is originally a preposition before the dative of a verbal noun, but in the modern languages it is widely regarded as a particle which serves as a marker of the infinitive. In German, this marker ("zu") sometimes precedes the infinitive, but is not regarded as part of it. In English, on the other hand, it is traditional to speak of the "bare infinitive" without "to" and the "full infinitive" with it, and to conceive of "to" as part of the full infinitive. (In the sentence "I made my daughter clean her room," "clean" is a bare infinitive; in "I told my daughter to clean her room," "to clean" is a full infinitive.) Possibly this is because the absence of an "inflected" infinitive form made it useful to include the particle in the citation form of the verb, and in some nominal constructions in which other Germanic languages would omit it (e.g. "to know her is to love her"). The concept of a two-word infinitive can reinforce an intuitive sense that the two words belong together. For instance, the rhetorician John Duncan Quackenbos said, ""To have" is as much one thing, and as inseparable by modifiers, as the original form "habban", or the Latin "habere"." The usage writer John Opdycke based a similar argument on the closest French, German, and Latin translations.
The two-part infinitive is disputed, however, and some linguists would say that the infinitive in English is also a single-word verb form, which may or may not be preceded by the particle "to". Some modern generative analysts classify "to" as a "peculiar" auxiliary verb; other analysts, as the infinitival subordinator. Moreover, even when the concept of the full infinitive is accepted, it does not necessarily follow that any two words that belong together grammatically need be adjacent to each other. They usually are, but counter-examples are easily found, such as an adverb splitting a two-word finite verb ("will not do", "has not done").
The argument from classical languages.
A frequently discussed argument states that the split-infinitive prohibition is based on Latin. An infinitive in Latin is never used with a marker equivalent to English "to", and thus there is no parallel there for the construction. The claim that those who dislike split infinitives are applying rules of Latin grammar to English is asserted in many references that accept the split infinitive. One example is in the "American Heritage Book of English Usage": "The only rationale for condemning the construction is based on a false analogy with Latin." In more detail, the usage author Marilyn Moriarty states:
The rule forbidding a split infinitive comes from the time when Latin was the universal language of the world. All scholarly, respectable writing was done in Latin. Scientists and scholars even took Latin names to show that they were learned. In Latin, infinitives appear as a single word. The rule which prohibits splitting an infinite shows deference to Latin and to the time when the rules which governed Latin grammar were applied to other languages.
The assertion is also made in the "Oxford Guide to Plain English", "Compact Oxford English Dictionary", and Steven Pinker’s "Language Instinct," among other sources.
The argument implies an adherence to the humanist idea of the greater purity of the classics, which particularly in Renaissance times led people to regard aspects of English that differed from Latin as inferior. However by the 19th century such views were no longer widespread; Moriarty is in error about the age of the prohibition. It has also been stated that an argument from Latin would be fallacious because "there is no precedent in these languages for condemning the split infinitive because in Greek and Latin (and all the other romance languages) the infinitive is a single word that is impossible to sever."
However, this argument is something of a straw man argument as very few "proponents" of the rule argue from Latin in any case. Certainly, it is clear that dislike of the split infinitive does not originate from Latin. As shown above, none of the prescriptivists who started the split-infinitive controversy in the 19th century mentioned Latin in connection with it. Occasionally teachers and bloggers can be found who do oppose the split infinitive with such an argument, but it is not found in any statements of the position from the 19th or early 20th century, when the prohibition developed. Of the writers cited here (and the many others consulted) who ascribe the split-infinitive prohibition to Latinism, none cite an authority who condemned the construction on that basis. According to Richard Bailey, the prohibition does not come from a comparison with Latin, and the belief that it does is “part of the folklore of linguistics.”
Current views.
Present style and usage manuals deem simple split infinitives unobjectionable. For example, Curme's "Grammar of the English Language" (1931) says that not only is the split infinitive correct, but it "should be furthered rather than censured, for it makes for clearer expression." "The Columbia Guide to Standard American English" notes that the split infinitive "eliminates all possibility of ambiguity," in contrast to the "potential for confusion" in an unsplit construction. "Merriam-Webster's Dictionary of English Usage" says, "the objection to the split infinitive has never had a rational basis." According to Mignon Fogarty, "today almost everyone agrees that it is OK to split infinitives."
Nevertheless, many teachers of English still admonish students against using split infinitives in writing. Because the prohibition has become so widely known, the "Columbia Guide" recommends that writers "follow the conservative path avoiding split infinitives when they are not necessary, especially when you're uncertain of your readers' expectations and sensitivities in this matter." Likewise, the Oxford Dictionaries do not regard the split infinitive as ungrammatical, but on balance consider it likely to produce a weak style and advise against its use for formal correspondence. R. W. Burchfield's revision of Fowler's "Modern English Usage" goes farther (quoting Burchfield's own 1981 book "The Spoken Word"): "Avoid splitting infinitives whenever possible, but do not suffer undue remorse if a split infinitive is unavoidable for the completion of a sentence already begun." Still more strongly, the style guide of "The Economist" says, "Happy the man who has never been told that it is wrong to split an infinitive: the ban is pointless. Unfortunately, to see it broken is so annoying to so many people that you should observe it."
As well as register, tolerance of split infinitives varies according to type. While most authorities accept split infinitives in general, it is not hard to construct an example which any native speaker would reject. Interestingly, Wycliff's Middle English compound split would, if transferred to modern English, be regarded by most people as un-English:
Attempts to define the boundaries of normality are controversial. In 1996, the usage panel of "The American Heritage Book" was evenly divided for and against such sentences as,
but more than three-quarters of the panel rejected
Here the problem appears to be the breaking up of the verbal phrase "to be seeking a plan to relieve": a segment of the head verbal phrase is so far removed from the remainder that the listener or reader must expend greater effort to understand the sentence. By contrast, 87 percent of the panel deemed acceptable the multi-word adverbial in
not surprisingly perhaps, because here there is no other place to put the words "more than" without substantially recasting the sentence.
Although the usage of 'not' in splitting infinitives is an issue that has not attracted much attention from the English users, contemporary English grammar puts the phrase into the same category as above. This appears to be because the traditional idiom, placing the negation before the marker ("I soon learned not to provoke her") or with verbs of desire, negating the finite verb ("I don't want to see you anymore") remains easy and natural, and is still overwhelmingly the more common construction in British English. On the other hand, in the case of American English, a search on the corpus of contemporary American English displays a different aspect. Its usage is more commonly found in the American context, where we can find at least 2200 cases of the usage of 'not' in splitting infinitives.
Some argue that the two forms have different meanings, while others see a grammatical difference, but most speakers do not make such a distinction.
Avoiding split infinitives.
Writers who avoid splitting infinitives either place the splitting element elsewhere in the sentence or reformulate the sentence, perhaps rephrasing it without an infinitive and thus avoiding the issue. However, a sentence such as "to more than double" must be completely rewritten to avoid the split infinitive; it is ungrammatical to put the words "more than" anywhere else in the sentence. While split infinitives can be avoided, a writer must be careful not to produce an awkward or ambiguous sentence. Fowler (1926) stressed that, if a sentence is to be rewritten to remove a split infinitive, this must be done without compromising the language:
It is of no avail merely to fling oneself desperately out of temptation; one must so do it that no traces of the struggle remain; that is, sentences must be thoroughly remodeled instead of having a word lifted from its original place & dumped elsewhere ...
In some cases, moving the adverbial creates an ungrammatical sentence or changes the meaning. R. L. Trask uses this example:
The sentence can be rewritten to maintain its meaning, however, by using a noun or a different grammatical aspect of the verb, or by eschewing the informal "get rid":
Fowler notes that the option of rewriting is always available but questions whether it is always worth the trouble.

</doc>
<doc id="28901" url="https://en.wikipedia.org/wiki?curid=28901" title="Symmetric group">
Symmetric group

In abstract algebra, the symmetric group S"n" on a finite set of "n" symbols is the group whose elements are all the permutation operations that can be performed on "n" distinct symbols, and whose group operation is the composition of such permutation operations, which are defined as bijective functions from the set of symbols to itself. Since there are "n"! ("n" factorial) possible permutation operations that can be performed on a tuple composed of "n" symbols, it follows that the order (the number of elements) of the symmetric group S"n" is "n"!.
Although symmetric groups can be defined on infinite sets as well, this article discusses only the finite symmetric groups: their applications, their elements, their conjugacy classes, a finite presentation, their subgroups, their automorphism groups, and their representation theory. For the remainder of this article, "symmetric group" will mean a symmetric group on a finite set.
The symmetric group is important to diverse areas of mathematics such as Galois theory, invariant theory, the representation theory of Lie groups, and combinatorics. Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on "G".
Definition and first properties.
The symmetric group on a finite set "X" is the group whose elements are all bijective functions from "X" to "X" and whose group operation is that of function composition. For finite sets, "permutations" and "bijective functions" refer to the same operation, namely rearrangement. The symmetric group of degree "n" is the symmetric group on the set 
The symmetric group on a set "X" is denoted in various ways including S"X", 𝔖"X", Σ"X", "X"! and Sym("X"). If "X" is the set then the symmetric group on "X" is also denoted S"n", 𝔖"n", Σ"n", and Sym("n").
Symmetric groups on infinite sets behave quite differently from symmetric groups on finite sets, and are discussed in , , and . This article concentrates on the finite symmetric groups.
The symmetric group on a set of "n" elements has order "n"!. It is abelian if and only if . For and (the empty set and the singleton set) the symmetric group is trivial (note that this agrees with ), and in these cases the alternating group equals the symmetric group, rather than being an index two subgroup. The group S"n" is solvable if and only if . This is an essential part of the proof of the Abel–Ruffini theorem that shows that for every there are polynomials of degree "n" which are not solvable by radicals, i.e., the solutions cannot be expressed by performing a finite number of operations of addition, subtraction, multiplication, division and root extraction on the polynomial's coefficients.
Applications.
The symmetric group on a set of size "n" is the Galois group of the general polynomial of degree "n" and plays an important role in Galois theory. In invariant theory, the symmetric group acts on the variables of a multi-variate function, and the functions left invariant are the so-called symmetric functions. In the representation theory of Lie groups, the representation theory of the symmetric group plays a fundamental role through the ideas of Schur functors. In the theory of Coxeter groups, the symmetric group is the Coxeter group of type A"n" and occurs as the Weyl group of the general linear group. In combinatorics, the symmetric groups, their elements (permutations), and their representations provide a rich source of problems involving Young tableaux, plactic monoids, and the Bruhat order. Subgroups of symmetric groups are called permutation groups and are widely studied because of their importance in understanding group actions, homogeneous spaces, and automorphism groups of graphs, such as the Higman–Sims group and the Higman–Sims graph.
Elements.
The elements of the symmetric group on a set "X" are the permutations of "X".
Multiplication.
The group operation in a symmetric group is function composition, denoted by the symbol ∘ or simply by juxtaposition of the permutations. The composition of permutations "f" and "g", pronounced ""f" of "g"", maps any element "x" of "X" to "f"("g"("x")). Concretely, let (see permutation for an explanation of notation):
Applying "f" after "g" maps 1 first to 2 and then 2 to itself; 2 to 5 and then to 4; 3 to 4 and then to 5, and so on. So composing "f" and "g" gives
A cycle of length , taken to the "k"-th power, will decompose into "k" cycles of length "m": For example (, ),
Verification of group axioms.
To check that the symmetric group on a set "X" is indeed a group, it is necessary to verify the group axioms of closure, associativity, identity, and inverses. 1) The operation of function composition is closed in the set of permutations of the given set "X", 2) function composition is always associative, 3) The trivial bijection that assigns each element of "X" to itself serves as an identity for the group, and 4) Every bijection has an inverse function that undoes its action, and thus each element of a symmetric group does have an inverse which is a permutation too.
Transpositions.
A transposition is a permutation which exchanges two elements and keeps all others fixed; for example (1 3) is a transposition. Every permutation can be written as a product of transpositions; for instance, the permutation "g" from above can be written as "g" = (1 2)(2 5)(3 4). Since "g" can be written as a product of an odd number of transpositions, it is then called an odd permutation, whereas "f" is an even permutation.
The representation of a permutation as a product of transpositions is not unique; however, the number of transpositions needed to represent a given permutation is either always even or always odd. There are several short proofs of the invariance of this parity of a permutation.
The product of two even permutations is even, the product of two odd permutations is even, and all other products are odd. Thus we can define the sign of a permutation:
With this definition,
is a group homomorphism ({+1, –1} is a group under multiplication, where +1 is e, the neutral element). The kernel of this homomorphism, i.e. the set of all even permutations, is called the alternating group A"n". It is a normal subgroup of S"n", and for it has elements. The group S"n" is the semidirect product of A"n" and any subgroup generated by a single transposition.
Furthermore, every permutation can be written as a product of "adjacent transpositions", that is, transpositions of the form . For instance, the permutation "g" from above can also be written as . The sorting algorithm Bubble sort is an application of this fact. The representation of a permutation as a product of adjacent transpositions is also not unique.
Cycles.
A cycle of "length" "k" is a permutation "f" for which there exists an element "x" in {1...,"n"} such that "x", "f"("x"), "f"2("x"), ..., "f""k"("x") = "x" are the only elements moved by "f"; it is required that since with the element "x" itself would not be moved either. The permutation "h" defined by
is a cycle of length three, since , and , leaving 2 and 5 untouched. We denote such a cycle by , but it could equally well be written or by starting at a different point. The order of a cycle is equal to its length. Cycles of length two are transpositions. Two cycles are "disjoint" if they move disjoint subsets of elements. Disjoint cycles commute, e.g. in "S"6 we have . Every element of S"n" can be written as a product of disjoint cycles; this representation is unique up to the order of the factors, and the freedom present in representing each individual cycle by choosing its starting point.
Cycles admits the following conjugation property with any permutation formula_8, this property is often used to obtain its Generators and relations.
Special elements.
Certain elements of the symmetric group of {1, 2, ..., "n"} are of particular interest (these can be generalized to the symmetric group of any finite totally ordered set, but not to that of an unordered set).
The is the one given by:
This is the unique maximal element with respect to the Bruhat order and the
longest element in the symmetric group with respect to generating set consisting of the adjacent transpositions , .
This is an involution, and consists of formula_11 (non-adjacent) transpositions
so it thus has sign:
which is 4-periodic in "n".
In S2"n", the "perfect shuffle" is the permutation that splits the set into 2 piles and interleaves them. Its sign is also formula_15
Note that the reverse on "n" elements and perfect shuffle on 2"n" elements have the same sign; these are important to the classification of Clifford algebras, which are 8-periodic.
Conjugacy classes.
The conjugacy classes of S"n" correspond to the cycle structures of permutations; that is, two elements of S"n" are conjugate in S"n" if and only if they consist of the same number of disjoint cycles of the same lengths. For instance, in S5, (1 2 3)(4 5) and (1 4 3)(2 5) are conjugate; (1 2 3)(4 5) and (1 2)(4 5) are not. A conjugating element of S"n" can be constructed in "two line notation" by placing the "cycle notations" of the two conjugate permutations on top of one another. Continuing the previous example:
which can be written as the product of cycles, namely: (2 4).
This permutation then relates (1 2 3)(4 5) and (1 4 3)(2 5) via conjugation, i.e.
It is clear that such a permutation is not unique.
Low degree groups.
The low-degree symmetric groups have simpler and exceptional structure, and often must be treated separately.
Maps between symmetric groups.
Other than the trivial map and the sign map , the most notable homomorphisms between symmetric groups, in order of relative dimension, are:
There are also a host of other homomorphisms where .
Properties.
Symmetric groups are Coxeter groups and reflection groups. They can be realized as a group of reflections with respect to hyperplanes . Braid groups B"n" admit symmetric groups S"n" as quotient groups.
Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on the elements of "G", as a group acts on itself faithfully by (left or right) multiplication.
Relation with alternating group.
For , the alternating group A"n" is simple, and the induced quotient is the sign map: which is split by taking a transposition of two elements. Thus S"n" is the semidirect product , and has no other proper normal subgroups, as they would intersect An in either the identity (and thus themselves be the identity or a 2-element group, which is not normal), or in A"n" (and thus themselves be A"n" or S"n").
S"n" acts on its subgroup A"n" by conjugation, and for , S"n" is the full automorphism group of A"n": Aut(A"n") ≅ S"n". Conjugation by even elements are inner automorphisms of A"n" while the outer automorphism of A"n" of order 2 corresponds to conjugation by an odd element. For , there is an exceptional outer automorphism of A"n" so S"n" is not the full automorphism group of A"n".
Conversely, for , S"n" has no outer automorphisms, and for it has no center, so for it is a complete group, as discussed in automorphism group, below.
For , S"n" is an almost simple group, as it lies between the simple group A"n" and its group of automorphisms.
formula_18 can be embedded into formula_19 by appending the transposition formula_20 to all odd permutations, while embedding into formula_21 is impossible for formula_22.
Generators and relations.
The symmetric group on "n"-letters, S"n", may be described as follows. It has generators: formula_23 and relations:
One thinks of formula_27 as swapping the "i"th and position.
Other popular generating sets include the set of transpositions that swap 1 and "i" for and a set containing any "n"-cycle and a 2-cycle of adjacent elements in the "n"-cycle.
Subgroup structure.
A subgroup of a symmetric group is called a permutation group.
Normal subgroups.
The normal subgroups of the finite symmetric groups are well understood. If , S"n" has at most 2 elements, and so has no nontrivial proper subgroups. The alternating group of degree "n" is always a normal subgroup, a proper one for and nontrivial for ; for it is in fact the only non-identity proper normal subgroup of S"n", except when where there is one additional such normal subgroup, which is isomorphic to the Klein four group.
The symmetric group on an infinite set does not have an associated alternating group: not all elements can be written as a (finite) product of transpositions. However it does contain a normal subgroup "S" of permutations that fix all but finitely many elements, and such permutations can be classified as either even or odd. The even elements of "S" form the alternating subgroup "A" of "S", and since "A" is even a characteristic subgroup of "S", it is also a normal subgroup of the full symmetric group of the infinite set. The groups "A" and "S" are the only non-identity proper normal subgroups of the symmetric group on a countably infinite set. For more details see or .
Maximal subgroups.
The maximal subgroups of the finite symmetric groups fall into three classes: the intransitive, the imprimitive, and the primitive. The intransitive maximal subgroups are exactly those of the form for . The imprimitive maximal subgroups are exactly those of the form Sym("k") wr Sym("n"/"k") where is a proper divisor of "n" and "wr" denotes the wreath product acting imprimitively. The primitive maximal subgroups are more difficult to identify, but with the assistance of the O'Nan–Scott theorem and the classification of finite simple groups, gave a fairly satisfactory description of the maximal subgroups of this type according to .
Sylow subgroups.
The Sylow subgroups of the symmetric groups are important examples of "p"-groups. They are more easily described in special cases first:
The Sylow "p"-subgroups of the symmetric group of degree "p" are just the cyclic subgroups generated by "p"-cycles. There are such subgroups simply by counting generators. The normalizer therefore has order "p"·("p"−1) and is known as a Frobenius group (especially for ), and is the affine general linear group, .
The Sylow "p"-subgroups of the symmetric group of degree "p"2 are the wreath product of two cyclic groups of order "p". For instance, when "p" = 3, a Sylow 3-subgroup of Sym(9) is generated by "a" = (1 4 7)(2 5 8)(3 6 9) and the elements "x" = (1 2 3), "y" = (4 5 6), "z" = (7 8 9), and every element of the Sylow 3-subgroup has the form "a""i""x""j""y""k""z""l" for 0 ≤ "i","j","k","l" ≤ 2.
The Sylow "p"-subgroups of the symmetric group of degree "p""n" are sometimes denoted W"p"("n"), and using this notation one has that is the wreath product of W"p"("n") and W"p"(1).
In general, the Sylow "p"-subgroups of the symmetric group of degree "n" are a direct product of "a""i" copies of W"p"("i"), where 0 ≤ "ai" ≤ "p" − 1 and "n" = "a"0 + "p"·"a"1 + ... + "p"k·"a""k".
For instance, W2(1) = C2 and W2(2) = D8, the dihedral group of order 8, and so a Sylow 2-subgroup of the symmetric group of degree 7 is generated by { (1,3)(2,4), (1,2), (3,4), (5,6) } and is isomorphic to D8 × C2.
These calculations are attributed to and described in more detail in . Note however that attributes the result to an 1844 work of Cauchy, and mentions that it is even covered in textbook form in .
Transitive subgroups.
A transitive subgroup of S"n" is a subgroup whose action on {1, 2, ..., "n"} is transitive. For example, the Galois group of a (finite) Galois extension is a transitive subgroup of S"n", for some "n".
Automorphism group.
For , S"n" is a complete group: its center and outer automorphism group are both trivial.
For , the automorphism group is trivial, but S2 is not trivial: it is isomorphic to C2, which is abelian, and hence the center is the whole group.
For , it has an outer automorphism of order 2: , and the automorphism group is a semidirect product
In fact, for any set "X" of cardinality other than 6, every automorphism of the symmetric group on "X" is inner, a result first due to according to .
Homology.
The group homology of S"n" is quite regular and stabilizes: the first homology (concretely, the abelianization) is:
The first homology group is the abelianization, and corresponds to the sign map S"n" → S2 which is the abelianization for "n" ≥ 2; for "n" < 2 the symmetric group is trivial. This homology is easily computed as follows: S"n" is generated by involutions (2-cycles, which have order 2), so the only non-trivial maps are to S2 and all involutions are conjugate, hence map to the same element in the abelianization (since conjugation is trivial in abelian groups). Thus the only possible maps send an involution to 1 (the trivial map) or to −1 (the sign map). One must also show that the sign map is well-defined, but assuming that, this gives the first homology of S"n".
The second homology (concretely, the Schur multiplier) is:
This was computed in , and corresponds to the double cover of the symmetric group, 2 · S"n".
Note that the exceptional low-dimensional homology of the alternating group (formula_31 corresponding to non-trivial abelianization, and formula_32 due to the exceptional 3-fold cover) does not change the homology of the symmetric group; the alternating group phenomena do yield symmetric group phenomena – the map formula_33 extends to formula_34 and the triple covers of A6 and A7 extend to triple covers of S6 and S7 – but these are not "homological" – the map formula_35 does not change the abelianization of S4, and the triple covers do not correspond to homology either.
The homology "stabilizes" in the sense of stable homotopy theory: there is an inclusion map , and for fixed "k", the induced map on homology is an isomorphism for sufficiently high "n". This is analogous to the homology of families Lie groups stabilizing.
The homology of the infinite symmetric group is computed in , with the cohomology algebra forming a Hopf algebra.
Representation theory.
The representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.
The symmetric group S"n" has order "n"!. Its conjugacy classes are labeled by partitions of "n". Therefore according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of "n". Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representation by the same set that parametrizes conjugacy classes, namely by partitions of "n" or equivalently Young diagrams of size "n".
Each such irreducible representation can be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram.
Over other fields the situation can become much more complicated. If the field "K" has characteristic equal to zero or greater than "n" then by Maschke's theorem the group algebra "K"S"n" is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).
However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called "Specht modules", and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general. 
The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.

</doc>
<doc id="28902" url="https://en.wikipedia.org/wiki?curid=28902" title="SMS (disambiguation)">
SMS (disambiguation)

SMS is Short Message Service, a form of text messaging communication on phones and mobile phones.
SMS may also refer to:

</doc>
<doc id="28904" url="https://en.wikipedia.org/wiki?curid=28904" title="Short Message Peer-to-Peer">
Short Message Peer-to-Peer

The Short Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESME), Routing Entities (RE) and Message Centres.
SMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, Voice Mail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, the SMPP is the most commonly used protocol for short message exchange outside SS7 networks.
History.
SMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (now split off and known as Acision). The protocol was originally created by a developer, Ian J Chambers, to test functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Acision due to the disbanding of the SMS forum.
To date SMPP development is suspended and SMS forum is disbanded. From SMS forum website:
July 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007
A press release, attached to the news, also warns that site will be suspended soon. In spite of this the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).
The site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Acision for the SMPP specification. The files may also be available from other websites.
Operation.
Contrary to its name, the SMPP uses the client-server model of operation. The Message Center usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.
The protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.
Before exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.
Message exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a "window"; for the best performance both communicating sides must be configured with the same window size.
Versions.
The SMPP standard has evolved during the time. The most commonly used versions of SMPP are:
The applicable version is passed in the interface_version parameter of a bind command.
PDU format.
The SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:
PDU header.
Each PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:
All numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).
Example.
This is an example of the binary encoding of a 60-octet "submit_sm" PDU. The data is shown in Hex octet values as a single dump and followed by a header and body break-down of that PDU.
This is best compared with the definition of the submit_sm PDU from the SMPP specification in order to understand how the encoding matches the field by field definition.
The value break-downs are shown with decimal in parentheses and Hex values after that. Where you see one or several hex octets appended, this is because the given field size uses 1 or more octets encoding.
Again, reading the definition of the submit_sm PDU from the spec will make all this clearer.
PDU body.
Note that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:
The meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers—it may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.
Implementations.
SMPP has been implemented for Java in the jSMPP project. This is used in Apache Camel and various other popular free software projects for SMS messaging. Alternative Java implementation is nmote-smpp. The python-smpp project provides SMPP for Python users. The php-smpp project provides SMPP for PHP users.
Quirks.
Despite its wide acceptance, the SMPP has a number of problematic features:
No data_coding for GSM 7 bit default alphabet.
Although data_coding values in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7 bit default alphabet.
Not standardized meaning of data_coding=0.
According to SMPP 3.4 and 5.0 the data_coding=0 means ″SMSC Default Alphabet″. Which encoding it really is, depends on the type of the SMSC and its configuration.
Unclear support for Shift-JIS encoding.
One of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.
Incompatibility of submit_sm_resp between SMPP versions.
When a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ″empty″ message_id.
For the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.
Comment from Cormac Long 
The original intention of error scenarios was that no body would be returned in the PDU response. This was the standard behavior exhibited on all Aldiscon/Logica SMSC and also in most of the other vendors. When SMPP 3.4 was being taken on by the WAP forum, several clarifications were requested on whether a body should be included with NACKed response and measures were taken to clarify this in several places in the specification including the submit_sm section and also in the bind_transceiver section. What should have been done was to add the clarification that we eventually added in V5.0.. that bodies are not supposed to be included in error responses. Some vendors have been very silly in their implementations including bodies on rejected bind_transmitter responses but not on bind_transceiver responses etc. The recommendation I would make to vendors.. as suggested above.. accept both variants. But its also wise to allow yourself issue NACKed submit_sm_resp and deliver_sm_resp PDUs with and without an empty body. In the case of these two PDUs, that empty body will look like a single NULL octet at the end of the stream. The reason you may need this ability to include what I call dummy bodies with NACKed requests is that the other side of the equation may be unable or unwilling to change their implementation to tolerate the missing body.
Message Id in SMPP 3.3 SMSC Delivery Receipts.
The only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:
Extensibility, compatibility and interoperability.
Since introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:
Information applicable to one version of SMPP can often be found in another version of SMPP; e.g. the only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose.

</doc>
<doc id="28906" url="https://en.wikipedia.org/wiki?curid=28906" title="Strike from the record">
Strike from the record

To strike from the record is for a judge to forbid a decision maker (such as a juror) to consider a particular piece of testimony or other evidence when deciding the case even though he or she has already learned what that evidence or testimony concerned. The commonly heard request is "move to strike", with the intent to erase previous testimony or court proceeding from record.

</doc>
<doc id="28908" url="https://en.wikipedia.org/wiki?curid=28908" title="Suburb">
Suburb

A suburb is a residential area or a mixed use area, either existing as part of a city or urban area or as a separate residential community within commuting distance of a city. In most English-speaking regions, suburban areas are defined in contrast to central or inner city areas, but in Australian English, "suburb" has become largely synonymous with what is called a "neighborhood" in other countries and the term extends to inner city areas. In some areas, such as Australia, China, New Zealand, the United Kingdom, and a few U.S. states, new suburbs are routinely annexed by adjacent cities. In others, such as Arabia, Canada, France, and much of the United States, many suburbs remain separate municipalities or are governed as part of a larger local government area such as a county.
Suburbs first emerged on a large scale in the 19th and 20th centuries as a result of improved rail and road transport, which led to an increase in commuting. In general, they have lower population densities than inner city neighborhoods within a metropolitan area, and most residents commute to central cities or other business districts; however, there are many exceptions, including industrial suburbs, planned communities, and satellite cities. Suburbs tend to proliferate around cities that have an abundance of adjacent flat land.
Etymology and usage.
The English word is derived from the Old French "subburbe", which is in turn derived from the Latin "suburbium", formed from "sub" (meaning "under" or "below") and "urbs" ("city"). The first recorded usage of the term in English, according to the "Oxford English Dictionary", was made by John Wycliffe in 1380, where the form "subarbis" was used.
North America.
In the United States and Canada, "suburb" can refer either to an outlying residential area of a city or town or to a separate municipality or unincorporated area outside a town or city.
British Isles.
In the United Kingdom and in Ireland, "suburb" merely refers to a residential area outside the city centre, regardless of administrative boundaries. Suburbs, in this sense, can range from areas that seem more like residential areas of a city proper to areas separated by open countryside from the city centre. In large cities such as London, suburbs include formerly separate towns and villages that have been gradually absorbed during a city's growth and expansion, such as Ealing or Bromley.
Australia and New Zealand.
In Australia and New Zealand, suburbs have become formalised as geographic subdivisions of a city and are used by postal services in addressing. In rural areas in both countries, their equivalents are called localities (see suburbs and localities). The terms "inner suburb" and "outer suburb" are used to differentiate between the higher-density suburbs in proximity to the city center, and the lower-density suburbs on the outskirts of the urban area. The term 'middle suburbs' is also used. Inner suburbs, such as Te Aro in Wellington, Mt Eden in Auckland, Prahran in Melbourne and Ultimo in Sydney, are usually characterised by higher density apartment housing and greater integration between commercial and residential areas.
History.
Early history.
The earliest appearance of suburbs coincided with the spread of the first urban settlements. Large walled towns tended to be the focus around which smaller villages grew up in a symbiotic relationship with the market town. The word 'suburbani' was first used by the Roman statesman Cicero in reference to the large villas and estates built by the wealthy patricians of Rome on the city's outskirts.
As populations grew during the Early Modern Period in Europe, urban towns swelled with a steady influx of people from the countryside. In some places, nearby settlements were swallowed up as the main city expanded. The peripheral areas on the outskirts of the city were generally inhabited by the very poorest.
Origins of the modern suburb.
Due to the rapid migration of the rural poor to the industrialising cities of England in the late 18th century, a trend in the opposite direction began to develop; - newly rich members of the middle classes began to purchase estates and villas on the outskirts of London. This trend accelerated through the 19th century, especially in cities like London and Manchester that were experiencing tremendous growth, and the first suburban districts sprung up around the city centre to accommodate those who wanted to escape the squalid conditions of the industrial town. Toward the end of the century, with the development of public transit systems such as the underground railways, trams and buses, it became possible for the majority of the city's population to reside outside the city and to commute into the center for work.
By the mid-19th century, the first major suburban areas were springing up around London as the city (then the largest in the world) became more overcrowded and unsanitary. A major catalyst in suburban growth came from the opening of the Metropolitan Railway in the 1860s. The line joined the capital's financial heart in the City to what were to become the suburbs of Middlesex. Harrow was reached in 1880, and the line eventually extended as far as in Buckinghamshire, more than from Baker Street and the centre of London.
Unlike other railway companies, which were required to dispose of surplus land, the Met was allowed to retain such land that it believed was necessary for future railway use. Initially, the surplus land was managed by the Land Committee, and, from the 1880s, the land was developed and sold to domestic buyers in places like Willesden Park Estate, Cecil Park, near Pinner and at Wembley Park.
In 1912, it was suggested that a specially formed company should take over from the Surplus Lands Committee and develop suburban estates near the railway. However, World War I delayed these plans and it was only in 1919, with expectation of a postwar housing boom, that the Metropolitan Railway Country Estates Limited (MRCE) was formed. The MRCE went on to develop estates at Kingsbury Garden Village near Neasden, Wembley Park, Cecil Park and Grange Estate at Pinner and the Cedars Estate at Rickmansworth and create places such as Harrow Garden Village.
The term "Metro-land" was coined by the Met's marketing department in 1915 when the "Guide to the Extension Line" became the "Metro-land" guide, priced at 1d. This promoted the land served by the Met for the walker, visitor and later the house-hunter. Published annually until 1932, the last full year of independence for the Met, the guide extolled the benefits of "The good air of the Chilterns", using language such as "Each lover of Metroland may well have his own favourite wood beech and coppice — all tremulous green loveliness in Spring and russet and gold in October". The dream promoted was of a modern home in beautiful countryside with a fast railway service to central London. By 1915, people from across London had flocked to live the new suburban dream in large newly built areas across North West London.
Interwar suburban expansion in England.
Suburbanisation in the interwar period was heavily influenced by the garden city movement of Ebenezer Howard and the creation of the first garden suburbs at the turn of the 20th century. The first garden suburb was developed through the efforts of social reformer Henrietta Barnett and her husband; inspired by Ebenezer Howard and the model housing development movement (then exemplified by Letchworth garden city), as well as the desire to protect part of Hampstead Heath from development, they established trusts in 1904 which bought 243 acres of land along the newly opened Northern line extension to Golders Green and created the Hampstead Garden Suburb. The suburb attracted the talents of architects including Raymond Unwin and Sir Edwin Lutyens, and it ultimately grew to encompass over 800 acres.
During the First World War the Tudor Walters Committee was commissioned to make recommendations for the post war reconstruction and housebuilding. In part, this was a response to the shocking lack of fitness amongst many recruits during World War One, attributed to poor living conditions; a belief summed up in a housing poster of the period "you cannot expect to get an A1 population out of C3 homes" - referring to military fitness classifications of the period.
The Committee's report of 1917 was taken up by the government, which passed the Housing, Town Planning, &c. Act 1919, also known as the Addison Act after Dr. Christopher Addison, the then Minister for Housing. The Act allowed for the building of large new housing estates in the suburbs after the First World War, and marked the start of a long 20th century tradition of state-owned housing, which would later evolve into council estates.
The Report also legislated on the required, minimum standards necessary for further suburban construction; this included regulation on the maximum housing density and their arrangement and it even made recommendations on the ideal number of bedrooms and other rooms per house. Although the semi-detached house was first designed by the Shaws (a father and son architectural partnership) in the 19th century, it was during the suburban housing boom of the interwar period that that the design first proliferated as a suburban icon, being preferred by middle class home owners to the smaller terraced houses. The design of many of these houses, highly characteristic of the era, was heavily influenced by the Art Deco movement, taking influence from Tudor Revival, chalet style, and even ship design.
Within just a decade suburbs dramatically increased in size. Harrow Weald went from just 1,500 to over 10,000 while Pinner jumped from 3,000 to over 20,000. During the 1930s, over 4 million new suburban houses were built, the 'suburban revolution' had made England the most heavily suburbanized country in the world, by a considerable margin.
North America.
Boston and New York spawned the first suburbs. The streetcar lines in Boston and the rail lines into Manhattan made daily commutes possible. No metropolitan area in the world was as well served by railroad commuter lines at the turn of the twentieth century as New York, and it was the rail lines to Westchester from the Grand Central Terminal commuter hub that enabled its development. Westchester's true importance in the history of American suburbanization derives from the upper-middle class development of villages including Scarsdale, New Rochelle and Rye serving thousands of businessmen and executives from Manhattan.
Post-war suburban expansion.
The suburban population in North America exploded during the post-World War II economic expansion. Returning veterans wishing to start a settled life moved in masses to the suburbs. Levittown developed as a major prototype of mass-produced housing. At the same time, African Americans were rapidly moving north for better jobs and educational opportunities than were available to them in the segregated South. Their arrival in Northern cities en masse, in addition to being followed by race riots in several large cities such as Detroit, Chicago, Washington, D.C., and Philadelphia, further stimulated white suburban migration. The growth of the suburbs was facilitated by the development of zoning laws, redlining and numerous innovations in transport. After World War II availability of FHA loans stimulated a housing boom in American suburbs. In the older cities of the northeast U.S., streetcar suburbs originally developed along train or trolley lines that could shuttle workers into and out of city centers where the jobs were located. This practice gave rise to the term "bedroom community", meaning that most daytime business activity took place in the city, with the working population leaving the city at night for the purpose of going home to sleep.
Economic growth in the United States encouraged the suburbanization of American cities that required massive investments for the new infrastructure and homes. Consumer patterns were also shifting at this time, as purchasing power was becoming stronger and more accessible to a wider range of families. Suburban houses also brought about needs for products that were not needed in urban neighborhoods, such as lawnmowers and automobiles. During this time commercial shopping malls were being developed near suburbs to satisfy consumers' needs and their car–dependent lifestyle.
Zoning laws also contributed to the location of residential areas outside of the city center by creating wide areas or "zones" where only residential buildings were permitted. These suburban residences are built on larger lots of land than in the central city. For example, the lot size for a residence in Chicago is usually deep, while the width can vary from wide for a row house to wide for a large stand–alone house. In the suburbs, where stand–alone houses are the rule, lots may be wide by deep, as in the Chicago suburb of Naperville. Manufacturing and commercial buildings were segregated in other areas of the city.
Alongside suburbanization, many companies began locating their offices and other facilities in the outer areas of the cities, which resulted in the increased density of older suburbs and the growth of lower density suburbs even further from city centers. An alternative strategy is the deliberate design of "new towns" and the protection of green belts around cities. Some social reformers attempted to combine the best of both concepts in the garden city movement.
In the U.S., 1950 was the first year that more people lived in suburbs than elsewhere. In the U.S, the development of the skyscraper and the sharp inflation of downtown real estate prices also led to downtowns being more fully dedicated to businesses, thus pushing residents outside the city center.
Suburbs worldwide.
United States.
In the 20th century, many suburban areas began to see independence from the central city as an asset. In some cases, suburbanites saw self-government as a means to keep out people who could not afford the added suburban property maintenance costs not needed in city living. Federal subsidies for suburban development accelerated this process as did the practice of redlining by banks and other lending institutions. In some cities such as Miami and San Francisco, the main city is much smaller than the surrounding suburban areas, leaving the city proper with a small portion of the metro area's population and land area. Cleveland, Ohio is typical of many American central cities; its municipal borders have changed little since 1922, even though the Cleveland urbanized area has grown many times over. Several layers of suburban municipalities now surround cities like Boston, Cleveland, Chicago, Detroit, Los Angeles, Dallas, Denver, Fort Worth, Houston, New York City, San Francisco, Sacramento, Atlanta, Miami, Pittsburgh, Philadelphia, Minneapolis, and Washington, D.C..
Suburbs in America have a prevalence of usually detached single-family homes.
They are characterized by:
By 2010 suburbs increasingly gained people in racial minority groups, as many members of minority groups became better educated, more affluent, and sought more favorable living conditions compared to inner city areas; many white Americans also moved back to city centers. Many major city downtowns (such as Downtown Miami, Downtown Detroit, Center City Philadelphia or Downtown Los Angeles) are experiencing a renewal, with large population growth, residential apartment construction, and increased social, cultural, and infrastructural investments. Better public transit, proximity to work and cultural attractions, and frustration with suburban life and gridlock have attracted young Americans to the city centers.
Canadian suburbs.
Compared to the American counterpart, Canadian suburbs are more dense (mostly in major cities), with the Toronto suburb of Mississauga itself being Canada's 6th largest city. Land use patterns in Canadian suburbs are often more mixed-use. There are often high- or mid-rise developments interspersed with low-rise housing tracts and in many suburban areas, there are numerous slab-style residential highrises that were constructed in the 1970s and onward. In Canada, densities are generally slightly higher than in Australia, but below typical European values. Often, Canadian suburbs are less automobile-centred and public transit use is encouraged but can be notably unused. Throughout Canada, especially in Toronto and Vancouver, there are comprehensive plans in place to curb sprawl, such as Ontario's Places to Grow act. This act is intended to manage growth in Toronto's suburbs, including Pickering and Ajax, Markham, Richmond Hill, Thornhill, Vaughan, Bolton/Caledon, Brampton, Mississauga, and Oakville, among others.
Canada is an urbanized nation where over 80% of the population live in urban areas (loosely defined), and roughly two-thirds live in one of Canada's 33 census metropolitan areas (CMAs) with a population of over 100,000. However, of this metropolitan population, in 2001 nearly half lived in low-density neighborhoods, with only one in five living in a typical "urban" neighborhood. The percentage living in low-density neighborhoods varied from a high of nearly two-thirds of Calgary CMA residents (67%), to a low of about one-third of Montreal CMA residents (34%).
Population and income growth in Canadian suburbs had tended to outpace growth in core urban or rural areas, but in many areas this trend has now reversed. The suburban population increased 87% between 1981 and 2001, well ahead of urban growth. The majority of recent population growth in Canada's three largest metropolitan areas (Greater Toronto, Greater Montreal, and Greater Vancouver) has occurred in non-core municipalities, although this trend has already reversed itself in Toronto, where a building boom has begun to take place. This trend is also beginning to take effect in Vancouver, and to a lesser extent, Montreal. In certain cities, particularly Edmonton and Calgary, suburban growth takes place within the city boundaries as opposed to in bedroom communities. This is due to annexation and large geographic footprint within the city borders.
Other countries.
In many parts of the developed world, suburbs can be economically distressed areas, inhabited by higher proportions of recent immigrants, with higher delinquency rates and social problems. Sometimes the notion of suburb may even refer to people in real misery, who are kept at the limit of the city borders for economic, social, and sometimes ethnic reasons. An example in the developed world would be the "banlieues" of France, or the concrete suburbs of Sweden, even if the suburbs of these countries also include middle-class and upper-class neighborhoods that often consist of single-family houses. Thus some of the suburbs of most of the developed world are comparable to several inner cities of the U.S. and Canada.
The growth in the use of trains, and later automobiles and highways, increased the ease with which workers could have a job in the city while commuting in from the suburbs. In the United Kingdom, as mentioned above, railways stimulated the first mass exodus to the suburbs. The Metropolitan Railway, for example, was active in building and promoting its own housing estates in the north-west of London, consisting mostly of detached houses on large plots, which it then marketed as "Metro-land". The Australian and New Zealand usage came about as outer areas were quickly surrounded in fast-growing cities, but retained the appellation "suburb"; the term was eventually applied to the original core as well. In Australia, Sydney's urban sprawl has occurred predominantly in the Western Suburbs. The locality of Olympic Park was designated an official suburb in 2009.
In the UK, the government is seeking to impose minimum densities on newly approved housing schemes in parts of South East England. The goal is to "build sustainable communities" rather than housing estates. However, commercial concerns tend to delay the opening of services until a large number of residents have occupied the new neighbourhood.
In Mexico, suburbs are generally similar to their United States counterparts. Houses are made in many different architectural styles which may be of European, American and International architecture and which vary in size. Suburbs can be found in Guadalajara, Mexico City, Monterrey, and most major cities. Lomas de Chapultepec is an example of an affluent suburb, although it is located inside the city and by no means is today a suburb in the strict sense of the word. In the rest of Latin America, the situation is similar to that of Mexico, with many suburbs being built, most notably in Argentina, Brazil, and Chile, which have experienced a boom in the construction of suburbs since the late 1970s and early 80s. As the growth of middle-class and upper-class suburbs increased, low-class squatter areas have increased, most notably "lost cities" in Mexico, campamentos in Chile, barriadas in Peru, villa miserias in Argentina, asentamientos in Guatemala and favelas of Brazil.
In Africa, since the beginning of the 1990s, the development of middle-class suburbs boomed. Due to the industrialization of many African countries, particularly in cities such as Cairo, Johannesburg and Lagos, the middle class has grown. In an illustrative case of South Africa, RDP housing has been built. In much of Soweto, many houses are American in appearance, but are smaller, and often consist of a kitchen and living room, two or three bedrooms, and a bathroom. However, there are more affluent neighborhoods, more comparable to American suburbs, particularly east of the FNB Stadium. In Cape Town there is a distinct European style which is due to the European influence during the mid-1600s when the Dutch conquered the area. Houses like these are called Cape Dutch Houses and can be found in the affluent suburbs of Constantia and Bishopscourt.
In the illustrative case of Rome, Italy, in the 1920s and 1930s, suburbs were intentionally created "ex novo" in order to give lower classes a destination, in consideration of the actual and foreseen massive arrival of poor people from other areas of the country. Many critics have seen in this development pattern (which was circularly distributed in every direction) also a quick solution to a problem of public order (keeping the unwelcome poorest classes together with the criminals, in this way better controlled, comfortably remote from the elegant "official" town). On the other hand, the expected huge expansion of the town soon effectively covered the distance from the central town, and now those suburbs are completely engulfed by the main territory of the town. Other newer suburbs (called exurbs) were created at a further distance from them.
In Russia, the term suburb refers to high-rise residential apartments which usually consist of two bedrooms, one bathroom, a kitchen and a living room. These suburbs, however are usually not in poor neighborhoods, unlike the banlieuees.
In China, the term suburb is new, although suburbs are already being constructed rapidly. Chinese suburbs mostly consist of rows upon rows of apartment blocks and condos that end abruptly into the countryside. Also new town developments are extremely common. Single family suburban homes tend to be similar to their Western equivalents; although primarily outside Beijing and Shanghai, also mimic Spanish and Italian architecture. In Hong Kong, however, suburbs are mostly government-planned new towns containing numerous public housing estates. New Towns such as Tin Shui Wai may gain notoriety as a slum. However, other new towns also contain private housing estates and low density developments for the upper classes.
In Japan, the construction of suburbs has boomed since the end of World War II and many cities are experiencing the urban sprawl effect.
In Malaysia, suburbs are common, especially in areas surrounding the Klang Valley, which is the largest conurbation in the country. These suburbs also serve as major housing areas and commuter towns. Terraced houses, semi-detached houses and shophouses are common concepts in suburbs. In certain areas such as Klang, Subang Jaya and Petaling Jaya, suburbs form the core of these places. The latter one has been turned into a satellite city of Kuala Lumpur. Suburbs are also evident in other smaller conurbations including Ipoh, Johor Bahru, Kota Kinabalu, Kuching, Alor Setar and Penang.
Traffic flows.
Suburbs typically have longer travel times to work than traditional neighborhoods. Only the traffic "within" the short streets themselves is less. This is due to three factors: almost-mandatory automobile ownership due to poor suburban bus systems, longer travel distances and the hierarchy system, which is less efficient at distributing traffic than the traditional grid of streets.
In the suburban system, most trips from one component to another component requires that cars enter a collector road, no matter how short or long the distance is. This is compounded by the hierarchy of streets, where entire neighborhoods and subdivisions are dependent on one or two collector roads. Because all traffic is forced onto these roads, they are often heavy with traffic all day. If a traffic crash occurs on a collector road, or if road construction inhibits the flow, then the entire road system may be rendered useless until the blockage is cleared. The traditional "grown" grid, in turn, allows for a larger number of choices and alternate routes.
Suburban systems of the sprawl type are also quite inefficient for cyclists or pedestrians, as the direct route is usually not available for them either. This encourages car trips even for distances as low as several hundreds of yards or meters (which may have become up to several miles or kilometers due to the road network). Improved sprawl systems, though retaining the car detours, possess cycle paths and footpaths connecting across the arms of the sprawl system, allowing a more direct route while still keeping the cars out of the residential and side streets.
According to "Governing, Cities and Localities" section More commonly, central cities seek ways to tax nonresidents working downtown – known as commuter taxes – as property tax bases dwindle. Taken together, these two groups of taxpayers represent a largely untapped source of potential revenue that cities may begin to target more aggressively, particularly if they're struggling. According to struggling cities, this will help bring in a substantial revenue for the city which is a great way to tax the people who make the most use of the highways and repairs.
Today more companies settle down in suburbs because of low property costs.
Academic study of suburbs.
The history of suburbia is part of the study of urban history, which focuses on the origins, growth, diverse typologies, culture, and politics of suburbs, as well as on the gendered and family-oriented nature of suburban space. Many people have assumed that early-20th-century suburbs were enclaves for middle-class whites, a concept that carries tremendous cultural influence yet is actually stereotypical. Many suburbs are based on a heterogeneous society of working-class and minority residents, many of whom want to own their own house. Mary Corbin Sies argues that it is necessary to examine how "suburb" is defined as well as the distinction made between cities and suburbs, geography, economic circumstances, and the interaction of numerous factors that move research beyond acceptance of stereotyping and its influence on scholarly assumptions.
In popular culture.
Suburbs and suburban living have been the subject for a wide variety of films, books, television shows and songs.
French songs like "La Zone" by Fréhel (1933), "Aux quatre coins de la banlieue" by Damia (1936), "Ma banlieue" by Reda Caire (1937), or "Banlieue" by Robert Lamoureux (1953), evoke the suburbs of Paris explicitly since the 1930s. Those singers give a sunny festive, almost bucolic, image of the suburbs, yet still few urbanized. During the fifties and the sixties, French singer-songwriter Léo Ferré evokes in his songs popular and proletarian suburbs of Paris, to oppose them to the city, considered by comparison as a bourgeois and conservative place.
French cinema was although soon interested in urban changes in the suburbs, with such movies as "Mon oncle" by Jacques Tati (1958), "L'Amour existe" by Maurice Pialat (1961) or "Two or Three Things I Know About Her" by Jean-Luc Godard (1967).
In his one-act opera "Trouble in Tahiti" (1952), Leonard Bernstein skewers American suburbia, which produces misery instead of happiness.
The American photojournalist Bill Owens documented the culture of suburbia in the 1970s, most notably in his book "Suburbia". The 1962 song "Little Boxes" by Malvina Reynolds lampoons the development of suburbia and its perceived bourgeois and conformist values, while the 1982 song "Subdivisions" by the Canadian band Rush also discusses suburbia, as does Rocking the Suburbs by Ben Folds. The 2010 album "The Suburbs" by the Canadian-based alternative band Arcade Fire dealt with aspects of growing up in suburbia, suggesting aimlessness, apathy and endless rushing are ingrained into the suburban culture and mentality. "Suburb The Musical," was written by Robert S. Cohen and David Javerbaum. Over the Hedge is a syndicated comic strip written and drawn by Michael Fry and T. Lewis. It tells the story of a raccoon, turtle, a squirrel, and their friends who come to terms with their woodlands being taken over by suburbia, trying to survive the increasing flow of humanity and technology while becoming enticed by it at the same time. A film adaptation of Over the Hedge was produced in 2006.
British television series such as "The Good Life", "Butterflies" and "The Fall and Rise of Reginald Perrin" have depicted suburbia as well-manicured but relentlessly boring, and its residents as either overly conforming or prone to going stir crazy. Contrastingly, U.S. shows – such as "Knots Landing", "Desperate Housewives" and "Weeds" – portray the suburbs as concealing darker secrets behind a façade of perfectly manicured lawns, friendly people, and beautifully up-kept houses. Films such as "The 'Burbs", "Disturbia" and "Hot Fuzz", have brought this theme to the cinema. This trope was also used in the episode of "The X-Files" "Arcadia" and on one level of the video game "Psychonauts".

</doc>
<doc id="28910" url="https://en.wikipedia.org/wiki?curid=28910" title="Shōnen manga">
Shōnen manga

Summary.
Shōnen manga is typically characterized by high-action, often humorous plots featuring male protagonists. The camaraderie between boys or men on sports teams, fighting squads and the like is often emphasized. Attractive female characters with exaggerated features are also common, such as Sakura Haruno from "Naruto" or Nami from "One Piece" (see fan service). Main characters may also feature an ongoing desire to better themselves.
Such manga often portray challenges to the protagonist's abilities, skills, and maturity, stressing self-perfection, austere self-discipline, sacrifice in the cause of duty, and honorable service to society, community, family, and friends.
None of these listed characteristics are a requirement, as seen in shōnen manga like "Yotsuba&!", which features a female lead and almost no fan service or action; what most defines whether or not a series is shōnen are things like the magazine it is serialized in or the time slot it airs on television. After the case of Tsutomu Miyazaki, depictions of violence and sexual matters became more highly regulated in manga in general, but especially in shōnen manga. The art style of shōnen is generally less "flowery" than that of shōjo manga, although this varies greatly from artist to artist, and some artists draw both shōnen and shōjo manga.
Different shōnen manga stories may feature different themes, such as martial arts, robots, science fiction, sports, terror, and mythological creatures.
Shōnen manga today.
Akira Toriyama's "Dragon Ball" (1984–1995) is credited with setting the trend of popular shōnen manga from the 1980s onward, with manga critic Jason Thompson in 2011 calling it "by far the most influential shōnen manga of the last 30 years." Many currently successful shōnen authors such as Eiichiro Oda, Masashi Kishimoto, Tite Kubo, Atsushi Suzumi, Kentaro Yabuki, Shinya Suzuki and Hiro Mashima cite him and "Dragon Ball" as an influence on their own now popular works.
History.
Before World War II.
Manga has been said to have existed since the eighteenth century, but originally did not target a specific gender or age group. By 1905, however, a boom in publishing manga magazines occurred, and began targeting genders as evidenced by their names, such as "Shōnen Sekai", "Shōjo Sekai", and "Shōnen Pakku" (a kodomo manga magazine). "Shōnen Sekai" was one of the first shōnen manga magazines, and was published from 1895 to 1914.
Post-Occupation.
The post-World War II occupation of Japan had a profound impact on its culture during the 1950s and beyond (see culture of Post-occupation Japan), including on manga. Modern manga developed during this period, including the modern format of shōnen manga we experience today, of which boys and young men were among the earliest readers. During this time, Shōnen manga focused on topics thought to interest the archetypical boy: sci-tech subjects like robots and space travel, and heroic action-adventure. Osamu Tezuka, creator of "Astro Boy" is said to have played an influential role in manga during this period. Between 1950 and 1969, an increasingly large readership for manga emerged in Japan with the solidification of its two main marketing genres, shōnen manga aimed at boys and shōjo manga aimed at girls.
The magazine "Weekly Shōnen Jump" began production in 1968, and continues to be produced today as the best-selling manga magazine in Japan. Many of the most popular shōnen manga titles have been serialized in "Jump", including "Dragon Ball", "Captain Tsubasa", "Slam Dunk", "One Piece", "Naruto", "Bleach", and others.
With the relaxation of censorship in Japan in the 1990s, a wide variety of explicit sexual themes appeared in manga intended for male readers, and correspondingly occur in English translations. However, in 2010 the Tokyo Metropolitan Government passed the controversial Bill 156 to restrict harmful content despite opposition by many authors and publishers in the manga industry.
Women's roles in shōnen manga.
In early shōnen manga, men and boys played all the major roles, with women and girls having only auxiliary places as sisters, mothers, and occasionally girlfriends. Of the nine cyborgs in Shotaro Ishinomori's 1964 "Cyborg 009", only one is female, and she soon vanishes from the action. Some recent shōnen manga virtually omit women, e.g. the martial arts story "Baki the Grappler" by Itagaki Keisuke, and the supernatural fantasy "Sand Land" by Akira Toriyama. By the 1980s, however, girls and women began to play increasingly important roles in shōnen manga. For example, in Toriyama's 1980 "Dr. Slump", the main character is the mischievous and powerful girl robot Arale Norimaki.
The role of girls and women in manga for male readers has evolved considerably since Arale. One class is the "bishōjo" or "beautiful young girl." Sometimes the woman is unattainable, and she is always an object of the hero's emotional and/or sexual interest, like Belldandy from "Oh My Goddess!" by Kōsuke Fujishima and Shao-lin from "Guardian Angel Getten" by Minene Sakurano. In other stories, the hero is surrounded by such girls and women, as in "Negima! Magister Negi Magi" by Ken Akamatsu and "Hanaukyo Maid Team" by Morishige. The male protagonist does not always succeed in forming a relationship with the woman, for example when Bright Honda and Aimi Komori fail to bond in "Shadow Lady" by Masakazu Katsura. In other cases, a successful couple's sexual activities are depicted or implied, like in "Outlanders" by Johji Manabe. In still other cases, the initially naive and immature hero grows up to become a man by learning how to deal and live with women emotionally and sexually; examples of heroes who follow this path include Yota in "Video Girl Ai" by Masakazu Katsura and Train Man in "" by Hidenori Hara.
However, since the 90s, women have acquired a heightened role in various manga, albeit lesser in number. They are often portrayed as central characters or characters with important roles in manga. Some examples include "Fullmetal Alchemist", "Inuyasha", "Ranma ½", "Fairy Tail", "Gunslinger Girl", "The Qwaser of Stigmata", "WataMote", "Nisekoi", "Strawberry Marshmallow", and "Soul Eater".

</doc>
<doc id="28912" url="https://en.wikipedia.org/wiki?curid=28912" title="Srebrenica">
Srebrenica

Srebrenica (, ) is a Bosnian town in the easternmost Republika Srpska, Bosnia and Herzegovina. Srebrenica is a small mountain town, its main industry being salt mining and a nearby spa.
During the Bosnian War, the town was the site of a July 1995 massacre of the town's Bosniak population, determined to have been a crime of genocide.
On 24 March 2007, Srebrenica's municipal assembly adopted a resolution demanding independence from the Republika Srpska entity (although not from Bosnia's sovereignty); the Serb members of the assembly did not vote on the resolution.
Local communities.
The municipality (општина or "opština") is further subdivided into the following local communities (мјесне заједнице or "mjesne zajednice"):
Demographics.
The borders of the municipality in the 1953 and 1961 census were different. In 1953, Muslims by nationality had been yet to emerge as an ethnicity leading Slavic Muslims to identify as Yugoslavs. As "Yugoslav" was itself not adopted in 1948, they were all classified as "other". In 2003, Bosnian Serbs comprised 95% of the population of Srebrenica.
Economy.
Before 1992, there was a metal factory in the town, and lead, zinc, and gold mines nearby. The town's name (Srebrenica) means "silver mine", the same meaning of its old Latin name "Argentaria".
Before the war, Srebrenica also had a big spa and the town prospered from tourism. Nowadays, Srebrenica has some tourism but a lot less developed than before the war. Currently, a pension, motel and a hostel are operating in the town.
History.
Roman era.
During the Roman times, there was a settlement of Domavia, known to have been near a mine. Silver ore from there was moved to the mints in Salona in the southwest and Sirmium in the northeast using the Via Argentaria.
Middle Ages.
In the 13th and 14th century the region was part of the Banate of Bosnia, and, subsequently, the Bosnian Kingdom. The earliest reference to the name Srebrenica was in 1376, by which time it was already an important centre for trade in the western Balkans, based especially on the silver mines of the region. By that time, a large number of merchants of the Republic of Ragusa were established there, and they controlled the domestic silver trade and the export by sea, almost entirely via the port of Ragusa (Dubrovnik). During the 14th century, many German miners moved into the area. There were often armed conflicts about Srebrenica because of its mines. According to Czech historian Konstantin Josef Jireček, from 1410 to 1460, Srebrenica switched hands several times, being Serbian five times, Bosnian four times, and Ottoman three times. The mines of Bosnian Podrinje and Usora were part of the Serbian Despotate prior to the Ottoman conquest.
Ottoman period.
With the town coming under Ottoman rule, becoming less influenced by the Republic of Ragusa, the economic importance of Srebrenica went into decline, as did the proportion of Christians in the population. The Franciscan monastery was converted into a mosque, but the large number of Catholics, Ragusa and Saxon, caused the transformation of the town to Islam to be slower than in most of the other towns in the area.
The area of Osat was liberated for a short time during the First Serbian Uprising (1804–13), under the leadership of Kara-Marko Vasić from Crvica. Upon the breakout of the uprising, Metropolitan Hadži Melentije Stevanović contacted Vasić, who met with the rebel leadership. After participated in battles on the Drina (1804), Vasić asked Karađorđe for an army to liberate Osat; Lazar Mutap was dispatched and the region came under rebel rule. In 1808, the Ottomans cleared out Osat, and by 1813, the rebels left the region.
Second World War.
During the Second World War, the Ustaše massacred hundreds of Serbs in villages surrounding Srebrenica. In early January 1941, the Chetniks entered Srebrenica and killed around a thousand Muslim civilians in the town and in nearby villages.
Bosnian War.
The town of Srebrenica came to international prominence as a result of events during the Bosnian War (1992–1995). The strategic objectives proclaimed by the secessionist Bosnian Serb presidency included the creation of a border separating the Serb people from Bosnia's other ethnic communities and the abolition of the border along the River Drina separating Serbia and the Bosnian Serbs' Republika Srpska. The Bosnian Muslim/Bosniak majority population of the Drina Valley posed a major obstacle to the achievement of these objectives. In the early days of the campaign of forcible transfer (ethnic cleansing) that followed the outbreak of war in April 1992 the town of Srebrenica was occupied by Serb/Serbian forces. It was subsequently retaken by Bosniak resistance groups. Refugees expelled from towns and villages across the central Drina valley sought shelter in Srebrenica, swelling the town's population.
The town and its surrounding area was surrounded and besieged by Serb forces. On 16 April 1993, the United Nations declared the Bosnian Muslim/Bosniak enclave a UN safe area, to be "free from any armed attack or any other hostile act", and guarded by a small unit operating under the mandate of United Nations Protection Force (UNPROFOR).
Srebrenica and the other UN safe areas of Žepa and Goražde were isolated pockets of Bosnian government-held territory in eastern Bosnia. In July 1995, despite the town's UN-protected status, it was attacked and captured by the Army of Republika Srpska. Following the town's capture, all men of fighting age who fell into Bosnian Serb hands were massacred in a systematically organised series of summary executions. The women of the town and men below 16 years of age and above 55 were transferred by bus to Tuzla.
The Srebrenica massacre is considered the worst massacre in post-Second World War European history to this day.
In 2001, the Srebrenica massacre was determined by judgment of the International Criminal Tribunal for the former Yugoslavia (ICTY) to have been a crime of genocide (confirmed on appeal in 2004). This finding was upheld in 2007 by the International Court of Justice. The decision of the ICTY was followed by an admission to and an apology for the massacre by the Republika Srpska government.
Under the 1995 Dayton Agreement which ended the Bosnian War, Srebrenica was included in the territory assigned to Bosnian Serb control as the Republika Srpska entity of Bosnia and Herzegovina. Although guaranteed under the provisions of the Dayton Agreement, the return of survivors was repeatedly obstructed. In 2007, verbal and physical attacks on returning refugees continued to be reported in the region around Srebrenica.
Fate of Bosnian Muslim villages.
In 1992, Bosniak villages around Srebrenica were under constant attacks by Serb forces. The Bosnian Institute in the United Kingdom has published a list of 296 villages destroyed by Serb forces around Srebrenica three years before the genocide and in the first three months of war (April–June 1992):
According to the Naser Orić trial judgement:

</doc>
<doc id="28913" url="https://en.wikipedia.org/wiki?curid=28913" title="Steve Bracks">
Steve Bracks

Stephen Phillip "Steve" Bracks AC (born 15 October 1954) is a former Australian politician and the 44th Premier of Victoria. He first won the electoral district of Williamstown in 1994 for the Australian Labor Party and was party leader and premier from 1999 to 2007.
Bracks led Labor in Victoria to minority government at the 1999 election, defeating the incumbent Jeff Kennett Liberal and National coalition government. Labor was returned with a majority government after a landslide win at the 2002 election. Labor was elected for a third term at the 2006 election with a substantial but reduced majority. Bracks is the second longest-serving Labor premier in Victorian history. The treasurer, John Brumby, became Labor leader and premier in 2007 when Bracks retired from politics.
Early life.
Steve Bracks was born in Ballarat, where his family owns a fashion business. He is a Lebanese Australian; his paternal grandfather came to Australia as a child from Zahlé in the Beqaa Valley of Lebanon in the 1890s.
Bracks was educated in Ballarat at St Patrick's College and the Ballarat College of Advanced Education (now the Federation University), where he graduated in business studies and education. He became a keen follower of Australian rules football, supporting the Geelong Football Club.
Before politics.
From 1976 to 1981 Bracks was a school commerce teacher. During the 1980s he worked in local government in Ballarat and then as Executive Director of the Ballarat Education Centre. While in these positions he twice (1985 and 1988) contested the seat of Ballarat North in the Victorian Legislative Assembly for the Australian Labor Party.
In 1989 Bracks was appointed statewide manager of Victorian state government employment programs, under the Labor government of John Cain. He then became an adviser to both Cain and Cain's successor as Premier, Joan Kirner. Here he was able to witness from the inside the collapse of the Labor government following the economic and budgetary crisis which began in 1988. This experience gave Bracks a very conservative and cautious view of economic management in government.
Following the defeat of the Kirner government by the Liberal leader Jeff Kennett in late 1992, Bracks became Executive Director of the Victorian Printing Industry Training Board. He quit this post in 1994 when Kirner resigned from Parliament and Bracks was elected for Kirner's seat of Williamstown in the western suburbs of Melbourne, where he now lives with his wife Terry and their three children.
State politics.
Early days.
Bracks was immediately elected to Labor's front bench, as Shadow Minister for Employment, Industrial Relations and Tourism. In 1996, after Labor under John Brumby was again defeated, he became Shadow Treasurer. In March 1999, when it became apparent that Labor was headed for another defeat under Brumby's leadership, Brumby resigned and Bracks was elected Opposition Leader.
First term as Premier.
Political observers were almost unanimous that Bracks had no chance of defeating Liberal premier Jeff Kennett at the September 1999 election: polls gave Kennett a 60% popularity rating. Bracks and his senior colleagues (particularly Brumby, who comes from Bendigo) campaigned heavily in regional areas, accusing Kennett of ignoring regional communities. In response, voters in regional areas deserted the Kennett government. On election night, much to its own surprise, Labor increased its seat count from 29 to 41, with the Liberals and their National Party allies retaining 43, and three falling to rural independents. With the Coalition one seat short of government, the election was to be decided in Frankston East, when the death of incumbent Peter McLellan forced a supplementary election. That supplementary election was won by Labor on a large swing, resulting in a hung parliament. The independents agreed to support a minority Labor government, making Bracks the first Catholic Labor Premier of Victoria since 1932.
Former leader Brumby, appointed Treasurer, was regarded as a major part of the government's success. He and the Deputy Premier and Minister for Health, John Thwaites, and the Attorney-General, Rob Hulls, were regarded as the key ministers in the Bracks government.
Following a pre-1999 election commitment to consider the feasibility of introducing fast rail services to regional centres, in 2000 the government approved funding for the Regional Fast Rail project, upgrading rail lines between Melbourne and Ballarat, Bendigo, Geelong and Traralgon. However, in 2006 the Victorian Auditor General noted that in spite of $750 million spent, "We found that the delivery of more frequent fast rail services in the Geelong, Ballarat, and Bendigo corridors by the agreed dates was not achieved. In total, the journey time outcomes will be more modest than we would have expected with only a minority of travellers likely to benefit from significant journey time improvements. These outcomes occur because giving some passengers full express services means bypassing often large numbers of passengers at intermediate stations along the corridors."
On 14 December 2000, Steve Bracks released a document outlining his government's intent to introduce the Racial and Religious Tolerance Act 2001.
The major criticism of Bracks's first government was that their insistence on consultation stood in the way of effective, proactive government. Bracks, according to critics, achieved little, and lost the excitement of constant change that was characteristic of the Kennett years. The talents of some of the more junior ministers in the government were also questioned. Nevertheless, Bracks got through his first term without major mishaps, and his popularity undiminished.
Second term as Premier.
Labor won the 2002 election in a landslide, taking 62 seats out of 88 in the Legislative Assembly—only the third time in Victoria's history that a Labor government had been reelected. In another first, Labor won a slim but clear majority in the Legislative Council as well. While this was the greatest victory Labor had ever had in a Victorian state election, it brought with it considerable risks. With majorities in both houses Bracks could no longer cite his weak parliamentary position as an excuse for inaction.
On 28 August 2002, Bracks, in conjunction with his then New South Wales counterpart, Bob Carr, opened the Mowamba aqueduct between Jindabyne and Dalgety, to divert 38 gigalitres of water a year from Lake Eucumbene to the Snowy and Murray rivers. The ten-year plan cost A$300million with Victoria and NSW splitting the costs. Melbourne Water has stated that within 50 years there will be 20 percent less water going into Victorian reservoirs.
In May 2003 Bracks broke an election promise and announced that the proposed Scoresby Freeway in Melbourne's eastern suburbs would be a tollway rather than a freeway, as promised at the 2002 elections. As well as risking a loss of support in marginal seats in eastern Melbourne, this decision brought about a strong response from the Howard Federal government, which cut off federal funding for the project on the grounds that the Bracks government had reneged on the terms of the federal-state funding agreement. The decision seems to have been on the recommendation of Brumby, who was concerned with the state's budgetary position. Also opposing the decision was the Federal Labor Opposition, which feared anti-Labor reaction at the 2004 Federal election. The then Opposition Leader Mark Latham described a meeting with Bracks and federal shadow ministers, writing:
This backflip, while seen by many as an opportunity for the Liberals to make ground, saw the then leader of the Liberals, Robert Doyle, adopt a much-criticised policy of half tolls, which was later overturned by his successor, Ted Baillieu.
In 2005, Bracks announced that Victorian cattlemen would be banned from using Victoria's "High Plains" to graze cattle, ending a 170-year tradition. Stockmen had been fearing this decision since 1984, when a Labor government excised land to create the Alpine National Park. 300 cattlemen rode horses down Bourke street in protest. Victorian National Party leader Peter Ryan was quoted as saying that Bracks had "killed the man from Snowy River", a reference to the Banjo Paterson poem "The Man from Snowy River".
Bracks' second government achieved one of Victorian Labor's longest-held goals with a complete reform of the state's system for electing its upper house. It saw the introduction of proportional representation, with eight five-member regions replacing the current single-member constituencies. This system increases the opportunity for minor parties such as the Greens and DLP to win seats in the Legislative Council, giving them a greater chance of holding the balance of power. Illustrating the historic importance Labor assigns to the changes, in a speech to a conference celebrating the 150th anniversary of the Eureka Stockade, Bracks said it was "another victory for the aspirations of Eureka", and has described the changes as "his proudest achievement".
The staging of the 2006 Commonwealth Games, generally viewed as a success (albeit an expensive one), was viewed as a plus for Bracks and the government. With times reasonably good, a perception arguably reinforced by an extensive government advertising campaign selling the virtues of Victoria to Victorians, polls indicated little interest in change, although towards the end of the election campaign polling indicated that the Liberals under Baillieu were closing the gap.
Third term as Premier.
The election campaign was a relatively low-key affair, with the Government and Bracks largely running on their record, as well as their plans to tackle infrastructure issues in their third term. Bracks' image loomed large in Labor's election advertising. Liberal attacks concentrated on the slow process of infrastructure development under Bracks (notably on water supply issues relating to the severe drought affecting Victoria in the election leadup), and new Liberal leader Ted Baillieu promised to start construction on a range of new infrastructure initiatives, including a new dam on the Maribyrnong River and a desalination plant. Labor's broken election promise on Eastlink was also expected to be a factor in some seats in the eastern suburbs of Melbourne.
On 25 November 2006, Steve Bracks won his third election, comfortably defeating Baillieu to secure a third term, with a slightly reduced majority in the Lower House. This marked only the second time that the Victorian Labor Party had won a third term in office. His third term Cabinet was sworn in on 1 December 2006 with Bracks also holding the portfolio of Veterans' Affairs and Multicultural Affairs.
Resignation.
Bracks announced his resignation as Premier on 27 July 2007, saying this was in order to spend more time with his family. He stepped down on 30 July 2007. According to the ABC, Bracks had been under political and personal pressure in the weeks before his resignation. Alone among State Premiers, he had refused to agree to the Federal Government's $10 billion Murray-Darling Basin water conservation plan, and his son had been involved in an accident involving a charge of drink driving. Bracks told a media conference he could no longer give a 100 per cent commitment to politics:
Bracks' deputy John Thwaites announced his resignation on the same day. News of the resignations caused surprise to the general community as well as to politicians. It was revealed that then Federal Labor Leader Kevin Rudd was informed only minutes before the announcement, and tried to talk Bracks out of his decision. Bracks' Treasurer John Brumby was elected unopposed by the Victorian Labor Caucus as Premier, while Attorney-General Rob Hulls was elected Deputy Premier.
One consequence of Bracks leaving politics may have been the introduction of abortion law reform in Victoria. It has been suggested that the resignation of Premier Bracks sowed the seeds for abortion law reform by legislation that parliamentarians previously had refused to support, fearing a backlash from anti-abortion groups led by veteran campaigner Margaret Tighe. Bracks, as a Catholic of Lebanese descent, almost certainly would not have allowed abortion legislation into the parliament, but his successor John Brumby did not share this view, and the Abortion Law Reform Bill introduced by upper house member Candy Broad was passed by the Parliament in 2008.
After politics.
In August 2007, following his resignation as Premier, Bracks announced he would provide a short-term pro bono advising role in East Timor working alongside the newly elected Prime Minister Xanana Gusmão. Bracks was to spend a year travelling between Melbourne and Dili helping with the establishment of Gusmão's administration, the key departments that would need to be involved, and advising on how they would be accountable and reportable to the legislature.
During 2008 Bracks indicated his support for Victorian abortion law reform in Victoria.
In addition to his role advising Gusmão, Bracks also joined several company advisory boards: KPMG, insurance firm Jardine Lloyd Thompson Group, the AIMS Financial Group and the NAB. The KPMG appointment was controversial, as the Victorian government had awarded the firm over 100 contracts during Bracks' time as Premier. On 14 February 2008, the Federal Labor Government appointed Bracks to head an inquiry into the ongoing viability of the Australian car industry.
In 2010, Bracks was appointed a Companion of the Order of Australia for services to the community and the Parliament of Victoria. In recognition of his distinguished services to the Victorian community, he was awarded the degree of Doctor of Laws (honoris causa) – LL.D "(h.c.)" by Deakin University on 27 April 2010. He was also appointed to the Honorary Chair of the Deakin University Foundation.
In February 2013 after the announcement that Nicola Roxon would retire from federal politics, Bracks was cited as a possible candidate for her safe Labor seat of Gellibrand, but he ruled out running for the seat.
Bracks was appointed to the role of Australian Consul-General in New York in May 2013, by the Federal ALP Government of Julia Gillard. At the time, shadow Foreign Minister, the Coalition's Julie Bishop, described the appointment as 'inappropriate' because of the proximity to the upcoming election and 'arrogant' because of a lack of consultation with the then opposition. Following the defeat of the ALP at the 7 September election, incoming foreign minister Julie Bishop reversed the appointment in a decision described as 'petty and vindictive' by acting ALP foreign affairs spokeswoman Tanya Plibersek. Also fires affecting the Otway Ranges in Wye River and Separation Creek in December 2015 destroyed the former Premier's holiday home along with the other 115 homes destroyed in the fires.

</doc>
<doc id="28915" url="https://en.wikipedia.org/wiki?curid=28915" title="Small Isles">
Small Isles

The Small Isles (Scottish Gaelic: Na h-Eileanan Tarsainn) are a small archipelago of islands in the Inner Hebrides, off the west coast of Scotland. They lie south of Skye and north of Mull and Ardnamurchan – the most westerly point of mainland Scotland. The Small Isles is one of 40 National Scenic Areas in Scotland.
Main islands.
The four main islands are Canna, Rùm, Eigg and Muck. The largest is Rùm with an area of .
The islands now form part of Lochaber, in the Highland council area. Until 1891 Canna, Rùm and Muck were historically part of the counties of Argyll; Eigg was historically part of Inverness-shire, and all of the Small Isles were in Inverness-shire, from 1891 to 1975. The Gaelic name is literally "cross isles" referring to their position between the Morar and the Uists.
Smaller islands.
Smaller islands surrounding the main four include: 
Skerries.
There are also a number of skerries: 

</doc>
<doc id="28916" url="https://en.wikipedia.org/wiki?curid=28916" title="Shetland">
Shetland

Shetland (; ; ), also called the Shetland Islands, is a subarctic archipelago of Scotland that lies northeast of the island of Great Britain and forms part of the United Kingdom.
The islands lie some to the northeast of Orkney and southeast of the Faroe Islands and form part of the division between the Atlantic Ocean to the west and the North Sea to the east. The total area is and the population totalled 23,210 in 2012. Comprising the Shetland constituency of the Scottish Parliament, Shetland is also one of the 32 council areas of Scotland; the islands' administrative centre and only burgh is Lerwick, which is also the capital of Shetland since taking over from Scalloway in 1708.
The largest island, known simply as "Mainland", has an area of , making it the third-largest Scottish island and the fifth-largest of the British Isles. There are an additional 15 inhabited islands. The archipelago has an oceanic climate, a complex geology, a rugged coastline and many low, rolling hills.
Humans have lived there since the Mesolithic period, and the earliest written references to the islands date back to Roman times. The early historic period was dominated by Scandinavian influences, especially Norway, and the islands did not become part of Scotland until the 15th century. When Scotland became part of the Kingdom of Great Britain in 1707, trade with northern Europe decreased. Fishing has continued to be an important aspect of the economy up to the present day. The discovery of North Sea oil in the 1970s significantly boosted Shetland incomes, employment and public sector revenues.
The local way of life reflects the joint Norse and Scottish heritage including the Up Helly Aa fire festival, and a strong musical tradition, especially the traditional fiddle style. The islands have produced a variety of writers of prose and poetry, often in Shetland Scots. There are numerous areas set aside to protect the local fauna and flora, including a number of important seabird nesting sites. The Shetland pony and Shetland Sheepdog are two well-known Shetland animal breeds. Shetland also has a breed of pig.
The islands' motto, which appears on the Council's coat of arms, is "". This Icelandic phrase is taken from the Danish 1241 Basic Law, "Codex Holmiensis", and is also mentioned in "Njáls saga", and means "By law shall land be built".
Etymology.
The name of Shetland is derived from the Old Norse words, ' (hilt), and ' (land).
In AD 43 and 77 the Roman authors Pomponius Mela and Pliny the Elder referred to the seven islands they call ' and ' respectively, both of which are assumed to be Shetland. Another possible early written reference to the islands is Tacitus' report in AD 98, after describing the discovery and conquest of Orkney, that the Roman fleet had seen "Thule, too". In early Irish literature, Shetland is referred to as '—"the Isles of Cats", which may have been the pre-Norse inhabitants' name for the islands. The Cat tribe also occupied parts of the northern Scottish mainland and their name can be found in Caithness, and in the Gaelic name for Sutherland (', meaning "among the Cats").
The oldest version of the modern name Shetland is ', the Latinised adjectival form of the Old Norse name recorded in a letter from Harald count of Shetland in 1190, becoming "Hetland" in 1431 after various intermediate transformations. It is possible that the Pictish "cat" sound forms part of this Norse name. It then became "" in the 16th century.
As Norn was gradually replaced by Scots, ' became '. The initial letter is the Middle Scots letter, "yogh", the pronunciation of which is almost identical to the original Norn sound, "". When the use of the letter yogh was discontinued, it was often replaced by the similar-looking letter z, hence "", the misspelt form used to describe the pre-1975 county council. This is also the source of the ZE postcode used for the Shetlands.
Most of the individual islands have Norse names, although the derivations of some are obscure and may represent pre-Norse, possibly Pictish or even pre-Celtic names or elements.
Geography and geology.
Shetland is around north of mainland Scotland, covers an area of and has a coastline long.
Lerwick, the capital and largest settlement, has a population of 6,958 and about half of the archipelago's total population of 23,167 people live within of the town.
Scalloway on the west coast, which was the capital until 1708, has a population of less than 1,000.
Only 16 of about 100 islands are inhabited. The main island of the group is known as Mainland, and of the next largest, Yell, Unst, and Fetlar lie to the north and Bressay and Whalsay lie to the east. East and West Burra, Muckle Roe, Papa Stour, Trondra and Vaila are smaller islands to the west of Mainland. The other inhabited islands are Foula west of Walls, Fair Isle south-west of Sumburgh Head, and the Out Skerries to the east.
The uninhabited islands include Mousa, known for the Broch of Mousa, the finest preserved example in Scotland of these Iron Age round towers, St Ninian's Isle connected to Mainland by the largest active tombolo in the UK, and Out Stack, the northernmost point of the British Isles. Shetland's location means that it provides a number of such records: Muness is the most northerly castle in the United Kingdom and Skaw the most northerly settlement.
The geology of Shetland is complex, with numerous faults and fold axes. These islands are the northern outpost of the Caledonian orogeny, and there are outcrops of Lewisian, Dalriadan and Moine metamorphic rocks with histories similar to their equivalents on the Scottish mainland. There are also Old Red Sandstone deposits and granite intrusions. The most distinctive features are the ultrabasic ophiolite, peridotite and gabbro on Unst and Fetlar, which are remnants of the Iapetus Ocean floor. Much of Shetland's economy depends on the oil-bearing sediments in the surrounding seas. Geological evidence shows that in around 6100 BC a tsunami caused by the Storegga Slides hit Shetland, as well as the rest of the east coast of Scotland, and may have created a wave of up to high in the voes where modern populations are highest.
The highest point of Shetland is Ronas Hill, which only reaches . The Pleistocene glaciations entirely covered the islands. During this period, the Stanes of Stofast, a 2000-tonne glacial erratic, came to rest on a prominent hilltop in Lunnasting.
Shetland is a National Scenic Area which, unusually, is made up of a number of discrete locations: Fair Isle, Foula, South West Mainland (including the Scalloway Islands), Muckle Roe, Esha Ness, Fethaland and Herma Ness.
Climate.
Shetland has an oceanic, temperate maritime climate bordering on the subpolar variety, with long but cool winters and short mild summers. The climate all year round is moderate due to the influence of the surrounding seas, with average peak temperatures of in March and in July and August. Temperatures over are very rare. The highest temperature on record was in July 1991 and the coldest in the Januaries of 1952 and 1959. The frost-free period may be as little as three months. In contrast, inland areas of nearby Scandinavia on similar latitudes experience significantly larger temperature differences between summer and winter, with the average highs of regular July days comparable to Lerwick's all-time record heat that is around , further demonstrating the moderating effect of the Atlantic Ocean. In contrast, winters are considerably milder than those expected in nearby continental areas, even comparable to winter temperatures of many parts of England and Wales much further south.
The general character of the climate is windy and cloudy with at least of rain falling on more than 250 days a year. Average yearly precipitation is , with November and December the wettest months. Snowfall is usually confined to the period November to February, and snow seldom lies on the ground for more than a day. Less rain falls from April to August although no month receives less than . Fog is common during summer due to the cooling effect of the sea on mild southerly airflows.
Due to the islands' latitude, on clear winter nights the "northern lights" can sometimes be seen in the sky, while in summer there is almost perpetual daylight, a state of affairs known locally as the "simmer dim". Annual bright sunshine averages 1110 hours, rending overcast days common.
Prehistory.
Due to the practice, dating to at least the early Neolithic, of building in stone on virtually treeless islands, Shetland is extremely rich in physical remains of the prehistoric eras and there are over 5,000 archaeological sites all told. A midden site at West Voe on the south coast of Mainland, dated to 4320–4030 BC, has provided the first evidence of Mesolithic human activity on Shetland. The same site provides dates for early Neolithic activity and finds at Scord of Brouster in Walls have been dated to 3400 BC. "Shetland knives" are stone tools that date from this period made from felsite from Northmavine.
Pottery shards found at the important site of Jarlshof also indicate that there was Neolithic activity there although the main settlement dates from the Bronze Age. This includes a smithy, a cluster of wheelhouses and a later broch. The site has provided evidence of habitation during various phases right up until Viking times. Heel-shaped cairns, are a style of chambered cairn unique to Shetland, with a particularly large example on Vementry.
Numerous brochs were erected during the Iron Age. In addition to Mousa there are significant ruins at Clickimin, Culswick, Old Scatness and West Burrafirth, although their origin and purpose is a matter of some controversy. The later Iron Age inhabitants of the Northern Isles were probably Pictish, although the historical record is sparse. Hunter (2000) states in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.” In 2011, the collective site, "The Crucible of Iron Age Shetland", including Broch of Mousa, Old Scatness and Jarlshof, joined the UKs "Tentative List" of World Heritage Sites.
History.
Scandinavian colonisation.
The expanding population of Scandinavia led to a shortage of available resources and arable land there and led to a period of Viking expansion, the Norse gradually shifting their attention from plundering to invasion. Shetland was colonised during the late 8th and 9th centuries, the fate of the existing indigenous population being uncertain. Modern Shetlanders have almost identical proportions of Scandinavian matrilineal and patrilineal genetic ancestry, suggesting that the islands were settled by both men and women in equal measure.
Vikings then made the islands the headquarters of pirate expeditions carried out against Norway and the coasts of mainland Scotland. In response, Norwegian king Harald Hårfagre ("Harald Fair Hair") annexed the Northern Isles (comprising Orkney and Shetland) in 875. Rognvald Eysteinsson received Orkney and Shetland from Harald as an earldom as reparation for the death of his son in battle in Scotland, and then passed the earldom on to his brother Sigurd the Mighty.
The islands were Christianised in the late 10th century. King Olav Tryggvasson summoned the "jarl" Sigurd the Stout during a visit to Orkney and said, "I order you and all your subjects to be baptised. If you refuse, I'll have you killed on the spot and I swear I will ravage every island with fire and steel." Unsurprisingly, Sigurd agreed and the islands became Christian at a stroke. Unusually, from c. 1100 onwards the Norse "jarls" owed allegiance both to Norway and to the Scottish crown through their holdings as Earls of Caithness.
In 1194, when Harald Maddadsson was Earl of Orkney and Shetland, a rebellion broke out against King Sverre Sigurdsson of Norway. The "" ("Island Beardies") sailed for Norway but were beaten in the Battle of Florvåg near Bergen. After his victory King Sverre placed Shetland under direct Norwegian rule, a state of affairs that continued for nearly two centuries.
Increased Scottish interest.
From the mid-13th century onwards Scottish monarchs increasingly sought to take control of the islands surrounding the mainland. The process was begun in earnest by Alexander II and was continued by his successor Alexander III. This strategy eventually led to an invasion by Haakon Haakonsson, King of Norway. His fleet assembled in Bressay Sound before sailing for Scotland. After the stalemate of the Battle of Largs, Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. His death halted any further Norwegian expansion in Scotland and following this ill-fated expedition, the Hebrides and Mann were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth, although the Scots recognised continuing Norwegian sovereignty over Orkney and Shetland.
Pawned to Scotland.
In the 14th century, Orkney and Shetland remained a Norwegian province, but Scottish influence was growing. Jon Haraldsson, who was murdered in Thurso in 1231, was the last of an unbroken line of Norse jarls, and thereafter the earls were Scots noblemen of the houses of Angus and St. Clair. On the death of Haakon VI in 1380, Norway formed a political union with Denmark after which the interest of the royal house in the islands declined. In 1469, Shetland was pledged by Christian I, in his capacity as King of Norway, as security against the payment of the dowry of his daughter Margaret, betrothed to James III of Scotland. As the money was never paid, the connection with the crown of Scotland has become perpetual. In 1470, William Sinclair, 1st Earl of Caithness ceded his title to James III and the following year the Northern Isles were directly annexed to the Crown of Scotland, a process confirmed by Parliament in 1472. Nonetheless, Shetland's connection with Norway has proven to be enduring.
From the early 15th century on the Shetlanders sold their goods through the Hanseatic League of German merchantmen. The Hansa would buy shiploads of salted fish, wool and butter and import salt, cloth, beer and other goods. The late 16th century and early 17th century was dominated by the influence of the despotic Robert Stewart, Earl of Orkney, who was granted the islands by his half-sister Mary Queen of Scots, and his son Patrick. The latter commenced the building of Scalloway Castle, but after his imprisonment in 1609 the Crown annexed Orkney and Shetland again until 1643 when Charles I granted them to William Douglas, 7th Earl of Morton. These rights were held on and off by the Mortons until 1766, when they were sold by James Douglas, 14th Earl of Morton to Laurence Dundas.
Early British rule.
The trade with the North German towns lasted until the 1707 Act of Union when high salt duties prohibited the German merchants from trading with Shetland. Shetland then went into an economic depression as the Scottish and local traders were not as skilled in trading with salted fish. However, some local merchant-lairds took up where the German merchants had left off, and fitted out their own ships to export fish from Shetland to the Continent. For the independent farmers of Shetland this had negative consequences, as they now had to fish for these merchant-lairds.
Smallpox afflicted the islands in the 17th and 18th centuries, but as vaccines became common after 1760 the population increased to a maximum of 31,670 in 1861. However, British rule came at price for many ordinary people as well as traders. The Shetlanders' nautical skills were sought by the Royal Navy. Some 3,000 served during the Napoleonic wars from 1800 to 1815 and press gangs were rife. During this period 120 men were taken from Fetlar alone and only 20 of them returned home. By the late 19th century 90% of all Shetland was owned by just 32 people, and between 1861 and 1881 more than 8,000 Shetlanders emigrated. With the passing of the Crofters' Act in 1886 the Liberal prime minister William Gladstone emancipated crofters from the rule of the landlords. The Act enabled those who had effectively been landowners' serfs to become owner-occupiers of their own small farms. By this time fishermen from Holland, who had traditionally gathered each year off the coast of Shetland to fish for herring, triggered an industry in the islands that boomed from around 1880 until the 1920s when stocks of the fish began to dwindle. The production peaked in 1905 at more than a million barrels, of which seven hundred and eight thousand were exported.
20th century.
During World War I many Shetlanders served in the Gordon Highlanders, a further 3,000 served in the Merchant Navy and more than 1500 in a special local naval reserve. The 10th Cruiser Squadron was stationed at Swarbacks Minn and during a single year from March 1917 more than 4,500 ships sailed from Lerwick as part of an escorted convoy system. In total, Shetland lost more than 500 men, a higher proportion than any other part of Britain, and there were further waves of emigration in the 1920s and 1930s.
During World War II a Norwegian naval unit nicknamed the "Shetland Bus" was established by the Special Operations Executive in the autumn of 1940 with a base first at Lunna and later in Scalloway to conduct operations around the coast of Norway. About 30 fishing vessels used by Norwegian refugees were gathered and the Shetland Bus conducted covert operations, carrying intelligence agents, refugees, instructors for the resistance, and military supplies. It made over 200 trips across the sea with Leif Larsen, the most highly decorated allied naval officer of the war, making 52 of them. Several RAF bases were also established at Sullom Voe and several lighthouses suffered enemy air attacks.
Oil reserves discovered in the later 20th century in the seas both east and west of Shetland have provided a much needed alternative source of income for the islands. The East Shetland Basin is one of Europe's largest oil fields and as a result of the oil revenue and the cultural links with Norway, a small independence movement developed briefly to recast the constitutional position of Shetland. It saw as its model the Isle of Man, as well as Shetland's closest neighbour, the Faroe Islands, an autonomous dependency of Denmark.
Economy.
Today, the main revenue producers in Shetland are agriculture, aquaculture, fishing, renewable energy, the petroleum industry (crude oil and natural gas production), the creative industries and tourism.
Fishing remains central to the islands' economy today, with the total catch being in 2009, valued at over £73.2 million. Mackerel makes up more than half of the catch in Shetland by weight and value, and there are significant landings of haddock, cod, herring, whiting, monkfish and shellfish. Farming is mostly concerned with the raising of Shetland sheep, known for their unusually fine wool. Crops raised include oats and barley; however, the cold, windswept islands make for a harsh environment for most plants. Crofting, the farming of small plots of land on a legally restricted tenancy basis, is still practised and is viewed as a key Shetland tradition as well as an important source of income.
Oil and gas were first landed in 1978 at Sullom Voe, which has subsequently become one of the largest terminals in Europe. Taxes from the oil have increased public sector spending on social welfare, art, sport, environmental measures and financial development. Three quarters of the islands' workforce is employed in the service sector, and the Shetland Islands Council alone accounted for 27.9% of output in 2003. Shetland's access to oil revenues has funded the Shetland Charitable Trust, which in turn funds a wide variety of local programmes. The balance of the fund in 2011 was £217 million, i.e., about £9,500 per head.
In January 2007, the Shetland Islands Council signed a partnership agreement with Scottish and Southern Energy for the Viking Wind Farm, a 200-turbine wind farm and subsea cable. This renewable energy project would produce about 600 megawatts and contribute about £20 million to the Shetland economy per year. The plan is meeting significant opposition within the islands, primarily resulting from the anticipated visual impact of the development. The PURE project on Unst is a research centre which uses a combination of wind power and fuel cells to create a wind hydrogen system. The project is run by the Unst Partnership, the local community's development trust.
Knitwear is important both to the economy and culture of Shetland, and the Fair Isle design is well known. However, the industry faces challenges due to plagiarism of the word "Shetland" by manufacturers operating elsewhere, and a certification trademark, "The Shetland Lady", has been registered.
Shetland is served by a weekly local newspaper, "The Shetland Times" and the online "Shetland News" http://www.shetnews.co.uk/ with radio service being provided by BBC Radio Shetland and the commercial radio station SIBC.
Shetland is a popular destination for cruise ships, and in 2010 the Lonely Planet guide named Shetland as the sixth best region in the world for tourists seeking unspoilt destinations. The islands were described as "beautiful and rewarding" and the Shetlanders as "a fiercely independent and self-reliant bunch". Overall visitor expenditure was worth £16.4 million in 2006, in which year just under 26,000 cruise liner passengers arrived at Lerwick Harbour. In 2009, the most popular visitor attractions were the Shetland Museum, the RSPB reserve at Sumburgh Head, Bonhoga Gallery at Weisdale Mill and Jarlshof.
Transport.
Transport between islands is primarily by ferry, and Shetland Islands Council operates various inter-island services. Shetland is also served by a domestic connection from Lerwick to Aberdeen on mainland Scotland. This service, which takes about 12 hours, is operated by NorthLink Ferries. Some services also call at Kirkwall, Orkney, which increases the journey time between Aberdeen and Lerwick by 2 hours. There are plans of road tunnels to some of the islands, escecially Bressay and Whalsay, however it is hard to convince the mainland government to finance them.
Sumburgh Airport, the main airport on Shetland, is located close to Sumburgh Head, south of Lerwick. Loganair operates flights for FlyBe to other parts of Scotland up to ten times a day, the destinations being Kirkwall, Aberdeen, Inverness, Glasgow and Edinburgh. Lerwick/Tingwall Airport is located west of Lerwick. Operated by Directflight Ltd. in partnership with Shetland Islands Council, it is devoted to inter-island flights from the Shetland Mainland to most of the inhabited islands.
Scatsta Airport near Sullom Voe allows frequent charter flights from Aberdeen to transport oilfield workers and this small terminal has the fifth largest number of international passengers in Scotland.
Public bus services are operated on Mainland, Whalsay, Burra, Unst and Yell.
The archipelago is exposed to wind and tide, and there are numerous sites of wrecked ships. Lighthouses are sited as an aid to navigation at various locations.
Public services.
The Shetland Islands Council is the Local Government authority for all the islands, based in Lerwick Town Hall.
Shetland is sub-divided into 18 community council areas and into 12 civil parishes that are used for statistical purposes.
Education.
In Shetland there are two High Schools—Anderson and Brae—five Junior High Schools, and 24 primary schools.
Shetland is also home to the North Atlantic Fisheries College, the Centre for Nordic Studies and Shetland College, which are all associated with the University of the Highlands and Islands.
Sport.
The islands are represented by the Shetland football team who regularly compete in the Island Games. The islands' senior football league is the G&S Flooring Premier League.
Churches and religion.
The Reformation reached the archipelago in 1560. This was an apparently peaceful transition and there is little evidence of religious intolerance in Shetland's recorded history.
A variety of different religious denominations are represented in the islands.
The Methodist Church has a relatively high membership in Shetland, which is a District of the Methodist Church (with the rest of Scotland comprising a separate District).
The Church of Scotland has a Presbytery of Shetland that includes St. Columba's Church in Lerwick.
The Catholic population is served by the church of St. Margaret and the Sacred Heart in Lerwick. The Parish is part of the Diocese of Aberdeen.
The Scottish Episcopal Church (part of the Anglican Communion) has regular worship at St Magnus' Church, Lerwick, St Colman's Church, Burravoe, and the Chapel of Christ the Encompasser, Fetlar, the last of which is maintained by the Society of Our Lady of the Isles, the most northerly and remote Anglican religious order of nuns.
The Church of Jesus Christ of Latter-day Saints has a congregation in Lerwick. The former print works and offices of the local newspaper, The Shetland Times, has been converted into a chapel.
Politics.
Shetland is represented in the House of Commons as part of the Orkney and Shetland constituency, which elects one Member of Parliament, the current incumbent being Alistair Carmichael. This seat has been held by the Liberal Democrats or their predecessors the Liberal Party since 1950, longer than any other they represent in the UK.
In the Scottish Parliament the Shetland constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system. The current MSP is Tavish Scott of the Scottish Liberal Democrats. Shetland is within the Highlands and Islands electoral region.
The political composition of the Council is 22 Independents. Thus it is one of only three Councils in Scotland with a majority of elected members not representing a political party.
Roy Grönneberg, who founded the local chapter of the Scottish National Party in 1966, designed the flag of Shetland in cooperation with Bill Adams to mark the 500th anniversary of the transfer of the islands from Norway to Scotland. The colours are identical to those of the flag of Scotland, but are shaped in the Nordic cross. After several unsuccessful attempts, including a plebiscite in 1985, the Lord Lyon King of Arms approved it as the official flag of Shetland in 2005.
Local culture and the arts.
After the islands were transferred to Scotland, thousands of Scots families emigrated to Shetland in the 16th and 17th centuries but studies of the genetic makeup of the islands' population indicate that Shetlanders are just under half Scandinavian in origin. A sizeable component of Scandinavian patrilineal ancestry has been reported in Orkney (55%) and Shetland (68%). This combination is reflected in many aspects of local life. For example, almost every place name in use can be traced back to the Vikings. The Norn language was a form of Old Norse, which continued to be spoken until the 18th century when it was replaced by an insular dialect of Scots known as Shetlandic, which is in turn being replaced by Scottish English. Although Norn was spoken for hundreds of years it is now extinct and few written sources remain. Shetlandic is used both in local radio and dialect writing, and kept alive by the Shetland Folk Society.
The Lerwick Up Helly Aa is one of a variety of fire festivals held in Shetland annually in the middle of winter, it is always started on the last Tuesday of January. The festival is just over 100 years old in its present, highly organised form. Originally, a festival held to break up the long nights of winter and mark the end of Yule, the festival has become one celebrating the isles' heritage and includes a procession of men dressed as Vikings and the burning of a replica longship.
The cuisine of Shetland is based on locally produced lamb, beef and seafood, much of it organic. Inevitably, the real ale-producing Valhalla Brewery is the most northerly in Britain. The Shetland Black is a variety of blue potato with a dark skin and indigo coloured flesh markings.
Shetland competes in the biennial International Island Games, which it hosted in 2005.
Music.
Shetland's culture and landscapes have inspired a variety of musicians, writers and film-makers. The Forty Fiddlers was formed in the 1950s to promote the traditional fiddle style, which is a vibrant part of local culture today. Notable exponents of Shetland folk music include Aly Bain, Fiddlers' Bid, and the late Tom Anderson and Peerie Willie Johnson. Thomas Fraser was a country musician who never released a commercial recording during his life, but whose work has become popular more than 20 years after his untimely death in 1978.
Writers.
Walter Scott's 1822 novel "The Pirate" is set in "a remote part of Shetland", and was inspired by his 1814 visit to the islands. The name "Jarlshof" meaning "Earl's Mansion" is a coinage of his. Robert Cowie, a doctor born in Lerwick published the 1874 work 
Hugh MacDiarmid, the Scots poet and writer lived in Whalsay from the mid-1930s through 1942, and wrote many poems there, including a number that directly address or reflect the Shetland environment such as "On A Raised Beach", which was inspired by a visit to West Linga. The 1975 novel "North Star" by Hammond Innes is largely set in Shetland and Raman Mundair's 2007 book of poetry "A Choreographer's Cartography" offers a British Asian perspective on the landscape. The "Shetland Quartet" by Ann Cleeves, who previously lived in Fair Isle, is a series of crime novels set around the islands. In 2013 her novel "Red Bones" became the basis of BBC crime drama television series "Shetland".
Vagaland, who grew up in Walls, was arguably Shetland's finest poet of the 20th century. Haldane Burgess was a Shetland historian, poet, novelist, violinist, linguist and socialist and Rhoda Bulter (1929–94) is one of the best-known Shetland poets of recent times. Other 20th and 21st century poets and novelists include Christine De Luca, Robert Alan Jamieson who grew up in Sandness, the late Lollie Graham of Veensgarth, Stella Sutherland of Bressay, the late William J Tait from Yell and Laureen Johnson.
There are two monthly magazines in production: "Shetland Life" and "i'i' Shetland". The quarterly "The New Shetlander", founded in 1947, is said to be Scotland's longest-running literary magazine. For much of the later 20th century it was the major vehicle for the work of local writers — and others, including early work by George Mackay Brown.
Films.
Michael Powell made "The Edge of the World" in 1937, a dramatisation based on the true story of the evacuation of the last 36 inhabitants of the remote island of St Kilda on 29 August 1930. St Kilda lies in the Atlantic Ocean, west of the Outer Hebrides but Powell was unable to get permission to film there. Undaunted, he made the film over four months during the summer of 1936 on Foula and the film transposes these events to Shetland. Forty years later, the documentary "Return to the Edge of the World" was filmed, capturing a reunion of cast and crew of the film as they revisited the island in 1978.
A number of other films have been made on or about Shetland including "A Crofter's Life in Shetland" (1932) "A Shetland Lyric" (1934), "Devil's Gate" (2003) and "It's Nice Up North" (2006), a comedy documentary by Graham Fellows. An annual film festival takes place in the newly built Mareel, a cinema, music and education venue.
Wildlife.
Shetland has three national nature reserves, at the seabird colonies of Hermaness and Noss, and at Keen of Hamar to preserve the serpentine flora. There are a further 81 SSSIs, which cover 66% or more of the land surfaces of Fair Isle, Papa Stour, Fetlar, Noss and Foula. Mainland has 45 separate sites.
Flora.
The landscape in Shetland is marked by the grazing of sheep and the harsh conditions have limited the total number of plant species to about 400. Native trees such as rowan and crab apple are only found in a few isolated places such as cliffs and loch islands. The flora is dominated by Arctic-alpine plants, wild flowers, moss and lichen. Spring squill, buck's-horn plantain, Scots lovage, roseroot and sea campion are abundant, especially in sheltered places. Shetland mouse-ear ("Cerastium nigrescens") is an endemic flowering plant found only in Shetland. It was first recorded in 1837 by botanist Thomas Edmondston. Although reported from two other sites in the nineteenth century, it currently grows only on two serpentine hills on the island of Unst. The nationally scarce oysterplant is found on several islands and the British Red Listed bryophyte "Thamnobryum alopecurum" has also been recorded.
Fauna.
Shetland has numerous seabird colonies. Birds found on the islands include Atlantic puffin, storm-petrel, red-throated diver, northern gannet and bonxie. Numerous rarities have also been recorded including black-browed albatross and snow goose, and a single pair of snowy owls bred on Fetlar from 1967 to 1975. The Shetland wren, Fair Isle wren and Shetland starling are subspecies endemic to Shetland. There are also populations of various moorland birds such as curlew, snipe and golden plover.
The geographical isolation and recent glacial history of Shetland have resulted in a depleted mammalian fauna and the brown rat and house mouse are two of only three species of rodent present on the islands. The Shetland field mouse is the third and the archipelago's fourth endemic subspecies, of which there are three varieties on Yell, Foula and Fair Isle. They are variants of "Apodemus sylvaticus" and archaeological evidence suggests that this species was present during the Middle Iron Age (around 200 BC to AD 400). It is possible that "Apodemus" was introduced from Orkney where a population has existed since at the least the Bronze Age.
Domesticated animals.
There is a variety of indigenous breeds, of which the diminutive Shetland pony is probably the best known, as well as being an important part of the Shetland farming tradition. The first written record of the pony was in 1603 in the Court Books of Shetland and, for its size, it is the strongest of all the horse breeds. Others are the Shetland Sheepdog or "Sheltie", the endangered Shetland cattle and Shetland Goose and the Shetland sheep which is believed to have originated prior to 1000 AD. The Grice was a breed of semi-domesticated pig that had a habit of attacking lambs, and that became extinct in 1930.

</doc>
<doc id="28917" url="https://en.wikipedia.org/wiki?curid=28917" title="Soay, Skye">
Soay, Skye

Soay (, ) is an island just off the coast of Skye, in the Inner Hebrides of Scotland.
Geography.
Soay lies to the west of Loch Scavaig on the south-west coast of Skye, from which it is separated by Soay Sound. Unlike its neighbours Skye and Rùm, Soay is low-lying, reaching at Beinn Bhreac. The dumb-bell shaped island is virtually cut in half by inlets that form Soay Harbour (N) and the main bay, Camas nan Gall (to the S). The main settlement, Mol-chlach, is on the shore of Camas nan Gall. It is normally reached by boat from Elgol. The island is part of the Cuillin Hills National Scenic Area, one of 40 in Scotland.
History.
The name derives from Old Norse "so-øy" meaning "Sheep Island". Camas nan Gall (G: "Bay of Foreigners") is probably named after the Norse invaders, after whom the Hebrides ("Na h-Innse Gall") are also named.
The population peaked at 158 in 1851, following eviction of crofters from Skye in the Highland Clearances.
In 1946, author Gavin Maxwell bought the island and established a factory to process shark oil from basking sharks. The enterprise was unsuccessful, lasting just three years. Maxwell wrote about it in his book "Harpoon at a Venture". After the failure of the business the island was sold on to Maxwell's business partner, Tex Geddes. The island had the first solar-powered telephone exchange in the world.
Previously mainly Scottish Gaelic-speaking, most of the population was evacuated to Mull on 20 June 1953, since when the island has been sparsely populated. In 2001 the population was 7. By 2003 this had dwindled to 2 and the usually resident population in 2011 was a single individual.
Stamps.
Local stamps were issued for Soay between 1965 and 1967, all on the Europa theme, some being overprinted to commemorate Sir Winston Churchill. As the stamps were produced without the owner's permission, they are regarded as bogus.

</doc>
<doc id="28918" url="https://en.wikipedia.org/wiki?curid=28918" title="Storytelling game">
Storytelling game

A storytelling game is a game where two or more persons collaborate on telling a spontaneous story. Usually, each player takes care of one or more characters in the developing story. Some games in the tradition of role-playing games require one participant to take the roles of the various supporting characters, as well as introducing non-character forces (for example, a flood), but other systems dispense with this figure and distribute this function among all players.
Since this person usually sets the ground and setting for the story, he or she is often referred to as the "storyteller" (often contracted to "ST") or "narrator". Any number of other alternate forms may be used, many of which are variations on the term "gamemaster"; these variants are especially common in storytelling games derived from or similar to role-playing games.
In contrast to improv theater, storytelling gamers describe the actions of their characters rather than acting them out, except during dialogue or, in some games, monologue. However, "live action" versions exist, which are very much akin to theater except in the crucial absence of a non-participating audience.
Role-playing games.
The most popular modern storytelling games originated as a subgenre of role-playing games, where the game rules and statistics are heavily de-emphasised in favor of creating a believable story and immersive experience for all involved. So while in a conventional game the announcement that one's character is going to leap over a seven-meters-wide canyon will be greeted with the request to roll a number of dice, a player in a storytelling game who wishes to have a character perform a similar feat will have to convince the others (especially the storyteller) why it is both probable and keeping within the established traits of their character to successfully do so. As such, these games are a subclass of diceless role-playing games.
Not all players find the storytelling style of role-playing satisfying. Many role-playing gamers are more comfortable in a system that gives them less freedom, but where they do not need to police themselves; others find it easier to enjoy a system where a more concrete framework of rules is already present. These three types of player are discussed by the GNS theory.
Some role-playing game systems which describe themselves as "storytelling games" nevertheless use randomisers rather than story in the arbitration of the rules, often in the form of a contest of Rock, Paper, Scissors or a card drawn from a deck of cards. Such "storytelling" games are instead simplified or streamlined forms of traditional role-playing games. Conversely, most modern role-playing games encourage gamemasters to ignore their gaming systems if it makes for a more enjoyable story, even though they may not describe themselves as "storytelling" games.
A growing number of websites utilize a bulletin board system, in which the gaming is akin to Collaborative Fiction but known as a "Literary Role-Playing Game" (not to be confused with LARPs). The players contribute to an ongoing story with defined parameters but no narrator or directing force. A 'moderator' may oversee the gamers to ensure that the rules, guidelines and parameters of the gaming "world" are being upheld, but otherwise the writers are free to interact as players in an improvisational play. Many of these "Literary RPGs" are fan-fiction based, such as (most prevalently) Tolkien's Middle-earth, Star Wars, Harry Potter, Twilight, any number of anime and manga sources, or they are simply based in thematic worlds such as the mythologies of Ancient Greece, fairy tales, the Renaissance or science fiction. Most often referred to as "Literary RPGs" and place a greater emphasis on writing skill and storytelling ability than on any sense of competition driven outcome.
White Wolf Game Studio's Storyteller System, which is used in World of Darkness role-playing games such as "" and live-action games under the Mind's Eye Theatre imprint, is the best-known and most popular role-playing game described as a "storytelling game".
Alternate form role-playing games.
An early design of a collaborative storytelling game not based in simulation was created by Chris Engle c. 1988 with his "Matrix Game". In this system, a referee decides the likeliness of the facts proposed by the players, and those facts happen or are rejected according with a dice roll. Players can propose counter-arguments that are resolved in a dice rolling contest. A conflict round can follow to resolve any inconsistencies or further detail new plot points. Matrix Games are now presented in a board game format.
In 1999, game designer Ian Millington developed an early work called "Ergo" which established the basis for collaborative role-playing. It was designed with the rules of the Fudge universal role-playing system in mind but added modifications necessary to get rid of the need for a gamemaster, distributing the responsibility for the game and story equally among all players and undoing the equivalence between player and character.
Modern rule systems (such as the coin system in Universalis) rely less on randomness and more in collaboration between players. This includes rules based on economic systems that force players to negotiate the details of the story, and solve conflicts based on the importance that they give to a given plot element and the resources they're willing to spend to make it into the story.

</doc>
<doc id="28922" url="https://en.wikipedia.org/wiki?curid=28922" title="Scorpion">
Scorpion

Scorpions are predatory arachnids of the order Scorpiones. They have eight legs and are easily recognised by the pair of grasping pedipalps and the narrow, segmented tail, often carried in a characteristic forward curve over the back, ending with a venomous stinger. Scorpions range in size from 9 mm / 0.3 in. ("Typhlochactas mitchelli") to 23 cm / 9 in. ("Heterometrus swammerdami").
The evolutionary history of scorpions goes back to the Silurian era 430 million years ago. They have adapted to a wide range of environmental conditions and can now be found on all continents except Antarctica. Scorpions number about 1750 described species, with 13 extant families recognised to date. Only about 25 of these species are known to have venom capable of killing a human being.The taxonomy has undergone changes and is likely to change further, as genetic studies are bringing forth new information.
Scorpion stings are painful but are usually harmless. For stings from species found in the United States, no treatment is normally needed for healthy adults although medical care should be sought for children and for the elderly. Stings from species found elsewhere may require medical attention.
Etymology.
The word "scorpion" is thought to have originated in Middle English between 1175 and 1225 AD from Old French ', or from Italian ', both derived from the Latin word ', which is the romanization of the Greek word  – '.
Geographical distribution.
Scorpions are found on all major land masses except Antarctica. Scorpions did not occur naturally in Great Britain, New Zealand and some of the islands in Oceania, but now have been accidentally introduced in some of these places by human trade and commerce. The greatest diversity of scorpions in the Northern Hemisphere is to be found in the subtropical areas lying between latitudes 23° N and 38° N. Above these latitudes, the diversity decreases, with the northernmost natural occurrence of scorpions being the northern scorpion "Paruroctonus boreus" at Medicine Hat, Alberta, Canada 50° N.
Today, scorpions are found in virtually every terrestrial habitat, including high-elevation mountains, caves and intertidal zones, with the exception of boreal ecosystems, such as the tundra, high-altitude taiga and the permanently snow-clad tops of some mountains. As regards microhabitats, scorpions may be ground-dwelling, tree-living, lithophilic (rock-loving) or psammophilic (sand-loving); some species, such as "Vaejovis janssi", are versatile and found in every type of habitat in Baja California, while others occupy specialized niches such as "Euscorpius carpathicus", which occupies the littoral zone of the shore.
Five colonies of scorpions ("Euscorpius flavicaudis") have established themselves in Sheerness on the Isle of Sheppey in the United Kingdom. This small population has been resident since the 1860s, having probably arrived with imported fruit from Africa. This scorpion species is small and completely harmless to humans. At just over 51° N, this marks the northernmost limit where scorpions live in the wild.
Classification.
There are thirteen families and about 1,750 described species and subspecies of scorpions. In addition, there are 111 described taxa of extinct scorpions.
This classification is based on that of Soleglad & Fet (2003), which replaced the older, unpublished classification of Stockwell. Additional taxonomic changes are from papers by Soleglad et al. (2005).
Systematics.
The following classification covers extant taxa to the rank of family.
Karsch, 1880 Been demoted to Family level-->
Fossil record.
Scorpions have been found in many fossil records, including marine Silurian and estuarine Devonian deposits, coal deposits from the Carboniferous Period and in amber. The oldest known scorpions lived around 430 million years ago in the Silurian period. Though once believed to have lived on the bottom of shallow tropical seas, early scorpions are now believed to have been terrestrial and to have washed into marine settings together with plant matter. These first scorpions were believed to have had gills instead of the present forms' book lungs though this has subsequently been refuted. The oldest Gondwanan scorpiones ("Gondwanascorpio") comprise the earliest known terrestrial animals from Gondwana. Currently, 111 fossil species of scorpion are known. Unusually for arachnids, there are more species of Palaeozoic scorpion than Mesozoic or Cenozoic ones.
The eurypterids, marine creatures that lived during the Palaeozoic era, share several physical traits with scorpions and may be closely related to them. Various species of Eurypterida could grow to be anywhere from to in length. However, they exhibit anatomical differences marking them off as a group distinct from their Carboniferous and Recent relatives. Despite this, they are commonly referred to as "sea scorpions". Their legs are thought to have been short, thick, tapering and to have ended in a single strong claw; it appears that they were well-adapted for maintaining a secure hold upon rocks or seaweed against the wash of waves, like the legs of a shore crab. Cladistic analyses have supported the idea that the eurypterids are a distinct group from the scorpions.
Morphology.
The body of a scorpion is divided into two parts (tagmata): the head (cephalothorax) and the abdomen (opisthosoma), which is subdivided into a broad anterior (mesosoma), or preabdomen, and a narrow taillike posterior (metasoma), or postabdomen.
Cephalothorax.
The cephalothorax, also called the "prosoma", comprises the carapace, eyes, chelicerae (mouth parts), pedipalps (the pedipalps of scorpions have chelae, commonly called claws or pincers) and four pairs of walking legs. The scorpion's exoskeleton is thick and durable, providing good protection from predators. Scorpions have two eyes on the top of the cephalothorax, and usually two to five pairs of eyes along the front corners of the cephalothorax. The position of the eyes on the cephalothorax depends in part on the hardness or softness of the soil upon which they spend their lives.
The pedipalp is a segmented, chelate (clawed) appendage used for prey immobilization, defense and sensory purposes. The segments of the pedipalp (from closest to the body outwards) are coxa, trochanter, femur (humerus), patella, tibia (including the fixed claw and the manus) and tarsus (moveable claw). A scorpion has darkened or granular raised linear ridges, called "keels" or "carinae" on the pedipalp segments and on other parts of the body, which are useful taxonomically.
Mesosoma.
The abdomen, also called the "opisthosoma", consists of seven segments (somites), each covered dorsally by a sclerotosed plate (tergum) and also ventrally for segments 3 to 7. The first abdominal segment bears a pair of genital opercula covering the gonopore. Segment 2 consists of the basal plate with the pectines, which are a pair of limbs transformed into sensory organs. Segments 3 to 7 each have a pair of spiracles, the openings for the scorpion's respiratory organs, known as book lungs. The spiracle openings may be slits, circular, elliptical, or oval.
Metasoma.
The metasoma, the scorpion's tail, comprises five caudal segments (the first tail segment looks like a last mesosoman segment) and the sixth bearing the telson (the sting). The telson, in turn, consists of the vesicle, which holds a pair of venom glands, and the hypodermic aculeus, the venom-injecting barb.
On rare occasions, scorpions can be born with two metasomata (tails). Two-tailed scorpions are not a different species, merely a genetic abnormality.
Fluorescence.
Scorpions are also known to glow a vibrant blue-green when exposed to certain wavelengths of ultraviolet light such as that produced by a black light, due to the presence of fluorescent chemicals in the cuticle. One fluorescent component is now known to be beta-carboline. A hand-held UV lamp has long been a standard tool for nocturnal field surveys of these animals. Fluorescence occurs as a result of sclerotisation and increases in intensity with each successive instar. This fluorescence may have an active role in scorpion light detection.
Biology.
Scorpions prefer areas where the temperatures range from , but may survive temperatures ranging from well below freezing to desert heat. Scorpions of the genus "Scorpiops" living in high Asian mountains, bothriurid scorpions from Patagonia and small "Euscorpius" scorpions from Central Europe can all survive winter temperatures of about . In Repetek (Turkmenistan), seven species of scorpion (of which "Pectinibuthus birulai" is endemic) live in temperatures varying from .
They are nocturnal and fossorial, finding shelter during the day in the relative cool of underground holes or undersides of rocks, and emerging at night to hunt and feed. Scorpions exhibit photophobic behavior, primarily to evade detection by predators such as birds, centipedes, lizards, mice, opossums, and rats.
Scorpions are opportunistic predators of small arthropods, although the larger kinds have been known to kill small lizards and mice. The large pincers are studded with highly sensitive tactile hairs, and the moment an insect touches these, they use their chelae (pincers) to catch the prey. Depending on the toxicity of their venom and size of their claws, they will then either crush the prey or inject it with neurotoxic venom. This will kill or paralyze the prey so the scorpion can eat it. Scorpions have an unusual style of eating using chelicerae, small claw-like structures that protrude from the mouth that are unique to the Chelicerata among arthropods. The chelicerae, which are very sharp, are used to pull small amounts of food off the prey item for digestion into a "pre-oral cavity" below the chelicerae and carapace. Scorpions can ingest food only in a liquid form; they have external digestion. The digestive juices from the gut are egested onto the food and the digested food sucked in liquid form. Any solid indigestible matter (fur, exoskeleton, etc.) is trapped by setae in the pre-oral cavity, which is ejected by the scorpion.
Scorpions can consume huge amounts of food at one sitting. They have a very efficient food storage organ and a very low metabolic rate combined with a relatively inactive lifestyle. This enables scorpions to survive long periods when deprived of food; some are able to survive 6 to 12 months of starvation. Scorpions excrete very little; their waste consists mostly of insoluble nitrogenous compounds, such as xanthine, guanine and uric acid.
Reproduction.
Most scorpions reproduce sexually, and most species have male and female individuals. However, some species, such as "Hottentotta hottentotta", "Hottentotta caboverdensis", "Liocheles australasiae", "Tityus columbianus", "Tityus metuendus", "Tityus serrulatus", "Tityus stigmurus", "Tityus trivittatus" and "Tityus urugayensis", reproduce through parthenogenesis, a process in which unfertilised eggs develop into living embryos. Parthenogenic reproduction starts following the scorpion's final moult to maturity and continues thereafter.
Sexual reproduction is accomplished by the transfer of a spermatophore from the male to the female; scorpions possess a complex courtship and mating ritual to effect this transfer. Mating starts with the male and female locating and identifying each other using a mixture of pheromones and vibrational communication. Once they have satisfied the other that they are of opposite sex and of the correct species, mating can commence.
The courtship starts with the male grasping the female's pedipalps with his own; the pair then perform a "dance" called the ""promenade à deux"". In this "dance", the male leads the female around searching for a suitable place to deposit his spermatophore. The courtship ritual can involve several other behaviours, such as juddering and a cheliceral kiss, in which the male's chelicerae – pincers – grasp the female's in a smaller more intimate version of the male's grasping the female's pedipalps and in some cases injecting a small amount of his venom into her pedipalp or on the edge of her cephalothorax, probably as a means of pacifying the female.
When the male has identified a suitable location, he deposits the spermatophore and then guides the female over it. This allows the spermatophore to enter her genital opercula, which triggers release of the sperm, thus fertilising the female. The mating process can take from 1 to 25+ hours and depends on the ability of the male to find a suitable place to deposit his spermatophore. If mating continues too long, the female may lose interest, ending the process.
Once the mating is complete, the male and female will separate. The male will generally retreat quickly, most likely to avoid being cannibalised by the female, although sexual cannibalism is infrequent with scorpions.
Birth and development.
Unlike the majority of species in the class Arachnida, which are oviparous, scorpions seem to be universally ovoviviparous. The young are born one by one after hatching and expelling the embryonic membrane, if any, and the brood is carried about on its mother's back until the young have undergone at least one moult. Before the first moult, scorplings cannot survive naturally without the mother, since they depend on her for protection and to regulate their moisture levels. Especially in species that display more advanced sociability (e.g. "Pandinus" spp.), the young/mother association can continue for an extended period of time. The size of the litter depends on the species and environmental factors, and can range from two to over a hundred scorplings. The average litter however, consists of around 8 scorplings.
The young generally resemble their parents. Growth is accomplished by periodic shedding of the exoskeleton (ecdysis). A scorpion's developmental progress is measured in instars (how many moults it has undergone). Scorpions typically require between five and seven moults to reach maturity. Moulting commences with a split in the old exoskeleton just below the edge of the carapace (at the front of the prosoma). The scorpion then emerges from this split; the pedipalps and legs are first removed from the old exoskeleton, followed eventually by the metasoma. When it emerges, the scorpion's new exoskeleton is soft, making the scorpion highly vulnerable to attack. The scorpion must constantly stretch while the new exoskeleton hardens to ensure that it can move when the hardening is complete. The process of hardening is called sclerotisation. The new exoskeleton does not fluoresce; as sclerotisation occurs, the fluorescence gradually returns.
Relationship with humans.
Sting and venom.
All known scorpion species possess venom and use it primarily to kill or paralyze their prey so that it can be eaten. In general, it is fast-acting, allowing for effective prey capture. However, as a general rule, they will kill their prey with brute force if they can, as opposed to using venom. It is also used as a defense against predators. The venom is a mixture of compounds (neurotoxins, enzyme inhibitors, etc.) each not only causing a different effect but possibly also targeting a specific animal. Each compound is made and stored in a pair of glandular sacs and is released in a quantity regulated by the scorpion itself. Of the 1,000+ known species of scorpion, only 25 have venom that is deadly to humans; most of those belong to the family Buthidae (including "Leiurus quinquestriatus", "Hottentotta", "Centruroides" and "Androctonus").
Treatment.
First aid for scorpion stings is generally symptomatic. It includes strong analgesia, either systemic (opiates or paracetamol) or locally applied (such as a cold compress). Hypertensive crises are treated with anxiolytics and vasodilators. Scorpion envenomation with high morbidity and mortality is usually due to either excessive autonomic activity and cardiovascular toxic effects or neuromuscular toxic effects. Antivenom is the specific treatment for scorpion envenomation combined with supportive measures including vasodilators in patients with cardiovascular toxic effects and benzodiazepines when there is neuromuscular involvement. Although rare, severe hypersensitivity reactions including anaphylaxis to scorpion antivenin (SAV) are possible.
Medical use.
Short-chain scorpion toxins constitute the largest group of potassium (K+) channel blocking peptides; an important physiological role of the KCNA3 channel, also known as KV1.3, is to help maintain large electrical gradients for the sustained transport of ions such as Ca2+ that controls T lymphocyte (T cell) proliferation. Thus KV1.3 blockers could be potential immunosuppressants for the treatment of autoimmune disorders (such as rheumatoid arthritis, inflammatory bowel disease and multiple sclerosis).
The venom of "Uroplectes lineatus" is clinically important in dermatology.
Toxins being investigated include the following:
Production.
Scorpions for use in the pharmaceutical industry are collected from the wild in Pakistan. Farmers in the Thatta District are paid about US$100 for each 40 gram scorpion and 60 gram specimens are reported to fetch at least US$50,000. The trade is reported to be illegal but thriving.
Consumption.
Fried scorpion is a traditional dish from Shandong, China.
As a part of Chinese medicine, scorpion wine and snake wine are used as analgesic and antidote.

</doc>
<doc id="28923" url="https://en.wikipedia.org/wiki?curid=28923" title="Shriners">
Shriners

Shriners International, also commonly known as The Shriners, is a society established in 1870 and is headquartered in Tampa, Florida, USA. It is an appendant body to Freemasonry.
Shriners International describes itself as a fraternity based on fun, fellowship, and the Masonic principles of brotherly love, relief, and truth. There are approximately 350,000 members from 195 temples (chapters) in the U.S., Canada, Brazil, Mexico, the Republic of Panama, the Philippines, Puerto Rico, Europe, and Australia. The organization is best known for the Shriners Hospitals for Children that it administers, and the red fezzes that members wear.
The organization was previously known as Ancient Arabic Order of the Nobles of the Mystic Shrine (A.A.O.N.M.S.) or Shriners North America. The name was changed in 2010 across North America, Central America, South America, Europe, and Southeast Asia.
History.
In 1870, there were several thousand Masons in Manhattan, many of whom lunched at the Knickerbocker Cottage at a special table on the second floor. There, the idea of a new fraternity for Masons stressing fun and fellowship was discussed. Walter M. Fleming, M.D., and William J. Florence took the idea seriously enough to act upon it.
Florence, a world-renowned actor, while on tour in Marseille, was invited to a party given by an Arabian diplomat. The entertainment was something in the nature of an elaborately staged musical comedy. At its conclusion, the guests became members of a secret society. Florence took copious notes and drawings at his initial viewing and on two other occasions, once in Algiers and once in Cairo. When he returned to New York in 1870, he showed his material to Fleming.
Fleming took the ideas supplied by Florence and converted them into what would become the "Ancient Arabic Order of the Nobles of the Mystic Shrine (A.A.O.N.M.S.)". Fleming created the ritual, emblem and costumes. Florence and Fleming were initiated August 13, 1870, and initiated 11 other men on June 16, 1871.
The group adopted a Middle Eastern theme and soon established Temples (though the term Temple has now generally been replaced by Shrine Auditorium or Shrine Center). The first Temple established was Mecca Temple (now known as Mecca Shriners), established at the New York City Masonic Hall on September 26, 1872. Fleming was the first Potentate.
In 1875, there were only 43 Shriners in the organization. In an effort to spur membership, at the June 6, 1876 meeting of Mecca Temple, the Imperial Grand Council of the Ancient Order of the Nobles of the Mystic Shrine for North America was created. Fleming was elected the first Imperial Potentate. After some other reworking, by 1878 there were 425 members in 13 temples in eight states, and by 1888, there were 7,210 members in 48 temples in the United States and Canada. By the Imperial Session held in Washington, D.C. in 1900, there were 55,000 members and 82 Temples.
By 1938 there were about 340,000 members in the United States. That year "Life" published photographs of its rites for the first time. It described the Shriners as "among secret lodges the No. 1 in prestige, wealth and show", and stated that "n the typical city, especially in the Middle West, the Shriners will include most of the prominent citizens."
Shriners often participate in local parades, sometimes as rather elaborate units: miniature vehicles in themes (all sports cars; all miniature 18-wheeler trucks; all fire engines, and so on), an "Oriental Band" dressed in cartoonish versions of Middle Eastern dress; pipe bands, drummers, motorcycle units, Drum and Bugle Corps, and even traditional brass bands.
Membership.
Despite its theme, the Shrine is not connected to Arab culture or Islam. It is a men's fraternity rather than a religion or religious group. Its only religious requirement is indirect: all Shriners must be Masons (with the exception being in the State of Arkansas), and petitioners to Freemasonry must profess a belief in a Supreme Being. To further minimize confusion with religion, the use of the words "temple" and "mosque" to describe Shriners' buildings has been replaced by "Shrine Center", although individual local chapters are still called temples.
Until 2000, before being eligible for membership in the Shrine, a person had to complete either the Scottish Rite or York Rite degrees of Masonry, but now any Master Mason can join.
Shriners count among their ranks presidents, senators, local business leaders, professional golfers, country music stars, astronauts and racecar drivers.
Women's auxiliaries.
While there are plenty of activities for Shriners and their wives, there are two organizations tied to the Shrine that are for women only: The Ladies' Oriental Shrine and the Daughters of the Nile. They both support the Shriners Hospitals and promote sociability, and membership in either organization is open to any woman 18 years of age and older who is related to a Shriner or Master Mason by birth, marriage, or adoption.
The Ladies Oriental Shrine of North America was founded in Wheeling, West Virginia, in 1903, and the Daughters of the Nile was founded in 1913 in Seattle, Washington. The latter organization has locals called "Temples". There were ten of these in 1922. Among the famous members of the Daughters of the Nile was First Lady Florence Harding, wife of Warren G. Harding.
Architecture.
Some of the earliest Shrine Centers often chose a Moorish Revival style for their Temples. Architecturally notable Shriners Temples include the Shrine Auditorium in Los Angeles, the former Mecca Temple, now called New York City Center and used primarily as a concert hall, Newark Symphony Hall, the Landmark Theater (formerly The Mosque) in Richmond, Virginia, the Tripoli Shrine Temple in Milwaukee, Wisconsin, the Helena Civic Center (Montana) (formerly the Algeria Shrine Temple), Abou Ben Adhem Shrine Mosque in Springfield, Missouri and the Fox Theatre (Atlanta, Georgia) which was jointly built between the Atlanta Shriners and movie mogul William Fox.
Shriners Hospitals for Children.
The Shrine's charitable arm is the Shriners Hospitals for Children, a network of twenty-two hospitals in the United States, Mexico, and Canada. In June 1920, the Imperial Council Session voted to establish a "Shriners Hospital for Crippled Children." The purpose of this hospital was to treat orthopedic injuries, diseases, and birth defects in children.
After much research and debate, the committee chosen to determine the site of the hospital decided there should be not just one hospital but a network of hospitals spread across North America. The first hospital was opened in 1922 in Shreveport, Louisiana, and by the end of the decade 13 more hospitals were in operation. Shriners Hospitals now provide orthopedic care, burn treatment, cleft lip and palate care, and spinal cord injury rehabilitation.
The rules for all of the Shriners Hospitals are simple and to the point: Any child under the age of 18 can be admitted to the hospital if, in the opinion of the doctors, the child can be treated. There is no requirement for religion, race, or relationship to a Shriner.
Until June 2012, all care at Shriners Hospitals was provided without charge to patients and their families. At that time, because the size of their endowment had decreased due to losses in the stock market, Shriners Hospitals started billing patients' insurance companies, but still offered free care to those that didn't have insurance.
In 2008, Shriners Hospitals had a total budget of $826 million. In 2007 they approved 39,454 new patient applications, and attended to the needs of 125,125 patients.
Parade unit.
Most Shrine Temples support several parade units. These units are responsible for promoting a positive Shriner image to the public by participating in local parades. The parade units often include miniature cars powered by lawn mower engines.
Shriners in St. Louis have several parade motor units, including miniature cars styled after 1932 Ford coupes and 1970s-era Jeep CJ models, and a unit of miniature Indianapolis-styled race cars. Some of these are outfitted with high-performance, alcohol-fueled engines. The drivers' skills are demonstrated during parades with high-speed spinouts.
Other events.
The Shriners are committed to community service and have been instrumental in countless public projects throughout their domain.
Shriners host the annual "East-West Shrine Game", a college football all-star game.
The Shriners originally hosted a golf tournament in association with singer/actor Justin Timberlake, titled the "Justin Timberlake Shriners Hospitals for Children Open", a PGA TOUR golf tournament held in Las Vegas, Nevada. The relationship between Timberlake and the Shriners ended in 2012, due to the lack of previously agreed participation on Timberlake's part. In July 2012, The PGA TOUR and Shriners Hospitals for Children announced a five-year title sponsorship extension, carrying the commitment to the Shriners Hospitals for Children Open through 2017. now titled "The Shriners Hospitals for Children Open", It is still held in Las Vegas, Nevada.
Once a year, the fraternity meets for the Imperial Council Session in a major North American city. It is not uncommon for these conventions to have 20,000 participants or more, which generates significant revenue for the local economy.
Many Shrine Centers also hold a yearly "Shrine Circus" as a fundraiser.

</doc>
<doc id="28925" url="https://en.wikipedia.org/wiki?curid=28925" title="Science fiction fandom">
Science fiction fandom

Science fiction fandom or SF fandom (or Sci-Fi fandom) is a community or fandom of people actively interested in science fiction in contact with one another based upon that interest. SF fandom has a life of its own, but not much in the way of formal organization (although clubs such as the Futurians the Los Angeles Science Fantasy Society [1934–present, and the National Fantasy Fan Federation [1941–present] are recognized features of fandom).
Most often called simply "fandom" within the community, it can be viewed as a distinct subculture, with its own literature and jargon; marriages and other relationships among fans are common, as are multi-generation fannish families.
Origins and history.
Science fiction fandom started through the letter column of Hugo Gernsback's fiction magazines. Not only did fans write comments about the stories—they sent their addresses, and Gernsback published them. Soon, fans were writing letters directly to each other, and meeting in person when they lived close together, or when one of them could manage a trip. In New York City, David Lasser, Gernsback's managing editor, nurtured the birth of a small local club called the Scienceers, which held its first meeting in a Harlem apartment on December 11, 1929. Almost all the members were adolescent boys. Around this time a few other small local groups began to spring up in metropolitan areas around the United States, many of them connecting with fellow enthusiasts via the Science Correspondence Club. In May 1930 the first science fiction fan magazine, "The Comet", was produced by the Chicago branch of the Science Correspondence Club under the editorship of Raymond A. Palmer (later a noted, and notorious, sf magazine editor) and Walter Dennis. In January 1932, the New York City circle, which by then included future comic book editors Julius Schwartz and Mort Weisinger, brought out the first issue of their own publication, "The Time Traveller", with Forrest J Ackerman of the embryonic Los Angeles group as a contributing editor.
In 1934, Gernsback established a correspondence club for fans called the Science Fiction League, the first fannish organization. Local groups across the nation could join by filling out an application. A number of clubs came into being around this time. LASFS (the Los Angeles Science Fantasy Society) was founded at this time as a local branch of the SFL, while several competing local branches sprang up in New York City and immediately began feuding among themselves.
In 1935, PSFS (the Philadelphia Science Fiction Society, 1935–present) was formed. The next year, half a dozen fans from NYC came to Philadelphia to meet with the PSFS members, as the first Philadelphia Science Fiction Conference, which some claim as the world's first science fiction convention.
Soon after the fans started to communicate directly with each other came the creation of science fiction fanzines. These amateur publications might or might not discuss science fiction and were generally traded rather than sold. They ranged from the utilitarian or inept to professional-quality printing and editing. In recent years, Usenet newsgroups such as rec.arts.sf.fandom, websites and blogs have somewhat supplanted printed fanzines as an outlet for expression in fandom, though many popular fanzines continue to be published. Science-fiction fans have been among the first users of computers, email, personal computers and the Internet.
Many professional science fiction authors started their interest in science fiction as fans, and some still publish their own fanzines or contribute to those published by others.
A widely regarded (though by no means error-free) history of fandom in the 1930s can be found in Sam Moskowitz's "The Immortal Storm: A History of Science Fiction Fandom" Hyperion Press 1988 ISBN 0-88355-131-4 (original edition The Atlanta Science Fiction Organization Press, Atlanta, Georgia 1954). Moskowitz was himself involved in some of the incidents chronicled and has his own point of view, which has often been criticized.
By country.
Fandom in Sweden.
Fandom in Sweden ("Sverifandom") emerged in the 1950s. The first Swedish science fiction fanzine was started in the early 1950s. The oldest still existing club, Club Cosmos in Gothenburg, was formed in 1954, and the first Swedish science fiction convention, LunCon, was held in Lund in 1956.
Today, there are a number of science fiction clubs in the country, including Skandinavisk Förening för Science Fiction (whose club fanzine, "Science Fiction Forum", was once edited by Stieg Larsson, a board member and one-time chairman thereof), Linköpings Science Fiction-Förening and Sigma Terra Corps. Between one and four science fiction conventions are held each year in Sweden, among them Swecon, the annual national Swedish con. An annual prize is awarded to someone that has contributed to the national fandom by the Alvar Appeltofft Memorial Fund.
Conventions.
Since the late 1930s, SF fans have organized conventions, non-profit gatherings where the fans (some of whom are also professionals in the field) meet to discuss SF and generally enjoy themselves. (A few fannish couples have held their weddings at conventions.) The 1st World Science Fiction Convention or Worldcon was held in conjunction with the 1939 New York World's Fair, and has been held annually since the end of World War II. Worldcon has been the premier convention in fandom for over half a century; it is at this convention that the Hugo Awards are bestowed, and attendance can approach 8,000 or more.
SF writer Cory Doctorow calls science fiction "perhaps the most social of all literary genres", and states, "Science fiction is driven by organized fandom, volunteers who put on hundreds of literary conventions in every corner of the globe, every weekend of the year."
SF conventions can vary from minimalist "relaxacons" with a hundred or so attendees to heavily programmed events with four to six or more simultaneous tracks of programming, such as WisCon and Worldcons.
Commercial shows dealing with SF-related fields are sometimes billed as 'science fiction conventions,' but are operated as for-profit ventures, with an orientation towards passive spectators, rather than actively involved fans, and a tendency to neglect or ignore written SF in favor of television, film, comics, video games, etc. One of the largest of these is the annual Dragon*Con in Atlanta, Georgia with an attendance of more than 20,000 since 2000.
Science fiction societies.
In the United States, many science fiction societies were launched as chapters of the Science Fiction League and, when it faded into history, several of the original League chapters remained viable and were subsequently incorporated as independent organizations. Most notable among the former League chapters which were spun off was the Philadelphia Science Fiction Society, which served as a model for subsequent SF societies formed independent of the League history.
Science fiction societies, more commonly referred to as "clubs" except on the most formal of occasions, form a year-round base of activities for science fiction fans. They are often associated with an SF convention or group of conventions, but maintain a separate existence as cultural institutions within specific geographic regions. Several have purchased property and maintain ongoing collections of SF literature available for research, as in the case of the Los Angeles Science Fantasy Society, the New England Science Fiction Association, and the Baltimore Science Fiction Society. Other SF Societies maintain a more informal existence, meeting at general public facilities or the homes of individual members, such as the Bay Area Science Fiction Association.
Offshoots and subcommunities.
As a community devoted to discussion and exploration of new ideas, fandom has become an incubator for many groups that started out as special interests within fandom, some of which have partially separated into independent intentional communities not directly associated with science fiction. Among these groups are comic book fandom, media fandom, the Society for Creative Anachronism, gaming, and furry fandom, sometimes referred to collectively as "fringe fandoms" with the implication that the original fandom centered on science fiction texts (magazines and later books and fanzines) is the "true" or "core" fandom. Fandom also welcomes and shares interest with other groups including LGBT communities, libertarians, neo-pagans, and space activist groups like the L5 Society, among many others. Some groups exist almost entirely within fandom but are distinct and cohesive subcultures in their own rights, such as filkers, costumers, and convention runners (sometimes called "SMOFs").
Fandom encompasses subsets of fans that are principally interested in a single writer or subgenre, such as Tolkien fandom, and "Star Trek" fandom ("Trekkies"). Even short-lived television series may have dedicated followings, such as the fans of Joss Whedon's "Firefly" television series and movie "Serenity", known as Browncoats.
Participation in science fiction fandom often overlaps with other similar interests, such as fantasy role-playing games, comic books and anime, and in the broadest sense fans of these activities are felt to be part of the greater community of SF fandom.
There are active SF fandoms around the world. Fandom in non-Anglophone countries is based partially on local literature and media, with cons and other elements resembling those of English-speaking fandom, but with distinguishing local features. For example, Finland's national gathering Finncon is funded by the government, while all conventions and fan activities in Japan are heavily influenced by anime and manga.
Fanspeak.
Science fiction and fantasy fandom has its own slang or jargon, sometimes called "fanspeak" (the term has been in use since at least 1962).
Fanspeak is made up of acronyms, blended words, obscure in-jokes, and standard terms used in specific ways. Some terms used in fanspeak have spread to members of the Society for Creative Anachronism ("Scadians"), Renaissance Fair participants ("Rennies"), hacktivists, and internet gaming and chat fans, due to the social and contextual intersection between the communities. Examples of fanspeak used in these broader fannish communities include gafiate, a term meaning to drop out of SF related community activities, with the implication to Get A Life. The word is derived via the acronym for "get away from it all". A related term is fafiate, for "forced away from it all". The implication is that one would really rather still be involved in fandom, but circumstances make it impossible.
Two other acronyms commonly used in the community are FIAWOL (Fandom Is A Way Of Life) and its opposite FIJAGH (Fandom Is Just A Goddamned Hobby) to describe two ways of looking at the place of fandom in one's life.
Science-fiction fans often refer to themselves using the irregular plural "fen": man/men, fan/fen.
In fiction.
As science fiction fans became professional writers, they started slipping the names of their friends into stories. Wilson "Bob" Tucker slipped so many of his fellow fans and authors into his works that doing so is called tuckerization.
The subgenre of "recursive science fiction" has a fan-maintained bibliography at the New England Science Fiction Association's website; some of it is about science fiction fandom, some not.
In Robert Bloch's 1956 short story, "A Way Of Life", science fiction fandom is the only institution to survive a nuclear holocaust and eventually becomes the basis for the reconstitution of civilization. The science fiction novel "Gather in the Hall of the Planets", by K.M. O'Donnell (aka Barry Malzberg), 1971, takes place at a New York City science fiction convention and features broad parodies of many SF fans and authors. A pair of SF novels by Gene DeWeese and Robert "Buck" Coulson, "Now You See It/Him/Them" and "Charles Fort Never Mentioned Wombats" are set at Worldcons; the latter includes an in-character "introduction" by Wilson Tucker (himself a character in the novel) which is a sly self-parody verging on a self-tuckerization.
The 1991 SF novel "Fallen Angels" by Larry Niven, Jerry Pournelle and Michael Flynn constitutes a tribute to SF fandom. The story includes a semi-illegal fictional Minneapolis Worldcon in a post-disaster world where science, and thus fandom, is disparaged. Many of the characters are barely tuckerized fans, mostly from the Greater Los Angeles area.
Mystery writer Sharyn McCrumb's "Bimbos of the Death Sun" and "Zombies of the Gene Pool" are murder mysteries set at a science fiction convention and within the broader culture of fandom respectively. While containing mostly nasty caricatures of fans and fandom, some fans take them with good humor; others consider them vicious and cruel.
In 1994 and 1996, two anthologies of alternate history science fiction involving World Science Fiction Conventions, titled "Alternate Worldcons" and "Again, Alternate Worldcons", edited by Mike Resnick were published.
Fans are slans.
A.E. van Vogt's 1940 novel "Slan" was about a mutant variety of humans who are superior to regular humanity and are therefore hunted down and killed by the normal human population. While the story has nothing to do with fandom, many science fiction fans felt very close to the protagonists, feeling their experience as bright people in a mundane world mirrored that of the mutants; hence, the rallying cry, "Fans Are Slans!"; and the tradition that a building inhabited primarily by fans can be called a slan shack.

</doc>
<doc id="28926" url="https://en.wikipedia.org/wiki?curid=28926" title="Spin">
Spin

Spin or spinning may refer to:

</doc>
<doc id="28927" url="https://en.wikipedia.org/wiki?curid=28927" title="Stellar classification">
Stellar classification

In astronomy, stellar classification is the classification of stars based on their spectral characteristics. Electromagnetic radiation from the star is analyzed by splitting it with a prism or diffraction grating into a spectrum exhibiting the rainbow of colors interspersed with absorption lines. Each line indicates an ion of a certain chemical element, with the line strength indicating the abundance of that ion. The relative abundance of the different ions varies with the temperature of the photosphere. The "spectral class" of a star is a short code summarizing the ionization state, giving an objective measure of the photosphere's temperature and density.
Most stars are currently classified under the Morgan–Keenan (MK) system using the letters "O", "B", "A", "F", "G", "K", and "M", a sequence from the hottest ("O" type) to the coolest ("M" type). Each letter class is then subdivided using a numeric digit with "0" being hottest and "9" being coolest (e.g. A8, A9, F0, F1 form a sequence from hotter to cooler). The sequence has been expanded with classes for other stars and star-like objects that do not fit in the classical system, such as class "D" for white dwarfs and class "C" for carbon stars.
In the MK system, a luminosity class is added to the spectral class using Roman numerals. This is based on the width of certain absorption lines in the star's spectrum, which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class "0" or "Ia+" stars for "hypergiants", class "I" stars for "supergiants", class "II" for bright "giants", class "III" for regular "giants", class "IV" for "sub-giants", class "V" for "main-sequence stars", class "sd" for "sub-dwarfs", and class "D" for "white dwarfs". The full spectral class for the Sun is then G2V, indicating a main-sequence star with a temperature around 5,800K.
Conventional color description.
The conventional color description takes into account only the peak of the stellar spectrum. However, in actuality stars radiate in all parts of the spectrum, and because all spectral colors combined appear white, the actual apparent colors the human eye would observe are lighter than the conventional color descriptions. This means that the simplified assignment of colors of the spectrum can be misleading. There are no green, indigo, or violet stars. And the "brown" dwarfs are not literally brown.
Modern classification.
The modern classification system is known as the Morgan–Keenan (MK) classification. Each star is assigned a "spectral class" from the older Harvard spectral classification and a "luminosity class" using Roman numerals as explained below, forming the star's spectral type.
Harvard spectral classification.
The Harvard classification system is a one-dimensional classification scheme using single letters of the alphabet, optionally with numeric subdivisions, to group stars according to their spectral characteristics. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000 K, whereas more-evolved stars can have temperatures above 100,000 K. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.
The spectral classes O through M, as well as other more specialized classes discussed later, are subdivided by Arabic numerals (0–9), where 0 denotes the hottest stars of a given class. For example, A0 denotes the hottest stars in the A class and A9 denotes the coolest ones. Fractional numbers are allowed; for example, the star Mu Normae is classified as O9.7. The Sun is classified as G2.
The conventional color descriptions are traditional in astronomy, and represent colors relative to the mean color of an A-class star, which is considered to be white. The apparent color descriptions are what the observer would see if trying to describe the stars under a dark sky without aid to the eye, or with binoculars. However, most stars in the sky, except the brightest ones, appear white or bluish white to the unaided eye because they are too dim for color vision to work. Red supergiants are cooler and redder than dwarfs of the same spectral type, and stars with particular spectral features such as carbon stars may be far redder than any black body.
O-, B-, and A-type stars are sometimes called "early type", whereas K and M stars are said to be "late type″. This stems from an early 20th-century model of stellar evolution in which stars were powered by gravitational contraction via the Kelvin–Helmholtz mechanism whereby stars start their lives as very hot "early-type" stars, and then gradually cool down, evolving into "late-type″ stars. This mechanism provided ages of the Sun that were much smaller than what is observed, and was rendered obsolete by the discovery that stars are powered by nuclear fusion.
The fact that the Harvard classification of a star indicated its surface or photospheric temperature (or more precisely, its effective temperature) was not fully understood until after its development, though by the time the first Hertzsprung–Russell diagram was formulated (by 1914), this was generally suspected to be true. In the 1920s, the Indian physicist Meghnad Saha derived a theory of ionization by extending well-known ideas in physical chemistry pertaining to the dissociation of molecules to the ionization of atoms. First he applied it to the solar chromosphere, then to stellar spectra. The Harvard astronomer Cecilia Helena Payne (later to become Cecilia Payne-Gaposchkin) then demonstrated that the OBAFGKM spectral sequence is actually a sequence in temperature. Because the classification sequence predates our understanding that it is a temperature sequence, the placement of a spectrum into a given subtype, such as B3 or A7, depends upon (largely subjective) estimates of the strengths of absorption features in stellar spectra. As a result, these subtypes are not evenly divided into any sort of mathematically representable intervals.
Yerkes spectral classification.
The Yerkes spectral classification, also called the MKK system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory. This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity, which is related to luminosity (whilst the Harvard classification is based on just surface temperature). Later, in 1953, after some revisions of list of standard stars and classification criteria, the scheme was named the Morgan–Keenan classification, or MK (by William Wilson Morgan and Philip C. Keenan's initials), and this system remains the system in modern use today.
Denser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore, differences in the spectrum can be interpreted as "luminosity effects" and a luminosity class can be assigned purely from examination of the spectrum.
A number of different luminosity classes are distinguished
Marginal cases are allowed; for example, a star may be either a supergiant or a bright giant, or may be in between the subgiant and main-sequence classifications. In these cases, two special symbols are used: a slash (/) means that a star is either one class or the other, and a dash (-) means that the star is in between the two classes. For example, a star classified as A3-4III/IV would be in between spectral types A3 and A4, while being either a giant star or a subgiant. Sub-dwarf classes have also been used: VI for sub-dwarfs, stars slightly less luminous than the main sequence; VII and sometimes higher numerals for white dwarf or "hot sub-dwarf" classes.
Spectral peculiarities.
Additional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum.
For example, 59 Cygni is listed as spectral type B1.5Vnne, indicating a spectrum with the general classification B1.5V, as well as very broad absorption lines and certain emission lines.
History.
The reason for the odd arrangement of letters in the Harvard classification is historical, having evolved from the earlier Secchi classes and been progressively modified as understanding improved.
Secchi classes.
During the 1860s and 1870s, pioneering stellar spectroscopist Angelo Secchi created the Secchi classes in order to classify observed spectra. By 1866, he had developed three classes of stellar spectra:
In 1868, he discovered carbon stars, which he put into a distinct group:
In 1877, he added a fifth class:
In the late 1890s, this classification began to be superseded by the Harvard classification, which is discussed in the remainder of this article. The roman numerals used for Secchi classes should not be confused with the completely unrelated roman numerals used for Yerkes luminosity classes.
Draper system.
In the 1880s, the astronomer Edward C. Pickering began to make a survey of stellar spectra at the Harvard College Observatory, using the objective-prism method. A first result of this work was the "Draper Catalogue of Stellar Spectra", published in 1890. Williamina Fleming classified most of the spectra in this catalogue. It used a scheme in which the previously used Secchi classes (I to IV) were divided into more specific classes, given letters from A to N. Also, the letters O, P and Q were used, O for stars whose spectra consisted mainly of bright lines, P for planetary nebulae, and Q for stars not fitting into any other class.
Harvard system.
In 1897, another worker at Harvard, Antonia Maury, placed the Orion subtype of Secchi class I ahead of the remainder of Secchi class I, thus placing the modern type B ahead of the modern type A. She was the first to do so, although she did not use lettered spectral types, but rather a series of twenty-two types numbered from I to XXII.
In 1901, Annie Jump Cannon returned to the lettered types, but dropped all letters except O, B, A, F, G, K, and M, used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so forth. Finally, by 1912, Cannon had changed the types B, A, B5A, F2G, etc. to B0, A0, B5, F2, etc. This is essentially the modern form of the Harvard classification system. A common mnemonic for remembering the spectral type letters is "Oh, Be A Fine Guy/Girl, Kiss Me".
Spectral types.
Class O.
O-type stars are very hot and extremely luminous, with most of their radiated output in the ultraviolet range. These are the rarest of all main-sequence stars. About 1 in 3,000,000 (0.00003%) of the main-sequence stars in the solar neighborhood are O-type stars. Some of the most massive stars lie within this spectral class. O-type stars frequently have complicated surroundings which make measurement of their spectra difficult.
O-type spectra used to be defined by the ratio of the strength of the He II λ4541 relative to that of He I λ4471, where λ is the wavelength, measured in ångströms. Spectral type O7 was defined to be the point at which the two intensities are equal, with the He I line weakening towards earlier types. Type O3 was, by definition, the point at which said line disappears altogether, although it can be seen very faintly with modern technology. Due to this, the modern definition uses the ratio of the nitrogen line N IV λ4058 to N III λλ4634-40-42.
O-type stars have dominant lines of absorption and sometimes emission for He II lines, prominent ionized (Si IV, O III, N III, and C III) and neutral helium lines, strengthening from O5 to O9, and prominent hydrogen Balmer lines, although not as strong as in later types. Because they are so massive, O-type stars have very hot cores and burn through their hydrogen fuel very quickly, so they are the first stars to leave the main sequence.
When the MKK classification scheme was first described in 1943, the only subtypes of class O used were O5 to O9.5. The MKK scheme was extended to O9.7 in 1971 and O4 in 1978, and new classification schemes have subsequently been introduced which add types O2, O3 and O3.5.
Class B.
B-type stars are very luminous and blue. Their spectra have neutral helium, which are most prominent at the B2 subclass, and moderate hydrogen lines. As O- and B-type stars are so energetic, they only live for a relatively short time. Thus, due to the low probability of kinematic interaction during their lifetime, they do not, and are unable to, stray far from the area in which they were formed, apart from runaway stars.
The transition from class O to class B was originally defined to be the point at which the He II λ4541 disappears. However, with today's better equipment, the line is still apparent in the early B-type stars. Today for main-sequence stars, the B-class is instead defined by the intensity of the He I violet spectrum, with the maximum intensity corresponding to class B2. For supergiants, lines of silicon are used instead; the Si IV λ4089 and Si III λ4552 lines are indicative of early B. At mid B, the intensity of the latter relative to that of Si II λλ4128-30 is the defining characteristic, while for late B, it is the intensity of Mg II λ4481 relative to that of He I λ4471
These stars tend to be found in their originating OB associations, which are associated with giant molecular clouds. The Orion OB1 association occupies a large portion of a spiral arm of the Milky Way and contains many of the brighter stars of the constellation Orion. About 1 in 800 (0.125%) of the main-sequence stars in the solar neighborhood are B-type main-sequence objects.
Massive yet non-supergiant entities known as "Be stars" are main-sequence stars that notably have, or had at some time, one or more Balmer lines in emission, with the hydrogen-related electromagnetic radiation series projected out by the stars being of particular interest. Be stars are generally thought to feature unusually strong stellar winds, high surface temperatures, and significant attrition of stellar mass as the objects rotate at a curiously rapid rate. Objects known as possess distinctive neutral or low ionisation emission lines that are considered to have 'forbidden mechanisms', undergoing processes not normally allowed under current understandings of quantum mechanics.
Class A.
A-type stars are among the more common naked eye stars, and are white or bluish-white. They have strong hydrogen lines, at a maximum by A0, and also lines of ionized metals (Fe II, Mg II, Si II) at a maximum at A5. The presence of Ca II lines is notably strengthening by this point. About 1 in 160 (0.625%) of the main-sequence stars in the solar neighborhood are A-type stars.
Class F.
F-type stars have strengthening "H" and "K" lines of Ca II. Neutral metals (Fe I, Cr I) beginning to gain on ionized metal lines by late F. Their spectra are characterized by the weaker hydrogen lines and ionized metals. Their color is white. About 1 in 33 (3.03%) of the main-sequence stars in the solar neighborhood are F-type stars.
Class G.
G-type stars, including the Sun have prominent "H" and "K" lines of Ca II, which are most pronounced at G2. They have even weaker hydrogen lines than F, but along with the ionized metals, they have neutral metals. There is a prominent spike in the G band of CH molecules. Class G main-sequence stars make up about 7.5%, nearly one in thirteen, of the main-sequence stars in the solar neighborhood.
G is host to the "Yellow Evolutionary Void". Supergiant stars often swing between O or B (blue) and K or M (red). While they do this, they do not stay for long in the yellow supergiant G classification as this is an extremely unstable place for a supergiant to be.
Class K.
K-type stars are orangish stars that are slightly cooler than the Sun. They make up about 12%, nearly one in eight, of the main-sequence stars in the solar neighborhood. There are also giant K-type stars, which range from hypergiants like RW Cephei, to giants and supergiants, such as Arcturus, whereas orange dwarfs, like Alpha Centauri B, are main-sequence stars.
They have extremely weak hydrogen lines, if they are present at all, and mostly neutral metals (Mn I, Fe I, Si I). By late K, molecular bands of titanium oxide become present. There is a suggestion that K Spectrum stars may potentially increase the chances of life developing on orbiting planets that are within the habitable zone.
Class M.
Class M stars are by far the most common. About 76% of the main-sequence stars in the Solar neighborhood are class M stars. However, because main-sequence stars of spectral class M have such low luminosities, none are bright enough to be visible to see with the unaided eye unless under exceptional conditions. The brightest known M-class main-sequence star is M0V Lacaille 8760 at magnitude 6.6 (the limiting magnitude for typical naked-eye visibility under good conditions is typically quoted as 6.5) and it is extremely unlikely that any brighter examples will be found.
Although most class M stars are red dwarfs, the class also hosts most giants and some supergiants such as VY Canis Majoris, Antares and Betelgeuse. Furthermore, the late-M group holds hotter brown dwarfs that are above the L spectrum. This is usually in the range of M6.5 to M9.5. The spectrum of a class M star shows lines belonging to oxide molecules, TiO in particular, in the visible and all neutral metals, but absorption lines of hydrogen are usually absent. TiO bands can be strong in class M stars, usually dominating their visible spectrum by about M5. Vanadium monoxide bands become present by late M.
Extended spectral types.
A number of new spectral types have been taken into use from newly discovered types of stars.
Hot blue emission star classes.
Spectra of some very hot and bluish stars exhibit marked emission lines from carbon or nitrogen, or sometimes oxygen.
Class W: Wolf–Rayet.
Class W or WR represents the Wolf–Rayet stars, notable for spectra lacking hydrogen lines. Instead their spectra are dominated by broad emission lines of highly ionized helium, nitrogen, carbon and sometimes oxygen. They are thought to mostly be dying supergiants with their hydrogen layers blown away by stellar winds, thereby directly exposing their hot helium shells. Class W is further divided into subclasses according to the relative strength of nitrogen and carbon emission lines in their spectra (and outer layers).
WR spectra range is listed below:
Although the central stars of most planetary nebulae (CSPNe) show O-type spectra, around 10% are hydrogen-deficient and show WR spectra. These are low-mass stars and to distinguish them from the massive Wolf Rayet stars, their spectra are enclosed in square brackets: e.g. Most of these show [WC spectra, some and very rarely [WN.
The "Slash" stars.
The "slash" stars are O-type stars with WN-like lines in their spectra. The name "slash" comes from the their printed spectral type having a slash in it (e.g. "Of/WNL")
There is a secondary group found with this spectra, a cooler, "intermediate" group designated "Ofpe/WN9". These stars have also been referred to as WN10 or WN11, but that has become less popular with the realisation of the evolutionary difference from other Wolf–Rayet stars. Recent discoveries of even rarer stars have extended the range of slash stars as far as O2-3.5If*/WN5-7, which are even hotter than the original slash stars.
Cool red and brown dwarf classes.
The new spectral types L, T and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visual spectrum.
Brown dwarfs, whose energy comes from gravitational attraction alone, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes; faster the less massive they are—the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to a degeneracy between mass and age for a given effective temperature and luminosity, no unique values can be assigned to a given spectral type.
Class L.
Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.
Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. It may be possible for these L-type supergiants to form through stellar collisions, however. An example of which is V838 Monocerotis while in the height of its luminous red nova eruption.
Class T: methane dwarfs.
Class T dwarfs are cool brown dwarfs with surface temperatures between approximately 550 and 1,300 K. Their emission peaks in the infrared. Methane is prominent in their spectra.
Classes T and L could be more common than all the other classes combined if recent research is accurate. Study of the number of proplyds (protoplanetary discs, clumps of gas in nebulae from which stars and planetary systems are formed) indicates that the number of stars in the galaxy should be several orders of magnitude higher than what we know about. It is theorized that these proplyds are in a race with each other. The first one to form will become a proto-star, which are very violent objects and will disrupt other proplyds in the vicinity, stripping them of their gas. The victim proplyds will then probably go on to become main-sequence stars or brown dwarfs of the L and T classes, which are quite invisible to us. Because brown dwarfs can live so long, these smaller bodies accumulate over time.
Class Y.
Brown dwarfs of spectral class Y are cooler than those of spectral class T and have qualitatively different spectra from them. A total of 17 objects have been placed in class Y as of . Although such dwarfs have been modelled and detected within forty light years by the Wide-field Infrared Survey Explorer (WISE) there is no well-defined spectral sequence yet with prototypes. Nevertheless, several objects have been assigned spectral classes Y0, Y1, and Y2. The spectra of these objects display absorption around 1.55 micrometers. Delorme et al. has suggested that this feature is due to absorption from ammonia and that this should be taken as indicating the T–Y transition, making these objects of type Y0. In fact, this ammonia-absorption feature is the main criterion that has been adopted to define this class. However, this feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.
The brown dwarf with the latest assigned spectral type, WISE 1828+2650, is a >Y2 dwarf with an effective temperature originally estimated around 300 K, the temperature of the human body. Parallax measurements have, however, since shown that its luminosity is inconsistent with it being colder than ~400 K; the likely coolest Y dwarf currently known is WD 0806−661B with approximately 350 K.
The mass range for Y dwarfs is 9–25 Jupiter masses, but for young objects might reach below one Jupiter mass, which means that Y class objects straddle the 13 Jupiter mass deuterium-fusion limit that marks the division between brown dwarfs and planets.
Carbon-related late giant star classes.
Carbon-related stars are stars whose spectra indicate production of carbon by helium triple-alpha fusion. With increased carbon abundance, and some parallel s-process heavy element production, the spectra of these stars become increasingly deviant from the usual late spectral classes G, K and M. The giants among those stars are presumed to produce this carbon themselves, but not too few of this class of stars are believed to be double stars whose odd atmosphere once was transferred from a former carbon star companion that is now a white dwarf.
Class C: carbon stars.
Originally classified as R and N stars, these are also known as 'carbon stars'. These are red giants, near the end of their lives, in which there is an excess of carbon in the atmosphere. The old R and N classes ran parallel to the normal classification system from roughly mid G to late M. These have more recently been remapped into a unified carbon classifier C with N0 starting at roughly C6. Another subset of cool carbon stars are the J-type stars, which are characterized by the strong presence of molecules of 13CN in addition to those of 12CN. A few main-sequence carbon stars are known, but the overwhelming majority of known carbon stars are giants or supergiants. There are several subclasses:
Class S.
Class S stars have zirconium monoxide lines in addition to (or, rarely, instead of) those of titanium monoxide, and are in between the class M stars and the carbon stars. S stars have excess amounts of zirconium and other elements produced by the s-process, and have their carbon and oxygen abundances closer to equal than is the case for M stars. The latter condition results in both carbon and oxygen being locked up almost entirely in carbon monoxide molecules. For stars cool enough for carbon monoxide to form, that molecule tends to "eat up" all of whichever element is less abundant, resulting in "leftover oxygen" (which becomes available to form titanium oxide) in stars of normal composition, "leftover carbon" (which becomes available to form the diatomic carbon molecules) in carbon stars, and "leftover nothing" in the S stars. The relation between these stars and the ordinary M stars indicates a continuum of carbon abundance. Like carbon stars, nearly all known S stars are giants or supergiants.
Classes MS and SC: intermediary carbon-related classes.
In between the M and S classes, border cases are named MS stars. In a similar way, border cases between the S and C-N classes are named SC or CS. The sequence M → MS → S → SC → C-N is believed to be a sequence of increased carbon abundance with age for carbon stars in the asymptotic giant branch.
White dwarf classifications.
The class D (for Degenerate) is the modern classification used for white dwarfs – low-mass stars that are no longer undergoing nuclear fusion and have shrunk to planetary size, slowly cooling down. Class D is further divided into spectral types DA, DB, DC, DO, DQ, DX, and DZ. The letters are not related to the letters used in the classification of other stars, but instead indicate the composition of the white dwarf's visible outer layer or atmosphere.
The white dwarf types are as follows:
The type is followed by a number giving the white dwarf's surface temperature. This number is a rounded form of 50400/"T"eff, where "T"eff is the effective surface temperature, measured in kelvins. Originally, this number was rounded to one of the digits 1 through 9, but more recently fractional values have started to be used, as well as values below 1 and above 9.
Two or more of the type letters may be used to indicate a white dwarf that displays more than one of the spectral features above.
Extended white dwarf spectral types:
A different set of spectral peculiarity symbols are used for white dwarfs than for other types of stars:
Supernova remnants.
These objects are not stars but are stellar remnants. They are much dimmer and if placed on the HR diagram, would be placed further to the lower left-hand corner.

</doc>
<doc id="28928" url="https://en.wikipedia.org/wiki?curid=28928" title="Sinope">
Sinope

Sinope may refer to:

</doc>
<doc id="28929" url="https://en.wikipedia.org/wiki?curid=28929" title="Seven Sisters">
Seven Sisters

Seven Sisters is the common name for the Pleiades, a star cluster named for mythological characters.
Seven Sisters may also refer to:

</doc>
<doc id="28930" url="https://en.wikipedia.org/wiki?curid=28930" title="SN 1987A">
SN 1987A

SN 1987A was a supernova in the outskirts of the Tarantula Nebula in the Large Magellanic Cloud (a nearby dwarf galaxy). It occurred approximately 51.4 kiloparsecs from Earth, approximately 168,000 light-years, close enough that it was visible to the naked eye. It could be seen from the Southern Hemisphere. It was the closest observed supernova since SN 1604, which occurred in the Milky Way itself. The light from the new supernova reached Earth on February 23, 1987. As it was the first supernova discovered in 1987, it was labeled “1987A”. Its brightness peaked in May with an apparent magnitude of about 3 and slowly declined in the following months. It was the first opportunity for modern astronomers to study the development of a supernova in detail, and observations have provided much insight into core-collapse supernovae. Of special importance, SN1987A provided the first chance to confirm by direct observation the radioactive source of the energy for visible light emissions by detection of predicted gamma-ray line radiation from two of its abundant radioactive nuclei, 56Co and 57Co. This proved the radioactive nature of the long-duration post-explosion glow of supernovae.
Discovery.
SN 1987A was discovered by Ian Shelton and Oscar Duhalde at the Las Campanas Observatory in Chile on February 24, 1987, and within the same 24 hours independently by Albert Jones in New Zealand. On March 4–12, 1987, it was observed from space by Astron, the largest ultraviolet space telescope of that time.
Progenitor.
Four days after the event was recorded, the progenitor star was tentatively identified as Sanduleak -69° 202, a blue supergiant.
After the supernova faded, the identification was definitely confirmed by Sanduleak -69° 202 having disappeared. This was an unexpected identification, because at the time a blue supergiant was not considered a possibility for a supernova event in existing models of high mass stellar evolution. Many models of the progenitor have attributed the color to its chemical composition, particularly the low levels of heavy elements, among other factors. There has been some speculation that the star may have merged with a companion star prior to the supernova. However, it is now widely understood that blue supergiants are natural progenitors of supernovae, although there is still speculation that the evolution of such stars requires mass loss involving a binary companion. It is of note that the supernova of the blue giant Sanduleak -69° 202 was about one-tenth as luminous as the average observed type II supernova, which is associated with the denser makeup of the star. Because blue supergiant supernovae are not as bright as those generated by red supergiants, we cannot see them in as large a volume. We would thus not expect to see as many of them, and so they might not be as rare or unusual as previously thought.
Neutrino emissions.
Approximately two to three hours before the visible light from SN 1987A reached Earth, a burst of neutrinos was observed at three separate neutrino observatories. This is likely due to neutrino emission, which occurs simultaneously with core collapse, but preceding the emission of visible light. Transmission of visible light is a slower process that occurs only after the shock wave reaches the stellar surface.
At 07:35 UT, Kamiokande II detected 11 antineutrinos; IMB, 8 antineutrinos; and Baksan, 5 antineutrinos; in a burst lasting less than 13 seconds. Approximately three hours earlier, the Mont Blanc liquid scintillator detected a five-neutrino burst, but this is generally not believed to be associated with SN 1987A.
Although the actual neutrino count was only 24, it was a significant rise from the previously observed background level. This was the first time neutrinos known to be emitted from a supernova had been observed directly, which marked the beginning of neutrino astronomy. The observations were consistent with theoretical supernova models in which 99% of the energy of the collapse is radiated away in the form of neutrinos. The observations are also consistent with the models' estimates of a total neutrino count of 1058 with a total energy of 1046 joules.
The neutrino measurements allowed upper bounds on neutrino mass and charge, as well as the number of flavors of neutrinos and other properties. For example, the data show that within 5% confidence, the rest mass of the electron neutrino is at most 16 eV/c2, 1/30,000 the mass of an electron.
The data suggest that the total number of neutrino flavors is at most 8 but other observations and experiments give tighter estimates. Many of these results have since been confirmed or tightened by other neutrino experiments such as more careful analysis of solar neutrinos and atmospheric neutrinos as well as experiments with artificial neutrino sources.
Missing neutron star.
SN 1987A appears to be a core-collapse supernova, which should result in a neutron star given the size of the original star. The neutrino data indicate that a compact object did form at the star's core. However, since the supernova first became visible, astronomers have been searching for the collapsed core but have not detected it. The Hubble Space Telescope has taken images of the supernova regularly since August 1990, but, so far, the images have shown no evidence of a neutron star. A number of possibilities for the 'missing' neutron star are being considered, although none are clearly favored. The first is that the neutron star is enshrouded in dense dust clouds so that it cannot be seen. Another is that a pulsar was formed, but with either an unusually large or small magnetic field. It is also possible that large amounts of material fell back on the neutron star, so that it further collapsed into a black hole. Neutron stars and black holes often give off light when material falls onto them. If there is a compact object in the supernova remnant, but no material to fall onto it, it would be very dim and could therefore avoid detection. Other scenarios have also been considered, such as if the collapsed core became a quark star.
Light curve.
Much of the light curve, or graph of luminosity as a function of time, after the explosion of a Type II Supernova such as SN 1987A is provided its energy by radioactive decay. Although the luminous emission consists of optical photons, it is the radioactive power absorbed that keeps the remnant hot enough to radiate light. Without radioactive heat it would quickly dim. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half life 6 days) while energy for the later light curve in particular fit very closely with the 77.3 day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power source.
Because the 56Co in SN1987A has today completely decayed, it no longer supports the luminosity of the SN 1987A ejecta. That is currently powered by the radioactive decay of 44Ti with a half life of about 60 years. X-ray lines from 44Ti observed by the INTEGRAL space X-ray telescope showed that the total mass of radioactive 44Ti synthesized during the explosion was .
Observations of the radioactive power from their decays in the 1987A light curve have measured accurate total masses of the 56Ni, 57Ni, and 44Ti created in the explosion, which agree with the masses measured by gamma-ray line space telescopes and provides nucleosynthesis constraints on the computed supernova model.
Interaction with circumstellar material.
The three bright rings around SN 1987A are material from the stellar wind of the progenitor. These rings were ionized by the ultraviolet flash from the supernova explosion, and consequently began emitting in various emission lines. These rings did not "turn on" until several months after the supernova; the turn-on process can be very accurately studied through spectroscopy. The rings are large enough that their angular size can be measured accurately: the inner ring is 0.808 arcseconds in radius. Using the distance light must have traveled to light up the inner ring as the base of a right angle triangle and the angular size as seen from the Earth for the local angle, one can use basic trigonometry to calculate the distance to SN1987A, which is about 168,000 light-years. The material from the explosion is catching up with the material expelled during both its red and blue supergiant phases and heating it, so we observe ring structures about the star.
Around 2001, the expanding (>7000 km/s) supernova ejecta collided with the inner ring. This caused its heating and the generation of x-rays — the x-ray flux from the ring increased by a factor of three between 2001 and 2009. A part of the x-ray radiation, which is absorbed by the dense ejecta close to the center, is responsible for a comparable increase in the optical flux from the supernova remnant in 2001–2009. This increase of the brightness of the remnant reversed the trend observed before 2001, when the optical flux was decreasing due to the decaying of 44Ti isotope.
A study reported in June 2015, using images from the Hubble Space Telescope and the Very Large Telescope taken between 1994 and 2014, shows that the emissions from the clumps of matter making up the rings are fading as the clumps are destroyed by the shock wave. It is predicted the ring will fade away between 2020 and 2030. As the shock wave passes the circumstellar ring it will trace the history of mass loss of the supernova's progenitor and provide useful information for discriminating among various models for the progenitor of SN 1987A.

</doc>
<doc id="28931" url="https://en.wikipedia.org/wiki?curid=28931" title="Standard Oil">
Standard Oil

Standard Oil Co. Inc. was an American oil producing, transporting, refining, and marketing company. Established in 1870 by John D. Rockefeller as a corporation in Ohio, it was the largest oil refiner in the world of its time. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard Oil was an illegal monopoly.
Standard Oil dominated the oil products market initially through horizontal integration in the refining sector, then, in later years vertical integration; the company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers.
John D. Rockefeller was a founder, chairman and major shareholder. With the dissolution of the Standard Oil trust into 33 smaller companies, Rockefeller became the richest man in the world. By 1882, his top aide was John Dustin Archbold. After 1896, Rockefeller disengaged from business to concentrate on his philanthropy, leaving Archbold in control. Other notable Standard Oil principals include Henry Flagler, developer of the Florida East Coast Railway and resort cities, and Henry H. Rogers, who built the Virginian Railway.
Early years.
Standard Oil began as an Ohio partnership formed by the well-known industrialist John D. Rockefeller, his brother William Rockefeller, Henry Flagler, chemist Samuel Andrews, silent partner Stephen V. Harkness, and Oliver Burr Jennings, who had married the sister of William Rockefeller's wife. In 1870 Rockefeller incorporated Standard Oil in Ohio. Of the initial 10,000 shares, John D. Rockefeller received 2,667; Harkness received 1,334; William Rockefeller, Flagler, and Andrews received 1,333 each; Jennings received 1,000; and the firm of Rockefeller, Andrews & Flagler received 1,000. Using highly effective tactics, later widely criticized, it absorbed or destroyed most of its competition in Cleveland in less than two months in 1872 and later throughout the northeastern United States.
In the early years, John D. Rockefeller dominated the combine; he was the single most important figure in shaping the new oil industry. He quickly distributed power and the tasks of policy formation to a system of committees, but always remained the largest shareholder. Authority was centralized in the company's main office in Cleveland, but decisions in the office were made in a cooperative way.
In response to state laws trying to limit the scale of companies, Rockefeller and his associates developed innovative ways of organizing, to effectively manage their fast growing enterprise. On January 2, 1882, they combined their disparate companies, spread across dozens of states, under a single group of trustees. By a secret agreement, the existing thirty-seven stockholders conveyed their shares "in trust" to nine Trustees: John and William Rockefeller, Oliver H. Payne, Charles Pratt, Henry Flagler, John D. Archbold, William G. Warden, Jabez Bostwick, and Benjamin Brewster. This organization proved so successful that other giant enterprises adopted this "trust" form.
The company grew by increasing sales and also through acquisitions. After purchasing competing firms, Rockefeller shut down those he believed to be inefficient and kept the others. In a seminal deal, in 1868, the Lake Shore Railroad, a part of the New York Central, gave Rockefeller's firm a going rate of one cent a gallon or forty-two cents a barrel, an effective 71 percent discount from its listed rates in return for a promise to ship at least 60 carloads of oil daily and to handle the loading and unloading on its own. Smaller companies decried such deals as unfair because they were not producing enough oil to qualify for discounts.
In 1872, Rockefeller joined the South Improvement Co. which would have allowed him to receive rebates for shipping and receive drawbacks on oil his competitors shipped. But when this deal became known, competitors convinced the Pennsylvania Legislature to revoke South Improvement's charter. No oil was ever shipped under this arrangement.
Standard's actions and secret transport deals helped its kerosene price to drop from 58 to 26 cents from 1865 to 1870. Competitors disliked the company's business practices, but consumers liked the lower prices. Standard Oil, being formed well before the discovery of the Spindletop oil field and a demand for oil other than for heat and light, was well placed to control the growth of the oil business. The company was perceived to own and control all aspects of the trade.
In 1885, Standard Oil of Ohio moved its headquarters from Cleveland to its permanent headquarters at 26 Broadway in New York City. Concurrently, the trustees of Standard Oil of Ohio chartered the Standard Oil Co. of New Jersey (SOCNJ) to take advantages of New Jersey's more lenient corporate stock ownership laws.
Also in 1890, Congress passed the Sherman Antitrust Act — a source of American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase "restraint of trade" remained subjective. The Standard Oil group quickly attracted attention from antitrust authorities leading to a lawsuit filed by Ohio Attorney General David K. Watson.
From 1882 to 1906, Standard paid out $548,436,000 in dividends at 65.4 percent payout ratio. The total net earnings from 1882 to 1906 amounted to $838,783,800, exceeding the dividends by $290,347,800, which was used for plant expansions.
1895–1913.
In 1896, John Rockefeller retired from the Standard Oil Co. of New Jersey, the holding company of the group, but remained president and a major shareholder. Vice-president John Dustin Archbold took a large part in the running of the firm. At the same time, state and federal laws sought to counter this development with "antitrust" laws. In 1911, the US Justice Department sued the group under the federal antitrust law and ordered its breakup into 34 companies.
Standard Oil's market position was initially established through an emphasis on efficiency and responsibility. While most companies dumped gasoline in rivers (this was before the automobile was popular), Standard used it to fuel its machines. While other companies' refineries piled mountains of heavy waste, Rockefeller found ways to sell it. For example, Standard created the first synthetic competitor for beeswax and bought the company that invented and produced Vaseline, the Chesebrough Manufacturing Co., which was a Standard company only from 1908 until 1911.
One of the original "Muckrakers" was Ida M. Tarbell, an American author and journalist. Her father was an oil producer whose business had failed due to Rockefeller's business dealings. After extensive interviews with a sympathetic senior executive of Standard Oil, Henry H. Rogers, Tarbell's investigations of Standard Oil fueled growing public attacks on Standard Oil and on monopolies in general. Her work was published in 19 parts in "McClure's" magazine from November 1902 to October 1904, then in 1904 as the book "The History of the Standard Oil Co.".
The Standard Oil Trust was controlled by a small group of families. Rockefeller stated in 1910: "I think it is true that the Pratt family, the Payne-Whitney family (which were one, as all the stock came from Colonel Payne), the Harkness-Flagler family (which came into the company together) and the Rockefeller family controlled a majority of the stock during all the history of the company up to the present time".
These families reinvested most of the dividends in other industries, especially railroads. They also invested heavily in the gas and the electric lighting business (including the giant Consolidated Gas Co. of New York City). They made large purchases of stock in US Steel, Amalgamated Copper, and even Corn Products Refining Co..
British petroleum entrepreneur in Mexico Weetman Pearson, began negotiating with Standard Oil in 1912–13 to sell his "El Aguila" oil company, since Pearson was no longer bound to promises to the Porfirio Díaz regime (1876–1911) to not to sell to U.S. interests. However, the deal fell through and the firm was sold to Royal Dutch Shell.
Standard Oil In China.
Standard Oil's production increased so rapidly it soon exceeded US demand and the company began viewing export markets. In the 1890s, Standard Oil began marketing kerosene to China's large population of close to 400 million as lamp fuel. For its Chinese trademark and brand Standard Oil adopted the name "Mei Foo" (), (which translates to American Trust). Mei Foo also became the name of the tin lamp that Standard Oil produced and gave away or sold cheaply to Chinese farmers, encouraging them to switch from vegetable oil to kerosene. Response was positive, sales boomed and China became Standard Oil's largest market in Asia. Prior to Pearl Harbor, Stanvac was the largest single US investment in SE Asia.
Socony's North China Department operated a subsidiary called Socony River and Coastal Fleet, North Coast Division, which became the North China Division of Stanvac after that company was formed in 1933. To distribute its products, Standard Oil constructed storage tanks, canneries (bulk oil from large ocean tankers was re-packaged into 5-gallon tins), warehouses and offices in key Chinese cities. For inland distribution the company had motor tank trucks and railway tank cars, and for river navigation it had a fleet of low draft steamers and other vessels.
Stanvac's North China Division, based in Shanghai, owned hundreds of river going vessels, including motor barges, steamers, launches, tugboats and tankers. Up to 13 tankers operated on the Yangtze River, the largest of which were "Mei Ping" (1,118 gt), "Mei Hsia" (1,048 gt),and "Mei An" (934 gt). All three were destroyed in the 1937 USS "Panay" incident. "Mei An" was launched in 1901 and was the first vessel in the fleet. Other vessels included "Mei Chuen", "Mei Foo", "Mei Hung", "Mei Kiang", "Mei Lu", "Mei Tan", "Mei Su", "Mei Xia", "Mei Ying", and "Mei Yun". "Mei Hsia", a tanker, was specially designed for river duty and was built by New Engineering and Shipbuilding Works of Shanghai, who also built the 500-ton launch "Mei Foo" in 1912. "Mei Hsia" ("Beautiful Gorges") was launched in 1926 and carried 350 tons of bulk oil in three holds, plus a forward cargo hold and space between decks for carrying general cargo or packed oil. She had a length of , a beam of , depth of and had a bulletproof wheelhouse. "Mei Ping" ("Beautiful Tranquility") launched in 1927, was designed offshore but assembled and finished in Shanghai. Its oil fuel burners came from the U.S. and water tube boilers came from England.
Standard Oil in the Middle East.
Standard Oil Company and Socony-Vacuum Oil Company became partners in providing markets for the oil reserves in the Middle East. In 1906, SOCONY (later Mobil) opened its first fuel terminals in Alexandria. It explored in Palestine before the World War broke out, but ran into conflict with the British government.
Monopoly charges and anti-trust legislation.
By 1890, Standard Oil controlled 88 percent of the refined oil flows in the United States. The state of Ohio successfully sued Standard, compelling the dissolution of the trust in 1892. But Standard simply separated Standard Oil of Ohio and kept control of it. Eventually, the state of New Jersey changed its incorporation laws to allow a company to hold shares in other companies in any state. So, in 1899, the Standard Oil Trust, based at 26 Broadway in New York, was legally reborn as a holding company, the "Standard Oil Co. of New Jersey" (SOCNJ), which held stock in 41 other companies, which controlled other companies, which in turn controlled yet other companies. This conglomerate was seen by the public as all-pervasive, controlled by a select group of directors, and completely unaccountable.
In 1904, Standard controlled 91 percent of production and 85 percent of final sales. Most of its output was kerosene, of which 55 percent was exported around the world. After 1900 it did not try to force competitors out of business by underpricing them. The federal Commissioner of Corporations studied Standard's operations from the period of 1904 to 1906 and concluded that "beyond question... the dominant position of the Standard Oil Co. in the refining industry was due to unfair practices—to abuse of the control of pipe-lines, to railroad discriminations, and to unfair methods of competition in the sale of the refined petroleum products". Due to competition from other firms, their market share had gradually eroded to 70 percent by 1906 which was the year when the antitrust case was filed against Standard, and down to 64 percent by 1911 when Standard was ordered broken up and at least 147 refining companies were competing with Standard including Gulf, Texaco, and Shell. It did not try to monopolize the exploration and pumping of oil (its share in 1911 was 11 percent).
In 1909, the US Department of Justice sued Standard under federal anti-trust law, the Sherman Antitrust Act of 1890, for sustaining a monopoly and restraining interstate commerce by:
"Rebates, preferences, and other discriminatory practices in favor of the combination by railroad companies; restraint and monopolization by control of pipe lines, and unfair practices against competing pipe lines; contracts with competitors in restraint of trade; unfair methods of competition, such as local price cutting at the points where necessary to suppress competition; espionage of the business of competitors, the operation of bogus independent companies, and payment of rebates on oil, with the like intent."
The lawsuit argued that Standard's monopolistic practices had taken place over the preceding four years:
"The general result of the investigation has been to disclose the existence of numerous and flagrant discriminations by the railroads in behalf of the Standard Oil Co. and its affiliated corporations. With comparatively few exceptions, mainly of other large concerns in California, the Standard has been the sole beneficiary of such discriminations. In almost every section of the country that company has been found to enjoy some unfair advantages over its competitors, and some of these discriminations affect enormous areas."
The government identified four illegal patterns: 1) secret and semi-secret railroad rates; (2) discriminations in the open arrangement of rates; (3) discriminations in classification and rules of shipment; (4) discriminations in the treatment of private tank cars. The government alleged:
"Almost everywhere the rates from the shipping points used exclusively, or almost exclusively, by the Standard are relatively lower than the rates from the shipping points of its competitors. Rates have been made low to let the Standard into markets, or they have been made high to keep its competitors out of markets. Trifling differences in distances are made an excuse for large differences in rates favorable to the Standard Oil Co., while large differences in distances are ignored where they are against the Standard. Sometimes connecting roads prorate on oil—that is, make through rates which are lower than the combination of local rates; sometimes they refuse to prorate; but in either case the result of their policy is to favor the Standard Oil Co. Different methods are used in different places and under different conditions, but the net result is that from Maine to California the general arrangement of open rates on petroleum oil is such as to give the Standard an unreasonable advantage over its competitors"
The government said that Standard raised prices to its monopolistic customers but lowered them to hurt competitors, often disguising its illegal actions by using bogus supposedly independent companies it controlled.
"The evidence is, in fact, absolutely conclusive that the Standard Oil Co. charges altogether excessive prices where it meets no competition, and particularly where there is little likelihood of competitors entering the field, and that, on the other hand, where competition is active, it frequently cuts prices to a point which leaves even the Standard little or no profit, and which more often leaves no profit to the competitor, whose costs are ordinarily somewhat higher."
On May 15, 1911, the US Supreme Court upheld the lower court judgment and declared the Standard Oil group to be an "unreasonable" monopoly under the Sherman Antitrust Act, Section II. It ordered Standard to break up into 90 independent companies with different boards of directors, the biggest two of the companies were Standard Oil of New Jersey (which became Exxon) and Standard Oil of New York (which became Mobil).
Standard's president, John D. Rockefeller, had long since retired from any management role. But, as he owned a quarter of the shares of the resultant companies, and those share values mostly doubled, he emerged from the dissolution as the richest man in the world. The dissolution had actually propelled Rockefeller's personal wealth.
Breakup.
By 1911, with public outcry at a climax, the Supreme Court of the United States ruled, in "Standard Oil Co. of New Jersey v. United States", that the Standard Oil Trust must be dissolved under the Sherman Antitrust Act and split into 34 companies. Two of these companies were Jersey Standard ("Standard Oil Co. of New Jersey"), which eventually became Exxon, and Socony ("Standard Oil Co. of New York"), which eventually became Mobil.
Over the next few decades, both companies grew significantly. Jersey Standard, led by Walter C. Teagle, became the largest oil producer in the world. It acquired a 50 percent share in Humble Oil & Refining Co., a Texas oil producer. Socony purchased a 45 percent interest in Magnolia Petroleum Co., a major refiner, marketer and pipeline transporter. In 1931, Socony merged with Vacuum Oil Co., an industry pioneer dating back to 1866, and a growing Standard Oil spin-off in its own right.
In the Asia-Pacific region, Jersey Standard had oil production and refineries in Indonesia but no marketing network. Socony-Vacuum had Asian marketing outlets supplied remotely from California. In 1933, Jersey Standard and Socony-Vacuum merged their interests in the region into a 50–50 joint venture. Standard-Vacuum Oil Co., or "Stanvac", operated in 50 countries, from East Africa to New Zealand, before it was dissolved in 1962.
The original Standard Oil Company corporate entity continues in existence and was the operating entity for Sohio; it is now a subsidiary of BP. Other Standard oil entities include "Standard Oil of Indiana" which became Amoco after other mergers and a name change in the 1980s, and "Standard Oil of California" which became the Chevron Corp.
Legacy and criticism of breakup.
The U.S. Supreme Court ruled in 1911 that antitrust law required Standard Oil to be broken into smaller, independent companies. Among the "baby Standards" that still exist are ExxonMobil and Chevron. Some have speculated that if not for that court ruling, Standard Oil could have possibly been worth more than $1 trillion today. 
Whether the breakup of Standard Oil was beneficial is a matter of some controversy.
Some economists believe that Standard Oil was not a monopoly, and also argue that the intense free market competition resulted in cheaper oil prices and more diverse petroleum products. Critics claimed that success in meeting consumer needs was driving other companies out of the market who were not as successful. An example of this thinking was given in 1890 when Rep. William Mason, arguing in favor of the Sherman Antitrust Act, said: "trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to one cent a barrel, it would not right the wrong done to people of this country by the "trusts" which have destroyed legitimate competition and driven honest men from legitimate business enterprise".
The Sherman Antitrust Act prohibits the restraint of trade. Defenders of Standard Oil insist that the company did not restrain trade; they were simply superior competitors. The federal courts ruled otherwise.
Some economic historians have observed that Standard Oil was in the process of losing its monopoly at the time of its breakup in 1911. Although Standard had 90 percent of American refining capacity in 1880, by 1911 that had shrunk to between 60 and 65 percent, due to the expansion in capacity by competitors. Numerous regional competitors (such as Pure Oil in the East, Texaco and Gulf Oil in the Gulf Coast, Cities Service and Sun in the Midcontinent, Union in California, and Shell overseas) had organized themselves into competitive vertically integrated oil companies, the industry structure pioneered years earlier by Standard itself. In addition, demand for petroleum products was increasing more rapidly than the ability of Standard to expand. The result was that although in 1911 Standard still controlled most production in the older regions of the Appalachian Basin (78 percent share, down from 92 percent in 1880), Lima-Indiana (90 percent, down from 95 percent in 1906), and the Illinois Basin (83 percent, down from 100 percent in 1906), its share was much lower in the rapidly expanding new regions that would dominate U.S. oil production in the 20th century. In 1911 Standard controlled only 44 percent of production in the Midcontinent, 29 percent in California, and 10 percent on the Gulf Coast.
Some analysts argue that the breakup was beneficial to consumers in the long run, and no one has ever proposed that Standard Oil be reassembled in pre-1911 form. ExxonMobil, however, does represent a substantial part of the original company.
Since the breakup of Standard Oil, several companies, such as General Motors and Microsoft, have come under antitrust investigation for being inherently too large for market competition; however, most of them remained together. The only company since the breakup of Standard Oil that was divided into parts like Standard Oil was AT&T, which after decades as a regulated natural monopoly, was forced to divest itself of the Bell System in 1984.
Successor companies.
The successor companies from Standard Oil's breakup form the core of today's US oil industry. (Several of these companies were considered among the Seven Sisters who dominated the industry worldwide for much of the 20th century.) They include:
Other Standard Oil spin-offs:
Other companies divested in the 1911 breakup:
Note: Standard Oil of Colorado was not a successor company; the name was used to capitalize on the Standard Oil brand in the 1930s. Standard Oil of Connecticut is a fuel oil marketer not related to the Rockefeller companies.
Rights to the name.
Of the 34 "Baby Standards", 11 were given rights to the Standard Oil name, based on the state they were in. Conoco and Atlantic elected to use their respective names instead of the Standard name, and their rights would be claimed by other companies.
By the 1980s, most companies were using their individual brand names instead of the Standard name, with Amoco being the last one to have widespread use of the "Standard" name, as it gave Midwestern owners the option of using the Amoco name or Standard.
Three supermajor companies now own the rights to the Standard name in the United States: ExxonMobil, Chevron Corp., and BP. BP acquired its rights through acquiring Standard Oil of Ohio and Amoco, and has a small handful of stations in the Midwestern United States using the Standard name. Likewise, BP continues to sell marine fuel under the Sohio brand at various marinas throughout Ohio. Chevron has one station in each state it owns the rights to branded as Standard except in Kentucky, which it withdrew from in July 2010. ExxonMobil keeps the Esso trademark alive at stations that sell diesel fuel by selling "Esso Diesel" displayed on the pumps. ExxonMobil has full international rights to the Standard name, and continues to use the Esso name overseas and in Canada.

</doc>
<doc id="28935" url="https://en.wikipedia.org/wiki?curid=28935" title="Seismology">
Seismology

Seismology (; from Greek σεισμός "earthquake" and -λογία "study of") is the scientific study of earthquakes and the propagation of elastic waves through the Earth or through other planet-like bodies. The field also includes studies of earthquake environmental effects, such as tsunamis as well as diverse seismic sources such as volcanic, tectonic, oceanic, atmospheric, and artificial processes (such as explosions). A related field that uses geology to infer information regarding past earthquakes is paleoseismology. A recording of earth motion as a function of time is called a seismogram. A seismologist is a scientist who does research in seismology.
History.
Scholarly interest in earthquakes can be traced back to antiquity. Early speculations on the natural causes of earthquakes were included in the writings of Thales of Miletus (c. 585 BCE), Anaximenes of Miletus (c. 550 BCE), Aristotle (c. 340 BCE) and Zhang Heng (132 CE).
In 132 CE, Zhang Heng of China's Han dynasty designed the first known seismoscope.
In 1664, Athanasius Kircher argued that earthquakes were caused by the movement of fire within a system of channels inside the Earth.
In 1703, Martin Lister (1638 to 1712) and Nicolas Lemery (1645 to 1715) proposed that earthquakes were caused by chemical explosions within the earth.
The Lisbon earthquake of 1755, coinciding with the general flowering of science in Europe, set in motion intensified scientific attempts to understand the behaviour and causation of earthquakes. The earliest responses include work by John Bevis (1757) and John Michell (1761). Michell determined that earthquakes originate within the Earth and were waves of movement caused by "shifting masses of rock miles below the surface."
From 1857, Robert Mallet laid the foundation of instrumental seismology and carried out seismological experiments using explosives.
In 1897, Emil Wiechert's theoretical calculations led him to conclude that the Earth's interior consists of a mantle of silicates, surrounding a core of iron.
In 1906 Richard Dixon Oldham identified the separate arrival of P-waves, S-waves and surface waves on seismograms and found the first clear evidence that the Earth has a central core.
In 1910, after studying the 1906 San Francisco earthquake, Harry Fielding Reid put forward the "elastic rebound theory" which remains the foundation for modern tectonic studies. The development of this theory depended on the considerable progress of earlier independent streams of work on the behaviour of elastic materials and in mathematics.
In 1926, Harold Jeffreys was the first to claim, based on his study of earthquake waves, that below the crust, the core of the Earth is liquid.
In 1937, Inge Lehmann determined that within the earth's liquid outer core there is a solid "inner" core.
By the 1960s, earth science had developed to the point where a comprehensive theory of the causation of seismic events had come together in the now well-established theory of plate tectonics.
Types of seismic wave.
Seismic waves are elastic waves that propagate in solid or fluid materials. They can be divided into "body waves" that travel through the interior of the materials; "surface waves" that travel along surfaces or interfaces between materials; and "normal modes", a form of standing wave.
Body waves.
There are two types of body waves, P-waves and S-waves. Pressure waves or Primary waves (P-waves), are longitudinal waves that involve compression and rarefaction (expansion) in the direction that the wave is traveling. P-waves are the fastest waves in solids and are therefore the first waves to appear on a seismogram. S-waves, also called shear or secondary waves, are transverse waves that involve perpendicular motion to the direction of propagation. S-waves are slower than P-waves. Therefore, they appear later than P-waves on a seismogram. Fluids cannot support this perpendicular motion, or shear, so S-waves only travel in solids. P-waves travel in both solids and fluids.
Surface waves.
The two primary types of surface waves are the Rayleigh waves,which has some compressional motion, and the Love wave, which does not. Rayleigh waves can be explained theoretically in terms of interacting P- and S-waves of vertical polarization that are required to satisfy the boundary conditions on the free surface. Love waves can exist in the presence of a subsurface layer, and they are formed by S-waves of horizontal polarization only. Surface waves travel more slowly than P-waves and S-waves; however, because they are guided by the surface of the Earth (and their energy is thus trapped near the Earth's surface) they can be much larger in amplitude than body waves, and can be the largest signals seen in earthquake seismograms. They are particularly strongly excited when their source is close to the surface of the Earth, as in a shallow earthquake or explosion.
Normal modes.
Both body and surface waves are traveling waves; however, large earthquakes can also make the Earth "ring" like a bell. This ringing is a mixture of normal modes with discrete frequencies and periods of an hour or shorter. Motion caused by a large earthquake can be observed for up to a month after the event. The first observations of normal modes were made in the 1960s as the advent of higher fidelity instruments coincided with two of the largest earthquakes of the 20th century - the 1960 Valdivia earthquake and the 1964 Alaska earthquake. Since then, the normal modes of the Earth have given us some of the strongest constraints on the deep structure of the Earth.
Earthquakes.
One of the first attempts at the scientific study of earthquakes followed the 1755 Lisbon earthquake. Other notable earthquakes that spurred major advancements in the science of seismology include the 1857 Basilicata earthquake, 1906 San Francisco earthquake, the 1964 Alaska earthquake, the 2004 Sumatra-Andaman earthquake, and the 2011 Great East Japan earthquake.
Controlled seismic sources.
Seismic waves produced by explosions or vibrating controlled sources are one of the primary methods of underground exploration in geophysics (in addition to many different electromagnetic methods such as induced polarization and magnetotellurics). Controlled-source seismology has been used to map salt domes, anticlines and other geologic traps in petroleum-bearing rocks, faults, rock types, and long-buried giant meteor craters. For example, the Chicxulub Crater, which was caused by an impact that has been implicated in the extinction of the dinosaurs, was localized to Central America by analyzing ejecta in the Cretaceous–Paleogene boundary, and then physically proven to exist using seismic maps from oil exploration.
Detection of seismic waves.
Seismometers are sensors that sense and record the motion of the Earth arising from elastic waves. Seismometers may be deployed at the Earth's surface, in shallow vaults, in boreholes, or underwater. A complete instrument package that records seismic signals is called a seismograph. Networks of seismographs continuously record ground motions around the world to facilitate the monitoring and analysis of global earthquakes and other sources of seismic activity. Rapid location of earthquakes makes tsunami warnings possible because seismic waves travel considerably faster than tsunami waves. Seismometers also record signals from non-earthquake sources ranging from explosions (nuclear and chemical), to local noise from wind or anthropogenic activities, to incessant signals generated at the ocean floor and coasts induced by ocean waves (the global microseism), to cryospheric events associated with large icebergs and glaciers. Above-ocean meteor strikes with energies as high as 4.2 × 1013 J (equivalent to that released by an explosion of ten kilotons of TNT) have been recorded by seismographs, as have a number of industrial accidents and terrorist bombs and events (a field of study referred to as forensic seismology). A major long-term motivation for the global seismographic monitoring has been for the detection and study of nuclear testing.
Mapping the earth's interior.
Because seismic waves commonly propagate efficiently as they interact with the internal structure of the Earth, they provide high-resolution noninvasive methods for studying the planet's interior. One of the earliest important discoveries (suggested by Richard Dixon Oldham in 1906 and definitively shown by Harold Jeffreys in 1926) was that the outer core of the earth is liquid. Since S-waves do not pass through liquids, the liquid core causes a "shadow" on the side of the planet opposite of the earthquake where no direct S-waves are observed. In addition, P-waves travel much slower through the outer core than the mantle.
Processing readings from many seismometers using seismic tomography, seismologists have mapped the mantle of the earth to a resolution of several hundred kilometers. This has enabled scientists to identify convection cells and other large-scale features such as Ultra Low Velocity Zones near the core–mantle boundary.
Seismology and society.
Earthquake prediction.
Forecasting a probable timing, location, magnitude and other important features of a forthcoming seismic event is called earthquake prediction. Various attempts have been made by seismologists and others to create effective systems for precise earthquake predictions, including the VAN method. Most seismologists do not believe that a system to provide timely warnings for individual earthquakes has yet been developed, and many believe that such a system would be unlikely to give useful warning of impending seismic events. However, more general forecasts routinely predict seismic hazard. Such forecasts estimate the probability of an earthquake of a particular size affecting a particular location within a particular time-span, and they are routinely used in earthquake engineering.
Public controversy over earthquake prediction erupted after Italian authorities indicted six seismologists and one government official for manslaughter in connection with a magnitude 6.3 earthquake in L'Aquila, Italy on April 5, 2009. The indictment has been widely perceived as an indictment for failing to predict the earthquake and has drawn condemnation from the American Association for the Advancement of Science and the American Geophysical Union. The indictment claims that, at a special meeting in L'Aquila the week before the earthquake occurred, scientists and officials were more interested in pacifying the population than providing adequate information about earthquake risk and preparedness.
Engineering seismology.
Engineering seismology is the study and application of seismology for engineering purposes. It generally applied to the branch of seismology that deals with the assessment of the seismic hazard of a site or region for the purposes of earthquake engineering. It is, therefore, a link between earth science and civil engineering. There are two principal components of engineering seismology. Firstly, studying earthquake history (e.g. historical and instrumental catalogs of seismicity) and tectonics to assess the earthquakes that could occur in a region and their characteristics and frequency of occurrence. Secondly, studying strong ground motions generated by earthquakes to assess the expected shaking from future earthquakes with similar characteristics. These strong ground motions could either be observations from accelerometers or seismometers or those simulated by computers using various techniques.
Tools.
Seismological instruments can generate large amounts of data. Systems for processing such data include:

</doc>
<doc id="28936" url="https://en.wikipedia.org/wiki?curid=28936" title="Cyanoacrylate">
Cyanoacrylate

Cyanoacrylates are a family of strong fast-acting adhesives with industrial, medical, and household uses. Cyanoacrylate adhesives have a short shelf life if not used, about one year from manufacture if unopened, one month once opened. They have some minor toxicity.
Cyanoacrylates include methyl 2-cyanoacrylate, ethyl-2-cyanoacrylate (commonly sold under trade names such as "Super Glue" and "Krazy Glue"), n-butyl cyanoacrylate and 2-octyl cyanoacrylate (used in medical, veterinary and first aid applications). Octyl cyanoacrylate was developed to address toxicity concerns and to reduce skin irritation and allergic response. Cyanoacrylate adhesives are sometimes known generically as instant glues, power glues or superglues (although "Super Glue" is a trade name). The abbreviation "CA" is commonly used for industrial grades.
Development.
The original patent for cyanoacrylate was filed in 1942 by Goodrich Company. as an outgrowth of a search for materials suitable for clear plastic gun sights for the war effort. In 1942, a team of scientists headed by Harry Coover Jr. stumbled upon a formulation that stuck to everything with which it came in contact. The team quickly rejected the substance for the wartime application, but in 1951, while working as researchers for Eastman Kodak, Coover and a colleague, Fred Joyner, rediscovered cyanoacrylates. The two realized the true commercial potential, and a form of the adhesive was first sold in 1958 under the title "Eastman #910" (later "Eastman 910").
During the 1960s, Eastman Kodak sold cyanoacrylate to Loctite, which in turn repackaged and distributed it under a different brand name "Loctite Quick Set 404". In 1971 Loctite developed its own manufacturing technology and introduced its own line of cyanoacrylate, called "Super Bonder". Loctite quickly gained market share, and by the late 1970s it was believed to have exceeded Eastman Kodak's share in the North American industrial cyanoacrylate market. National Starch and Chemical Company purchased Eastman Kodak’s cyanoacrylate business and combined it with several acquisitions made throughout the 1970s forming Permabond. Other manufacturers of cyanoacrylate include LePage (a Canadian company acquired by Henkel in 1996), the Permabond Division of National Starch and Chemical, Inc., which was a subsidiary of Unilever. Together, Loctite, Eastman and Permabond accounted for approximately 75% of the industrial cyanoacrylate market. Permabond continued to manufacture the original 910 formula.
Properties.
In its liquid form, cyanoacrylate consists of monomers of cyanoacrylate molecules. Methyl-2-cyanoacrylate (CH2=C(CN)COOCH3 or C5H5NO2) has a molecular weight equal to 111.1, a flashpoint of 79 °C, and a density of 1.1 g/ml. Ethyl 2-cyanoacrylate (C6H7NO2) has a molecular weight equal to 125 and a flashpoint of >75 °C. To facilitate easy handling, a cyanoacrylate adhesive is frequently formulated with an ingredient such as fumed silica to make it more viscous or gel-like. More recently, formulations are available with additives to increase shear strength, creating a more impact resistant bond. Such additives may include rubber, as in Loctite's "Ultra Gel", or others which are not specified.
In general, cyanoacrylate is an acrylic resin that rapidly polymerises in the presence of water (specifically hydroxide ions), forming long, strong chains, joining the bonded surfaces together. Because the presence of moisture causes the glue to set, exposure to normal levels of humidity in the air causes a thin skin to start to form within seconds, which very greatly slows the reaction. Because of this cyanoacrylate is applied thinly, to ensure that the reaction proceeds rapidly for bonding.
The reaction with moisture can cause a container of glue which has been opened and resealed to become unusable more quickly than if never opened. To minimise this reduction in shelf life cyanoacrylate, once opened, can be stored in an airtight container with a package of silica gel desiccant. Another technique is to insert a hypodermic needle into the opening of a tube. After using the glue, residual glue soon clogs the needle, keeping moisture out. The clog is removed by heating the needle (e.g. with a lighter) before use.. The polymerisation is also temperature-dependant: storage at zero degrees or below stops it, so keeping it in the freezer is also effective.
Uses.
Behaviours.
Cyanoacrylates are mainly used as adhesives. They require some care and knowledge for effective use: they do not bond some materials; their shelf life at room temperature is about 12 months unopened and one month once opened; they do not fill spaces, unlike epoxies, and a very thin layer bonds more effectively than a thicker one that does not cure properly; they bond many substances, including human skin and tissues. They have an exothermic reaction to natural fibres: cotton, wool, leather, see reaction with cotton below.
Cyanoacrylate glue has a low shearing strength, which has led to its use as a temporary adhesive in cases where the piece needs to be sheared off later. Common examples include mounting a workpiece to a sacrificial glue block on a lathe, and tightening pins and bolts. It's also used in conjunction with another, slower, but more resilient adhesive as way of rapidly forming a joint, which then holds the pieces in the appropriate configuration until the second adhesive has set.
Cyanoacrylate is not resistant to friction if applied to a smooth surface. Using either sugar or sandpaper can remove a good amount of cyanoacrylate from a user's fingertips.
Electronics.
Cyanoacrylates are used to assemble prototype electronics (see wire wrap), flying model aircraft, and as retention dressings for nuts and bolts. Their effectiveness in bonding metal and general versatility have made them popular among modeling and miniatures hobbyists.
Aquaria.
Cyanoacrylate glue's ability to resist water has made it popular with marine aquarium hobbyists for fragging corals. The cut branches of hard corals such as Acropora can be glued to a piece of live rock (harvested reef coral) or Milliput (epoxy putty) to allow the new frag to grow out. It is safe to use directly in the tank, unlike silicone, which must be cured to be safe. However, as a class of adhesives, traditional cyanoacrylates are classified as having "weak" resistance to both moisture and heat although the inclusion of phthalic anhydride reportedly counteracts both of these characteristics.
Bonding smooth surfaces.
Most standard cyanoacrylate adhesives do not bond well with smooth glass, although they can be used as a quick, temporary bond prior to application of an epoxy or cyanoacrylate specifically formulated for use on glass. A mechanical adhesive bond may be formed around glass fibre mat or tissue to reinforce joints or to fabricate small parts.
Filler.
When added to baking soda (sodium bicarbonate), cyanoacrylate glue forms a hard, lightweight adhesive filler (baking soda is first used to fill a gap then the adhesive is dropped onto the baking soda). This works well with porous materials that the glue does not work well with alone. This method is sometimes used by aircraft modelers to assemble or repair polystyrene foam parts. It is also used to repair small nicks in the leading edge of composite propeller blades on light aircraft. The reaction between cyanoacrylate and baking soda is very exothermic (heat-producing) and also produces noxious vapors. See Reaction with cotton below.
Forensics.
Cyanoacrylate is used as a forensic tool to capture latent fingerprints on non-porous surfaces like glass, plastic, etc. Cyanoacrylate is warmed to produce fumes that react with the invisible fingerprint residues and atmospheric moisture to form a white polymer (polycyanoacrylate) on the fingerprint ridges. The ridges can then be recorded. The developed fingerprints are, on most surfaces (except on white plastic or similar), visible to the naked eye. Invisible or poorly visible prints can be further enhanced by applying a luminescent or non-luminescent stain.
Woodworking.
Thin CA glue has application in woodworking. It can be used as a fast-drying, glossy finish. The use of oil (such as boiled linseed oil) may be used to control the rate at which the CA cures. CA glue is also used in combination with sawdust (from a saw or sanding) to fill voids and cracks. These repair methods are used on piano soundboards, wood instruments, and wood furniture.
Medical.
CA glue was in veterinary use for mending bone, hide, and tortoise shell by the early 1970s or before. Harry Coover said in 1966 that a CA spray was used in the Vietnam War to reduce bleeding in wounded soldiers until they could be brought to a hospital. Butyl cyanoacrylate has been used medically since the 1970s. In the US, due to its potential to irritate the skin, the U.S. Food and Drug Administration did not approve its use as a medical adhesive until 1998 with Dermabond. Studies confirm that cyanoacrylate is safer and more functional for wound closure than traditional suturing (stitches). The adhesive is superior in time required to close wounds, incidence of infection (suture canals through the skin's epidermal, dermal, and subcutaneous fat layers introduce extra routes of contamination), and final cosmetic appearance.
Some rock climbers use cyanoacrylate to repair damage to the skin on their fingertips. Glue covered fingertips do not leave fingerprints. Similarly, stringed-instrument players can form protective finger caps (in addition to calluses) with cyanoacrylates. While the glue is not very toxic and wears off quickly with shed skin, applying large quantities of glue and its fumes directly to the skin can cause chemical burns.
While standard "superglue" is 100% ethyl cyanoacrylate, many custom formulations ("e.g.,", 91% ECA, 9% poly(methyl methacrylate), <0.5% hydroquinone, and a small amount of organic sulfonic acid and variations on the compound N-butyl-cyanoacrylate's for medical applications) have come to be used for specific applications.
Archery.
Cyanoacrylate is used in archery to glue fletching to arrow shafts. Some special fletching glues are primarily cyanoacrylate repackaged in special fletching glue kits. Such tubes often have a long thin metal nozzle for improved precision in applying the glue to the base of the fletching and to ensure secure bonding to the arrow shaft.
Cosmetics.
Cyanoacrylate is used in the cosmetology/beauty industry as a "nail glue" for some artificial nail enhancements such as nail tips and nail wraps, and is sometimes mistaken for eye drops causing accidental injury.
Safety issues.
Skin injuries.
CA adhesives may adhere to body parts, and injuries may occur when parts of the skin are torn off. Without force, however, the glue will spontaneously separate from the skin in time (up to four days). Separation can be accelerated by applying vegetable oil near, on, and around the glue. In the case of glued eyelids, a doctor should be consulted.
Toxicity.
The fumes from CA are a vaporized form of the cyanoacrylate monomer that irritate sensitive membranes in the eyes, nose, and throat. They are immediately polymerized by the moisture in the membranes and become inert. These risks can be minimized by using CA in well ventilated areas. About 5% of the population can become sensitized to CA fumes after repeated exposure, resulting in flu-like symptoms. CA may also be a skin irritant, causing an allergic skin reaction. The ACGIH assign a Threshold Limit Value exposure limit of 200 parts per billion. On rare occasions, inhalation may trigger asthma. There is no singular measurement of toxicity for all cyanoacrylate adhesives because of the large number of adhesives that contain various cyanoacrylate formulations.
The United States National Toxicology Program and the United Kingdom Health and Safety Executive have concluded that the use of ethyl cyanoacrylate is safe and that additional study is unnecessary. The compound 2-octyl cyanoacrylate degrades much more slowly due to its longer organic backbone and the adhesive does not reach the threshold of tissue toxicity. Due to the toxicity issues of ethyl cyanoacrylate, the use of 2-octyl cyanoacrylate for sutures is preferred.
Reaction with cotton.
Applying cyanoacrylate to some natural materials such as cotton, leather or wool (cotton swabs, cotton balls, and certain yarns or fabrics) results in a powerful, rapid exothermic reaction. The heat released may cause serious burns, ignite the cotton product, or release irritating white smoke. Material Safety Data Sheets for cyanoacrylate instruct users not to wear cotton or wool clothing, especially cotton gloves, when applying or handling cyanoacrylates.
Solvents and debonders.
Acetone, commonly found in nail polish remover, is a widely available solvent capable of softening cured cyanoacrylate. Other solvents include nitromethane, dimethyl sulfoxide, and methylene chloride. gamma-Butyrolactone may also be used to remove cured cyanoacrylate. Commercial debonders are also available.
Shelf life.
CA adhesives have a short shelf life. Date-stamped containers help to ensure that the adhesive is still viable. One manufacturer supplies the following information and advice: When kept unopened in a cool, dry location such as a refrigerator at a temperature of about 55 °F (13 °C), the shelf life of cyanoacrylate will be extended from about one year from manufacture to at least 15 months. If the adhesive is to be used within six months, it is not necessary to refrigerate it. Cyanoacrylates are moisture-sensitive, and moving from a cool to a hot location will create condensation; after removing from the refrigerator, it is best to let the adhesive reach room temperature before opening. After opening, it should be used within 30 days. Open containers should not be refrigerated. Another manufacturer says that the maximum shelf life of 12 months is obtained for some of their cyanoacrylates if the original containers are stored at . User forums and some manufacturers say that an almost unlimited shelf life is attainable by storing unopened at , the typical temperature of a domestic freezer, and allowing the contents to reach room temperature before use. Rechilling an opened container may cause moisture from the air to condense in the container; however, reports from hobbyists suggest that storing CA in a freezer can preserve opened cyanoacrylate indefinitely.
As cyanoacrylates age, they polymerize, become thicker, and cure more slowly. They can be thinned with a cyanoacrylate of the same chemical composition with lower viscosity. Storing cyanoacrylates below will nearly stop the polymerization process and prevent aging.

</doc>
<doc id="28938" url="https://en.wikipedia.org/wiki?curid=28938" title="Shell script">
Shell script

A shell script is a computer program designed to be run by the Unix shell, a command-line interpreter. The various dialects of shell scripts are considered to be scripting languages.
Typical operations performed by shell scripts include file manipulation, program execution, and printing text. A script which sets up the environment, runs the program, and does any necessary cleanup, logging, &c is called a wrapper.
The term is also used more generally to mean the automated mode of running an operating system shell; in specific operating systems they are called other things such as batch files (MSDos-Win95 stream, OS/2), command procedures (VMS), and shell scripts (Windows NT stream and third-party derivatives like 4NT—article is at cmd.exe), and mainframe operating systems are associated with a number of terms.
The typical Unix/Linux/Posix-compliant installation includes the Korn Shell (ksh) in several possible versions such as ksh88, Korn Shell '93 and others. The oldest shell still in common use is the Bourne shell (sh); Unix systems invariably include also the C Shell (csh), Bourne Again Shell (bash), a remote shell (rsh), a secure shell for SSL telnet connections (ssh), and a shell which is a main component of the Tcl/Tk installation usually called tclsh; wish is a GUI-based Tcl/Tk shell. The C and Tcl shells have syntax quite similar to that of said programming languages, and the Korn shells and Bash are developments of the Bourne shell, which is based on the ALGOL language with elements of a number of others added as well. On the other hand, the various shells plus tools like awk, sed, grep, and BASIC, Lisp, C and so forth contributed to the Perl programming language.
Other shells available on a machine or available for download and/or purchase include ash, msh, ysh, zsh (a particularly common enhanced Korn Shell), the Tenex C Shell (tcsh), a Perl-like shell (psh) and others. Related programmes such as shells based on Python, Ruby, C, Java, Perl, Pascal, Rexx &c in various forms are also widely available. Another somewhat common shell is osh, whose manual page states it "is an enhanced, backward-compatible port of the standard command interpreter from Sixth Edition UNIX."
Windows-Unix interoperability software such as the MKS Toolkit, Cygwin, UWIN, Interix and others make the above shells and Unix programming available on Windows systems all the way down to such things as signals and other inter-process communication, system calls and APIs; the Hamilton C Shell is a Windows shell very similar to the Unix C Shell, and Microsoft distributes Windows Services for UNIX for use with its NT-based operating systems in particular, which have a Posix environmental subsystem.
Capabilities.
Shortcuts.
A shell script can provide a convenient variation of a system command where special environment settings, command options, or post-processing apply automatically, but in a way that allows the new script to still act as a fully normal Unix command.
One example would be to create a version of ls, the command to list files, giving it a shorter command name of l, which would be normally saved in a user's bin directory as /home/"username"/bin/l, and a default set of command options pre-supplied.
Here, the first line (shebang) indicates which interpreter should execute the rest of the script, and the second line makes a listing with options for file format indicators, columns, all files (none omitted), and a size in blocks. The LC_COLLATE=C sets the default collation order to not fold upper and lower case together, not intermix dotfiles with normal filenames as a side effect of ignoring punctuation in the names (dotfiles are usually only shown if an option like -a is used), and the "$@" causes any parameters given to l to pass through as parameters to ls, so that all of the normal options and other syntax known to ls can still be used.
The user could then simply use l for the most commonly used short listing.
Another example of a shell script that could be used as a shortcut would be to print a list of all the files and directories within a given directory.
In this case, the shell script would start with its normal starting line of #!/bin/sh. Following this, the script executes the command clear which clears the terminal of all text before going to the next line. The following line provides the main function of the script. The ls -al command list the files and directories that are in the directory from which the script is being run. The ls command attributes could be changed to reflect the needs of the user.
Note: If an implementation does not have the clear command, try using the clr command instead.
Batch jobs.
Shell scripts allow several commands that would be entered manually at a command-line interface to be executed automatically, and without having to wait for a user to trigger each stage of the sequence. For example, in a directory with three C source code files, rather than manually running the four commands required to build the final program from them, one could instead create a C shell script, here named build and kept in the directory with them, which would compile them automatically:
The script would allow a user to save the file being edited, pause the editor, and then just run ./build to create the updated program, test it, and then return to the editor. Since the 1980s or so, however, scripts of this type have been replaced with utilities like make which are specialized for building programs.
Generalization.
Simple batch jobs are not unusual for isolated tasks, but using shell loops, tests, and variables provides much more flexibility to users. A Bash (Unix shell) script to convert JPEG images to PNG images, where the image names are provided on the command-line—possibly via wildcards—instead of each being listed within the script, can be created with this file, typically saved in a file like /home/"username"/bin/jpg2png
The jpg2png command can then be run on an entire directory full of JPEG images with just /home/"username"/bin/jpg2png *.jpg
Verisimilitude.
A key feature of shell scripts is that the invocation of their interpreters is handled as a core operating system feature. So rather than a user's shell only being able to execute scripts in that shell's language, or a script only having its interpreter directive handled correctly if it was run from a shell (both of which were limitations in the early Bourne shell's handling of scripts), shell scripts are set up and executed by the OS itself. A modern shell script is not just on the same footing as system commands, but rather many system commands are actually shell scripts (or more generally, scripts, since some of them are not interpreted by a shell, but instead by Perl, Python, or some other language). This extends to returning exit codes like other system utilities to indicate success or failure, and allows them to be called as components of larger programs regardless of how those larger tools are implemented.
Like standard system commands, shell scripts classically omit any kind of filename extension unless intended to be read into a running shell through a special mechanism for this purpose (such as sh’s "codice_1", or csh’s source).
Programming.
Many modern shells also supply various features usually found only in more sophisticated general-purpose programming languages, such as control-flow constructs, variables, comments, arrays, subroutine and so on. With these sorts of features available, it is possible to write reasonably sophisticated applications as shell scripts. However, they are still limited by the fact that most shell languages have little or no support for data typing systems, classes, threading, complex math, and other common full language features, and are also generally much slower than compiled code or interpreted languages written with speed as a performance goal.
The standard Unix tools sed and awk provide extra capabilities for shell programming; Perl can also be embedded in shell scripts as can other scripting languages like Tcl. Perl and Tcl come with graphics toolkits as well.
Other scripting languages.
Many powerful scripting languages have been introduced for tasks that are too large or complex to be comfortably handled with ordinary shell scripts, but for which the advantages of a script are desirable and the development overhead of a full-blown, compiled programming language would be disadvantageous. The specifics of what separates scripting languages from high-level programming languages is a frequent source of debate. But generally speaking a scripting language is one which requires an interpreter.
Life cycle.
Shell scripts often serve as an initial stage in software development, and are often subject to conversion later to a different underlying implementation, most commonly being converted to Perl, Python, or C. The interpreter directive allows the implementation detail to be fully hidden inside the script, rather than being exposed as a filename extension, and provides for seamless reimplementation in different languages with no impact on end users.
While files with the ".sh" file extension are usually a shell script of some kind, most shell scripts do not have any filename extension.
Advantages and disadvantages.
Perhaps the biggest advantage of writing a shell script is that the commands and syntax are exactly the same as those directly entered at the command-line. The programmer does not have to switch to a totally different syntax, as they would if the script were written in a different language, or if a compiled language were used.
Often, writing a shell script is much quicker than writing the equivalent code in other programming languages. The many advantages include easy program or file selection, quick start, and interactive debugging. A shell script can be used to provide a sequencing and decision-making linkage around existing programs, and for moderately sized scripts the absence of a compilation step is an advantage. Interpretive running makes it easy to write debugging code into a script and re-run it to detect and fix bugs. Non-expert users can use scripting to tailor the behavior of programs, and shell scripting provides some limited scope for multiprocessing.
On the other hand, shell scripting is prone to costly errors. Inadvertent typing errors such as rm -rf * / (instead of the intended rm -rf */) are folklore in the Unix community; a single extra space converts the command from one that deletes everything in the sub-directories to one which deletes everything—and also tries to delete everything in the root directory. Similar problems can transform cp and mv into dangerous weapons, and misuse of the > redirect can delete the contents of a file. This is made more problematic by the fact that many UNIX commands differ in name by only one letter: cp, cd, dd, df, etc.
Another significant disadvantage is the slow execution speed and the need to launch a new process for almost every shell command executed. When a script's job can be accomplished by setting up a pipeline in which efficient filter commands perform most of the work, the slowdown is mitigated, but a complex script is typically several orders of magnitude slower than a conventional compiled program that performs an equivalent task.
There are also compatibility problems between different platforms. Larry Wall, creator of Perl, famously wrote that "It is easier to port a shell than a shell script."
Similarly, more complex scripts can run into the limitations of the shell scripting language itself; the limits make it difficult to write quality code, and extensions by various shells to ameliorate problems with the original shell language can make problems worse.
Many disadvantages of using some script languages are caused by design flaws within the language syntax or implementation, and are not necessarily imposed by the use of a text-based command-line; there are a number of shells which use other shell programming languages or even full-fledged languages like Scsh (which uses Scheme).
Shell Scripting On Other Operating Systems.
Interoperability software such as Cygwin, the MKS Toolkit, Interix (which is available in the Microsoft Windows Services for UNIX), Hamilton C shell, UWIN (AT&T Unix for Windows) and others allow Unix shell programmes to be run on machines running Windows NT and its successors, with some loss of functionality on the MS-DOS-Windows 95 branch, as well as earlier MKS Toolkit versions for OS/2. At least three DCL implementations for Windows type operating systems—in addition to XLNT, a multiple-use scripting language package which is used with the command shell, Windows Script Host and CGI programming—are available for these systems as well. Mac OS X and subsequent are Unix-like as well.
In addition to the aforementioned tools, some Posix and OS/2 functionality can be used with the corresponding environmental subsystems of the Windows NT operating system series up to Windows 2000 as well. A third, 16-bit subsystem often called the MS-DOS subsystem uses the Command.com provided with these operating systems to run the aforementioned MS-DOS batch files.
The console alternatives 4NT, 4DOS, 4OS2, and the GUI Take Command which add functionality to the Windows NT-style Cmd.exe, MS-DOS/Windows 95 batch files (run by Command.com), OS/2's Cmd.exe, and 4NT respectively are similar to the shells that they enhance and are more integrated with the Windows Script Host, which comes with three pre-installed engines, VBScript, JScript, and VBA and to which numerous third-party engines can be added, with Rexx, Perl, Python, Ruby, and Tcl having pre-defined functions in 4NT and related programmes. PC DOS is quite similar to MS-DOS, whilst DR DOS is more different. Earlier versions of Windows NT are able to run contemporary versions of 4OS2 by the OS/2 subsystem.
Scripting languages are, by definition, able to be extended; for example, a MS-DOS/Windows 95/98 and Windows NT type systems allows for shell/batch programmes to call tools like KixTart, QBasic, various Basic, Rexx, Perl, and Python implementations, the Windows Script Host and its installed engines. On Unix and other Posix-compliant systems, awk and sed are used to extend the string and numeric processing ability of shell scripts. Tcl, Perl, Rexx, and Python have graphics toolkits and can be used to code functions and procedures for shell scripts which pose a speed bottleneck (C, Fortran, assembly language &c are much faster still) and to add functionality not available in the shell language such as sockets and other connectivity functions, heavy-duty text processing, working with numbers if the calling script does not have those abilities, self-writing and self-modifying code, techniques like recursion, direct memory access, various types of sorting and more, which are difficult or impossible in the main script, and so on. Visual Basic for Applications and VBScript can be used to control and communicate with such things as spreadsheets, databases, scriptable programmes of all types, telecommunications software, development tools, graphics tools and other software which can be accessed through the Component Object Model.

</doc>
<doc id="28940" url="https://en.wikipedia.org/wiki?curid=28940" title="Subtitle">
Subtitle

A subtitle can refer to:

</doc>
<doc id="28942" url="https://en.wikipedia.org/wiki?curid=28942" title="Solder">
Solder

Solder (, or in North America ) is a fusible metal alloy used to create a permanent bond between metal workpieces. The word solder comes from the Middle English word "soudur", via Old French "solduree" and "soulder", from the Latin "solidare", meaning "to make solid". In fact, solder must be melted in order to adhere to and connect the pieces together, so a suitable alloy for use as solder will have a lower melting point than the pieces it is intended to join. Whenever possible, the solder should also be resistant to oxidative and corrosive effects that would degrade the joint over time. Solders intended for use in making electrical connections between electronic components also usually have favorable electrical characteristics.
Soft solder typically has a melting point range of , and is commonly used in electronics, plumbing, and sheet metal work. Manual soldering uses a soldering iron or soldering gun. Alloys that melt between are the most commonly used. Soldering performed using alloys with a melting point above is called 'hard soldering', 'silver soldering', or brazing.
In specific proportions, some alloys can become eutectic — that is, their melting point is the same as their freezing point. Non-eutectic alloys have markedly different "solidus" and "liquidus" temperatures, and within that range they exist as a paste of solid particles in a melt of the lower-melting phase. In electrical work, if the joint is disturbed in the pasty state before it has solidified totally, a poor electrical connection may result; use of eutectic solder reduces this problem. The pasty state of a non-eutectic solder can be exploited in plumbing as it allows molding of the solder during cooling, e.g. for ensuring watertight joint of pipes, resulting in a so-called 'wiped joint'.
For electrical and electronics work, solder wire is available in a range of thicknesses for hand-soldering, and with cores containing flux. It is also available as a paste or as a preformed foil shaped to match the workpiece, more suitable for mechanized mass-production. Alloys of lead and tin were universally used in the past, and are still available; they are particularly convenient for hand-soldering. Lead-free solders are somewhat less convenient for hand-soldering due to their generally higher melting points and tendency to dissolve copper wire, but have been increasing in use due to the environmental benefits theorized from avoiding lead-based electronic components.
Plumbers often use bars of solder, much thicker than the wire used for electrical applications. Jewelers often use solder in thin sheets, which they cut into snippets.
Lead solder.
Tin-lead (Sn-Pb) solders, also called soft solders, are commercially available with tin concentrations between 5% and 70% by weight. The greater the tin concentration, the greater the solder’s tensile and shear strengths. Alloys commonly used for electrical soldering are 60/40 Sn-Pb, which melts at , and 63/37 Sn-Pb used principally in electrical/electronic work. 63/37 is a eutectic alloy of these metals, which:
In plumbing, a higher proportion of lead was used, commonly 50/50. This had the advantage of making the alloy solidify more slowly. With the pipes being physically fitted together before soldering, the solder could be wiped over the joint to ensure watertightness. Although lead water pipes were displaced by copper when the significance of lead poisoning began to be fully appreciated, lead solder was still used until the 1980s because it was thought that the amount of lead that could leach into water from the solder was negligible from a properly soldered joint. The electrochemical couple of copper and lead promotes corrosion of the lead and tin. Tin, however, is protected by insoluble oxide. Since even small amounts of lead have been found detrimental to health, lead in plumbing solder was replaced by silver (food grade applications) or antimony, with copper often added, and the proportion of tin was increased (see Lead-free solder.)
The addition of tin—more expensive than lead—improves wetting properties of the alloy; lead itself has poor wetting characteristics. High-tin tin-lead alloys have limited use as the workability range can be provided by a cheaper high-lead alloy.
In electronics, components on printed circuit boards (PCBs) are connected to the printed circuit, and hence to other components, by soldered joints. For miniaturized PCB joints with surface mount components, solder paste has largely replaced solid solder.
Lead-tin solders readily dissolve gold plating and form brittle intermetallics.
60/40 Sn-Pb solder oxidizes on the surface, forming a complex 4-layer structure: tin(IV) oxide on the surface, below it a layer of tin(II) oxide with finely dispersed lead, followed by a layer of tin(II) oxide with finely dispersed tin and lead, and the solder alloy itself underneath.
Lead, and to some degree tin, as used in solder contains small but significant amounts of radioisotope impurities. Radioisotopes undergoing alpha decay are a concern due to their tendency to cause soft errors. Polonium-210 is especially problematic; lead-210 beta decays to bismuth-210 which then beta decays to polonium-210, an intense emitter of alpha particles. Uranium-238 and thorium-232 are other significant contaminants of alloys of lead.
Lead-free solder.
On July 1, 2006 the European Union Waste Electrical and Electronic Equipment Directive (WEEE) and Restriction of Hazardous Substances Directive (RoHS) came into effect prohibiting the inclusion of significant quantities of lead in most consumer electronics produced in the EU. In the US, manufacturers may receive tax benefits by reducing the use of lead-based solder. Lead-free solders in commercial use may contain tin, copper, silver, bismuth, indium, zinc, antimony, and traces of other metals. Most lead-free replacements for conventional 60/40 and 63/37 Sn-Pb solder have melting points from 5 to 20 °C higher, though there are also solders with much lower melting points.
It may be desirable to use minor modification of the solder pots (e.g. titanium liners or impellers) used in wave-soldering, to reduce maintenance cost due to increased tin-scavenging of high-tin solder.
Lead-free solder may be less desirable for critical applications, such as aerospace and medical projects, because its properties are less thoroughly known. "Tin whiskers" were a problem with early electronic solders, and lead was initially added to the alloy in part to eliminate them.
Tin-Silver-Copper (Sn-Ag-Cu) solders are used by two-thirds of Japanese manufacturers for reflow and wave soldering, and by about 75% of companies for hand soldering. The widespread use of this popular lead-free solder alloy family is based on the reduced melting point of the Sn-Ag-Cu ternary eutectic behavior (217 ˚C), which is below the 22/78 Sn-Ag (wt.%) eutectic of 221 °C and the 59/41 Sn-Cu eutectic of 227 °C (recently revised by P. Snugovsky to 53/47 Sn-Cu). The ternary eutectic behavior of Sn-Ag-Cu and its application for electronics assembly was discovered (and patented) by a team of researchers from Ames Laboratory, Iowa State University, and from Sandia National Laboratories-Albuquerque.
Much recent research has focused on selection of 4th element additions to Sn-Ag-Cu to provide compatibility for the reduced cooling rate of solder sphere reflow for assembly of ball grid arrays, e.g., 18/64/14/4 Tin-Silver-Copper-Zinc (Sn-Ag-Cu-Zn) (melting range of 217–220 ˚C) and 18/64/16/2 Tin-Silver-Copper-Manganese (Sn-Ag-Cu-Mn) ( (melting range of 211–215 ˚C).
Tin-based solders readily dissolve gold, forming brittle intermetallics; for Sn-Pb alloys the critical concentration of gold to embrittle the joint is about 4%. Indium-rich solders (usually indium-lead) are more suitable for soldering thicker gold layer as the dissolution rate of gold in indium is much slower. Tin-rich solders also readily dissolve silver; for soldering silver metallization or surfaces, alloys with addition of silvers are suitable; tin-free alloys are also a choice, though their wettability is poorer. If the soldering time is long enough to form the intermetallics, the tin surface of a joint soldered to gold is very dull.
Lead-free solder has a higher Young's modulus than lead-based solder, making it more brittle when deformed. When the PCB on which the electronic components are mounted is subject to bending stress due to warping, the solder joint deteriorates and fractures can appear. This effect is called solder cracking. Another fault is Kirkendall voids which are microscopic cavities in solder. When two different types of metal that are in contact are heated, dispersion occurs (see also Kirkendall effect). Repeated thermal cycling cause the formation of voids which tends to cause solder cracks. Lead-free solder can cause short life cycles of products, as well as planned obsolescence.
Flux-core solder.
Flux is a reducing agent designed to help reduce (return oxidized metals to their metallic state) metal oxides at the points of contact to improve the electrical connection and mechanical strength. The two principal types of flux are acid flux, used for metal mending and plumbing, and rosin flux, used in electronics, where the corrosiveness of acid flux and vapors released when solder is heated would risk damaging delicate circuitry.
Due to concerns over atmospheric pollution and hazardous waste disposal, the electronics industry has been gradually shifting from rosin flux to water-soluble flux, which can be removed with deionized water and detergent, instead of hydrocarbon solvents.
In contrast to using traditional bars or coiled wires of all-metal solder and manually applying flux to the parts being joined, much hand soldering since the mid-20th century has used flux-core solder. This is manufactured as a coiled wire of solder, with one or more continuous bodies of non-acid flux embedded lengthwise inside it. As the solder melts onto the joint, it frees the flux and releases that on it as well.
Hard solder.
Hard solders are used for brazing, and melt at higher temperatures. Alloys of copper with either zinc or silver are the most common.
In silversmithing or jewelry making, special hard solders are used that will pass assay. They contain a high proportion of the metal being soldered and lead is not used in these alloys. These solders vary in hardness, designated as "enameling", "hard", "medium" and "easy". Enameling solder has a high melting point, close to that of the material itself, to prevent the joint desoldering during firing in the enameling process. The remaining solder types are used in decreasing order of hardness during the process of making an item, to prevent a previously soldered seam or joint desoldering while additional sites are soldered. Easy solder is also often used for repair work for the same reason. Flux or rouge is also used to prevent joints from desoldering.
Silver solder is also used in manufacturing to join metal parts that cannot be welded. The alloys used for these purposes contain a high proportion of silver (up to 40%), and may also contain cadmium.
Solder alloys.
Notes on the above table.
Temperature ranges for solidus and liquidus (the boundaries of the mushy state) are listed as solidus/liquidus.
In the Sn-Pb alloys, tensile strength increases with increasing tin content. Indium-tin alloys with high indium content have very low tensile strength.
For soldering semiconductor materials, e.g. die attachment of silicon, germanium and gallium arsenide, it is important that the solder contains no impurities that could cause doping in the wrong direction. For soldering n-type semiconductors, solder may be doped with antimony; indium may be added for soldering p-type semiconductors. Pure tin and pure gold can be used.
Various fusible alloys can be used as solders with very low melting points; examples include Field's metal, Lipowitz's alloy, Wood's metal, and Rose's metal.
Properties.
The thermal conductivity of common solders ranges from 32 to 94 W/(m·K) and the density from 9.25 to 15.00 g/cm3.
Solidifying.
The solidifying behavior depends on the alloy composition. Pure metals solidify at a certain temperature, forming crystals of one phase. Eutectic alloys also solidify at a single temperature, all components precipitating simultaneously in so-called coupled growth. Non-eutectic compositions on cooling start to first precipitate the non-eutectic phase; dendrites when it is a metal, large crystals when it is an intermetallic compound. Such a mixture of solid particles in a molten eutectic is referred to as a mushy state. Even a relatively small proportion of solids in the liquid can dramatically lower its fluidity.
The temperature of total solidification is the solidus of the alloy, the temperature at which all components are molten is the liquidus.
The mushy state is desired where a degree of plasticity is beneficial for creating the joint, allowing filling larger gaps or being wiped over the joint (e.g. when soldering pipes). In hand soldering of electronics it may be detrimental as the joint may appear solidified while it is not yet. Premature handling of such joint then disrupts its internal structure and leads to compromised mechanical integrity.
Alloying element roles.
Different elements serve different roles in the solder alloy:
Impurities in solders.
Impurities usually enter the solder reservoir by dissolving the metals present in the assemblies being soldered. Dissolving of process equipment is not common as the materials are usually chosen to be insoluble in solder.
Intermetallics in solders.
Many different intermetallic compounds are formed during solidifying of solders and during their reactions with the soldered surfaces.
The intermetallics form distinct phases, usually as inclusions in a ductile solid solution matrix, but also can form the matrix itself with metal inclusions or form crystalline matter with different intermetallics. Intermetallics are often hard and brittle. Finely distributed intermetallics in a ductile matrix yield a hard alloy while coarse structure gives a softer alloy. A range of intermetallics often forms between the metal and the solder, with increasing proportion of the metal; e.g. forming a structure of Cu-Cu3Sn-Cu6Sn5-Sn.
Layers of intermetallics can form between the solder and the soldered material. These layers may cause mechanical reliability weakening and brittleness, increased electrical resistance, or electromigration and formation of voids. The gold-tin intermetallics layer is responsible for poor mechanical reliability of tin-soldered gold-plated surfaces where the gold plating did not completely dissolve in the solder.
Gold and palladium readily dissolve in solders. Copper and nickel tend to form intermetallic layers during normal soldering profiles. Indium forms intermetallics as well.
Indium-gold intermetallics are brittle and occupy about 4 times more volume than the original gold. Bonding wires are especially susceptible to indium attack. Such intermetallic growth, together with thermal cycling, can lead to failure of the bonding wires.
Copper plated with nickel and gold is often used. The thin gold layer facilitates good solderability of nickel as it protects the nickel from oxidation; the layer has to be thin enough to rapidly and completely dissolve so bare nickel is exposed to the solder.
Lead-tin solder layers on copper leads can form copper-tin intermetallic layers; the solder alloy is then locally depleted of tin and form a lead-rich layer. The Sn-Cu intermetallics then can get exposed to oxidation, resulting in impaired solderability.
Two processes play a role in a solder joint formation: interaction between the substrate and molten solder, and solid-state growth of intermetallic compounds. The base metal dissolves in the molten solder in an amount depending on its solubility in the solder. The active constituent of the solder reacts with the base metal with a rate dependent on the solubility of the active constituents in the base metal. The solid-state reactions are more complex – the formation of intermetallics can be inhibited by changing the composition of the base metal or the solder alloy, or by using a suitable barrier layer to inhibit diffusion of the metals.
Glass solder.
Glass solders are used to join glasses to other glasses, ceramics, metals, semiconductors, mica, and other materials, in a process called glass frit bonding. The glass solder has to flow and wet the soldered surfaces well below the temperature where deformation or degradation of either of the joined materials or nearby structures (e.g., metallization layers on chips or ceramic substrates) occurs. The usual temperature of achieving flowing and wetting is between 450 and 550 °C.
Two types of glass solders are used: vitreous, and devitrifying. Vitreous solders retain their amorphous structure during remelting, can be reworked repeatedly, and are relatively transparent. Devitrifying solders undergo partial crystallization during solidifying, forming a glass-ceramic, a composite of glassy and crystalline phases. Devitrifying solders usually create a stronger mechanical bond, but are more temperature-sensitive and the seal is more likely to be leaky; due to their polycrystalline structure they tend to be translucent or opaque. Devitrifying solders are frequently "thermosetting", as their melting temperature after recrystallization becomes significantly higher; this allows soldering the parts together at lower temperature than the subsequent bake-out without remelting the joint afterwards. Devitrifying solders frequently contain up to 25% zinc oxide. In production of cathode ray tubes, devitrifying solders based on PbO-B2O3-ZnO are used.
Very low temperature melting glasses, fluid at 200–400 °C, were developed for sealing applications for electronics. They can consist of binary or ternary mixtures of thallium, arsenic and sulfur. Zinc-silicoborate glasses can also be used for passivation of electronics; their coefficient of thermal expansion must match silicon (or the other semiconductors used) and they must not contain alkaline metals as those would migrate to the semiconductor and cause failures.
The bonding between the glass or ceramics and the glass solder can be either covalent, or, more often, van der Waals. The seal can be leak-tight; glass soldering is frequently used in vacuum technology. Glass solders can be also used as sealants; a vitreous enamel coating on iron lowered its permeability to hydrogen 10 times. Glass solders are frequently used for glass-to-metal seals and glass-ceramic-to-metal seals.
Glass solders are available as frit powder with grain size below 60 micrometers. They can be mixed with water or alcohol to form a paste for easy application, or with dissolved nitrocellulose or other suitable binder for adhering to the surfaces until being melted. The eventual binder has to be burned off before melting proceeds, requiring careful firing regime. The solder glass can be also applied from molten state to the area of the future joint during manufacture of the part. Due to their low viscosity in molten state, lead glasses with high PbO content (often 70–85%) are frequently used. The most common compositions are based on lead borates (leaded borate glass or borosilicate glass). Smaller amount of zinc oxide or aluminium oxide can be added for increasing chemical stability. Phosphate glasses can be also employed. Zinc oxide, bismuth trioxide, and copper(II) oxide can be added for influencing the thermal expansion; unlike the alkali oxides, these lower the softening point without increasing of thermal expansion.
Glass solders are frequently used in electronic packaging. CERDIP packagings are an example. Outgassing of water from the glass solder during encapsulation was a cause of high failure rates of early CERDIP integrated circuits. Removal of glass-soldered ceramic covers, e.g., for gaining access to the chip for failure analysis or reverse engineering, is best done by shearing; if this is too risky, the cover is polished away instead.
As the seals can be performed at much lower temperature than with direct joining of glass parts and without use of flame (using a temperature-controlled kiln or oven), glass solders are useful in applications like subminiature vacuum tubes or for joining mica windows to vacuum tubes and instruments (e.g., Geiger tube). Thermal expansion coefficient has to be matched to the materials being joined and often is chosen in between the coefficients of expansion of the materials. In case of having to compromise, subjecting the joint to compression stresses is more desirable than to tensile stresses. The expansion matching is not critical in applications where thin layers are used on small areas, e.g., fireable inks, or where the joint will be subjected to a permanent compression (e.g., by an external steel shell) offsetting the thermally introduced tensile stresses.
Glass solder can be used as an intermediate layer when joining materials (glasses, ceramics) with significantly different coefficient of thermal expansion; such materials cannot be directly joined by diffusion welding. Evacuated glazing windows are made of glass panels soldered together.
A glass solder is used, e.g., for joining together parts of cathode ray tubes and plasma display panels. Newer compositions lowered the usage temperature from 450 to 390 °C by reducing the lead(II) oxide content down from 70%, increasing the zinc oxide content, adding titanium dioxide and bismuth(III) oxide and some other components. The high thermal expansion of such glass can be reduced by a suitable ceramic filler. Lead-free solder glasses with soldering temperature of 450 °C were also developed.
Phosphate glasses with low melting temperature were developed. One of such compositions is phosphorus pentoxide, lead(II) oxide, and zinc oxide, with addition of lithium and some other oxides.
Conductive glass solders can be also prepared.
Preform.
A preform is a pre-made shape of solder specially designed for the application where it is to be used. Many methods are used to manufacture the solder preform, stamping being the most common. The solder preform may include the solder flux needed for the soldering process. This can be an internal flux, inside the solder preform, or external, with the solder preform coated.

</doc>
<doc id="28943" url="https://en.wikipedia.org/wiki?curid=28943" title="Shogun">
Shogun

A was a hereditary military dictator in Japan during the period from 1192 to 1867. In this period, the shoguns were the "de facto" rulers of the country; although nominally they were appointed by the Emperor as a ceremonial formality. The Shogun held almost absolute power over territories through military means, in contrast to the concept of a colonial "governor" in Western culture. Nevertheless, an unusual situation occurred in the Kamakura period (1199-1333) upon the death of the first shogun, whereby the Hōjō clan's hereditary titles of Shikken and Tokuso (1256-1333) monopolized the shogunate, collectively known as the . The shogun during this period met the same fate as the Emperor and was reduced to a figurehead until a coup in 1333, when the shogun was restored to power.
The modern rank of shogun is roughly equivalent to a generalissimo. "Shogun" is the short form of , the individual governing the country at various times in the history of Japan, ending when Tokugawa Yoshinobu relinquished the office to Emperor Meiji in 1867.
A shogun's office or administration is the shogunate, known in Japanese as the , which originally referred to the house of the general and later also suggested a private government under a shogun. The tent symbolized the field commander but also denoted that such an office was meant to be temporary. The shogun's officials were collectively the "bakufu", and were those who carried out the actual duties of administration, while the imperial court retained only nominal authority. In this context, the office of the shogun had a status equivalent to that of a viceroy or governor-general, but in reality shoguns dictated orders to everyone including the reigning Emperor.
Heian period (794–1185).
Originally, the title of "Sei-i Taishōgun" ("Commander-in-Chief of the Expeditionary Force Against the Barbarians") was given to military commanders during the early Heian Period for the duration of military campaigns against the Emishi, who resisted the governance of the Kyoto-based imperial court. Ōtomo no Otomaro was the first "Sei-i Taishōgun". The most famous of these shoguns was Sakanoue no Tamuramaro, who conquered the Emishi in the name of Emperor Kanmu. Eventually, the title was abandoned in the later Heian period after the Ainu had been either subjugated or driven to Hokkaidō.
In the later Heian, one more shogun was appointed. Minamoto no Yoshinaka was named "sei-i taishōgun" during the Genpei War, only to be killed shortly thereafter by Minamoto no Yoshitsune.
Kamakura shogunate (1192–1333).
In the early 11th century, daimyo protected by samurai came to dominate internal Japanese politics. Two of the most powerful families – the Taira and Minamoto – fought for control over the declining imperial court. The Taira family seized control from 1160 to 1185, but was defeated by the Minamoto in the Battle of Dan-no-ura. Minamoto no Yoritomo seized power from the central government and aristocracy and established a feudal system based in Kamakura in which the private military, the samurai, gained some political powers while the Emperor and the aristocracy remained the "de jure" rulers. In 1192, Yoritomo was awarded the title of "Sei-i Taishōgun" by the Emperor and the political system he developed with a succession of shogun as the head became known as a shogunate. Yoritomo's wife's family, the Hōjō, seized power from the Kamakura shoguns. When Yoritomo's sons and heirs were assassinated, the shogun himself became a hereditary figurehead. Real power rested with the Hōjō regents. The Kamakura shogunate lasted for almost 150 years, from 1192 to 1333.
In 1274 and 1281, the Mongol Empire launched invasions against Japan. An attempt by Emperor Go-Daigo to restore imperial rule in the Kenmu restoration in 1331 was unsuccessful, but weakened the shogunate significantly and led to its eventual downfall.
The end of the Kamakura shogunate came when Kamakura fell in 1333, and the Hōjō Regency was destroyed. Two imperial families – the senior Northern Court and the junior Southern Court – had a claim to the throne. The problem was solved with the intercession of the Kamakura Shogunate, who had the two lines alternate. This lasted until 1331, when Emperor Go-Daigo (of the Southern Court) tried to overthrow the shogunate in order to stop the alternation. As a result, Daigo was exiled. Around 1334–1336, Ashikaga Takauji helped Daigo regain his throne.
The fight against the shogunate left the Emperor with too many people claiming a limited supply of land. Takauji turned against the Emperor when the discontent about the distribution of land grew great enough. In 1336 Daigo was banished again, in favor of a new Emperor.
During the Kemmu Restoration, after the fall of the Kamakura shogunate in 1333, another short-lived shogun arose. Prince Moriyoshi (Morinaga), son of Go-Daigo, was awarded the title of "Sei-i Taishōgun". However, Prince Moriyoshi was later put under house arrest and, in 1335, killed by Ashikaga Tadayoshi.
Ashikaga shogunate (1336–1573).
In 1338, Ashikaga Takauji, like Yoritomo, a descendant of the Minamoto princes, was awarded the title of "sei-i taishōgun" and established the Ashikaga shogunate, which lasted until 1573. The Ashikaga had their headquarters in the Muromachi district of Kyoto, and the time during which they ruled is also known as the Muromachi Period.
Tokugawa shogunate (1603–1868).
Tokugawa Ieyasu seized power and established a government at Edo (now known as Tokyo) in 1600. He received the title "sei-i taishōgun" in 1603, after he forged a family tree to show he was of Minamoto descent. The Tokugawa shogunate lasted until 1867, when Tokugawa Yoshinobu resigned as shogun and abdicated his authority to Emperor Meiji.
During the Edo period, effective power rested with the Tokugawa shogun, not the Emperor in Kyoto, even though the former ostensibly owed his position to the latter. The shogun controlled foreign policy, the military, and feudal patronage. The role of the Emperor was ceremonial, similar to the position of the Japanese monarchy after the Second World War.
Legacy.
Today, the head of the Japanese government is the Prime Minister; the usage of the term "shogun" has nevertheless continued in colloquialisms. A retired Prime Minister who still wields considerable power and influence behind the scenes is called a , a sort of modern incarnation of the cloistered rule. Examples of "shadow shoguns" are former Prime Minister Kakuei Tanaka and the politician Ichirō Ozawa.
Shogunate.
The term "bakufu" originally meant the dwelling and household of a shogun, but in time, became a metonym for the system of government of a feudal military dictatorship, exercised in the name of the shogun; this is the broader meaning conveyed by the term "shogunate".
The shogunate system was originally established under the Kamakura shogunate by Minamoto no Yoritomo. Although theoretically, the state (and therefore the Emperor) held ownership of all land in Japan. The system had some feudal elements, with lesser territorial lords pledging their allegiance to greater ones. Samurai were rewarded for their loyalty with agricultural surplus, usually rice, or labor services from peasants. In contrast to European feudal knights, samurai were not landowners. The hierarchy that held this system of government together was reinforced by close ties of loyalty between the daimyo, samurai and their subordinates.
Each shogunate was dynamic, not static. Power was constantly shifting and authority was often ambiguous. The study of the ebbs and flows in this complex history continues to occupy the attention of scholars. Each shogunate encountered competition. Sources of competition included the Emperor and the court aristocracy, the remnants of the imperial governmental systems, the "shōen" system, the great temples and shrines, the "shugo" and the "jitō", the "kokujin" and early modern daimyo. Each shogunate reflected the necessity of new ways of balancing the changing requirements of central and regional authorities.

</doc>
<doc id="28944" url="https://en.wikipedia.org/wiki?curid=28944" title="Short-term memory">
Short-term memory

Short-term memory (or "primary" or "active memory") is the capacity for holding, but not manipulating, a small amount of information in mind in an active, readily available state for a short period of time. The duration of short-term memory (when rehearsal or active maintenance is prevented) is believed to be in the order of seconds. A commonly cited capacity is "The Magical Number Seven, Plus or Minus Two" (which is frequently referred to as "Miller's Law".) In contrast, long-term memory can hold an indefinite amount of information.
Short-term memory should be distinguished from working memory, which refers to structures and processes used for temporarily storing and manipulating information (see details below).
Existence of a separate store.
The idea of the division of memory into short-term and long-term dates back to the 19th century. A classical model of memory developed in the 1960s assumed that all memories pass from a short-term to a long-term store after a small period of time. This model is referred to as the "modal model" and has been most famously detailed by Shiffrin. The exact mechanisms by which this transfer takes place, whether all or only some memories are retained permanently, and indeed the existence of a genuine distinction between the two stores, remain controversial topics among experts.
One form of evidence, cited in favor of the separate existence of a short-term store comes from anterograde amnesia, the inability to learn new facts and episodes. Patients with this form of amnesia, have intact ability to retain small amounts of information over short time scales (up to 30 seconds) but are dramatically impaired in their ability to form longer-term memories (a famous example is patient HM). This is interpreted as showing that the short-term store is spared from amnesia and other brain diseases.
Other evidence comes from experimental studies showing that some manipulations (e.g., a distractor task, such as repeatedly subtracting a single-digit number from a larger number following learning; cf Brown-Peterson procedure) impair memory for the 3 to 5 most recently learned words of a list (it is presumed, still held in short-term memory), while leaving recall for words from earlier in the list (it is presumed, stored in long-term memory) unaffected; other manipulations (e.g., semantic similarity of the words) affect only memory for earlier list words, but do not affect memory for the last few words in a list. These results show that different factors affect short-term recall (disruption of rehearsal) and long-term recall (semantic similarity). Together, these findings show that long-term memory and short-term memory can vary independently of each other.
Not all researchers agree that short-term and long-term memory are separate systems. Some theorists propose that memory is unitary over all time scales, from milliseconds to years. Support for the unitary memory hypothesis comes from the fact that it has been difficult to demarcate a clear boundary between short-term and long-term memory. For instance, Tarnow shows that the recall probability vs. latency curve is a straight line from 6 to 600 seconds (ten minutes), with the probability of failure to recall only saturating after 600 seconds. If there were really two different memory stores operating in this time frame, one could expect a discontinuity in this curve. Other research has shown that the detailed pattern of recall errors looks remarkably similar for recall of a list immediately after learning (it is presumed, from short-term memory) and recall after 24 hours (necessarily from long-term memory).
Further evidence against the existence of a short-term memory store comes from experiments involving continual distractor tasks. In 1974, Robert Bjork and William B. Whitten presented subjects with word pairs to be remembered; however, before and after each word pair, subjects had to do a simple multiplication task for 12 seconds. After the final word-pair, subjects had to do the multiplication distractor task for 20 seconds. In their results, Bjork and Whitten found that the recency effect (the increased probability of recall of the last items studied) and the primacy effect (the increased probability of recall of the first few items) still remained. These results would seem inconsistent with the idea of short-term memory as the distractor items would have taken the place of some of the word-pairs in the buffer, thereby weakening the associated strength of the items in long-term memory. Bjork and Whitten hypothesized that these results could be attributed to the memory processes at work for long-term memory retrieval versus short-term memory retrieval.
Ovid J.L. Tzeng (1973) also found an instance where the recency effect in free recall did not seem to result from the function of a short-term memory store. Subjects were presented with four study-test periods of 10 word lists, with a continual distractor task (20-second period of counting-backward). At the end of each list, participants had to free recall as many words from the list as possible. After free-recall of the fourth list, participants were asked to free recall items from all four lists. Both the initial free recall and the final free recall showed a recency effect. These results went against the predictions of a short-term memory model, where no recency effect would be expected in either initial or final free recall.
Koppenaal and Glanzer (1990) attempted to explain these phenomena as a result of the subjects’ adaptation to the distractor task, which therefore allowed them to preserve at least some of the functions of the short-term memory store. As evidence, they provided the results of their experiment, in which the long-term recency effect disappeared when the distractor after the last item differed from the distractors that preceded and followed all the other items (e.g., arithmetic distractor task and word reading distractor task).
Thapar and Greene challenged this theory. In one of their experiments, participants were given a different distractor task after every item to be studied. According to Koppenaal’s and Glanzer’s theory, there should be no recency effect as subjects would not have had time to adapt to the distractor; yet such a recency effect remained in place in the experiment.
One proposed explanation of the existence of the recency effect in a continual distractor condition, and the disappearance of it in an end-only distractor task is the influence of contextual and distinctive processes. According to this model, recency is a result of the final items’ processing context being similar to the processing context of the other items and the distinctive position of the final items versus items in the middle of the list. In the end distractor task, the processing context of the final items is no longer similar to the processing context of the other list items. At the same time, retrieval cues for these items are no longer as effective as without the distractor. Therefore, the recency effect recedes or vanishes. However, when distractor tasks are placed before and after each item, the recency effect returns, because all the list items once again have similar processing context.
Biological basis.
Synaptic Theory of Short-term memory.
Various researchers have proposed that stimuli are coded in short-term memory using transmitter depletion. According to this hypothesis, a stimulus activates a spatial pattern of activity across neurons in a brain region. As these neurons fire, the available neurotransmitters in their store are depleted and this pattern of depletion is iconic, representing stimulus information and functions as a memory trace. The memory trace decays over time as a consequence of neurotransmitter reuptake mechanisms that restore neurotransmitters to the levels that existed prior to stimulus presentation.
Relationship with working memory.
The relationship between short-term memory and working memory is described differently by various theories, but it is generally acknowledged that the two concepts are distinct. Working memory is a theoretical framework that refers to structures and processes used for temporarily storing and manipulating information. As such, working memory might also be referred to as "working attention". Working memory and attention together play a major role in the processes of thinking. Short-term memory in general refers, in a theory-neutral manner, to the short-term storage of information, and it does not entail the manipulation or organization of material held in memory. Thus, while there are short-term memory components to working memory models, the concept of short-term memory is distinct from these more hypothetical concepts. Within Baddeley's influential 1986 model of working memory there are two short-term storage mechanisms: the phonological loop and the visuospatial sketchpad. Most of the research referred to here involves the phonological loop, because most of the work done on short-term memory has used verbal material. Since the 1990s, however, there has been a surge in research on visual short-term memory, and also increasing work on spatial short-term memory.
Duration of short-term memory.
The limited duration of short-term memory (~18 seconds without a form of memory rehearsal) quickly suggests that its contents spontaneously decay over time. The decay assumption is part of many theories of short-term memory, the most notable one being Baddeley's model of working memory. The decay assumption is usually paired with the idea of rapid covert rehearsal: In order to overcome the limitation of short-term memory, and retain information for longer, information must be periodically repeated or rehearsed — either by articulating it out loud or by mentally simulating such articulation. In this way, the information will re-enter the short-term store and be retained for a further period.
Several researchers, however, dispute that spontaneous decay plays any significant role in forgetting over the short-term, and the evidence is far from conclusive.
Authors doubting that decay causes forgetting from short-term memory often offer as an alternative some form of interference: When several elements (such as digits, words, or pictures) are held in short-term memory simultaneously, their representations compete with each other for recall, or degrade each other. Thereby, new content gradually pushes out older content, unless the older content is actively protected against interference by rehearsal or by directing attention to it.
Capacity of short-term memory.
Whatever the cause or causes of forgetting over the short-term may be, there is consensus that it severely limits the amount of new information that we can retain over brief periods of time. This limit is referred to as the finite capacity of short-term memory. The capacity of short-term memory is often called memory span, in reference to a common procedure of measuring it. In a memory span test, the experimenter presents lists of items (e.g. digits or words) of increasing length. An individual's span is determined as the longest list length that he or she can recall correctly in the given order on at least half of all trials.
In an early and highly influential article, The Magical Number Seven, Plus or Minus Two, the psychologist George Miller suggested that human short-term memory has a forward memory span of approximately seven items plus or minus two and that that was well known at the time (it seems to go back to the 19th-century researcher Wundt). More recent research has shown that this "magical number seven" is roughly accurate for college students recalling lists of digits, but memory span varies widely with populations tested and with material used. For example, the ability to recall words in order depends on a number of characteristics of these words: fewer words can be recalled when the words have longer spoken duration; this is known as the "word-length effect", or when their speech sounds are similar to each other; this is called the "phonological similarity effect". More words can be recalled when the words are highly familiar or occur frequently in the language. Recall performance is also better when all of the words in a list are taken from a single semantic category (such as games) than when the words are taken from same categories. A more up-to-date estimate of short-term memory capacity is about four pieces or "chunks" of information. However other prominent theories of short-term memory capacity argue against measuring capacity in terms of a fixed number of elements.
Rehearsal.
Rehearsal is the process where information is kept in short-term memory by mentally repeating it. When the information is repeated each time, that information is reentered into the short-term memory, thus keeping that information for another 10 to 20 seconds (the average storage time for short-term memory).
Chunking.
Chunking is the process by which one can expand his/her ability to remember things in the short term. Chunking is also a process by which a person organizes material into meaningful groups. Although the average person may retain only about four different units in short-term memory, chunking can greatly increase a person's recall capacity. For example, in recalling a phone number, the person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering phone numbers is far more effective than attempting to remember a string of 10 digits.
Practice and the usage of existing information in long-term memory can lead to additional improvements in one's ability to use chunking. In one testing session, an American cross-country runner was able to recall a string of 79 digits after hearing them only once by chunking them into different running times (e.g., the first four numbers were 1518, a three-mile time.)
Factors affecting short-term memory.
It is very difficult to demonstrate the exact capacity of short-term memory (STM) because it will vary depending on the nature of the material to be recalled. There is currently no way of defining the basic unit of information to be stored in the STM store. It is also possible that STM is not the store described by Atkinson and Shiffrin. In that case, the task of defining the task of STM becomes even more difficult.
However, capacity of STM can be affected by the following:
Influence of long-term memory, Reading aloud, Pronunciation time and Individual differences.
Diseases that cause neurodegeneration, such as Alzheimer's Disease can also be a factor in a person's short-term and eventually long-term memory. Damage to certain sections of the brain due to this disease causes a shrinkage in the cerebral cortex which disables the ability to think and recall memories.
Conditions that may impact short-term memory.
Memory loss is a natural process in aging. One study investigated whether or not there were deficits in short-term memory in older adults. This was a previous study which compiled normative French data for three short-term memory tasks (Verbal, visual and spatial). They found impairments present in participants between the ages of 55 and 85 years of age.
Alzheimer’s disease.
Memory distortion in Alzheimer’s disease is a very common disorder found in older adults. Performance of patients with mild to moderate Alzheimer’s disease was compared with the performance of age matched healthy adults. Researchers concluded the study with findings that showed reduced short-term memory recall for Alzheimer’s patients. Episodic memory and semantic abilities deteriorate early in Alzheimer’s disease. Since the cognitive system includes interconnected and reciprocally influenced neuronal networks, one study hypothesized that stimulation of lexical-semantic abilities may benefit semantically structured episodic memory. They found that with Lexical-Semantic stimulation treatment may improve episodic memory in Alzheimer’s Disease patients. It could also be regarded as a clinical option to counteract the cognitive decline typical of the disease
Aphasia.
Aphasias are also seen in many elder adults. Aphasias are responsible for many sentence comprehension deficits. Many language-impaired patients make several complaints about short-term memory deficits, with several family members confirming that patients have trouble recalling previously known names and events. The opinion is supported by many studies showing that many aphasics also have trouble with visual-memory required tasks.
Schizophrenia.
Core symptoms of Schizophrenia patients have been linked to cognitive deficits. One neglected factor that contributes to those deficits is the comprehension of time. In this study, results confirm that cognitive dysfunctions are a major deficit in patients with schizophrenia. The study provided evidence that patients with schizophrenia process temporal information inefficiently.
Advanced age.
Advanced age is associated with decrements in episodic memory. The associative deficit is in which age differences in recognition memory reflect difficulty in binding components of a memory episode and bound units. A previous study used mixed and blocked test designs to examine deficits in short-term memory of older adults and found there was an associative deficit for older adults. This study along with many other previous studies, continue to build evidence of deficits found in older adults short-term memory.
Even when neurological diseases and disorders are not present, there is a progressive and gradual loss of some intellectual functions that become evident in later years. There are several tests used to examine the psychophysical characteristics of the elderly and of them, a well suitable test would be the functional reach (FR) test, and the mini–mental state examination (MMSE). The FR test is an index of the aptitude to maintain balance in an upright position and the MMSE test is a global index of cognitive abilities. These tests were both used by to evaluate the psychophysical characteristics of older adults. They found a loss of physical performance (FR, related to height) as well as a loss of cognitive abilities (MMSE).
Post Traumatic stress disorder.
Post Traumatic stress disorder (PTSD) is associated with altered processing of emotional material with a strong attentional bias toward trauma-related information and interferes with cognitive processing. Aside from trauma processing specificities, a wide range of cognitive impairments have been related to PTSD state with predominant attention and verbal memory deficits.
Short-term memory and intelligence.
There have been few studies done on the relationship between short-term memory and intelligence in PTSD. However, examined whether people with PTSD had equivalent levels of short-term, non-verbal memory on the Benton Visual Retention Test (BVRT), and whether they had equivalent levels of intelligence on the Raven standard Progressive Matrices (RSPM). They found that people with PTSD had worse short-term, non-verbal memory on the BVRT, despite having comparable levels of intelligence on the RSPM, concluding impairments in memory influence intelligence assessments in the subjects.

</doc>
<doc id="28945" url="https://en.wikipedia.org/wiki?curid=28945" title="State supreme court">
State supreme court

In the United States, a state supreme court (known by other names in some states) is the ultimate judicial tribunal in the court system of a particular state ("i.e.", that state's court of last resort).
Generally, the state supreme court, like most appellate tribunals, is exclusively for hearing appeals of legal issues. It does not make any finding of facts, and thus holds no trials. In the case where the trial court made an egregious error in its finding of facts, the state supreme court will remand to the trial court for a new trial. This responsibility of correcting the errors of inferior courts is the origin of a number of the different names for supreme courts in various state court systems.
The court consists of a panel of judges selected by methods outlined in the state constitution. State supreme courts are completely distinct from any United States federal courts located within the geographical boundaries of a state's territory, or the federal United States Supreme Court (although appeals, on some issues, from judgments of a state's highest court can be sought in the U.S. Supreme Court).
Appellate jurisdiction.
Under American federalism, the interpretation of a state supreme court on a matter of state law is normally final and binding and must be accepted in both state and federal courts.
Federal courts may overrule a state court only when there is a federal question, which is to say, a specific issue (such as consistency with the Federal Constitution) that gives rise to federal jurisdiction. Federal appellate review of state supreme court rulings on such matters may be sought by way of a petition for writ of "certiorari" to the Supreme Court of the United States. As the U.S. Supreme Court recognized in "Erie Railroad Co. v. Tompkins" (1938), no part of the federal Constitution actually grants federal courts or the federal Congress the power to directly dictate the content of state law (as distinguished from creating altogether separate federal law that in a particular situation may override state law). Clause 1 of Section 2 of Article Three of the United States Constitution describes the scope of federal judicial power, but only extended it to "the Laws of the United States" and not the laws of the several or individual states. It is this silence on that latter issue that gave rise to the American distinction between state and federal common law not found in other English-speaking common law federations like Australia and Canada.
One of the informal traditions of the American legal system, derived from the common law, is that all litigants are guaranteed at least one appeal after a final judgment on the merits. However, appeal is merely a "privilege" provided by statute in 47 states and in federal judicial proceedings; the U.S. Supreme Court has repeatedly ruled that there is no federal constitutional "right" to an appeal.
Since a few states lack intermediate appellate courts, the state supreme court may operate under "mandatory review", in which it "must" hear all appeals from the trial courts. This is the case, for example, in Nevada. Such judicial systems are usually very congested.
Most state supreme courts have implemented "discretionary review," like their federal counterpart. Under such a system, intermediate appellate courts are entrusted with deciding the vast majority of appeals. Intermediate appellate courts generally focus on the mundane task of what appellate specialists call "error correction," which means their primary task is to decide whether the record reflects that the trial court correctly applied existing law. 
For certain limited categories of cases, the state supreme court still operates under mandatory review, usually with regard to cases involving the interpretation of the state constitution or capital punishment. But for the vast majority, the state supreme court possesses the discretion to grant "certiorari" (known as "review" in states that discourage the use of Latin). These cases usually pertain to issues which different appellate courts within its jurisdiction have decided differently, or highly controversial cases involving a completely new legal issue never seen in that state. In other words, once the state supreme court is able to offload the tedious burden of error correction to intermediate courts, it can then focus on the long-term task (i.e., a policymaking role) of developing a coherent body of case law for the people of its state. 
Iowa and Oklahoma have a unique procedure for appeals. In those states, "all" appeals are filed with the appropriate Supreme Court (Iowa has a single Supreme Court, while Oklahoma has separate civil and criminal Supreme Courts) which then keeps all cases of first impression for itself to decide. It forwards the remaining caseswhich deal with points of law it has already addressedto the intermediate Court of Appeals.
Notably, the Supreme Court of Virginia operates under discretionary review for nearly all cases, but the intermediate Court of Appeals of Virginia hears appeals as a matter of right only in family and administrative cases. The result is that there is "no" first appeal of right for the vast majority of civil and criminal cases in that state. Appellants are still free to petition for review, of course, but such petitions are subject to severe length constraints (6,125 words or 35 pages in Virginia) and necessarily are more narrowly targeted than a long opening appellate brief to an intermediate appellate court (by way of contrast, an opening brief to a California intermediate appellate court can run up to 14,000 words). In turn, the vast majority of decisions of Virginia circuit courts in civil and criminal cases are thereby insulated from appellate review on the merits. 
New Hampshire and West Virginia formerly also provided only discretionary review for nearly all cases even though they had no intermediate appellate court. Both states gradually recognized that even if this arrangement did not offend the federal Constitution, it was unduly harsh for hapless appellants, and transitioned to mandatory review, respectively, in 2004 and 2010.
Influence of the federal Supreme Court on the state supreme courts.
As noted above, the U.S. Supreme Court may hear appeals from state supreme courts "only" if there is a question of law under the United States Constitution (which includes issues arising from federal treaties, statutes, or regulations), and those appeals are heard at the Court's sole discretion (that is, only if the Court grants a petition for writ of certiorari). 
In theory, state supreme courts are bound by the precedent established by the U.S. Supreme Court as to all issues of federal law, but in practice, the Supreme Court reviews very few decisions from state courts. For example, in 2007 the Court reviewed 244 cases appealed from federal courts and only 22 from state courts. Despite the relatively small number of decisions reviewed, Professors Sara Benesh and Wendy Martinek found that state supreme courts follow precedent more closely than federal courts in the area of search and seizure and appear to follow precedent in confessions as well.
Location.
Traditionally, state supreme courts are headquartered in the capital cities of their respective states, though they may occasionally hold oral argument elsewhere. The seven main exceptions are:
As for the court's actual facilities, a state supreme court may be housed in the state capitol, in a nearby state office building shared with other courts or state executive branch agencies, or in a small courthouse reserved for its exclusive use. State supreme courts normally require a courtroom for oral argument, private chambers for all justices, a conference room, offices for law clerks and other support staff, a law library, and a lobby with a window where the court clerk can accept filings and release new decisions in the form of "slip opinions" (that is, in looseleaf format held together only by a staple).
Terminology.
Court of Appeals.
Because state supreme courts generally hear only appeals, some courts have names which directly indicate their functionin the states of New York and Maryland, and in the District of Columbia, the highest court is called the "Court of Appeals". In New York, the "Supreme Court" is the trial court of general unlimited jurisdiction and the intermediate appellate court is called the "Supreme Court—Appellate Division". Maryland's jury trial courts are called "Circuit Courts" (non-jury trials are usually conducted by the "District Courts," whose decisions may be appealed to the Circuit Courts), and the intermediate appellate court is called the "Court of Special Appeals". West Virginia mixes the two; its highest court is called the "Supreme Court of Appeals".
Other states' supreme courts have used the term "Appeals": New Jersey's supreme courts under the 1844 constitution and Delaware's supreme court were both the "Court of Errors and Appeals"; The term "Errors" refers to the now-obsolete writ of error, which was used by state supreme courts to correct certain types of egregious errors committed by lower courts.
Older terminology.
Massachusetts and New Hampshire originally named their highest courts the "Superior Court of Judicature." Currently, Massachusetts uses the names "Supreme Judicial Court" (to distinguish itself from the state legislature, which is called the Massachusetts General Court), while New Hampshire uses the name "Supreme Court". Additionally the highest court in Maine is named the "Supreme Judicial Court". This similar terminology is probably a holdover from the time when Maine was part of Massachusetts. In Connecticut, Delaware, New Jersey, and New York, the highest courts formerly used variations of the term "Court of Errors," which indicated that the court's primary purpose was to correct the errors of lower courts.
Dual supreme courts.
Oklahoma and Texas have two separate supreme courts: one for criminal appeals and one for civil cases. In both states, the first is formally called the Court of Criminal Appeals, and the second is called the Supreme Court.
Statistics.
Texas and Oklahoma have dual supreme courts. In Texas, both have nine justices. In Oklahoma the Supreme Court has nine justices and the Court of criminal appeals has five (assimilated to nine in the above table).
Selection.
Judges are either appointed, selected through a merit process (with an election thereafter in some cases), or elected. The elections may be through partisan or non-partisan elections. A non-partisan election does not mean that the judges run and are selected with no regard to political beliefs. In many cases "non-partisan election" merely means the prospective judges' parties are not printed on the ballot. Political contributions to these campaigns may be allowed, including from trade associations such as the U.S. Chamber of Commerce.

</doc>
<doc id="28946" url="https://en.wikipedia.org/wiki?curid=28946" title="Stability">
Stability

Stability may refer to:

</doc>
