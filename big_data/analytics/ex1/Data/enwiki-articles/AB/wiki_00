<doc id="6655" url="https://en.wikipedia.org/wiki?curid=6655" title="Coldcut">
Coldcut

Coldcut are an English electronic music duo composed of Matt Black and Jonathan More. Credited as pioneers for pop sampling in the ‘80s, Coldcut are also considered the first stars of UK electronic dance music due to their innovative style, which featured cut-up samples of hip-hop, breaks, jazz, spoken word and various other types of music, as well as video and multimedia. According to "Spin", "in ’87 Coldcut pioneered the British fad for ‘DJ records’".
Coldcut's records first introduced the public to pop artists Yazz and Lisa Stansfield, through which these artists achieved pop chart success. In addition, Coldcut has remixed and created productions on tracks by the likes of Eric B & Rakim, James Brown, Queen Latifah, Eurythmics, INXS, Steve Reich, Blondie, The Fall, Pierre Henry, Nina Simone, Fog, Red Snapper, and BBC Radiophonic Workshop.
Beyond their work as a production duo, Coldcut are the founders of Ninja Tune, an independent record label in London, England (with a satellite office in Montreal) with an overall emphasis on encouraging interactive technology and finding innovative uses of software. The label’s first releases (the first four volumes of DJ Food - 'Jazz Brakes') were produced by Coldcut in the early 90s, and composed of instrumental hip-hop cuts that led the duo to help pioneer the trip hop genre, with artists such as Funky Porcini, The Herbaliser and DJ Vadim.
History.
1980s.
In 1986, computer programmer Matt Black and ex-art teacher Jonathan More were part-time DJs on the rare groove scene. More also DJed on pirate radio, hosting the "Meltdown Show" on Kiss FM and worked at the Reckless Records store on Berwick Street, London where Black visited as a customer. The first collaboration between the two artists was 'Say Kids What Time Is It?' on a white label in January 1987, which mixed Jungle Book's "King of the Swingers" with the break from James Brown's "Funky Drummer." The innovation of "Say Kids..." caused More and Black to be heralded by SPIN as "the first Brit artists to really get hip-hop’s class-cutup aesthetic". It’s regarded as the UK’s first breaks record, the first UK record to be built entirely of samples and "the final link in the chain connecting European collage-experiment with the dance-remix-scratch edit". This was later sampled in "Pump Up the Volume" by MARRS, a single that reached #1 in the UK in October 1987.
Though Black had joined Kiss FM with his own mix-based show, the pair eventually joined forces on its own show later in 1987 called "Solid Steel". The eclectic show became a unifying force in underground experimental electronic music and was still running, celebrating 25 years in 2013.
The duo adopted the name "Coldcut" and set up a record label called Ahead Of Our Time to release the single "Beats + Pieces" (one of the formats also included "That Greedy Beat") in 1987. All of these tracks were assembled using cassette pause button edits and later spliced tape edits that would sometimes run "all over the room". The duo used sampling from Led Zeppelin to James Brown. Electronic act The Chemical Brothers have described "Beats + Pieces" as the ‘first bigbeat record’, a style which appeared in the mid-90s.
Coldcut's first mainstream success came when Julian Palmer from Island Records asked them to remix Eric B. & Rakim's "Paid in Full". Released in October 1987, the landmark remix is said to have "laid the groundwork for hip hop’s entry into the UK mainstream", becoming a breakthrough hit for Eric B & Rakim outside the U.S., reaching #15 in the UK and the top 20 in a number of European countries. It featured a prominent Ofra Haza sample and many other vocal cut ups as well as a looped rhythm which later, when speeded up, proved popular in the Breakbeat genre. Off the back of its success in clubs, the Coldcut "Seven Minutes of Madness" remix ended up being promoted as the single in the UK.
In 1988, More and Black formed Hex, a self-titled "multimedia pop group," with Mile Visman and Rob Pepperell. While working on videos for artists such as Kevin Saunderson, Queen Latifah and Spiritualized, Hex’s collaborative work went on to incorporate 3D modelling, punk video art, and algorithmic visuals on desktop machines. The video for Coldcut’s ‘Christmas Break’ in 1989 is arguably one of the first pop promos produced entirely on microcomputers.
In 1988, Coldcut released "Out To Lunch With Ahead Of Our Time", a double LP of Coldcut productions and re-cuts, and the various aliases under which the duo had recorded. This continued the duo’s tradition of releasing limited available vinyl.
The next Coldcut single, released in February 1988, moved towards a more house-influenced style. "Doctorin' the House", which debuted singer Yazz, became a top ten hit, and peaked at #6. In the same year, under the guise Yazz and the Plastic Population, they produced "The Only Way Is Up", a cover of a Northern Soul song. The record reached #1 in the UK in August, and remained there for five weeks, becoming 1988’s second biggest selling single. Producer Youth of Killing Joke also helped Coldcut with this record. The duo had another top hit in September with "Stop This Crazy Thing", which featured reggae vocalist Junior Reid and reached number 21 in the UK.
The single "People Hold On" became another UK Top 20 hit. Released in March 1989, it helped launch the career of the then relatively unknown singer Lisa Stansfield. Coldcut and Mark Saunders produced her debut solo single "This Is the Right Time", which became another UK Top 20 hit in August as well as reaching #21 on the U.S. "Billboard" Hot 100 the following year.
As the duo started to enjoy critical and commercial success, their debut album "What's That Noise?" was released in April 1989 on Ahead of Our Time and distributed by Big Life Records. The album gave "breaks the full length treatment", and showcased "their heady blend of hip-hop production aesthetics and proto-acid house grooves". It also rounded up a heap of unconventional guest features, quoted by SPIN as having "somehow found room at the same table for Queen Latifah and Mark E. Smith". The album’s track "I'm in Deep" (featuring Smith) prefigured the indie-dance guitar-breaks crossover of such bands as the Stone Roses and Happy Mondays, utilizing Smith’s freestyle raucous vocals over an acid house backing, and also including psych guitar samples from British rock band Deep Purple. "What’s That Noise?" reached the Top 20 in the UK and was certified Silver.
1990s.
Coldcut's second album, "Some Like It Cold", released in 1990 on Ahead Of Our Time, featured a collaboration with Queen Latifah on the single "Find a Way". Though "Find a Way" was a minor hit in the UK, no more singles were released from the album. The duo was given the BPI "Producer of the Year Award" in 1990. Hex - alongside some other London visual experimenters such as iE - produced a series of videos for a longform VHS version of the album. This continued Coldcut and Hex’s pioneering of the use of microcomputers to synthesize electronic music visuals.
After their success with Lisa Stansfield, Coldcut signed with her label, Arista Conflicts arose with the major label, as Coldcut’s "vision extended beyond the formulae of house and techno" and mainstream pop culture (CITATION: The Virgin Encyclopedia Of Nineties Music, 2000). Eventually, the duo’s album Philosophy emerged in 1993. Singles "Dreamer" and "Autumn Leaves" (1994) were both minor hits but the album did not chart.
"Autumn Leaves" had strings recorded at Abbey Road, with a 30-piece string section and an arrangement by film composer Ed Shearmur. The leader of the string section was Simon Jeffes of Penguin Cafe Orchestra. Coldcut’s insistence on their friend Mixmaster Morris to remix "Autumn Leaves" led to one of Morris’ most celebrated remixes, which became a minor legend in ambient music. It has appeared on numerous compilations.
In 1990, whilst on their first tour in Japan (which also featured Norman Cook, who later became Fatboy Slim), Matt and Jon formed their second record label, Ninja Tune, as a self-titled ’technocoloured escape pod,’ and a way to escape the creative control of major labels. The label enabled them to release music under different aliases (e.g. Bogus Order, DJ Food), which also helped them to avoid pigeonholing as producers. Ninja Tune’s first release was Bogus Order’s ‘Zen Brakes.’ The name Coldcut stayed with Arista so there were no official Coldcut releases for the next three years.
During this time, Coldcut still produced for artists on their new label, releasing a flood of material under different names and continuing to work with young groups. They additionally kept on with "Solid Steel" on Kiss FM and running the night club Stealth (Club of the Year in the NME, "The Face", and "Mixmag" in 1996).
In 1991, Hex released their first video game, ‘Top Banana’, which was included on a Hex release for the Commodore CDTV machine in 1992, arguably the first complete purpose-designed multimedia system. ‘Top Banana’ was innovative in that it used sampled graphics, contained an ecological theme and a female lead character (dubbed ‘KT’), and its music changed through random processes. Coldcut and Hex presented this multimedia project as an example of the forthcoming convergence of pop music and computer-game characters.
In 1992, Hex’s first single - ‘Global Chaos’ / ‘Digital Love Opus 1’ - combined rave visuals with techno and ambient interactive visuals.
In November of that year, Hex released Global Chaos CDTV, which took advantage of the possibilities of the new CD-ROM medium. The Global Chaos CDTV disk (which contained the ‘Top Banana’ game, interactive visuals and audio), was a forerunner of the "CD+" concept, uniting music, graphics, and video games into one. This multi-dimensional entertainment product received wide coverage in the national media, including features on Dance Energy, Kaleidoscope on BBC Radio 4, "What's Up Doc?" on ITV and "Reportage" on BBC 2. "i-D Magazine" was quoted as saying, "It's like your TV tripping".
Coldcut videos were made for most songs, often by Hexstatic, and used a lot of stock and sampled footage. Their ‘Timber’ video, which created an AV collage piece using analogous techniques to audio sample collage, was put on heavy rotation on MTV. Stuart Warren Hill of Hexstatic referred to this technique as: "What you see is what you hear." ‘Timber’ (which appears on both ‘Let Us Play’, Coldcut’s fourth album, and ‘Let Us Replay,’ their fifth) won awards for its innovative use of repetitive video clips synced to the music, including being shortlisted at the Edinburgh Television and Film Festival in their top five music videos of the year in 1998.
Coldcut began integrating video sampling into their live DJ gigs at the time, and incorporated multimedia content that caused press to credit the act as segueing "into the computer age". Throughout the 90s, Hex created visuals for Coldcut’s live performances, and developed the CD-ROM portion of Coldcut’s ‘Let Us Play’ and ‘Let Us Replay,’ in addition to software developed specifically for the album’s world tour. Hex’s inclusion of music videos and ‘playtools’ (playful art/music software programs) on Coldcut’s CD-Roms was completely ahead of the curve at that time, offering viewers/listeners a high level of interactivity. Playtools such as My Little Funkit and Playtime were the prototypes for Ninja Jamm, the app Coldcut designed and launched 16 years later. Playtime followed on from Coldcut and Hex’s Synopticon installation, developing the auto-cutup algorhythm, and using other random processes to generate surprising combinations. Coldcut and Hex performed live using Playtime at the 1st Sonar Festival in 1994. Playtime was also used to generate the backing track for Coldcut’s collaboration with Jello Biafra, ‘Every Home a Prison’.
In 1994 Coldcut and Hex contributed an installation to the Glasgow Gallery of Modern Art. The piece, called 'Generator' was installed in the Fire Gallery. Generator was an interactive installation which allowed users to mix sound, video, text and graphics and make their own audio-visual mix, modelled on the techniques and technology used by Coldcut in clubs and live performance events. It consisted of two consoles: the left controlling how the sounds are played, the right controlling how the images are played.
As part of the JAM exhibition of "Style, Music and Media" at the Barbican Art Gallery in 1996, Coldcut and Hex were commissioned to produce an interactive audiovisual piece called Synopticon. Conceived and designed by Robert Pepperell and Matt Black, the digital culture synthesiser allows users to "remix" sounds, images, text and music in a partially random, partially controlled way.
The year 1996 also brought the Coldcut name back to More and Black, and the pair celebrated with ‘70 Minutes of Madness,’ a mix CD that became part of the Journeys by DJ series. The release was credited with "bringing to wider attention the sort of freestyle mixing the pair were always known for through their radio show on KISS FM, Solid Steel, and their steady club dates". It was voted "Best Compilation of All Time" by Jockey Slut in 1998.
In February 1997, they released a double pack single "Atomic Moog 2000" / "Boot the System", the first Coldcut release on Ninja Tune. This was not eligible for the UK chart because time and format restrictions prevented the inclusion of the ‘Natural Rhythm’ video on the CD. In August 1997, a reworking of the early track "More Beats + Pieces" gave them their first UK Top 40 hit since 1989.
The album Let Us Play! followed in September and also made the Top 40. The fourth album by Coldcut, Let Us Play! paid homage to the greats that inspired them. Their first album to be released on Ninja Tune, it featured guest appearances by Grandmaster Flash, Steinski, Jello Biafra, Jimpster, The Herbaliser, Talvin Singh, Daniel Pemberton and Selena Saliva. Coldcut’s cut 'n' paste method on the album was compared to that of Dadaism and William Burroughs. Hex collaborated with Coldcut to produce the multimedia CD-Rom for the album. Hex later evolved the software into the engine that was used on the Let Us Play! world tour.
In 1997, Matt Black - alongside Cambridge based developers Camart - created real-time video manipulation software VJAMM. It allowed users to be a "digital video jockey,", remixing and collaging sound and images and trigger audio and visual samples simultaneously, subsequently bringing futuristic technology to the audio-visual field. VJAMM rivalled some of the features of high-end and high cost tech at the time. The VJAMM technology, praised as being proof of how far computers changed the face of live music, became seminal in both Coldcut's live sets (which were called a "revelaton" by Melody Maker and DJ sets. Their CCTV live show was featured at major festivals including Glastonbury, Roskilde, Sónar, the Montreux Jazz Festival, and John Peel's Meltdown. The "beautifully simple and devastatingly effective" software was deemed revolutionary, and became recognized as a major factor in the evolution of clubs. It eventually earned a place in the American Museum of the Moving Image's permanent collection. As quoted by The Independent: "Coldcut's motto? 'Don't hate the media, be the media." NME was quoted as saying: "Veteran duo Coldcut are so cool they invented the remix - now they are doing the same for television."
Also working with Camart, Black designed DJamm software in 1998, which Coldcut used on laptops for their live shows, providing the audio bed alongside VJAMM’s audiovisual samples. Matt Black explained they designed DJamm so they "could perform electronic music in a different way – i.e., not just taking a session band out to reproduce what you put together in the studio using samples. It had a relationship to DJing, but was more interactive and more effective." Excitingly at that time, DJamm was pioneering in its ability to shuffle sliced loops into intricate sequences, enabling users to split loops into any number of parts.
In 1999, "Let Us Replay!" was released, a double-disc remix album where Coldcut’s classic tunes were remixed by the likes of Cornelius (which was heralded as a highlight of the album, Irresistible Force, Shut Up And Dance, Carl Craig and J Swinscoe. Let Us Replay! pieces together "short sharp shocks that put the mental in ‘experimental’ and still bring the breaks till the breakadawn". It also includes a few live tracks from the duo’s innovative world tour. The CD-Rom of the album, which also contained a free demo disc of the VJamm software, was one of the earliest audiovisual CD- ROMs on the market, and Muzik claimed deserved to "have them canonized...it’s like buying an entire mini studio for under $15.".
2000s.
In 2000, the "Solid Steel" show moved to BBC London.
Coldcut continued to forge interesting collaborations, including 2001's "Re:volution" as an EP in which Coldcut created their own political party (The Guilty Party). Featuring scratches and samples of Tony Blair and William Hague speeches, the 3-track EP included Nautilus' "Space Journey," which won an Intermusic contest in 2000. The video was widely played on MTV. With ‘Space Journey,’ Coldcut were arguably the first group to give fans access to the multitrack parts, or "stems" of their songs, building on the idea of interactivity and sharing from Let Us Play.
In 2001, Coldcut produced tracks for the Sega music video game "Rez". "Rez" replaced typical video-game sound effects with electronic music; the player created sounds and melodies, intended to simulate a form of synesthesia. The soundtrack also featured Adam Freeland and Oval.
In 2002, while utilizing VJamm and Detraktor, Coldcut and Juxta remixed Herbie Hancock’s classic "Rockit", creating both an audio and video remix.
Working with Marcus Clements in 2002, Coldcut released the sample manipulation algorhythm from their DJamm software as a standalone VST plugin that could be used in other software, naming it the "Coldcutter".
Also in 2002, Coldcut with UK VJs Headspace (now mainly performing as the VJamm Allstars developed Gridio, an interactive, immersive audio-visual installation for the Pompidou Centre as part of the ‘Sonic Process’ exhibition. The ‘Sonic Process’ exhibition was launched at the MACBA in Barcelona in conjunction with Sónar, featuring Gridio as its centerpiece. In 2003, a commission for Graz led to a specially built version of Gridio, in a cave inside the castle mountain in Austria. Gridio was later commissioned by O2 for two simultaneous customised installations at the O2 Wireless Festivals in Leeds and London in 2007. That same year, Gridio was featured as part of Optronica at the opening week of the new BFI Southbank development in London.
In 2003, Black worked with Penny Rimbaud (ex Crass) on Crass Agenda's Savage Utopia project. Black performed the piece with Rimbaud, Eve Libertine and other players at London’s Vortex Jazz Club.
In 2004, Coldcut collaborated with American video mashup artist TV Sheriff to produce their cut-up entitled ‘Revolution USA.’ The tactical-media project (coordinated with Canadian art duo NomIg) followed on from the UK version and extended the premise "into an open access participatory project". Through the multimedia political art project, over 12 gigabytes of footage from the last 40 years of US politics were made accessible to download, allowing participants to create a cut-up over a Coldcut beat. Coldcut also collaborated with TV Sheriff and NomIg to produce two audiovisual pieces "World of Evil" (2004) and "Revolution '08" (2008), both composed of footage from the United States presidential elections of respective years. The music used was composed by Coldcut, with "Revolution '08" featuring a remix by the Qemists.
Later that year, a collaboration with the British Antarctic Survey (BAS) led to the psychedelic art documentary "Wavejammer". Coldcut was given access to the BAS archive in order to create sounds and visuals for the short film.
2004 also saw Coldcut produce a radio play in conjunction with renowned young author Hari Kunzru for BBC Radio 3 (incidentally called "Sound Mirrors").
Coldcut returned with the single "Everything Is Under Control” at the end of 2005, featuring Jon Spencer (of Jon Spencer Blues Explosion) and Mike Ladd. It was followed in 2006 by their fifth studio album "Sound Mirrors", which was quoted as being “one of the most vital and imaginative records Jon Moore and Matt Black have ever made”, and saw the duo "continue, impressively, to find new ways to present political statements through a gamut of pristine electronics and breakbeats" (CITATION: Future Music, 2007). The fascinating array of guest vocalists included Soweto Kinch, Annette Peacock, Ameri Baraka, and Saul Williams. The latter followed on from Coldcut’s remix of Williams’ "The Pledge" for a project with DJ Spooky.
A 100-date audiovisual world tour commenced for "Sound Mirrors", which was considered "no small feat in terms of technology or human effort". Coldcut was accompanied by scratch DJ Raj and AV artist Juxta, in addition to guest vocalists from the album, including UK rapper Juice Aleem, Roots Manuva, Mpho Skeef, Jon Spencer and house legend Robert Owens.
Three further singles were released from the album including the Top 75 hit "True Skool" with Roots Manuva. The same track appeared on the soundtrack of the video game FIFA Street 2.
Sponsored by the British Council, in 2005 Coldcut introduced AV mixing to India with the Union project, alongside collaborators Howie B and Aki Nawaz of Fun-Da-Mental. Coldcut created an A/V remix of the Bollywood hit movie "Kal Ho Naa Ho".
In 2006, Coldcut performed an A/V set based on "Music for 18 Musicians" as part of Steve Reich’s 70th birthday gig at the Barbican Centre in London.
Coldcut remixed another classic song in 2007: Nina Simone's "Save Me". This was part of a remix album called "Nina Simone: Remixed & Re-imagined", featuring remixes from Tony Humphries, Francois K and Chris Coco.
In February 2007, Coldcut and Mixmaster Morris created a psychedelic AV obituary/tribute Coldcut, Mixmaster Morris, Ken Campbell, Bill Drummond and Alan Moore (18 March 2007). Robert Anton Wilson tribute show. Queen Elizabeth Hall, London: Mixmaster Morris. (28 August 2009) to Robert Anton Wilson, the 60s author of Illuminatus! Trilogy. The tribute featured graphic novel writer Alan Moore and artist Bill Drummond and a performance by experimental theatre legend Ken Campbell. Coldcut and Morris' hour and a half performance resembled a documentary being remixed on the fly, cutting up nearly 15 hours’ worth of Wilson’s lectures.
In 2008, an international group of party organisers, activists and artists including Coldcut received a grant from the Intelligent Energy Department of the European Union, to create a project that promoted intelligent energy and environmental awareness to the youth of Europe. The result was Energy Union, a piece of VJ cinema, political campaign, music tour, party, art exhibition and social media hub. Energy Union toured 12 EU countries throughout 2009 and 2010, completing 24 events in total. Coldcut created the Energy Union show for the tour, a one-hour Audio/Visual montage on the theme of Intelligent Energy. In presenting new ideas for climate, environmental and energy communication strategies, the Energy Union tour was well received, and reached a widespread audience in cities across the UK, Germany, Belgium, The Netherlands, Croatia, Slovenia, Austria, Hungary, Bulgaria, Spain and the Czech Republic.
Also in 2008, Coldcut was asked to remix the theme song for British cult TV show "Doctor Who" for the program's 40th anniversary. In October 2008, Coldcut celebrated the legacy of the BBC Radiophonic Workshop (the place where the Doctor Who theme was created) with a live DJ mix at London’s legendary Roundhouse. The live mix incorporated classic Radiophonic Workshop compositions with extended sampling of the original gear.
Additionally in 2008, Coldcut remixed "Ourselves", a Japanese #1 hit from the single "&" by Ayumi Hamasaki. This mix was included on the album "".
Starting in 2009, Matt Black, with musician/artist/coder Paul Miller (creator of the TX Modular Open Source synth), developed Granul8, a new type of visual fx/source Black termed a ‘granular video synthesiser’. Granul8 allows the use of realtime VJ techniques including video feedback combined with VDMX VJ software.
From 2009 onwards, Black has been collaborating with coder and psychedelic mathematician William Rood to create a forthcoming project called Liveloom, a social media AV mixer.
Recent work.
In 2010, Coldcut celebrated 20 years of releasing music with its label, Ninja Tune. A book entitled "Ninja Tune: 20 Years of Beats and Pieces" was released on 12 August 2010, and an exhibition was held at Black Dog Publishing's Black Dog Space in London, showcasing artwork, design and photography from the label's 20-year history. A compilation album was released on 20 September in two formats: a regular version consisting of two 2-disc volumes, and a limited edition which contained six CDs, six 7" vinyl singles, a hardback copy of the book, a poster and additional items. Ninja Tune also incorporated a series of international parties. This repositioned Ninja as a continually compelling and influential label, being one of the "longest-running (and successful) UK indie labels to come out of the late-1980s/early-90s explosion in dance music and hip-hop" (Pitchfork, 28 September 2010). Pitchfork claimed it had a "right to show off a little".
In July 2013, Coldcut produced a piece entitled "D'autre" based on the writings of French poet Arthur Rimbaud, for Forum Des Images in Paris. The following month, in August, Coldcut produced a new soundtrack for a section of André Sauvage’s classic film "Études sur Paris", which was shown as part of Noise of Art at the BFI in London, which celebrated 100 years of electronic music and silent cinema. Coldcut put new music to films from the Russolo era, incorporating original recordings of Russolo's proto-synths.
Most recently, Coldcut released Ninja Jamm, a music making app, for Android and iOS, in collaboration with London-based arts and technology firm Seeper. Geared toward both casual listeners and more experienced DJs and music producers, the freemium app allows users to download, remix and make music with samplepacks and tunepacks that feature pro quality sample libraries and also original tracks and mixes by Coldcut, as well as other Ninja artists, creating something new altogether. With the "intuitive yet deep" app, users can turn instruments on and off, swap between clips, add glitches and effects, trigger and pitch-bend stabs and one-off samples, and change the tempo of the track instantly. Users can additionally record as they mix and instantly upload to SoundCloud or save the mixes locally. Tunepack releases for Ninja Jamm are increasingly synchronised with Ninja Tune releases on conventional formats. To date over 30 tunepacks have been released, including Amon Tobin, Bonobo, Coldcut, DJ Food, Martyn, Lapalux, Machinedrum, Raffertie, Irresistible Force, FaltyDL, Shuttle, Starkey. Ninja Jamm was featured by Apple in the New and Noteworthy section of the App Store in the week of release and it received over 100,000 downloads in the first week. Coldcut are developing Ninja Jamm further after the Android release garnered acclaim from the Guardian, Independent, Gizmodo and many more reviewers.
In 2015, Coldcut are working on a new album, collaborating with producer Dave Taylor (a.k.a. Solid Groove a.k.a. Switch). This is planned for release in 2016.

</doc>
<doc id="6656" url="https://en.wikipedia.org/wiki?curid=6656" title="Cuisine">
Cuisine

A cuisine ( ; from French , in turn from Latin "coquere" "to cook") is a style of cooking characterized by distinctive ingredients, techniques and dishes, and usually associated with a specific culture or geographic region. A cuisine is primarily influenced by the ingredients that are available locally or through trade. Religious food laws, such as Hindu, Islamic and Jewish dietary laws, can also exercise a strong influence on cuisine. Regional food preparation traditions, customs and ingredients often combine to create dishes unique to a particular region.
Factors that affect a cuisine.
Some factors that have an influence on a region's cuisine include the area's climate, the trade among different countries, religiousness or sumptuary laws and culinary culture exchange.
The area's climate in large measure determines the native foods that are available. In addition, climate influences food preservation. For example, foods preserved for winter consumption by smoking, curing, and pickling have remained significant in world cuisines for their altered gustatory properties.
The trade among different countries also largely affects a region's cuisine. Dating back to the ancient spice trade, seasonings such as cinnamon, cassia, cardamom, ginger, and turmeric were important items of commerce in the earliest evolution of trade. Cinnamon and cassia found their way to the Middle East at least 4,000 years ago.
Certain foods and food preparations are required or proscribed by the religiousness or sumptuary laws, such as Islamic dietary laws and Jewish dietary laws.
Culinary culture exchange is also an important factor for cuisine in many regions: Japan’s first substantial and direct exposure to the West came with the arrival of European missionaries in the second half of the 16th century. At that time, the combination of Spanish and Portuguese game frying techniques with a Chinese method for cooking vegetables in oil led to the development of "tempura", the popular Japanese dish in which seafood and many different types of vegetables are coated with batter and deep fried.
History.
Cuisine dates back to the Antiquity. Rome was known for its cuisine, wealthy families would dine in the Triclinium on a variety of dishes, their diet consisted of eggs, cheese, bread, meat and honey.
New cuisines.
Cuisines evolve continually, and new cuisines are created by innovation and cultural interaction. One recent example is fusion cuisine, which combines elements of various culinary traditions while not being categorized per any one cuisine style, and generally refers to the innovations in many contemporary restaurant cuisines since the 1970s. "Nouvelle cuisine" (New cuisine) is an approach to cooking and food presentation in French cuisine that was popularized in the 1960s by the food critics Henri Gault, who invented the phrase, and his colleagues André Gayot and Christian Millau in a new restaurant guide, the Gault-Millau, or "Le Nouveau Guide". Molecular gastronomy, is a modern style of cooking which takes advantage of many technical innovations from the scientific disciplines. The term was coined in 1988 by late Oxford physicist Nicholas Kurti and the French INRA chemist Hervé This. It is also named as multi sensory cooking, modernist cuisine, culinary physics, and experimental cuisine by some chefs. Besides, international trade brings new foodstuffs including ingredients to existing cuisines and leads to changes. The introduction of hot pepper to China from South America around the end of the 17th century, greatly influencing Sichuan cuisine, which combines the original taste with the taste of introduced hot pepper and creates a unique flavor of both spicy and pungent.
Global cuisine.
A global cuisine is a cuisine that is practiced around the world, and can be categorized according to the common use of major foodstuffs, including grains, produce and cooking fats.
Regional cuisines.
Regional cuisines may vary based upon food availability and trade, cooking traditions and practices, and cultural differences. For example, in Central and South America, corn (maize), both fresh and dried, is a staple food. In northern Europe, wheat, rye, and fats of animal origin predominate, while in southern Europe olive oil is ubiquitous and rice is more prevalent. In Italy the cuisine of the north, featuring butter and rice, stands in contrast to that of the south, with its wheat pasta and olive oil. China likewise can be divided into rice regions and noodle & bread regions. Throughout the Middle East and Mediterranean there is a common thread marking the use of lamb, olive oil, lemons, peppers, and rice. The vegetarianism practiced in much of India has made pulses (crops harvested solely for the dry seed) such as chickpeas and lentils as significant as wheat or rice. From India to Indonesia the use of spices is characteristic; coconuts and seafood are used throughout the region both as foodstuffs and as seasonings.
African cuisine.
African cuisines use a combination of locally available fruits, cereal grains and vegetables, as well as milk and meat products. In some parts of the continent, the traditional diet features a preponderance of milk, curd and whey products. In much of tropical Africa, however, cow's milk is rare and cannot be produced locally (owing to various diseases that affect livestock). The continent's diverse demographic makeup is reflected in the many different eating and drinking habits, dishes, and preparation techniques of its manifold populations.
Asian cuisine.
Asian cuisines are many and varied. Ingredients common to many cultures in the east and Southeast regions of the continent include rice, ginger, garlic, sesame seeds, chilies, dried onions, soy, and tofu. Stir frying, steaming, and deep frying are common cooking methods. While rice is common to most Asian cuisines, different varieties are popular in the various regions; Basmati rice is popular in the subcontinent, Jasmine is often found across the southeast, while long-grain rice is popular in China and short-grain in Japan and Korea. Curry is also a common dish found in southern and eastern Asia, however they are not as popular in western Asian cuisines. Those curry dishes with origins in India and other South Asian countries usually have a yogurt base while Southeastern and Eastern curries generally use coconut milk as their foundation.
European cuisine.
European cuisine (alternatively, "Western cuisine") include the cuisines of Europe and other Western countries. European cuisine includes that of Europe and to some extent Russia, as well as non-indigenous cuisines of North America, Australasia, Oceania, and Latin America. The term is used by East Asians to contrast with Asian styles of cooking. This is analogous to Westerners referring collectively to the cuisines of Asian countries as Asian cuisine. When used by Westerners, the term may refer more specifically to cuisine "in" Europe; in this context, a synonym is Continental cuisine, especially in British English.
Oceanian cuisine.
Oceanian cuisines include Australian cuisine, New Zealand cuisine, Tasmanian cuisine, and the cuisines from many other islands or island groups throughout Oceania.
Cuisines of the Americas.
The cuisines of the Americas are found across North and South America are based on the cuisines of the countries from which the immigrant peoples came, primarily Europe. However, the traditional European cuisine has been adapted by the addition of many local ingredients, and many techniques have been added to the tradition as well. In the case of Latin American cuisines, many pre-Columbian ingredients and techniques are still used. The regional cuisines are Canadian cuisine, American cuisine, Mexican cuisine, Central American cuisine, South American cuisine, and Caribbean cuisine.

</doc>
<doc id="6660" url="https://en.wikipedia.org/wiki?curid=6660" title="Codec">
Codec

A codec is a device or computer program for encoding or decoding a digital data stream or signal. "Codec" is a portmanteau of "coder-decoder" or, less commonly, "compressor-decompressor".
A codec encodes a data stream or signal for transmission, storage or encryption, or decodes it for playback or editing. Codecs are used in videoconferencing, streaming media, and video editing applications.
Related concepts.
In the mid 20th century, a codec was a hardware device that coded analog signals into digital form using pulse-code modulation (PCM). Late in the century the name was also applied to a class of software for converting between different digital signal formats, including compander functions.
A modem is a contraction of "modulator-demodulator". The telecommunications industry referred to the device as a "dataset". It converts digital data from computers to analog signals for transmission over telephone lines. On the receiving end the analog signal is converted back to digital data.
An audio codec converts analog audio signals into digital signals for transmission or storage. A receiving device then converts the digital signals back to analog using an audio decompressor, for playback. An example of this is the codecs used in the sound cards of personal computers. A video codec accomplishes the same task for video signals.
Compression quality.
Media codecs.
Two principal techniques are used in codecs, pulse-code modulation and delta modulation. Codecs are often designed to emphasize certain aspects of the media to be encoded. For example, a digital video (using a DV codec) of a sports event needs to encode motion well but not necessarily exact colors, while a video of an art exhibit needs to encode color and surface texture well.
Audio codecs for cell phones need to have very low latency between source encoding and playback. In contrast, audio codecs for recording or broadcast can use high-latency audio compression techniques to achieve higher fidelity at a lower bit-rate.
There are thousands of audio and video codecs, ranging in cost from free to hundreds of dollars or more. This variety of codecs can create compatibility and obsolescence issues. The impact is lessened for older formats, for which free or nearly-free codecs have existed for a long time. The older formats are often ill-suited to modern applications, however, such as playback in small portable devices. For example, raw uncompressed PCM audio (44.1 kHz, 16 bit stereo, as represented on an audio CD or in a .wav or .aiff file) has long been a standard across multiple platforms, but its transmission over networks is slow and expensive compared with more modern compressed formats, such as Opus and MP3.
Many multimedia data streams contain both audio and video, and often some metadata that permit synchronization of audio and video. Each of these three streams may be handled by different programs, processes, or hardware; but for the multimedia data streams to be useful in stored or transmitted form, they must be encapsulated together in a container format.
Lower bitrate codecs allow more users, but they also have more distortion. Beyond the initial increase in distortion, lower bit rate codecs also achieve their lower bit rates by using more complex algorithms that make certain assumptions, such as those about the media and the packet loss rate. Other codecs may not make those same assumptions. When a user with a low bitrate codec talks to a user with another codec, additional distortion is introduced by each transcoding.
AVI is sometimes erroneously described as a codec, but AVI is actually a container format, while a codec is a software or hardware tool that encodes or decodes audio or video into or from some audio or video format. Audio and video encoded with many codecs might be put into an AVI container, although AVI is not an ISO standard. There are also other well-known container formats, such as Ogg, ASF, QuickTime, RealMedia, Matroska, and DivX Media Format. Some container formats which are ISO standards are MPEG transport stream, MPEG program stream, MP4 and ISO base media file format.

</doc>
<doc id="6663" url="https://en.wikipedia.org/wiki?curid=6663" title="Clyde Tombaugh">
Clyde Tombaugh

Clyde William Tombaugh (; February 4, 1906January 17, 1997) was an American astronomer. He discovered Pluto in 1930, the first object to be discovered in what would later be identified as the Kuiper belt. At the time of discovery, Pluto was considered a planet but was later reclassified as a dwarf planet. Tombaugh also discovered many asteroids. He also called for the serious scientific research of unidentified flying objects, or UFOs.
Life and career.
Tombaugh was born in Streator, Illinois, the son of Adella Pearl (Chritton) and Muron Dealvo Tombaugh, a farmer. After his family moved to Burdett, Kansas in 1922, Tombaugh's plans for attending college were frustrated when a hailstorm ruined his family's farm crops. Starting in 1926, he built several telescopes with lenses and mirrors by himself. In order to better test his telescope mirrors, Tombaugh, with just a pick and shovel, dug a pit 24 feet long, 8 feet deep, and 7 feet wide. This cave provided a constant air temperature, free of air currents. It was also used by the family as a root cellar and emergency shelter. He sent drawings of Jupiter and Mars to the Lowell Observatory, which offered him a job. Tombaugh worked there from 1929 to 1945.
Following his discovery of Pluto, Tombaugh earned bachelor's and master's degrees in astronomy from the University of Kansas in 1936 and 1938. During World War II he taught naval personnel navigation at Northern Arizona University. He worked at White Sands Missile Range in the early 1950s, and taught astronomy at New Mexico State University from 1955 until his retirement in 1973.
The asteroid 1604 Tombaugh, discovered in 1931, is named after him. He discovered hundreds of asteroids, beginning with 2839 Annette in 1929, mostly as a by-product of his search for Pluto and his searches for other celestial objects. Tombaugh named some of them after his wife, children and grandchildren. The Royal Astronomical Society awarded him the Jackson-Gwilt Medal in 1931.
In 1980, he wrote a book "Out of the Darkness:The Planet Pluto" with Patrick Moore.
In August 1992, JPL scientist Robert Staehle called Tombaugh, requesting permission to visit his planet. "I told him he was welcome to it," Tombaugh later remembered, "though he's got to go one long, cold trip." The call eventually led to the launch of the New Horizons space probe to Pluto in 2006.
Following the passage on July 14, 2015 of Pluto by the New Horizons spacecraft the "Cold Heart of Pluto" was named Tombaugh Regio.
Death.
Tombaugh died on January 17, 1997, when he was in Las Cruces, New Mexico, at the age of 90. A small portion of his ashes was placed aboard the New Horizons spacecraft. The container includes the inscription: "Interned herein are remains of American Clyde W. Tombaugh, discoverer of Pluto and the solar system's 'third zone'. Adelle and Muron's boy, Patricia's husband, Annette and Alden's father, astronomer, teacher, punster, and friend: Clyde W. Tombaugh (1906–1997)".
Tombaugh was survived by his wife, Patricia (1912–2012), and their children, Annette and Alden.
Religion.
Tombaugh was an active Unitarian-Universalist, and he and his wife helped found the Unitarian Universalist Church of Las Cruces, New Mexico.
Family.
Through the daughter of his youngest brother, Robert M., Tombaugh is the great uncle of Los Angeles Dodgers pitcher Clayton Kershaw.
Discovery of Pluto.
While a young researcher working for the Lowell Observatory in Flagstaff, Arizona, Tombaugh was given the job to perform a systematic search for a trans-Neptunian planet (also called Planet X), which had been predicted by Percival Lowell and William Pickering.
Tombaugh used the observatory's 13-inch astrograph to take photographs of the same section of sky several nights apart. He then used a blink comparator to compare the different images. When he shifted between the two images, a moving object, such as a planet, would appear to jump from one position to another, while the more distant objects such as stars would appear stationary. Tombaugh noticed such a moving object in his search, near the place predicted by Lowell, and subsequent observations showed it to have an orbit beyond that of Neptune. This ruled out classification as an asteroid, and they decided this was the ninth planet that Lowell had predicted. The discovery was made on Tuesday, February 18, 1930, using images taken the previous month. The name "Pluto" was suggested by Venetia Burney, then an 11-year-old English schoolgirl, who died in April 2009, having lived to see the reclassification of Pluto as a dwarf planet. It won out over numerous other suggestions because it was the name of the Roman god of the underworld, who was able to render himself invisible, and because Percival Lowell's initials PL formed the first 2 letters. The name Pluto was officially adopted on May 1, 1930.
Following the discovery, starting in the 1990s, of other Kuiper belt objects, Pluto began to be seen not as a planet orbiting alone at 40 AU, but as the largest of a group of icy bodies in that region of space. After it was shown that at least one such body was more massive than Pluto, on August 24, 2006 the International Astronomical Union (IAU) reclassified Pluto, grouping it with two similarly sized "dwarf planets" rather than with the eight "classical planets".
Tombaugh's widow Patricia stated after the IAU's decision that while Clyde may have been disappointed with the change, since he had resisted attempts to remove Pluto's planetary status in his lifetime, he would have accepted the decision now if he were alive. She noted that he "was a scientist. He would understand they had a real problem when they start finding several of these things flying around the place." Hal Levison offered this perspective on Tombaugh's place in history: "Clyde Tombaugh discovered the Kuiper Belt. That's a helluva lot more interesting than the ninth planet."
Further search.
Tombaugh continued searching for some years after the discovery of Pluto, and the lack of further discoveries left him satisfied that no other object of a comparable apparent magnitude existed near the ecliptic. No more trans-Neptunian objects were discovered until , in 1992.
However, more recently the relatively bright object has been discovered. It has a relatively high orbital inclination, but at the time of Tombaugh's discovery of Pluto, Makemake was only a few degrees from the ecliptic near the border of Taurus and Auriga at an apparent magnitude of 16. This position was also very near the galactic equator, making it almost impossible to find such an object within the dense concentration of background stars of the Milky Way. In the fourteen years of looking for planets, Tombaugh looked for motion in 90 million star images.
Asteroids discovered.
Tombaugh is officially credited by the Minor Planet Center with discovering 15 asteroids, and he observed nearly 800 asteroids during his search for Pluto and years of follow-up searches looking for another candidate for the postulated Planet X. Tombaugh is also credited with the discovery of periodic comet 274P/Tombaugh–Tenagra. He also discovered hundreds of variable stars, as well as star clusters, galaxy clusters, and a galaxy supercluster.
Interest in UFOs.
Tombaugh was probably the most eminent astronomer to have reported seeing unidentified flying objects and to support the extraterrestrial hypothesis. On August 20, 1949, Tombaugh saw several unidentified objects near Las Cruces, New Mexico. He described them as six to eight rectangular lights, stating: "I doubt that the phenomenon was any terrestrial reflection, because... nothing of the kind has ever appeared before or since... I was so unprepared for such a strange sight that I was really petrified with astonishment.".
Tombaugh observed these rectangles of light for about 3 seconds and his wife saw them for about seconds. He never supported the interpretation as a spaceship that has often been attributed to him. He considered other possibilities, with a temperature inversion as the most likely cause.From my own studies of the solar system I cannot entertain any serious possibility for intelligent life on other planets, not even for Mars... The logistics of visitations from planets revolving around the nearer stars is staggering. In consideration of the hundreds of millions of years in the geologic time scale when such visits may have possibly occurred, the odds of a single visit in a given century or millennium are overwhelmingly against such an event.
A much more likely source of explanation is some natural optical phenomenon in our own atmosphere. In my 1949 sightings the faintness of the object, together with the manner of fading in intensity as it traveled away from the zenith towards the southeastern horizon, is quite suggestive of a reflection from an optical boundary or surface of slight contrast in refractive index, as in an inversion layer.
I have never seen anything like it before or since, and I have spent a lot of time where the night sky could be seen well. This suggests that the phenomenon involves a comparatively rare set of conditions or circumstances to produce it, but nothing like the odds of an interstellar visitation.
Another sighting by Tombaugh a year or two later while at a White Sands observatory was of an object of −6 magnitude, four times brighter than Venus at its brightest, going from the zenith to the southern horizon in about 3 seconds. The object executed the same maneuvers as in Tombaugh's first sighting.
Tombaugh later reported having seen three of the mysterious green fireballs, which suddenly appeared over New Mexico in late 1948 and continued at least through the early 1950s. A researcher on Project Twinkle reported that Tombaugh "... never observed an unexplainable aerial object despite his continuous and extensive observations of the sky."
According to an entry in "UFO updates", Tombaugh said: "I have seen three objects in the last seven years which defied any explanation of known phenomenon, such as Venus, atmospheric optic, meteors or planes. I am a professional, highly skilled, professional astronomer. In addition I have seen three green fireballs which were unusual in behavior from normal green fireballs... I think that several reputable scientists are being unscientific in refusing to entertain the possibility of extraterrestrial origin and nature."
Shortly after this in January 1957, in an Associated Press article in the "Alamogordo Daily News" titled "Celestial Visitors May Be Invading Earth's Atmosphere," Tombaugh was again quoted on his sightings and opinion about them. "Although our own solar system is believed to support no other life than on Earth, other stars in the galaxy may have hundreds of thousands of habitable worlds. Races on these worlds may have been able to utilize the tremendous amounts of power required to bridge the space between the stars..." Tombaugh stated that he had observed celestial phenomena which he could not explain, but had seen none personally since 1951 or 1952. "These things, which do appear to be directed, are unlike any other phenomena I ever observed. Their apparent lack of obedience to the ordinary laws of celestial motion gives credence."
In 1949, Tombaugh had also told the Naval missile director at White Sands Missile Range, Commander Robert McLaughlin, that he had seen a bright flash on Mars on August 27, 1941, which he now attributed to an atomic blast. Tombaugh also noted that the first atomic bomb tested in New Mexico would have lit up the dark side of the Earth like a neon sign and that Mars was coincidentally quite close at the time, the implication apparently being that the atomic test would have been visible from Mars.
In June 1952, Dr. J. Allen Hynek, an astronomer acting as a scientific consultant to the Air Force's Project Blue Book UFO study, secretly conducted a survey of fellow astronomers on UFO sightings and attitudes while attending an astronomy convention. Tombaugh and four other astronomers, including Dr. Lincoln LaPaz of the University of New Mexico, told Hynek about their sightings. Tombaugh also told Hynek that his telescopes were at the Air Force's disposal for taking photos of UFOs, if he was properly alerted.
Near-Earth satellite search.
Tombaugh's offer may have led to his involvement in a search for near-Earth satellites, first announced in late 1953 and sponsored by the Army Office of Ordnance Research. Another public statement was made on the search in March 1954 (photo at right), emphasizing the rationale that such an orbiting object would serve as a natural space station. However, according to Donald Keyhoe, later director of the National Investigations Committee on Aerial Phenomena (NICAP), the real reason for the sudden search was because two near-Earth orbiting objects had been picked up on new long-range radar in the summer of 1953, according to his Pentagon source.
By May 1954, Keyhoe was making public statements that his sources told him the search had indeed been successful, and either one or two objects had been found. However, the story did not break until August 23, 1954, when "Aviation Week" magazine stated that two satellites had been found only 400 and 600 miles out. They were termed "natural satellites" and implied that they had been recently captured, despite this being a virtual impossibility. The next day, the story was in many major newspapers. Dr. LaPaz was implicated in the discovery in addition to Tombaugh. LaPaz had earlier conducted secret investigations on behalf of the Air Force on the green fireballs and other unidentified aerial phenomena over New Mexico. The "New York Times" reported on August 29 that "a source close to the O. O. R. unit here described as 'quite accurate' the report in the magazine Aviation Week that two previously unobserved satellites had been spotted and identified by Dr. Lincoln Lepaz of the University of New Mexico as natural and not artificial objects. This source also said there was absolutely no connection between the reported satellites and flying saucer reports." However, in the October 10th issue, LaPaz said the magazine article was "false in every particular, in so far as reference to me is concerned."
Both LaPaz and Tombaugh were to issue public denials that anything had been found. The October 1955 issue of "Popular Mechanics" magazine reported: "Professor Tombaugh is closemouthed about his results. He won't say whether or not any small natural satellites have been discovered. He does say, however, that newspaper reports of 18 months ago announcing the discovery of natural satellites at 400 and 600 miles out are not correct. He adds that there is no connection between the search program and the reports of so-called flying saucers."
At a meteor conference in Los Angeles in 1957, Tombaugh reiterated that his four-year search for "natural satellites" had been unsuccessful. In 1959, Tombaugh was to issue a final report stating that nothing had been found in his search. His personal 16-inch telescope was reassembled and dedicated on September 17, 2009 at Rancho Hidalgo, New Mexico (near Animas, New Mexico), adjacent to "Astronomy" 's new observatory.

</doc>
<doc id="6666" url="https://en.wikipedia.org/wiki?curid=6666" title="Christopher Báthory">
Christopher Báthory

Christopher Báthory (Hungarian: "Báthory Kristóf") (1530, Szilágysomlyó – 27 May 1581, Gyulafehérvár) was a voivode of Transylvania. He succeeded his brother Stephen Báthory. He was the father of Sigismund Báthory and Gryzelda Bathory.

</doc>
<doc id="6667" url="https://en.wikipedia.org/wiki?curid=6667" title="CPAN">
CPAN

CPAN, the Comprehensive Perl Archive Network, is a software repository of over 150,929 modules of software in 33,069 distributions, written by 12,528 authors, written in the Perl programming language, as well as documentation for them. The modules can be downloaded from metacpan.org and also from mirrored sites worldwide. "CPAN" can denote either the archive network itself, or the Perl program that acts as an interface to the network and as an automated software installer (somewhat like a package manager). Most software on CPAN is free and open source software. CPAN was conceived in 1993 and active online since October 1995.
Modules.
Like many programming languages, Perl has mechanisms to use external libraries of code, making one file contain common routines used by several programs. Perl calls these "modules". Perl modules are typically installed in one of several directories whose paths are placed in the Perl interpreter when it is first compiled; on Unix-like operating systems, common paths include "/usr/lib/perl5", "/usr/local/lib/perl5", and several of their subdirectories.
Perl comes with a small set of "core modules". Some of these perform bootstrapping tasks, such as , which is used for building and installing other extension modules; others, like CGI.pm, are merely commonly used. The authors of Perl do not expect this limited group to meet every need, however.
Role.
The CPAN's main purpose is to help programmers locate modules and programs not included in the Perl standard distribution. Its structure is decentralized. Authors maintain and improve their own modules. Forking, and creating competing modules for the same task or purpose is common. There is no formal bug tracking system, but there is a third-party bug tracking system that CPAN designated as the suggested official method of reporting issues with modules. Continuous development on modules is rare; many are abandoned by their authors, or go years between new versions being released. Sometimes a maintainer will be appointed to an abandoned module. They can release new versions of the module, and accept patches from the community to the module as their time permits. CPAN has no revision control system, although the source for the modules is often stored on GitHub. Also, the complete history of the CPAN and all its modules is available as the GitPAN project, allowing to easily see the complete history for all the modules and for easy maintenance of forks. CPAN is also used to distribute new versions of Perl, as well as related projects, such as Parrot.
Structure.
Files on the CPAN are referred to as "distributions". A distribution may consist of one or more modules, documentation files, or programs packaged in a common archiving format, such as a gzipped tar archive or a ZIP file. Distributions will often contain installation scripts (usually called "Makefile.PL" or "Build.PL") and test scripts which can be run to verify the contents of the distribution are functioning properly. New distributions are uploaded to the Perl Authors Upload Server, or PAUSE (see the section Uploading distributions with PAUSE).
In 2003, distributions started to include metadata files, called "META.yml", indicating the distribution's name, version, dependencies, and other useful information; however, not all distributions contain metadata. When metadata is not present in a distribution, the PAUSE's software will usually try to analyze the code in the distribution to look for the same information; this is not necessarily very reliable.
With thousands of distributions, CPAN needs to be structured to be useful. Distributions on the CPAN are divided into 24 broad "chapters" based on their purpose, such as "Internationalization and Locale"; "Archiving, Compression, And Conversion"; and "Mail and Usenet News". Distributions can also be browsed by author. Finally, the natural hierarchy of Perl module names (such as "Apache::DBI" or "Lingua::EN::Inflect") can sometimes be used to browse modules in the CPAN.
CPAN module distributions usually have names in the form of "CGI-Application-3.1" (where the :: used in the module's name has been replaced with a dash, and the version number has been appended to the name), but this is only a convention; many prominent distributions break the convention, especially those that contain multiple modules. Security restrictions prevent a distribution from ever being replaced, so virtually all distribution names do include a version number.
Components.
Mirrors.
The heart of CPAN is its worldwide network of more than 260 mirrors in more than 60 countries. CPAN's master site has over 149 direct public mirrors. Each site contains up to the full 3.9 gigabytes of data, or a subset of it if the mirror's maintainer wishes to selectively choose.
Most mirrors update themselves hourly, daily or bidaily from the CPAN master site. Some sites are major FTP servers which mirror lots of other software, but others are simply servers owned by companies that use Perl heavily. There are at least two mirrors on every continent except Antarctica.
For more information on CPAN mirrors, see mirrors.cpan.org.
Search engines.
Several search engines have been written to help Perl programmers sort through the CPAN. The most popular and official is search.cpan.org, which includes textual search, a browsable index of modules, and extracted copies of all distributions currently on the CPAN. Other CPAN search engines that have been set up are:
Testers.
CPAN Testers are a group of volunteers, who will download and test distributions as they are uploaded to CPAN. This enables the authors to have their modules tested on many platforms and environments that they would otherwise not have access to, thus helping to promote portability, as well as a degree of quality. Smoke testers send reports, which are then collated and used for a variety of presentation websites, including the main reports site, statistics and dependencies.
Other supporting websites.
A family of other loosely integrated support websites have been created as the CPAN has grown in size and scale. These are created and managed by individual Perl developers, and provide data feeds to each other in various ad-hoc ways.
CPAN.pm and CPANPLUS.
There is also a Perl core module named CPAN; it is usually differentiated from the repository itself by using the name CPAN.pm. CPAN.pm is mainly an interactive shell which can be used to search for, download, and install distributions. An interactive shell called cpan is also provided in the Perl core, and is the usual way of running CPAN.pm. After a short configuration process and mirror selection, it uses tools available on the user's computer to automatically download, unpack, compile, test, and install modules. It is also capable of updating itself.
More recently, an effort to replace CPAN.pm with something cleaner and more modern has resulted in the CPANPLUS (or CPAN++) set of modules. CPANPLUS separates the back-end work of downloading, compiling, and installing modules from the interactive shell used to issue commands. It also supports several advanced features, such as cryptographic signature checking and test result reporting. Finally, CPANPLUS can uninstall a distribution. CPANPLUS was added to the Perl core in version 5.10.0.
Both modules can check a distribution's dependencies and can be set to recursively install any prerequisites, either automatically or with individual user approval. Both support FTP and HTTP and can work through firewalls and proxies.
Uploading distributions with PAUSE.
Authors can upload new distributions to the CPAN through the "Perl Authors Upload Server" (PAUSE). To do so, they must request a PAUSE account. Registration information can be found at the PAUSE faq
Registrations are manually reviewed, so the process may take a week or longer.
Once registered, the new PAUSE account has a directory in the CPAN under "authors/id/(first letter)/(first two letters)/(author ID)". They may use a web interface at pause.perl.org, or the PAUSE ftp server to upload files to their directory and delete them. PAUSE will warn an administrator if a user uploads a module that already exists, unless they are listed as a "co-maintainer". This can be specified through PAUSE's web interface.
Influence.
Experienced Perl programmers often comment that half of Perl's power is in the CPAN. It has been called Perl's killer app. Though the TeX typesetting language has an equivalent, the CTAN (and in fact the CPAN's name is based on the CTAN), few languages have an exhaustive central repository for libraries. The PHP language has PECL and PEAR, Python has a PyPI (Python Package Index) repository, Ruby has RubyGems, R has CRAN, Node.js has npm, Lua has LuaRocks, Haskell has Hackage and an associated installer/make clone cabal; but none of these are as large as the CPAN. Recently, Common Lisp has a de facto CPAN-like system—the Quicklisp repositories. Other major languages, such as Java and C++, have nothing similar to the CPAN (though for Java there is central Maven).
The CPAN has grown so large and comprehensive over the years that Perl users are known to express surprise when they start to encounter topics for which a CPAN module "doesn't" exist already.
The CPAN's influence on Perl's eclectic culture should not be underestimated either. As a hive of activity in the Perl world, the CPAN both shapes and is shaped by Perl culture. Its "self-appointed master librarian", Jarkko Hietaniemi, often takes part in the April Fools Day jokes so popular on the Internet; on 1 April 2002 the site was temporarily named to "CJAN", where the "J" stood for "Java". In 2003, the www.cpan.org domain name was redirected to Matt's Script Archive, a site infamous in the Perl community for having badly written code.
Beyond April Fools', however, some of the distributions on the CPAN are jokes in themselves. The Acme:: hierarchy is reserved for joke modules; for instance, Acme::Don't adds a codice_1 function that doesn't run the code given to it (to complement the codice_2 built-in, which does). Even outside the Acme:: hierarchy, some modules are still written largely for amusement; one example is Lingua::Romana::Perligata, which can be used to write Perl programs in a subset of Latin.
Derivative works.
In 2005, a group of Perl developers who also had an interest in JavaScript got together to create JSAN, the JavaScript Archive Network. The JSAN is a near-direct port of the CPAN infrastructure for use with the JavaScript language, which for most of its lifespan did not have a cohesive "community".
In 2008, after a chance meeting with CPAN admin Adam Kennedy at the Open Source Developers Conference, Linux kernel developer Rusty Russell created the CCAN, the Comprehensive C Archive Network. The CCAN is a direct port of the CPAN architecture for use with the C language.

</doc>
<doc id="6669" url="https://en.wikipedia.org/wiki?curid=6669" title="Colorado Rockies">
Colorado Rockies

The Colorado Rockies are an American professional baseball team based in Denver, Colorado. They are currently members of Major League Baseball (MLB)'s National League (NL) West division. Their home venue is Coors Field. Their current manager is Walt Weiss. The Rockies have won one National League championship (2007). They mounted a rally in the last month of the 2007 regular season, winning 21 of their final 22 games, and reached the 2007 World Series. However, they were swept by the American League (AL) champion Boston Red Sox in four games.
History.
Denver had long been a hotbed of Denver Bears/Zephyrs Minor league baseball and many in the area desired a Major League team. Following the Pittsburgh drug trials, an unsuccessful attempt was made to purchase the Pittsburgh Pirates and relocate them. However, in 1991, as part of Major League Baseball's two-team expansion (they also added the Florida (now Miami) Marlins), an ownership group representing Denver led by John Antonucci and Michael I. Monus were granted a franchise; they took the name "Rockies" due to Denver's proximity to the Rocky Mountains, which is reflected in their logo. Monus and Antonucci were forced to drop out in 1992 after Monus' reputation was ruined by an accounting scandal. Trucking magnate Jerry McMorris stepped in at the 11th hour to save the franchise, allowing the team to begin play in 1993. The Rockies shared Mile High Stadium (which had originally been built for the Zephyrs) with the National Football League's Denver Broncos their first two seasons while Coors Field was constructed. It was completed for the 1995 Major League Baseball season.
In 1993, they started play in the Western division of the National League. Since that date, the Rockies have reached the Major League Baseball postseason three times, each time as the National League wild card team. Twice (1995 and 2009) they were eliminated in the first round of the playoffs. In 2007, the Rockies advanced to the World Series, only to be swept by the Boston Red Sox.
The Rockies have played their home games at Coors Field since 1995. Their newest Spring Training home, Salt River Fields at Talking Stick in Scottsdale, Arizona, opened in March 2011 and is shared with the Arizona Diamondbacks.
Uniform.
At the start of the 2012 season, the Rockies introduced "Purple Mondays" in which the team wears its purple uniform every Monday game day.
Baseball Hall of Famers.
No inducted members of the Baseball Hall of Fame have played for or managed the Rockies.
Retired numbers.
Todd Helton is the sole Colorado player to have his number (17) retired, which was done on Sunday, August 17, 2014.
Jackie Robinson's number, 42, was retired throughout all of baseball in 1997.
Keli McGregor had worked with the Rockies since their inception in 1993, rising from senior director of operations to team president in 2002, until his death on April 20, 2010. He is honored at Coors Field alongside Helton and Robinson with his initials.
Radio and television.
As of 2010, Rockies' flagship radio station is KOA 850AM, with some late-season games broadcast on KHOW 630 AM due to conflicts with Denver Broncos games. Jerry Schemmel and Jack Corrigan are the radio announcers which both serve as backup TV announcers whenever Drew Goodman is not on the broadcast. The Rockies Radio Network is composed of 38 affiliate stations in eight states.
As of 2013, Spanish broadcasts of the Rockies are heard on KNRV 1150 AM.
As of 2013, all games will be produced and televised by Root Sports Rocky Mountain. All 150 games produced by Root Sports Rocky Mountain will be broadcast in HD. Jeff Huson, Drew Goodman and George Frazier form the TV broadcast team with Marc Stout, Jenny Cavnar, Ryan Spilborghs, Jason Hirsh and Cory Sullivan handling the pre-game and post-game shows.

</doc>
<doc id="6670" url="https://en.wikipedia.org/wiki?curid=6670" title="Cement">
Cement

A cement is a binder, a substance used in construction that sets and hardens and can bind other materials together. The most important types of cement are used as a component in the production of mortar in masonry, and of concrete- which is a combination of cement and an aggregate to form a strong building material.
Cements used in construction can be characterized as being either hydraulic or non-hydraulic, depending upon the ability of the cement to set in the presence of water (see hydraulic and non-hydraulic lime plaster).
Non-hydraulic cement will not set in wet conditions or underwater; rather, it sets as it dries and reacts with carbon dioxide in the air. It can be attacked by some aggressive chemicals after setting.
Hydraulic cements (e.g., Portland cement) set and become adhesive due to a chemical reaction between the dry ingredients and water. The chemical reaction results in mineral hydrates that are not very water-soluble and so are quite durable in water and safe from chemical attack. This allows setting in wet condition or underwater and further protects the hardened material from chemical attack. The chemical process for hydraulic cement found by ancient Romans used volcanic ash (activated aluminium silicates) with lime (calcium oxide).
The word "cement" can be traced back to the Roman term "opus caementicium", used to describe masonry resembling modern concrete that was made from crushed rock with burnt lime as binder. The volcanic ash and pulverized brick supplements that were added to the burnt lime, to obtain a hydraulic binder, were later referred to as "cementum", "cimentum", "cäment", and "cement".
Chemistry.
Non-hydraulic cement, such as slaked lime (calcium hydroxide mixed with water), hardens by carbonation in the presence of carbon dioxide which is naturally present in the air. First calcium oxide (lime) is produced from calcium carbonate by calcination at temperatures above 825 °C (1,517 °F) for about 10 hours at atmospheric pressure: 
The calcium oxide is then "spent" (slaked) mixing it with water to make slaked lime (calcium hydroxide):
Once the excess water is completely evaporated (this process is technically called "setting"), the carbonation starts:
This reaction takes a significant amount of time because the partial pressure of carbon dioxide in the air is low. The carbonation reaction requires the dry cement to be exposed to air, and for this reason the slaked lime is a non-hydraulic cement and cannot be used under water. This whole process is called the "lime cycle".
Conversely, hydraulic cement hardens by hydration when water is added. Hydraulic cements (such as Portland cement) are made of a mixture of silicates and oxides, the four main components being: 
The silicates are responsible of the mechanical properties of the cement, the tricalcium aluminate and the brownmillerite are essential to allow the formation of the liquid phase during the kiln sintering (firing).
The chemistry of the above listed reactions is not completely clear and is still the object of research.
History.
Alternatives to cement used in antiquity.
Cement, chemically speaking, is a product including lime as the primary curing ingredient, but it is far from the first material used for cement"ation". The Babylonians and Assyrians used bitumen to bind together burnt brick or alabaster slabs. In Egypt stone blocks were cemented together with mortar, a combination of sand and roughly burnt gypsum (CaSO4·2H2O), which often contained calcium carbonate (CaCO3).
Macedonians and Romans.
Lime (calcium oxide) was used on Crete and by the ancient Greeks. There is evidence that the Minoans of Crete used crushed potshards as an artificial pozzolan for hydraulic cement. It is uncertain where it was first discovered that a combination of hydrated non-hydraulic lime and a pozzolan produces a hydraulic mixture (see also: Pozzolanic reaction), but concrete made from such mixtures was used by the Ancient Macedonians and three centuries later on a large scale by Roman engineers.
The Greeks used volcanic tuff from the island of Thera as their pozzolan and the Romans used crushed volcanic ash (activated aluminium silicates) with lime. This mixture was able to set under water increasing its resistance. The material was called "pozzolana" from the town of Pozzuoli, west of Naples where volcanic ash was extracted. In the absence of pozzolanic ash, the Romans used powdered brick or pottery as a substitute and they may have used crushed tiles for this purpose before discovering natural sources near Rome. The huge dome of the Pantheon in Rome and the massive Baths of Caracalla are examples of ancient structures made from these concretes, many of which are still standing. The vast system of Roman aqueducts also made extensive use of hydraulic cement.
Middle Ages.
Although any preservation of this knowledge in literary sources from the Middle Ages is unknown, medieval masons and some military engineers maintained an active tradition of using hydraulic cement in structures such as canals, fortresses, harbors, and shipbuilding facilities.
Cements in the 18th century.
Technical knowledge of making hydraulic cement was later formalized by French and British engineers in the 18th century. Tabby, a building material using oyster-shell lime, sand, and whole oyster shells to form a concrete, was introduced to the Americas by the Spanish in the sixteenth century.
John Smeaton made an important contribution to the development of cements while planning the construction of the third Eddystone Lighthouse (1755–59) in the English Channel now known as Smeaton's Tower. He needed a hydraulic mortar that would set and develop some strength in the twelve-hour period between successive high tides. He performed experiments with combinations of different limestones and additives including trass and pozzolanas and did exhaustive market research on the available hydraulic limes, visiting their production sites, and noted that the "hydraulicity" of the lime was directly related to the clay content of the limestone from which it was made. Smeaton was a civil engineer by profession, and took the idea no further.
In the South Atlantic seaboard of the United States, tabby relying upon the oyster-shell middens of earlier Native American populations was used in house construction from the 1730s to the 1860s.
In Britain particularly, good quality building stone became ever more expensive during a period of rapid growth, and it became a common practice to construct prestige buildings from the new industrial bricks, and to finish them with a stucco to imitate stone. Hydraulic limes were favored for this, but the need for a fast set time encouraged the development of new cements. Most famous was Parker's "Roman cement". This was developed by James Parker in the 1780s, and finally patented in 1796. It was, in fact, nothing like material used by the Romans, but was a "natural cement" made by burning septaria – nodules that are found in certain clay deposits, and that contain both clay minerals and calcium carbonate. The burnt nodules were ground to a fine powder. This product, made into a mortar with sand, set in 5–15 minutes. The success of "Roman cement" led other manufacturers to develop rival products by burning artificial hydraulic lime cements of clay and chalk.
Roman cement quickly became popular but was largely replaced by Portland cement in the 1850s.
Cements in the 19th century.
Apparently unaware of Smeaton's work, the same principle was identified by Frenchman Louis Vicat in the first decade of the nineteenth century. Vicat went on to devise a method of combining chalk and clay into an intimate mixture, and, burning this, produced an "artificial cement" in 1817 considered the "principal forerunner" of Portland cement and "...Edgar Dobbs of Southwark patented a cement of this kind in 1811."
In Russia, Egor Cheliev created a new binder by mixing lime and clay. His results were published in 1822 in his book "A Treatise on the Art to Prepare a Good Mortar" published in St. Petersburg. A few years later in 1825, he published another book, which described the various methods of making cement and concrete, as well as the benefits of cement in the construction of buildings and embankments.
James Frost, working in Britain, produced what he called "British cement" in a similar manner around the same time, but did not obtain a patent until 1822. In 1824, Joseph Aspdin patented a similar material, which he called "Portland cement", because the render made from it was in color similar to the prestigious Portland stone. However, Aspdins' cement was nothing like modern Portland cement but was a first step in its development, called a "proto-Portland cement". Joseph Aspdins' son William Aspdin had left his fathers company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. William Aspdin's innovation was counterintuitive for manufacturers of "artificial cements", because they required more lime in the mix (a problem for his father), a much higher kiln temperature (and therefore more fuel), and the resulting clinker was very hard and rapidly wore down the millstones, which were the only available grinding technology of the time. Manufacturing costs were therefore considerably higher, but the product set reasonably slowly and developed strength quickly, thus opening up a market for use in concrete. The use of concrete in construction grew rapidly from 1850 onward, and was soon the dominant use for cements. Thus Portland cement began its predominant role.
Isaac Charles Johnson further refined the production of "meso-Portland cement" (middle stage of development) and claimed to be the real father of Portland cement.
Setting time and "early strength" are important characteristics of cements. Hydraulic limes, "natural" cements, and "artificial" cements all rely upon their belite content for strength development. Belite develops strength slowly. Because they were burned at temperatures below , they contained no alite, which is responsible for early strength in modern cements. The first cement to consistently contain alite was made by William Aspdin in the early 1840s: This was what we call today "modern" Portland cement. Because of the air of mystery with which William Aspdin surrounded his product, others ("e.g.," Vicat and Johnson) have claimed precedence in this invention, but recent analysis of both his concrete and raw cement have shown that William Aspdin's product made at Northfleet, Kent was a true alite-based cement. However, Aspdin's methods were "rule-of-thumb": Vicat is responsible for establishing the chemical basis of these cements, and Johnson established the importance of sintering the mix in the kiln.
In the US the first large-scale use of cement was Rosendale cement, a natural cement mined from a massive deposit of a large dolostone rock deposit discovered in the early 19th century near Rosendale, New York. Rosendale cement was extremely popular for the foundation of buildings ("e.g.", Statue of Liberty, Capitol Building, Brooklyn Bridge) and lining water pipes.
Sorel cement was patented in 1867 by Frenchman Stanislas Sorel and was stronger than Portland cement but its poor water restive and corrosive qualities limited its use in building construction. The next development with the manufacture of Portland cement was the introduction of the rotary kiln which allowed a stronger, more homogeneous mixture and a continuous manufacturing process.
Cements in the 20th century.
Calcium aluminate cements were patented in 1908 in France by Jules Bied for better resistance to sulfates.
In the US, the long curing time of at least a month for Rosendale cement made it unpopular after World War One in the construction of highways and bridges and many states and construction firms turned to the use of Portland cement. Because of the switch to Portland cement, by the end of the 1920s of the 15 Rosendale cement companies, only one had survived. But in the early 1930s it was discovered that, while Portland cement had a faster setting time it was not as durable, especially for highways, to the point that some states stopped building highways and roads with cement. Bertrain H. Wait, an engineer whose company had worked on the construction of the New York City's Catskill Aqueduct, was impressed with the durability of Rosendale cement, and came up with a blend of both Rosendale and synthetic cements which had the good attributes of both: it was highly durable and had a much faster setting time. Mr. Wait convinced the New York Commissioner of Highways to construct an experimental section of highway near New Paltz, New York, using one sack of Rosendale to six sacks of synthetic cement. It was proved a success and for decades the Rosendale-synthetic cement blend became common use in highway and bridge construction.
Modern cements.
Modern hydraulic cements began to be developed from the start of the Industrial Revolution (around 1800), driven by three main needs:
Modern cements are often Portland cement or Portland cement blends, but other cements are used in industry.
Portland cement.
Portland cement is by far the most common type of cement in general use around the world. This cement is made by heating limestone (calcium carbonate) with other materials (such as clay) to 1450 °C in a kiln, in a process known as calcination, whereby a molecule of carbon dioxide is liberated from the calcium carbonate to form calcium oxide, or quicklime, which is then blended with the other materials that have been included in the mix to form calcium silicates and other cementitious compounds. The resulting hard substance, called 'clinker', is then ground with a small amount of gypsum into a powder to make 'Ordinary Portland Cement', the most commonly used type of cement (often referred to as OPC).
Portland cement is a basic ingredient of concrete, mortar and most non-specialty grout. The most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load bearing) element. Portland cement may be grey or white.
Portland cement blends.
Portland cement blends are often available as inter-ground mixtures from cement producers, but similar formulations are often also mixed from the ground components at the concrete mixing plant.
Portland blast-furnace slag cement, or Blast furnace cement (ASTM C595 and EN 197-1 nomenclature respectively), contains up to 95% ground granulated blast furnace slag, with the rest Portland clinker and a little gypsum. All compositions produce high ultimate strength, but as slag content is increased, early strength is reduced, while sulfate resistance increases and heat evolution diminishes. Used as an economic alternative to Portland sulfate-resisting and low-heat cements.
Portland-fly ash cement contains up to 40% fly ash under ASTM standards (ASTM C595), or 35% under EN standards (EN 197-1). The fly ash is pozzolanic, so that ultimate strength is maintained. Because fly ash addition allows a lower concrete water content, early strength can also be maintained. Where good quality cheap fly ash is available, this can be an economic alternative to ordinary Portland cement.
Portland pozzolan cement includes fly ash cement, since fly ash is a pozzolan, but also includes cements made from other natural or artificial pozzolans. In countries where volcanic ashes are available (e.g. Italy, Chile, Mexico, the Philippines) these cements are often the most common form in use. The maximum replacement ratios are generally defined as for Portland-fly ash cement.
Portland silica fume cement. Addition of silica fume can yield exceptionally high strengths, and cements containing 5–20% silica fume are occasionally produced, with 10% being the maximum allowed addition under EN 197-1. However, silica fume is more usually added to Portland cement at the concrete mixer.
Masonry cements are used for preparing bricklaying mortars and stuccos, and must not be used in concrete. They are usually complex proprietary formulations containing Portland clinker and a number of other ingredients that may include limestone, hydrated lime, air entrainers, retarders, waterproofers and coloring agents. They are formulated to yield workable mortars that allow rapid and consistent masonry work. Subtle variations of Masonry cement in the US are Plastic Cements and Stucco Cements. These are designed to produce controlled bond with masonry blocks.
Expansive cements contain, in addition to Portland clinker, expansive clinkers (usually sulfoaluminate clinkers), and are designed to offset the effects of drying shrinkage that is normally encountered with hydraulic cements. This allows large floor slabs (up to 60 m square) to be prepared without contraction joints.
White blended cements may be made using white clinker (containing little or no iron) and white supplementary materials such as high-purity metakaolin.
Colored cements are used for decorative purposes. In some standards, the addition of pigments to produce "colored Portland cement" is allowed. In other standards (e.g. ASTM), pigments are not allowed constituents of Portland cement, and colored cements are sold as "blended hydraulic cements".
Very finely ground cements are made from mixtures of cement with sand or with slag or other pozzolan type minerals that are extremely finely ground together. Such cements can have the same physical characteristics as normal cement but with 50% less cement particularly due to their increased surface area for the chemical reaction. Even with intensive grinding they can use up to 50% less energy to fabricate than ordinary Portland cements.
Other cements.
Pozzolan-lime cements. Mixtures of ground pozzolan and lime are the cements used by the Romans, and can be found in Roman structures still standing (e.g. the Pantheon in Rome). They develop strength slowly, but their ultimate strength can be very high. The hydration products that produce strength are essentially the same as those produced by Portland cement.
Slag-lime cements. Ground granulated blast-furnace slag is not hydraulic on its own, but is "activated" by addition of alkalis, most economically using lime. They are similar to pozzolan lime cements in their properties. Only granulated slag (i.e. water-quenched, glassy slag) is effective as a cement component.
Supersulfated cements contain about 80% ground granulated blast furnace slag, 15% gypsum or anhydrite and a little Portland clinker or lime as an activator. They produce strength by formation of ettringite, with strength growth similar to a slow Portland cement. They exhibit good resistance to aggressive agents, including sulfate.
Calcium aluminate cements are hydraulic cements made primarily from limestone and bauxite. The active ingredients are monocalcium aluminate CaAl2O4 (CaO · Al2O3 or CA in Cement chemist notation, CCN) and mayenite Ca12Al14O33 (12 CaO · 7 Al2O3, or C12A7 in CCN). Strength forms by hydration to calcium aluminate hydrates. They are well-adapted for use in refractory (high-temperature resistant) concretes, e.g. for furnace linings.
Calcium sulfoaluminate cements are made from clinkers that include ye'elimite (Ca4(AlO2)6SO4 or C4A3 in Cement chemist's notation) as a primary phase. They are used in expansive cements, in ultra-high early strength cements, and in "low-energy" cements. Hydration produces ettringite, and specialized physical properties (such as expansion or rapid reaction) are obtained by adjustment of the availability of calcium and sulfate ions. Their use as a low-energy alternative to Portland cement has been pioneered in China, where several million tonnes per year are produced. Energy requirements are lower because of the lower kiln temperatures required for reaction, and the lower amount of limestone (which must be endothermically decarbonated) in the mix. In addition, the lower limestone content and lower fuel consumption leads to a CO2 emission around half that associated with Portland clinker. However, SO2 emissions are usually significantly higher.
"Natural" cements correspond to certain cements of the pre-Portland era, produced by burning argillaceous limestones at moderate temperatures. The level of clay components in the limestone (around 30–35%) is such that large amounts of belite (the low-early strength, high-late strength mineral in Portland cement) are formed without the formation of excessive amounts of free lime. As with any natural material, such cements have highly variable properties.
Geopolymer cements are made from mixtures of water-soluble alkali metal silicates and aluminosilicate mineral powders such as fly ash and metakaolin.
Setting and curing.
Cement starts to set when mixed with water which causes a series of hydration chemical reactions. The constituents slowly hydrate and the mineral hydrates solidify; the interlocking of the hydrates gives cement its strength. Contrary to popular perceptions, hydraulic cements do not set by drying out; proper curing requires maintaining the appropriate moisture content during the curing process. If hydraulic cements dry out during curing, the resulting product can be significantly weakened.
Safety issues.
Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is exothermic. As a result, wet cement is strongly caustic (water pH = 13.5) and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. Some trace elements, such as chromium, from impurities naturally present in the raw materials used to produce cement may cause allergic dermatitis. Reducing agents such as ferrous sulfate (FeSO4) are often added to cement to convert the carcinogenic hexavalent chromate (CrO42-) in trivalent chromium (Cr3+), a less toxic chemical species. Cement users need also to wear appropriate gloves and protective clothing.
Cement industry in the world.
In 2010, the world production of hydraulic cement was 3,300 million tonnes. The top three producers were China with 1,800, India with 220, and USA with 63.5 million tonnes for a combined total of over half the world total by the world's three most populated states.
For the world capacity to produce cement in 2010, the situation was similar with the top three states (China, India, and USA) accounting for just under half the world total capacity.
Over 2011 and 2012, global consumption continued to climb, rising to 3585 Mt in 2011 and 3736 Mt in 2012, while annual growth rates eased to 8.3% and 4.2%, respectively.
China, representing an increasing share of world cement consumption, continued to be the main engine of global growth. By 2012, Chinese demand was recorded at 2160 Mt, representing 58% of world consumption. Annual growth rates, which reached 16% in 2010, appear to have softened, slowing to 5–6% over 2011 and 2012, as China’s economy targets a more sustainable growth rate.
Outside of China, worldwide consumption climbed by 4.4% to 1462 Mt in 2010, 5% to 1535 Mt in 2011, and finally 2.7% to 1576 Mt in 2012.
Iran is now the 3rd largest cement producer in the world and has increased its output by over 10% from 2008 to 2011. Due to climbing energy costs in Pakistan and other major cement-producing countries, Iran is a unique position as a trading partner, utilizing its own surplus petroleum to power clinker plants. Now a top producer in the Middle-East, Iran is further increasing its dominant position in local markets and abroad.
The performance in North America and Europe over the 2010–12 period contrasted strikingly with that of China, as the global financial crisis evolved into a sovereign debt crisis for many economies in this region and recession. Cement consumption levels for this region fell by 1.9% in 2010 to 445 Mt, recovered by 4.9% in 2011, then dipped again by 1.1% in 2012.
The performance in the rest of the world, which includes many emerging economies in Asia, Africa and Latin America and representing some 1020 Mt cement demand in 2010, was positive and more than offset the declines in North America and Europe. Annual consumption growth was recorded at 7.4% in 2010, moderating to 5.1% and 4.3% in 2011 and 2012, respectively.
As at year-end 2012, the global cement industry consisted of 5673 cement production facilities, including both integrated and grinding, of which 3900 were located in China and 1773 in the rest of the world.
Total cement capacity worldwide was recorded at 5245 Mt in 2012, with 2950 Mt located in China and 2295 Mt in the rest of the world.
China.
"For the past 18 years, China consistently has produced more cement than any other country in the world. [...] (However,) China's cement export peaked in 1994 with 11 million tonnes shipped out and has been in steady decline ever since. Only 5.18 million tonnes were exported out of China in 2002. Offered at $34 a ton, Chinese cement is pricing itself out of the market as Thailand is asking as little as $20 for the same quality."
In 2006, it was estimated that China manufactured 1.235 billion tonnes of cement, which was 44% of the world total cement production. "Demand for cement in China is expected to advance 5.4% annually and exceed 1 billion tonnes in 2008, driven by slowing but healthy growth in construction expenditures. Cement consumed in China will amount to 44% of global demand, and China will remain the world's largest national consumer of cement by a large margin."
In 2010, 3.3 billion tonnes of cement was consumed globally. Of this, China accounted for 1.8 billion tonnes.
Environmental impacts.
Cement manufacture causes environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust, gases, noise and vibration when operating machinery and during blasting in quarries, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.
CO2 emissions.
Carbon concentration in cement spans from ≈5% in cement structures to ≈8% in the case of roads in cement. Cement manufacturing releases CO2 in the atmosphere both directly when calcium carbonate is heated, producing lime and carbon dioxide, and also indirectly through the use of energy if its production involves the emission of CO2. The cement industry produces about 5% of global man-made CO2 emissions, of which 50% is from the chemical process, and 40% from burning fuel.
The amount of CO2 emitted by the cement industry is nearly 900 kg of CO2 for every 1000 kg of cement produced.
In the European union the specific energy consumption for the production of cement clinker has been reduced by approximately 30% since the 1970s. This reduction in primary energy requirements is equivalent to approximately 11 million tonnes of coal per year with corresponding benefits in reduction of CO2 emissions. This accounts for approximately 5% of anthropogenic CO2.
The high proportion of carbon dioxide produced in the chemical reaction leads to a large decrease in mass in the conversion from limestone to cement. So, to reduce the transport of heavier raw materials and to minimize the associated costs, it is more economical for cement plants to be closer to the limestone quarries rather than to the consumer centers.
In certain applications, lime mortar reabsorbs the same amount of CO2 as was released in its manufacture, and has a lower energy requirement in production than mainstream cement. Newly developed cement types from Novacem and Eco-cement can absorb carbon dioxide from ambient air during hardening. Use of the Kalina cycle during production can also increase energy efficiency.
Heavy metal emissions in the air.
In some circumstances, mainly depending on the origin and the composition of the raw materials used, the high-temperature calcination process of limestone and clay minerals can release in the atmosphere gases and dust rich in volatile heavy metals, a.o, thallium, cadmium and mercury are the most toxic. Heavy metals (Tl, Cd, Hg, ...) are often found as trace elements in common metal sulfides (pyrite (FeS2), zinc blende (ZnS), galena (PbS), ...) present as secondary minerals in most of the raw materials. Environmental regulations exist in many countries to limit these emissions. As of 2011 in the United States, cement kilns are "legally allowed to pump more toxins into the air than are hazardous-waste incinerators."
Heavy metals present in the clinker.
The presence of heavy metals in the clinker arises both from the natural raw materials and from the use of recycled by-products or alternative fuels. The high pH prevailing in the cement porewater (12.5 < pH < 13.5) limits the mobility of many heavy metals by decreasing their solubility and increasing their sorption onto the cement mineral phases. Nickel, zinc and lead are commonly found in cement in non-negligible concentrations.
Use of alternative fuels and by-products materials.
A cement plant consumes 3 to 6 GJ of fuel per tonne of clinker produced, depending on the raw materials and the process used. Most cement kilns today use coal and petroleum coke as primary fuels, and to a lesser extent natural gas and fuel oil. Selected waste and by-products with recoverable calorific value can be used as fuels in a cement kiln (referred to as co-processing), replacing a portion of conventional fossil fuels, like coal, if they meet strict specifications. Selected waste and by-products containing useful minerals such as calcium, silica, alumina, and iron can be used as raw materials in the kiln, replacing raw materials such as clay, shale, and limestone. Because some materials have both useful mineral content and recoverable calorific value, the distinction between alternative fuels and raw materials is not always clear. For example, sewage sludge has a low but significant calorific value, and burns to give ash containing minerals useful in the clinker matrix.
Normal operation of cement kilns provides combustion conditions which are more than adequate for the destruction of even the most difficult to destroy organic substances. This is primarily due to the very high temperatures of the kiln gases (2000 °C in the combustion gas from the main burners and 1100 °C in the gas from the burners in the precalciner). The gas residence time at high temperature in the rotary kiln is of the order of 5–10 seconds and in the precalciner more than 3 seconds.
Due to bovine spongiform encephalopathy (BSE) in the European beef industry, the use of animal-derived products to feed cattle is now severely restricted. Large quantities of waste animal meat and bone meal (MBM), also known as animal flour, have to be safely disposed of or transformed. The production of cement kilns, together with the incineration, is to date one of the two main ways to treat this solid effluent of the food industry.
Green cement.
Green cement is a cementitious material that meets or exceeds the functional performance capabilities of ordinary Portland cement by incorporating and optimizing recycled materials, thereby reducing consumption of natural raw materials, water, and energy, resulting in a more sustainable construction material.
New manufacturing processes for producing green cement are being researched with the goal to reduce, or even eliminate, the production and release of damaging pollutants and greenhouse gasses, particularly CO2.
Growing environmental concerns and increasing cost of fuels of fossil origin have resulted in many countries in sharp reduction of the resources needed to produce cement and effluents (dust and exhaust gases).
Peter Trimble, a design student at the University of Edinburgh has proposed 'DUPE' based on "Sporosarcina pasteurii", a bacterium with binding qualities which, when mixed with sand and urine produces a concrete said to be 70% as strong as conventional materials.

</doc>
<doc id="6671" url="https://en.wikipedia.org/wiki?curid=6671" title="Cincinnati Reds">
Cincinnati Reds

The Cincinnati Reds are an American professional baseball team based in Cincinnati, Ohio. The Reds compete in Major League Baseball (MLB) as a member club of the National League (NL) Central division. The team was a charter member of the American Association (AA) in 1882 and joined the National League in 1890.
The Reds played in the NL West division from 1969 to 1993, before joining the Central division after it was founded in 1994. They have won five World Series titles, nine NL pennants, one AA pennant, and 10 division titles. The team plays its home games at Great American Ball Park, which opened in 2003 replacing Riverfront Stadium. Bob Castellini has been their chief executive officer since 2006.
Franchise history.
The birth of the Reds and the American Association (1881–1889).
The origins of the modern Cincinnati Reds can be traced to the expulsion of an earlier team bearing that name. In 1876, Cincinnati became one of the charter members of the new National League, but the club ran afoul of league organizer and long-time president William Hulbert for selling beer during games and renting out their ballpark on Sundays. Both were important activities to entice the city's large German population. While Hulbert made clear his distaste for both beer and Sunday baseball at the founding of the league, neither practice was actually against league rules in those early years. On October 6, 1880, however, seven of the eight team owners pledged at a special league meeting to formally ban both beer and Sunday baseball at the regular league meeting that December. Only Cincinnati president W. H. Kennett refused to sign the pledge, so the other owners formally expelled Cincinnati for violating a rule that would not actually go into effect for two more months.
Cincinnati's expulsion from the National League incensed "Cincinnati Enquirer" sports editor O. P. Caylor, who made two attempts to form a new league on behalf of the receivers for the now bankrupt Reds franchise. When these attempts failed, he formed a new independent ballclub known as the Red Stockings in the Spring of 1881, and brought the team to St. Louis for a weekend exhibition. The Reds' first game was a 12–3 victory over the St. Louis club. After the 1881 series proved a success, Caylor and a former president of the old Reds named Justus Thorner received an invitation from Philadelphia businessman Horace Phillips to attend a meeting of several clubs in Pittsburgh with the intent of establishing a rival to the National League. Upon arriving in the city, however, Caylor and Thorner discovered that no other owners had decided to accept the invitation, with even Phillips not bothering to attend his own meeting. By chance, the duo met a former pitcher named Al Pratt, who hooked them up with former Pittsburgh Alleghenys president H. Denny McKnight. Together, the three men hatched a scheme to form a new league by sending a telegram to each of the other owners who were supposed to attend the meeting stating that he was the only person who did not attend and that everyone else was enthusiastic about the new venture and eager to attend a second meeting in Cincinnati. The ploy worked, and the American Association was officially formed at the Hotel Gibson in Cincinnati with the new Reds a charter member with Thorner as president.
Led by the hitting of third baseman Hick Carpenter, the defense of future Hall of Fame second baseman Bid McPhee, and the pitching of 40-game-winner Will White, the Reds won the inaugural AA pennant in 1882. With the establishment of the Union Association Justus Thorner left the club to finance the Cincinnati Outlaw Reds and managed to acquire the lease on the Reds Bank Street Grounds playing field, forcing new president Aaron Stern to relocate three blocks away at the hastily built League Park. The club never placed higher than second or lower than fifth for the rest of its tenure in the American Association.
The National League returns to Cincinnati (1890–1911).
The Cincinnati Red Stockings left the American Association on November 14, 1889 and joined the National League along with the Brooklyn Bridegrooms after a dispute with St. Louis Browns owner Chris Von Der Ahe over the selection of a new league president. The National League was happy to accept the teams in part due to the emergence of the new Player's League. This new league, an early failed attempt to break the reserve clause in baseball, threatened both existing leagues. Because the National League decided to expand while the American Association was weakening, the team accepted an invitation to join the National League. It was also at this time that the team first shortened their name from "Red Stockings" to "Reds". The Reds wandered through the 1890s signing local stars and aging veterans. During this time, the team never finished above third place (1897) and never closer than 10½ games (1890).
At the start of the 20th century, the Reds had hitting stars Sam Crawford and Cy Seymour. Seymour's .377 average in 1905 was the first individual batting crown won by a Red. In 1911, Bob Bescher stole 81 bases, which is still a team record. Like the previous decade, the 1900s (decade) were not kind to the Reds, as much of the decade was spent in the league's second division.
Redland Field to the Great Depression (1912–1932).
In 1912, the club opened a new steel-and-concrete ballpark, Redland Field (later to be known as Crosley Field). The Reds had been playing baseball on that same site, the corner of Findlay and Western Avenues on the city's west side, for 28 years, in wooden structures that had been occasionally damaged by fires. By the late 1910s the Reds began to come out of the second division. The 1918 team finished fourth, and new manager Pat Moran led the Reds to an NL pennant in 1919, in what the club advertised as its "Golden Anniversary". The 1919 team had hitting stars Edd Roush and Heinie Groh while the pitching staff was led by Hod Eller and left-hander Harry "Slim" Sallee. The Reds finished ahead of John McGraw's New York Giants, and then won the world championship in eight games over the Chicago White Sox.
By 1920, the "Black Sox" scandal had brought a taint to the Reds' first championship. After 1926, and well into the 1930s, the Reds were second division dwellers. Eppa Rixey, Dolf Luque and Pete Donohue were pitching stars, but the offense never lived up to the pitching. By 1931, the team was bankrupt, the Great Depression was in full swing and Redland Field was in a state of disrepair.
Championship baseball and revival (1933–1940).
Powel Crosley, Jr., an electronics magnate who, with his brother Lewis M. Crosley, produced radios, refrigerators, and other household items, bought the Reds out of bankruptcy in 1933, and hired Larry MacPhail to be the General Manager. Crosley had started WLW radio, the Reds flagship radio broadcaster, and the Crosley Broadcasting Corporation in Cincinnati, where he was also a prominent civic leader. MacPhail began to develop the Reds' minor league system and expanded the Reds' fan base. The Reds, throughout the 1930s, became a team of "firsts". The now-renamed Crosley Field became the host of the first night game in 1935, which was also the first baseball fireworks night, the fireworks at the game were shot by Joe Rozzi of Rozzi's Famous Fireworks. Johnny Vander Meer became the only pitcher in major league history to throw back-to-back no-hitters in 1938. Thanks to Vander Meer, Paul Derringer and second baseman/third baseman-turned-pitcher Bucky Walters, the Reds had a solid pitching staff. The offense came around in the late 1930s. By 1938 the Reds, now led by manager Bill McKechnie, were out of the second division finishing fourth. Ernie Lombardi was named the National League's Most Valuable Player in 1938. By 1939, they were National League champions, but in the World Series, they were swept by the New York Yankees. In 1940, they repeated as NL Champions, and for the first time in 21 years, the Reds captured a World championship, beating the Detroit Tigers 4 games to 3. Frank McCormick was the 1940 NL MVP. Other position players included Harry Craft, Lonny Frey, Ival Goodman, Lew Riggs and Bill Werber.
1941–1969.
World War II and age finally caught up with the Reds. Throughout the 1940s and early 1950s, Cincinnati finished mostly in the second division. In 1944, Joe Nuxhall (who was later to become part of the radio broadcasting team), at age 15, pitched for the Reds on loan from Wilson Junior High school in Hamilton, Ohio. He became the youngest player ever to appear in a major league game—a record that still stands today. Ewell "The Whip" Blackwell was the main pitching stalwart before arm problems cut short his career. Ted Kluszewski was the NL home run leader in 1954. The rest of the offense was a collection of over-the-hill players and not-ready-for-prime-time youngsters.
In April 1953, the Reds, fearing that their traditional club nickname would associate them with the threat of Communism, officially changed the name of the team to the "Cincinnati Redlegs". From 1956 to 1960, the club's logo was altered to remove the term "REDS" from the inside of the "wishbone "C" symbol. The "REDS" reappeared on the 1961 uniforms, but the point of the "C" was removed, leaving a smooth, non-wishbone curve. The traditional home-uniform logo was restored in 1967.
In 1956, led by National League Rookie of the Year Frank Robinson, the Redlegs hit 221 HR to tie the NL record. By 1961, Robinson was joined by Vada Pinson, Wally Post, Gordy Coleman, and Gene Freese. Pitchers Joey Jay, Jim O'Toole, and Bob Purkey led the staff.
The Reds captured the 1961 National League pennant, holding off the Los Angeles Dodgers and the San Francisco Giants, only to be defeated by the perennially powerful New York Yankees in the World Series.
The Reds had winning teams during the rest of the 1960s, but did not produce any championships. They won 98 games in 1962, paced by Purkey's 23, but finished third. In 1964, they lost the pennant by one game to the Cardinals after having taken first place when the Phillies collapsed in September. Their beloved manager Fred Hutchinson died of cancer just weeks after the end of the 1964 season. The failure of the Reds to win the 1964 pennant led to owner Bill DeWitt's selling off key components of the team, in anticipation of relocating the franchise. In response to DeWitt's threatened move, the women of Cincinnati banded together to form the Rosie Reds to urge DeWitt to keep the franchise in Cincinnati. The Rosie Reds are still in existence, and are currently the oldest fan club in Major League Baseball. After the 1965 season he executed what may be the most lopsided trade in baseball history, sending former Most Valuable Player Frank Robinson to the Baltimore Orioles for pitchers Milt Pappas and Jack Baldschun, and outfielder Dick Simpson. Robinson went on to win the MVP and triple crown in the American league for 1966, and lead Baltimore to its first ever World Series title in a sweep of the Los Angeles Dodgers. The Reds did not recover from this trade until the rise of the "Big Red Machine" of the 1970s.
Starting in the early 1960s, the Reds' farm system began producing a series of stars, including Jim Maloney (the Reds' pitching ace of the 1960s), Pete Rose, Tony Pérez, Johnny Bench, Lee May, Tommy Helms, Bernie Carbo, Hal McRae, Dave Concepción, and Gary Nolan. The tipping point came in 1967 with the appointment of Bob Howsam as general manager. That same year the Reds avoided a move to San Diego when the city of Cincinnati and Hamilton County agreed to build a state of the art, downtown stadium on the edge of the Ohio River. The Reds entered into a 30-year lease in exchange for the stadium commitment keeping the franchise in its original home city. In a series of strategic moves, Howsam brought in key personnel to complement the homegrown talent. The Reds' final game at Crosley Field, home to more than 4,500 baseball games, was played on June 24, 1970, a 5–4 victory over the San Francisco Giants.
Under Howsam's administration starting in the late 1960s, the Reds instituted a strict rule barring the team's players from wearing facial hair and long hair. The clean cut look was meant to present the team as wholesome in an era of turmoil. All players coming to the Reds were required to shave and cut their hair for the next three decades. Over the years, the rule was controversial, but persisted well into the ownership of Marge Schott. On at least one occasion, in the early 1980s, enforcement of this rule lost them the services of star reliever and Ohio native Rollie Fingers, who would not shave his trademark handlebar mustache in order to join the team. The rule was not officially rescinded until 1999 when the Reds traded for slugger Greg Vaughn, who had a goatee. The New York Yankees continue to have a similar rule today, though unlike the Reds during this period, Yankees players are permitted to have mustaches. Much like when players leave the Yankees today, players who left the Reds took advantage with their new teams; Pete Rose, for instance, grew his hair out much longer than would be allowed by the Reds once he signed with the Philadelphia Phillies in 1979.
The Reds' rules also included conservative uniforms. In Major League Baseball, a club generally provides most of the equipment and clothing needed for play. However, players are required to supply their gloves and shoes themselves. Many players enter into sponsorship arrangements with shoe manufacturers, but through the mid-1980s, the Reds had a strict rule that players were to wear only plain black shoes with no prominent logo. Reds players decried what they considered to be the boring color choice as well as the denial of the opportunity to earn more money through shoe contracts. A compromise was struck in which players were allowed to wear red shoes.
The Big Red Machine (1970–1976).
In , little known George "Sparky" Anderson was hired as manager, and the Reds embarked upon a decade of excellence, with a team that came to be known as "The Big Red Machine". Playing at Crosley Field until June 30, 1970, when the Reds moved into brand-new Riverfront Stadium, a 52,000 seat multi-purpose venue on the shores of the Ohio River, the Reds began the 1970s with a bang by winning 70 of their first 100 games. Johnny Bench, Tony Pérez, Pete Rose, Lee May and Bobby Tolan were the early Red Machine offensive leaders; Gary Nolan, Jim Merritt, Wayne Simpson and Jim McGlothlin led a pitching staff which also contained veterans Tony Cloninger and Clay Carroll and youngsters Pedro Borbón and Don Gullett. The Reds breezed through the 1970 season, winning the NL West and captured the NL pennant by sweeping the Pittsburgh Pirates in three games. By the time the club got to the World Series, however, the Reds pitching staff had run out of gas and the veteran Baltimore Orioles beat the Reds in five games.
After the disastrous season (the only season of the 1970s during which the Reds finished with a losing record) the Reds reloaded by trading veterans Jimmy Stewart, May, and Tommy Helms for Joe Morgan, César Gerónimo, Jack Billingham, Ed Armbrister, and Denis Menke. Meanwhile, Dave Concepción blossomed at shortstop. 1971 was also the year a key component of the future world championships was acquired in George Foster from the San Francisco Giants in a trade for shortstop Frank Duffy.
The Reds won the NL West in baseball's first ever strike-shortened season and defeated the Pittsburgh Pirates in an exciting five-game playoff series. They then faced the Oakland Athletics in the World Series. Six of the seven games were won by one run. With powerful slugger Reggie Jackson sidelined due to an injury incurred during Oakland's playoff series, Ohio native Gene Tenace got a chance to play in the series, delivering four home runs that tied the World Series record for homers, propelling Oakland to a dramatic seven-game series win. This was one of the few World Series in which no starting pitcher for either side pitched a complete game.
The Reds won a third NL West crown in after a dramatic second half comeback, that saw them make up games on the Los Angeles Dodgers after the All-Star break. However they lost the NL pennant to the New York Mets in five games in the NLCS. In game one, Tom Seaver faced Jack Billingham in a classic pitching duel, with all three runs of the 2–1 margin being scored on home runs. John Milner provided New York's run off Billingham, while Pete Rose tied the game in the seventh inning off Seaver, setting the stage for a dramatic game ending home run by Johnny Bench in the bottom of the ninth. The New York series provided plenty of controversy with the riotous behavior of Shea Stadium fans towards Pete Rose when he and Bud Harrelson scuffled after a hard slide by Rose into Harrelson at second base during the fifth inning of Game 3. A full bench-clearing fight resulted after Harrelson responded to Rose's aggressive move to prevent him from completing a double play by calling him a name. This also led to two more incidents in which play was stopped. The Reds trailed 9–3 and New York's manager, Yogi Berra, and legendary outfielder Willie Mays, at the request of National League president Warren Giles, appealed to fans in left field to restrain themselves. The next day the series was extended to a fifth game when Rose homered in the 12th inning to tie the series at two games each.
The Reds won 98 games in but they finished second to the 102-win Los Angeles Dodgers. The 1974 season started off with much excitement, as the Atlanta Braves were in town to open the season with the Reds. Hank Aaron entered opening day with 713 home runs, one shy of tying Babe Ruth's record of 714. The first pitch Aaron swung at in the 1974 season was the record tying home run off Jack Billingham. The next day the Braves benched Aaron, hoping to save him for his record breaking home run on their season opening homestand. The commissioner of baseball, Bowie Kuhn, ordered Braves management to play Aaron the next day, where he narrowly missed the historic home run in the fifth inning. Aaron went on to set the record in Atlanta two nights later. 1974 also was the debut of Hall of Fame radio announcer Marty Brennaman, who replaced Al Michaels, after Michaels left the Reds to broadcast for the San Francisco Giants.
With 1975, the Big Red Machine lineup solidified with the "Great Eight" starting team of Johnny Bench (catcher), Tony Pérez (first base), Joe Morgan (second base), Dave Concepción (shortstop), Pete Rose (third base), Ken Griffey (right field), César Gerónimo (center field), and George Foster (left field). The starting pitchers included Don Gullett, Fred Norman, Gary Nolan, Jack Billingham, Pat Darcy, and Clay Kirby. The bullpen featured Rawly Eastwick and Will McEnaney combining for 37 saves, and veterans Pedro Borbón and Clay Carroll. On Opening Day, Rose still played in left field, Foster was not a starter, while John Vukovich, an off-season acquisition, was the starting third baseman. While Vuckovich was a superb fielder, he was a weak hitter. In May, with the team off to a slow start and trailing the Dodgers, Sparky Anderson made a bold move by moving Rose to third base, a position where he had very little experience, and inserting Foster in left field. This was the jolt that the Reds needed to propel them into first place, with Rose proving to be reliable on defense, while adding Foster to the outfield gave the offense some added punch. During the season, the Reds compiled two notable streaks: (1) by winning 41 out of 50 games in one stretch, and (2) by going a month without committing any errors on defense.
In the 1975 season, Cincinnati clinched the NL West with 108 victories, then swept the Pittsburgh Pirates in three games to win the NL pennant. In the World Series, the Boston Red Sox were the opponents. After splitting the first four games, the Reds took Game 5. After a three-day rain delay, the two teams met in Game 6, one of the most memorable baseball games ever played and considered by many to be the best World Series game ever. The Reds were ahead 6–3 with 5 outs left, when the Red Sox tied the game on former Red Bernie Carbo's three-run home run. It was Carbo's second pinch-hit three-run homer in the series. After a few close-calls either way, Carlton Fisk hit a dramatic 12th inning home run off the foul pole in left field to give the Red Sox a 7–6 win and force a deciding Game 7. Cincinnati prevailed the next day when Morgan's RBI single won Game 7 and gave the Reds their first championship in 35 years. The Reds have not lost a World Series game since Carlton Fisk's home run, a span of 9 straight wins.
1976 saw a return of the same starting eight in the field. The starting rotation was again led by Nolan, Gullett, Billingham, and Norman, while the addition of rookies Pat Zachry and Santo Alcalá comprised an underrated staff in which four of the six had ERAs below 3.10. Eastwick, Borbon, and McEnaney shared closer duties, recording 26, 8, and 7 saves respectively. The Reds won the NL West by ten games. They went undefeated in the postseason, sweeping the Philadelphia Phillies (winning Game 3 in their final at-bat) to return to the World Series. They continued to dominate by sweeping the Yankees in the newly renovated Yankee Stadium, the first World Series games played in Yankee Stadium since 1964. This was only the second ever sweep of the Yankees in the World Series. In winning the Series, the Reds became the first NL team since the 1921–22 New York Giants to win consecutive World Series championships, and the Big Red Machine of 1975–76 is considered one of the best teams ever. So far in MLB history, the 1975 and '76 Reds were the last NL team to repeat as champions.
Beginning with the 1970 National League pennant, the Reds beat either the Philadelphia Phillies or the Pittsburgh Pirates to win their pennants (Pirates in 1970, 1972, 1975, and 1990, Phillies in 1976), making The Big Red Machine part of the rivalry between the two Pennsylvania teams. In 1979, Pete Rose added further fuel in The Big Red Machine being part of the rivalry when he signed with the Phillies and helped them win their first World Series championship in .
The Machine dismantled (1977–1989).
The later years of the 1970s brought turmoil and change. Popular Tony Pérez was sent to Montreal after the 1976 season, breaking up the Big Red Machine's starting lineup. Manager Sparky Anderson and General Manager Bob Howsam later considered this trade the biggest mistake of their careers. Starting pitcher Don Gullett left via free agency and signed with the New York Yankees. In an effort to fill that gap, a trade with the Oakland A's for starting ace Vida Blue was arranged during the 1976–77 off-season. However, Bowie Kuhn, the Commissioner of Baseball, vetoed the trade for the stated reason of maintaining competitive balance in baseball. Some have suggested that the actual reason had more to due with Kuhn's continued feud with Oakland A's owner Charlie Finley. On June 15, 1977, the Reds acquired Mets' franchise pitcher Tom Seaver for Pat Zachry, Doug Flynn, Steve Henderson, and Dan Norman. In other deals that proved to be less successful, the Reds traded Gary Nolan to the Angels for Craig Hendrickson, Rawly Eastwick to St. Louis for Doug Capilla and Mike Caldwell to Milwaukee for Rick O'Keeffe and Garry Pyka, and got Rick Auerbach from Texas. The end of the Big Red Machine era was heralded by the replacement of General Manager Bob Howsam with Dick Wagner.
In Rose's last season as a Red, he gave baseball a thrill as he challenged Joe DiMaggio's 56-game hitting streak, tying for the second-longest streak ever at 44 games. The streak came to an end in Atlanta after striking out in his fifth at bat in the game against Gene Garber. Rose also earned his 3,000th hit that season, on his way to becoming baseball's all-time hits leader when he rejoined the Reds in the mid-1980s. The year also witnessed the only no-hitter of Hall of Fame pitcher Tom Seaver's career, coming against the St. Louis Cardinals on June 16, 1978.
After the 1978 season and two straight second-place finishes, Wagner fired manager Anderson—an unpopular move. Pete Rose, who since 1963 had played almost every position for the team except pitcher and catcher, signed with Philadelphia as a free agent. By 1979, the starters were Bench (c), Dan Driessen (1b), Morgan (2b), Concepción (ss), Ray Knight (3b), with Griffey, Foster, and Geronimo again in the outfield. The pitching staff had experienced a complete turnover since 1976 except for Fred Norman. In addition to ace starter Tom Seaver; the remaining starters were Mike LaCoss, Bill Bonham, and Paul Moskau. In the bullpen, only Borbon had remained. Dave Tomlin and Mario Soto worked middle relief with Tom Hume and Doug Bair closing. The Reds won the 1979 NL West behind the pitching of Tom Seaver but were dispatched in the NL playoffs by Pittsburgh. Game 2 featured a controversial play in which a ball hit by Pittsburgh's Phil Garner was caught by Cincinnati outfielder Dave Collins but was ruled a trap, setting the Pirates up to take a 2–1 lead. The Pirates swept the series 3 games to 0 and went on to win the World Series against the Baltimore Orioles.
The 1981 team fielded a strong lineup, but with only Concepción, Foster, and Griffey retaining their spots from the 1975–76 heyday. After Johnny Bench was able to play only a few games at catcher each year after 1980 due to ongoing injuries, Joe Nolan took over as starting catcher. Driessen and Bench shared 1st base, and Knight starred at third. Morgan and Geronimo had been replaced at second base and center field by Ron Oester and Dave Collins. Mario Soto posted a banner year starting on the mound, only surpassed by the outstanding performance of Seaver's Cy Young runner-up season. La Coss, Bruce Berenyi, and Frank Pastore rounded out the starting rotation. Hume again led the bullpen as closer, joined by Bair and Joe Price. In , Cincinnati had the best overall record in baseball, but they finished second in the division in both of the half-seasons that were created after a mid-season players' strike, and missed the playoffs. To commemorate this, a team photo was taken, accompanied by a banner that read "Baseball's Best Record 1981".
By , the Reds were a shell of the original Red Machine; they lost 101 games that year. Johnny Bench, after an unsuccessful transition to 3rd base, retired a year later.
After the heartbreak of 1981, General Manager Dick Wagner pursued the strategy of ridding the team of veterans including third-baseman Knight and the entire starting outfield of Griffey, Foster, and Collins. Bench, after being able to catch only seven games in 1981, was moved from platooning at first base to be the starting third baseman; Alex Treviño became the regular starting catcher. The outfield was staffed with Paul Householder, César Cedeño, and future Colorado Rockies & Pittsburgh Pirates manager Clint Hurdle on opening day. Hurdle was an immediate bust, and rookie Eddie Milner took his place in the starting outfield early in the year. The highly touted Householder struggled throughout the year despite extensive playing time. Cedeno, while providing steady veteran play, was a disappointment, and was unable to recapture his glory days with the Houston Astros. The starting rotation featured the emergence of a dominant Mario Soto, and featured strong years by Pastore and Bruce Berenyi, but Seaver was injured all year, and their efforts were wasted without a strong offensive lineup. Tom Hume still led the bullpen, along with Joe Price. But the colorful Brad "The Animal" Lesley was unable to consistently excel, and former all-star Jim Kern was a big disappointment. Kern was also publicly upset over having to shave off his prominent beard to join the Reds, and helped force the issue of getting traded during mid-season by growing it back.
The Reds fell to the bottom of the Western Division for the next few years. After the 1982 season, Seaver was traded back to the Mets. The year 1983 found Dann Bilardello behind the plate, Bench returning to part-time duty at first base, rookies Nick Esasky taking over at third base and Gary Redus taking over from Cedeno. Tom Hume's effectiveness as a closer had diminished, and no other consistent relievers emerged. Dave Concepción was the sole remaining starter from the Big Red Machine era.
Wagner's tenure ended in 1983, when Howsam, the architect of the Big Red Machine, was brought back. The popular Howsam began his second term as Reds' General Manager by signing Cincinnati native Dave Parker as a free agent from Pittsburgh. In the Reds began to move up, depending on trades and some minor leaguers. In that season Dave Parker, Dave Concepción and Tony Pérez were in Cincinnati uniforms. In August 1984, Pete Rose was reacquired and hired to be the Reds player-manager. After raising the franchise from the grave, Howsam gave way to the administration of Bill Bergesch, who attempted to build the team around a core of highly regarded young players in addition to veterans like Parker. However, he was unable to capitalize on an excess of young and highly touted position players including Kurt Stillwell, Tracy Jones, and Kal Daniels by trading them for pitching. Despite the emergence of Tom Browning as rookie of the year in 1985 when he won 20 games, the rotation was devastated by the early demise of Mario Soto's career to arm injury.
Under Bergesch, from –89 the Reds finished second four times. Among the highlights, Rose became the all-time hits leader, Tom Browning threw a perfect game, Eric Davis became the first player in baseball history to hit at least 35 home runs and steal 50 bases, and Chris Sabo was the 1988 National League Rookie of the Year. The Reds also had a bullpen star in John Franco, who was with the team from 1984 to 1989. Rose once had Concepción pitch late in a game at Dodger Stadium. Following the release of the Dowd Report which accused Rose for betting on baseball games, in Rose was banned from baseball by Commissioner Bart Giamatti, who declared Rose guilty of "conduct detrimental to baseball". Controversy also swirled around Reds owner Marge Schott, who was accused several times of ethnic and racial slurs.
World Championship and the end of an era (1990–2002).
In , General Manager Bergesch was replaced by Murray Cook, who initiated a series of deals that would finally bring the Reds back to the championship, starting with acquisitions of Danny Jackson and José Rijo. An aging Dave Parker was let go after a revival of his career in Cincinnati following the Pittsburgh drug trials. Barry Larkin emerged as the starting shortstop over Kurt Stillwell, who along with reliever Ted Power, was traded for Jackson. In , Cook was succeeded by Bob Quinn, who put the final pieces of the championship puzzle together, with the acquisitions of Hal Morris, Billy Hatcher, and Randy Myers.
In , the Reds under new manager Lou Piniella shocked baseball by leading the NL West from wire-to-wire. They started off 33–12, winning their first nine games, and maintained their lead throughout the year. Led by Chris Sabo, Barry Larkin, Eric Davis, Paul O'Neill and Billy Hatcher in the field, and by José Rijo, Tom Browning and the "Nasty Boys" of Rob Dibble, Norm Charlton and Randy Myers on the mound, the Reds took out the Pirates in the NLCS. The Reds swept the heavily favored Oakland Athletics in four straight, and extended a Reds winning streak in the World Series to 9 consecutive games. The World Series, however, saw Eric Davis severely bruise a kidney diving for a fly ball in Game 4, and his play was greatly limited the next year. In winning the World Series the Reds became the only National League team to go wire to wire.
In , Quinn was replaced in the front office by Jim Bowden. On the field, manager Lou Piniella wanted outfielder Paul O'Neill to be a power-hitter to fill the void Eric Davis left when he was traded to the Los Angeles Dodgers in exchange for Tim Belcher. However, O'Neill only hit .246 and 14 homers. The Reds returned to winning after a losing season in 1991, but 90 wins was only enough for 2nd place behind the division-winning Atlanta Braves. Before the season ended, Piniella got into an altercation with reliever Rob Dibble. In the off season, Paul O'Neill was traded to the New York Yankees for outfielder Roberto Kelly. Kelly was a disappointment for the Reds over the next couple of years, while O'Neill blossomed, leading a down-trodden Yankees franchise to a return to glory. Also, the Reds would replace their outdated "Big Red Machine" era uniforms in favor of a pinstriped uniform with no sleeves.
For the 1993 season Piniella was replaced by fan favorite Tony Pérez, but he lasted only 44 games at the helm, replaced by Davey Johnson. With Johnson steering the team, the Reds made steady progress upward. In 1994, the Reds were in the newly created National League Central Division with the Chicago Cubs, St. Louis Cardinals, as well as fellow rivals Pittsburgh Pirates and Houston Astros. By the time the strike hit, the Reds finished a half-game ahead of the Astros for first-place in the NL Central. By , the Reds won the division thanks to Most Valuable Player Barry Larkin. After defeating the NL West champion Dodgers in the first NLDS since 1981, they lost to the Atlanta Braves.
Team owner Marge Schott announced mid-season that Johnson would be gone by the end of the year, regardless of outcome, to be replaced by former Reds third baseman Ray Knight. Johnson and Schott had never gotten along and she did not approve of Johnson living with his fiancée before they were married, In contrast, Knight, along with his wife, professional golfer Nancy Lopez, were friends of Schott. The team took a dive under Knight and he was unable to complete two full seasons as manager, subject to complaints in the press about his strict managerial style.
In the Reds won 96 games, led by manager Jack McKeon, but lost to the New York Mets in a one game playoff. Earlier that year, Schott sold controlling interest in the Reds to Cincinnati businessman Carl Lindner. Despite an 85–77 finish in 2000, and being named 1999 NL manager of the year, McKeon was fired after the 2000 season. The Reds did not have another winning season until 2010.
Contemporary era (2003–).
Riverfront Stadium, then known as Cinergy Field, was demolished in . Great American Ball Park opened in with high expectations for a team led by local favorites, including outfielder Ken Griffey, Jr., shortstop Barry Larkin, and first baseman Sean Casey. Although attendance improved considerably with the new ballpark, the team continued to lose. Schott had not invested much in the farm system since the early 1990s, leaving the team relatively thin on talent. After years of promises that the club was rebuilding toward the opening of the new ballpark, General Manager Jim Bowden and manager Bob Boone were fired on July 28. This broke up the father-son combo of manager Bob Boone and third baseman Aaron Boone, and Aaron was soon traded to the New York Yankees. Following the season Dan O'Brien was hired as the Reds' 16th General Manager.
The and seasons continued the trend of big hitting, poor pitching, and poor records. Griffey, Jr. joined the 500 home run club in 2004, but was again hampered by injuries. Adam Dunn emerged as consistent home run hitter, including a home run against José Lima. He also broke the major league record for strikeouts in 2004. Although a number of free agents were signed before 2005, the Reds were quickly in last place and manager Dave Miley was forced out in the 2005 mid season and replaced by Jerry Narron. Like many other small market clubs, the Reds dispatched some of their veteran players and began entrusting their future to a young nucleus that included Adam Dunn and Austin Kearns.
Late summer 2004 saw the opening of the Cincinnati Reds Hall of Fame (HOF). The Reds HOF had been in existence in name only since the 1950s, with player plaques, photos and other memorabilia scattered throughout their front offices. Ownership and management desired a stand-alone facility, where the public could walk through inter-active displays, see locker room recreations, watch videos of classic Reds moments and peruse historical items. The first floor houses a movie theater which resembles an older, ivy-covered brick wall ball yard. The hallways contain many vintage photographs. The rear of the building features a three-story wall containing a baseball for every hit Pete Rose had during his career. The third floor contains interactive exhibits including a pitcher's mound, radio booth, and children's area where the fundamentals of baseball are taught through videos featuring former Reds players.
Robert Castellini took over as controlling owner from Lindner in 2006. Castellini promptly fired general manager Dan O'Brien and hired Wayne Krivsky. The Reds made a run at the playoffs but ultimately fell short. The 2007 season was again mired in mediocrity. Midway through the season Jerry Narron was fired as manager and replaced by Pete Mackanin. The Reds ended up posting a winning record under Mackanin, but finished the season in 5th place in the Central Division. Mackanin was manager in an interim capacity only, and the Reds, seeking a big name to fill the spot, ultimately brought in Dusty Baker. Early in the 2008 season, Krivsky was fired and replaced by Walt Jocketty. Though the Reds did not win under Krivsky, he is credited with revamping the farm system and signing young talent that could potentially lead the Reds to success in the future.
The Reds failed to post winning records in both 2008 and 2009. In 2010, with NL MVP Joey Votto and Gold Glovers Brandon Phillips and Scott Rolen the Reds posted a 91-71 record and were NL Central champions. The following week, the Reds became only the second team in MLB history to be no-hit in a postseason game when Philadelphia's Roy Halladay shut down the National League's number one offense in game one of the NLDS. The Reds lost in a 3-game sweep of the NLDS for Philadelphia.
After coming off their surprising 2010 NL Central Division Title, the Reds fell short of many expectations for the 2011 season. Multiple injuries and inconsistent starting pitching played a big role in their mid-season collapse, along with a less productive offense as compared to the previous year. The Reds ended the season at 79-83. The Reds won the 2012 NL Central Division Title. On September 28, Homer Bailey threw a 1-0 no-hitter against the Pittsburgh Pirates at PNC Park, this was the first Reds no-hitter since Tom Browning's perfect game in September of the 1988 season. Finishing with a 97–65 record, they earned the second seed in the Division Series and a match-up with the eventual World Series champion San Francisco Giants. After taking a 2–0 lead with road victories at AT&T Park, they headed home looking to win the series. However, they lost three straight at their home ballpark to become the first National League team since the Cubs in 1984 to lose a division series after leading 2–0.
In the off-season, the team traded outfielder Drew Stubbs, as part of a three team deal with the Arizona Diamondbacks and Cleveland Indians, to the Indians, and in turn received right fielder Shin-Soo Choo. On July 2, 2013, Homer Bailey pitched a no-hitter against the San Francisco Giants for a 4-0 Reds victory, making Bailey the third pitcher in Reds history with two complete game no-hitters in their career.
Following six consecutive losses to close out the 2013 season, including a loss to the Pittsburgh Pirates, at PNC Park, in the National League wild-card playoff game, the Reds decided to fire Dusty Baker. During his six years as manager, Baker led the Reds to the playoff three times; however, they never advanced beyond the first round.
On October 22, 2013, the Reds hired pitching coach Bryan Price to replace Baker as manager.
Under Bryan Price, the Reds were lead by pitchers Johnny Cueto and the hard-throwing Cuban Aroldis Chapman. While the offense was led by all-star third baseman Todd Frazier, Joey Votto, and Brandon Phillips. Although with plenty of star power, the Reds never got off to a good start and ending the season in lowly fourth place in the division to go along with a 76-86 record. During the offseason, the Reds traded pitchers Alfredo Simón to the Tigers and Mat Latos to the Marlins. In return, they acquired young talents such as Eugenio Suárez and Anthony DeSclafani They also acquired veteran slugger Marlon Byrd from the Phillies to play left field.
The Reds' 2015 season wasn't much better, as they finished with the second worst record in the league with a record of 64-98. Their worst finish since 1982. Reds were forced to trade star pitchers Johnny Cueto (to the Kansas City Royals) and Mike Leake (to the San Francisco Giants), receiving minor league pitching prospects for both. Shortly after the season's end, the Reds traded home run derby champion Todd Frazier to the Chicago White Sox, and closing pitcher Aroldis Chapman to the New York Yankees.
Ballpark.
The Cincinnati Reds play their home games at Great American Ball Park, located at 100 Joe Nuxhall Way, in downtown Cincinnati. Great American Ball Park opened in 2003 at the cost of $290 million and has a capacity of 42,271. Along with serving as the home field for the Reds, the stadium also holds the Cincinnati Reds Hall of Fame. The Hall of Fame was added as a part of Reds tradition allowing fans to walk through the history of the franchise as well as participating in many interactive baseball features.
Great American Ball Park is the seventh home of the Cincinnati Reds, built immediately to the north of the site on which Riverfront Stadium, later named Cinergy Field, once stood. The first ballpark the Reds occupied was Bank Street Grounds from 1882–1883 until they moved to League Park I in 1884, where they would remain until 1893. Through the late 1890s and early 1900s (decade), the Reds moved to two different parks where they stayed for less than ten years. League Park II was the third home field for the Reds from 1894–1901, and then moved to the Palace of the Fans which served as the home of the Reds in the 1910s. It was in 1912 that the Reds moved to Crosley Field which they called home for fifty-eight years. Crosley served as the home field for the Reds for two World Series titles and five National League pennants. Beginning June 30, 1970, and during the dynasty of the Big Red Machine, the Reds played in Riverfront Stadium, appropriately named due to its location right by the Ohio River. Riverfront saw three World Series titles and five National League pennants. It was in the late 1990s that the city agreed to build two separate stadiums on the riverfront for the Reds and the Cincinnati Bengals. Thus, in 2003, the Reds began a new era with the opening of the current stadium.
The Reds hold their spring training in Goodyear, Arizona at Goodyear Ballpark. The Reds moved into this stadium and the Cactus League in 2010 after staying in the Grapefruit League for most of their history. The Reds share Goodyear Park with their rivals in Ohio, the Cleveland Indians.
Logos and uniforms.
Logo.
Throughout the history of the Cincinnati Reds, many different variations of the classic wishbone "C" logo have been introduced. For most of the history of the Reds, especially during the early history, the Reds logo has been simply the wishbone "C" with the word "REDS" inside, the only colors used being red and white. However, during the 1950s, during the renaming and re-branding of the team as the Cincinnati Redlegs because of the connections to communism of the word 'Reds', the color blue was introduced as part of the Reds color combination. During the 1960s and 1970s the Reds saw a move towards the more traditional colors, abandoning the navy blue. A new logo also appeared with the new era of baseball in 1972, when the team went away from the script "REDS" inside of the "C", instead, putting their mascot Mr. Redlegs in its place as well as putting the name of the team inside of the wishbone "C". In the 1990s the more traditional, early logos of Reds came back with the current logo reflecting more of what the team's logo was when they were first founded.
Uniform.
Along with the logo, the Reds' uniforms have been changed many different times throughout their history. Following their departure from being called the "Redlegs" in 1956 the Reds made a groundbreaking change to their uniforms with the use of sleeveless jerseys, seen only once before in the Major Leagues by the Chicago Cubs. At home and away, the cap was all-red with a white wishbone C insignia. The long-sleeved undershirts were red. The uniform was plain white with a red wishbone C logo on the left and the uniform number on the right. On the road the wishbone C was replaced by the mustachioed "Mr. Red" logo, the pillbox-hat-wearing man with a baseball for a head. The home stockings were red with six white stripes. The away stockings had only three white stripes.
The Reds changed uniforms again in 1961, when they replaced the traditional wishbone C insignia with an oval C logo, but continued to use the sleeveless jerseys. At home, the Reds wore white caps with the red bill with the oval C in red, white sleeveless jerseys with red pinstripes, with the oval C-REDS logo in black with red lettering on the left breast and the number in red on the right. The gray away uniform included a gray cap with the red oval C and a red bill. Their gray away uniforms, which also included a sleeveless jersey, bore CINCINNATI in an arched block style across with the number below on the left. In 1964, players' last names were placed on the back of each set of uniforms, below the numbers. Those uniforms were scrapped after the 1966 season.
However, the Cincinnati uniform design most familiar to baseball enthusiasts is the one whose basic form, with minor variations, held sway for the 26 seasons from 1967 to 1992. Most significantly, the point was restored to the C insignia, making it a wishbone again. During this era, the Reds wore all-red caps both at home and on the road. The caps bore the simple wishbone C insignia in white. The uniforms were standard short-sleeved jerseys and standard trousers—white at home and grey on the road. The home uniform featured the Wishbone C-REDS logo in red with white type on the left breast and the uniform number in red on the right. The away uniform bore CINCINNATI in an arched block style across the front with the uniform number below on the left. Red, long-sleeved undershirts and plain red stirrups over white sanitary stockings completed the basic design.
The 1993 uniforms (which did away with the pullovers and brought back button-down jerseys) kept white and gray as the base colors for the home and away uniforms, but added red pinstripes. The home jerseys were sleeveless, showing more of the red undershirts. The color scheme of the C-REDS logo on the home uniform was reversed, now red lettering on a white background. A new home cap was created that had a red bill and a white crown with red pinstripes and a red wishbone C insignia. The away uniform kept the all-red cap, but moved the uniform number to the left, to more closely match the home uniform. The only additional change to these uniforms was the introduction of black as a primary color of the Reds in 1999, especially on their road uniforms.
The Reds latest uniform change came in December 2006 which differed significantly from the uniforms worn during the previous eight seasons. The home caps returned to an all-red design with a white wishbone C, lightly outlined in black. Caps with red crowns and a black bill became the new road caps. Additionally, the sleeveless jersey was abandoned for a more traditional design. The numbers and lettering for the names on the backs of the jerseys were changed to an early-1900s style typeface, and a handlebar mustached "Mr. Redlegs" – reminiscent of the logo used by the Reds in the 1950s and 1960s – was placed on the left sleeve.
Awards and accolades.
Retired numbers.
The Cincinnati Reds have retired nine numbers in franchise history, as well as honoring Jackie Robinson, retired in all major league baseball.
All of the retired numbers are located at Great American Ball Park behind home-plate on the outside of the press box. Along with the retired player and manager number, the following broadcasters are honored with microphones by the broadcast booth: Marty Brennaman, Waite Hoyt, and Joe Nuxhall.
On April 15, 1997, #42 was retired throughout Major League Baseball in honor of Jackie Robinson.
Out of Circulation, but not retired.
Since Pete Rose was banned from baseball, the Reds have not retired his #14. However, they have not reissued it except for Pete Rose, Jr. in his 11-game tenure in 1997. However, in January 2016, Commissioner Rob Manfred announced that Rose would be allowed in the Reds team Hall of Fame, and it was announced that his number would be retired on their Hall of Fame weekend of June 24–26.
Ohio Cup.
The Ohio Cup was an annual pre-season baseball game, which pitted the Ohio rivals Cleveland Indians and Cincinnati Reds. In its first series it was a single-game cup, played each year at minor-league Cooper Stadium in Columbus, Ohio, was staged just days before the start of each new Major League Baseball season. A total of eight Ohio Cup games were played, in 1989 to 1996, with the Indians winning six of them. The winner of the game each year was awarded the Ohio Cup in postgame ceremonies. The Ohio Cup was a favorite among baseball fans in Columbus, with attendances regularly topping 15,000.
The Ohio Cup games ended with the introduction of regular-season interleague play in 1997. Thereafter, the two teams competed annually in the regular-season Battle of Ohio or Buckeye Series. The Ohio Cup was revived in 2008 as a reward for the team with the better overall record in the Reds-Indians series each year. The Indians currently lead the interleague series 36–35. 
Media.
Radio.
The Reds' flagship radio station has been WLW, 700AM since 1969. Prior to that, the Reds were heard over: WKRC, WCPO, WSAI and WCKY. WLW, a 50,000-watt station, is "clear channel" in more than one way, as iHeartMedia owns the "blowtorch" outlet which is also known as "The Nation's Station".
Marty Brennaman has been the Reds' play-by-play voice since 1974 and has won the Ford C. Frick Award for his work, which includes his famous call of "... and this one belongs to the Reds!" after a win. Joining him for years on color was former Reds pitcher Joe Nuxhall, who worked in the radio booth from 1967 (the year after his retirement as an active player) until 2004, plus three more seasons doing select home games until his death, in 2007.
In 2007, Thom Brennaman, a veteran announcer seen nationwide on Fox Sports, joined his father Marty in the radio booth. Retired relief pitcher Jeff Brantley, formerly of ESPN, also joined the network in 2007. As of 2010, Brantley and Thom Brennaman's increased TV schedule (see below) has led to more appearances for Jim Kelch, who had filled in on the network since 2008.
Television.
Televised games are seen exclusively on Fox Sports Ohio and Fox Sports Indiana. In addition, Fox Sports South televises Fox Sports Ohio broadcasts of Reds games to Tennessee and western North Carolina. George Grande, who hosted the first "SportsCenter" on ESPN in 1979, was the play-by-play announcer, usually alongside Chris Welsh, from 1993 until his retirement during the final game of the 2009 season. Since 2009, Grande has worked part-time for the Reds as play-by-play announcer in September when Thom Brennaman is covering the NFL for Fox Sports. He has also made guest appearances throughout the season. Brennaman has been the head play-by-play commentator since 2010, with Welsh and Brantley sharing time as the color commentators. Paul Keels, who left in 2011 to become the play-by-play announcer for the Ohio State Buckeyes Radio Network, was the Reds backup play-by-play television announcer during the 2010 season. Jim Kelch served as Keels' replacement. The Reds also added former Cincinnati First Baseman Sean Casey – known as "The Mayor" by Reds fans – to do color commentary for approximately 15 games in 2011.
NBC affiliate WLWT carried Reds games from 1948–1995. Among those that have called games for WLWT include Waite Hoyt, Ray Lane, Steve Physioc, Johnny Bench, Joe Morgan, and Ken Wilson. Al Michaels, who established a long career with ABC and NBC, spent three years in Cincinnati early in his career. The last regularly-scheduled, over-the-air broadcasts of Reds games were on WSTR-TV from 1996–98. Since 2010, WKRC-TV has simulcast Opening Day games with Fox Sports Ohio.

</doc>
<doc id="6672" url="https://en.wikipedia.org/wiki?curid=6672" title="Caribbean cuisine">
Caribbean cuisine

Caribbean cuisine is a fusion of African, Amerindian, European, East Indian, Arab and Chinese cuisine. These traditions were brought from many different countries when they came to the Caribbean. In addition, the population has created styles that are unique to the region.
Ingredients which are common in most islands' dishes are rice, plantains, beans, cassava, cilantro (coriander), bell peppers, chickpeas, tomatoes, sweet potatoes, coconut, and any of various meats that are locally available like beef, poultry, pork or fish. A characteristic seasoning for the region is a green herb and oil based marinade which imparts a flavor profile which is quintessentially Caribbean in character. Ingredients may include garlic, onions, scotch bonnet peppers, celery, green onions, and herbs like cilantro, marjoram, rosemary, tarragon and thyme. This green seasoning is used for a variety of dishes like curries, stews and roasted meats.
Traditional dishes are so important to regional culture that, for example, the local version of Caribbean goat stew has been chosen as the official national dish of Montserrat and is also one of the signature dishes of St. Kitts and Nevis. Another popular dish in the Anglophone Caribbean is called "Cook-up", or Pelau. Ackee and saltfish is another popular dish that is unique to Jamaica. Callaloo is a dish containing leafy vegetables and sometimes okra amongst others, widely distributed in the Caribbean, with a distinctively mixed African and indigenous character.
The variety of dessert dishes in the area also reflects the mixed origins of the recipes. In some areas, Black Cake, a derivative of English Christmas pudding may be served, especially on special occasions.

</doc>
<doc id="6673" url="https://en.wikipedia.org/wiki?curid=6673" title="Central Powers">
Central Powers

The Central Powers (; ; ; ), consisting of Germany, , the Ottoman Empire and Bulgaria – hence also known as the Quadruple Alliance () – was one of the two main factions during World War I (1914–18). It faced and was defeated by the Allied Powers that had formed around the Triple Entente, after which it was dissolved.
The Powers' origin was the alliance of Germany and Austria-Hungary in 1879. The Ottoman Empire and Bulgaria did not join until after World War I had begun.
Member states.
The Central Powers consisted of the German Empire and the Austro-Hungarian Empire at the beginning of the war. The Ottoman Empire joined the Central Powers later in 1914. In 1915, the Kingdom of Bulgaria joined the alliance. The name "Central Powers" is derived from the location of these countries; all four (including the other groups that supported them except for Finland and Lithuania) were located between the Russian Empire in the east and France and the United Kingdom in the west. Finland, Azerbaijan, and Lithuania joined them in 1918 before the war ended and after the Russian Empire collapsed.
The Central Powers were composed of the following nations:
Combatants.
Germany.
War justifications.
In early July 1914, in the aftermath of the assassination of Austro-Hungarian Franz Ferdinand and the immediate likelihood of war between Austria-Hungary and Serbia, Kaiser Wilhelm II and the German government informed the Austro-Hungarian government that Germany would uphold its alliance with Austria-Hungary and defend it from possible Russia intervention if a war between Austria-Hungary and Serbia took place. When Russia enacted a general mobilization, Germany viewed the act as provocative. The Russian government promised Germany that its general mobilization did not mean preparation for war with Germany but was a reaction to the events between Austria-Hungary and Serbia. The German government regarded the Russian promise of no war with Germany to be nonsense in light of its general mobilization, and Germany in turn mobilized for war. On August 1, Germany sent an ultimatum to Russia stating that since both Germany and Russia were in a state of military mobilization, an effective state of war existed between the two countries. Later that day, France, an ally of Russia, declared a state of general mobilization,
In August 1914, Germany waged war on Russia, the German government justified military action against Russia as necessary because of Russian aggression as demonstrated by the mobilization of the Russian army that had resulted in Germany mobilizing in response.
After Germany declared war on Russia, France with its alliance with Russia prepared a general mobilization in expectation of war. On 3 August 1914, Germany responded to this action by declaring war on France. Germany facing a two-front war enacted what was known as the Schlieffen Plan, that involved German armed forces needing to move through Belgium and swing south into France and towards the French capital of Paris. This plan was hoped to quickly gain victory against the French and allow German forces to concentrate on the Eastern Front. Belgium was a neutral country and would not accept German forces crossing its territory. Germany disregarded Belgian neutrality and invaded the country to launch an offensive towards Paris. This caused Great Britain to declare war against the German Empire, as the action violated the Treaty of London that both nations signed in 1839 guaranteeing Belgian neutrality and defense of the kingdom if a nation reneged.
Subsequently several states declared war on Germany, in late August 1914; Italy declaring war on Austria-Hungary in 1915 and Germany on August 27, 1916; the United States declaring war on Germany on April 6, 1917 and Greece declaring war on Germany in July 1917.
Colonies and dependencies.
Upon its founding in 1871, the German Empire controlled Alsace-Lorraine as an "imperial territory" incorporated from France after the Franco-Prussian War. It was held as part of Germany's sovereign territory.
Germany held multiple African colonies at the time of World War I. All of Germany's African colonies were invaded and occupied by Allied forces during the war.
Cameroon, German East Africa, and German Southwest Africa were German colonies in Africa. Togoland was a German protectorate in Africa.
German New Guinea was a German protectorate in the Pacific. It was occupied by Australian forces in 1914.
The Kiautschou Bay concession was a German dependency in East Asia leased from China in 1898. It was occupied by Japanese forces following the Siege of Tsingtao.
Austria-Hungary.
War justifications.
Austria-Hungary regarded the assassination of Archduke Franz Ferdinand as being orchestrated with the assistance of Serbia. The country viewed the assassination as setting a dangerous precedent of encouraging the country's South Slav population to rebel and threaten to tear apart the multinational country. Austria-Hungary formally sent an ultimatum to Serbia demanding a full-scale investigation of Serbian government complicity in the assassination, and complete compliance by Serbia in agreeing to the terms demanded by Austria-Hungary. Serbia submitted to accept most of the demands, however Austria-Hungary viewed this as insufficient and used this lack of full compliance to justify military intervention. These demands have been viewed as a diplomatic cover for what was going to be an inevitable Austro-Hungarian declaration of war on Serbia.
Austria-Hungary had been warned by Russia that the Russian government would not tolerate Austria-Hungary crushing Serbia. However, with Germany supporting Austria-Hungary's actions, the Austro-Hungarian government hoped that Russia would not intervene and that the conflict with Serbia would be a regional conflict.
Austria-Hungary's invasion of Serbia resulted in Russia declaring war on the country and Germany in turn declared war on Russia, setting off the beginning of the clash of alliances that resulted in the World War.
Austria-Hungary was internally divided into two states with their own governments, joined in communion through the Habsburg throne. Austrian Cisleithania contained various duchies and principalities but also the Kingdom of Bohemia, the Kingdom of Dalmatia, the Kingdom of Galicia and Lodomeria. Hungarian Transleithania comprised the Kingdom of Hungary and the Kingdom of Croatia-Slavonia. In Bosnia and Herzegovina sovereign authority was shared by both Austria and Hungary.
Ottoman Empire.
War justifications.
The Ottoman Empire joined the war on the side of the Central Powers in November 1914. The Ottoman Empire had gained strong economic connections with Germany through the Berlin-to-Baghdad railway project that was still incomplete at the time. The Ottoman Empire made a formal alliance with Germany signed on 2 August 1914. The alliance treaty expected that the Ottoman Empire would become involved in the conflict in a short amount of time. However, for the first several months of the war the Ottoman Empire maintained neutrality though it allowed a German naval squadron to enter and stay near the strait of Bosphorus. Ottoman officials informed the German government that the country needed time to prepare for conflict. Germany provided financial aid and weapons shipments to the Ottoman Empire.
After pressure escalated from the German government demanding that the Ottoman Empire fulfill its treaty obligations, or else Germany would expel the country from the alliance and terminate economic and military assistance, the Ottoman government entered the war with the recently acquired cruisers from Germany, the "Yavuz Sultan Selim" (formerly "SMS Goeben") and the "Midilli" (formerly "SMS Breslau") launching a naval raid on the Russian port of Odessa, thus engaging in a military action in accordance with its alliance obligations with Germany. Russia and the Triple Entente declared war on the Ottoman Empire.
Bulgaria.
War justifications.
Bulgaria was still resentful after its defeat in July 1913 at the hands of Serbia, Greece and Romania. It signed a treaty of defensive alliance with the Ottoman Empire on 19 August 1914. It was the last country to join the Central Powers, which Bulgaria did in October 1915 by declaring war on Serbia. It invaded Serbia in conjunction with German and Austro-Hungarian forces. Bulgaria held irredentist aims on the region of Vardar Macedonia held by Serbia. 
Co-belligerents.
Emirate of Jabal Shammar.
The Emirate of Jabal Shammar fought in the Middle Eastern theatre.
Dervish State.
The Dervish State was a rebel Somali state seeking independence of Somali territories. Dervish forces fought against Italian and British forces in Italian Somaliland and British Somaliland during World War I in the Somaliland Campaign. The Dervish State received support from Germany and the Ottoman Empire.
South African Republic.
In opposition to the Union of South Africa, which had joined the war, Boer rebels founded the South African Republic in 1914 and engaged in the Maritz Rebellion. Germany assisted the rebels, and the rebels operated in and out of the German colony of German South-West Africa. The rebels were defeated by British imperial forces.
Sultanate of Darfur.
The Sultanate of Darfur forces fought against British forces in Anglo-Egyptian Sudan during World War I in the Anglo-Egyptian Darfur Expedition.
Client states.
During 1917 and 1918, the Finns under Carl Gustaf Emil Mannerheim and Lithuanian nationalists fought Russia for a common cause. With the Bolshevik attack of late 1917, the General Secretariat of Ukraine sought military protection first from the Central Powers and later from the armed forces of the Entente.
The Ottoman Empire also had its own allies in Azerbaijan and the Northern Caucasus. The three nations fought alongside each other under the Army of Islam in the Battle of Baku.
Non-state combatants.
Other movements supported the efforts of the Central Powers for their own reasons, such as the Irish Nationalists who launched the Easter Rising in Dublin in April 1916; they referred to their "gallant allies in Europe". In 1914, Józef Piłsudski was permitted by Germany and Austria-Hungary to form independent Polish legions. Piłsudski wanted his legions to help the Central Powers defeat Russia and then side with France and the UK and win the war with them.
Armistice and treaties.
Bulgaria signed an armistice with the Allies on 29 September 1918, following a successful Allied advance in Macedonia. The Ottoman Empire followed suit on 30 October 1918 in the face of British and Arab gains in Palestine and Syria. Austria and Hungary concluded ceasefires separately during the first week of November following the disintegration of the Habsburg Empire and the Italian offensive at Vittorio Veneto; Germany signed the armistice ending the war on the morning of 11 November 1918 after the Hundred Days Offensive, and a succession of advances by New Zealand, Australian, Canadian, Belgian, British, French and US forces in north-eastern France and Belgium. There was no unified treaty ending the war; the Central Powers were dealt with in separate treaties.

</doc>
<doc id="6675" url="https://en.wikipedia.org/wiki?curid=6675" title="Conservatism">
Conservatism

Conservatism as a political and social philosophy promotes retaining traditional social institutions in the context of culture and civilization. Some conservatives seek to preserve things as they are, emphasizing stability and continuity, while others, called reactionaries, oppose modernism and seek a return to "the way things were". The first established use of the term in a political context originated with François-René de Chateaubriand in 1818, during the period of Bourbon restoration that sought to roll back the policies of the French Revolution. The term, historically associated with right-wing politics, has since been used to describe a wide range of views.
There is no single set of policies that are universally regarded as conservative because the meaning of conservatism depends on what is considered traditional in a given place and time. Thus conservatives from different parts of the world—each upholding their respective traditions—may disagree on a wide range of issues. Edmund Burke, an 18th-century politician who opposed the French Revolution but supported the American Revolution, is credited as one of the main theorists of conservatism in Britain in the 1790s. According to Quintin Hogg, the chairman of the British Conservative Party in 1959, "Conservatism is not so much a philosophy as an attitude, a constant force, performing a timeless function in the development of a free society, and corresponding to a deep and permanent requirement of human nature itself."
Development of Western conservatism.
Great Britain.
In Britain, conservative ideas (though not yet called that) emerged in the Tory movement during the Restoration period (1660–1688). Toryism supported a hierarchical society with a monarch who ruled by divine right. Tories opposed the idea that sovereignty derived from the people, and rejected the authority of parliament and freedom of religion. Robert Filmer's "Patriarcha: or the Natural Power of Kings", published posthumously in 1680 but written before the English Civil War of 1642–1651, became accepted as the statement of their doctrine. However, the Glorious Revolution of 1688 destroyed this principle to some degree by establishing a constitutional government in England, leading to the hegemony of the Tory-opposed Whig ideology. Faced with defeat, the Tories reformed their movement, now holding that sovereignty was vested in the three estates of Crown, Lords, and Commons rather than solely in the Crown. Toryism became marginalized during the long period of Whig ascendancy in the 18th century.
Conservatives typically see Richard Hooker (1554–1600) as the founding father of conservatism, along with the Marquess of Halifax (1633–1695), David Hume (1711–1776) and Edmund Burke (1729–1797). Halifax promoted pragmatism in government, whilst Hume argued against political rationalism and utopianism. Burke served as the private secretary to the Marquis of Rockingham and as official pamphleteer to the Rockingham branch of the Whig party. Together with the Tories, they were the conservatives in the late 18th century United Kingdom. Burke's views were a mixture of liberal and conservative. He supported the American Revolution of 1765–1783 but abhorred the violence of the French Revolution (1789–1799). He accepted the liberal ideals of private property and the economics of Adam Smith (1723–1790), but thought that economics should remain subordinate to the conservative social ethic, that capitalism should be subordinate to the medieval social tradition and that the business class should be subordinate to aristocracy. He insisted on standards of honor derived from the medieval aristocratic tradition, and saw the aristocracy as the nation's natural leaders. That meant limits on the powers of the Crown, since he found the institutions of Parliament to be better informed than commissions appointed by the executive. He favored an established church, but allowed for a degree of religious toleration. Burke justified the social order on the basis of tradition: tradition represented the wisdom of the species and he valued community and social harmony over social reforms. Burke was a leading theorist in his day, finding extreme idealism (either Tory or Whig) an endangerment to broader liberties, and (like Hume) rejecting abstract reason as an unsound guide for political theory. Despite their influence on future conservative thought, none of these early contributors were explicitly involved in Tory politics. Hooker lived in the 16th century, long before the advent of toryism, whilst Hume was an apolitical philosopher and Halifax similarly politically independent. Burke described himself as a Whig.
Shortly after Burke's death in 1797, conservatism revived as a mainstream political force as the Whigs suffered a series of internal divisions. This new generation of conservatives derived their politics not from Burke but from his predecessor, the Viscount Bolingbroke (1678–1751), who was a Jacobite and traditional Tory, lacking Burke's sympathies for Whiggish policies such as Catholic Emancipation and American independence (famously attacked by Samuel Johnson in "Taxation No Tyranny"). In the first half of the 19th century many newspapers, magazines, and journals promoted loyalist or right-wing attitudes in religion, politics, and international affairs. Burke was seldom mentioned but William Pitt the Younger (1759–1806) became a conspicuous hero. The most prominent journals included "The Quarterly Review", founded in 1809 as a counterweight to the Whigs' "Edinburgh Review", and the even more conservative "Blackwood's Edinburgh Magazine". Sack finds that the "Quarterly Review" promoted a balanced Canningite toryism; was neutral on Catholic emancipation and only mildly critical of Nonconformist Dissent; it opposed slavery and supported the current poor laws. It was "aggressively imperialist". The high-church clergy of the Church of England read the "Orthodox Churchman's Magazine" which was equally hostile to Jewish, Catholic, Jacobin, Methodist, and Unitarian spokesmen. Anchoring the ultra tories, "Blackwood's Edinburgh Magazine" stood firmly against Catholic emancipation, and favoured slavery, cheap money, mercantilism, the Navigation acts, and the Holy Alliance.
In the 19th century, conflict between wealthy businessmen and the aristocracy split the British conservative movement, with the aristocracy calling for a return to medieval ideas while the business classes advocated laissez-faire capitalism.
Although conservatives opposed attempts to allow greater representation of the middle class in parliament, in 1834 they conceded that electoral reform could not be reversed and promised to support further reforms so long as they did not erode the institutions of church and state. These new principles were presented in the Tamworth Manifesto of 1834, which historians regard as the basic statement of the beliefs of the new Conservative Party.
Some conservatives lamented the passing of a pastoral world where the ethos of "noblesse oblige" had promoted respect from the lower classes. They saw the Anglican Church and the aristocracy as balances against commercial wealth. They worked toward legislation for improved working conditions and urban housing. This viewpoint would later be called Tory Democracy. However, since Burke there has always been tension between traditional aristocratic conservatism and the wealthy business class.
In 1834 Tory Prime Minister Robert Peel issued the Tamworth Manifesto in which he pledged to endorse moderate political reform. This marked the beginning of the transformation of British conservatism from High Tory reactionism towards a more modern form based on "conservation". The party became known as the Conservative Party as a result, a name it has retained to this day. Peel, however, would also be the root of a split in the party between the traditional Tories (led by the Earl of Derby and Benjamin Disraeli) and the 'Peelites' (led first by Peel himself, then by the Earl of Aberdeen). The split occurred in 1846 over the issue of free trade, which Peel supported, versus protectionism, supported by Derby. The majority of the party sided with Derby, whilst about a third split away, eventually merging with the Whigs and the radicals to form the Liberal Party. Despite the split, the mainstream Conservative Party accepted the doctrine of free trade in 1852.
In the second half of the 19th century the Liberal Party faced political schisms, especially over Irish Home Rule. Leader William Gladstone (himself a former Peelite) sought to give Ireland a degree of autonomy, a move that elements in both the left and right wings of his party opposed. These split off to become the Liberal Unionists (led by Joseph Chamberlain), forming a coalition with the Conservatives before merging with them in 1912. The Liberal Unionist influence dragged the Conservative Party towards the left; Conservative governments passing a number of progressive reforms at the turn of the 20th century. By the late 19th century the traditional business supporters of the UK Liberal Party had joined the Conservatives, making them the party of business and commerce.
After a period of Liberal dominance before the First World War, the Conservatives gradually became more influential in government, regaining full control of the cabinet in 1922. In the interwar period conservatism was the major ideology in Britain, as the Liberal Party vied with the Labour Party for control of the left. After the Second World War, the first Labour government (1945–1951) under Clement Attlee embarked on a program of nationalization of industry and the promotion of social welfare. The Conservatives generally accepted those policies until the 1980s. In the 1980s the Conservative government of Margaret Thatcher, guided by neoliberal economics, reversed many of Labour's programmes.
Small conservative political parties, such as the United Kingdom Independence Party (founded in 1993) and the Democratic Unionist Party (founded in 1971), began to appear, although they have yet to make any significant impact at Westminster ( the DUP comprises the largest political party in the ruling coalition in the Northern Ireland Assembly).
Germany.
Conservative thought developed alongside nationalism in Germany, culminating in Germany's victory over France in the Franco-Prussian War, the creation of the unified German Empire in 1871, and the simultaneous rise of Otto von Bismarck on the European political stage. Bismarck's "balance of power" model maintained peace in Europe for decades at the end of the 19th century. His "revolutionary conservatism" was a conservative state-building strategy designed to make ordinary Germans—not just the Junker elite—more loyal to state and emperor, he created the modern welfare state in Germany in the 1880s. According to Kees van Kersbergen and Barbara Vis, his strategy was:
Bismarck also enacted universal male suffrage in the new German Empire in 1871. He became a great hero to German conservatives, who erected many monuments to his memory after he left office in 1890.
With the rise of Nazism in 1933, agrarian movements faded and was supplanted by a more command-based economy and forced social integration. Though Adolf Hitler succeeded in garnering the support of many German industrialists, prominent traditionalists openly and secretly opposed his policies of euthanasia, genocide, and attacks on organized religion, including Claus von Stauffenberg, Dietrich Bonhoeffer, Henning von Tresckow, Bishop Clemens August Graf von Galen, and the monarchist Carl Friedrich Goerdeler.
More recently, the work of conservative CDU leader Helmut Kohl helped bring about German Reunification, along with the closer integration of Europe in the form of the Maastricht Treaty. Today, German conservatism is often associated with Chancellor Angela Merkel, whose tenure has been marked by attempts to save the common European currency (euro) from demise.
United States.
In the United States, conservatism is rooted in the American Revolution and its commitment to republicanism, sovereignty of the people, and the rights and liberties of Englishmen while expelling the king and his supporters. Most European conservative writers do not accept American conservatism as genuine; they consider it to be a variety of liberalism. Modern American liberals in the New Deal do not disagree with that consensus view, but conservatives spend much more emphasis on the Revolutionary origins, with the Tea Party advocates using an episode from the 1770s for their name and some even dress in costumes from that era at their rallies.
Historian Gregory Schneider identifies several constants in American conservatism: respect for tradition, support of republicanism, "the rule of law and the Christian religion," and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments."
France.
Another form of conservatism developed in France in parallel to conservatism in Britain. It was influenced by Counter-Enlightenment works by men such as Joseph de Maistre and Louis de Bonald. Latin conservatism was less pragmatic and more reactionary than the conservatism of Burke. Many Continental or Traditionalist conservatives do not support separation of Church and state, with most supporting state recognition of and cooperation with the Catholic Church, such as had existed in France before the Revolution.
Eventually conservatives added patriotism and nationalism to the list of traditional values they support. German conservatives were the first to embrace nationalism, which was previously associated with liberalism and the Revolution in France.
Forms of conservatism.
Liberal conservatism.
Liberal conservatism is a variant of conservatism that combines conservative values and policies with classical liberal stances. As these latter two terms have had different meanings over time and across countries, liberal conservatism also has a wide variety of meanings. Historically, the term often referred to the combination of economic liberalism, which champions laissez-faire markets, with the classical conservatism concern for established tradition, respect for authority and religious values. It contrasted itself with classical liberalism, which supported freedom for the individual in both the economic and social spheres.
Over time, the general conservative ideology in many countries adopted economic liberal arguments, and the term "liberal conservatism" was replaced with "conservatism". This is also the case in countries where liberal economic ideas have been the tradition, such as the United States, and are thus considered conservative. In other countries where liberal conservative movements have entered the political mainstream, such as Italy and Spain, the terms "liberal" and "conservative" may be synonymous. The liberal conservative tradition in the United States combines the economic individualism of the classical liberals with a Burkean form of conservatism (which has also become part of the American conservative tradition, such as in the writings of Russell Kirk).
A secondary meaning for the term "liberal conservatism" that has developed in Europe is a combination of more modern conservative (less traditionalist) views with those of social liberalism. This has developed as an opposition to the more collectivist views of socialism. Often this involves stressing what are now conservative views of free-market economics and belief in individual responsibility, with social liberal views on defence of civil rights, environmentalism and support for a limited welfare state. In continental Europe, this is sometimes also translated into English as social conservatism.
Conservative liberalism.
Conservative liberalism is a variant of liberalism that combines liberal values and policies with conservative stances, or, more simply, the right wing of the liberal movement. The roots of conservative liberalism are found at the beginning of the history of liberalism. Until the two World Wars, in most European countries the political class was formed by conservative liberals, from Germany to Italy. Events after World War I brought the more radical version of classical liberalism to a more conservative (i.e. more moderate) type of liberalism.
Libertarian conservatism.
Libertarian conservatism describes certain political ideologies within the United States and Canada which combine libertarian economic issues with aspects of conservatism. Its four main branches are Constitutionalism, paleolibertarianism, small government conservatism and Christian libertarianism. They generally differ from paleoconservatives, in that they are in favor of more personal and economic freedom.
Agorists such as Samuel Edward Konkin III labeled libertarian conservatism right-libertarianism.
In contrast to paleoconservatives, libertarian conservatives support strict laissez-faire policies such as free trade, opposition to any national bank and opposition to business regulations. They are vehemently opposed to environmental regulations, corporate welfare, subsidies, and other areas of economic intervention.
Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.
Fiscal conservatism.
Fiscal conservatism is the economic philosophy of prudence in government spending and debt. Edmund Burke, in his "Reflections on the Revolution in France", argued that a government does not have the right to run up large debts and then throw the burden on the taxpayer:
...is to the property of the citizen, and not to the demands of the creditor of the state, that the first and original faith of civil society is pledged. The claim of the citizen is prior in time, paramount in title, superior in equity. The fortunes of individuals, whether possessed by acquisition or by descent or in virtue of a participation in the goods of some community, were no part of the creditor's security, expressed or implied...[The public, whether represented by a monarch or by a senate, can pledge nothing but the public estate; and it can have no public estate except in what it derives from a just and proportioned imposition upon the citizens at large.
Most conservatives, especially in the United States, believe that government action should focus on moral and social questions and oppose government action to help the poor, to regulate the economy, or to protect the environment. They believe that government programs that seek to provide services and opportunities for the poor actually encourage dependence and reduce self-reliance. They oppose affirmative action. They oppose a progressive income tax.
National and traditional conservatism.
National conservatism is a political term used primarily in Europe to describe a variant of conservatism which concentrates more on national interests than standard conservatism as well as upholding cultural and ethnic identity, while not being outspokenly nationalist or supporting a far-right approach. In Europe, national conservatives are usually eurosceptics.
National conservatism is heavily oriented towards the traditional family and social stability as well as in favour of limiting immigration. As such, national conservatives can be distinguished from economic conservatives, for whom free market economic policies, deregulation and fiscal conservatism are the main priorities. Some commentators have identified a growing gap between national and economic conservatism: "most parties of the Right are run by economic conservatives who, in varying degrees, have marginalized social, cultural, and national conservatives." National conservatism is also related to traditionalist conservatism.
Traditionalist conservatism is a political philosophy emphasizing the need for the principles of natural law and transcendent moral order, tradition, hierarchy and organic unity, agrarianism, classicism and high culture, and the intersecting spheres of loyalty. Some traditionalists have embraced the labels "reactionary" and "counterrevolutionary", defying the stigma that has attached to these terms since the Enlightenment. Having a hierarchical view of society, many traditionalist conservatives, including a few Americans, defend the monarchical political structure as the most natural and beneficial social arrangement.
Cultural and social conservatism.
Cultural conservatives support the preservation of the heritage of one nation, or of a shared culture that is not defined by national boundaries. The shared culture may be as divergent as Western culture or Chinese culture. In the United States, the term "cultural conservative" may imply a conservative position in the culture war. Cultural conservatives hold fast to traditional ways of thinking even in the face of monumental change. They believe strongly in traditional values and traditional politics, and often have an urgent sense of nationalism.
Social conservatism is distinct from cultural conservatism, although there are some overlaps. Social conservatives may believe that the government has a role in encouraging or enforcing traditional values or behaviours. A social conservative wants to preserve traditional morality and social mores, often by opposing what they consider radical policies or social engineering. Social change is generally regarded as suspect.
A second meaning of the term "social conservatism" developed in the Nordic countries and continental Europe. There it refers to liberal conservatives supporting modern European welfare states.
Social conservatives (in the first meaning of the word) in many countries generally favour the pro-life position in the abortion controversy and oppose human embryonic stem cell research (particularly if publicly funded); oppose both eugenics and human enhancement (transhumanism) while supporting bioconservatism; support a traditional definition of marriage as being one man and one woman; view the nuclear family model as society's foundational unit; oppose expansion of civil marriage and child adoption rights to couples in same-sex relationships; promote public morality and traditional family values; oppose atheism, especially militant atheism, secularism and the separation of church and state; support the prohibition of drugs, prostitution, and euthanasia; and support the censorship of pornography and what they consider to be obscenity or indecency. Most conservatives in the U.S. support the death penalty.
Religious conservatism.
Religious conservatives principally seek to apply the teachings of particular religions to politics, sometimes by merely proclaiming the value of those teachings, at other times by having those teachings influence laws.
In most modern democracies, political conservatism seeks to uphold traditional family structures and social values. Religious conservatives typically oppose abortion, homosexual behavior, drug use, and sexual activity outside of marriage. In some cases, conservative values are grounded in religious beliefs, and some conservatives seek to increase the role of religion in public life.
Progressive conservatism.
Progressive conservatism incorporates progressive policies alongside conservative policies. It stresses the importance of a social safety net to deal with poverty, support of limited redistribution of wealth along with government regulation to regulate markets in the interests of both consumers and producers. Progressive conservatism first arose as a distinct ideology in the United Kingdom under Prime Minister Benjamin Disraeli's "One Nation" Toryism.
There have been a variety of progressive conservative governments. In the UK, the Prime Ministers Disraeli, Stanley Baldwin, Neville Chamberlain, Winston Churchill, Harold Macmillan, and present Prime Minister David Cameron are progressive conservatives.
In the United States, the administration of President William Howard Taft was progressive conservative and he described himself as "a believer in progressive conservatism" and President Dwight D. Eisenhower declared himself an advocate of "progressive conservatism". In Germany, Chancellor Leo von Caprivi promoted a progressive conservative agenda called the "New Course". In Canada, a variety of conservative governments have been progressive conservative, with Canada's major conservative movement being officially named the Progressive Conservative Party of Canada from 1942 to 2003. In Canada, the Prime Ministers Arthur Meighen, R. B. Bennett, John Diefenbaker, Joe Clark, Brian Mulroney, and Kim Campbell led progressive conservative federal governments.
Historic conservatism in different countries.
Conservative political parties vary widely from country to country in the goals they wish to achieve. Both conservative and liberal parties tend to favor private ownership of property, in opposition to communist, socialist and green parties, which favor communal ownership or laws requiring social responsibility on the part of property owners. Where conservatives and liberals differ is primarily on social issues. Conservatives tend to reject behavior that does not conform to some social norm. Modern conservative parties often define themselves by their opposition to liberal or labor parties. The United States usage of the term "conservative" is unique to that country.
According to Alan Ware, Belgium, Denmark, Iceland, Finland, France, Greece, Iceland, Luxembourg, Netherlands, Norway, Sweden, Switzerland, and the UK retained viable conservative parties into the 1980s. Ware said that Australia, Germany, Israel, Italy, Japan, Malta, New Zealand, Spain and the US had no conservative parties, although they had either Christian Democrats or liberals as major right-wing parties. Canada, Ireland, and Portugal had right-wing political parties that defied categorization: the Progressive Conservative Party of Canada; Fianna Fáil, Fine Gael, and Progressive Democrats in Ireland; and the Social Democratic Party of Portugal. Since then, the Swiss People's Party has moved to the extreme right and is no longer considered to be conservative.
Klaus von Beyme, who developed the method of party categorization, found that no modern Eastern European parties could be considered conservative, although the communist and communist-successor parties had strong similarities.
In Italy, which was united by liberals and radicals ("risorgimento"), liberals not conservatives emerged as the party of the Right. In the Netherlands, conservatives merged into a new Christian democratic party in 1980. In Austria, Germany, Portugal and Spain, conservatism was transformed into and incorporated into fascism or the far right. In 1940, all Japanese parties were merged into a single fascist party. Following the war, Japanese conservatives briefly returned to politics but were largely purged from public office.
Louis Hartz explained the absence of conservatism in Australia or the United States as a result of their settlement as radical or liberal fragments of Great Britain. Although he said English Canada had a negligible conservative influence, subsequent writers claimed that loyalists opposed to the American Revolution brought a Tory ideology into Canada. Hartz explained conservatism in Quebec and Latin America as a result of their settlement as feudal societies. The American conservative writer Russell Kirk provided the opinion that conservatism had been brought to the US and interpreted the American revolution as a "conservative revolution".
Conservative elites have long dominated Latin American nations. Mostly this has been achieved through control of and support for civil institutions, the church and the armed forces, rather than through party politics. Typically the church was exempt from taxes and its employees immune from civil prosecution. Where national conservative parties were weak or non-existent, conservatives were more likely to rely on military dictatorship as a preferred form of government. However, in some nations where the elites were able to mobilize popular support for conservative parties, longer periods of political stability were achieved. Chile, Colombia and Venezuela are examples of nations that developed strong conservative parties. Argentina, Brazil, El Salvador and Peru are examples of nations where this did not occur. The Conservative Party of Venezuela disappeared following the Federal Wars of 1858-1863. Chile's conservative party, the National Party disbanded in 1973 following a military coup and did not re-emerge as a political force following the subsequent return to democracy.
Belgium.
Founded in 1945 as the Christian People's Party, the Flemish Christian Democrats (CD&V) dominated politics in post-war Belgium. In 1999, the party's support collapsed and it became the country's fifth largest party. Currently the N-VA (nieuw-vlaamse alliantie / new-Flemish alliance) is the largest party in Belgium.
Canada.
Canada's "Conservatives" had their roots in the Loyalists – Tories – who left America after the American Revolution. They developed in the socio-economic and political cleavages that existed during the first three decades of the 19th century, and had the support of the business, professional and established Church (Anglican) elites in Ontario and to a lesser extent in Quebec. Holding a monopoly over administrative and judicial offices, they were called the "Family Compact" in Ontario and the "Chateau Clique" in Quebec. John A. Macdonald's successful leadership of the movement to confederate the provinces and his subsequent tenure as prime minister for most of the late 19th century rested on his ability to bring together the English-speaking Protestant oligarchy and the ultramontane Catholic hierarchy of Quebec and to keep them united in a conservative coalition.
The Conservatives combined pro-market liberalism and Toryism. They generally supported an activist government and state intervention in the marketplace, and their policies were marked by "noblesse oblige", a paternalistic responsibility of the elites for the less well-off. From 1942, the party was known as the Progressive Conservatives, until 2003, when the national party merged with the Canadian Alliance to form the Conservative Party of Canada.
The conservative Union Nationale governed the province of Quebec in periods from 1936 to 1960, in a close alliance with English Canadian business elites and the Catholic Church. This period, known as the Great Darkness ended with the Quiet Revolution and the party went into terminal decline.
Colombia.
The Colombian Conservative Party, founded in 1849, traces its origins to opponents of General Francisco de Paula Santander's 1833–37 administration. While the term "liberal" had been used to describe all political forces in Colombia, the conservatives began describing themselves as "conservative liberals" and their opponents as "red liberals". From the 1860s until the present, the party has supported strong central government, and supported the Catholic Church, especially its role as protector of the sanctity of the family, and opposed separation of church and state. Its policies include the legal equality of all men, the citizen's right to own property and opposition to dictatorship. It has usually been Colombia's second largest party, with the Colombian Liberal Party being the largest.
Denmark.
Founded in 1915, the Conservative People's Party of Denmark. was the successor of "Højre" (literally "The Right"). In the 2005 election it won 18 out of 179 seats in the "Folketing" and became a junior partner in coalition with the Liberals. The party is preceded by 11 years by the Young Conservatives (KU), today the youth movement of the party. The Party suffered a major defeat in the parliamentary elections of September 2011 in which the party lost more than half of its seat and also lost governmental power. A liberal cultural policy dominated during the postwar period. However, by the 1990s disagreements regarding immigrants from entirely different cultures ignited a conservative backlash.
Finland.
The conservative party in Finland is the National Coalition Party (in Finnish "Kansallinen Kokoomus", "Kok"). The party was founded in 1918 when several monarchist parties united. Although in the past the party was right-wing, today it is a moderate party. While the party advocates economic liberalism, it is committed to the social market economy.
France.
Conservatism in France focused on The rejection of the French Revolution, support for the Catholic Church, and the restoration of the monarchy. The monarchist cause was on the verge of victory in the 1870s but then collapsed because of disagreements on who would be king, and what the national flag should be. Religious tensions heightened in the 1890–1910 era, but moderated after the spirit of unity in fighting the First World War. An extreme form of conservatism characterized the Vichy regime of 1940–1944 with heightened anti-Semitism, opposition to individualism, emphasis on family life, and national direction of the economy.
Following the Second World War, conservatives in France supported Gaullist groups and have been nationalistic, and emphasized tradition, order, and the regeneration of France. Gaullists held divergent views on social issues. The number of Conservative groups, their lack of stability, and their tendency to be identified with local issues defy simple categorization. Conservatism has been the major political force in France since the second world war. Unusually, post-war French conservatism was formed around the personality of a leader, Charles de Gaulle, and did not draw on traditional French conservatism, but on the Bonapartism tradition. Gaullism in France continues under Les Republicains (formerly Union for a Popular Movement). The word "conservative" itself is a term of abuse in France.
Greece.
The main interwar conservative party was called the People's Party (PP), which supported constitutional monarchy and opposed the republican Liberal Party. Both it and the Liberal party were suppressed by the authoritarian, arch-conservative and royalist 4th of August Regime of Ioannis Metaxas in 1936–41. The PP was able to re-group after the Second World War as part of a "United Nationalist Front" which achieved power campaigning on a simple anticommunist, ultranationalist platform during the Greek Civil War (1946–49). However, the vote received by the PP declined during the so-called "Centrist Interlude" in 1950–52. In 1952, Marshal Alexandros Papagos created the Greek Rally as an umbrella for the right-wing forces. The Greek Rally came to power in 1952 and remained the leading party in Greece until 1963—after Papagos' death in 1955 reformed as the National Radical Union under Konstantinos Karamanlis. Right-wing governments backed by the palace and the army overthrew the Centre Union government in 1965, and governed the country until the establishment of the far-right Regime of the Colonels (1967–74). After the regime's collapse in August 1974, Karamanlis returned from exile to lead the government, and founded the New Democratic Party. The new party had four objectives: to confront Turkish expansionism in Cyprus, to reestablish and solidify democratic rule, to give the country a strong government, and to make a powerful moderate party a force in Greek politics.
The Independent Greeks, a newly formed political party in Greece has also supported conservatism, particularly national and religious conservatism. The Founding Declaration of the Independent Greeks strongly emphasises in the preservation of the Greek state and its sovereignty, the Greek people and the Greek Orthodox Church.
Iceland.
Founded in 1926 as the Conservative Party, Iceland's Independence Party adopted its current name in 1929. From the beginning they have been the largest vote-winning party, averaging around 40%. They combined liberalism and conservatism, supported nationalization of infrastructure and opposed class conflict. While mostly in opposition during the 1930s, they embraced economic liberalism, but accepted the welfare state after the war and participated in governments supportive of state intervention and protectionism. Unlike other Scandanivian conservative (and liberal) parties, it has always had a large working-class following. After the financial crisis in 2008 the party has sunk to a lower support level around 20-25%.
Italy.
After WW2 in Italy the conservative theories were mainly represented by the Christian Democracy, which government form the foundation of the Republic until party's dissolution in 1994. Officially DC refused the ideology of Conservatism, but in many aspects, for example family values, it was a typical social conservative party.
In 1994 the media tycoon and entrepreneur Silvio Berlusconi founded the liberal conservative Forza Italia movement. Berlusconi won three elections in 1994, 2001 and 2008 governing the country for almost ten years as Prime Minister.
Besides FI, now the conservative ideas are mainly expressed by the New Centre-Right party led by Angelino Alfano, former Berlusconi's protégé who split from the reborn Forza Italia founding a new conservative movement. Alfano is the current Minister of the Interior in the government of Matteo Renzi.
Luxembourg.
Luxembourg's major conservative party, the Christian Social People's Party (CSV or PCS) was formed as the Party of the Right in 1914, and adopted its present name in 1945. It was consistently the largest political party in Luxembourg and dominated politics throughout the 20th century.
Norway.
The Conservative Party of Norway (Norwegian: Høyre, literally "right") was formed by the old upper class of state officials and wealthy merchants to fight the populist democracy of the Liberal Party, but lost power in 1884 when parliamentarian government was first practised. It formed its first government under parliamentarism in 1889, and continued to alternate in power with the Liberals until the 1930s, when Labour became the dominant political party. It has elements both of paternalism, stressing the responsibilities of the state, and of economic liberalism. It first returned to power in the 1960s. During Kåre Willoch's premiership in the 1980s, much emphasis was laid on liberalizing the credit- and housing market and abolishing the NRK TV and radio monopoly, while supporting law and order in criminal justice and traditional norms in education
Sweden.
Sweden's conservative party, the Moderate Party, was formed in 1904, two years after the founding of the liberal party. The party emphasizes tax reductions, deregulation of private enterprise, and privatization of schools, hospitals and kindergartens.
Switzerland.
There are a number of conservative parties in Switzerland's parliament, the Federal Assembly. These include the largest, the Swiss People's Party (SVP), the Christian Democratic People's Party (CVP), represented in the Federal Council or cabinet by Doris Leuthard (in 2011), and the Conservative Democratic Party of Switzerland (BDP), which is a splinter of the SVP created in the aftermath to the election of Eveline Widmer-Schlumpf as Federal Council.
The Swiss People's Party (SVP or UDC) was formed from the 1971 merger of the Party of Farmers, Traders, and Citizens, formed in 1917 and the smaller Swiss Democratic Party, formed in 1942. The SVP emphasized agricultural policy, and was strong among farmers in German-speaking Protestant areas. As Switzerland considered closer relations with the European Union in the 1990s, the SVP adopted a more militant protectionist and isolationist stance. This stance has allowed it to expand into German-speaking Catholic mountainous areas. The Anti-Defamation League, a non-Swiss lobby group based in the USA has accused them of manipulating issues such as immigration, Swiss neutrality and welfare benefits, awakening anti-Semitism and racism. The Council of Europe has called the SVP "extreme right", although some scholars dispute this classification. Hans-Georg Betz for example describes it as "populist radical right".
United Kingdom.
Edmund Burke is often considered the father of English conservatism and was a member of the Whig Party. Though the modern Conservative Party is generally thought to derive from the Tory party and the MP'S of that party are still frequently referred to as Tories.
Modern conservatism in different countries.
While conservatism has been seen as an appeal to traditional, hierarchical society, some writers, such as Samuel P. Huntington, see it as situational. Under this definition, conservatives are seen as defending the established institutions of their time.
Australia.
The Liberal Party of Australia adheres to the principles of social conservatism and liberal conservatism. It is Liberal in the sense of economics. Other conservative parties are the National Party of Australia, a sister party of the Liberals, Family First Party, Democratic Labor Party, Shooters Party and the Katter's Australian Party.
The second largest party in the country, the Australian Labor Party's dominant faction is Labor Right, a socially conservative element. Australia undertook significant economic reform under the Australian Labor Party in the mid-1980s. Consequently, issues like protectionism, welfare reform, privatization and deregulation are no longer debated in the political space as they are in Europe or North America. Moser and Catley explain, "In America, 'liberal' means left-of-center, and it is a pejorative term when used by conservatives in adversarial political debate. In Australia, of course, the conservatives are in the Liberal Party." Jupp points out that, " decline in English influences on Australian reformism and radicalism, and appropriation of the symbols of Empire by conservatives continued under the Liberal Party leadership of Sir Robert Menzies, which lasted until 1966."
India.
In India, Bharatiya Janata Party (BJP) represents conservative politics nationally and occupies the traditional right wing position in the left–right political spectrum. Social conservatism, Hindutva and Cultural Nationalism are some major ideologies of BJP.
South Korea.
South Korea's major conservative party, the Saenuri Party or 새누리당, changed its form throughout its history. First it was the Democratic-Republican Party(1963~1980); its head was Park Chung-hee who seized power in a 1961 military coup d'état and ruled as an unelected military strongman until his formal election as president in 1963. He was president for 16 years, until his assassination on October 26, 1979. The Democratic Justice Party inherited the same ideology as the Democratic-Republican Party. Its head, Chun Doo-hwan, also gained power through a coup. His followers called themselves the Hanahoe. The Democratic Justice Party changed its form and acted to suppress the opposition party and to follow the people's demand for direct elections. The party's Roh Tae-woo became the first president who was elected through direct election. The next form of the major conservative party was the Democratic-Liberal Party. Again, through election, its second leader, Kim Young-sam, became the fourteenth president of Korea. When the conservative party was beaten by the opposition party in the general election, it changed its form again to follow the party members' demand for reforms. It became the New Korean Party. It changed again one year later since the President Kim Young-sam was blamed by the citizen for the IMF. It changed its name to Grand National Party (Hannara-dang) (1998~2011). Since the late Kim Dae-jung assumed the presidency in 1998, GNP had not been the ruling party until Lee Myung-bak won the presidential election of 2007. It renamed to Saenoori Party (새누리당) in 2011.
United States.
The meaning of "conservatism" in America has little in common with the way the word is used elsewhere. As Ribuffo (2011) notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism." Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that largely controlled domestic policy in Congress from 1937 to 1963.
Major priorities within American conservatism include support for tradition, law-and-order, Christianity, anti-communism, and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments." Economic conservatives and libertarians favor small government, low taxes, limited regulation, and free enterprise. Some social conservatives see traditional social values threatened by secularism, so they support school prayer and oppose abortion and homosexuality. Neoconservatives want to expand American ideals throughout the world and show a strong support for Israel. Paleoconservatives, in opposition to multiculturalism, press for restrictions on immigration. Most U.S. conservatives prefer Republicans over Democrats, and most factions favor a strong foreign policy and a strong military. The conservative movement of the 1950s attempted to bring together these divergent strands, stressing the need for unity to prevent the spread of "Godless Communism", which Reagan later labeled an "evil empire". During the Reagan administration, conservatives also supported the so-called "Reagan Doctrine" under which the U.S., as part of a Cold War strategy, provided military and other support to guerrilla insurgencies that were fighting governments identified as socialist or communist.
Other modern conservative positions include opposition to world government and opposition to environmentalism. On average, American conservatives desire tougher foreign policies than liberals do.
Most recently, the Tea Party movement, founded in 2009, has proven a large outlet for populist American conservative ideas. Their stated goals include rigorous adherence to the U.S. Constitution, lower taxes, and opposition to a growing role for the federal government in health care. Electorally, it was considered a key force in Republicans reclaiming control of the U.S. House of Representatives in 2010.
Characteristics of conservatism in France, Italy, Russia, Poland, UK, US and Israel.
This is a broad Checklist of modern conservatism in seven countries.
Psychology.
Following the Second World War, psychologists conducted research into the different motives and tendencies that account for ideological differences between left and right. The early studies focused on conservatives, beginning with Theodor W. Adorno's "The Authoritarian Personality" (1950) based on the F-scale personality test. This book has been heavily criticized on theoretical and methodological grounds, but some of its findings have been confirmed by further empirical research.
In 1973, British psychologist Glenn Wilson published an influential book providing evidence that a general factor underlying conservative beliefs is "fear of uncertainty". A meta-analysis of research literature by Jost, Glaser, Kruglanski, and Sulloway in 2003 found that many factors, such as intolerance of ambiguity and need for cognitive closure, contribute to the degree of one's political conservatism. A study by Kathleen Maclay stated these traits "might be associated with such generally valued characteristics as personal commitment and unwavering loyalty." The research also suggested that while most people are resistant to change, liberals are more tolerant of it.
According to psychologist Bob Altemeyer, individuals who are politically conservative tend to rank high in right-wing authoritarianism on his RWA scale. This finding was echoed by Theodor Adorno. A study done on Israeli and Palestinian students in Israel found that RWA scores of right-wing party supporters were significantly higher than those of left-wing party supporters. However, a 2005 study by H. Michael Crowson and colleagues suggested a moderate gap between RWA and other conservative positions. "The results indicated that conservatism is not synonymous with RWA."
Psychologist Felicia Pratto and her colleagues have found evidence to support the idea that a high social dominance orientation (SDO) is strongly correlated with conservative political views, and opposition to social engineering to promote equality, though Pratto's findings have been highly controversial . Pratto and her colleagues found that high SDO scores were highly correlated with measures of prejudice. David J. Schneider, however, argued for a more complex relationships between the three factors, writing "correlations between prejudice and political conservative are reduced virtually to zero when controls for SDO are instituted, suggesting that the conservatism–prejudice link is caused by SDO". Kenneth Minogue criticized Pratto's work, saying "It is characteristic of the conservative temperament to value established identities, to praise habit and to respect prejudice, not because it is irrational, but because such things anchor the darting impulses of human beings in solidities of custom which we do not often begin to value until we are already losing them. Radicalism often generates youth movements, while conservatism is a condition found among the mature, who have discovered what it is in life they most value."
A 1996 study on the relationship between racism and conservatism found that the correlation was stronger among more educated individuals, though "anti-Black affect had essentially no relationship with political conservatism at any level of educational or intellectual sophistication". They also found that the correlation between racism and conservatism could be entirely accounted for by their mutual relationship with social dominance orientation.
A 2008 research report found that conservatives are happier than liberals, and that as income inequality increases, this difference in relative happiness increases, because conservatives (more than liberals) possess an ideological buffer against the negative hedonic effects of economic inequality.

</doc>
<doc id="6677" url="https://en.wikipedia.org/wiki?curid=6677" title="Classical liberalism">
Classical liberalism

Classical liberalism is a political ideology and a branch of liberalism which advocates civil liberties and political freedom with representative democracy under the rule of law and emphasizes economic freedom.
Classical liberalism developed in the 19th century in Europe and the United States. Although classical liberalism built on ideas that had already developed by the end of the 18th century, it advocated a specific kind of society, government and public policy as a response to the Industrial Revolution and urbanization. Notable individuals whose ideas have contributed to classical liberalism include John Locke, Jean-Baptiste Say, Thomas Malthus, and David Ricardo. It drew on the economics of Adam Smith and on a belief in natural law, utilitarianism, and progress.
Meaning of the term.
The term "classical liberalism" was applied in retrospect to distinguish earlier 19th-century liberalism from the newer social liberalism. The phrase "classical liberalism" is also sometimes used to refer to all forms of liberalism before the 20th century, and some conservatives and libertarians use the term classical liberalism to describe their belief in the primacy of individual freedom and minimal government. It is not always clear which meaning is intended.
Evolution of core beliefs.
Core beliefs of classical liberals included new ideas—which departed from both the older conservative idea of society as a family and from later sociological concept of society as complex set of social networks—that individuals were "egoistic, coldly calculating, essentially inert and atomistic" and that society was no more than the sum of its individual members.
Classical liberals agreed with Thomas Hobbes that government had been created by individuals to protect themselves from one another, and that the purpose of government should be to minimize conflict between individuals that would otherwise arise in a state of nature.
These beliefs were complemented by a belief that labourers could be best motivated by financial incentive. This led classical liberal politicians at the time to pass the Poor Law Amendment Act 1834, which limited the provision of social assistance, because classical liberals believed in markets as the mechanism that would most efficiently lead to wealth. Adopting Thomas Malthus's population theory, they saw poor urban conditions as inevitable; they believed population growth would outstrip food production, and they regarded that consequence desirable, because starvation would help limit population growth. They opposed any income or wealth redistribution, which they believed would be dissipated by the lowest orders.
Drawing on selected ideas of Adam Smith, classical liberals believed that it is in the common interest that all individuals must be able to secure their own economic self-interest, without government direction. They were critical of the welfare state as interfering in a free market. They criticised labour's group rights being pursued at the expense of individual rights, while they accepted big corporations' rights being pursued at the expense of inequality of bargaining power noted by Adam Smith:
Classical liberals believed that individuals should be free to obtain work from the highest-paying employers, while the profit motive would ensure that products that people desired were produced at prices they would pay. In a free market, both labour and capital would receive the greatest possible reward, while production would be organised efficiently to meet consumer demand.
An important caveat, as exemplified by the law of equal liberty and the Lockean proviso is that all must be able to pursue liberty equally, without infringing on the equal rights of others.
It was not until emergence of social liberalism that child labour was forbidden, minimum standards of worker safety were introduced, a minimum wage and old age pensions were established, and financial institutions were regulated with the goal of fighting cyclic depressions, monopolies, and cartels. Classical liberals opposed these new laws, which they viewed as an unjust interference of the state. They argued for what they called a "slim state", limited to the following functions:
They assert that rights are of a "negative" nature which require other individuals (and governments) to refrain from interfering with the free market, whereas social liberals asserts that individuals have positive rights, such as the right to vote, the right to an education, the right to health care, and the right to a living wage. For society to guarantee positive rights requires taxation over and above the minimum needed to enforce negative rights.
Core beliefs of classical liberals did not necessarily include democracy where law is made by majority vote by citizens, because "there is nothing in the bare idea of majority rule to show that majorities will always respect the rights of property or maintain rule of law." For example, James Madison argued for a constitutional republic with protections for individual liberty over a pure democracy, reasoning that, in a pure democracy, a "common passion or interest will, in almost every case, be felt by a majority of the whole...and there is nothing to check the inducements to sacrifice the weaker party..."
In the late 19th century, classical liberalism developed into neo-classical liberalism, which argued for government to be as small as possible to allow the exercise of individual freedom. In its most extreme form, neo-classical liberalism advocated Social Darwinism. Right-libertarianism is a modern form of neo-classical liberalism.
Hayek's typology of beliefs.
Friedrich Hayek identified two different traditions within classical liberalism: the "British tradition" and the "French tradition". Hayek saw the British philosophers Bernard Mandeville, David Hume, Adam Smith, Adam Ferguson, Josiah Tucker and William Paley as representative of a tradition that articulated beliefs in empiricism, the common law, and in traditions and institutions which had spontaneously evolved but were imperfectly understood. The French tradition included Rousseau, Condorcet, the Encyclopedists and the Physiocrats. This tradition believed in rationalism and sometimes showed hostility to tradition and religion. Hayek conceded that the national labels did not exactly correspond to those belonging to each tradition: Hayek saw the Frenchmen Montesquieu, Constant and Tocqueville as belonging to the "British tradition" and the British Thomas Hobbes, Priestley, Richard Price and Thomas Paine as belonging to the "French tradition". Hayek also rejected the label "laissez faire" as originating from the French tradition and alien to the beliefs of Hume and Smith.
History.
Classical liberalism in Britain developed from Whiggery and radicalism, and was also heavily influenced by French physiocracy, and represented a new political ideology. Whiggery had become a dominant ideology following the Glorious Revolution of 1688, and was associated with the defence of Parliament, upholding the rule of law and defending landed property. The origins of rights were seen as being in an ancient constitution, which had existed from time immemorial. These rights, which some Whigs considered to include freedom of the press and freedom of speech, were justified by custom rather than by natural rights. They believed that the power of the executive had to be constrained. While they supported limited suffrage, they saw voting as a privilege, rather than as a right. However, there was no consistency in Whig ideology, and diverse writers including John Locke, David Hume, Adam Smith and Edmund Burke were all influential among Whigs, although none of them was universally accepted.
British radicals, from the 1790s to the 1820s, concentrated on parliamentary and electoral reform, emphasising natural rights and popular sovereignty. Richard Price and Joseph Priestley adapted the language of Locke to the ideology of radicalism. The radicals saw parliamentary reform as a first step toward dealing with their many grievances, including the treatment of Protestant Dissenters, the slave trade, high prices and high taxes.
There was greater unity to classical liberalism ideology than there had been with Whiggery. Classical liberals were committed to individualism, liberty and equal rights. They believed that required a free economy with minimal government interference. Writers such as John Bright and Richard Cobden opposed both aristocratic privilege and property, which they saw as an impediment to the development of a class of yeoman farmers. Some elements of Whiggery opposed this new thinking, and were uncomfortable with the commercial nature of classical liberalism. These elements became associated with conservatism.
Classical liberalism was the dominant political theory in Britain from the early 19th century until the First World War. Its notable victories were the Catholic Emancipation Act of 1829, the Reform Act of 1832, and the repeal of the Corn Laws in 1846. The Anti-Corn Law League brought together a coalition of liberal and radical groups in support of free trade under the leadership of Richard Cobden and John Bright, who opposed militarism and public expenditure. Their policies of low public expenditure and low taxation were adopted by William Ewart Gladstone when he became chancellor of the exchequer and later prime minister. Classical liberalism was often associated with religious dissent and nonconformism.
Although classical liberals aspired to a minimum of state activity, they accepted the principle of government intervention in the economy from the early 19th century with passage of the Factory Acts. From around 1840 to 1860, "laissez-faire" advocates of the Manchester School and writers in "The Economist" were confident that their early victories would lead to a period of expanding economic and personal liberty and world peace but would face reversals as government intervention and activity continued to expand from the 1850s. Jeremy Bentham and James Mill, although advocates of "laissez faire", non-intervention in foreign affairs, and individual liberty, believed that social institutions could be rationally redesigned through the principles of Utilitarianism. The Conservative prime minister, Benjamin Disraeli, rejected classical liberalism altogether and advocated Tory Democracy. By the 1870s, Herbert Spencer and other classical liberals concluded that historical development was turning against them. By the First World War, the Liberal Party had largely abandoned classical liberal principles.
The changing economic and social conditions of the 19th century led to a division between neo-classical and social (or welfare) liberals who, while agreeing on the importance of individual liberty, differed on the role of the state. Neo-classical liberals, who called themselves "true liberals", saw Locke's "Second Treatise" as the best guide, and emphasised "limited government", while social liberals supported government regulation and the welfare state. Herbert Spencer in Britain and William Graham Sumner were the leading neo-classical liberal theorists of the 19th century. Neo-classical liberalism has continued into the contemporary era, with writers such as John Rawls. The evolution from classical to social/welfare liberalism is reflected in Britain in, for example, the evolution of the thought of John Maynard Keynes.
In the United States, liberalism took a strong root because it had little opposition to its ideals, whereas in Europe liberalism was opposed by many reactionary interests. Thomas Jefferson adopted many of the ideals of liberalism but, in the Declaration of Independence, changed Locke's "life, liberty, and property" to the more socially liberal "life, liberty, and the pursuit of happiness". As America grew, industry became a larger and larger part of American life; and, during the term of America's first populist president, Andrew Jackson, economic questions came to the forefront. The economic ideas of the Jacksonian era were almost universally the ideas of classical liberalism. Freedom was maximised when the government took a "hands off" attitude toward industrial development and supported the value of the currency by freely exchanging paper money for gold. The ideas of classical liberalism remained essentially unchallenged until a series of depressions, thought to be impossible according to the tenets of classical economics, led to economic hardship from which the voters demanded relief. In the words of William Jennings Bryan, "You shall not crucify the American farmer on a cross of gold." Classical liberalism remained the orthodox belief among American businessmen until the Great Depression. The Great Depression saw a sea change in liberalism, leading to the development of modern liberalism. In the words of Arthur Schlesinger Jr.:
Some modern scholars of liberalism, however, say that no particularly meaningful distinction between classical and modern liberalism exists. Alan Wolfe summarizes this viewpoint, which:
Intellectual sources.
John Locke.
Central to classical liberal ideology was their interpretation of John Locke's "Second Treatise of Government" and "A Letter Concerning Toleration", which had been written as a defence of the Glorious Revolution of 1688. Although these writings were considered too radical at the time for Britain's new rulers, they later came to be cited by Whigs, radicals and supporters of the American Revolution. However, much of later liberal thought was absent in Locke's writings or scarcely mentioned, and his writings have been subject to various interpretations. There is little mention, for example, of constitutionalism, the separation of powers, and limited government.
James L. Richardson identified five central themes in Locke's writing: individualism, consent, the concepts of the rule of law and government as trustee, the significance of property, and religious toleration. Although Locke did not develop a theory of natural rights, he envisioned individuals in the state of nature as being free and equal. The individual, rather than the community or institutions, was the point of reference. Locke believed that individuals had given consent to government and therefore authority derived from the people rather than from above. This belief would influence later revolutionary movements.
As a trustee, Government was expected to serve the interests of the people, not the rulers, and rulers were expected to follow the laws enacted by legislatures. Locke also held that the main purpose of men uniting into commonwealths and governments was for the preservation of their property. Despite the ambiguity of Locke's definition of property, which limited property to "as much land as a man tills, plants, improves, cultivates, and can use the product of", this principle held great appeal to individuals possessed of great wealth.
Locke held that the individual had the right to follow his own religious beliefs and that the state should not impose a religion against Dissenters. But there were limitations. No tolerance should be shown for atheists, who were seen as amoral, or to Catholics, who were seen as owing allegiance to the Pope over their own national government.
Adam Smith.
Adam Smith's "The Wealth of Nations", published in 1776, was to provide most of the ideas of economics, at least until the publication of J. S. Mill's "Principles" in 1848. Smith addressed the motivation for economic activity, the causes of prices and the distribution of wealth, and the policies the state should follow to maximise wealth.
Smith wrote that as long as supply, demand, prices, and competition were left free of government regulation, the pursuit of material self-interest, rather than altruism, would maximise the wealth of a society through profit-driven production of goods and services. An "invisible hand" directed individuals and firms to work toward the nation's good as an unintended consequence of efforts to maximise their own gain. This provided a moral justification for the accumulation of wealth, which had previously been viewed by some as sinful.
He assumed that workers could be paid wages as low as was necessary for their survival, which was later transformed by Ricardo and Malthus into the "Iron Law of Wages". His main emphasis was on the benefit of free internal and international trade, which he thought could increase wealth through specialisation in production. He also opposed restrictive trade preferences, state grants of monopolies, and employers' organisations and trade unions. Government should be limited to defence, public works and the administration of justice, financed by taxes based on income.
Smith's economics was carried into practice in the nineteenth century with the lowering of tariffs in the 1820s, the repeal of the Poor Relief Act, that had restricted the mobility of labour, in 1834, and the end of the rule of the East India Company over India in 1858.
Say, Malthus, and Ricardo.
In addition to Adam Smith's legacy, Say's law, Malthus' theories of population and Ricardo's iron law of wages became central doctrines of classical economics. The pessimistic nature of these theories provided a basis for criticism of capitalism by its opponents and helped perpetuate the tradition of calling economics the "dismal science."
Jean-Baptiste Say was a French economist who introduced Adam Smith's economic theories into France and whose commentaries on Smith were read in both France and Britain. Say challenged Smith's labour theory of value, believing that prices were determined by utility and also emphasised the critical role of the entrepreneur in the economy. However neither of those observations became accepted by British economists at the time. His most important contribution to economic thinking was Say's law, which was interpreted by classical economists that there could be no overproduction in a market, and that there would always be a balance between supply and demand. This general belief influenced government policies until the 1930s. Following this law, since the economic cycle was seen as self-correcting, government did not intervene during periods of economic hardship because it was seen as futile.
Thomas Malthus wrote two books, "An essay on the principle of population", published in 1798, and "Principles of political economy", published in 1820. The second book which was a rebuttal of Say's law had little influence on contemporary economists. His first book however became a major influence on classical liberalism. In that book, Malthus claimed that population growth would outstrip food production, because population grew geometrically, while food production grew arithmetically. As people were provided with food, they would reproduce until their growth outstripped the food supply. Nature would then provide a check to growth in the forms of vice and misery. No gains in income could prevent this, and any welfare for the poor would be self-defeating. The poor were in fact responsible for their own problems which could have been avoided through self-restraint.
David Ricardo, who was an admirer of Adam Smith, covered many of the same topics but while Smith drew conclusions from broadly empirical observations, Ricardo used deduction, drawing conclusions by reasoning from basic assumptions. While Ricardo accepted Smith's labour theory of value, as the general rule of prices under a free market, he acknowledged that utility influences the price of scarce items. According to his Law of rent, the rent of land was determined by the surplus production obtainable compared to the best free marginal land, or, if none were available, to the subsistence required by the tenants. Wages were thus set by the best available rent-free land. This contradicted Malthus's Iron Law of Wages, in which wages could never rise beyond subsistence levels, and sparked a lengthy debate between the two men. Ricardo explained profits as a return on capital, which itself was the product of labour. However, it was a source of controversy as to whether interest should exist at all in a free market. Ricardian socialists argued that it would not. By contrast, Henry George's complementary law of interest showed, through deduction, that interest and wages rise and fall together, and that they can be substituted.
Utilitarianism.
Utilitarianism provided the political justification for implementation of economic liberalism by British governments, which was to dominate economic policy from the 1830s. Although utilitarianism prompted legislative and administrative reform, and John Stuart Mill's later writings on the subject foreshadowed the welfare state, it was mainly used as a justification for "laissez faire".
The central concept of utilitarianism, which was developed by Jeremy Bentham, was that public policy should seek to provide "the greatest happiness of the greatest number". While this could be interpreted as a justification for state action to reduce poverty, it was used by classical liberals to justify inaction with the argument that the net benefit to all individuals would be higher.
Political economy.
Classical liberals saw utility as the foundation for public policies. This broke both with conservative "tradition" and Lockean "natural rights", which were seen as irrational. Utility, which emphasises the happiness of individuals, became the central ethical value of all liberalism. Although utilitarianism inspired wide-ranging reforms, it became primarily a justification for "laissez-faire" economics. However, classical liberals rejected Adam Smith's belief that the "invisible hand" would lead to general benefits and embraced Thomas Robert Malthus' view that population expansion would prevent any general benefit and David Ricardo's view of the inevitability of class conflict. Laissez faire was seen as the only possible economic approach, and any government intervention was seen as useless and harmful. The Poor Law Amendment Act 1834 was defended on "scientific or economic principles" while the authors of the Elizabethan Poor Law of 1601 were seen as not having had the benefit of reading Malthus.
Commitment to "laissez faire", however, was not uniform. Some economists advocated state support of public works and education. Classical liberals were also divided on free trade. Ricardo, for example, expressed doubt that the removal of grain tariffs advocated by Richard Cobden and the Anti-Corn Law League would have any general benefits. Most classical liberals also supported legislation to regulate the number of hours that children were allowed to work and usually did not oppose factory reform legislation.
Despite the pragmatism of classical economists, their views were expressed in dogmatic terms by such popular writers as Jane Marcet and Harriet Martineau. The strongest defender of "laissez faire" was "The Economist" founded by James Wilson in 1843. "The Economist" criticised Ricardo for his lack of support for free trade and expressed hostility to welfare, believing that the lower orders were responsible for their economic circumstances. "The Economist" took the position that regulation of factory hours was harmful to workers and also strongly opposed state support for education, health, the provision of water, and granting of patents and copyrights.
"The Economist" also campaigned against the Corn Laws that protected landlords in the United Kingdom of Great Britain and Ireland against competition from less expensive foreign imports of cereal products. A rigid belief in "laissez faire" guided the government response in 1846–1849 to the Great Famine in Ireland, during which an estimated 1.5 million people died. The minister responsible for economic and financial affairs, Charles Wood, expected that private enterprise and free trade, rather than government intervention, would alleviate the famine. The Corn Laws were finally repealed in 1846 by removal tariffs on grain which kept the price of bread artificially high. However, repeal of the Corn Laws came too late to stop Irish famine, partly because it was done in stages over three years.
Free trade and world peace.
Several liberals, including Adam Smith and Richard Cobden, argued that the free exchange of goods between nations could lead to world peace. Erik Gartzke states, "Scholars like Montesquieu, Adam Smith, Richard Cobden, Norman Angell, and Richard Rosecrance have long speculated that free markets have the potential to free states from the looming prospect of recurrent warfare." American political scientists John R. Oneal and Bruce M. Russett, well known for their work on the democratic peace theory, state:
Adam Smith argued in the "Wealth of Nations" that, as societies progressed from hunter gatherers to industrial societies, the spoils of war would rise but that the costs of war would rise further, making war difficult and costly for industrialised nations.
Cobden believed that military expenditures worsened the welfare of the state and benefited a small but concentrated elite minority, summing up British imperialism, which he believed was the result of the economic restrictions of mercantilist policies. To Cobden, and many classical liberals, those who advocated peace must also advocate free markets.
The belief that free trade would promote peace was widely shared by English liberals of the 19th and early 20th century, leading the economist John Maynard Keynes (1883–1946), who was a classical liberal in his early life, to say that this was a doctrine on which he was "brought up" and which he held unquestioned until at least the 1920s. A related manifestation of this idea was the argument of Norman Angell (1872–1967), most famously before World War I in "The Great Illusion" (1909), that the interdependence of the economies of the major powers was now so great that war between them was futile and irrational (and therefore unlikely).

</doc>
<doc id="6678" url="https://en.wikipedia.org/wiki?curid=6678" title="Cat">
Cat

The domestic cat () or the feral cat () is a small, typically furry, carnivorous mammal. They are often called house cats when kept as indoor pets or simply cats when there is no need to distinguish them from other felids and felines. Cats are often valued by humans for companionship and for their ability to hunt vermin. There are more than 70 cat breeds; different associations proclaim different numbers according to their standards.
Cats are similar in anatomy to the other felids, with a strong, flexible body, quick reflexes, sharp retractable claws, and teeth adapted to killing small prey. Cat senses fit a crepuscular and predatory ecological niche. Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals. They can see in near darkness. Like most other mammals, cats have poorer color vision and a better sense of smell than humans. Cats, despite being solitary hunters, are a social species and cat communication includes the use of a variety of vocalizations (mewing, purring, trilling, hissing, growling, and grunting), as well as cat pheromones and types of cat-specific body language.
Cats have a high breeding rate. Under controlled breeding, they can be bred and shown as registered pedigree pets, a hobby known as cat fancy. Failure to control the breeding of pet cats by neutering and the abandonment of former household pets has resulted in large numbers of feral cats worldwide, requiring population control. This has contributed, along with habitat destruction and other factors, to the extinction of many bird species. Cats have been known to extirpate a bird species within specific regions and may have contributed to the extinction of isolated island populations. Cats are thought to be primarily, though not solely, responsible for the extinction of 33 species of birds, and the presence of feral and free ranging cats makes some locations unsuitable for attempted species reintroduction in otherwise suitable locations.
Since cats were venerated in ancient Egypt, they were commonly believed to have been domesticated there, but there may have been instances of domestication as early as the Neolithic from around 9,500 years ago (7,500 BCE). A genetic study in 2007 concluded that domestic cats are descended from Near Eastern wildcats, having diverged around 8,000 BCE in West Asia. A 2016 study found that leopard cats were undergoing domestication independently in China around 5,500 BCE, though this line of partially domesticated cats leaves no trace in the domesticated populations of today.
As of a 2007 study, cats are the second most popular pet in the United States by number of pets owned, behind the first, which is freshwater fish.
Taxonomy and evolution.
The felids are a rapidly evolving family of mammals that share a common ancestor only 10–15 million years ago and include lions, tigers, cougars and many others. Within this family, domestic cats ("Felis catus") are part of the genus "Felis", which is a group of small cats containing about seven species (depending upon classification scheme). Members of the genus are found worldwide and include the jungle cat ("Felis chaus") of southeast Asia, European wildcat ("F. silvestris silvestris"), African wildcat ("F. s. lybica"), the Chinese mountain cat ("F. bieti"), and the Arabian sand cat ("F. margarita"), among others.
The domestic cat was first classified as "Felis catus" by Carl Linnaeus in the 10th edition of his "Systema Naturae" published in 1758. Because of modern phylogenetics, domestic cats are usually regarded as another subspecies of the wildcat, "F. silvestris". This has resulted in mixed usage of the terms, as the domestic cat can be called by its subspecies name, "Felis silvestris catus". Wildcats have also been referred to as various subspecies of "F. catus", but in 2003, the International Commission on Zoological Nomenclature fixed the name for wildcats as "F. silvestris". The most common name in use for the domestic cat remains "F. catus", following a convention for domesticated animals of using the earliest (the senior) synonym proposed. Sometimes, the domestic cat has been called "Felis domesticus" or "Felis domestica", as proposed by German naturalist J. C. P. Erxleben in 1777 but these are not valid taxonomic names and have been used only rarely in scientific literature, because Linnaeus's binomial takes precedence. A population of Transcaucasian black feral cats was once classified as "Felis daemon" (Satunin 1904) but now this population is considered to be a part of domestic cat.
All the cats in this genus share a common ancestor that probably lived around 6–7 million years ago in Asia. The exact relationships within the Felidae are close but still uncertain, e.g. the Chinese mountain cat is sometimes classified (under the name "Felis silvestris bieti") as a subspecies of the wildcat, like the North African variety "F. s. lybica".
In comparison to dogs, cats have not undergone major changes during the domestication process, as the form and behavior of the domestic cat is not radically different from those of wildcats and domestic cats are perfectly capable of surviving in the wild. Fully domesticated house cats often interbreed with feral "F. catus" populations. This limited evolution during domestication means that hybridisation can occur with many other felids, notably the Asian leopard cat. Several natural behaviors and characteristics of wildcats may have preadapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play and relatively high intelligence. Several small felid species may have an inborn tendency towards tameness.
Cats have either a mutualistic or commensal relationship with humans. Two main theories are given about how cats were domesticated. In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin. This has been criticized as implausible, because the reward for such an effort may have been too little; cats generally do not carry out commands and although they do eat rodents, other species such as ferrets or terriers may be better at controlling these pests. The alternative idea is that cats were simply tolerated by people and gradually diverged from their wild relatives through natural selection, as they adapted to hunting the vermin found around humans in towns and villages.
Nomenclature and etymology.
The English word 'cat' (Old English "catt") is in origin a loanword, introduced to many languages of Europe from Latin "cattus" and Byzantine Greek , including Portuguese and Spanish "gato", French "chat", German "Katze", Lithuanian "katė", and Old Church Slavonic "kotka", among others. The ultimate source of the word is Afroasiatic, presumably from Late Egyptian "čaute", the feminine of "čaus" "wildcat". An alternative word with cognates in many languages is English 'puss' ('pussycat'). Attested only from the 16th century, it may have been introduced from Dutch "poes" or from Low German "puuskatte", related to Swedish "kattepus", or Norwegian "pus", "pusekatt". Similar forms exist in Lithuanian "puižė" and Irish "puiscín". The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.
A group of cats is referred to as a "clowder" or a "glaring", a male cat is called a "tom" or "tomcat" (or a "gib", if neutered), an unaltered female is called a "queen", and a juvenile cat is referred to as a "kitten". The male progenitor of a cat, especially a pedigreed cat, is its "sire", and its female progenitor is its "dam". In Early Modern English, the word 'kitten' was interchangeable with the now-obsolete word 'catling'.
A pedigreed cat is one whose ancestry is recorded by a cat fancier organization. A purebred cat is one whose ancestry contains only individuals of the same breed. Many pedigreed and especially purebred cats are exhibited as show cats. Cats of unrecorded, mixed ancestry are referred to as domestic short-haired or domestic long-haired cats, by coat type, or commonly as random-bred, moggies (chiefly British), or (using terms borrowed from dog breeding) mongrels or mutt-cats.
While the African wildcat is the ancestral subspecies from which domestic cats are descended, and wildcats and domestic cats can completely interbreed (Being subspecies of the same species), several intermediate stages occur between domestic pet and pedigree cats on one hand and those entirely wild animals on the other. The semiferal cat, a mostly outdoor cat, is not owned by any one individual, but is generally friendly to people and may be fed by several households. Feral cats are associated with human habitation areas and may be fed by people or forage for food, but are typically wary of human interaction.
Biology.
Anatomy.
Domestic cats are similar in size to the other members of the genus "Felis", typically weighing between . Some breeds, however, such as the Maine Coon, can occasionally exceed . Conversely, very small cats, less than , have been reported. The world record for the largest cat is . The smallest adult cat ever officially recorded weighed around . Feral cats tend to be lighter as they have more limited access to food than house cats. In the Boston area, the average feral adult male will weigh and average feral female . Cats average about in height and in head/body length (males being larger than females), with tails averaging in length.
Cats have seven cervical vertebrae, as do almost all mammals; 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae like most mammals (humans have five); and a variable number of caudal vertebrae in the tail (humans retain three to five caudal vertebrae, fused into an internal coccyx). The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis. Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.
The cat skull is unusual among mammals in having very large eye sockets and a powerful and specialized jaw. Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication. Though cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar, they are nonetheless subject to occasional tooth loss and infection.
Cats, like dogs, are digitigrades. They walk directly on their toes, with the bones of their feet making up the lower part of the visible leg. Cats are capable of walking very precisely, because like all felines, they directly register; that is, they place each hind paw (almost) directly in the print of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for their hind paws when they navigate rough terrain. Unlike most mammals, when cats walk, they use a "pacing" gait; that is, they move the two legs on one side of the body before the legs on the other side. This trait is shared with camels and giraffes. As a walk speeds up into a trot, a cat's gait changes to be a "diagonal" gait, similar to that of most other mammals (and many other land animals, such as lizards): the diagonally opposite hind and fore legs move simultaneously.
Like almost all members of the Felidae, cats have protractable and retractable claws. In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the fore feet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Most cats have five claws on their front paws, and four on their rear paws. The fifth front claw (the dewclaw) is proximal to the other claws. More proximally is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists, is the carpal pad, also found on the paws of big cats and dogs. It has no function in normal walking, but is thought to be an antiskidding device used while jumping. Some breeds of cats are prone to polydactyly (extra toes and claws). These are particularly common along the northeast coast of North America.
Physiology.
Cats are familiar and easily kept animals, and their physiology has been particularly well studied; it generally resembles those of other carnivorous mammals, but displays several unusual features probably attributable to cats' descent from desert-dwelling species. For instance, cats are able to tolerate quite high temperatures: Humans generally start to feel uncomfortable when their skin temperature passes about , but cats show no discomfort until their skin reaches around , and can tolerate temperatures of up to if they have access to water.
Cats conserve heat by reducing the flow of blood to their skin and lose heat by evaporation through their mouths. Cats have minimal ability to sweat, with glands located primarily in their paw pads, and pant for heat relief only at very high temperatures (but may also pant when stressed). A cat's body temperature does not vary throughout the day; this is part of cats' general lack of circadian rhythms and may reflect their tendency to be active both during the day and at night. Cats' feces are comparatively dry and their urine is highly concentrated, both of which are adaptations to allow cats to retain as much water as possible. Their kidneys are so efficient, they can survive on a diet consisting only of meat, with no additional water, and can even rehydrate by drinking seawater.
Cats are obligate carnivores: their physiology has evolved to efficiently process meat, and they have difficulty digesting plant matter. In contrast to omnivores such as rats, which only require about 4% protein in their diet, about 20% of a cat's diet must be protein. Cats are unusually dependent on a constant supply of the amino acid arginine, and a diet lacking arginine causes marked weight loss and can be rapidly fatal. Another unusual feature is that the cat cannot produce taurine, with taurine deficiency causing macular degeneration, wherein the cat's retina slowly degenerates, causing irreversible blindness.
A cat's gastrointestinal tract is adapted to meat eating, being much shorter than that of omnivores and having low levels of several of the digestive enzymes needed to digest carbohydrates. These traits severely limit the cat's ability to digest and use plant-derived nutrients, as well as certain fatty acids. Despite the cat's meat-oriented physiology, several vegetarian or vegan cat foods have been marketed that are supplemented with chemically synthesized taurine and other nutrients, in attempts to produce a complete diet. However, some of these products still fail to provide all the nutrients cats require, and diets containing no animal products pose the risk of causing severe nutritional deficiencies.
Cats do eat grass occasionally. A proposed explanation is that cats use grass as a source of folic acid. Another proposed explanation is that it is used to supply dietary fiber.
Senses.
Cats have excellent night vision and can see at only one-sixth the light level required for human vision. This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Another adaptation to dim light is the large pupils of cats' eyes. Unlike some big cats, such as tigers, domestic cats have slit pupils. These slit pupils can focus bright light without chromatic aberration, and are needed since the domestic cat's pupils are much larger, relative to their eyes, than the pupils of the big cats. At low light levels a cat's pupils will expand to cover most of the exposed surface of its eyes. However, domestic cats have rather poor color vision and (like most nonprimate mammals) have only two types of cones, optimized for sensitivity to blue and yellowish green; they have limited ability to distinguish between red and green. A 1993 paper reported a response to middle wavelengths from a system other than the rods which might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.
Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than either dogs or humans, detecting frequencies from 55 Hz to 79,000 Hz, a range of 10.5 octaves, while humans and dogs both have ranges of about 9 octaves. Cats can hear ultrasound, which is important in hunting because many species of rodents make ultrasonic calls. However, they do not communicate using ultrasound like rodents do. Cats' hearing is also sensitive and among the best of any mammal, being most acute in the range of 500 Hz to 32 kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their pinnae), which both amplify sounds and help detect the direction of a noise.
Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about in area, which is about twice that of humans. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol, which they use to communicate through urine spraying and marking with scent glands. Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion. About 70—80% of cats are affected by nepetalactone. This response is also produced by other plants, such as silver vine ("Actinidia polygama") and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.
Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue). Domestic and wild cats share a gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness. Their taste buds instead respond to amino acids, bitter tastes, and acids. Cats and many other animals have a Jacobson's organ located in their mouths that allows them to taste-smell certain aromas in a way which humans have no experience of. Cats also have a distinct temperature preference for their food, preferring food with a temperature around which is similar to that of a fresh kill and routinely rejecting food presented cold or refrigerated (which would signal to the cat that the "prey" item is long dead and therefore possibly toxic/ decomposing).
To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.
Most breeds of cat have a noted fondness for settling in high places, or perching. In the wild, a higher place may serve as a concealed site from which to hunt; domestic cats may strike prey by pouncing from a perch such as a tree branch, as does a leopard. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. During a fall from a high place, a cat can reflexively twist its body and right itself using its acute sense of balance and flexibility. This is known as the cat righting reflex. An individual cat always rights itself in the same way, provided it has the time to do so, during a fall. The height required for this to occur is around . Cats without a tail (e.g. Manx cats) also have this ability, since a cat mostly moves its hind legs and relies on conservation of angular momentum to set up for landing, and the tail is little used for this feat.
Health.
The average lifespan of pet cats has risen in recent years. In the early 1980s it was about seven years, rising to 9.4 years in 1995 and 12–15 years in 2014.
However, cats have been reported as surviving into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38.
Spaying or neutering increases life expectancy: one study found neutered male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females. Having a cat neutered confers health benefits, because castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer.
Despite widespread concern about the welfare of free-roaming cats, the lifespans of neutered feral cats in managed colonies compare favorably with those of pet cats. Neutered cats in managed colonies can also live long lives.
Diseases.
Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries, and chronic disease. Vaccinations are available for many of these diseases, and domestic cats are regularly given treatments to eliminate parasites such as worms and fleas.
Poisoning.
In addition to obvious dangers such as rodenticides, insecticides, and herbicides, cats may be poisoned by many chemicals usually considered safe by their human guardians, because their livers are less effective at some forms of detoxification than those of many other animals, including humans and dogs. Some of the most common causes of poisoning in cats are antifreeze and rodent baits. Cats may be particularly sensitive to environmental pollutants. When a cat has a sudden or prolonged serious illness without any obvious cause, it has possibly been exposed to a toxin.
Many human medicines should never be given to cats. For example, the painkiller paracetamol (or acetaminophen, sold as Tylenol and Panadol) is extremely toxic to cats: even very small doses need immediate treatment and can be fatal. Even aspirin, which is sometimes used to treat arthritis in cats, is much more toxic to them than to humans and must be administered cautiously. Similarly, application of minoxidil (Rogaine) to the skin of cats, either accidentally or by well-meaning guardians attempting to counter loss of fur, has sometimes been fatal. Essential oils can be toxic to cats and cases have been reported of serious illnesses caused by tea tree oil, including flea treatments and shampoos containing it.
Other common household substances that should be used with caution around cats include mothballs and other naphthalene products. Phenol-based products (e.g. Pine-Sol, Dettol/Lysol or hexachlorophene) are often used for cleaning and disinfecting near cats' feeding areas or litter boxes, but these can sometimes be fatal. Ethylene glycol, often used as an automotive antifreeze, is particularly appealing to cats, and as little as a teaspoonful can be fatal. Some human foods are toxic to cats; for example chocolate can cause theobromine poisoning, although (unlike dogs) few cats will eat chocolate. Large amounts of onions or garlic are also poisonous to cats. Many houseplants are also dangerous, such as "Philodendron" species and the leaves of the Easter lily ("Lilium longiflorum"), which can cause permanent and life-threatening kidney damage.
Genetics.
The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes. About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors. The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.
Behavior.
Outdoor cats are active both day and night, although they tend to be slightly more active at night. The timing of cats' activity is quite flexible and varied, which means house cats may be more active in the morning and evening, as a response to greater human activity at these times. Although they spend the majority of their time in the vicinity of their home, housecats can range many hundreds of meters from this central point, and are known to establish territories that vary considerably in size, in one study ranging from .
Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.
Sociability.
Although wildcats are solitary, the social behavior of domestic cats is much more variable and ranges from widely dispersed individuals to feral cat colonies that form around a food source, based on groups of co-operating females. Within such groups, one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling, and if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, they do not have a social survival strategy, or a pack mentality, and always hunt alone.
However, some pet cats are poorly socialized. In particular, older cats may show aggressiveness towards newly arrived kittens, which may include biting and scratching; this type of behavior is known as feline asocial aggression.
Though cats and dogs are believed to be natural enemies, they can live together if correctly socialized.
Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, the human keeper of a cat may function as a sort of surrogate for the cat's mother, and adult housecats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. The high-pitched sounds housecats make to solicit food may mimic the cries of a hungry human infant, making them particularly hard for humans to ignore.
Communication.
Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. By contrast, feral cats are generally silent. Their types of body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats; for example, a raised tail acts as a friendly greeting, and flattened ears indicates hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate animals. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.
Purring may have developed as an evolutionary advantage as a signalling mechanism of reassurance between mother cats and nursing kittens. Post-nursing cats often purr as a sign of contentment: when being petted, becoming relaxed, or eating. The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound. It was, until recent times, believed that only the cats of the "Felis" genus could purr. However, felids of the "Panthera" genus (tiger, lion, jaguar, and leopard) also produce sounds similar to purring, but only when exhaling.
Grooming.
Cats are known for spending considerable amounts of time licking their coat to keep it clean. The cat's tongue has backwards-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them rigid so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush. Some cats can develop a compulsive behavior known as psychogenic alopecia, or excessive grooming.
Fighting.
Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.
When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.
Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus. Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.
Hunting and feeding.
Cats hunt small prey, primarily birds and rodents, and are often used as a form of pest control. Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.4–3.7 billion birds and 6.9–20.7 billion mammals annually. The bulk of predation in the United States is done by 80 million feral and stray cats. Effective measures to reduce this population are elusive, meeting opposition from cat enthusiasts. In the case of free-ranging pets, equipping cats with bells and not letting them out at night will reduce wildlife predation.
Free-fed feral cats and house cats tend to consume many small meals in a single day, although the frequency and size of meals varies between individuals. Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured. Although it is not certain, the strategy used may depend on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.
Perhaps the best known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to "play" with prey by releasing it after capture. This behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat. This behavior is referred to in the idiom "cat-and-mouse game" or simply "cat and mouse".
Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. Ethologist Paul Leyhausen proposed that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at, or near, the top. Anthropologist and zoologist Desmond Morris, in his 1986 book "Catwatching", suggests, when cats bring home mice or birds, they are attempting to teach their human to hunt, or trying to help their human as if feeding "an elderly cat, or an inept kitten". Morris's theory is inconsistent with the fact that male cats also bring home prey, despite males having no involvement with raising kittens.
Domestic cats select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats may reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They may also avoid sugary foods and milk. Most adult cats are lactose intolerant; the sugars in milk are not easily digested and may cause soft stools or diarrhea. They can also develop odd eating habits. Some cats like to eat or chew on other things, most commonly wool, but also plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.
Though cats usually prey on animals less than half their size, a feral cat in Australia has been photographed killing an adult pademelon weighing around the cat's size at .
Since cats cannot fully close their lips around something to create suction, they use a lapping method with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it, drawing water upwards.
Play.
Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.
Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest (they become habituated) in a toy they have played with before. Cats also tend to play with toys more when they are hungry. String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase. There are several important issues related to using a laser with a cat; first most, lasers can cause blindness in cats, even lasers which are sold as "eye safe" can actually be of much higher power of that proclaimed and can cause damages to the eyes. In addition, the cat thinks of the laser point as prey, but gets frustrated as he is unable to catch it.
Reproduction.
Female cats are seasonally polyestrous, which means they may have many periods of heat over the course of a year, the season beginning in spring and ending in late autumn. Heat periods occur about every two weeks and last about 4 to 7 days. Multiple males will be attracted to a female in heat. The males will fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backwards-pointing penile spines, which are about 1 mm long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which is a trigger for ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.
After mating, the female washes her vulva thoroughly. If a male attempts to mate with her at this point, the female will attack him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.
Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.
At 124 hours after conception, the morula forms. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.
The gestation period for cats is between 64 and 67 days, with an average of 66 days. The size of a litter usually is three to five kittens, with the first litter usually smaller than subsequent litters. Kittens are weaned between six and seven weeks old, and cats normally reach sexual maturity at 5–10 months (females) and to 5–7 months (males), although this can vary depending on breed. Females can have two to three litters per year, so may produce up to 150 kittens in their breeding span of around ten years.
Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. They can be surgically sterilized (spayed or castrated) as early as 7 weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed prior to puberty, at about three to six months. In the US, about 80% of household cats are neutered.
Ecology.
Habitats.
Cats are a cosmopolitan species and are found across much of the world. Geneticist Stephen James O'Brien, of the National Cancer Institute in Frederick, Maryland, remarked on how successful cats have been in evolutionary terms: "Cats are one of evolution's most charismatic creatures. They can live on the highest mountains and in the hottest deserts." They are extremely adaptable and are now present on all continents except Antarctica, and on 118 of the 131 main groups of islands—even on isolated islands such as the Kerguelen Islands.
Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands. Their habitats even include small oceanic islands with no human inhabitants. Further, the close relatives of domestic cats, the African wildcat ("Felis silvestris lybica") and the Arabian sand cat ("Felis margarita") both inhabit desert environments, and domestic cats still show similar adaptations and behaviors. The cat's ability to thrive in almost any terrestrial habitat has led to its designation as one of the world's worst invasive species.
As domestic cats are little altered from wildcats, they can readily interbreed. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula.
Feral cats.
Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the US feral population range from 25 to 60 million. Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.
Public attitudes towards feral cats vary widely, ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed 'trap-neuter-return', where the cats are trapped, neutered, immunized against rabies and the feline leukemia virus, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark it as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespans are increased, and behavior and nuisance problems caused by competition for food are reduced.
Impact on prey species.
To date, little scientific data is available to assess the impact of cat predation on prey populations. Even well-fed domestic cats may hunt and kill, mainly catching small mammals, but also birds, amphibians, reptiles, fish, and invertebrates. Hunting by domestic cats may be contributing to the decline in the numbers of birds in urban areas, although the importance of this effect remains controversial. In the wild, the introduction of feral cats during human settlement can threaten native species with extinction. In many cases, controlling or eliminating the populations of non-native cats can produce a rapid recovery in native animals. However, the ecological role of introduced cats can be more complicated. For example, cats can control the numbers of rats, which also prey on birds' eggs and young, so a cat population can protect an endangered bird species by suppressing mesopredators.
In isolated landmasses, such as Australasia, there are often no other native, medium-sized quadrupedal predators (including other feline species); this tends to exacerbate the impact of feral cats on small native animals. Native species such as the New Zealand kakapo and the Australian bettong, for example, tend to be more ecologically vulnerable and behaviorally "naive", when faced with predation by cats. Feral cats have had a major impact on these native species and have played a leading role in the endangerment and extinction of many animals.
Even in places with ancient and numerous cat populations, such as Western Europe, cats appear to be growing in number and independently of their environments' carrying capacity (such as the numbers of prey available). This may be explained, at least in part, by an abundance of food, from sources including feeding by pet owners and scavenging. For instance, research in Britain suggests that a high proportion of cats hunt only "recreationally". And in South Sweden, where research in 1982 found that the population density of cats was as high as .
Impact on birds.
The domestic cat is a significant predator of birds. UK assessments indicate they may be accountable for an estimated 64.8 million bird deaths each year. Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins ("Erithacus rubecula") and dunnocks ("Prunella modularis"), 31% of deaths were a result of cat predation. The presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety. The proposal that cat populations will increase when the numbers of these top predators decline is called the mesopredator release hypothesis. However, a new study suggests cats are a much greater menace than previously thought and feral cats kill several billion birds each year in the United States.
On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a 'mesopredator release' effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham Islands rail, the Auckland Islands merganser, and the common diving petrel are a few from a long list, with the most extreme case being the flightless Stephens Island wren, which was driven to extinction only a few years after its discovery.
Some of the same factors that have promoted adaptive radiation of island avifauna over evolutionary time appear to promote vulnerability to non-native species in modern time. The susceptibility of many island birds is undoubtedly due to evolution in the absence of mainland predators, competitors, diseases, and parasites, in addition to lower reproductive rates and extended incubation periods. The loss of flight, or reduced flying ability is also characteristic of many island endemics. These biological aspects have increased vulnerability to extinction in the presence of introduced species, such as the domestic cat. Equally, behavioral traits exhibited by island species, such as "predatory naivety" and ground-nesting, have also contributed to their susceptibility.
Interaction with humans.
Cats are common pets in Europe and North America, and their worldwide population exceeds 500 million. Although cat guardianship has commonly been associated with women, a 2007 Gallup poll reported that men and women in the United States of America were equally likely to own a cat.
As well as being kept as pets, cats are also used in the international fur and leather industries for making coats, hats, blankets and stuffed toys; and shoes, gloves and musical instruments respectively (about 24 cats are needed to make a cat fur coat). This use has now been outlawed in the United States, Australia, and the European Union. Cat pelts have been used for superstitious purposes as part of the practise of witchcraft, and are still made into blankets in Switzerland as folk remedies believed to help rheumatism. In the Western intellectual tradition, the idea of cats as everyday objects have served to illustrate problems of quantum mechanics in the Schrödinger's cat thought experiment.
A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as the Canadian Federation of Humane Societies's one) and over the net, but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.
History and mythology.
Traditionally, historians tended to think ancient Egypt was the site of cat domestication, owing to the clear depictions of house cats in Egyptian paintings about 3,600 years old. However, in 2004, a Neolithic grave excavated in Shillourokambos, Cyprus, contained the skeletons, laid close to one another, of both a human and a cat. The grave is estimated to be 9,500 years old, pushing back the earliest known feline–human association significantly. The cat specimen is large and closely resembles the African wildcat, rather than present-day domestic cats. This discovery, combined with genetic studies, suggests cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture and then they were brought to Cyprus and Egypt.
Direct evidence for the domestication of cats 5,300 years ago in Quanhucun, China has been published by archaeologists and paleontologists from the University of Washington and Chinese Academy of Sciences. The cats are believed to have been attracted to the village by rodents, which in turn were attracted by grain cultivated and stored by humans.
In ancient Egypt, cats were sacred animals, with the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness. The Romans are often credited with introducing the domestic cat from Egypt to Europe; in Roman Aquitaine, a first- or second-century engraving of a young girl holding a cat is one of two earliest depictions of the Roman domesticated cat. However, cats possibly were already kept in Europe prior to the Roman Empire, as they may have been present in Britain in the late Iron Age. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as they were carried on sailing ships to control shipboard rodents and as good-luck charms (see Ship's cat).
Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the "maneki neko" cat is a symbol of good fortune.
Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza. He is reported to have loved cats so much, "he would do without his cloak rather than disturb one that was sleeping on it". The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad.
Freyja, the goddess of love, beauty, and fertility in Norse mythology, is depicted as riding a chariot drawn by cats.
Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade).
According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives, while in Turkish and Arabic traditions, the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.

</doc>
<doc id="6681" url="https://en.wikipedia.org/wiki?curid=6681" title="Crank">
Crank

Crank may refer to:

</doc>
<doc id="6682" url="https://en.wikipedia.org/wiki?curid=6682" title="Clade">
Clade

A clade (from , "klados", "branch") is a group of organisms that consists of a common ancestor and all its lineal descendants, and represents a single "branch" on the "tree of life".
The common ancestor may be an individual, a population, a species (extinct or extant), and so on right up to a kingdom. Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. Clades are termed monophyletic (Greek: "one clan").
Over the last few decades, the cladistic approach has revolutionized biological classification and revealed surprising evolutionary relationships among organisms. Increasingly, taxonomists try to avoid naming taxa that are not clades; that is, taxa that are not monophyletic.
Etymology.
The term "clade" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch.
Many commonly named groups are clades, for example, rodents, or insects; because in each case, their name comprises a common ancestor with all its descendant branches. Rodents, for example, are a branch of mammals that split off after the end of the period when the clade Dinosauria stopped being the dominant terrestrial vertebrates 66 million years ago. The original population and all its descendants are a clade. The rodent clade corresponds to the order Rodentia, and insects to the class Insecta. These clades include smaller clades, such as chipmunk or ant, each of which comprises even smaller clades. The clade "rodent" is in turn included in the mammal, vertebrate and animal clades.
History of nomenclature and taxonomy.
The idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms – although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is however responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.
With the publication of Darwin's theory of evolution in 1859, the idea was born that groups used in a system of classification should represent branches on the evolutionary tree of life. Thomas Henry Huxley, an early advocate of evolutionary theory, proposed a revised taxonomy based on clades. 
For example, he grouped birds with reptiles, based on fossil evidence.
German biologist Emil Hans Willi Hennig (1913 – 1976) is considered to be the founder of cladistics.
He proposed a classification system that represented repeated branchings of the family tree, as opposed to the previous systems, which put organisms on a "ladder," with supposedly more "advanced" organisms at the top.
Taxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature. In the latter, only taxa associated with a rank can be named, yet there are not enough ranks to name a long series of nested clades; also, taxon names cannot be defined in a way that guarantees them to refer to clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.
Definitions.
A clade is by definition monophyletic, meaning it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.
Clades and phylogenetic trees.
The science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term being derived from "clade" by Ernst Mayr (1965). The results of phylogenetic/cladistic analyses are tree-shaped diagrams called "cladograms"; they, and all their branches, are phylogenetic hypotheses.
Three methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see here for detailed definitions).
Terminology.
The relationship between clades can be described in several ways:
Cultural references.
"Clade" is the title of a novel by James Bradley, who chose it both because of its biological meaning but also because of the larger implications of the word.
An episode of "Elementary" was titled “Dead Clade Walking,” and dealt with a case involving a rare fossil.

</doc>
<doc id="6684" url="https://en.wikipedia.org/wiki?curid=6684" title="Communications in Afghanistan">
Communications in Afghanistan

Communications in Afghanistan is under the control of the Ministry of Communications and Information Technology (MCIT). It has rapidly expanded after the Karzai administration took over in late 2001, and has embarked on wireless companies, internet, radio stations and television channels. The Afghan government signed a $64.5 agreement in 2006 with China's ZTE on the establishment of a countrywide optical fiber cable network. The project began to improve telephone, internet, television and radio broadcast services throughout Afghanistan. As of 2014, about 90% of the country's population has access to communication services.
There are about 18 million mobile phone users in the country. Etisalat, Roshan, Afghan Wireless and MTN are the leading telecom companies. Etisalat became the first company planning to launch 4G services in 2014. It is predicted that over 50% of the population will have access to the internet by 2015. In 2014, Afghanistan leased a space satellite from Eutelsat, called AFGHANSAT 1.
Telephone.
There are about 18 million GSM mobile phone subscribers in Afghanistan as of 2009, with over 75,000 fixed-telephone-lines and little over 190,000 CDMA subscribers. Mobile communications have improved because of the introduction of wireless carriers into this developing country. The first was Afghan Wireless, which is US based that was founded by Ehsan Bayat. The second was Roshan, which began providing services to all major cities within Afghanistan. There are also a number of VSAT stations in major cities such as Kabul, Kandahar, Herat, Mazari Sharif, and Jalalabad, providing international and domestic voice/data connectivity. The international calling code for Afghanistan is +93. The following is a partial list of mobile phone companies in the country:
All the companies providing communication services are obligated to deliver 2.5% of their income to the communication development fund annually. According to the Ministry of Communication and Information Technology there are 4760 active towers throughout the country which covers 85% of the population. The Ministry of Communication and Information Technology plans to expand its services in remote parts of the country where the remaining 15% of the population will be covered with the installation of 700 new towers.
Phone calls in Afghanistan have been monitored by the National Security Agency according to Wikileaks.
Internet.
Afghanistan was given legal control of the ".af" domain in 2003, and the Afghanistan Network Information Center (AFGNIC) was established to administer domain names. As of 2010, there are at least 46 internet service providers (ISPs) in the country. Internet in Afghanistan is also at the peak with 1 million users as of 2009.
According to the Ministry of Communications, the following are some of the different ISPs operating in Afghanistan:
Television.
There are over 50 Afghan television channels worldwide, many of which are based inside Afghanistan while others are broadcast from North America and Europe. Selected foreign channels are also shown to the public in Afghanistan, but with the use of the internet, over 3,500 international TV channels may be accessed in Afghanistan.
Radio.
As of 2007, there are an estimated 50 private radio stations throughout the country. Broadcasts are in Dari, Pashto, English, Uzbeki and many other languages.
The number of radio listeners are decreasing and are being slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost have a lot of radio listeners. Kabul and Jalalabad have moderate number of listeners. However, Mazar-e-Sharif and especially Herat have very few radio listeners.
Postal service.
In 1870, a central post office was established at Bala Hissar in Kabul and a post office in the capital of each province. The service was slowly being expanded over the years as more postal offices were established in each large city by 1918. Afghanistan became a member of the Universal Postal Union in 1928, and the postal administration elevated to the Ministry of Communication in 1934. Civil war caused a disruption in issuing official stamps during the 1980s-90s war but in 1999 postal service was operating again. Postal services to/from Kabul worked remarkably well all throughout the war years. Postal services to/from Herat resumed in 1997. The Afghan government has reported to the UPU several times about illegal stamps being issued and sold in 2003 and 2007.
Afghanistan Post has been reorganizing the postal service in 2000s with the help of Pakistan Post. The Afghanistan Postal commission was formed to prepare a written policy for the development of the postal sector, which will form the basis of a new postal services law governing licensing of postal services providers. The project was expected to finish by 2008.
Satellite.
It was announced in 2013 that Afghanistan would soon have its own satellite in space. In January 2014 the Afghan Ministry of Communications and Information Technology signed an agreement with Eutelsat for the use of satellite resources to enhance deployment of Afghanistan's national broadcasting and telecommunications infrastructure as well as its international connectivity. AFGHANSAT 1 was officially launched in May 2014, with expected service for at least seven years in Afghanistan. The Afghan government plans to launch AFGHANSAT 2 after the AFGHANSAT 1 lease ends.

</doc>
<doc id="6689" url="https://en.wikipedia.org/wiki?curid=6689" title="Christian of Oliva">
Christian of Oliva

Christian of Oliva (), also Christian of Prussia () (died 4 December(?) 1245) was the first missionary bishop of Prussia. 
Christian was born about 1180 in the Duchy of Pomerania, possibly in the area of Chociwel (according to Johannes Voigt). Probably as a juvenile he joined the Cistercian Order at newly established Kołbacz ("Kolbatz") Abbey and in 1209 entered Oliwa Abbey near Gdańsk, founded in 1178 by the Samboride dukes of Pomerelia. At this time the Piast duke Konrad I of Masovia with the consent of Pope Innocent III had started the first of several unsuccessful Prussian Crusades into the adjacent Chełmno Land and Christian acted as a missionary among the Prussians east of the Vistula River. In 1215 he is recorded as abbot of Łekno Abbey near Gniezno in Greater Poland, most but not all authors identify him with Godfrey of Łękno. 
In 1209, Christian was commissioned by the Pope to be responsible for the Prussian missions between the Vistula and Neman Rivers and in 1212 he was appointed bishop. In 1215 he went to Rome in order to report to the Curia on the condition and prospects of his mission, and was consecrated first "Bishop of Prussia" at the Fourth Council of the Lateran. His seat as a bishop remained at Oliwa Abbey on the western side of the Vistula, whereas the pagan Prussian (later East Prussian) territory was on the eastern side of it.
The attempts by Konrad of Masovia to subdue the Prussian lands had picked long-term and intense border quarrels, whereby the Polish lands of Masovia, Cuyavia and even Greater Poland became subject to continuous Prussian raids. Bishop Christian asked the new Pope Honorius III for the consent to start another Crusade, however a first campaign in 1217 proved a failure and even the joint efforts by Duke Konrad with the Polish High Duke Leszek I the White and Duke Henry I the Bearded of Silesia in 122/23 only led to the reconquest of Chełmno Land but did not stop the Prussian invasions. At least Christian was able to establish the Diocese of Chełmno east of the Vistula, adopting the episcopal rights from the Masovian Bishop of Płock, confirmed by both Duke Konrad and the Pope.
Duke Konrad of Masovia still was not capable to end the Prussian attacks on his territory and in 1224 began to conduct negotiations with the Teutonic Knights under Grand Master Hermann von Salza in order to strengthen his forces. As von Salza initially hesitated to offer his services, Christian created the military Order of Dobrzyń ("Fratres Milites Christi") in 1228, however to little avail.
Meanwhile, von Salza had to abandon his hope to establish an Order's State in the Burzenland region of Transylvania, which had led to an éclat with King Andrew II of Hungary. He obtained a charter by Emperor Frederick II issued in the 1226 Golden Bull of Rimini, whereby Chełmno Land would be the unshared possession of the Teutonic Knights, which was confirmed by Duke Konrad of Masovia in the 1230 Treaty of Kruszwica. Christian ceded his possessions to the new State of the Teutonic Order and in turn was appointed Bishop of Chełmno the next year.
Bishop Christian continued his mission in Sambia ("Samland"), where from 1233 to 1239 he was held captive by pagan Prussians, and freed in trade for five other hostages who then in turn were released for a ransom of 800 Marks, granted to him by Pope Gregory IX. He had to deal with the constant cut-back of his autonomy by the Knights and asked the Roman Curia for mediation. In 1243, the Papal legate William of Modena divided the Prussian lands of the Order's State into four dioceses, whereby the bishops retained the secular rule over about on third of the diocesan territory:
all suffragan dioceses under the Archbishopric of Riga. Christian was supposed to choose one of them, but did not agree to the division. He possibly retired to the Cistercians Abbey in Sulejów, where he died before the conflict was solved.

</doc>
<doc id="6690" url="https://en.wikipedia.org/wiki?curid=6690" title="Coca-Cola">
Coca-Cola

Coca-Cola is a carbonated soft drink. It is produced by The Coca-Cola Company of Atlanta, Georgia, and is often referred to simply as Coke (a registered trademark of The Coca-Cola Company in the United States since March 27, 1944). Originally intended as a patent medicine when it was invented in the late 19th century by John Pemberton, Coca-Cola was bought out by businessman Asa Griggs Candler, whose marketing tactics led Coke to its dominance of the world soft-drink market throughout the 20th century. The name refers to two of its original ingredients: kola nuts, a source of caffeine, and coca leaves. The current formula of Coca-Cola remains a trade secret, although a variety of reported recipes and experimental recreations have been published.
The company produces concentrate, which is then sold to licensed Coca-Cola bottlers throughout the world. The bottlers, who hold exclusive territory contracts with the company, produce finished product in cans and bottles from the concentrate in combination with filtered water and sweeteners. A typical 12 oz (355 ml) can contains 38g of sugar (usually in the form of HFCS). The bottlers then sell, distribute and merchandise Coca-Cola to retail stores, restaurants and vending machines. The Coca-Cola Company also sells concentrate for soda fountains to major restaurants and food service distributors.
The Coca-Cola Company has, on occasion, introduced other cola drinks under the Coke brand name. The most common of these is Diet Coke, with others including Caffeine-Free Coca-Cola, Diet Coke Caffeine-Free, Coca-Cola Cherry, Coca-Cola Zero, Coca-Cola Vanilla, and special versions with lemon, lime, or coffee. In 2013, Coke products could be found in over 200 countries worldwide, with consumers downing more than 1.8 billion company beverage servings each day.
Based on Interbrand's best global brand study of 2015, Coca-Cola was the world's third most valuable brand.
History.
19th-century historical origins.
Confederate Colonel John Pemberton who was wounded in the American Civil War, became addicted to morphine, and began a quest to find a substitute for the dangerous opiate. The prototype Coca-Cola recipe was formulated at Pemberton's Eagle Drug and Chemical House, a drugstore in Columbus, Georgia, originally as a coca wine. He may have been inspired by the formidable success of Vin Mariani, a European coca wine.
In 1885, Pemberton registered his French Wine Coca nerve tonic. In 1886, when Atlanta and Fulton County passed prohibition legislation, Pemberton responded by developing Coca-Cola, essentially a nonalcoholic version of French Wine Coca.
The first sales were at Jacob's Pharmacy in Atlanta, Georgia, on May 8, 1886. It was initially sold as a patent medicine for five cents a glass at soda fountains, which were popular in the United States at the time due to the belief that carbonated water was good for the health. Pemberton claimed Coca-Cola cured many diseases, including morphine addiction, dyspepsia, neurasthenia, headache, and impotence. Pemberton ran the first advertisement for the beverage on May 29 of the same year in the "Atlanta Journal".
By 1888, three versions of Coca-Cola – sold by three separate businesses – were on the market. A co-partnership had been formed on January 14, 1888 between Pemberton and four Atlanta businessmen: J.C. Mayfield, A.O. Murphey; C.O. Mullahy and E.H. Bloodworth. Not codified by any signed document, a verbal statement given by Asa Candler years later asserted under testimony that he had acquired a stake in Pemberton's company as early as 1887. John Pemberton declared that the "name" "Coca-Cola" belonged to his son, Charley, but the other two manufacturers could continue to use the "formula".
Charley Pemberton's record of control over the "Coca-Cola" name was the underlying factor that allowed for him to participate as a major shareholder in the March 1888 Coca-Cola Company incorporation filing made in his father's place. Charley's exclusive control over the "Coca Cola" name became a continual thorn in Asa Candler's side.
Candler's oldest son, Charles Howard Candler, authored a book in 1950 published by Emory University. In this definitive biography about his father, Candler specifically states: "..., on April 14, 1888, the young druggist Griggs Candler purchased a one-third interest in the formula of an almost completely unknown proprietary elixir known as Coca-Cola."
The deal was actually between John Pemberton's son Charley and Walker, Candler & Co. – with John Pemberton acting as cosigner for his son. For $50 down and $500 in 30 days, Walker, Candler & Co. obtained all of the one-third interest in the Coca-Cola Company that Charley held, all while Charley still held on to the name. After the April 14 deal, on April 17, 1888, one-half of the Walker/Dozier interest shares were acquired by Candler for an additional $750.
The Coca-Cola Company.
In 1892, Candler set out to incorporate a second company; "The Coca-Cola Company" (the current corporation). When Candler had the earliest records of the "Coca-Cola Company" burned in 1910, the action was claimed to have been made during a move to new corporation offices around this time.
After Candler had gained a better foothold on Coca-Cola in April 1888, he nevertheless was forced to sell the beverage he produced with the recipe he had under the names "Yum Yum" and "Koke". This was while Charley Pemberton was selling the elixir, although a cruder mixture, under the name "Coca-Cola", all with his father's blessing. After both names failed to catch on for Candler, by the summer of 1888, the Atlanta pharmacist was quite anxious to establish a firmer legal claim to Coca-Cola, and hoped he could force his two competitors, Walker and Dozier, completely out of the business, as well.
On August 16, 1888, Dr. John Stith Pemberton suddenly died, Asa G. Candler then sought to move swiftly forward to attain his vision of taking full control of the whole Coca-Cola operation.
Charley Pemberton, an alcoholic, was the one obstacle who unnerved Asa Candler more than anyone else. Candler is said to have quickly maneuvered to purchase the exclusive rights to the name "Coca-Cola" from Pemberton's son Charley right after Dr. Pemberton's death. One of several stories was that Candler bought the title to the name from Charley's mother for $300; approaching her at Dr. Pemberton's funeral. Eventually, Charley Pemberton was found on June 23, 1894, unconscious, with a stick of opium by his side. Ten days later, Charley died at Atlanta's Grady Hospital at the age of 40.
In Charles Howard Candler's 1950 book about his father, he stated: "On August 30th he [Asa Candler became sole proprietor of Coca-Cola, a fact which was stated on letterheads, invoice blanks and advertising copy."
With this action on August 30, 1888, Candler's sole control became technically all true. Candler had negotiated with Margaret Dozier and her brother Woolfolk Walker a full payment amounting to $1,000, which all agreed Candler could pay off with a series of notes over a specified time span. By May 1, 1889, Candler was now claiming full ownership of the Coca-Cola beverage, with a total investment outlay by Candler for the drink enterprise over the years amounting to $2,300.
In 1914, Margaret Dozier, as co-owner of the original Coca-Cola Company in 1888, came forward to claim that her signature on the 1888 Coca-Cola Company bill of sale had been forged. Subsequent analysis of certain similar transfer documents had also indicated John Pemberton's signature was most likely a forgery, as well, which some accounts claim was precipitated by his son Charley.
On September 12, 1919, Coca-Cola Co. was purchased by a group of investors for $25 million and reincorporated. The company publicly offered 500,000 shares of the company for $40 a share.
In 1986, The Coca-Cola Company merged with two of their bottling operators (owned by JTL Corporation and BCI Holding Corporation) to form Coca-Cola Enterprises Inc. (CCE).
In December 1991, Coca-Cola Enterprises merged with the Johnston Coca-Cola Bottling Group, Inc.
Origins of bottling.
The first bottling of Coca-Cola occurred in Vicksburg, Mississippi, at the Biedenharn Candy Company in 1891. The proprietor of the bottling works was Joseph A. Biedenharn. The original bottles were Biedenharn bottles, very different from the much later hobble-skirt design of 1915 now so familiar.
It was then a few years later that two entrepreneurs from Chattanooga, Tennessee, namely; Benjamin F. Thomas and Joseph B. Whitehead, proposed the idea of bottling and were so persuasive that Candler signed a contract giving them control of the procedure for only one dollar. Candler never collected his dollar, but in 1899, Chattanooga became the site of the first Coca-Cola bottling company. Candler remained very content just selling his company's syrup. The loosely termed contract proved to be problematic for The Coca-Cola Company for decades to come. Legal matters were not helped by the decision of the bottlers to subcontract to other companies, effectively becoming parent bottlers. This contract specified that bottles would be sold at 5¢ each and had no fixed duration, leading to the fixed price of Coca-Cola from 1886 to 1959.
20th century.
The first outdoor wall advertisement that promoted the Coca-Cola drink was painted in 1894 in Cartersville, Georgia. Cola syrup was sold as an over-the-counter dietary supplement for upset stomach. By the time of its 50th anniversary, the soft drink had reached the status of a national icon in the USA. In 1935, it was certified kosher by Atlanta Rabbi Tobias Geffen, after the company made minor changes in the sourcing of some ingredients.
The longest running commercial Coca-Cola soda fountain anywhere was Atlanta's Fleeman's Pharmacy, which first opened its doors in 1914. Jack Fleeman took over the pharmacy from his father and ran it until 1995; closing it after 81 years. On July 12, 1944, the one-billionth gallon of Coca-Cola syrup was manufactured by The Coca-Cola Company. Cans of Coke first appeared in 1955.
New Coke.
On April 23, 1985, Coca-Cola, amid much publicity, attempted to change the formula of the drink with "New Coke". Follow-up taste tests revealed most consumers preferred the taste of New Coke to both Coke and Pepsi but Coca-Cola management was unprepared for the public's nostalgia for the old drink, leading to a backlash. The company gave in to protests and returned to a variation of the old formula using high fructose corn syrup instead of cane sugar as the main sweetener, under the name Coca-Cola Classic, on July 10, 1985.
21st century.
On July 5, 2005, it was revealed that Coca-Cola would resume operations in Iraq for the first time since the Arab League boycotted the company in 1968.
In April 2007, in Canada, the name "Coca-Cola Classic" was changed back to "Coca-Cola". The word "Classic" was removed because "New Coke" was no longer in production, eliminating the need to differentiate between the two. The formula remained unchanged. In January 2009, Coca-Cola stopped printing the word "Classic" on the labels of bottles sold in parts of the southeastern United States. The change is part of a larger strategy to rejuvenate the product's image. The word "Classic" was removed from all Coca-Cola products by 2011.
In November 2009, due to a dispute over wholesale prices of Coca-Cola products, Costco stopped restocking its shelves with Coke and Diet Coke for two months; a separate pouring rights deal in 2013 saw Coke products removed from Costco food courts in favor of Pepsi. Some Costco locations (such as the ones in Tucson, Arizona) additionally sell imported Coca-Cola from Mexico with cane sugar instead of corn syrup from separate distributors. Coca-Cola introduced the 7.5-ounce mini-can in 2009, and on September 22, 2011, the company announced price reductions, asking retailers to sell eight-packs for $2.99. That same day, Coca-Cola announced the 12.5-ounce bottle, to sell for 89 cents. A 16-ounce bottle has sold well at 99 cents since being re-introduced, but the price was going up to $1.19.
In 2012, Coca-Cola resumed business in Myanmar after 60 years of absence due to U.S.-imposed investment sanctions against the country. Coca-Cola's bottling plant will be located in Yangon and is part of the company's five-year plan and $200 million investment in Myanmar. Coca-Cola with its partners is to invest USD 5 billion in its operations in India by 2020. In 2013, it was announced that Coca-Cola Life would be introduced in Argentina that would contain stevia and sugar.
In August 2014 the company announced it was forming a long-term partnership with Monster Beverage, with the two forging a strategic marketing and distribution alliance, and product line swap. As part of the deal Coca-Cola was to acquire a 16.7% stake in Monster for $2.15 billion, with an option to increase it to 25%.
Production.
Ingredients.
A typical can of Coca-Cola (12 fl ounces/355 ml) contains 38 grams of sugar (usually in the form of HFCS), 50 mg of sodium, 0 grams fat, 0 grams potassium, and 140 calories.
On May 5, 2014, Coca-Cola said it is working to remove a controversial ingredient, brominated vegetable oil, from all of its drinks.
Formula of natural flavorings.
The exact formula of Coca-Cola's natural flavorings (but not its other ingredients, which are listed on the side of the bottle or can) is a trade secret. The original copy of the formula was held in SunTrust Bank's main vault in Atlanta for 86 years. Its predecessor, the Trust Company, was the underwriter for the Coca-Cola Company's initial public offering in 1919. On December 8, 2011, the original secret formula was moved from the vault at SunTrust Banks to a new vault containing the formula which will be on display for visitors to its World of Coca-Cola museum in downtown Atlanta.
According to Snopes, a popular myth states that only two executives have access to the formula, with each executive having only half the formula. However, several sources state that while Coca-Cola does have a rule restricting access to only two executives, each knows the entire formula and others, in addition to the prescribed duo, have known the formulation process.
On February 11, 2011, Ira Glass revealed on his PRI radio show, "This American Life", that the secret formula to Coca-Cola had been uncovered in a 1979 newspaper. The formula found basically matched the formula found in Pemberton's diary.
Use of stimulants in formula.
When launched, Coca-Cola's two key ingredients were cocaine and caffeine. The cocaine was derived from the coca leaf and the caffeine from kola nut, leading to the name Coca-Cola (the "K" in Kola was replaced with a "C" for marketing purposes).
Coca – cocaine.
Pemberton called for five ounces of coca leaf per gallon of syrup, a significant dose; in 1891, Candler claimed his formula (altered extensively from Pemberton's original) contained only a tenth of this amount. Coca-Cola once contained an estimated nine milligrams of cocaine per glass. In 1903, it was removed.
After 1904, instead of using fresh leaves, Coca-Cola started using "spent" leaves – the leftovers of the cocaine-extraction process with trace levels of cocaine. Since then, Coca-Cola uses a cocaine-free coca leaf extract prepared at a Stepan Company plant in Maywood, New Jersey.
In the United States, the Stepan Company is the only manufacturing plant authorized by the Federal Government to import and process the coca plant, which it obtains mainly from Peru and, to a lesser extent, Bolivia. Besides producing the coca flavoring agent for Coca-Cola, the Stepan Company extracts cocaine from the coca leaves, which it sells to Mallinckrodt, a St. Louis, Missouri, pharmaceutical manufacturer that is the only company in the United States licensed to purify cocaine for medicinal use.
Long after the syrup had ceased to contain any significant amount of cocaine, in the southeastern U.S., "dope" remained a common colloquialism for Coca-Cola, and "dope-wagons" were trucks that transported it.
Kola nuts – caffeine.
Kola nuts act as a flavoring and the source of caffeine in Coca-Cola. In Britain, for example, the ingredient label states "Flavourings (Including Caffeine)." Kola nuts contain about 2.0 to 3.5% caffeine, are of bitter flavor and are commonly used in cola soft drinks. In 1911, the U.S. government initiated "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", hoping to force Coca-Cola to remove caffeine from its formula. The case was decided in favor of Coca-Cola. Subsequently, in 1912, the U.S. Pure Food and Drug Act was amended, adding caffeine to the list of "habit-forming" and "deleterious" substances which must be listed on a product's label.
Coca-Cola contains 34 mg of caffeine per 12 fluid ounces (9.8 mg per 100 ml).
Franchised production model.
The actual production and distribution of Coca-Cola follows a franchising model. The Coca-Cola Company only produces a syrup concentrate, which it sells to bottlers throughout the world, who hold Coca-Cola franchises for one or more geographical areas. The bottlers produce the final drink by mixing the syrup with filtered water and sweeteners, and then carbonate it before putting it in cans and bottles, which the bottlers then sell and distribute to retail stores, vending machines, restaurants and food service distributors.
The Coca-Cola Company owns minority shares in some of its largest franchises, such as Coca-Cola Enterprises, Coca-Cola Amatil, Coca-Cola Hellenic Bottling Company and Coca-Cola FEMSA, but fully independent bottlers produce almost half of the volume sold in the world.
Independent bottlers are allowed to sweeten the drink according to local tastes.
The bottling plant in Skopje, Macedonia, received the 2009 award for "Best Bottling Company".
Geographic spread.
Since it announced its intention to begin distribution in Burma in June 2012, Coca-Cola has been officially available in every country in the world except Cuba and North Korea. However, it is reported to be available in both countries as a grey import.
Coca-Cola has been a point of legal discussion in the Middle East. In the early 20th century, a fatwa was created in Egypt to discuss the question of "whether Muslims were permitted to drink Coca-Cola and Pepsi cola." The fatwa states: "According to the Muslim Hanefite, Shafi'ite, etc., the rule in Islamic law of forbidding or allowing foods and beverages is based on the presumption that such things are permitted unless it can be shown that they are forbidden on the basis of the Qur'an." The Muslim jurists stated that, unless the Qu'ran specifically prohibits the consumption of a particular product, it is permissible to consume. Another clause was discussed, whereby the same rules apply if a person is unaware of the condition or ingredients of the item in question.
Brand portfolio.
This is a list of variants of Coca-Cola introduced around the world. In addition to the caffeine-free version of the original, additional fruit flavors have been included over the years. Not included here are versions of Diet Coke and Coca-Cola Zero; variant versions of those no-calorie colas can be found at their respective articles.
Logo design.
The Coca-Cola logo was created by John Pemberton's bookkeeper, Frank Mason Robinson, in 1885. Robinson came up with the name and chose the logo's distinctive cursive script. The writing style used, known as Spencerian script, was developed in the mid-19th century and was the dominant form of formal handwriting in the United States during that period.
Robinson also played a significant role in early Coca-Cola advertising. His promotional suggestions to Pemberton included giving away thousands of free drink coupons and plastering the city of Atlanta with publicity banners and streetcar signs.
Contour bottle design.
The Coca-Cola bottle, called the "contour bottle" within the company, was created by bottle designer Earl R. Dean. In 1915, The Coca-Cola Company launched a competition among its bottle suppliers to create a new bottle for their beverage that would distinguish it from other beverage bottles, "a bottle which a person could recognize even if they felt it in the dark, and so shaped that, even if broken, a person could tell at a glance what it was."
Chapman J. Root, president of the Root Glass Company of Terre Haute, Indiana, turned the project over to members of his supervisory staff, including company auditor T. Clyde Edwards, plant superintendent Alexander Samuelsson, and Earl R. Dean, bottle designer and supervisor of the bottle molding room. Root and his subordinates decided to base the bottle's design on one of the soda's two ingredients, the coca leaf or the kola nut, but were unaware of what either ingredient looked like. Dean and Edwards went to the Emeline Fairbanks Memorial Library and were unable to find any information about coca or kola. Instead, Dean was inspired by a picture of the gourd-shaped cocoa pod in the Encyclopædia Britannica. Dean made a rough sketch of the pod and returned to the plant to show Root. He explained to Root how he could transform the shape of the pod into a bottle. Root gave Dean his approval.
Faced with the upcoming scheduled maintenance of the mold-making machinery, over the next 24 hours Dean sketched out a concept drawing which was approved by Root the next morning. Dean then proceeded to create a bottle mold and produced a small number of bottles before the glass-molding machinery was turned off.
Chapman Root approved the prototype bottle and a design patent was issued on the bottle in November 1915. The prototype never made it to production since its middle diameter was larger than its base, making it unstable on conveyor belts. Dean resolved this issue by decreasing the bottle's middle diameter. During the 1916 bottler's convention, Dean's contour bottle was chosen over other entries and was on the market the same year. By 1920, the contour bottle became the standard for The Coca-Cola Company. A revised version was also patented in 1923. Because the Patent Office releases the "Patent Gazette" on Tuesday, the bottle was patented on December 25, 1923, and was nicknamed the "Christmas bottle." Today, the contour Coca-Cola bottle is one of the most recognized packages on the planet..."even in the dark!".
As a reward for his efforts, Dean was offered a choice between a $500 bonus or a lifetime job at the Root Glass Company. He chose the lifetime job and kept it until the Owens-Illinois Glass Company bought out the Root Glass Company in the mid-1930s. Dean went on to work in other Midwestern glass factories.
One alternative depiction has Raymond Loewy as the inventor of the unique design, but, while Loewy did serve as a designer of Coke cans and bottles in later years, he was in the French Army the year the bottle was invented and did not emigrate to the United States until 1919. Others have attributed inspiration for the design not to the cocoa pod, but to a Victorian hooped dress.
In 1944, Associate Justice Roger J. Traynor of the Supreme Court of California took advantage of a case involving a waitress injured by an exploding Coca-Cola bottle to articulate the doctrine of strict liability for defective products. Traynor's concurring opinion in "Escola v. Coca-Cola Bottling Co." is widely recognized as a landmark case in U.S. law today.
In 2007, the company's logo on cans and bottles changed. The cans and bottles retained the red color and familiar typeface, but the design was simplified, leaving only the logo and a plain white swirl (the "dynamic ribbon").
Designer bottles.
Karl Lagerfeld is the latest designer to have created a collection of aluminum bottles for Coca-Cola. Lagerfeld is not the first fashion designer to create a special version of the famous Coca-Cola Contour bottle. A number of other limited edition bottles by fashion designers for Coca Cola Light soda have been created in the last few years.
In 2009, in Italy, Coca-Cola Light had a Tribute to Fashion to celebrate 100 years of the recognizable contour bottle. Well known Italian designers Alberta Ferretti, Blumarine, Etro, Fendi, Marni, Missoni, Moschino, and Versace each designed limited edition bottles.
Competitors.
Pepsi, the flagship product of PepsiCo, The Coca-Cola Company's main rival in the soft drink industry, is usually second to Coke in sales, and outsells Coca-Cola in some markets. RC Cola, now owned by the Dr Pepper Snapple Group, the third largest soft drink manufacturer, is also widely available.
Around the world, many local brands compete with Coke. In South and Central America Kola Real, known as Big Cola in Mexico, is a growing competitor to Coca-Cola. On the French island of Corsica, Corsica Cola, made by brewers of the local Pietra beer, is a growing competitor to Coca-Cola. In the French region of Brittany, Breizh Cola is available. In Peru, Inca Kola outsells Coca-Cola, which led The Coca-Cola Company to purchase the brand in 1999. In Sweden, Julmust outsells Coca-Cola during the Christmas season. In Scotland, the locally produced Irn-Bru was more popular than Coca-Cola until 2005, when Coca-Cola and Diet Coke began to outpace its sales. In the former East Germany, Vita Cola, invented during Communist rule, is gaining popularity.
In India, Coca-Cola ranked third behind the leader, Pepsi-Cola, and local drink Thums Up. The Coca-Cola Company purchased Thums Up in 1993. , Coca-Cola held a 60.9% market-share in India. Tropicola, a domestic drink, is served in Cuba instead of Coca-Cola, due to a United States embargo. French brand Mecca Cola and British brand Qibla Cola are competitors to Coca-Cola in the Middle East.
In Turkey, Cola Turka, in Iran and the Middle East, Zamzam Cola and Parsi Cola, in some parts of China, China Cola, in Slovenia, Cockta and the inexpensive Mercator Cola, sold only in the country's biggest supermarket chain, Mercator, are some of the brand's competitors. Classiko Cola, made by Tiko Group, the largest manufacturing company in Madagascar, is a serious competitor to Coca-Cola in many regions. Laranjada is the top-selling soft drink on Madeira.
Advertising.
Coca-Cola's advertising has significantly affected American culture, and it is frequently credited with inventing the modern image of Santa Claus as an old man in a red-and-white suit. Although the company did start using the red-and-white Santa image in the 1930s, with its winter advertising campaigns illustrated by Haddon Sundblom, the motif was already common. Coca-Cola was not even the first soft drink company to use the modern image of Santa Claus in its advertising: White Rock Beverages used Santa in advertisements for its ginger ale in 1923, after first using him to sell mineral water in 1915. Before Santa Claus, Coca-Cola relied on images of smartly dressed young women to sell its beverages. Coca-Cola's first such advertisement appeared in 1895, featuring the young Bostonian actress Hilda Clark as its spokeswoman.
1941 saw the first use of the nickname "Coke" as an official trademark for the product, with a series of advertisements informing consumers that "Coke means Coca-Cola". In 1971 a song from a Coca-Cola commercial called "I'd Like to Teach the World to Sing", produced by Billy Davis, became a hit single.
Coke's advertising is pervasive, as one of Woodruff's stated goals was to ensure that everyone on Earth drank Coca-Cola as their preferred beverage. This is especially true in southern areas of the United States, such as Atlanta, where Coke was born.
Some Coca-Cola television commercials between 1960 through 1986 were written and produced by former Atlanta radio veteran Don Naylor (WGST 1936–1950, WAGA 1951–1959) during his career as a producer for the McCann Erickson advertising agency. Many of these early television commercials for Coca-Cola featured movie stars, sports heroes and popular singers.
During the 1980s, Pepsi-Cola ran a series of television advertisements showing people participating in taste tests demonstrating that, according to the commercials, "fifty percent of the participants who said they preferred Coke "actually" chose the Pepsi." Statisticians pointed out the problematic nature of a 50/50 result: most likely, the taste tests showed that in blind tests, most people cannot tell the difference between Pepsi and Coke. Coca-Cola ran ads to combat Pepsi's ads in an incident sometimes referred to as the "cola wars"; one of Coke's ads compared the so-called Pepsi challenge to two chimpanzees deciding which tennis ball was furrier. Thereafter, Coca-Cola regained its leadership in the market.
Selena was a spokesperson for Coca-Cola from 1989 till the time of her death. She filmed three commercials for the company. During 1994, to commemorate her five years with the company, Coca-Cola issued special Selena coke bottles.
The Coca-Cola Company purchased Columbia Pictures in 1982, and began inserting Coke-product images into many of its films. After a few early successes during Coca-Cola's ownership, Columbia began to under-perform, and the studio was sold to Sony in 1989.
Coca-Cola has gone through a number of different advertising slogans in its long history, including "The pause that refreshes", "I'd like to buy the world a Coke", and "Coke is it".
In 2006, Coca-Cola introduced My Coke Rewards, a customer loyalty campaign where consumers earn points by entering codes from specially marked packages of Coca-Cola products into a website. These points can be redeemed for various prizes or sweepstakes entries.
In Australia in 2011, Coca-Cola began the "share a Coke" campaign, where the Coca-Cola logo was replaced on the bottles and replaced with first names. Coca-Cola used the 150 most popular names in Australia to print on the bottles. The campaign was paired with a website page, Facebook page and an online "share a virtual Coke". The same campaign was introduced to Coca-Cola, Diet Coke & Coke Zero bottles and cans in the UK in 2013.
Coca-Cola has also advertised its product to be consumed as a breakfast beverage, instead of coffee or tea for the morning caffeine.
5 cents.
From 1886 to 1959, the price of Coca-Cola was fixed at five cents, in part due to an advertising campaign.
Holiday campaigns.
The "Holidays are coming!" advertisement features a train of red delivery trucks, emblazoned with the Coca-Cola name and decorated with Christmas lights, driving through a snowy landscape and causing everything that they pass to light up and people to watch as they pass through.
The advertisement fell into disuse in 2001, as the Coca-Cola company restructured its advertising campaigns so that advertising around the world was produced locally in each country, rather than centrally in the company's headquarters in Atlanta, Georgia. In 2007, the company brought back the campaign after, according to the company, many consumers telephoned its information center saying that they considered it to mark the beginning of Christmas. The advertisement was created by U.S. advertising agency Doner, and has been part of the company's global advertising campaign for many years.
Keith Law, a producer and writer of commercials for Belfast CityBeat, was not convinced by Coca-Cola's reintroduction of the advertisement in 2007, saying that "I don't think there's anything Christmassy about HGVs and the commercial is too generic."
In 2001, singer Melanie Thornton recorded the campaign's advertising jingle as a single, "Wonderful Dream (Holidays are Coming)", which entered the pop-music charts in Germany at no. 9. In 2005, Coca-Cola expanded the advertising campaign to radio, employing several variations of the jingle.
In 2011, Coca-Cola launched a campaign for the Indian holiday Diwali. The campaign included commercials, a song and an integration with Shah Rukh Khan’s film Ra.One.
Sports sponsorship.
Coca-Cola was the first commercial sponsor of the Olympic games, at the 1928 games in Amsterdam, and has been an Olympics sponsor ever since. This corporate sponsorship included the 1996 Summer Olympics hosted in Atlanta, which allowed Coca-Cola to spotlight its hometown. Most recently, Coca-Cola has released localized commercials for the 2010 Winter Olympics in Vancouver; one Canadian commercial referred to Canada's hockey heritage and was modified after Canada won the gold medal game on February 28, 2010 by changing the ending line of the commercial to say "Now they know whose game they're playing".
Since 1978, Coca-Cola has sponsored the FIFA World Cup, and other competitions organised by FIFA. One FIFA tournament trophy, the FIFA World Youth Championship from Tunisia in 1977 to Malaysia in 1997, was called "FIFA — Coca Cola Cup". In addition, Coca-Cola sponsors the annual Coca-Cola 600 and Coke Zero 400 for the NASCAR Sprint Cup Series at Charlotte Motor Speedway in Concord, North Carolina and Daytona International Speedway in Daytona, Florida.
Coca-Cola has a long history of sports marketing relationships, which over the years have included Major League Baseball, the National Football League, the National Basketball Association, and the National Hockey League, as well as with many teams within those leagues. Coca-Cola has had a longtime relationship with the NFL's Pittsburgh Steelers, due in part to the now-famous 1979 television commercial featuring "Mean Joe" Greene, leading to the two opening the Coca-Cola Great Hall at Heinz Field in 2001 and a more recent Coca-Cola Zero commercial featuring Troy Polamalu.
Coca-Cola is the official soft drink of many collegiate football teams throughout the nation, partly due to Coca-Cola providing those schools with upgraded athletic facilities in exchange for Coca-Cola's sponsorship. This is especially prevalent at the high school level, which is more dependent on such contracts due to tighter budgets.
Coca-Cola was one of the official sponsors of the 1996 Cricket World Cup held on the Indian subcontinent. Coca Cola is also one of the associate sponsor of Delhi Daredevils in Indian Premier League.
In England, Coca-Cola was the main sponsor of The Football League between 2004 and 2010, a name given to the three professional divisions below the Premier League in football (soccer). In 2005, Coca-Cola launched a competition for the 72 clubs of the football league — it was called "Win a Player". This allowed fans to place one vote per day for their favorite club, with one entry being chosen at random earning £250,000 for the club; this was repeated in 2006. The "Win A Player" competition was very controversial, as at the end of the 2 competitions, Leeds United A.F.C. had the most votes by more than double, yet they did not win any money to spend on a new player for the club. In 2007, the competition changed to "Buy a Player". This competition allowed fans to buy a bottle of Coca-Cola or Coca-Cola Zero and submit the code on the wrapper on the Coca-Cola website. This code could then earn anything from 50p to £100,000 for a club of their choice. This competition was favored over the old "Win a Player" competition, as it allowed all clubs to win some money. Between 1992 and 1998, Coca-Cola was the title sponsor of the Football League Cup (Coca-Cola Cup), the secondary cup tournament of England.
Between 1994 and 1997, Coca-Cola was also the title sponsor of the Scottish League Cup, renaming it the Coca-Cola Cup like its English counterpart.
Coca-Cola is the presenting sponsor of the Tour Championship, the final event of the PGA Tour held each year at East Lake Golf Club in Atlanta, GA.
Introduced March 1, 2010, in Canada, to celebrate the 2010 Winter Olympics, Coca Cola sold gold colored cans in packs of 12 each, in select stores.
In 2012, Coca-Cola (Philippines) hosted/sponsored the Coca-Cola PBA Youngstars in the Philippines.
In mass media.
Coca-Cola has been prominently featured in countless films and television programs. Since its creation, it remains as one of the most prominent elements of the popular culture. It was a major plot element in films such as "One, Two, Three", "The Coca-Cola Kid", and "The Gods Must Be Crazy", among many others. It provides a setting for comical corporate shenanigans in the novel "Syrup" by Maxx Barry. In music, in the Beatles' song, "Come Together", the lyrics say, "He shoot Coca-Cola, he say...". The Beach Boys also referenced Coca-Cola in their 1964 song "All Summer Long" (i.e. "'Member when you spilled Coke all over your blouse?")
The best selling artist of all time and worldwide cultural icon, Elvis Presley, promoted Coca-Cola during his last tour of 1977. The Coca-Cola Company used Elvis' image to promote the product. For example, the company used a song performed by Presley, A Little Less Conversation, in a Japanese Coca-Cola commercial.
Other artists that promoted Coca-Cola include the Beatles, David Bowie, George Michael, Elton John and Whitney Houston, who appeared in the Diet Coca-Cola commercial, among many others.
Not all musical references to Coca-Cola went well. A line in "Lola" by the Kinks was originally recorded as "You drink champagne and it tastes just like Coca-Cola." When the British Broadcasting Corporation refused to play the song because of the commercial reference, lead singer Ray Davies re-recorded the lyric as "it tastes just like cherry cola" to get airplay for the song.
Political cartoonist Michel Kichka satirized a famous Coca-Cola billboard in his 1982 poster "And I Love New York." On the billboard, the Coca-Cola wave is accompanied by the words "Enjoy Coke." In Kichka's poster, the lettering and script above the Coca-Cola wave instead read "Enjoy Cocaine."
Criticism.
Criticism of Coca-Cola has arisen from various groups, concerning a variety of issues, including health effects, environmental issues, and business practices. The Coca-Cola Company, its subsidiaries and products have been subject to sustained criticism by both consumer groups, leftist activists and watchdogs, particularly since the early 2000s.
Colombian death-squad allegations.
The Coca Cola company was sued over its alleged use of political far-right wing death squads (the United Self-Defense Forces of Colombia) to kidnap, torture, and kill, Columbian bottler workers that were linked with trade union activity. Coca Cola was sued in a US federal court in Miami by the Colombian food and drink union Sinaltrainal. The suit alleged that Coca Cola was indirectly responsible for "contracted with or otherwise directed paramilitary security forces that utilized extreme violence and murdered, tortured, unlawfully detained or otherwise silenced trade union leaders". This sparked campaigns to boycott Coca Cola in the UK, US, Germany, Italy and Australia.
Javier Correa, the president of Sinaltrainal, said the campaign aimed to put pressure on Coca-Cola "to mitigate the pain and suffering" that union members had suffered.
Speaking from the Coca Cola company's headquarters in Atlanta, company spokesperson Rafael Fernandez Quiros said "Coca-Cola denies any connection to any human-rights violation of this type" and added "We do not own or operate the plants".
Use as political and corporate symbol.
Coca-Cola has a high degree of identification with the United States, being considered by some an "American Brand" or as an item representing America. During World War II, this gave rise to brief production of the White Coke as a neutral brand. The drink is also often a metonym for the Coca-Cola Company.
There are some consumer boycotts of Coca-Cola in Arab countries due to Coke's early investment in Israel during the Arab League boycott of Israel (its competitor Pepsi stayed out of Israel). Mecca Cola and Pepsi have been successful alternatives in the Middle East.
A Coca-Cola fountain dispenser (officially a Fluids Generic Bioprocessing Apparatus-2 or FGBA-2) was developed for use on the Space Shuttle as a test bed to determine if carbonated beverages can be produced from separately stored carbon dioxide, water and flavored syrups and determine if the resulting fluids can be made available for consumption without bubble nucleation and resulting foam formation. The unit flew in 1996 aboard STS-77 and held 1.65 liters each of Coca-Cola and Diet Coke.
Social causes.
In 2012, Coca-Cola is listed as a partner of the (RED) campaign, together with other brands such as Nike, Girl, American Express and Converse. The campaign's mission is to prevent the transmission of the HIV virus from mother to child by 2015 (the campaign's byline is "Fighting For An AIDS Free Generation").

</doc>
<doc id="6693" url="https://en.wikipedia.org/wiki?curid=6693" title="Cofinality">
Cofinality

In mathematics, especially in order theory, the cofinality cf("A") of a partially ordered set "A" is the least of the cardinalities of the cofinal subsets of "A".
This definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set "A" can alternatively be defined as the least ordinal "x" such that there is a function from "x" to "A" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.
Cofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.
Properties.
If "A" admits a totally ordered cofinal subset, then we can find a subset "B" which is well-ordered and cofinal in "A". Any subset of "B" is also well-ordered. If two cofinal subsets of "B" have minimal cardinality (i.e. their cardinality is the cofinality of "B"), then they are order isomorphic to each other.
Cofinality of ordinals and other well-ordered sets.
The cofinality of an ordinal α is the smallest ordinal δ which is the order type of a cofinal subset of α. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.
Thus for a limit ordinal α, there exists a δ-indexed strictly increasing sequence with limit α. For example, the cofinality of ω² is ω, because the sequence ω·"m" (where "m" ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω. An uncountable limit ordinal may have either cofinality ω as does ωω or an uncountable cofinality.
The cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any nonzero limit ordinal is an infinite regular cardinal.
Regular and singular ordinals.
A regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.
Every regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the Axiom of choice, formula_1 is regular for each α. In this case, the ordinals 0, 1, formula_2, formula_3, and formula_4 are regular, whereas 2, 3, formula_5, and ωω·2 are initial ordinals which are not regular.
The cofinality of any ordinal "α" is a regular ordinal, i.e. the cofinality of the cofinality of "α" is the same as the cofinality of "α". So the cofinality operation is idempotent.
Cofinality of cardinals.
If κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an unbounded function from cf(κ) to κ; cf(κ) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is κ; more precisely
That the set above is nonempty comes from the fact that
i.e. the disjoint union of κ singleton sets. This implies immediately that cf(κ) ≤ κ.
The cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).
Using König's theorem, one can prove κ < κcf(κ) and κ < cf(2κ) for any infinite cardinal κ.
The last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,
the ordinal number ω being the first infinite ordinal, so that the cofinality of formula_9 is card(ω) = formula_10. (In particular, formula_9 is singular.) Therefore,
Generalizing this argument, one can prove that for a limit ordinal δ

</doc>
<doc id="6695" url="https://en.wikipedia.org/wiki?curid=6695" title="Citadel">
Citadel

A citadel is the core fortified area of a town or city. It may be a fortress, castle, or fortified center. The term is a diminutive of "city" and thus means "little city", so called because it is a smaller part of the city of which it is the defensive core.
In a fortification with bastions, the citadel is the strongest part of the system, sometimes well inside the outer walls and bastions, but often forming part of the outer wall for the sake of economy. It is positioned to be the last line of defense, should the enemy breach the other components of the fortification system. A citadel is also a term of the third part of a medieval castle, with higher walls than the rest. It was to be the last line of defense before the keep itself.
In various countries, the citadels gained a specific name such as "Kremlin" in Russia or "Alcázar" in the Iberian Peninsula. In European cities, the term "Citadel" and "City Castle" are often used interchangeably. The term "tower" is also used in some cases such as the Tower of London and Jerusalem's Tower of David. However, the Haitian citadel, which is the largest citadel in the Western Hemisphere, is called Citadelle Laferrière or simply the "Citadel" in English.
History.
8000 BC–600 AD.
In Ancient Greece, the Acropolis (literally: "peak of the city"), placed on a commanding eminence, was important in the life of the people, serving as a refuge and stronghold in peril and containing military and food supplies, the shrine of the god and a royal palace. The most well-known is the Acropolis of Athens, but nearly every Greek city-state had one – the Acrocorinth famed as a particularly strong fortress. In a much later period, when Greece was ruled by the Latin Empire, the same strong points were used by the new feudal rulers for much the same purpose.
167–160 BC.
Rebels who took power in the city but with the citadel still held by the former rulers could by no means regard their tenure of power as secure. One such incident played an important part in the history of the Maccabean Revolt against the Seleucid Empire. The Hellenistic garrison of Jerusalem and local supporters of the Seleucids held out for many years in the Acra citadel, making Maccabean rule in the rest of Jerusalem precarious. When finally gaining possession of the place, the Maccabeans pointedly destroyed and razed the Acra, though they constructed another citadel for their own use in a different part of Jerusalem.
3300–1300 BC.
Some of the oldest known structures which have served as citadels were built by the Indus Valley Civilization, where the citadel represented a centralised authority. The main citadel in Indus Valley was almost 12 meters tall. The purpose of these structures, however, remains debated. Though the structures found in the ruins of Mohenjo-daro were walled, it is far from clear that these structures were defensive against enemy attacks. Rather, they may have been built to divert flood waters.
500–1500 AD.
At various periods, and particularly during the Middle Ages, the citadel – having its own fortifications, independent of the city walls – was the last defence of a besieged army, often held after the town had been conquered. A city where the citadel held out against an invading army was not considered conquered. For example, in the 1543 Siege of Nice the Ottoman forces led by Barbarossa conquered and pillaged the town itself and took many captives – but the city castle held out, due to which the townspeople were accounted the victors.
1600–1860 AD.
As late as the 19th century, a similar situation developed at Antwerp, where a Dutch garrison under General David Hendrik Chassé held out in the city's citadel between 1830 and 1832, while the city itself had already become part of the independent Belgium.
In time of war the citadel in many cases afforded retreat to the people living in the areas around the town. However, Citadels were often used also to protect a garrison or political power from the inhabitants of the town where it was located, being designed to ensure loyalty from the town that they defended.
For example, Barcelona had a great citadel built in 1714 to intimidate the Catalans against repeating their mid-17th- and early-18th-century rebellions against the Spanish central government. In the 19th century, when the political climate had liberalized enough to permit it, the people of Barcelona had the citadel torn down, and replaced it with the city's main central park, the Parc de la Ciutadella. A similar example is the Citadella in Budapest, Hungary.
The attack on the Bastille in the French Revolution – though afterwards remembered mainly for the release of the handful of prisoners incarcerated there – was to considerable degree motivated by the structure being a Royal citadel in the midst of revolutionary Paris.
Similarly, after Garibaldi's overthrow of Bourbon rule in Palermo, during the 1860 Unification of Italy, Palermo's Castellamare Citadel – symbol of the hated and oppressive former rule – was ceremoniously demolished.
The Siege of the Alcázar in the Spanish Civil War, in which the Nationalists held out against a much larger Republican force for two months until relieved, shows that in some cases a citadel can be effective even in modern warfare; a similar case is the Battle of Huế during the Vietnam war, where a North Vietnamese Army division held the citadel of Huế for 26 days against roughly their own numbers of much better-equipped US and South Vietnamese troops.
1820–present AD.
The Citadelle of Québec (construction started 1673, completed 1820) still survives as the largest citadel still in official military operation in North America. It is home to the Royal 22nd Regiment of Canada; and forms part of the fortified walls of Vieux-Québec dating back to 1608.
Modern usage.
Citadels since the mid 20th century, are commonly military command and control centres built to resist attack commonly aerial or nuclear bombardment. The Military citadels under London such as the massive underground complex beneath the Ministry of Defense called Pindar is one such example, as is the Cheyenne Mountain nuclear bunker in the US.
Naval term.
On armored warships, the heavily armored section of the ship that protects the ammunition and machinery spaces is called the citadel.
The safe room on a ship is also called a citadel.

</doc>
<doc id="6696" url="https://en.wikipedia.org/wiki?curid=6696" title="Mail (armour)">
Mail (armour)

Mail or chain mail is a type of armour consisting of small metal rings linked together in a pattern to form a mesh. A coat of this armour is often referred to as a hauberk.
History.
The earliest example of mail was found in a Dacian chieftain's burial located in Ciumești, Romania. Its invention is commonly credited to the Celts, but there are examples of Etruscan pattern mail dating from at least the 4th century BC. Mail may have been inspired by the much earlier scale armour. Mail spread to North Africa, the Middle East, Central Asia, India, Tibet, South East Asia, and Japan.
Mail continues to be used in the 21st century as a component of stab-resistant body armour, cut-resistant gloves for butchers and woodworkers, shark-resistant wetsuits for defense against shark bites, and a number of other applications.
Etymology.
The origins of the word “mail” are not fully known. One theory is that it originally derives from the Latin word "macula", meaning "spot" or “opacity” (as in macula of retina). Another theory relates the word to the old French “maillier”, meaning “to hammer” (related to the modern English word “malleable”). In modern French "maille" refers to a loop or stitch.
The first attestations of the word “mail” are in Old French and Anglo-Norman: “maille” “maile”, or “male” or other variants, which became “mailye” “maille” “maile”, “male”, or “meile” in Middle English.
The modern usage of terms for mail armour is highly contested in popular and, to a lesser degree, academic culture. Medieval sources referred to armour of this type simply as “mail”, however “chain-mail” has become a commonly used, if incorrect, neologism first attested in Sir Walter Scott’s 1822 novel "The Fortunes of Nigel". Since then the word “mail” has been commonly, if incorrectly, applied to other types of armour, such as in “plate-mail” (first attested in 1835). The more correct term is “plate armour”.
Civilizations that used mail invented specific terms for each garment made from it. The standard terms for European mail armour derive from French: leggings are called chausses, a hood is a coif, and mittens, mitons. A mail collar hanging from a helmet is a camail or aventail. A shirt made from mail is a hauberk if knee-length and a haubergeon if mid-thigh length. A layer (or layers) of mail sandwiched between layers of fabric is called a jazerant.
A waist-length coat in medieval Europe was called a byrnie, although the exact construction of a byrnie is unclear, including whether it was constructed of mail or other armour-types. Noting that the byrnie was the "most highly valued piece of armour" to the Carolingian soldier, Bennet, Bradbury, DeVries, Dickie, and Jestice indicate that:
There is some dispute among historians as to what exactly constituted the Carolingian byrnie. Relying... only on artistic and some literary sources because of the lack of archaeological examples, some believe that it was a heavy leather jacket with metal scales sewn onto it. It was also quite long, reaching below the hips and covering most of the arms. Other historians claim instead that the Carolingian byrnie was nothing more than a coat of mail, but longer and perhaps heavier than traditional early medieval mail. Without more certain evidence, this dispute will continue.
Mail armour in Europe.
The use of mail as battlefield armour was common during the Iron Age and the Middle Ages, becoming less common over the course of the 16th and 17th centuries. It is believed that the Roman Republic first came into contact with mail fighting the Gauls in Cisalpine Gaul, now Northern Italy, but a different pattern of mail was already in use among the Etruscans. The Roman army adopted the technology for their troops in the form of the lorica hamata which was used as a primary form of armour through the Imperial period.
After the fall of the Western Empire, much of the infrastructure needed to create plate armour diminished. Eventually the word "mail" came to be synonymous with armour. It was typically an extremely prized commodity, as it was expensive and time-consuming to produce and could mean the difference between life and death in a battle. Mail from dead combatants was frequently looted and was used by the new owner or sold for a lucrative price. As time went on and infrastructure improved, it came to be used by more soldiers. Eventually with the rise of the lanced cavalry charge, impact warfare, and high-powered crossbows, mail came to be used as a secondary armour to plate for the mounted nobility.
By the 14th century, plate armour was commonly used to supplement mail. Eventually mail was supplanted by plate for the most part, as it provided greater protection against windlass crossbows, bludgeoning weapons, and lance charges. However, mail was still widely used by many soldiers as well as brigandines and padded jacks. These three types of armour made up the bulk of the equipment used by soldiers, with mail being the most expensive. It was sometimes more expensive than plate armour. Mail typically persisted longer in less technologically advanced areas such as Eastern Europe but was in use everywhere into the 16th century.
During the late 19th and early 20th century, mail was used as a material for bulletproof vests, most notably by the Wilkinson Sword Company. Results were unsatisfactory; Wilkinson mail worn by the Khedive of Egypt's regiment of "Iron Men" was manufactured from split rings which proved to be too brittle, and the rings would fragment when struck by bullets and aggravate the damage. The riveted mail armour worn by the opposing Sudanese Madhists did not have the same problem but also proved to be relatively useless against the firearms of British forces at the battle of Omdurman. During World War I, Wilkinson Sword transitioned from mail to a lamellar design which was the precursor to the flak jacket.
Also during World War I, a mail fringe, designed by Captain Cruise of the British Infantry, was added to helmets to protect the face. This proved unpopular with soldiers, in spite of being proven to defend against a three-ounce (100 g) shrapnel round fired at a distance of one hundred yards (90 m).
Mail armour in Asia.
Mail armour was introduced to the Middle East and Asia through the Romans and was adopted by the Sassanid Persians starting in the 3rd century AD, where it was supplemental to the scale and lamellar armour already used. Mail was commonly also used as horse armour for cataphracts and heavy cavalry as well as armour for the soldiers themselves. Asian mail was typically lighter than the European variety and sometimes had prayer symbols stamped on the rings as a sign of their craftsmanship as well as for divine protection. Indeed, mail armour is mentioned in the Koran as being a gift revealed by Allah to David:
21:80 "It was We Who taught him the making of coats of mail for your benefit, to guard you from each other's violence: will ye then be grateful?" (Yusuf Ali's translation).
From the Middle East, mail was quickly adopted in Central Asia by the Sogdians and by India in the South. It was not commonly used in Mongol armies due to its weight and the difficulty of its maintenance, but it eventually became the armour of choice in India. Indian mail was often used with plate protection. Plated mail was in common use in India until the Battle of Plassey and the subsequent British conquest of the sub-continent.
The Ottoman Empire used plated mail widely and it was used in their armies until the 18th century by heavy cavalry and elite units such as the Janissaries. They spread its use into North Africa where it was adopted by Mamluk Egyptians and the Sudanese who produced it until the early 20th century.
Mail was introduced to China when its allies in Central Asia paid tribute to the Tang Emperor in 718 by giving him a coat of "link armour" assumed to be mail. China first encountered the armour in 384 when its allies in the nation of Kuchi arrived wearing "armour similar to chains". Once in China, mail was imported but was not produced widely. Due to its flexibility and comfort, it was typically the armour of high-ranking guards and those who could afford the import rather than the armour of the rank and file, who used the easier to produce and maintain brigandine and lamellar types. However, it was one of the only military products that China imported from foreigners. Mail spread to Korea slightly later where it was imported as the armour of imperial guards and generals.
Japanese mail armour.
In Japan mail is called ' which means chain. When the word "kusari" is used in conjunction with an armoured item it usually means that mail makes up the majority of the armour composition. An example of this would be "kusari gusoku" which means chain armour. "Kusari" ', ', ', ', ', shoulder, ', and other armoured clothing were produced, even ' socks.
"" was used in samurai armour at least from the time of the Mongol invasion (1270s) but particularly from the Nambokucho period (1336–1392). The Japanese used many different weave methods including: a square 4-in-1 pattern ("so gusari"), a hexagonal 6-in-1 pattern ("hana gusari") and a European 4-in-1 ("nanban gusari"). The rings of Japanese mail were much smaller than their European counterparts; they would be used in patches to link together plates and to drape over vulnerable areas such as the armpits.
Riveted kusari was known and used in Japan. On page 58 of the book "Japanese Arms & Armor: Introduction" by H. Russell Robinson, there is a picture of Japanese riveted kusari, and
this quote from the translated reference of Sakakibara Kozan's 1800 book, "The Manufacture of Armour and Helmets in Sixteenth Century Japan", shows that the Japanese not only knew of and used riveted kusari but that they manufactured it as well.
... karakuri-namban (riveted namban), with stout links each closed by a rivet. Its invention is credited to Fukushima Dembei Kunitaka, pupil, of Hojo Awa no Kami Ujifusa, but it is also said to be derived directly from foreign models. It is heavy because the links are tinned (biakuro-nagashi) and these are also sharp edged because they are punched out of iron plate
Butted and or split (twisted) links made up the majority of "kusari" links used by the Japanese. Links were either "butted" together meaning that the ends touched each other and were not riveted, or the "kusari" was constructed with links where the wire was turned or twisted two or more times; these split links are similar to the modern split ring commonly used on keychains. The rings were lacquered black to prevent rusting, and were always stitched onto a backing of cloth or leather. The kusari was sometimes concealed entirely between layers of cloth.
Kusari gusoku or chain armour was commonly used during the Edo period 1603 to 1868 as a stand-alone defence. According to George Cameron Stone
Entire suits of mail "kusari gusoku" were worn on occasions, sometimes under the ordinary clothing
Ian Bottomley in his book "Arms and Armor of the Samurai: The History of Weaponry in Ancient Japan" shows a picture of a kusari armour and mentions "" (chain jackets) with detachable arms being worn by samurai police officials during the Edo period. The end of the samurai era in the 1860s, along with the 1876 ban on wearing swords in public, marked the end of any practical use for mail and other armour in Japan. Japan turned to a conscription army and uniforms replaced armour.
Effectiveness.
Mail armour provided an effective defence against slashing blows by edged weapons and penetration by thrusting and piercing weapons; in fact, a study conducted at the Royal Armouries at Leeds concluded that "it is almost impossible to penetrate using any conventional medieval weapon." Generally speaking, mail's resistance to weapons is determined by four factors: linkage type (riveted, butted, or welded), material used (iron versus bronze or steel), weave density (a tighter weave needs a thinner weapon to surpass), and ring thickness (generally ranging from 18 to 14 gauge (1.02–1.63 mm diameter wire) in most examples). Mail, if a warrior could afford it, provided a significant advantage to a warrior when combined with competent fighting techniques. When the mail was not riveted, a well-placed thrust from a spear or thin sword could penetrate, and a pollaxe or halberd blow could break through the armour. Some evidence indicates that during armoured combat, the intention was to actually get around the armour rather than through it—according to a study of skeletons found in Visby, Sweden, a majority of the skeletons showed wounds on less well protected legs.
The flexibility of mail meant that a blow would often injure the wearer, potentially causing serious bruising or fractures, and it was a poor defence against head trauma. Mail-clad warriors typically wore separate rigid helms over their mail coifs for head protection. Likewise, blunt weapons such as maces and warhammers could harm the wearer by their impact without penetrating the armour; usually a soft armour, such as gambeson, was worn under the hauberk. Medieval surgeons were very well capable of setting and caring for bone fractures resulting from blunt weapons. With the poor understanding of hygiene however, cuts that could get infected were much more of a problem. Thus mail armour proved to be sufficient protection in most situations.
Manufacture.
Several patterns of linking the rings together have been known since ancient times, with the most common being the 4-to-1 pattern (where each ring is linked with four others). In Europe, the 4-to-1 pattern was completely dominant. Mail was also common in East Asia, primarily Japan, with several more patterns being utilised and an entire nomenclature developing around them.
Historically, in Europe, from the pre-Roman period on, the rings composing a piece of mail would be riveted closed to reduce the chance of the rings splitting open when subjected to a thrusting attack or a hit by an arrow.
Up until the 14th century European mail was made of alternating rows of round riveted rings and solid rings. Sometime during the 14th century European mail makers started to transition from round rivets to wedge shaped rivets but continued using alternating rows of solid rings. Eventually European mail makers stopped using solid rings and almost all European mail was made from wedge riveted rings only with no solid rings. Both were commonly made of wrought iron, but some later pieces were made of heat-treated steel. Wire for the riveted rings was formed by either of two methods. One was to hammer out wrought iron into plates and cut or slit the plates. These thin pieces were then pulled through a draw plate repeatedly until the desired diameter was achieved. Waterwheel powered drawing mills are pictured in several period manuscripts. Another method was to simply forge down an iron billet into a rod and then proceed to draw it out into wire. The solid links would have been made by punching from a sheet. Guild marks were often stamped on the rings to show their origin and craftsmanship. Forge welding was also used to create solid links, but there are few possible examples known; the only well documented example from Europe is that of the camail (mail neck-defence) of the 7th century Coppergate helmet. Outside of Europe this practice was more common such as "theta" links from India. Very few examples of historic butted mail have been found and it is generally accepted that butted mail was never in wide use historically except in Japan where mail ("kusari") was commonly made from "butted" links.
Modern uses.
Practical uses.
Mail is used as protective clothing for butchers against meat-packing equipment. Workers may wear up to of mail under their white coats. Butchers also commonly wear a single mail glove to protect themselves from self-inflicted injury while cutting meat.
Woodcarvers sometimes use similar mail gloves to protect their hands from cuts and punctures.
Scuba divers use mail to protect them from sharkbite, as do animal control officers for protection against the animals they handle. Shark expert and underwater filmmaker Valerie Taylor was among the first to develop and test shark suits in 1979 while diving with sharks.
Mail is widely used in industrial settings as shrapnel guards and splash guards in metal working operations.
Electrical applications for mail include RF leakage testing and being worn as a faraday cage suit by tesla coil enthusiasts and high voltage electrical workers.
Stab-proof vests.
Conventional textile-based ballistic vests are designed to stop soft-nosed bullets but offer little defense from knife attacks. Knife-resistant armour are designed to defend against knife attacks; some of these use layers of metal plates, mail and metallic wires.
Historical re-enactment.
Many historical reenactment groups, especially those whose focus is Antiquity or the Middle Ages, commonly use mail both as practical armour and for costuming. Mail is especially popular amongst those groups which use steel weapons. A modern hauberk made from 1.5 mm diameter wire with 10 mm inner diameter rings weighs roughly and contains 15,000–45,000 rings.
One of the drawbacks of mail is the uneven weight distribution; the stress falls mainly on shoulders. Weight can be better distributed by wearing a belt over the mail, which provides another point of support.
Mail worn today for re-enactment and recreational use can be made in a variety of styles and materials. Most recreational mail today is made of butted links which are galvanized or stainless steel. This is historically inaccurate but is much less expensive to procure and maintain than historically accurate reproductions. Mail can also be made of titanium, aluminium, bronze, or copper. Riveted mail offers significantly better protection ability as well as historical accuracy than mail constructed with butted links. Riveted mail can be more labour-intensive and expensive to manufacture. Japanese mail ("kusari") is one of the few historically correct examples of mail being constructed with such "butted links".
Decorative uses.
Mail remained in use as a decorative and possibly high-status symbol with military overtones long after its practical usefulness had passed. It was frequently used for the epaulettes of military uniforms. It is still used in this form by the British Territorial Army.
Mail has applications in sculpture and jewellery, especially when made out of precious metals or colourful anodized metals. Mail artwork includes headdresses, Christmas ornaments, chess sets, and jewelry. For these non-traditional applications, hundreds of weaves or patterns have been invented.
Large-linked mail is occasionally used as a fetish clothing material, with the large links intended to reveal in part the body beneath them.
In film.
In some films, knitted string spray-painted with a metallic paint is used instead of actual mail in order to cut down on cost (an example being "Monty Python and the Holy Grail", which was filmed on a very small budget). Films more dedicated to costume accuracy often use ABS plastic rings, for the lower cost and weight. Such ABS mail coats were made for "The Lord of the Rings" film trilogy, in addition to many metal coats. The metal coats are used rarely because of their weight, except in close-up filming where the appearance of ABS rings is distinguishable. A large scale example of the ABS mail used in the "Lord of the Rings" can be seen in the entrance to the Royal Armouries museum in Leeds in the form of a large curtain bearing the logo of the museum. It was acquired from the makers of the film's armour, Weta Workshop, when the museum hosted an exhibition of WETA armour from their films. For the film "Mad Max Beyond Thunderdome", Tina Turner is said to have worn actual mail and she complained how heavy this was.

</doc>
<doc id="6697" url="https://en.wikipedia.org/wiki?curid=6697" title="Cerberus">
Cerberus

In Greek mythology, Cerberus (; "Kerberos" ), often called the "hound of Hades", is a monstrous multi-headed dog, who guards the gates of the underworld, preventing the dead from leaving. He was the offspring of the monsters Echidna and Typhon, and is usually described as having three heads, a serpent for a tail, with snakes protruding from various parts of his body. Cerberus is primarily known for his capture by Heracles, one of Heracles' twelve labours.
Descriptions.
Descriptions of Cerberus vary, including the number of his heads. Cerberus was usually three-headed, though not always. Cerberus had a multi-headed heritage. His father was the multi snake-headed Typhon, and Cerberus was the brother of three other multi-headed monsters, the multi-snake-headed Lernaean Hydra; Orthrus, the two-headed dog who guarded the Cattle of Geryon; and the Chimera, who had three heads, that of a lion, a goat, and a snake. And, like these close relatives, Cerberus was, with only the rare iconographic exception, multi-headed.
In the earliest description of Cerberus, Hesiod's "Theogony" (c. 8th – 7th century BC), Cerberus has fifty heads, while Pindar (c. 522 – c. 443 BC) gave him one hundred heads. However, later writers almost universally give Cerberus three heads. An exception is the Latin poet Horace's Cerberus which has a single dog head, and one hundred snake heads. Perhaps trying to reconcile these competing traditions, Apollodorus's Cerberus has three dog heads and the heads of "all sorts of snakes" along his back, while the Byzantine poet John Tzetzes (who probably based his account on Apollodorus) gives Cerberus fifty heads, three of which were dog heads, the rest being the "heads of other beasts of all sorts".
In art Cerberus is most commonly depicted with two dog heads (visible), never more than three, and occasionally only one. On one of the two earliest depictions (c. 590–580 BC), a Corinthian cup from Argos (see below), now lost, Cerberus is shown as a normal single-headed dog. The first appearance of a three-headed Cerberus occurs on a mid sixth century BC Laconian cup (see below).
Horace's many snake-headed Cerberus followed a long tradition of Cerberus being part snake. This is perhaps already implied as early as in Hesiod's "Theogony", where Cerberus' mother is the half-snake Echidna, and his father the snake-headed Typhon. In art Cerberus is often shown as being part snake, for example the lost Corinthian cup shows snakes protruding from Cerberus' body, while the mid sixth-century BC Laconian cup gives Cerberus a snake for a tail. In the literary record, the first certain indication of Cerberus' serpentine nature comes from the rationalized account of Hecataeus of Miletus (fl. 500–494 BC), who makes Cerberus a large poisonous snake. Plato refers to Cerberus' composite nature, and Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and presumably in connection to his serpentine nature, associates Cerberus with the creation of the poisonous aconite plant. Virgil has snakes writhe around Cerberus' neck, Ovid's Cerberus has a venomous mouth, necks "vile with snakes", and "hair inwoven with the threatening snake", while Seneca gives Cerberus a mane consisting of snakes, and a single snake tail.
Cerberus was given various other traits. According to Euripides, Cerberus not only had three heads but three bodies, and according to Virgil he had multiple backs. Cerberus ate raw flesh (according to Hesiod), had eyes which flashed fire (according to Euphorion), a three-tongued mouth (according to Horace), and acute hearing (according to Seneca).
The Twelfth Labour of Heracles.
As early as Homer we learn that Heracles was sent by Eurystheus, the king of Tiryns, to bring back Cerberus from Hades the king of the underworld. According to Apollodorus, this was the twelfth and final labour imposed on Heracles. In a fragment from a lost play "Pirithous", (attributed to either Euripides or Critias) Heracles says that, although Eurystheus commanded him to bring back Cerberus, it was not from any desire to see Cerberus, but only because Eurystheus thought that the task was impossible.
Heracles was aided in his mission by his being an initiate of the Eleusinian Mysteries. Euripides has his initiation being "lucky" for Heracles in capturing Cerberus. And both Diodorus Siculus and Apollodorus say that Heracles was initiated into the Mysteries, in preparation for his descent into the underworld. According to Diodorus, Heracles went to Athens, where Musaeus, the son of Orpheus, was in charge of the initiation rites, while according to Apollodorus, he went to Eumolpus at Eleusis.
Heracles also had the help of Hermes, the usual guide of the underworld, as well as Athena. In the "Odyssey", Homer has Hermes and Athena as his guides. And Hermes and Athena are often shown with Heracles on vase paintings depicting Cerberus' capture. By most accounts, Heracles made his descent into the underworld through an entrance at Tainaron, the most famous of the various Greek entrances to the underworld. The place is first mentioned in connection with the Cerberus story in the rationalized account of Hecataeus of Miletus (fl. 500–494 BC), and Euripides, Seneca, and Apolodorus, all have Heracles descend into the underworld there. However Xenophon reports that Heracles was said to have descended at the Acherusian Chersonese near Heraclea Pontica, on the Black Sea, a place more usually associated with Heracles' exit from the underworld (see below). Heraclea, founded c. 560 BC, perhaps took its name from the association of its site with Heracles' Cerberian exploit.
Theseus and Pirithous.
While in the underworld, Heracles met the heroes Theseus and Pirithous, where the two companions were being held prisoner by Hades for attempting to carry off Hades' wife Persephone. Along with bringing back Cerberus, Heracles also managed (usually) to rescue Theseus, and in some versions Pirithous as well. According to Apollodorus, Heracles found Theseus and Pirithous near the gates of Hades, bound to the "Chair of Forgetfulness, to which they grew and were held fast by coils of serpents", and when they saw Heracles, "they stretched out their hands as if they should be raised from the dead by his might", and Heracles was able to free Theseus, but when he tried to raise up Pirithous, "the earth quaked and he let go."
The earliest evidence for the involvement of Theseus and Pirithous in the Cerberus story, is found on a shield-band relief (c. 560 BC) from Olympia, where Theseus and Pirithous (named) are seated together on a chair, arms held out in supplication, while Heracles approaches, about to draw his sword. The earliest literary mention of the rescue occurs in Euripides, where Heracles saves Theseus (with no mention of Pirithous). In the lost play "Pirithous", both heroes are rescued, while in the rationalized account of Philochorus, Heracles was able to rescue Theseus, but not Pirithous. In one place Diodorus says Heracles brought back both Theseus and Pirithous, by the favor of Persephone, while in another he says that Pirithous remained in Hades, or according to "some writers of myth" that neither Theseus, nor Pirithous returned. Both are rescued in Hyginus.
Capture of Cerberus.
There are various versions of how Heracles accomplished Cerberus' capture. According to Apollodorus, Heracles asked Hades for Cerberus, and Hades told Heracles he would allow him to take Cerberus only if he "mastered him without the use of the weapons which he carried", and so, using his lion-skin as shield, Heracles squeezed Cerberus around the head until he submitted.
In some early sources Cerberus' capture seems to involve Heracles fighting Hades. Homer has Hades injured by an arrow shot by Heracles, while on the early sixth-century BC lost Corinthian cup, Heracles is shown attacking Hades with a stone. A scholium to the "Iliad" passage, explains that Hades had commanded that Heracles "master Cerberus without shield or Iron". Heracles did this, by (as in Apollodorus) using his lion-skin instead of his shield, and making stone points for his arrows, but when Hades still opposed him, Heracles shot Hades in anger. Consistent with the no iron requirement, the iconographic tradition, from c. 560 BC, often shows Heracles using his wooden club against Cerberus.
Euripides, has Amphitryon ask Heracles: "Did you conquer him in fight, or receive him from the goddess Persephone? To which, Heracles answers: "In fight", and the "Pirithous" fragment says that Heracles "overcame the beast by force". However, according to Diodorus, Persphone welcomed Heracles "like a brother" and gave Cerberus "in chains" to Heracles. Aristophanes, has Heracles seize Cerberus in a stranglehold and run off, while Seneca has Heracles again use his lion-skin as shield, and his wooden club, to subdue Cerberus, after which a quailing Hades and Persephone, allow Heracles to lead a chained and submissive Cerberus away. Cerberus is often shown being chained, and Ovid tells that Heracles dragged Cerberus with chains of adamant.
Exit from the underworld.
There were several locations which were said to be the place where Heracles brought up Cerberus from the underworld. The geographer Strabo (63/64 BC – c. AD 24) reports that "according to the myth writers" Cerberus was brought up at Tainaron, the same place where Euripides has Heracles enter the underworld. Seneca has Heracles enter and exit at Tainaron. Apollodorus, although he has Heracles enter at Tainaron, has him exit at Troezen. The geographer Pausanias tells us that there was a temple at Troezen with "altars to the gods said to rule under the earth", where it was said that, in addition to Cerberus being "dragged" up by Heracles, Semele was supposed to have been brought up out of the underworld by Dionysus.
Another tradition had Cerberus brought up at Heraclea Pontica (the same place which Xenophon had earlier associated with Heracles' descent) and the cause of the poisonous plant aconite which grew there in abundance. Herodorus of Heraclea and Euphorion said that when Heracles brought Cerberus up from the underworld at Heraclea, Cerberus "vomited bile" from which the aconite plant grew up. Ovid, also makes Cerberus the cause of the poisonous aconite, saying that on the "shores of Scythia", upon leaving the underworld, as Cerberus was being dragged by Heracles from a cave, dazzled by the unaccustomed daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous. Seneca's Cerberus too, like Ovid's, reacts violently to his first sight of daylight. Enraged, the previously submissive Cerberus struggles furiously, and Heracles and Theseus must together drag Cerberus into the light.
Pausanias reports that according to local legend Cerberus was brought up through a chasm in the earth dedicated to Clymenus (Hades) next to the sanctuary of Chthonia at Hermione, and in Euripides' "Heracles", thought Euripides does not say that Cerberus was brought out there, he has Cerberus kept for a while in the "grove of Chthonia" at Hermione. Pausanias also mentions that at Mount Laphystion in Boeotia, that there was a statue of Heracles Charops ("with bright eyes"), where the Boeotians said Heracles brought up Cerberus. Other locations which perhaps were also associated with Cerberus being brought out of the underworld include, Hierapolis, Thesprotia, and Emeia near Mycenae.
Cerberus presented to Eurystheus, returned to Hades.
In some accounts, after bringing Cerberus up from the underworld, Heracles paraded the captured Cerberus through Greece. Euphorion has Heracles lead Cerberus through Midea in Argolis, as women and children watch in fear, and Diodorus Siculus says of Cerberus, that Heracles "carried him away to the amazement of all and exhibited him to men." Seneca has Juno complain of Heracles "highhandedly parading the black hound through Argive cities" and Heracles greeted by laurel-wreathed crowds, "singing" his praises.
Then, according to Apollodorus, Heracles showed Cerberus to Eurystheus, as commanded, after which he returned Cerberus to the underworld. However, according Hesychius of Alexandria, Cerberus escaped, presumably returning to the underworld on his own.
Principal sources.
The earliest mentions of Cerberus (c. 8th – 7th century BC) occur in Homer's "Iliad" and "Odyssey", and Hesiod's "Theogony". Homer does not name or describe Cerberus, but simply refers to Heracles being sent by Eurystheus to fetch the "hound of Hades", with Hermes and Athena as his guides, and that Heracles shot Hades with an arrow. According to Hesiod, Cerberus was the offspring of the monsters Echidna and Typhon, was fifty-headed, ate raw flesh, and was the "brazen-voiced hound of Hades", who fawns on those that enter the house of Hades, but eats those who try to leave.
Stesichorus (c. 630 – 555 BC) apparently wrote a poem called "Cerberus", of which virtually nothing remains. However the early sixth century BC lost Corinthian cup from Argos, which showed a single head, and snakes growing out from many places on his body, was possibly influenced by Stesichorus' poem. The mid-sixth-century BC cup from Laconia gives Cerberus three heads and a snake tail, which eventually becomes the standard representation.
Pindar (c. 522 – c. 443 BC) apparently gave Cerberus one hundred heads. Bacchylides (5th century BC) also mentions Heracles bringing Cerberus up from the underworld, with no further details. Sophocles (c. 495 – c. 405 BC), in his "Women of Trachis", makes Cerberus three-headed, and in his "Oedipus at Colonus", the Chorus asks that Oedipus be allowed to pass the gates of the underworld undisturbed by Cerberus, called here the "untamable Watcher of Hades". Euripides (c. 480 – 406 BC) describes Cerberus as three-headed, and three-bodied, says that Heracles entered the underworld at Tainaron, has Heracles say that Cerberus was not given to him by Persephone, but rather he fought and conquered Cerberus, "for I had been lucky enough to witness the rites of the initiated", an apparent reference to his initiation into the Eleusinian Mysteries, and says that the capture of Cerberus was the last of Heracles' labors. The lost play "Pirthous" (attributed to either Euripides or his late contemporary Critias) has Heracles say that he came to the underworld at the command of Eurystheus, who had ordered him to bring back Cerberus alive, not because he wanted to see Cerberus, but only because Eurystheus thought Heracles would not be able to accomplish the task, and that Heracles "overcame the beast" and "received favour from the gods".
Plato (c. 425 – 348 BC) refers to Cerberus' composite nature, citing Cerberus, along with Scylla and the Chimera, as an example from "ancient fables" of a creature composed of many animal forms "grown together in one". Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and eyes that flashed, like sparks from a blacksmith's forge, or the volcaninc Mount Etna. From Euphorion, also comes the first mention of a story which told that at Heraclea Pontica, where Cerberus was brought out of the underworld, by Heracles, Cerberus "vomited bile" from which the poisonous aconite plant grew up.
According to Diodorus Siculus (1st century BC), the capture of Cerberus was the eleventh of Heracles' labors, the twelfth and last being stealing the Apples of the Hesperides. Diodorus says that Heracles thought it best to first go to Athens to take part in the Eleusinian Mysteries, "Musaeus, the son of Orpheus, being at that time in charge of the initiatory rites", after which, he entered into the underworld "welcomed like a brother by Persephone", and "receiving the dog Cerberus in chains he carried him away to the amazement of all and exhibited him to men."
In Virgil's "Aeneid" (1st century BC), Aeneas and the Sibyl encounter Cerberus in a cave, where he "lay at vast length", filling the cave "from end to end", blocking the entrance to the underworld. Cerberus is described as "triple-throated", with "three fierce mouths", multiple "large backs", and serpents writhing around his neck. The Sybyl throws Cerberus a loaf laced with honey and herbs to induce sleep, enabling Aeneas to enter the underworld, and so apparently for Virgil—contradicting Hesiod—Cerberus guarded the underworld against entrance. Later Virgil describes Cerberus, in his bloody cave, crouching over half-gnawed bones. In his "Georgics", Virgil refers to Cerberus, his "triple jaws agape" being tamed by Orpheus' playing his lyre.
Horace (65 – 8 BC) also refers to Cerberus yielding to Orphesus' lyre, here Cerberus has a single dog head, which "like a Fury's is fortified by a hundred snakes", with a "triple-tongued mouth" oozing "fetid breath and gore".
Ovid (43 BC – AD 17/18) has Cerberus' mouth produce venom, and like Euphorion, makes Cerberus the cause of the poisonous plant aconite. According to Ovid, Heracles dragged Cerberus from the underworld, emerging from a cave "where 'tis fabled, the plant grew / on soil infected by Cerberian teeth", and dazzled by the daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous.
Seneca, in his tragedy "Hercules Furens" gives a detailed description of Cerberus and his capture.
Seneca's Cerberus has three heads, a mane of snakes, and a snake tail, with his three heads being covered in gore, and licked by the many snakes which surround them, and with hearing so acute that he can hear "even ghosts". Seneca has Heracles use his lion-skin as shield, and his wooden club, to beat Cerberus into submission, after which Hades and Persephone, quailing on their thrones, let Heracles lead a chained and submissive Cerberus away. But upon leaving the underworld, at his first sight of daylight, a frightened Cerberus struggles furiously, and Heracles, with the help of Theseus (who had been held captive by Hades, but released, at Heracles' request) drag Cerberus into the light. Seneca, like Diodorus, has Heracles parade the captured Cerberus through Greece.
Apollodorus' Cerberus has three dog-heads, a serpent for a tail, and the heads of many snakes on his back. According to Apollodorus, Heracles' twelfth and final labor was to bring back Cerberus from Hades. Heracles first went to Eumolpus to be initiated into the Eleusinian Mysteries. Upon his entering the underworld, all the dead flee Heracles except for Meleager and the Gorgon Medusa. Heracles drew his sword against Medusa, but Hermes told Heracles that the dead are mere "empty phantoms". Heracles asked Hades (here called Pluto) for Cerberus, and Hades said that Heracles could take Cerberus provided he was able to subdue him without using weapons. Heracles found Cerberus at the gates of Acheron, and with his arms around Cerberus, though being bitten by Cerberus' serpent tail, Heracles squeezed until Cerberus submitted. Heracles carried Cerberus away, showed him to Eurystheus, then returned Cerberus to the underworld.
In an apparently unique version of the story, related by the sixth-century AD Pseudo-Nonnus, Heracles descended into Hades to abduct Persephone, and killed Cerberus on his way back up.
Iconography.
The capture of Cerberus was a popular theme in ancient Greek and Roman art. The earliest depictions date from the beginning of the sixth century BC. One of the two earliest depictions, a Corinthian cup (c. 590–580 BC) from Argos (now lost), shows a naked Heracles, with quiver on his back and bow in his right hand, striding left, accompanied by Hermes. Heracles threatens Hades with a stone, who flees left, while a goddess, perhaps Persephone or possibly Athena, standing in front of Hades' throne, prevents the attack. Cerberus, with a single canine head, and snakes rising from his head and body, flees right. On the far right a column indicates the entrance to Hades' palace. Many of the elements of this scene: Hermes, Athena, Hades, Persephone, and a column or portico, are common occurrences in later works. The other earliest depiction, a relief "pithos" fragment from Crete (c. 590–570 BC) is thought to show a single lion-headed Cerberus, with a snake (open-mouthed) over his back, being led to the right.
A mid sixth century BC Laconian cup, by the Hunt Painter, adds several new features to the scene which also become common in later works: three heads, a snake tail, Cerberus' chain and Heracles' club. Here Cerberus has three canine heads, is covered by a shaggy coat of snakes, and has a tail which ends in a snake head. He is being held on a chain leash by Heracles who holds his club raised over head.
In Greek art, the vast majority of depictions of Heracles and Cerberus occur on Attic vases. Although the lost Corinthian cup shows Cerberus with a single dog head, and the relief "pithos" fragment (c. 590–570 BC) apparently shows a single lion-headed Cerberus, in Attic vase painting, Cerberus usually has two dog heads. In other art, as in the Laconian cup, Cerberus is usually three-headed. Occasionally, in Roman art, Cerberus is shown with a large central lion head, and two smaller dog heads on either side.
As in the Corinthian and Laconian cups (and possibly the relief "pithos" fragment), Cerberus is often depicted as part snake. In Attic vase painting, Cerberus is usually shown with a snake for a tail, or tail which ends in the head of a snake. Snakes are also often shown rising from various parts of his body, including snout, head, neck, back, ankles, and paws.
Two Attic amphoras from Vulci, one (c. 530–515 BC), by the Bucci Painter (Munich 1493), the other (c. 525–510 BC), by the Andokides painter (Louvre F204), in addition to the usual two heads and snake tail, show Cerberus with a mane down his necks and back, another typical Cerberian feature of Attic vase painting. Andokides' amphora also has a small snake curling up from each of Cerberus' two heads.
Besides this lion-like mane, and the occasional lion-head, mentioned above, Cerberus was sometimes shown with other leonine features. A pitcher (c. 530–500) shows Cerberus with mane and claws, while a first century BC sardonyx cameo, shows Cerberus with leonine body and paws. In addition, a limestone relief fragment from Taranto (c. 320–300 BC) shows Cerberus with three lion-like heads.
During the second quarter of the 5th century BC, the capture of Cerberus disappears from Attic vase painting. And after the early third century BC, the subject become rare everywhere, until the Roman period. In Roman art, the capture of Cerberus is usually shown together with other labors. Heracles and Cerberus are usually alone, with Heracles leading Cerberus.
Etymology.
The etymology of Cerberus' name is uncertain. Ogden refers to attempts to establish an Indo-European etymology as "not yet successful". It has been claimed to be related to the Sanskrit word सर्वरा "sarvarā", used as an epithet of one of the dogs of Yama, from a Proto-Indo-European word *"k̑érberos", meaning "spotted". Lincoln (1991), among others, critiques this etymology. Lincoln notes a similarity between Cerberus and the Norse mythological dog Garmr, relating both names to a Proto-Indo-European root "*ger-" "to growl" (perhaps with the suffixes "-*m/*b" and "-*r"). However, as Ogden observes, this analysis actually requires "Kerberos" and "Garmr" to be derived from two "different" Indo-European roots (*"ker-" and *"gher-" respectively), and so does not actually establish a relationship between the two names.
Though probably not Greek, Greek etymologies for Cerberus have been offered. An etymology given by Servius (the late fourth century commentator on Virgil)—but rejected by Ogden—derives Cerberus from the Greek word "creoboros" meaning "flesh-devouring". Another suggested etymology derives Cerberus from "Ker berethrou", meaning 'evil of the pit'.
Cerberus rationalized.
At least as early as the 6th century BC, some ancients writers have attempted to explain away various fantastical features of Greek mythology, included in these are various rationalized accounts of the Cerberus story. The earliest such account (late 6th century BC) is that of Hecataeus of Miletus. In his account Cerberus was not a dog at all, but rather simply a large poisonous snake, which lived on Tainaron. The serpent was called the "hound of Hades" only because anyone bitten by it died immediately, and it was this snake that Heracles brought to Eurystheus. The geographer Pausanias (who preserves for us Hecataeus' version of the story) points out that, since Homer does not describe Cerberus, Hecataeus' account does not necessarily conflict with Homer, since Homer's "Hound of Hades" may not in fact refer to an actual dog.
Other rationalized accounts make Cerberus out to be a normal dog. According to Palaephatus (4th century BC) Cerberus was one of the two dogs who guarded the cattle of Geryon, the other being Orthrus. Geryon lived in a city named Tricranium (in Greek "Tricarenia," 'Three-Heads'), from which name both Cerberus and Geryon came to be called "three-headed". Heracles killed Orthus, and drove away Geryon's cattle, with Cerberus following along behind. Molossus, a Mycenaen, offered to buy Cerberus from Eurystheus (presumably having received the dog, along with the cattle, from Heracles). But when Eurystheus refused, Molossus stole the dog and penned him up in a cave in Tainaron. Eurystheus commanded Heracles to find Cerberus and bring him back. After searching the entire Peloponnesus, Heracles found where it was said Cerberus was being held, went down into the cave, and brought up Cerberus, after which it was said: "Heracles descended through the cave into Hades and brought up Cerberus."
In the rationalized account of Philochorus, in which Heracles rescues Theseus, Perithous is eaten by Cerberus. In this version of the story, Aidoneus (i.e., "Hades") is the mortal king of the Molossians, with a wife named Persephone, a daughter named Kore (another name for the goddess Persephone) and a large mortal dog named Cerberus, with whom all suiters of his daughter were required to fight. After having stolen Helen, to be Theseus' wife, Theseus and Perithous, attempt to abduct Kore, for Perithous, but Aidoneus catches the two heroes, imprisons Theseus, and feeds Perithous to Cerberus. Later, while a guest of Aidoneus, Heracles asks Aidoneus to release Theseus, as a favor, which Aidoneus grants.
A 2nd century AD Greek known as Heraclitus the paradoxographer (not to be confused with the 5th century BC Greek philosopher Heraclitus) – claimed that Cerberus had two pups that were never away from their father, which made Cerberus appear to be three-headed.
Cerberus allegorized.
Servius, a medieval commentator on Virgils' "Aeneid", derived Cerberus' name from the Greek word "creoboros" meaning "flesh-devouring" (see above), and held that Cerberus symbolized the corpse-consuming earth, with Heracles' triumph over Cerberus representing his victory over earthly desires. Later the mythographer Fulgentius, allegorizes Cerberus' three heads as representing the three origins of human strife: "nature, cause, and accident", and (drawing on the same flesh-devouring etymology as Servius) as symbolizing "the three ages—infancy, youth, old age, at which death enters the world."
The later Vatican Mythographers repeat and expand upon the traditions of Servius and Fulgentius. All three Vatican Mythographers repeat Servius' derivation of Cerberus' name from "creoboros". The Second Vatican Mythographer repeats (nearly word for word) what Fulgentius had to say about Cerberus, while the Third Vatican Mythographer, in another very similar passage to Fugentius', says (more specifically than Fugentius), that for "the philosophers" Cerberus represented hatred, his three heads symbolizing the three kinds of human hatred: natural, causal, and casual (i.e. accidental).
The Second and Third Vatican Mythographers, note that the three brothers Zeus, Poseidon and Hades each have tripartite insignia, associating Hades' three headed Cerberus, with Zeus' three-forked thunderbolt, and Poseidon's three-pronged trident, while the Third Vatican Mythographer adds that "some philosophers think of Cerberus as the tripartite earth: Asia, Africa, and Europe. This earth, swallowing up bodies, sends souls to Tartarus."
Virgil described Cerberus as "ravenous" ("fame rabida"), and a rapacious Cerberus became proverbial. Thus Cerberus came to symbolize avarice, and so, for example, in Dante's "Inferno," Cerberus is placed in the Third Circle of Hell, guarding over the gluttons, where he "rends the spirits, flays and quarters them," and Dante (perhaps echoing Servius' association of Cerbeus with earth) has his guide Virgil take up handfuls of earth and throw them into Cerberus' "rapacious gullets."
Constellation.
In the constellation Cerberus introduced by Johannes Hevelius in 1687, Cerberus is drawn as a three-headed snake, held in Hercules' hand (previously these stars had been depicted as a branch of the tree on which grew the Apples of the Hesperides).

</doc>
<doc id="6698" url="https://en.wikipedia.org/wiki?curid=6698" title="CamelCase">
CamelCase

CamelCase (also camel caps or medial capitals) is the practice of writing compound words or phrases such that each word or abbreviation begins with a capital letter (and omits hyphens). Camel case may start with a capital letter (called PascalCase or UpperCamelCase) or, especially in programming languages, with a lowercase letter. Common examples include: "PowerPoint" or "MySpace" and "iPhone" or "eCommerce" or in online usernames such as "JohnSmith".
Variations and synonyms.
The first letter of a camel case compound word may or may not be capitalized, and there is no real consensus on whether the term "camel case" generally implies an uppercase or lowercase first letter. For clarity, this article calls the two alternatives upper camel case (starts with upper case letter) and lower camel case (starts with lower case letter). Some people and organizations use the term "camel case" only for lower camel case. Other synonyms include:
StudlyCaps encompasses all such variations, and more, including even random mixed capitalization, as in "MiXeD CaPitALiZaTioN" (typically a stereotyped allusion to online culture).
Camel case is also distinct from title case, which is traditionally used for book titles and headlines. Title case capitalizes most of the words yet retains the spaces between the words. Camel case is also distinct from Tall Man lettering, which uses capitals to emphasize the differences between similar-looking words.
Traditional use in natural language.
In word combinations.
The use of medial capitals as a convention in the regular spelling of everyday texts is rare, but is used in some languages as a solution to particular problems which arise when two words or segments are combined.
In Italian, pronouns can be suffixed to verbs, and since the honorific form of second-person pronouns is capitalized, this can produce a sentence like "non ho trovato il tempo di risponderLe" ("I haven't found time to answer you" - where "Le" means "you").
In German, the medial capital letter I, called "Binnen-I", is sometimes used in a word like "StudentInnen" ("students") to indicate that both "Studenten" ("male students") and "Studentinnen" ("female students") are intended simultaneously. However, mid-word capitalisation does not conform to German orthography. The previous example could be correctly written using parentheses as "Student(inn)en", analogous to "congress(wo)man" in English.
In Irish, they are used when an inflectional prefix is attached to a proper noun, for example ("in Galway"), from ("Galway"); ("the Scottish person"), from ("Scottish person"); and ("to Ireland"), from ("Ireland). In recent Scots Gaelic orthography, a hyphen has been inserted: .
This convention is also used by several Bantu languages (e.g., "kiSwahili" = "Swahili language", "isiZulu" = "Zulu language") and several indigenous languages of Mexico (e.g. Nahuatl, Totonacan, Mixe–Zoque, and some Oto-Manguean languages).
In English, medial capitals are usually only found in Scottish or Irish "Mac-" or "Mc-" names, where for example "MacDonald, McDonald," and "Macdonald" are common spelling variants of the same name, and in Anglo-Norman "Fitz-" names, where for example both "FitzGerald" and "Fitzgerald" are found.
In their English style guide "The King's English", first published in 1906, H. W. Fowler and F. G. Fowler suggested that medial capitals could be used in triple compound words where hyphens would cause ambiguity—the examples they give are "KingMark-like" (as against "King Mark-like") and "Anglo-SouthAmerican" (as against "Anglo-South American"). However, they described the system as "too hopelessly contrary to use at present."
In transliterations.
In the scholarly transliteration of languages written in other scripts, medial capitals are used in similar situations. For example, in transliterated Hebrew, "haIvri" means "the Hebrew person" and "biYerushalayim" means "in Jerusalem". In Tibetan proper names like "rLobsang", the "r" stands for a prefix glyph in the original script that functions as tone marker rather than a normal letter. Another example is "tsIurku", a Latin transcription of the Chechen term for the capping stone of the characteristic Medieval defensive towers of Chechenia and Ingushetia; the capital letter "I" here denoting a phoneme distinct from the one transcribed as "i".
In abbreviations and acronyms.
Medial capitals are traditionally used in abbreviations to reflect the capitalization that the words would have when written out in full, for example in the academic titles PhD or BSc. In German, the names to statutes are abbreviated using embedded capitals, e.g. StGB (Strafgesetzbuch) for criminal code, PatG (Patentgesetz) for Patent Act, or the very common GmbH (Gesellschaft mit beschränkter Haftung) for Company with Limited Liability. In this context, there can even be three or more "CamelCase" capitals, e.g. in TzBfG for Teilzeit- und Befristungsgesetz (Act on Part-Time and Limited Term Occupations). In French, camel case acronyms such as OuLiPo (1960) were favored for a time as alternatives to initialisms.
Camel case is often used to transliterate initialisms into alphabets where two letters may be required to represent a single character of the original alphabet, e.g., DShK from Cyrillic ДШК. 
History of modern technical use.
Chemical formulae.
The first systematic and widespread use of medial capitals for technical purposes was the notation for chemical formulae invented by the Swedish chemist Berzelius in 1813. To replace the multitude of naming and symbol conventions used by chemists until that time, he proposed to indicate each chemical element by a symbol of one or two letters, the first one being capitalized. The capitalization allowed formulae like 'NaCl' to be written without spaces and still be parsed without ambiguity.
Berzelius's system remains in use to this day, augmented with three-letter symbols like 'Uut' for unnamed elements and abbreviations for some common substituents (especially in the field of organic chemistry, for instance 'Et' for 'ethyl-'). This has been further extended to describe the amino acid sequences of proteins and other similar domains.
Early use in trademarks.
Since the early 20th century, medial capitals have occasionally been used for corporate names and product trademarks, such as
Computer programming.
In the 1970s and 1980s, medial capitals were adopted as a standard or alternative naming convention for multi-word identifiers in several programming languages. The origin of this convention has not yet been settled. However, a 1954 conference proceedings informally referred to IBM's Speedcoding system as "SpeedCo". Christopher Strachey's paper on GPM (1965), shows a program that includes some medial capital identifiers, including "NextCh" and "WriteSymbol". Charles Simonyi, who oversaw the creation of Microsoft's flagship Office suite of applications, invented and taught the use of Hungarian Notation, in which the lower case letter at the start of a variable name denotes its type.
Background: multi-word identifiers.
Computer programmers often need to write descriptive (hence multi-word) identifiers, like codice_1 or codice_2, in order to improve the readability of their code. However, most popular programming languages forbid the use of spaces inside identifiers, since they are interpreted as delimiters between tokens, and allowing spaces inside of identifiers would significantly complicate lexical analysis (breaking the source code into tokens). The alternative of writing the words together as in codice_3 or codice_4 is not satisfactory, since the word boundaries may be quite difficult to discern in the result or it may even be misleading (e.g. codice_4 is ambiguous as it could be intended to mean"chart-able"—able to be charted—or "char table"—a table of characters).
Some early programming languages, notably Lisp (1958) and COBOL (1959), addressed this problem by allowing a hyphen ("-") to be used between words of compound identifiers, as in "END-OF-FILE"—Lisp because it worked well with prefix notation; a Lisp parser would not treat a hyphen in the middle of a symbol as a subtraction operator; COBOL because its operators were English words. This convention remains in use in these languages, and is also common in program names entered at a command line, as in Unix.
However, this solution was not adequate for algebra-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an intuitively obvious subtraction operator, and did not wish to require spaces around the hyphen, so one could write codice_6 as codice_7. These early languages instead allowed identifiers to have spaces in them, determining the end of the identifier by context; this was abandoned in later languages due to the complexity it adds to tokenization. In addition, common punched card character sets of the time were uppercase only and lacked other special characters. Further, FORTRAN initially restricted identifiers to six characters or fewer at the time, preventing multi-word identifiers except those made of very short words.
It was only in the late 1960s that the widespread adoption of the ASCII character set made both lower case and the underscore character codice_8 universally available. Some languages, notably C, promptly adopted underscores as word separators; and underscore-separated compounds like codice_9 are still prevalent in C programs and libraries, as well as other languages such as Python. However, some languages and programmers chose to avoid underscores, among other reasons to prevent confusing them with whitespace, and adopted camel case instead. Two accounts are commonly given for the origin of this convention.
"Alto Keyboard" hypothesis.
Another account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key, and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable multiword names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for Upper- and lowerCamelCase that was strictly followed by the Mesa libraries and the Alto operating system.
The Smalltalk language, which was developed originally on the Alto and became quite popular in the early 1980s, may have been instrumental in spreading the style outside PARC. Camel case was also used by convention for many names in the PostScript page description language (invented by Adobe Systems founder and ex-PARC scientist John Warnock), as well as for the language itself. A further boost was provided by Niklaus Wirth (the inventor of Pascal) who acquired a taste for camel case during a sabbatical at PARC and used it in Modula, his next programming language.
Spread to mainstream usage.
Whatever its origins within the computing world, the practice spread in the 1980s and 1990s, when the advent of the personal computer exposed hacker culture to the world. Camel case then became fashionable for corporate trade names, initially in technical fields; mainstream usage was well established by 1990:
During the dot-com bubble of the late 1990s, the lowercase prefixes "e" (for "electronic") and "i" (for "Internet", "information", "intelligent", etc.) became quite common, giving rise to names like Apple's iMac and the eBox software platform.
In 1998, Dave Yost suggested that chemists use medial capitals to aid readability of long chemical names, e.g. write AmidoPhosphoRibosylTransferase instead of amidophosphoribosyltransferase. This usage was still rare in 2012.
The practice is sometimes used for abbreviated names of certain neighborhoods, e.g. New York City neighborhoods "SoHo" ("So"uth of "Ho"uston Street) and "TriBeCa" ("Tri"angle "Be"low "Ca"nal Street) and San Francisco's "SoMa" ("So"uth of "Ma"rket). Such usages erode quickly, so the neighborhoods are now typically rendered as "Soho", "Tribeca", and "Soma".
Internal capitalization has also been used for other technical codes like HeLa (1983).
History of the name "camel case".
The original name of the practice, used in media studies, grammars and the "Oxford English Dictionary", was "medial capitals". Other names such as "InterCaps", "CamelCase" and variations thereof are relatively recent and seem more common in computer-related communities.
The earliest known occurrence of the term "InterCaps" on Usenet is in an April 1990 post to the group alt.folklore.computers by Avi Rappoport, with "BiCapitalization" appearing slightly later in a 1991 post by Eric S. Raymond to the same group. The earliest use of the name "CamelCase" occurs in 1995, in a post by Newton Love. ""With the advent of programming languages having these sorts of constructs, the humpiness of the style made me call it HumpyCase at first, before I settled on CamelCase. I had been calling it CamelCase for years,"" said Love, ""The citation above was just the first time I had used the name on USENET.""
The name "CamelCase" is not related to the "Camel Book" ("Programming Perl"), which uses all-lowercase identifiers with underscores in its sample code. However, in Perl programming camel case is also commonly used.
Current usage in computing.
Programming and coding.
The use of medial caps for compound identifiers is recommended by the coding style guidelines of many organizations or software projects. For some languages (such as Mesa, Pascal, Modula, Java and Microsoft's .NET) this practice is recommended by the language developers or by authoritative manuals and has therefore become part of the language's "culture".
Style guidelines often distinguish between upper and lower camel case, typically specifying which variety should be used for specific kinds of entities: variables, record fields, methods, procedures, types, etc. These rules are sometimes supported by static analysis tools that check source code for adherence.
The original Hungarian notation for programming, for example, specifies that a lowercase abbreviation for the "usage type" (not data type) should prefix all variable names, with the remainder of the name in upper camel case; as such it is a form of lower camel case.
Programming identifiers often need to contain acronyms and initialisms that are already in upper case, such as "old HTML file". By analogy with the title case rules, the natural camel case rendering would have the abbreviation all in upper case, namely "oldHTMLFile". However, this approach is problematic when two acronyms occur together (e.g., "parse DBM XML" would become "parseDBMXML") or when the standard mandates lower camel case but the name begins with an abbreviation (e.g. "SQL server" would become "sQLServer"). For this reason, some programmers prefer to treat abbreviations as if they were lower case words and write "oldHtmlFile", "parseDbmXml" or "sqlServer".
Wiki link markup.
Camel case is used in some wiki markup languages for terms that should be automatically linked to other wiki pages. This convention was originally used in Ward Cunningham's original wiki software, WikiWikiWeb, and can be activated in most other wikis. Some wiki engines such as TiddlyWiki, Trac and PMWiki make use of it in the default settings, but usually also provide a configuration mechanism or plugin to disable it. Wikipedia formerly used camel case linking as well, but switched to explicit link markup using square brackets and many other wiki sites have done the same. Some wikis that do not use camel case linking may still use the camel case as a naming convention, such as AboutUs.
Other uses.
The NIEM registry requires that XML data elements use upper camel case and XML attributes use lower camel case.
Most popular command-line interfaces and scripting languages cannot easily handle file names that contain embedded spaces (usually requiring the name to be put in quotes). Therefore, users of those systems often resort to camel case (or underscores, hyphens and other "safe" characters) for compound file names like MyJobResume.pdf.
Microblogging and social networking sites that limit the number of characters in a message (most famously Twitter, where the 140-character limit can be quite restrictive in languages that rely on alphabets, including English) are potential outlets for medial capitals. Using CamelCase between words reduces the number of spaces, and thus the number of characters, in a given message, allowing more content to fit into the limited space. Hashtags, especially long ones, often use CamelCase to maintain readability (e.g. #CollegeStudentProblems is easier to read than #collegestudentproblems).
In website URLs, spaces are percent-encoded as "%20", making the address longer and less human readable. By omitting spaces, CamelCase does not have this problem.
Criticism.
CamelCase has been criticised as negatively impacting readability due to the removing of spaces and upcasing of every word. One natural language study found that replacing spaces between words with letters or digits made it harder to recognise individual words, which resulted in increased reading times.
A 2009 paper describes a study that specifically compared snake case and CamelCase and found that camel case identifiers could be recognised with higher accuracy among both programmers and non-programmers, and that programmers already trained in CamelCase were able to recognise CamelCase identifiers faster than underscored identifiers.
A 2010 follow-up study, in the same conditions but using an improved measurement method, with use of eye-tracking equipment, indicates: ""While results indicate no difference in accuracy between the two styles, subjects recognize identifiers in the underscore style more quickly.""
Use of CamelCase can conflict with the regular use of uppercase letters for all caps acronyms e.g. to represent a concept like "the TCP IP socket ID" the writer must choose to either retain the capitalisation of the acronyms ("TCPIPSocketID"), which harms readability, or to retain capitalisation of only the first letter ("TcpIpSocketId"), which makes it harder to recognise that a given word is intended as an acronym. An alternative is to follow any instance of acronymic capitalization with a re-initialization of lower case camel, as TCPIPsocketID. This has the effect of enforcing the lower camel case standard.

</doc>
<doc id="6700" url="https://en.wikipedia.org/wiki?curid=6700" title="Cereal">
Cereal

A cereal is any grass cultivated for the edible components of its grain (botanically, a type of fruit called a caryopsis), composed of the endosperm, germ, and bran. Cereal grains are grown in greater quantities and provide more food energy worldwide than any other type of crop; they are therefore staple crops. Some plants often referred to as cereals, like buckwheat and quinoa, are considered instead pseudocereals, since they are not grasses.
In their natural form (as in "whole grain"), they are a rich source of vitamins, minerals, carbohydrates, fats, oils, and protein. When refined by the removal of the bran and germ, the remaining endosperm is mostly carbohydrate. In some developing nations, grain in the form of rice, wheat, millet, or maize constitutes a majority of daily sustenance. In developed nations, cereal consumption is moderate and varied but still substantial.
The word "cereal" derives from "Ceres", the name of the Roman goddess of harvest and agriculture.
History.
The first cereal grains were domesticated by early primitive humans. About 8,000 years ago, they were domesticated by ancient farming communities in the Fertile Crescent region. Emmer wheat, einkorn wheat, and barley were three of the so-called Neolithic founder crops in the development of agriculture. Around the same time, millets and rices were starting to become domesticated in east Asia. Sorghum and millets were also being domesticated in sub-Saharan West Africa.
Production.
The following table shows the annual production of cereals in 1961, 2010, 2011, 2012, and 2013 ranked by 2013 production.
Maize, wheat, and rice together accounted for 89% of all cereal production worldwide in 2012, and 43% of all food calories in 2009, while the production of oats and triticale have drastically fallen from their 1960s levels.
Other grains that are important in some places, but that have little production globally (and are not included in FAO statistics), include:
Several other species of wheat have also been domesticated, some very early in the history of agriculture:
In 2013 global cereal production reached a record 2,521 million tonnes. A slight dip to 2,498 million tonnes was forecast for 2014 by the FAO in July 2014.
Farming.
While each individual species has its own peculiarities, the cultivation of all cereal crops is similar. Most are annual plants; consequently one planting yields one harvest. Wheat, rye, triticale, oats, barley, and spelt are the "cool-season" cereals. These are hardy plants that grow well in moderate weather and cease to grow in hot weather (approximately 30 °C, but this varies by species and variety). The "warm-season" cereals are tender and prefer hot weather. Barley and rye are the hardiest cereals, able to overwinter in the subarctic and Siberia. Many cool-season cereals are grown in the tropics. However, some are only grown in cooler highlands, where it may be possible to grow multiple crops in a year.
For a few decades, there has also been increasing interest in perennial grain plants. This interest developed due to advantages in erosion control, reduced need of fertiliser, and potential lowered costs to the farmer. Though research is still in early stages, The Land Institute in Salina, Kansas has been able to create a few cultivars that produce a fairly good crop yield.
Planting.
The warm-season cereals are grown in tropical lowlands year-round and in temperate climates during the frost-free season. Rice is commonly grown in flooded fields, though some strains are grown on dry land. Other warm climate cereals, such as sorghum, are adapted to arid conditions.
Cool-season cereals are well-adapted to temperate climates. Most varieties of a particular species are either winter or spring types. Winter varieties are sown in the autumn, germinate and grow vegetatively, then become dormant during winter. They resume growing in the springtime and mature in late spring or early summer. This cultivation system makes optimal use of water and frees the land for another crop early in the growing season.
Winter varieties do not flower until springtime because they require vernalization: exposure to low temperatures for a genetically determined length of time. Where winters are too warm for vernalization or exceed the hardiness of the crop (which varies by species and variety), farmers grow spring varieties. Spring cereals are planted in early springtime and mature later that same summer, without vernalization. Spring cereals typically require more irrigation and yield less than winter cereals.
Period.
Once the cereal plants have grown their seeds, they have completed their life cycle. The plants die and become brown and dry. As soon as the parent plants and their seed kernels are reasonably dry, harvest can begin.
In developed countries, cereal crops are universally machine-harvested, typically using a combine harvester, which cuts, threshes, and winnows the grain during a single pass across the field. In developing countries, a variety of harvesting methods are in use, depending on the cost of labor, from combines to hand tools such as the scythe or cradle.
If a crop is harvested during wet weather, the grain may not dry adequately in the field to prevent spoilage during its storage. In this case, the grain is sent to a dehydrating facility, where artificial heat dries it.
In North America, farmers commonly deliver their newly harvested grain to a grain elevator, a large storage facility that consolidates the crops of many farmers. The farmer may sell the grain at the time of delivery or maintain ownership of a share of grain in the pool for later sale. Storage facilities should be protected from small grain pests, rodents and birds.
Nutritional facts.
Some grains are deficient in the essential amino acid lysine. That is why many vegetarian cultures, in order to get a balanced diet, combine their diet of grains with legumes. Many legumes, on the other hand, are deficient in the essential amino acid methionine, which grains contain. Thus, a combination of legumes with grains forms a well-balanced diet for vegetarians. Common examples of such combinations are dal (lentils) with rice by South Indians and Bengalis, dal with wheat in Pakistan and North India, and beans with corn tortillas, tofu with rice, and peanut butter with wheat bread (as sandwiches) in several other cultures, including Americans. The amount of crude protein found in grain is measured as the grain crude protein concentration.
Standardization.
The ISO has published a series of standards regarding cereal products which are covered by ICS 67.060.

</doc>
<doc id="6704" url="https://en.wikipedia.org/wiki?curid=6704" title="Christendom">
Christendom

Christendom has several meanings. In a cultural sense, it refers to the religion itself, or to the worldwide community of Christians, adherents of Christianity. In its historical sense, the term usually refers to the medieval and early modern period, during which the Christian world represented a geopolitical power juxtaposed with both the pagan and especially the Muslim world. In the traditional Roman Catholic sense of the word, it refers to the sum total of nations in which the Catholic Church is the established religion of the state, or which have ecclesiastical concordats with the Holy See.
In a contemporary sense, it may simply refer collectively to Christian majority countries or countries in which Christianity dominates or nations in which Christianity is the established religion.
Terminology and usage.
The term "cristendom" existed in Old English, but it had the sense now taken by "Christianity" (as is still the case with the cognate Dutch "christendom" , where it denotes mostly the religion itself, just like the German "Christentum"). 
The current sense of the word of "lands where Christianity is the dominant religion" emerges in Late Middle English (by c. 1400). This semantic development happened independently in the languages of late medieval Europe, which leads to the confusing semantics of English "Christendom" equalling German "Christenheit", French "chrétienté" vs. English "Christianity" equalling German "Christentum", French "christianisme". The reason is the increasing fragmentation of Western Christianity at that time both in theological and in political respect. 
"Christendom" as a geopolitical term is thus meaningful in the context of the Middle Ages, and arguably during the European wars of religion and the Ottoman wars in Europe.
The "Christian world" is also known collectively as the Corpus Christianum, a translated as "the Christian body", meaning the community of all Christians. The Christian polity, embodying a less secular meaning, can be compatible with the idea of both a religious and a temporal body: "Corpus Christianum". The "Corpus Christianum" can be seen as a Christian equivalent of the Muslim "Ummah". 
The word "Christendom" is also used with its other meaning to frame-true Christianity. A more secular meaning can denote that the term "Christendom" refers to Christians considered as a group, the "Political Christian World", as an informal cultural hegemony that Christianity has traditionally enjoyed in the West. 
In its most broad term, it refers to the world's Christian majority countries, which, share little in common aside from the predominance of the faith. Unlike the Muslim world, which has a geo-political and cultural definition that provides a primary identifier for a large swath of the world, Christendom is more complex.
It may be a cultural notion, but has very little weight in international discourse; very few political observers really discuss Christendom, while the Muslim World tends to comprise a civilization in itself.
For example, the Americas and Europe are considered part of Christendom, but this region is further subdivided into the West (representing the North Atlantic) and Latin America. It is also less geographically cohesive than the Muslim world, which stretches almost continuously from North Africa to South Asia.
There is a common and nonliteral sense of the word that is much like the terms "Western world", "known world" or "Free World". When Thomas F. Connolly said, "There isn't enough power in all Christendom to make that airplane what we want!", he was simply using a figure of speech, although it is true that during the Cold War, just as the totalitarianism of the Communist Bloc presented a contrast with the liberty of the Free World, the state atheism of the Communist Bloc contrasted with religious freedom and powerful religious institutions in North America and Western Europe.
History.
Early Christendom.
In the beginning of Christendom, early Christianity was a religion spread in the Greek/Roman world and beyond as a 1st-century Jewish sect, which historians refer to as Jewish Christianity. It may be divided into two distinct phases: the apostolic period, when the first apostles were alive and organizing the Church, and the post-apostolic period, when an early episcopal structure developed, whereby bishoprics were governed by bishops (overseers).
The post-apostolic period concerns the time roughly after the death of the apostles when bishops emerged as overseers of urban Christian populations. The earliest recorded use of the terms "Christianity" (Greek ) and "Catholic" (Greek ), dates to this period, the 2nd century, attributed to Ignatius of Antioch "c." 107. Early Christendom would close at the end of imperial persecution of Christians after the ascension of Constantine the Great and the Edict of Milan in AD 313 and the First Council of Nicaea in 325.
Late Antiquity and Early Middle Ages.
"Christendom" has referred to the medieval and renaissance notion of the "Christian world" as a sociopolitical polity. In essence, the earliest vision of Christendom was a vision of a Christian theocracy, a government founded upon and upholding Christian values, whose institutions are spread through and over with Christian doctrine. In this period, members of the Christian clergy wield political authority. The specific relationship between the political leaders and the clergy varied but, in theory, the national and political divisions were at times subsumed under the leadership of the church as an institution. This model of church-state relations was accepted by various Church leaders and political leaders in European history.
The Church gradually became a defining institution of the Empire. Emperor Constantine issued the Edict of Milan in 313 proclaiming toleration for the Christian religion, and convoked the First Council of Nicaea in 325 whose Nicene Creed included belief in "one holy catholic and apostolic Church". Emperor Theodosius I made Nicene Christianity the state church of the Roman Empire with the Edict of Thessalonica of 380.
As the Western Roman Empire disintegrated into feudal kingdoms and principalities, the concept of Christendom changed as the western church became one of five patriarchal of the Pentarchy and the Christians of the Eastern Roman Empire developed. The Byzantine Empire was the last bastion of Christendom. Christendom would take a turn with the rise of the Franks, a Germanic tribe who converted to the Christian faith and entered into communion with Rome.
On Christmas Day 800 AD, Pope Leo III crowned Charlemagne resulting in the creation of another Christian king beside the christian emperor in the Byzantine state. The Carolingian Empire created a definition of "Christendom" in juxtaposition with the Byzantine Empire, that of a distributed versus centralized culture respectively.
The classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. In the Greek philosopher Plato's ideal state there are three major classes, which was representative of the idea of the “tripartite soul”, which is expressive of three functions or capacities of the human soul: “reason”, “the spirited element”, and “appetites” (or “passions”). Will Durant made a convincing case that certain prominent features of Plato's ideal community where discernible in the organization, dogma and effectiveness of "the" Medieval Church in Europe:
"... For a thousand years Europe was ruled by an order of guardians considerably like that which was visioned by our philosopher. During the Middle Ages it was customary to classify the population of Christendom into laboratores (workers), bellatores (soldiers), and oratores (clergy). The last group, though small in number, monopolized the instruments and opportunities of culture, and ruled with almost unlimited sway half of the most powerful continent on the globe. The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled AD onwards, the clergy were as free from family cares as even Plato could desire such guardians... Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them..." "In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire".
Later Middle Ages and Renaissance.
After the collapse of Charlemagne's empire, the southern remnants of the Holy Roman Empire became a collection of states loosely connected to the Holy See of Rome. Tensions between Pope Innocent III and secular rulers ran high, as the pontiff exerted control over their temporal counterparts in the west and vice versa. The pontificate of Innocent III is considered the height of temporal power of the papacy. The "Corpus Christianum" described the then current notion of the community of all Christians united under the Roman Catholic Church. The community was to be guided by Christian values in its politics, economics and social life. Its legal basis was the "corpus iuris canonica" (body of canon law).
In the East, Christendom became more defined as the Byzantine Empire's gradual loss of territory to an expanding Islam and the muslim conquest of Persia. This caused Christianity to become important to the Byzantine identity. Before the East–West Schism which divided the Church religiously, there had been the notion of a "universal Christendom" that included the East and the West. After the East–West Schism, hopes of regaining religious unity with the West were ended by the Fourth Crusade, when Crusaders conquered the Byzantine capital of Constantinople and hastened the decline of the Byzantine Empire on the path to its destruction. With the breakup of the Byzantine Empire into individual nations with nationalist Orthodox Churches, the term Christendom described Western Europe, Catholicism, Orthodox Byzantines, and other Eastern rites of the Church.
The Catholic Church's peak of authority over all European Christians and their common endeavours of the Christian community — for example, the Crusades, the fight against the Moors in the Iberian Peninsula and against the Ottomans in the Balkans — helped to develop a sense of communal identity against the obstacle of Europe's deep political divisions. But this authority was also sometimes abused, and fostered the Inquisition and anti-Jewish pogroms, to root out divergent elements and create a religiously uniform community. Ultimately, the Inquisition was done away with by order of Pope Innocent III.
Christendom ultimately was led into specific crisis in the late Middle Ages, when the kings of France managed to establish a French national church during the 14th century and the papacy became ever more aligned with the Holy Roman Empire of the German Nation. Known as the Western Schism, western Christendom was a split between three men, who were driven by politics rather than any real theological disagreement for simultaneously claiming to be the true pope. The Avignon Papacy developed a reputation of corruption that estranged major parts of Western Christendom. The Avignon schism was ended by the Council of Constance.
Before the modern period, Christendom was in a general crisis at the time of the Renaissance Popes because of the moral laxity of these pontiffs and their willingness to seek and rely on temporal power as secular rulers did. Many in the Catholic Church's hierarchy in the Renaissance became increasingly entangled with insatiable greed for material wealth and temporal power, which led to many reform movements, some merely wanting a moral reformation of the Church's clergy, while others repudiated the Church and separated from it in order to form new sects. The Italian Renaissance produced ideas or institutions by which men living in society could be held together in harmony. In the early 16th century, Baldassare Castiglione (The Book of the Courtier) laid out his vision of the ideal gentleman and lady, while Machiavelli cast a jaundiced eye on "la verità effetuale delle cose" — the actual truth of things — in The Prince, composed, humanist style, chiefly of parallel ancient and modern examples of Virtù. Some Protestant movements grew up along lines of mysticism or renaissance humanism (cf. Erasmus). The Catholic Church fell partly into general neglect under the Renaissance Popes, whose inability to govern the Church by showing personal example of high moral standards set the climate for what would ultimately become the Protestant Reformation. During the Renaissance the papacy was mainly run by the wealthy families and also had strong secular interests. To safeguard Rome and the connected Papal States the popes became necessarily involved in temporal matters, even leading armies, as the great patron of arts Pope Julius II did. It during these intermediate times popes strove to make Rome the capital of Christendom while projecting it, through art, architecture, and literature, as the center of a Golden Age of unity, order, and peace.
Professor Frederick J. McGinness described Rome as essential in understanding the legacy the Church and its representatives encapsulated best by The Eternal City: "No other city in Europe matches Rome in its traditions, history, legacies, and influence in the Western world. Rome in the Renaissance under the papacy not only acted as guardian and transmitter of these elements stemming from the Roman Empire but also assumed the role as artificer and interpreter of its myths and meanings for the peoples of Europe from the Middle Ages to modern times... Under the patronage of the popes, whose wealth and income were exceeded only by their ambitions, the city became a cultural center for master architects, sculptors, musicians, painters, and artisans of every kind...In its myth and message, Rome had become the sacred city of the popes, the prime symbol of a triumphant Catholicism, the center of orthodox Christianity, a new Jerusalem."
It is clearly noticeable that the popes of the Italian Renaissance have been subjected by many writers with an overly harsh tone. Pope Julius II for example was not only an effective secular leader in military affairs, a deviously effective politician but foremost one of the greatest patron of the Renaissance period and person who also encouraged open criticism from noted humanists.
The blossoming of renaissance humanism was made very much possible due to the universality of the institutions of Catholic Church and represented by personalities such as Pope Pius II, Nicolaus Copernicus, Leon Battista Alberti, Desiderius Erasmus, sir Thomas More, Bartolomé de Las Casas, Leonardo da Vinci and Teresa of Ávila. George Santayana in his work "The Life of Reason" postulated the tenets of the all encompassing order the Church had brought and as the repository of the legacy of classical antiquity:
"The enterprise of individuals or of small aristocratic bodies has meantime sown the world which we call civilised with some seeds and nuclei of order. There are scattered about a variety of churches, industries, academies, and governments. But the universal order once dreamt of and nominally almost established, the empire of universal peace, all-permeating rational art, and philosophical worship, is mentioned no more. An unformulated conception, the prerational ethics of private privilege and national unity, fills the background of men's minds. It represents feudal traditions rather than the tendency really involved in contemporary industry, science, or philanthropy. Those dark ages, from which our political practice is derived, had a political theory which we should do well to study; for their theory about a universal empire and a catholic church was in turn the echo of a former age of reason, when a few men conscious of ruling the world had for a moment sought to survey it as a whole and to rule it justly."
Reformation and Early Modern era.
Developments in western philosophy and European events brought change to the notion of the "Corpus Christianum". The Hundred Years' War accelerated the process of transforming France from a feudal monarchy to a centralized state. The rise of strong, centralized monarchies denoted the European transition from feudalism to capitalism. By the end of the Hundred Years' War, both France and England were able to raise enough money through taxation to create independent standing armies. In the Wars of the Roses, Henry Tudor took the crown of England. His heir, the absolute king Henry VIII establishing the English church.
In modern history, the Reformation and rise of modernity in the early 16th century entailed a change in the "Corpus Christianum". In the Holy Roman Empire, the Peace of Augsburg of 1555 officially ended the idea among secular leaders that all Christians must be united under one church. The principle of "cuius regio, eius religio" ("whose the region is, his religion") established the religious, political and geographic divisions of Christianity, and this was established with the Treaty of Westphalia in 1648, which legally ended the concept of a single Christian hegemony in the territories of the Holy Roman Empire, despite the Catholic Church's doctrine that it alone is the one true Church founded by Christ.
Subsequently, each government determined the religion of their own state. Christians living in states where their denomination was "not" the established one were guaranteed the right to practice their faith in public during allotted hours and in private at their will.
The European wars of religion are usually taken to have ended with the Treaty of Westphalia (1648), or arguably, including the Nine Years' War and the War of the Spanish Succession in this period, with the Treaty of Utrecht of 1713.
In the 18th century, the focus shifts away from religious conflicts, either between Christian factions or against the external threat of Islam. 
The European Miracle, the Age of Enlightenment and the formation of the great Colonial empire together with the beginning decline of the Ottoman Empire mark the end of the geopolitical "history of Christendom". Instead the focus of western history shifts to the development of the nation-state, accompanied by increasing atheism and secularism, culminating with the French Revolution and the Napoleonic Wars at the turn of the 19th century.
Classical culture.
Art and literature.
Writings and poetry.
Christian literature is writing that deals with Christian themes and incorporates the Christian world view. This constitutes a huge body of extremely varied writing. Christian poetry is any poetry that contains Christian teachings, themes, or references. The influence of Christianity on poetry has been great in any area that Christianity has taken hold. Christian poems often directly reference the Bible, while others provide allegory.
Supplemental arts.
Christian art is art produced in an attempt to illustrate, supplement and portray in tangible form the principles of Christianity. Virtually all Christian groupings use or have used art to some extent. The prominence of art and the media, style, and representations change; however, the unifying theme is ultimately the representation of the life and times of Jesus and in some cases the Old Testament. Depictions of saints are also common, especially in Anglicanism, Roman Catholicism, and Eastern Orthodoxy.
Illumination.
An illuminated manuscript is a manuscript in which the text is supplemented by the addition of decoration. The earliest surviving substantive illuminated manuscripts are from the period AD 400 to 600, primarily produced in Ireland, Constantinople and Italy. The majority of surviving manuscripts are from the Middle Ages, although many illuminated manuscripts survive from the 15th century Renaissance, along with a very limited number from Late Antiquity.
Most illuminated manuscripts were created as codices, which had superseded scrolls; some isolated single sheets survive. A very few illuminated manuscript fragments survive on papyrus. Most medieval manuscripts, illuminated or not, were written on parchment (most commonly of calf, sheep, or goat skin), but most manuscripts important enough to illuminate were written on the best quality of parchment, called vellum, traditionally made of unsplit calfskin, though high quality parchment from other skins was also called "parchment".
Iconography.
Christian art began, about two centuries after Christ, by borrowing motifs from Roman Imperial imagery, classical Greek and Roman religion and popular art. Religious images are used to some extent by the Abrahamic Christian faith, and often contain highly complex iconography, which reflects centuries of accumulated tradition. In the Late Antique period iconography began to be standardised, and to relate more closely to Biblical texts, although many gaps in the canonical Gospel narratives were plugged with matter from the apocryphal gospels. Eventually the Church would succeed in weeding most of these out, but some remain, like the ox and ass in the Nativity of Christ.
An icon is a religious work of art, most commonly a painting, from Eastern Christianity. Christianity has used symbolism from its very beginnings. In both East and West, numerous iconic types of Christ, Mary and saints and other subjects were developed; the number of named types of icons of Mary, with or without the infant Christ, was especially large in the East, whereas Christ Pantocrator was much the commonest image of Christ.
Christian symbolism invests objects or actions with an inner meaning expressing Christian ideas. Christianity has borrowed from the common stock of significant symbols known to most periods and to all regions of the world. Religious symbolism is effective when it appeals to both the intellect and the emotions. Especially important depictions of Mary include the Hodegetria and Panagia types. Traditional models evolved for narrative paintings, including large cycles covering the events of the Life of Christ, the Life of the Virgin, parts of the Old Testament, and, increasingly, the lives of popular saints. Especially in the West, a system of attributes developed for identifying individual figures of saints by a standard appearance and symbolic objects held by them; in the East they were more likely to identified by text labels.
Each saint has a story and a reason why he or she led an exemplary life. Symbols have been used to tell these stories throughout the history of the Church. A number of Christian saints are traditionally represented by a symbol or iconic motif associated with their life, termed an attribute or emblem, in order to identify them. The study of these forms part of iconography in Art history. They were particularly
Architecture.
Christian architecture encompasses a wide range of both secular and religious styles from the foundation of Christianity to the present day, influencing the design and construction of buildings and structures in Christian culture.
Buildings were at first adapted from those originally intended for other purposes but, with the rise of distinctively ecclesiastical architecture, church buildings came to influence secular ones which have often imitated religious architecture. In the 20th century, the use of new materials, such as concrete, as well as simpler styles has had its effect upon the design of churches and arguably the flow of influence has been reversed. From the birth of Christianity to the present, the most significant period of transformation for Christian architecture in the west was the Gothic cathedral. In the east, Byzantine architecture was a continuation of Roman architecture.
Philosophy.
Christian philosophy is a term to describe the fusion of various fields of philosophy with the theological doctrines of Christianity. Scholasticism, which means "that belongs to the school", and was a method of learning taught by the academics (or "school people") of medieval universities c. 1100–1500. Scholasticism originally started to reconcile the philosophy of the ancient classical philosophers with medieval Christian theology. Scholasticism is not a philosophy or theology in itself but a tool and method for learning which places emphasis on dialectical reasoning.
Christian civilization.
Medieval conditions.
The Byzantine Empire, which was the most sophisticated culture during antiquity, suffered under Muslim conquests limiting its scientific prowess during the Medieval period. Christian Western Europe had suffered a catastrophic loss of knowledge following the fall of the Western Roman Empire. But thanks to the Church scholars such as Aquinas and Buridan, the West carried on at least the spirit of scientific inquiry which would later lead to Europe's taking the lead in science during the Scientific Revolution using translations of medieval works.
Medieval technology refers to the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth. The period saw major technological advances, including the adoption of gunpowder and the astrolabe, the invention of spectacles, and greatly improved water mills, building techniques, agriculture in general, clocks, and ships. The latter advances made possible the dawn of the Age of Exploration. The development of water mills was impressive, and extended from agriculture to sawmills both for timber and stone, probably derived from Roman technology. By the time of the Domesday Book, most large villages in Britain had mills. They also were widely used in mining, as described by Georg Agricola in De Re Metallica for raising ore from shafts, crushing ore, and even powering bellows.
Significant in this respect were advances within the fields of navigation. The compass and astrolabe along with advances in shipbuilding, enabled the navigation of the World Oceans and thus domination of the worlds economic trade. Gutenberg’s printing press made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience.
Renaissance innovations.
During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, math, manufacturing, and engineering. The rediscovery of ancient scientific texts was accelerated after the Fall of Constantinople, and the invention of printing which would democratize learning and allow a faster propagation of new ideas. "Renaissance technology" is the set of artifacts and customs, spanning roughly the 14th through the 16th century. The era is marked by such profound technical advancements like the printing press, linear perspectivity, patent law, double shell domes or Bastion fortresses. Draw-books of the Renaissance artist-engineers such as Taccola and Leonardo da Vinci give a deep insight into the mechanical technology then known and applied.
Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement. The "Scientific Renaissance" was the early phase of the Scientific Revolution. In the two-phase model of early modern science: a "Scientific Renaissance" of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a "Scientific Revolution" of the 17th century, when scientists shifted from recovery to innovation.
Demographics.
Geographic spread.
Christianity is the predominant religion in Europe, Russia, the Americas, Oceania, the Philippines, Eastern Indonesia, Southern Africa, Central Africa and East Africa. There are also large Christian communities in other parts of the world, such as China, India and Central Asia, where Christianity is the second-largest religion after Islam. The United States is the largest Christian country in the world by population, followed by Brazil and Mexico.
Many Christians not only live under, but also have an official status in, a state religion of the following nations: Armenia (Armenian Apostolic Church), Costa Rica (Roman Catholic Church), Denmark (Church of Denmark), El Salvador (Roman Catholic Church), England (Church of England), Georgia (Georgian Orthodox church), Greece (Church of Greece), Iceland (Church of Iceland), Liechtenstein (Roman Catholic Church), Malta (Roman Catholic Church), Monaco (Roman Catholic Church), Romania (Romanian Orthodox Church), Norway (Church of Norway), Vatican City (Roman Catholic Church), Switzerland (Roman Catholic Church, Swiss Reformed Church and Christian Catholic Church of Switzerland).
Number of adherents.
The estimated number of Christians in the world ranges from 1.5 billion to 2.2 billion people. Composed of around 34,000 different denominations, Christianity is the world's largest religion. Christians have composed about 33 percent of the world's population for around 100 years.
Notable Christian organizations.
A religious order is a lineage of communities and organizations of people who live in some way set apart from society in accordance with their specific religious devotion, usually characterized by the principles of its founder's religious practice. In contrast, the term Holy Orders is used by many Christian churches to refer to ordination or to a group of individuals who are set apart for a special role or ministry. Historically, the word "order" designated an established civil body or corporation with a hierarchy, and ordinatio meant legal incorporation into an ordo. The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Religious orders are composed of initiates (laity) and, in some traditions, ordained clergies.
Various organizations include:
Christianity law and ethics.
Church and state framing.
Within the framework of Christianity, there are at least three possible definitions for Church law. One is the Torah/Mosaic Law (from what Christians consider to be the Old Testament) also called Divine Law or Biblical law. Another is the instructions of Jesus of Nazareth in the Gospel (sometimes referred to as the Law of Christ or the New Commandment or the New Covenant). A third is canon law which is the internal ecclesiastical law governing the Roman Catholic Church, the Eastern Orthodox churches, and the Anglican Communion of churches. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was initially a rule adopted by a council (From Greek "kanon" / κανών, Hebrew kaneh / קנה, for rule, standard, or measure); these canons formed the foundation of canon law.
Christian ethics in general has tended to stress the need for grace, mercy, and forgiveness because of human weakness and developed while Early Christians were subjects of the Roman Empire. From the time Nero blamed Christians for setting Rome ablaze (64 AD) until Galarius (311 AD), persecutions against Christians erupted periodically. Consequently, Early Christian ethics included discussions of how believers should relate to Roman authority and to the empire.
Under the Emperor Constantine I (312-337), Christianity became a legal religion. While some scholars debate whether Constantine's conversion to Christianity was authentic or simply matter of political expediency, Constantine's decree made the empire safe for Christian practice and belief. Consequently, issues of Christian doctrine, ethics and church practice were debated openly, see for example the First Council of Nicaea and the First seven Ecumenical Councils. By the time of Theodosius I (379-395), Christianity had become the state religion of the empire. With Christianity in power, ethical concerns broaden and included discussions of the proper role of the state.
Render unto Caesar… is the beginning of a phrase attributed to Jesus in the synoptic gospels which reads in full, ""Render unto Caesar the things which are Caesar’s, and unto God the things that are God’s"". This phrase has become a widely quoted summary of the relationship between Christianity and secular authority. The gospels say that when Jesus gave his response, his interrogators "marvelled, and left him, and went their way." Time has not resolved an ambiguity in this phrase, and people continue to interpret this passage to support various positions that are poles apart. The traditional division, carefully determined, in Christian thought is the state and church have separate spheres of influence.
Thomas Aquinas thoroughly discussed that "human law" is positive law which means that it is natural law applied by governments to societies. All human laws were to be judged by their conformity to the natural law. An unjust law was in a sense no law at all. At this point, the natural law was not only used to pass judgment on the moral worth of various laws, but also to determine what the law said in the first place. This could result in some tension. Late ecclesiastical writers followed in his footsteps.
Democratic ideology.
Christian democracy is a political ideology that seeks to apply Christian principles to public policy. It emerged in 19th-century Europe, largely under the influence of Catholic social teaching. In a number of countries, the democracy's Christian ethos has been diluted by secularisation. In practice, Christian democracy is often considered conservative on cultural, social and moral issues and progressive on fiscal and economic issues. In places, where their opponents have traditionally been secularist socialists and social democrats, Christian democratic parties are moderately conservative, whereas in other cultural and political environments they can lean to the left.
Women's roles.
Attitudes and beliefs about the roles and responsibilities of women in Christianity vary considerably today as they have throughout the last two millennia — evolving along with or counter to the societies in which Christians have lived. The Bible and Christianity historically have been interpreted as excluding women from church leadership and placing them in submissive roles in marriage. Male leadership has been assumed in the church and within marriage, society and government.
Some contemporary writers describe the role of women in the life of the church as having been downplayed, overlooked, or denied throughout much of Christian history. Paradigm shifts in gender roles in society and also many churches has inspired reevaluation by many Christians of some long-held attitudes to the contrary. Christian egalitarians have increasingly argued for equal roles for men and women in marriage, as well as for the ordination of women to the clergy. Contemporary conservatives meanwhile have reasserted what has been termed a "complementarian" position, promoting the traditional belief that the Bible ordains different roles and responsibilities for women and men in the Church and family.
Major Christian denominations.
A Christian denomination is an identifiable religious body under a common name, structure, and doctrine within Christianity. Worldwide, Christians are divided, often along ethnic and linguistic lines, into separate churches and traditions. Technically, divisions between one group and another are defined by church doctrine and church authority. Centering on language of "professed Christianity" and "true Christianity", issues that separate one group of followers of Jesus from another include:
Christianity is composed of, but not limited to, five major branches of Churches: Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, Anglicanism, and Protestantism. Some listings include Anglicans among Protestants while others list the Eastern Orthodox and Oriental Orthodox together as one group, thus the number of distinct major branches can vary between three and five depending on the listing. The Assyrian Church of the East (Nestorians) and the Old Catholic churches are also distinct Christian bodies of historic importance, but much smaller in adherents and geographic scope. Each of the branches has important subdivisions. Because the Protestant subdivisions do not maintain a common theology or earthly leadership, they are far more distinct than the subdivisions of the other four groupings. "Denomination" typically refers to one of the many Christian groupings including each of the multitude of Protestant subdivisions.
Sizes of denomination.
Catholicism is the largest denomination, comprising just over half of Christians worldwide.
In Christendom, the largest denominations are:
Christendom and other beliefs.
In the interaction between Christendom and other belief systems, men and women when not at war with their neighbors have always made an effort to understand the Other (not least because understanding is a strategy for defense, but also because for as long as there is dialogue wars are delayed). Such interactions have led to various interfaith dialogue events. History records many examples of interfaith initiatives and dialogue throughout the ages. In the field of comparative religion, the interactions connects fundamental ideas in Christianity with similar ones in other religions. Christianity and other religions appear to share some elements. Regarding Christianity's relationship with other world beliefs, Christianity and other beliefs have differences and similarities in connection with each other.
Judaism.
Although Christianity and Judaism share historical roots, these two religions diverge in fundamental ways. Though Judeo-Christian tradition emphasizes continuities and convergences between the two religions, there are many other areas in which the faiths diverge.
Islam.
Christianity and Islam share their origins in the Abrahamic tradition, as well as Judaism. Islam accepts Jesus and his miracles and other aspects of Christianity as part of its faith - with some differences in interpretation, and rejects other aspects.
Buddhism.
There has been much speculation regarding a possible connection between both the Buddha and the Christ, and between Buddhism and Christianity. Buddhism originated in India about 500 years before the Apostolic Age and the origins of Christianity.
Hinduism.
The declaration "Nostra aetate" officially established inter-religious dialogue between Catholics and Hindus. It has promoted common values between religions. There are over 17.3 million Catholics in India, which represents less than 2% of the total population and is the largest Christian Church within India.
Secularism.
Irreligion is an absence, indifference or hostility to religion. Secularism, in one sense, may assert the right to be free to choose religious beliefs and non-beliefs. In its most prominent form, secularism is critical of religious orthodoxy and asserts that reason and the scientific method are better ways to understand reality than religious beliefs. Humanism refers to a philosophy centered on humankind. Much of Humanism's life stance upholds human reason, ethics, and justice, and rejects supernaturalism (Christian mythology).

</doc>
