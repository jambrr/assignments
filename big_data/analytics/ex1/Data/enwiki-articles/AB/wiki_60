<doc id="10326" url="https://en.wikipedia.org/wiki?curid=10326" title="Human evolution">
Human evolution

Human evolution is the evolutionary process that led to the emergence of anatomically modern humans. The topic typically focuses on the evolutionary history of the primates—in particular the genus "Homo", and the emergence of "Homo sapiens" as a distinct species of the hominids (or "great apes")—rather than studying the earlier history that led to the primates. The study of human evolution involves many scientific disciplines, including physical anthropology, primatology, archaeology, paleontology, neurobiology, ethology, linguistics, evolutionary psychology, embryology and genetics.
Genetic studies show that primates diverged from other mammals about , in the Late Cretaceous period, and the earliest fossils appear in the Paleocene, around . Within the Hominoidea (apes) superfamily, the Hominidae family diverged from the Hylobatidae (gibbon) family some 15–20 million years ago; African great apes (subfamily Homininae) diverged from orangutans (Ponginae) about ; the Hominini tribe (humans, "Australopithecines" and other extinct biped genera, and chimpanzees) parted from the Gorillini tribe (gorillas) about ; and, in turn, the subtribes Hominina (humans and biped ancestors) and Panina (chimps) separated about .
The basic adaptation of the hominin line is bipedalism. The earliest bipedal hominin is considered to be either "Sahelanthropus" or "Orrorin"; alternatively, either "Sahelanthropus" or "Orrorin" may instead be the last shared ancestor between chimps and humans. "Ardipithecus", a full biped, arose somewhat later, and the early bipeds eventually evolved into the australopithecines, and later into the genus "Homo".
The earliest documented representative of the genus "Homo" is "Homo habilis", which evolved around , and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of "Homo erectus" and "Homo ergaster" in the fossil record, cranial capacity had doubled to 850 cm3. (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that "Homo erectus" and "Homo ergaster" were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between .
According to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from "Homo heidelbergensis", "Homo rhodesiensis" or "Homo antecessor" and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of "Homo erectus", Denisova hominins, "Homo floresiensis" and "Homo neanderthalensis". Archaic "Homo sapiens", the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited inter-breeding between these species. The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago according to many anthropologists although some suggest a gradual change in behavior over a longer time span.
History of study.
Before Darwin.
The word "homo", the name of the biological genus to which humans belong, is Latin for "human". It was chosen originally by Carolus Linnaeus in his classification system. The word "human" is from the Latin "humanus", the adjectival form of "homo". The Latin "homo" derives from the Indo-European root *"dhghem", or "earth". Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.
Darwin.
The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's "On the Origin of Species", in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."
The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and apes, and did so particularly in his 1863 book "Evidence as to Man's Place in Nature". However, many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans when he published "The Descent of Man" in 1871.
First fossils.
A major problem at that time was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of "On the Origin of Species", and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were human remains of a creature suffering some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called "Homo erectus" at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described "Australopithecus africanus". The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocranial cast of the brain.
Although the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.
The East African fossils—and "Homo naledi" in South Africa.
During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. The driving force of these searches was the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave—all successful and world-renowned fossil hunters and palaeoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and "Homo" species, and even "Homo erectus".
These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of palaeoanthropology after "Lucy", the most complete fossil member of the species "Australopithecus afarensis", was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect. Lucy was classified as a new species, "Australopithecus afarensis", which is thought to be more closely related to the genus "Homo" as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range; "see" terms "hominid" and "hominin". (The specimen was nicknamed "Lucy" after the Beatles' song "Lucy in the Sky with Diamonds", which was played loudly and repeatedly in the camp during the excavations.) The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including "Ardipithecus ramidus" and "Ardipithecus kadabba".
In 2013, fossil skeletons of "Homo naledi", an extinct species of hominin assigned (provisionally) to the genus "Homo", were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg. , fossils of at least fifteen individuals, amounting to 1550 specimens, have been excavated from the cave. The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to "Australopithecus", and a cranial morphology (skull shape) similar to early "Homo" species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils have not yet been dated.
The genetic revolution.
The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.
In their seminal 1967 paper in "Science", Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy", and reinterpretation of older fossil materials, notably "Ramapithecus", showed the younger estimates to be correct and validated the albumin method.
Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.
On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimps noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimps to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimp populations in 8 locations suggests that chimps reproduce at age 26.5 years, on average; which suggests the human divergence from chimps occurred between 7 to 13 million years ago. And these data suggest that "Ardipithecus" (4.5 Ma), "Orrorin" (6 Ma) and "Sahelanthropus" (7 Ma) all may be on the hominin lineage, and even that the separation may have occurred outside the East African Rift region.
Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between "proto-human" and "proto-chimps" nonetheless occurred regularly enough to change certain genes in the new gene pool:
The research suggests:
The quest for the earliest hominin.
In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered "Australopithecus anamensis". The find was overshadowed by Tim D. White's 1995 discovery of "Ardipithecus ramidus", which pushed back the fossil record to .
In 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named "Orrorin tugenensis". And in 2001, a team led by Michel Brunet discovered the skull of "Sahelanthropus tchadensis" which was dated as , and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin ( Hominidae; terms "hominids" and hominins).
Human dispersal.
Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the "Homo" genus. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that genus "Homo" have migrated out of Africa at least three and possibly four times (e.g. "Homo erectus", "Homo heidelbergensis" and two or three times for "Homo sapiens").
Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artefacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, this strengthen the case that human tools have been found at a Chinese cave 2.48 million years ago. This suggests that the Asian "Chopper" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.
Dispersal of modern homo sapiens.
The "out of Africa" model proposed that modern "H. sapiens" speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in nearly complete replacement of other "Homo" species. This model has been developed by Chris B. Stringer and Peter Andrews.
In contrast, the multiregional hypothesis proposed that "Homo" genus contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple million years. This model was proposed in 1988 by Milford H. Wolpoff.
Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage. Aligned in genetic tree differences were interpreted as supportive of a recent single origin. Analyses have shown a greater diversity of DNA patterns throughout Africa, consistent with the idea that Africa is the ancestral home of mitochondrial Eve and Y-chromosomal Adam.
"Out of Africa" has gained support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.
A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters". The research also located the origin of modern human migration in south-western Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for Richard Leakey to resolve this debate. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. Evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans had been suggested by some studies.
Recent sequencing of Neanderthal and Denisovan genomes shows that some admixture occurred. Modern humans outside Africa have 2–4% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation. After recovery from a genetic bottleneck that might be due to the Toba supervolcano catastrophe, a fairly small group left Africa and briefly interbred with Neanderthals, probably in the middle-east or even North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in south-east Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations. which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant.
A second wave of humans may have dispersed across the Sinai Peninsula into Asia, resulting in the bulk of human population for Eurasia. This second group possibly possessed a more sophisticated tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA lineages, which support a single migration out of Africa that gave rise to all non-African populations.
Stephen Oppenheimer, on the basis of the early date of Badoshan Iranian Aurignacian, suggests that this second dispersal, may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.
Anatomical changes.
Human evolution is characterized by a number of morphological, developmental, physiological, and behavioral changes that have taken place since the split between the last common ancestor of humans and chimpanzees. The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in "H. erectus".
Bipedalism.
Bipedalism is the basic adaptation of the hominin and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominins. The earliest hominin, of presumably primitive bipedalism, is considered to be either "Sahelanthropus" or "Orrorin", both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorilla and chimpanzee, diverged from the hominin line over a period covering the same time, so either of "Sahelanthropus" or "Orrorin" may be our last shared ancestor. "Ardipithecus", a full biped, arose somewhat later. 
The early bipeds eventually evolved into the australopithecines and later the genus "Homo". There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion, enabled long distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna environment versus the previous forest habitat. A new study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking. However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal.
Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.
The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, permitting the passage of newborns due to the increase in cranial size but this is limited to the upper portion, since further increase can hinder normal bipedal movement.
The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller birth canal became an limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a big effect upon the female reproductive cycle, and the more frequent appearance of alloparenting in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more of their own.
Encephalization.
The human species developed a much larger brain than that of other primates—typically 1,330  cm3 in modern humans, over twice the size of that of a chimpanzee or gorilla. The pattern of encephalization started with "Homo habilis", which at approximately 600  cm3 had a brain slightly larger than that of chimpanzees, and continued with "Homo erectus" (800–1,100  cm3), reaching a maximum in Neanderthals with an average size of (1,200–1,900  cm3), larger even than "Homo sapiens". The pattern of human postnatal brain growth differs from that of other apes (heterochrony) and allows for extended periods of social learning and language acquisition in juvenile humans. However, the differences between the structure of human brains and those of other apes may be even more significant than differences in size.
The increase in volume over time has affected areas within the brain unequally—the temporal lobes, which contain centers for language processing, have increased disproportionately, and seems to favor a belief that there was evolution after leaving Africa, as has the prefrontal cortex which has been related to complex decision-making and moderating social behavior. Encephalization has been tied to an increasing emphasis on meat in the diet, or with the development of cooking, and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex. The human brain was able to expand because of the changes in the morphology of smaller mandibles and mandible muscle attachments to the skull into allowing more room for the brain to grow.
The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Traditionally the cerebellum has been associated with a paleocerebellum and archicerebellum as well as a neocerebellum. Its function has also traditionally been associated with balance, fine motor control but more recently speech and cognition. The great apes including humans and its antecessors had a more pronounced development of the cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and assisting in learning complex muscular action sequences, the cerebellum may have underpinned the evolution of human's technological adaptations including the preadaptation of speech.
The reason for this encephalization is difficult to discern, as the major changes from Homo erectus to Homo heidelbergensis were not associated with major changes in technology. It has been suggested that the changes have been associated with social changes, increased empathic abilities and increases in size of social groupings
Sexual dimorphism.
The reduced degree of sexual dimorphism is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans and bonobos are the only apes in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling during estrus).
Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.
Other changes.
A number of other changes have also characterized the evolution of humans, among them an increased importance on vision rather than smell; a smaller gut; loss of body hair; evolution of sweat glands; a change in the shape of the dental arcade from being u-shaped to being parabolic; development of a chin (found in "Homo sapiens" alone); development of styloid processes; and the development of a descended larynx.
Evidence.
The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.
Evidence from molecular biology.
The closest living relatives of humans are bonobos and chimpanzees (both genus "Pan") and gorillas (genus "Gorilla"). With the sequencing of both the human and chimpanzee genome, current estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.
The gibbons (family Hylobatidae) and then orangutans (genus "Pongo") were the first groups to split from the line leading to the hominins, including humans—followed by gorillas, and, ultimately, by the chimpanzees (genus "Pan"). The splitting date between hominin and chimpanzee lineages is placed by some between , that is, during the Late Miocene. Speciation, however, appears to have been unusually drawn-out. Initial divergence occurred sometime between , but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at .
Genetic evidence has also been employed to resolve the question of whether there was any gene flow between early modern humans and Neanderthals, and to enhance our understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.
Each time a certain mutation (Single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.
Genetics.
Human evolutionary genetics studies how one human genome differs from the other, the evolutionary past that gave rise to it, and its current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.
Evidence from the fossil record.
There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are "Sahelanthropus tchadensis" dating from , "Orrorin tugenensis" dating from , and "Ardipithecus kadabba" dating to . Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.
The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around and diverged into robust (also called "Paranthropus") and gracile branches, one of which (possibly "A. garhi") probably went on to become ancestors of the genus "Homo". The australopithecine species that is best represented in the fossil record is "Australopithecus afarensis" with more than one hundred fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as "Au. robustus" (or alternatively "Paranthropus robustus") and "Au./P. boisei" are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.
The earliest member of the genus "Homo" is "Homo habilis" which evolved around . "Homo habilis" is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider "Homo rudolfensis", a larger bodied group of fossils with similar morphology to the original "H. habilis" fossils, to be a separate species while others consider them to be part of "H. habilis"—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.
During the next million years, a process of encephalization began and, by the arrival (about ) of "Homo erectus" in the fossil record, cranial capacity had doubled. "Homo erectus" were the first of the hominins to emigrate from Africa, and, from , this species spread through Africa, Asia, and Europe. One population of "H. erectus", also sometimes classified as a separate species "Homo ergaster", remained in Africa and evolved into "Homo sapiens". It is believed that these species, "H. erectus" and "H. ergaster", were the first to use fire and complex tools.
The earliest transitional fossils between "H. ergaster/erectus" and archaic "H. sapiens" are from Africa, such as "Homo rhodesiensis", but seemingly transitional forms were also found at Dmanisi, Georgia. These descendants of African "H. erectus" spread through Eurasia from ca. 500,000 years ago evolving into "H. antecessor", "H. heidelbergensis" and "H. neanderthalensis". The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 200,000 years ago such as the Omo remains of Ethiopia; later fossils from Es Skhul cave in Israel and Southern Europe begin around 90,000 years ago ().
As modern humans spread out from Africa, they encountered other hominins such as "Homo neanderthalensis" and the so-called Denisovans, who may have evolved from populations of "Homo erectus" that had left Africa around . The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.
This migration out of Africa is estimated to have begun about 70,000 years BP (Before Present) and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.
Before "Homo".
Early evolution of primates.
Evolutionary history of the primates can be traced back 65 million years. One of the oldest known primate-like mammal species, the "Plesiadapis", came from North America; another, "Archicebus", came from China. Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.
David R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to "Dryopithecus", migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.
The earliest known catarrhine is "Kamoyapithecus" from uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to "Aegyptopithecus", "Propliopithecus", and "Parapithecus" from the Faiyum, at around 35 million years ago. In 2010, "Saadanius" was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 million years ago, helping to fill an 11-million-year gap in the fossil record.
In the Early Miocene, about 22 million years ago, the many kinds of arboreally adapted primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to "Victoriapithecus", the earliest Old World Monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are "Proconsul", "Rangwapithecus", "Dendropithecus", "Limnopithecus", "Nacholapithecus", "Equatorius", "Nyanzapithecus", "Afropithecus", "Heliopithecus", and "Kenyapithecus", all from East Africa.
The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant—"Otavipithecus" from cave deposits in Namibia, and "Pierolapithecus" and "Dryopithecus" from France, Spain and Austria—is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, "Oreopithecus", is from coal beds in Italy that have been dated to 9 million years ago.
Molecular evidence indicates that the lineage of gibbons (family Hylobatidae) diverged from the line of great apes some 18–12 million years ago, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown South East Asian hominoid population, but fossil proto-orangutans may be represented by "Sivapithecus" from India and "Griphopithecus" from Turkey, dated to around 10 million years ago.
Divergence of the human clade from other great apes.
Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by "Nakalipithecus" fossils found in Kenya and "Ouranopithecus" found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus "Pan") split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation—rain forest soils tend to be acidic and dissolve bone—and sampling bias probably contribute to this problem.
Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are "Sahelanthropus tchadensis" (7 Ma) and "Orrorin tugenensis" (6 Ma), followed by "Ardipithecus" (5.5–4.4 Ma), with species "Ar. kadabba" and "Ar. ramidus".
It has been argued in a study of the life history of "Ar. ramidus" that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape. This study demonstrated affinities between the skull morphology of "Ar. ramidus" and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos ("Pan paniscus") the less aggressive species of chimpanzee, may have evolved via the process of self-domestication. Consequently, arguing against the so-called "chimpanzee referential model" the authors suggest it is no longer tenable to use common chimpanzee ("Pan troglodytes") social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in "Ar. ramidus" and the implications this has for the evolution of hominin social psychology, they wrote:
The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.
Genus "Australopithecus".
The "Australopithecus" genus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including "Australopithecus anamensis", "Au. afarensis", "Au. sediba", and "Au. africanus". There is still some debate among academics whether certain African hominid species of this time, such as "Au. robustus" and "Au. boisei", constitute members of the same genus; if so, they would be considered to be "Au. robust australopiths" whilst the others would be considered "Au. gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, the "Paranthropus".
A new proposed species "Australopithecus deyiremeda" is claimed to have been discovered living at the same time period of "Au. afarensis". There is debate if Au. deyiremeda is a new species or is "Au. afarensis." Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the Australopithecus genus as old as the Afarensis. Given the opposable big toe found on Little Foot, it seems that he was a good climber, and it is thought given the night predators of the region, he probably, like Gorillas and Chimpanzees, built a nesting platform at night, in the trees.
Genus "Homo".
"Homo sapiens" is the only extant species of its genus, "Homo". While some (extinct) "Homo" species might have been ancestors of "Homo sapiens", many, perhaps most, were likely "cousins," having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the "Homo" genus. The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the early variation in the genus "Homo".
Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various "Homo" species and to study the role of diet in physical and behavioral evolution within "Homo".
Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences, killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.
"H. habilis" and "H. gautengensis".
"Homo habilis" lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines. "Homo habilis" had smaller molars and larger brains than the australopithecines, and made tools from stone and perhaps animal bones. One of the first known hominins, it was nicknamed 'handy man' by discoverer Louis Leakey due to its association with stone tools. Some scientists have proposed moving this species out of "Homo" and into "Australopithecus" due to the morphology of its skeleton being more adapted to living on trees rather than to moving on two legs like "Homo sapiens".
In May 2010, a new species, "Homo gautengensis" was discovered in South Africa.
"H. rudolfensis" and "H. georgicus".
These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to "Homo habilis" is not yet clear.
"H. ergaster" and "H. erectus".
The first fossils of "Homo erectus" were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material "Pithecanthropus erectus" based on its morphology, which he considered to be intermediate between that of humans and apes. "Homo erectus" lived from about 1.8 Ma to about 70,000 years ago—which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby "Homo floresiensis" survived it. The early phase of "Homo erectus", from 1.8 to 1.25 Ma, is considered by some to be a separate species, "Homo ergaster", or as "Homo erectus ergaster", a subspecies of "Homo erectus".
In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of "Homo habilis" are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, "Homo erectus"—in Africa. The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. suggests that the fact that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, "and swelled our brains to their current, horrendously fuel-inefficient size", suggest that control of fire and releasing increased nutritional value through cooking was the key adaptation that separated Homo from tree-sleeping Australopitheicines.
A famous example of "Homo erectus" is Peking Man; others were found in Asia (notably in Indonesia), Africa, and Europe. Many paleoanthropologists now use the term "Homo ergaster" for the non-Asian forms of this group, and reserve "Homo erectus" only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from "H. ergaster".
"H. cepranensis" and "H. antecessor".
These are proposed as species that may be intermediate between "H. erectus" and "H. heidelbergensis".
"H. heidelbergensis".
"H. heidelbergensis" ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as "Homo sapiens heidelbergensis" or "Homo sapiens paleohungaricus".
Neanderthal and Denisovan.
"Homo neanderthalensis", alternatively designated as "Homo sapiens neanderthalensis", lived in Europe and Asia from 400,000 to about 28,000 years ago. Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between "H. neanderthalensis" and "H. sapiens", and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans "circa" 45,000 to 80,000 years ago (at the approximate time that modern humans migrated out from Africa, but before they dispersed into Europe, Asia and elsewhere). The genetic sequencing of a human from Romania dated 40,000 years ago showed that 11% of their genome was Neanderthal. This would indicate that this individual had a Neanderthal great grandparent, 4 generations previously. It seems that this individual has left no living descendants.
Nearly all modern non-African humans have 1% to 4% of their DNA derived from Neanderthal DNA, and this finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although the interpretation of these studies has been questioned. Neanderthals and "Homo sapiens" could have co-existed in Europe for as long as 10,000 years, during which human populations exploded vastly outnumbering Neanderthals, possibly outcompeting them by sheer numerical strength.
In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.
While the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years, and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.
Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside of Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians,
The flow of genes from Neanderthal populations to modern human was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, has in 2016 reported that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show similarity to the modern human gene pool, more so than to European Neanderthal populations. The evidence suggests that the Neanderthal populations interbred with modern humans possibly 100,000 years ago, probably somewhere in the Near East.
Studies of a Neanderthal child at Gibraltar show from brain development and teeth eruption that Neanderthal children may have matured more rapidly than is the case for Homo spaiens.
"H. floresiensis".
"H. floresiensis", which lived from approximately 100,000 to 12,000 years before present, has been nicknamed "hobbit" for its small size, possibly a result of insular dwarfism. "H. floresiensis" is intriguing both for its size and its age, being an example of a recent species of the genus "Homo" that exhibits derived traits not shared with modern humans. In other words, "H. floresiensis" shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the "H. sapiens" average of 1400 cm3). 
However, there is an ongoing debate over whether "H. floresiensis" is indeed a separate species. Some scientists hold that "H. floresiensis" was a modern "H. sapiens" with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on "H. floresiensis" as a separate species is that it was found with tools only associated with "H. sapiens".
The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual. 
"H. sapiens".
"H. sapiens" (the adjective "sapiens" is Latin for "wise" or "intelligent") have lived from about 250,000 years ago to the present. Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from "H. erectus" to "H. sapiens". The direct evidence suggests there was a migration of "H. erectus" out of Africa, then a further speciation of "H. sapiens" from "H. erectus" in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed "H. erectus". This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. Current evidence does not preclude some multiregional evolution or some admixture of the migrant "H. sapiens" with existing "Homo" populations. This is a hotly debated area of paleoanthropology.
Current research has established that humans are genetically highly homogenous; that is, the DNA of individuals is more alike than usual for most species, which may have resulted from their relatively recent evolution or the possibility of a population bottleneck resulting from cataclysmic natural events such as the Toba catastrophe. Distinctive genetic characteristics have arisen, however, primarily as the result of small groups of people moving into new environmental circumstances. These adapted traits are a very small component of the "Homo sapiens" genome, but include various characteristics such as skin color and nose form, in addition to internal characteristics such as the ability to breathe more efficiently at high altitudes.
H. sapiens idaltu, from Ethiopia, is an extinct sub-species from about 160,000 years ago who is argued to be the direct ancestor of all modern humans.
Use of tools.
The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.
Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.
It should be noted that many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are the Oldowan stone tools from Ethiopia, 2.5–2.6 million years old. A "Homo" fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the "Homo" species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The Third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.
Bernard Wood noted that "Paranthropus" co-existed with the early "Homo" species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies "Paranthropus" as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early "Homo" species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, "Homo" was always present, but "Paranthropus" was not.
In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the "Homo" and "Paranthropus" species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.
Stone tools.
Stone tools are first attested around 2.6 Million years ago, when "H. habilis" in Eastern Africa used so-called pebble tools, choppers made out of round pebbles that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000-10,000 years ago.
Archaeologists working in the Great Rift Valley in Kenya claim to have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.
The period from 700,000–300,000 years ago is also known as the Acheulean, when "H. ergaster" (or "erectus") made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). In this period they also started to make tools out of bone.
Transition to behavioral modernity.
Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase ("H. habilis", "H. ergaster", "H. neanderthalensis") started at a higher level than the previous one, but after each phase started, further development was slow. Currently paleoanthropologists are debating whether these "Homo" species possessed some or many of the cultural and behavioral traits associated with modern humans such as language, complex symbolic thinking, technological creativity etc. It seems that they were culturally conservative maintaining simple technologies and foraging patterns over very long periods.
Around 50,000 BP, modern human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a Eurasian "Great Leap Forward," or as the "Upper Palaeolithic Revolution," due to the sudden appearance of distinctive signs of modern behavior and big game hunting in the archaeological record. Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African "Homo sapiens" since 200,000 years ago. Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a sea journey of up to 160 km by 60,000 years ago, which undermines the evidence of the Upper Paleolithic Revolution.
Modern humans started burying their dead, using animal hides to make clothing, hunting with more sophisticated techniques (such as using trapping pits or driving animals off cliffs), and engaging in cave painting. As human culture advanced, different populations of humans introduced novelty to existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of variation among different populations of humans, something that had not been seen in human cultures prior to 50,000 BP. Typically, "H. neanderthalensis" populations do not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal innovations produced as a result of exposure to the Homo sapiens Aurignacian technologies.
Among concrete examples of modern human behavior, anthropologists include specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (for example, burials with grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks. Debate continues as to whether a "revolution" led to modern humans ("the big bang of human consciousness"), or whether the evolution was more gradual.
Recent and current human evolution.
Natural selection still affects modern human populations. For example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations .
It has been argued that human evolution has accelerated since the development of agriculture and civilization some 10,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations. Lactase persistence is an example of such recent evolution. Recent human evolution seems to have been largely confined to genetic resistance to infectious disease that have appeared in human populations by crossing the species barrier from domesticated animals.
It is a common misconception that humans have stopped evolving and current genetic changes are purely genetic drift. Although selection pressure on some traits has decreased in modern human life (for instance, we are no longer evolving to survive smallpox), humans are still undergoing natural selection for many other traits (for instance, menopause is evolving to occur later).<ref name="doi10.1073/pnas.0906199106"/>
Species list.
This list is in chronological order across the page by genus.

</doc>
<doc id="10328" url="https://en.wikipedia.org/wiki?curid=10328" title="Evliya Çelebi">
Evliya Çelebi

Mehmed Zilli (25 March 1611 – after 1682), known as Evliya Çelebi (), was an Ottoman Turk who travelled through the territory of the Ottoman Empire and neighboring lands over a period of forty years, recording his commentary in a travelogue called "Seyâhatnâme".
Life.
Evliya Çelebi was born in Istanbul in the year 1611 to a wealthy family from Kütahya. His father was Derviş Mehmed Zilli, an Ottoman court jeweller and his mother was a Abkhazian. His mother was also a relative of the later grand vizier Melek Ahmed Pasha. In his book, Evliya Çelebi traces his paternal genealogy back to Ahmed Yesevi. Evliya Çelebi received a court education from the Imperial ulema. He may have joined the Gülşenî sufi order, as he shows an intimate knowledge of the sufi lodge in Cairo, and a graffito in which he referred to himself as "Evliya-yı Gülşenî" (Evliya of the Gülşenî). A devout Muslim opposed to fanaticism, Evliya could recite the Koran from memory and joked freely about Islam. Though employed as clergy and entertainer to the Ottoman grandees, Evliya refused employment that would keep him from travelling. His journal writing began in Constantinople, taking notes on buildings, markets, customs and culture, and in 1640 it was extended with accounts of his travels beyond the confines of the city. The collected notes of Evilya Çelebi's travels form a ten-volume work called the "Seyahatname" ("Travelogue").
He fought the Habsburgs in Transylvania.
Evliya Çelebi died sometime after 1682: it is unclear whether he was in Istanbul or Cairo at the time.
Travels.
Mostar.
According to Evliya Çelebi, the name "Mostar" means "bridge-keeper." Of the bridge, 28 meters long and 20 meters high, Çelebi wrote that "the bridge is like a rainbow arch soaring up to the skies, extending from one cliff to the other. ...I, a poor and miserable slave of Allah, have passed through 16 countries, but I have never seen such a high bridge. It is thrown from rock to rock as high as the sky."
Europe.
Çelebi claimed to have encountered Native Americans as a guest in Rotterdam during his visit of 1663. He wrote: " cursed those Jesuits, saying, 'Our world used to be peaceful, but it has been filled by greedy colonialists, who make war every year and shorten our lives.'"
While visiting Vienna in 1665–66, Çelebi noted some similarities between words in German and Persian, an early observation of the genetic relationship between what would later be known as two Indo-European languages.
Azerbaijan.
Of oil merchants in Baku Çelebi wrote: "By Allah's decree oil bubbles up out of the ground, but in the manner of hot springs, pools of water are formed with oil congealed on the surface like cream. Merchants wade into these pools and collect the oil in ladles and fill goatskins with it, these oil merchants then sell them in different regions. Revenues from this oil trade are delivered annually directly to the Safavid Shah."
Crimean Khanate.
Evliya Çelebi remarked on the impact of Cossack raids from Azak upon the territories of the Crimean Khanate, destroying trade routes and severely depopulating the regions. By the time of Çelebi's arrival, many of the towns visited were affected by the Cossacks, and the only place he reported as safe was the Ottoman fortress at Arabat.
Çelebi wrote of the slave trade in the Crimea:
Parthenon.
In 1667 Çelebi expressed his marvel at the Parthenon's sculptures and described the building as "like some impregnable fortress not made by human agency." He composed a poetic supplication that the Parthenon, as "a work less of human hands than of Heaven itself, should remain standing for all time."
The "Seyâhatnâme".
Although many of the descriptions the "Seyâhatnâme" were written in an exaggerated manner or were plainly inventive fiction or 3rd-source misinterpretation, his notes remain a useful guide to the culture and lifestyles of the 17th century Ottoman Empire. The first volume deals exclusively with Constantinople, the final volume with Egypt.
Currently there is no English translation of the entire "Seyahatname", although there are translations of various parts. The longest single English translation was published in 1834 by Ritter Joseph von Hammer-Purgstall, an Austrian orientalist: it may be found under the name "Evliya Efendi." Von Hammer's work covers the first two volumes (Constantinople and Anatolia) but its language is antiquated. Other translations include Erich Prokosch's nearly complete translation into German of the tenth volume, the 2004 introductory work entitled "The World of Evliya Çelebi: An Ottoman Mentality" written by University of Chicago professor Robert Dankoff, and Dankoff and Sooyong Kim's 2010 translation of select excerpts of the ten volumes, "An Ottoman Traveller: Selections from the Book of Travels of Evliya Çelebi".
Evliya is noted for having collected specimens of the languages in each region he traveled in. There are some 30 Turkic dialects and languages cataloged in the "Seyâhatnâme". Çelebi notes the similarities between several words from the German and Persian, though he denies any common Indo-European heritage. The "Seyâhatnâme" also contains the first transcriptions of many Caucasian languages and Tsakonian, and the only extant specimens of written Ubykh outside the linguistic literature.
In the 10 volumes of his Seyâhatnâme, he describes the following journeys:
Popular culture.
"İstanbul Kanatlarımın Altında" (Istanbul Under My Wings, 1996) is a film about the lives of legendary aviator brothers Hezarfen Ahmet Çelebi and Lagari Hasan Çelebi, and the Ottoman society in the early 17th century, during the reign of Murad IV, as witnessed and narrated by Evliya Çelebi.
Çelebi appears in Orhan Pamuk's novel "The White Castle", and is featured in the "The Adventures of Captain Bathory" (Dobrodružstvá kapitána Báthoryho) novels by Slovak writer Juraj Červenák.
"Evliya Çelebi ve Ölümsüzlük Suyu" (Evliya Çelebi and the Water of Life, 2014, dir. Serkan Zelzele), a children's adaptation of Çelebi's adventures, is the first full-length Turkish animated film.
United Nations Educational, Scientific and Cultural Organization, UNESCO included the 400th anniversary of Evliya Celebi's birth in its timetable for the celebration of anniversaries.

</doc>
<doc id="10331" url="https://en.wikipedia.org/wiki?curid=10331" title="Ancient Egyptian religion">
Ancient Egyptian religion

Ancient Egyptian religion was a complex system of polytheistic beliefs and rituals which were an integral part of ancient Egyptian society. It centered on the Egyptians' interaction with many deities who were believed to be present in, and in control of, the forces and elements of nature. The practices of Egyptian religion were efforts to provide for the gods and gain their favor. Formal religious practice centered on the pharaoh, the king of Egypt, who was believed to possess a divine power by virtue of his position. He acted as the intermediary between his people and the gods and was obligated to sustain the gods through rituals and offerings so that they could maintain order in the universe. The state dedicated enormous resources to Egyptian rituals and to the construction of the temples.
Individuals could interact with the gods for their own purposes, appealing for their help through prayer or compelling them to act through magic. These practices were distinct from, but closely linked with, the formal rituals and institutions. The popular religious tradition grew more prominent in the course of Egyptian history as the status of the Pharaoh declined. Another important aspect was the belief in the afterlife and funerary practices. The Egyptians made great efforts to ensure the survival of their souls after death, providing tombs, grave goods, and offerings to preserve the bodies and spirits of the deceased.
The religion had its roots in Egypt's prehistory and lasted for more than 3,000 years. The details of religious belief changed over time as the importance of particular gods rose and declined, and their intricate relationships shifted. At various times, certain gods became preeminent over the others, including the sun god Ra, the creator god Amun, and the mother goddess Isis. For a brief period, in the aberrant theology promulgated by the Pharaoh Akhenaten, a single god, the Aten, replaced the traditional pantheon. Ancient Egyptian religion and mythology left behind many writings and monuments, along with significant influences on ancient and modern cultures.
Theology.
The beliefs and rituals now referred to as "Ancient Egyptian religion" were integral within every aspect of Egyptian culture. Their language possessed no single term corresponding to the modern European concept of religion. Ancient Egyptian religion was not a monolithic institution, but consisted of a vast and varying set of beliefs and practices, linked by their common focus on the interaction between the world of humans and the world of the divine. The characteristics of the gods who populated the divine realm were inextricably linked to the Egyptians understanding of the properties of the world in which they lived.
Deities.
The Egyptians believed that the phenomena of nature were divine forces in and of themselves. These deified forces included the elements, animal characteristics, or abstract forces. The Egyptians believed in a pantheon of gods, which were involved in all aspects of nature and human society. Their religious practices were efforts to sustain and placate these phenomena and turn them to human advantage. This polytheistic system was very complex, as some deities were believed to exist in many different manifestations, and some had multiple mythological roles. Conversely, many natural forces, such as the sun, were associated with multiple deities. The diverse pantheon ranged from gods with vital roles in the universe to minor deities or "demons" with very limited or localized functions. It could include gods adopted from foreign cultures, and sometimes humans: deceased Pharaohs were believed to be divine, and occasionally, distinguished commoners such as Imhotep also became deified.
The depictions of the gods in art were not meant as literal representations of how the gods might appear if they were visible, as the gods' true natures were believed to be mysterious. Instead, these depictions gave recognizable forms to the abstract deities by using symbolic imagery to indicate each god's role in nature. Thus, for example, the funerary god Anubis was portrayed as a jackal, a creature whose scavenging habits threatened the preservation of the body, in an effort to counter this threat and employ it for protection. His black skin was symbolic of the color of mummified flesh and the fertile black soil that Egyptians saw as a symbol of resurrection. This iconography was not fixed, and many of the gods could be depicted in more than one form.
Many gods were associated with particular regions in Egypt where their cults were most important. However, these associations changed over time, and they did not mean that the god associated with a place had originated there. For instance, the god Monthu was the original patron of the city of Thebes. Over the course of the Middle Kingdom, however, he was displaced in that role by Amun, who may have arisen elsewhere. The national popularity and importance of individual gods fluctuated in a similar way.
Associations between deities.
The Egyptian gods had complex interrelationships, which partly reflected the interaction of the forces they represented. The Egyptians often grouped gods together to reflect these relationships. Some groups of deities were of indeterminate size, and were linked by their similar functions. These often consisted of minor deities with little individual identity. Other combinations linked independent deities based on the symbolic meaning of numbers in Egyptian mythology; for instance, pairs of deities usually represent the duality of opposite phenomena. One of the more common combinations was a family triad consisting of a father, mother, and child, who were worshipped together. Some groups had wide-ranging importance. One such group, the Ennead, assembled nine deities into a theological system that was involved in the mythological areas of creation, kingship, and the afterlife.
The relationships between deities could also be expressed in the process of syncretism, in which two or more different gods were linked to form a composite deity. This process was a recognition of the presence of one god "in" another when the second god took on a role belonging to the first. These links between deities were fluid, and did not represent the permanent merging of two gods into one; therefore, some gods could develop multiple syncretic connections. Sometimes syncretism combined deities with very similar characteristics. At other times it joined gods with very different natures, as when Amun, the god of hidden power, was linked with Ra, the god of the sun. The resulting god, Amun-Ra, thus united the power that lay behind all things with the greatest and most visible force in nature.
Unifying tendencies.
Many deities could be given epithets that seem to indicate that they were greater than any other god, suggesting some kind of unity beyond the multitude of natural forces. In particular, this is true of a few gods who, at various times in history, rose to supreme importance in Egyptian religion. These included the royal patron Horus, the sun god Ra, and the mother goddess Isis. During the New Kingdom (c. 1550–1070 BC), Amun held this position. The theology of the period described in particular detail Amun's presence in and rule over all things, so that he, more than any other deity, embodied the all-encompassing power of the divine.
Because of theological statements like this, many past Egyptologists, such as Siegfried Morenz, believed that beneath the polytheistic traditions of Egyptian religion there was an increasing belief in a unity of the divine, moving toward monotheism. Instances in Egyptian literature where "god" is mentioned without reference to any specific deity would seem to give this view added weight. However, in 1971 Erik Hornung pointed out that the traits of an apparently supreme being could be attributed to many different gods, even in periods when other gods were preeminent, and further argued that references to an unspecified "god" are meant to refer flexibly to any deity. He therefore argued that, while some individuals may have henotheistically chosen one god to worship, Egyptian religion as a whole had no notion of a divine being beyond the immediate multitude of deities. Yet the debate did not end there; Jan Assmann and James P. Allen have since asserted that the Egyptians did to some degree recognize a single divine force. In Allen's view, the notion of an underlying unity of the divine coexisted inclusively with the polytheistic tradition. It is possible that only the Egyptian theologians fully recognized this underlying unity, but it is also possible that ordinary Egyptians identified the single divine force with a single god in particular situations.
Atenism.
The Egyptians did have an aberrant period during the New Kingdom during which the pharaoh Akhenaten abolished the official worship of other gods in favor of the sun-disk Aten. This is often seen as the first instance of true monotheism in history, although the details of Atenist theology are still unclear and the suggestion that it was monotheistic is disputed. The exclusion of all but one god from worship was a radical departure from Egyptian tradition and some see Akhenaten as a practitioner of monolatry rather than monotheism, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten. Under Akhenaten's successors Egypt reverted to its traditional religion, and Akhenaten himself came to be reviled as a heretic.
Other important concepts.
Cosmology.
The Egyptian conception of the universe centered on "Ma'at", a word that encompasses several concepts in English, including "truth," "justice," and "order." It was the fixed, eternal order of the universe, both in the cosmos and in human society. It had existed since the creation of the world, and without it the world would lose its cohesion. In Egyptian belief, Ma'at was constantly under threat from the forces of disorder, so all of society was required to maintain it. On the human level this meant that all members of society should cooperate and coexist; on the cosmic level it meant that all of the forces of nature—the gods—should continue to function in balance. This latter goal was central to Egyptian religion. The Egyptians sought to maintain Ma'at in the cosmos by sustaining the gods through offerings and by performing rituals which staved off disorder and perpetuated the cycles of nature.
The most important part of the Egyptian view of the cosmos was the conception of time, which was greatly concerned with the maintenance of Ma'at. Throughout the linear passage of time, a cyclical pattern recurred, in which Ma'at was renewed by periodic events which echoed the original creation. Among these events were the annual Nile flood and the succession from one king to another, but the most important was the daily journey of the sun god Ra.
When envisioning the shape of the cosmos, the Egyptians saw the earth as a flat expanse of land, personified by the god Geb, over which arched the sky goddess Nut. The two were separated by Shu, the god of air. Beneath the earth lay a parallel underworld and undersky, and beyond the skies lay the infinite expanse of Nu, the chaos that had existed before creation. The Egyptians also believed in a place called the Duat, a mysterious region associated with death and rebirth, that may have lain in the underworld or in the sky. Each day, Ra traveled over the earth across the underside of the sky, and at night he passed through the Duat to be reborn at dawn.
In Egyptian belief, this cosmos was inhabited by three types of sentient beings. One was the gods; another was the spirits of deceased humans, who existed in the divine realm and possessed many of the gods' abilities. Living humans were the third category, and the most important among them was the pharaoh, who bridged the human and divine realms.
Divine pharaoh.
Egyptologists have long debated the degree to which the Pharaoh was considered a god. It seems most likely that the Egyptians viewed royal authority itself as a divine force. Therefore, although the Egyptians recognized that the Pharaoh was human and subject to human weakness, they simultaneously viewed him as a god, because the divine power of kingship was incarnated in him. He therefore acted as intermediary between Egypt's people and the gods. He was key to upholding Ma'at, both by maintaining justice and harmony in human society and by sustaining the gods with temples and offerings. For these reasons, he oversaw all state religious activity. However, the Pharaoh’s real-life influence and prestige could differ from that depicted in official writings and depictions, and beginning in the late New Kingdom his religious importance declined drastically.
The king was also associated with many specific deities. He was identified directly with Horus, who represented kingship itself, and he was seen as the son of Ra, who ruled and regulated nature as the Pharaoh ruled and regulated society. By the New Kingdom he was also associated with Amun, the supreme force in the cosmos. Upon his death, the king became fully deified. In this state, he was directly identified with Ra, and was also associated with Osiris, god of death and rebirth and the mythological father of Horus. Many mortuary temples were dedicated to the worship of deceased pharaohs as gods.
Afterlife.
The Egyptians had elaborate beliefs about death and the afterlife. They believed that humans possessed a "ka", or life-force, which left the body at the point of death. In life, the ka received its sustenance from food and drink, so it was believed that, to endure after death, the ka must continue to receive offerings of food, whose spiritual essence it could still consume. Each person also had a "ba", the set of spiritual characteristics unique to each individual. Unlike the ka, the ba remained attached to the body after death. Egyptian funeral rituals were intended to release the ba from the body so that it could move freely, and to rejoin it with the ka so that it could live on as an akh. However, it was also important that the body of the deceased be preserved, as the Egyptians believed that the ba returned to its body each night to receive new life, before emerging in the morning as an akh.
Originally, however, the Egyptians believed that only the pharaoh had a ba, and only he could become one with the gods; dead commoners passed into a dark, bleak realm that represented the opposite of life. The nobles received tombs and the resources for their upkeep as gifts from the king, and their ability to enter the afterlife was believed to be dependent on these royal favors. In early times the deceased pharaoh was believed to ascend to the sky and dwell among the stars. Over the course of the Old Kingdom (c. 2686–2181 BC), however, he came to be more closely associated with the daily rebirth of the sun god Ra and with the underworld ruler Osiris as those deities grew more important.
Judgment.
During the late Old Kingdom (2686–2181 BC) and the First Intermediate Period (c. 2181–2055 BC), the Egyptians gradually came to believe that possession of a "ba" and the possibility of a paradisiacal afterlife extended to everyone. In the fully developed afterlife beliefs of the New Kingdom, the soul had to avoid a variety of supernatural dangers in the Duat, before undergoing a final judgment known as the "Weighing of the Heart". In this judgment, the gods compared the actions of the deceased while alive (symbolized by the heart) to Ma'at, to determine whether he or she had behaved in accordance with Ma'at. If the deceased was judged worthy, his or her ka and ba were united into an akh. Several beliefs coexisted about the akh's destination. Often the dead were said to dwell in the realm of Osiris, a lush and pleasant land in the underworld. The solar vision of the afterlife, in which the deceased soul traveled with Ra on his daily journey, was still primarily associated with royalty, but could extend to other people as well. Over the course of the Middle and New Kingdoms, the notion that the akh could also travel in the world of the living, and to some degree magically affect events there, became increasingly prevalent.
Writings.
While the Egyptians had no unified religious scripture, they produced many religious writings of various types. Together the disparate texts provide a very extensive, but still incomplete, understanding of Egyptian religious practices and beliefs.
Mythology.
Egyptian myths were metaphorical stories intended to illustrate and explain the gods' actions and roles in nature. The details of the events they recounted could change to convey different symbolic perspectives on the mysterious divine events they described, so many myths exist in different and conflicting versions. Mythical narratives were rarely written in full, and more often texts only contain episodes from or allusions to a larger myth. Knowledge of Egyptian mythology, therefore, is derived mostly from hymns that detail the roles of specific deities, from ritual and magical texts which describe actions related to mythic events, and from funerary texts which mention the roles of many deities in the afterlife. Some information is also provided by allusions in secular texts. Finally, Greeks and Romans such as Plutarch recorded some of the extant myths late in Egyptian history.
Among the significant Egyptian myths were the creation myths. According to these stories, the world emerged as a dry space in the primordial ocean of chaos. Because the sun is essential to life on earth, the first rising of Ra marked the moment of this emergence. Different forms of the myth describe the process of creation in various ways: a transformation of the primordial god Atum into the elements that form the world, as the creative speech of the intellectual god Ptah, and as an act of the hidden power of Amun. Regardless of these variations, the act of creation represented the initial establishment of maat and the pattern for the subsequent cycles of time.
The most important of all Egyptian myths was the myth of Osiris and Isis. It tells of the divine ruler Osiris, who was murdered by his jealous brother Set, a god often associated with chaos. Osiris' sister and wife Isis resurrected him so that he could conceive an heir, Horus. Osiris then entered the underworld and became the ruler of the dead. Once grown, Horus fought and defeated Set to become king himself. Set's association with chaos, and the identification of Osiris and Horus as the rightful rulers, provided a rationale for Pharaonic succession and portrayed the Pharaohs as the upholders of order. At the same time, Osiris' death and rebirth were related to the Egyptian agricultural cycle, in which crops grew in the wake of the Nile inundation, and provided a template for the resurrection of human souls after death.
Another important mythic motif was the journey of Ra through the Duat each night. In the course of this journey, Ra met with Osiris, who again acted as an agent of regeneration, so that his life was renewed. He also fought each night with Apep, a serpentine god representing chaos. The defeat of Apep and the meeting with Osiris ensured the rising of the sun the next morning, an event that represented rebirth and the victory of order over chaos.
Ritual and magical texts.
The procedures for religious rituals were frequently written on papyri, which were used as instructions for those performing the ritual. These ritual texts were kept mainly in the temple libraries. Temples themselves are also inscribed with such texts, often accompanied by illustrations. Unlike the ritual papyri, these inscriptions were not intended as instructions, but were meant to symbolically perpetuate the rituals even if, in reality, people ceased to perform them. Magical texts likewise describe rituals, although these rituals were part of the spells used for specific goals in everyday life. Despite their mundane purpose, many of these texts also originated in temple libraries and later became disseminated among the general populace.
Hymns and prayers.
The Egyptians produced numerous prayers and hymns, written in the form of poetry. Hymns and prayers follow a similar structure and are distinguished mainly by the purposes they serve. Hymns were written to praise particular deities. Like ritual texts, they were written on papyri and on temple walls, and they were probably recited as part of the rituals they accompany in temple inscriptions. Most are structured according to a set literary formula, designed to expound on the nature, aspects, and mythological functions of a given deity. They tend to speak more explicitly about fundamental theology than other Egyptian religious writings, and became particularly important in the New Kingdom, a period of particularly active theological discourse. Prayers follow the same general pattern as hymns, but address the relevant god in a more personal way, asking for blessings, help, or forgiveness for wrongdoing. Such prayers are rare before the New Kingdom, indicating that in earlier periods such direct personal interaction with a deity was not believed possible, or at least was less likely to be expressed in writing. They are known mainly from inscriptions on statues and stelae left in sacred sites as votive offerings.
Funerary texts.
Among the most significant and extensively preserved Egyptian writings are funerary texts designed to ensure that deceased souls reached a pleasant afterlife. The earliest of these are the Pyramid Texts. They are a loose collection of hundreds of spells inscribed on the walls of royal pyramids during the Old Kingdom, intended to magically provide pharaohs with the means to join the company of the gods in the afterlife. The spells appear in differing arrangements and combinations, and few of them appear in all of the pyramids.
At the end of the Old Kingdom a new body of funerary spells, which included material from the Pyramid Texts, began appearing in tombs, inscribed primarily on coffins. This collection of writings is known as the Coffin Texts, and was not reserved for royalty, but appeared in the tombs of non-royal officials. In the New Kingdom, several new funerary texts emerged, of which the best-known is the Book of the Dead. Unlike the earlier books, it often contains extensive illustrations, or vignettes. The book was copied on papyrus and sold to commoners to be placed in their tombs.
The Coffin Texts included sections with detailed descriptions of the underworld and instructions on how to overcome its hazards. In the New Kingdom, this material gave rise to several "books of the netherworld", including the Book of Gates, the Book of Caverns, and the Amduat. Unlike the loose collections of spells, these netherworld books are structured depictions of Ra's passage through the Duat, and by analogy, the journey of the deceased person's soul through the realm of the dead. They were originally restricted to pharaonic tombs, but in the Third Intermediate Period they came to be used more widely.
As Egypt became more modernized, its archaic practices were substituted with new and efficient scientific techniques. Some of these scientific advancements were related to the development of mummification. By enhancing their advanced practice of mummification, the Egyptians were able to reach a new level of excellency concerning afterlife.
Practices.
Temples.
Temples existed from the beginning of Egyptian history, and at the height of the civilization they were present in most of its towns. They included both mortuary temples to serve the spirits of deceased pharaohs and temples dedicated to patron gods, although the distinction was blurred because divinity and kingship were so closely intertwined. The temples were not primarily intended as places for worship by the general populace, and the common people had a complex set of religious practices of their own. Instead, the state-run temples served as houses for the gods, in which physical images which served as their intermediaries were cared for and provided with offerings. This service was believed to be necessary to sustain the gods, so that they could in turn maintain the universe itself. Thus, temples were central to Egyptian society, and vast resources were devoted to their upkeep, including both donations from the monarchy and large estates of their own. Pharaohs often expanded them as part of their obligation to honor the gods, so that many temples grew to enormous size. However, not all gods had temples dedicated to them, as many gods who were important in official theology received only minimal worship, and many household gods were the focus of popular veneration rather than temple ritual.
The earliest Egyptian temples were small, impermanent structures, but through the Old and Middle Kingdoms their designs grew more elaborate, and they were increasingly built out of stone. In the New Kingdom, a basic temple layout emerged, which had evolved from common elements in Old and Middle Kingdom temples. With variations, this plan was used for most of the temples built from then on, and most of those that survive today adhere to it. In this standard plan, the temple was built along a central processional way that led through a series of courts and halls to the sanctuary, which held a statue of the temple's god. Access to this most sacred part of the temple was restricted to the pharaoh and the highest-ranking priests. The journey from the temple entrance to the sanctuary was seen as a journey from the human world to the divine realm, a point emphasized by the complex mythological symbolism present in temple architecture. Well beyond the temple building proper was the outermost wall. In the space between the two lay many subsidiary buildings, including workshops and storage areas to supply the temple's needs, and the library where the temple's sacred writings and mundane records were kept, and which also served as a center of learning on a multitude of subjects.
Theoretically it was the duty of the pharaoh to carry out temple rituals, as he was Egypt's official representative to the gods. In reality, ritual duties were almost always carried out by priests. During the Old and Middle Kingdoms, there was no separate class of priests; instead, many government officials served in this capacity for several months out of the year before returning to their secular duties. Only in the New Kingdom did professional priesthood become widespread, although most lower-ranking priests were still part-time. All were still employed by the state, and the pharaoh had final say in their appointments. However, as the wealth of the temples grew, the influence of their priesthoods increased, until it rivaled that of the pharaoh. In the political fragmentation of the Third Intermediate Period (c. 1070–664 BC), the high priests of Amun at Karnak even became the effective rulers of Upper Egypt. The temple staff also included many people other than priests, such as musicians and chanters in temple ceremonies. Outside the temple were artisans and other laborers who helped supply the temple's needs, as well as farmers who worked on temple estates. All were paid with portions of the temple's income. Large temples were therefore very important centers of economic activity, sometimes employing thousands of people.
Official rituals and festivals.
State religious practice included both temple rituals involved in the cult of a deity, and ceremonies related to divine kingship. Among the latter were coronation ceremonies and the sed festival, a ritual renewal of the pharaoh's strength that took place periodically during his reign. There were numerous temple rituals, including rites that took place across the country and rites limited to single temples or to the temples of a single god. Some were performed daily, while others took place annually or on rarer occasions. The most common temple ritual was the morning offering ceremony, performed daily in temples across Egypt. In it, a high-ranking priest, or occasionally the pharaoh, washed, anointed, and elaborately dressed the god's statue before presenting it with offerings. Afterward, when the god had consumed the spiritual essence of the offerings, the items themselves were taken to be distributed among the priests.
The less frequent temple rituals, or festivals, were still numerous, with dozens occurring every year. These festivals often entailed actions beyond simple offerings to the gods, such as reenactments of particular myths or the symbolic destruction of the forces of disorder. Most of these events were probably celebrated only by the priests and took place only inside the temple. However, the most important temple festivals, like the Opet Festival celebrated at Karnak, usually involved a procession carrying the god's image out of the sanctuary in a model barque to visit other significant sites, such as the temple of a related deity. Commoners gathered to watch the procession and sometimes received portions of the unusually large offerings given to the gods on these occasions.
Animal cults.
At many sacred sites, the Egyptians worshipped individual animals which they believed to be manifestations of particular deities. These animals were selected based on specific sacred markings which were believed to indicate their fitness for the role. Some of these cult animals retained their positions for the rest of their lives, as with the Apis bull worshipped in Memphis as a manifestation of Ptah. Other animals were selected for much shorter periods. These cults grew more popular in later times, and many temples began raising stocks of such animals from which to choose a new divine manifestation. A separate practice developed in the Twenty-sixth Dynasty, when people began mummifying any member of a particular animal species as an offering to the god whom the species represented. Millions of mummified cats, birds, and other creatures were buried at temples honoring Egyptian deities. Worshippers paid the priests of a particular deity to obtain and mummify an animal associated with that deity, and the mummy was placed in a cemetery near the god's cult center.
Oracles.
The Egyptians used oracles to ask the gods for knowledge or guidance. Egyptian oracles are known mainly from the New Kingdom and afterward, though they probably appeared much earlier. People of all classes, including the king, asked questions of oracles, and, especially in the late New Kingdom their answers could be used to settle legal disputes or inform royal decisions. The most common means of consulting an oracle was to pose a question to the divine image while it was being carried in a festival procession, and interpret an answer from the barque's movements. Other methods included interpreting the behavior of cult animals, drawing lots, or consulting statues through which a priest apparently spoke. The means of discerning the god's will gave great influence to the priests who spoke and interpreted the god's message.
Popular religion.
While the state cults were meant to preserve the stability of the Egyptian world, lay individuals had their own religious practices that related more directly to daily life. This popular religion left less evidence than the official cults, and because this evidence was mostly produced by the wealthiest portion of the Egyptian population, it is uncertain to what degree it reflects the practices of the populace as a whole.
Popular religious practice included ceremonies marking important transitions in life. These included birth, because of the danger involved in the process, and naming, because the name was held to be a crucial part of a person's identity. The most important of these ceremonies were those surrounding death (see "Funerary practices" below), because they ensured the soul's survival beyond it. Other religious practices sought to discern the gods' will or seek their knowledge. These included the interpretation of dreams, which could be seen as messages from the divine realm, and the consultation of oracles. People also sought to affect the gods' behavior to their own benefit through magical rituals (see "Magic" below).
Individual Egyptians also prayed to gods and gave them private offerings. Evidence of this type of personal piety is sparse before the New Kingdom. This is probably due to cultural restrictions on depiction of nonroyal religious activity, which relaxed during the Middle and New Kingdoms. Personal piety became still more prominent in the late New Kingdom, when it was believed that the gods intervened directly in individual lives, punishing wrongdoers and saving the pious from disaster. Official temples were important venues for private prayer and offering, even though their central activities were closed to laypeople. Egyptians frequently donated goods to be offered to the temple deity and objects inscribed with prayers to be placed in temple courts. Often they prayed in person before temple statues or in shrines set aside for their use. Yet in addition to temples, the populace also used separate local chapels, smaller but more accessible than the formal temples. These chapels were very numerous, and probably staffed by members of the community. Households, too, often had their own small shrines for offering to gods or deceased relatives.
The deities invoked in these situations differed somewhat from those at the center of state cults. Many of the important popular deities, such as the fertility goddess Taweret and the household protector Bes, had no temples of their own. However, many other gods, including Amun and Osiris, were very important in both popular and official religion. Some individuals might be particularly devoted to a single god. Often they favored deities affiliated with their own region, or with their role in life. The god Ptah, for instance, was particularly important in his cult center of Memphis, but as the patron of craftsmen he received the nationwide veneration of many in that occupation.
Magic.
The word "magic" is used to translate the Egyptian term "heka", which meant, as James P. Allen puts it, "the ability to make things happen by indirect means". Heka was believed to be a natural phenomenon, the force which was used to create the universe and which the gods employed to work their will. Humans could also use it, however, and magical practices were closely intertwined with religion. In fact, even the regular rituals performed in temples were counted as magic. Individuals also frequently employed magical techniques for personal purposes. Although these ends could be harmful to other people, no form of magic was considered inimical in itself. Instead, magic was seen primarily as a way for humans to prevent or overcome negative events.
Magic was closely associated with the priesthood. Because temple libraries contained numerous magical texts, great magical knowledge was ascribed to the lector priests who studied these texts. These priests often worked outside their temples, hiring out their magical services to laymen. Other professions also commonly employed magic as part of their work, including doctors, scorpion-charmers, and makers of magical amulets. It is also likely that the peasantry used simple magic for their own purposes, but because this magical knowledge would have been passed down orally, there is limited evidence of it.
Language was closely linked with heka, to such a degree that Thoth, the god of writing, was sometimes said to be the inventor of heka. Therefore, magic frequently involved written or spoken incantations, although these were usually accompanied by ritual actions. Often these rituals invoked the power of an appropriate deity to perform the desired action, using the power of heka to compel it to act. Sometimes this entailed casting the practitioner or subject of a ritual in the role of a character in mythology, thus inducing the god to act toward that person as it had in the myth. Rituals also employed sympathetic magic, using objects believed to have a magically significant resemblance to the subject of the rite. The Egyptians also commonly used objects believed to be imbued with heka of their own, such as the magically protective amulets worn in great numbers by ordinary Egyptians.
Funerary practices.
Because it was considered necessary for the survival of the soul, preservation of the body was a central part of Egyptian funerary practices. Originally the Egyptians buried their dead in the desert, where the arid conditions mummified the body naturally. In the Early Dynastic Period, however, they began using tombs for greater protection, and the body was insulated from the desiccating effect of the sand and was subject to natural decay. Thus the Egyptians developed their elaborate embalming practices, in which the corpse was artificially desiccated and wrapped to be placed in its coffin. The quality of the process varied according to cost, however, and those who could not afford it were still buried in desert graves.
Once the mummification process was complete, the mummy was carried from the deceased person's house to the tomb in a funeral procession that included his or her friends and relatives, along with a variety of priests. Before the burial, these priests performed several rituals, including the Opening of the mouth ceremony intended to restore the dead person's senses and give him or her the ability to receive offerings. Then the mummy was buried and the tomb sealed. Afterward, relatives or hired priests gave food offerings to the deceased in a nearby mortuary chapel at regular intervals. Over time, families inevitably neglected offerings to long-dead relatives, so most mortuary cults only lasted one or two generations. However, while the cult lasted, the living sometimes wrote letters asking deceased relatives for help, in the belief that the dead could affect the world of the living as the gods did.
The first Egyptian tombs were mastabas, rectangular brick structures where kings and nobles were entombed. Each of them contained a subterranean burial chamber and a separate, above ground chapel for mortuary rituals. In the Old Kingdom the mastaba developed into the pyramid, which symbolized the primeval mound of Egyptian myth. Pyramids were reserved for royalty, and were accompanied by large mortuary temples sitting at their base. Middle Kingdom pharaohs continued to build pyramids, but the popularity of mastabas waned. Increasingly, commoners with sufficient means were buried in rock-cut tombs with separate mortuary chapels nearby, an approach which was less vulnerable to tomb robbery. By the beginning of the New Kingdom even the pharaohs were buried in such tombs, and they continued to be used until the decline of the religion itself.
Tombs could contain a great variety of other items, including statues of the deceased to serve as substitutes for the body in case it was damaged. Because it was believed that the deceased would have to do work in the afterlife, just as in life, burials often included small models of humans to do work in place of the deceased. The tombs of wealthier individuals could also contain furniture, clothing, and other everyday objects intended for use in the afterlife, along with amulets and other items intended to provide magical protection against the hazards of the spirit world. Further protection was provided by funerary texts included in the burial. The tomb walls also bore artwork, including images of the deceased eating food which were believed to allow him or her to magically receive sustenance even after the mortuary offerings had ceased.
History.
Predynastic and Early Dynastic periods.
The beginnings of Egyptian religion extend into prehistory, and evidence for them comes only from the sparse and ambiguous archaeological record. Careful burials during the Predynastic period imply that the people of this time believed in some form of an afterlife. At the same time, animals were ritually buried, a practice which may reflect the development of zoomorphic deities like those found in the later religion. The evidence is less clear for gods in human form, and this type of deity may have emerged more slowly than those in animal shape. Each region of Egypt originally had its own patron deity, but it is likely that as these small communities conquered or absorbed each other, the god of the defeated area was either incorporated into the other god's mythology or entirely subsumed by it. This resulted in a complex pantheon in which some deities remained only locally important while others developed more universal significance. As the time changed and the shifting of the empires changed like the middle kingdom, new kingdom, and old kingdom, usually the religion followed stayed within the border of that territory.
The Early Dynastic period began with the unification of Egypt around 3000 BC. This event transformed Egyptian religion, as some deities rose to national importance and the cult of the divine pharaoh became the central focus of religious activity. Horus was identified with the king, and his cult center in the Upper Egyptian city of Nekhen was among the most important religious sites of the period. Another important center was Abydos, where the early rulers built large funerary complexes.
Old and Middle Kingdoms.
During the Old Kingdom, the priesthoods of the major deities attempted to organize the complicated national pantheon into groups linked by their mythology and worshipped in a single cult center, such as the Ennead of Heliopolis which linked important deities such as Atum, Ra, Osiris, and Set in a single creation myth. Meanwhile, pyramids, accompanied by large mortuary temple complexes, replaced mastabas as the tombs of pharaohs. In contrast with the great size of the pyramid complexes, temples to gods remained comparatively small, suggesting that official religion in this period emphasized the cult of the divine king more than the direct worship of deities. The funerary rituals and architecture of this time greatly influenced the more elaborate temples and rituals used in worshipping the gods in later periods.
Early in the Old Kingdom, Ra grew in influence, and his cult center at Heliopolis became the nation's most important religious site. By the Fifth Dynasty, Ra was the most prominent god in Egypt, and had developed the close links with kingship and the afterlife that he retained for the rest of Egyptian history. Around the same time, Osiris became an important afterlife deity. "The Pyramid Texts," first written at this time, reflect the prominence of the solar and Osirian concepts of the afterlife, although they also contain remnants of much older traditions. The texts are an extremely important source for understanding early Egyptian theology.
In the 22nd century BC, the Old Kingdom collapsed into the disorder of the First Intermediate Period, with important consequences for Egyptian religion. Old Kingdom officials had already begun to adopt the funerary rites originally reserved for royalty, but now, less rigid barriers between social classes meant that these practices and the accompanying beliefs gradually extended to all Egyptians, a process called the "democratization of the afterlife". The Osirian view of the afterlife had the greatest appeal to commoners, and thus Osiris became one of the most important gods.
Eventually rulers from Thebes reunified the Egyptian nation in the Middle Kingdom (c. 2055–1650 BC). These Theban pharaohs initially promoted their patron god Monthu to national importance, but during the Middle Kingdom, he was eclipsed by the rising popularity of Amun. In this new Egyptian state, personal piety grew more important and was expressed more freely in writing, a trend which continued in the New Kingdom.
New Kingdom.
The Middle Kingdom crumbled in the Second Intermediate Period (c. 1650–1550 BC), but the country was again reunited by Theban rulers, who became the first pharaohs of the New Kingdom. Under the new regime, Amun became the supreme state god. He was syncretized with Ra, the long-established patron of kingship, and his temple at Karnak in Thebes became Egypt's most important religious center. Amun's elevation was partly due to the great importance of Thebes, but it was also due to the increasingly professional priesthood. Their sophisticated theological discussion produced detailed descriptions of Amun's universal power.
Increased contact with outside peoples in this period led to the adoption of many Near Eastern deities into the pantheon. At the same time, the subjugated Nubians absorbed Egyptian religious beliefs, and in particular, adopted Amun as their own.
The New Kingdom religious order was disrupted when Akhenaten acceded, and replaced Amun with the Aten as the state god. Eventually he eliminated the official worship of most other gods, and moved Egypt's capital to a new city at Amarna. This part of Egyptian history, the Amarna period, is named after this. In doing so, Akhenaten claimed unprecedented status: only he could worship the Aten, and the populace directed their worship toward him. The Atenist system lacked well-developed mythology and afterlife beliefs, and the Aten seemed distant and impersonal, so the new order did not appeal to ordinary Egyptians. Thus, many probably continued to worship the traditional gods in private. Nevertheless, the withdrawal of state support for the other deities severely disrupted Egyptian society. Akhenaten's successors restored the traditional religious system, and eventually they dismantled all Atenist monuments.
Before the Amarna period, popular religion had trended toward more personal relationships between worshippers and their gods. Akhenaten's changes had reversed this trend, but once the traditional religion was restored, there was a backlash. The populace began to believe that the gods were much more directly involved in daily life. Amun, the supreme god, was increasingly seen as the final arbiter of human destiny, the true ruler of Egypt. The pharaoh was correspondingly more human and less divine. The importance of oracles as a means of decision-making grew, as did the wealth and influence of the oracles' interpreters, the priesthood. These trends undermined the traditional structure of society and contributed to the breakdown of the New Kingdom.
Later periods.
In the 1st millennium BC, Egypt was significantly weaker than in earlier times, and in several periods foreigners seized the country and assumed the position of pharaoh. The importance of the pharaoh continued to decline, and the emphasis on popular piety continued to increase. Animal cults, a characteristically Egyptian form of worship, became increasingly popular in this period, possibly as a response to the uncertainty and foreign influence of the time. Isis grew more popular as a goddess of protection, magic, and personal salvation, and became the most important goddess in Egypt.
In the 4th century BC, Egypt became a Hellenistic kingdom under the Ptolemaic dynasty (305–30 BC), which assumed the pharaonic role, maintaining the traditional religion and building or rebuilding many temples. The kingdom's Greek ruling class identified the Egyptian deities with their own. From this cross-cultural syncretism emerged Serapis, a god who combined Osiris and Apis with characteristics of Greek deities, and who became very popular among the Greek population. Nevertheless, for the most part the two belief systems remained separate, and the Egyptian deities remained Egyptian.
Ptolemaic-era beliefs changed little after Egypt became a province of the Roman Empire in 30 BC, with the Ptolemaic kings replaced by distant emperors. The cult of Isis appealed even to Greeks and Romans outside Egypt, and in Hellenized form it spread across the empire. In Egypt itself, as the empire weakened, official temples fell into decay, and without their centralizing influence religious practice became fragmented and localized. Meanwhile, Christianity spread across Egypt, and in the third and fourth centuries AD, edicts by Christian emperors and iconoclasm by local Christians eroded traditional beliefs. While it persisted among the populace for some time, Egyptian religion slowly faded away.
Legacy.
Egyptian religion produced the temples and tombs which are ancient Egypt's most enduring monuments, but it also influenced other cultures. In pharaonic times many of its symbols, such as the sphinx and winged solar disk, were adopted by other cultures across the Mediterranean and Near East, as were some of its deities, such as Bes. Some of these connections are difficult to trace. The Greek concept of Elysium may have derived from the Egyptian vision of the afterlife. In late antiquity, the Christian conception of Hell was most likely influenced by some of the imagery of the Duat. Biblical accounts of Jesus and Mary may have been influenced by that of Isis and Osiris. Egyptian beliefs also influenced or gave rise to several esoteric belief systems developed by Greeks and Romans, who considered Egypt as a source of mystic wisdom. Hermeticism, for instance, derived from the tradition of secret magical knowledge associated with Thoth.
Modern Times.
Traces of ancient beliefs remained in Egyptian folk traditions into modern times, but its influence on modern societies greatly increased with the French Campaign in Egypt and Syria in 1798 and their seeing the monuments and images. As a result of it, Westerners began to study Egyptian beliefs firsthand, and Egyptian religious motifs were adopted into Western art. Egyptian religion has since had a significant influence in popular culture. Due to continued interest in Egyptian belief, in the late 20th century, several new religious groups have formed based on different reconstructions of ancient Egyptian religion.

</doc>
<doc id="10332" url="https://en.wikipedia.org/wiki?curid=10332" title="Educational psychology">
Educational psychology

Educational psychology is the branch of psychology concerned with the scientific study of human learning. The study of learning processes, from both cognitive and behavioral perspectives, allows researchers to understand individual differences in intelligence, cognitive development, affect, motivation, self-regulation, and self-concept, as well as their role in learning. The field of educational psychology relies heavily on quantitative methods, including testing and measurement, to enhance educational activities related to instructional design, classroom management, assessment, which serve to facilitate learning processes in various educational settings across the lifespan.
Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. It is also informed by neuroscience. Educational psychology in turn informs a wide range of specialities within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks.
The field of educational psychology involves the study of memory, conceptual processes, and individual differences (via cognitive psychology) in conceptualizing new strategies for learning processes in humans. Educational psychology has been built upon theories of Operant conditioning, functionalism, structuralism, constructivism, humanistic psychology, Gestalt psychology, and information processing.
Educational psychology has seen rapid growth and development as a profession in the last twenty years. School psychology began with the concept of intelligence testing leading to provisions for special education students, who could not follow the regular classroom curriculum in the early part of the 20th century. However, "School Psychology" itself has built a fairly new profession based upon the practices and theories of several psychologists among many different fields. Educational Psychologists are working side by side with psychiatrists, social workers, teachers, speech and language therapists, and counselors in attempt to understand the questions being raised when combining behavioral, cognitive, and social psychology in the classroom setting.
History.
Early years.
Educational psychology is a fairly new and growing field of study. Though it can date back as early as the days of Plato and Aristotle, it was not identified as a specific practice. It was unknown that everyday teaching and learning in which individuals had to think about individual differences, assessment, development, the nature of a subject being taught, problem solving, and transfer of learning was the beginning to the field of educational psychology. These topics are important to education and as a result it is important to understanding human cognition, learning, and social perception.
Plato and Aristotle.
Educational psychology dates back to the time of Aristotle and Plato. Plato and Aristotle researched individual differences in the field of education, training of the body and the cultivation of psycho-motor skills, the formation of good character, the possibilities and limits of moral education. Some other educational topics they spoke about were the effects of music, poetry, and the other arts on the development of individual, role of teacher, and the relations between teacher and student. Plato saw knowledge as an innate ability, which evolves through experience and understanding of the world. Such a statement has evolved into a continuing argument of nature vs. nurture in understanding conditioning and learning today. Aristotle observed the phenomenon of "association." His four laws of association included succession, contiguity, similarity, and contrast. His studies examined recall and facilitated learning processes.
John Locke.
John Locke followed by contrasting Plato's theory of innate learning processes. Rather, he introduced the term "tabula rasa" meaning "blank slate." Locke explained that learning was primarily understood through experience only, and we were all born without knowledge. Locke introduced this idea as "empiricism," or the understanding that knowledge is only built on learning and experience.
Before 1890.
Philosophers of education such as Juan Vives, Johann Pestalozzi, Friedrich Fröbel, and Johann Herbart had examined, classified and judged the methods of education centuries before the beginnings of psychology in the late 1800s.
Juan Vives.
Juan Vives (1493–1540) proposed induction as the method of study and believed in the direct observation and investigation of the study of nature. His studies focus of humanistic learning, which opposed scholasticism and was influenced by a variety of sources including philosophy, psychology, politics,religion, and history. He was one of the first to emphasize that the location of the school is important to learning. He suggested that the school should be located away from disturbing noises; the air quality should be good and there should be plenty of food for the students and teachers. Vives emphasized the importance of understanding individual differences of the students and suggested practice as an important tool for learning.
Vives introduced his educational ideas in his writing, "De anima et vita" in 1538. In this publication, Vives explores moral philosophy as a setting for his educational ideals; with this, he explains that the different parts of the soul (similar to that of Aristotle's ideas) are each responsible for different operations, which function distinctively. The first book covers the different "souls": "The Vegatative Soul;" this is the soul of nutrition, growth, and reproduction, "The Sensitive Soul," which involves the five external senses; "The Cogitative soul," which includes internal senses and cognitive facilities. The second book involves functions of the rational soul: mind, will, and memory. Lastly, the third book explains the analysis of emotions.
Johann Pestalozzi.
Johann Pestalozzi (1746–1827), a Swiss educational reformer, emphasized the child rather than the content of the school. Pestalozzi fostered an educational reform backed by the idea that early education was crucial for children, and could be manageable for mothers. Eventually, this experience with early education would lead to a "wholesome person characterized by morality." Pestalozzi has been acknowledged for opening institutions for education, writing books for mother's teaching home education, and elementary books for students, mostly focusing on the kindergarten level. In his later years, he published teaching manuals and methods of teaching.
During the time of The Enlightenment, Pestalozzi's ideals introduced "educationalisation." This created the bridge between social issues and education by introducing the idea of social issues to be solved through education. Horlacher describes the most prominent example of this during The Enlightenment to be "improving agricultural production methods."
Johann Herbart.
Johann Herbart (1776–1841) is considered the father of educational psychology. He believed that learning was influenced by interest in the subject and the teacher. He thought that teachers should consider the students' existing mental sets—what they already know—when presenting new information or material. Herbart came up with what are now known as the formal steps. The 5 steps that teachers should use are:
1890–1920.
William James.
The period of 1890–1920 is considered the golden era of educational psychology where aspirations of the new discipline rested on the application of the scientific methods of observation and experimentation to educational problems. From 1840 to 1920 37 million people immigrated to the United States. This created an expansion of elementary schools and secondary schools. The increase in immigration also provided educational psychologists the opportunity to use intelligence testing to screen immigrants at Ellis Island. Darwinism influenced the beliefs of the prominent educational psychologists. Even in the earliest years of the discipline, educational psychologists recognized the limitations of this new approach. The pioneering American psychologist William James commented that: 
James is the father of psychology in America but he also made contributions to educational psychology. In his famous series of lectures "Talks to Teachers on Psychology", published in 1899 and now regarded as the first educational psychology textbook, James defines education as "the organization of acquired habits of conduct and tendencies to behavior". He states that teachers should "train the pupil to behavior" so that he fits into the social and physical world. Teachers should also realize the importance of habit and instinct. They should present information that is clear and interesting and relate this new information and material to things the student already knows about. He also addresses important issues such as attention, memory, and association of ideas.
Alfred Binet.
Alfred Binet published "Mental Fatigue" in 1898, in which he attempted to apply the experimental method to educational psychology. In this experimental method he advocated for two types of experiments, experiments done in the lab and experiments done in the classroom. In 1904 he was appointed the Minister of Public Education. This is when he began to look for a way to distinguish children with developmental disabilities. Binet strongly supported special education programs because he believed that "abnormality" could be cured. The Binet-Simon test was the first intelligence test and was the first to distinguish between "normal children" and those with developmental disabilities. Binet believed that it was important to study individual differences between age groups and children of the same age. He also believed that it was important for teachers to take into account individual students strengths and also the needs of the classroom as a whole when teaching and creating a good learning environment. He also believed that it was important to train teachers in observation so that they would be able to see individual differences among children and adjust the curriculum to the students. Binet also emphasized that practice of material was important. In 1916 Lewis Terman revised the Binet-Simon so that the average score was always 100. The test became known as the Stanford-Binet and was one of the most widely used tests of intelligence. Terman, unlike Binet, was interested in using intelligence test to identify gifted children who had high intelligence. In his longitudinal study of gifted children, who became known as the Termites, Terman found that gifted children become gifted adults.
Edward Thorndike.
Edward Thorndike (1874–1949) supported the scientific movement in education. He based teaching practices on empirical evidence and measurement. Thorndike developed the theory of instrumental conditioning or the law of effect. The law of effect states that associations are strengthened when it is followed by something pleasing and associations are weakened when followed by something not pleasing. He also found that learning is done a little at a time or in increments, learning is an automatic process and all the principles of learning apply to all mammals. Thorndike's research with Robert Woodworth on the theory of transfer found that learning one subject will only influence your ability to learn another subject if the subjects are similar. This discovery led to less emphasis on learning the classics because they found that studying the classics does not contribute to overall general intelligence. Thorndike was one of the first to say that individual differences in cognitive tasks were due to how many stimulus response patterns a person had rather than a general intellectual ability. He contributed word dictionaries that were scientifically based to determine the words and definitions used. The dictionaries were the first to take into consideration the users maturity level. He also integrated pictures and easier pronunciation guide into each of the definitions. Thorndike contributed arithmetic books based on learning theory. He made all the problems more realistic and relevant to what was being studied, not just to improve the general intelligence. He developed tests that were standardized to measure performance in school related subjects. His biggest contribution to testing was the CAVD intelligence test which used a multidimensional approach to intelligence and the first to use a ratio scale. His later work was on programmed instruction, mastery learning and computer-based learning:
John Dewey.
John Dewey (1859–1952) had a major influence on the development of progressive education in the United States. He believed that the classroom should prepare children to be good citizens and facilitate creative intelligence. He pushed for the creation of practical classes that could be applied outside of a school setting. He also thought that education should be student-oriented, not subject-oriented. For Dewey, education was a social experience that helped bring together generations of people. He stated that students learn by doing. He believed in an active mind that was able to be educated through observation, problem solving and enquiry. In his 1910 book "How We Think", he emphasizes that material should be provided in a way that is stimulating and interesting to the student since it encourages original thought and problem solving. He also stated that material should be relative to the student's own experience.
Jean Piaget.
Jean Piaget (1896–1980) developed the theory of cognitive development. The theory stated that intelligence developed in four different stages. The stages are the sensorimotor stage from birth to 2 years old, the preoperational state from 2 years old to 7 years old, the concrete operational stage from 7 years old to 10 years old, and formal operational stage from 11 years old and up. He also believed that learning was constrained to the child's cognitive development. Piaget influenced educational psychology because he was the first to believe that cognitive development was important and something that should be paid attention to in education. Most of the research on Piagetian theory was carried out by American educational psychologists.
1920–present.
The number of people receiving a high school and college education increased dramatically from 1920 to 1960. Because very few jobs were available to teens coming out of eighth grade, there was an increase in high school attendance in the 1930s. The progressive movement in the United State took off at this time and led to the idea of progressive education. John Flanagan, an educational psychologist, developed tests for combat trainees and instructions in combat training. In 1954 the work of Kenneth Clark and his wife on the effects of segregation on black and white children was influential in the Supreme Court case Brown v. Board of Education. From the 1960s to present day, educational psychology has switched from a behaviorist perspective to a more cognitive based perspective because of the influence and development of cognitive psychology at this time.
Jerome Bruner.
Jerome Bruner is notable for integrating Piaget's cognitive approaches into educational psychology. He advocated for discovery learning where teachers create a problem solving environment that allows the student to question, explore and experiment. In his book "The Process of Education" Bruner stated that the structure of the material and the cognitive abilities of the person are important in learning. He emphasized the importance of the subject matter. He also believed that how the subject was structured was important for the student's understanding of the subject and it is the goal of the teacher to structure the subject in a way that was easy for the student to understand. In the early 1960s Bruner went to Africa to teach math and science to schoolchildren, which influenced his view as schooling as a cultural institution. Bruner was also influential in the development of MACOS, Man a Course of Study, which was an educational program that combined anthropology and science. The program explored human evolution and social behavior. He also helped with the development of the head start program. He was interested in the influence of culture on education and looked at the impact of poverty on educational development.
Benjamin Bloom.
Benjamin Bloom (1913–1999) spent over 50 years at the University of Chicago, where he worked in the department of education. He believed that all students can learn. He developed taxonomy of educational objectives. The objectives were divided into three domains: cognitive, affective, and psychomotor. The cognitive domain deals with how we think. It is divided into categories that are on a continuum from easiest to more complex. The categories are knowledge or recall, comprehension application, analysis, synthesis and evaluation. The affective domain deals with emotions and has 5 categories. The categories are receiving phenomenon, responding to that phenomenon, valuing, organization, and internalizing values. The psychomotor domain deals with the development of motor skills, movement and coordination and has 7 categories, that also goes from simplest to complex. The 7 categories of the psychomotor domain are perception, set, guided response, mechanism, complex overt response, adaptation, and origination. The taxonomy provided broad educational objectives that could be used to help expand the curriculum to match the ideas in the taxonomy. The taxonomy is considered to have a greater influence internationally than in the United States. Internationally, the taxonomy is used in every aspect of education from training of the teachers to the development of testing material. Bloom believed in communicating clear learning goals and promoting an active student. He thought that teachers should provide feedback to the students on their strengths and weaknesses. Bloom also did research on college students and their problem solving processes. He found that they differ in understanding the basis of the problem and the ideas in the problem. He also found that students differ in process of problem solving in their approach and attitude toward the problem.
Nathaniel Gage.
Nathaniel Gage is an important figure in educational psychology as his research focused on improving teaching and understanding the processes involved in teaching. He edited the book "Handbook of Research on Teaching" (1963), which helped develop early research in teaching and educational psychology. Gage founded the Stanford Center for Research and Development in Teaching, which contributed research on teaching as well as influencing the education of important educational psychologists.
Perspectives.
Behavioral.
Applied behavior analysis, a research-based science utilizing behavioral principles of operant conditioning, is effective in a range of educational settings. For example, teachers can alter student behavior by systematically rewarding students who follow classroom rules with praise, stars, or tokens exchangeable for sundry items. Despite the demonstrated efficacy of awards in changing behavior, their use in education has been criticized by proponents of self-determination theory, who claim that praise and other rewards undermine intrinsic motivation. There is evidence that tangible rewards decrease intrinsic motivation in specific situations, such as when the student already has a high level of intrinsic motivation to perform the goal behavior. But the results showing detrimental effects are counterbalanced by evidence that, in other situations, such as when rewards are given for attaining a gradually increasing standard of performance, rewards enhance intrinsic motivation. Many effective therapies have been based on the principles of applied behavior analysis, including pivotal response therapy which is used to treat autism spectrum disorders.
Cognitive.
Among current educational psychologists, the cognitive perspective is more widely held than the behavioral perspective, perhaps because it admits causally related mental constructs such as traits, beliefs, memories, motivations and emotions. Cognitive theories claim that memory structures determine how information is perceived, processed, stored, retrieved and forgotten. Among the memory structures theorized by cognitive psychologists are separate but linked visual and verbal systems described by Allan Paivio's dual coding theory. Educational psychologists have used dual coding theory and cognitive load theory to explain how people learn from multimedia presentations.
The spaced learning effect, a cognitive phenomenon strongly supported by psychological research, has broad applicability within education. For example, students have been found to perform better on a test of knowledge about a text passage when a second reading of the passage is delayed rather than immediate (see figure). Educational psychology research has confirmed the applicability to education of other findings from cognitive psychology, such as the benefits of using mnemonics for immediate and delayed retention of information.
Problem solving, according to prominent cognitive psychologists, is fundamental to learning. It resides as an important research topic in educational psychology. A student is thought to interpret a problem by assigning it to a schema retrieved from long-term memory. A problem students run into while reading is called "activation." This is when the student's representations of the text are present during working memory. This causes the student to read through the material without absorbing the information and being able to retain it. When working memory is absent from the readers representations of the working memory they experience something called "deactivation." When deactivation occurs, the student has an understanding of the material and is able to retain information. If deactivation occurs during the first reading, the reader does not need to undergo deactivation in the second reading. The reader will only need to reread to get a "gist" of the text to spark their memory. When the problem is assigned to the wrong schema, the student's attention is subsequently directed away from features of the problem that are inconsistent with the assigned schema. The critical step of finding a mapping between the problem and a pre-existing schema is often cited as supporting the centrality of analogical thinking to problem solving.
Cognitive View of Intelligence.
Each person has an individual profile of characteristics, abilities and challenges that result from predisposition, learning and development. These manifest as individual differences in intelligence, creativity, cognitive style, motivation and the capacity to process information, communicate, and relate to others. The most prevalent disabilities found among school age children are attention deficit hyperactivity disorder (ADHD), learning disability, dyslexia, and speech disorder. Less common disabilities include intellectual disability, hearing impairment, cerebral palsy, epilepsy, and blindness.
Although theories of intelligence have been discussed by philosophers since Plato, intelligence testing is an invention of educational psychology, and is coincident with the development of that discipline. Continuing debates about the nature of intelligence revolve on whether intelligence can be characterized by a single factor known as general intelligence, multiple factors (e.g., Gardner's theory of multiple intelligences), or whether it can be measured at all. In practice, standardized instruments such as the Stanford-Binet IQ test and the WISC are widely used in economically developed countries to identify children in need of individualized educational treatment. Children classified as gifted are often provided with accelerated or enriched programs. Children with identified deficits may be provided with enhanced education in specific skills such as phonological awareness. In addition to basic abilities, the individual's personality traits are also important, with people higher in conscientiousness and hope attaining superior academic achievements, even after controlling for intelligence and past performance.
Developmental.
Developmental psychology, and especially the psychology of cognitive development, opens a special perspective for educational psychology. This is so because education and the psychology of cognitive development converge on a number of crucial assumptions. First, the psychology of cognitive development defines human cognitive competence at successive phases of development. Education aims to help students acquire knowledge and develop skills which are compatible with their understanding and problem-solving capabilities at different ages. Thus, knowing the students' level on a developmental sequence provides information on the kind and level of knowledge they can assimilate, which, in turn, can be used as a frame for organizing the subject matter to be taught at different school grades. This is the reason why Piaget's theory of cognitive development was so influential for education, especially mathematics and science education. In the same direction, the neo-Piagetian theories of cognitive development suggest that in addition to the concerns above, sequencing of concepts and skills in teaching must take account of the processing and working memory capacities that characterize successive age levels.
Second, the psychology of cognitive development involves understanding how cognitive change takes place and recognizing the factors and processes which enable cognitive competence to develop. Education also capitalizes on cognitive change, because the construction of knowledge presupposes effective teaching methods that would move the student from a lower to a higher level of understanding. Mechanisms such as reflection on actual or mental actions vis-à-vis alternative solutions to problems, tagging new concepts or solutions to symbols that help one recall and mentally manipulate them are just a few examples of how mechanisms of cognitive development may be used to facilitate learning.
Finally, the psychology of cognitive development is concerned with individual differences in the organization of cognitive processes and abilities, in their rate of change, and in their mechanisms of change. The principles underlying intra- and inter-individual differences could be educationally useful, because knowing how students differ in regard to the various dimensions of cognitive development, such as processing and representational capacity, self-understanding and self-regulation, and the various domains of understanding, such as mathematical, scientific, or verbal abilities, would enable the teacher to cater for the needs of the different students so that no one is left behind.
Constructivist.
Constructivism is a category of learning theory in which emphasis is placed on the agency and prior "knowing" and experience of the learner, and often on the social and cultural determinants of the learning process. Educational psychologists distinguish individual (or psychological) constructivism, identified with Piaget's theory of cognitive development, from social constructivism. A dominant influence on the latter type is Lev Vygotsky's work on sociocultural learning, describing how interactions with adults, more capable peers, and cognitive tools are internalized to form mental constructs. Elaborating on Vygotsky's theory, Jerome Bruner and other educational psychologists developed the important concept of instructional scaffolding, in which the social or information environment offers supports for learning that are gradually withdrawn as they become internalized.
Conditioning and learning.
To understand the characteristics of learners in childhood, adolescence, adulthood, and old age, educational psychology develops and applies theories of human development. Often represented as stages through which people pass as they mature, developmental theories describe changes in mental abilities (cognition), social roles, moral reasoning, and beliefs about the nature of knowledge.
For example, educational psychologists have conducted research on the instructional applicability of Jean Piaget's theory of development, according to which children mature through four stages of cognitive capability. Piaget hypothesized that children are not capable of abstract logical thought until they are older than about 11 years, and therefore younger children need to be taught using concrete objects and examples. Researchers have found that transitions, such as from concrete to abstract logical thought, do not occur at the same time in all domains. A child may be able to think abstractly about mathematics, but remain limited to concrete thought when reasoning about human relationships. Perhaps Piaget's most enduring contribution is his insight that people actively construct their understanding through a self-regulatory process.
Piaget proposed a developmental theory of moral reasoning in which children progress from a naïve understanding of morality based on behavior and outcomes to a more advanced understanding based on intentions. Piaget's views of moral development were elaborated by Kohlberg into a stage theory of moral development. There is evidence that the moral reasoning described in stage theories is not sufficient to account for moral behavior. For example, other factors such as modeling (as described by the social cognitive theory of morality) are required to explain bullying.
Rudolf Steiner's model of child development interrelates physical, emotional, cognitive, and moral development in developmental stages similar to those later described by Piaget.
Developmental theories are sometimes presented not as shifts between qualitatively different stages, but as gradual increments on separate dimensions. Development of epistemological beliefs (beliefs about knowledge) have been described in terms of gradual changes in people's belief in: certainty and permanence of knowledge, fixedness of ability, and credibility of authorities such as teachers and experts. People develop more sophisticated beliefs about knowledge as they gain in education and maturity.
Motivation.
Motivation is an internal state that activates, guides and sustains behavior. Motivation can have several impacting effects on how students learn and how they behave towards subject matter: 
Educational psychology research on motivation is concerned with the volition or will that students bring to a task, their level of interest and intrinsic motivation, the personally held goals that guide their behavior, and their belief about the causes of their success or failure. As intrinsic motivation deals with activities that act as their own rewards, extrinsic motivation deals with motivations that are brought on by consequences or punishments. A form of attribution theory developed by Bernard Weiner describes how students' beliefs about the causes of academic success or failure affect their emotions and motivations. For example, when students attribute failure to lack of ability, and ability is perceived as uncontrollable, they experience the emotions of shame and embarrassment and consequently decrease effort and show poorer performance. In contrast, when students attribute failure to lack of effort, and effort is perceived as controllable, they experience the emotion of guilt and consequently increase effort and show improved performance.
The self-determination theory (SDT) was developed by psychologists Edward Deci and Richard Ryan. SDT focuses on the importance of intrinsic and extrinsic motivation in driving human behavior and posits inherent growth and development tendencies. It emphasizes the degree to which an individual's behavior is self-motivated and self-determined. When applied to the realm of education, the self-determination theory is concerned primarily with promoting in students an interest in learning, a value of education, and a confidence in their own capacities and attributes.
Motivational theories also explain how learners' goals affect the way they engage with academic tasks. Those who have "mastery goals" strive to increase their ability and knowledge. Those who have "performance approach goals" strive for high grades and seek opportunities to demonstrate their abilities. Those who have "performance avoidance" goals are driven by fear of failure and avoid situations where their abilities are exposed. Research has found that mastery goals are associated with many positive outcomes such as persistence in the face of failure, preference for challenging tasks, creativity and intrinsic motivation. Performance avoidance goals are associated with negative outcomes such as poor concentration while studying, disorganized studying, less self-regulation, shallow information processing and test anxiety. Performance approach goals are associated with positive outcomes, and some negative outcomes such as an unwillingness to seek help and shallow information processing.
Locus of control is a salient factor in the successful academic performance of students. During the 1970s and '80s, Cassandra B. Whyte did significant educational research studying locus of control as related to the academic achievement of students pursuing higher education coursework. Much of her educational research and publications focused upon the theories of Julian B. Rotter in regard to the importance of internal control and successful academic performance. Whyte reported that individuals who perceive and believe that their hard work may lead to more successful academic outcomes, instead of depending on luck or fate, persist and achieve academically at a higher level. Therefore, it is important to provide education and counseling in this regard.
Technology.
Instructional design, the systematic design of materials, activities and interactive environments for learning, is broadly informed by educational psychology theories and research. For example, in defining learning goals or objectives, instructional designers often use a taxonomy of educational objectives created by Benjamin Bloom and colleagues. Bloom also researched mastery learning, an instructional strategy in which learners only advance to a new learning objective after they have mastered its prerequisite objectives. Bloom discovered that a combination of mastery learning with one-to-one tutoring is highly effective, producing learning outcomes far exceeding those normally achieved in classroom instruction. Gagné, another psychologist, had earlier developed an influential method of task analysis in which a terminal learning goal is expanded into a hierarchy of learning objectives connected by prerequisite relationships.
The following list of technological resources incorporate computer-aided instruction and intelligence for educational psychologists and their students: 
Technology is essential to the field of educational psychology, not only for the psychologist themselves as far as testing, organization, and resources, but also for students. Educational Psychologists whom reside in the K- 12 setting focus the majority of their time with Special Education students. It has been found that students with disabilities learning through technology such as IPad applications and videos are more engaged and motivated to learn in the classroom setting. Liu et al. explain that learning-based technology allows for students to be more focused, and learning is more efficient with learning technologies. The authors explain that learning technology also allows for students with social- emotional disabilities to participate in distance learning.
Applications.
Teaching.
Research on classroom management and pedagogy is conducted to guide teaching practice and form a foundation for teacher education programs. The goals of classroom management are to create an environment conducive to learning and to develop students' self-management skills. More specifically, classroom management strives to create positive teacher–student and peer relationships, manage student groups to sustain on-task behavior, and use counseling and other psychological methods to aid students who present persistent psychosocial problems.
Introductory educational psychology is a commonly required area of study in most North American teacher education programs. When taught in that context, its content varies, but it typically emphasizes learning theories (especially cognitively oriented ones), issues about motivation, assessment of students' learning, and classroom management. A developing gives more detail about the educational psychology topics that are typically presented in preservice teacher education.
Counseling.
Training.
In order to become an educational psychologist, students can complete an undergraduate degree in their choice. They then must go to graduate school to study education psychology, counseling psychology, and/ or school counseling. Most students today are also receiving their doctorate degrees in order to hold the "psychologist" title.Educational psychologists work in a variety of settings. Some work in university settings where they carry out research on the cognitive and social processes of human development, learning and education. Educational psychologists may also work as consultants in designing and creating educational materials, classroom programs and online courses.Educational psychologists who work in k–12 school settings (closely related are school psychologists in the US and Canada) are trained at the master's and doctoral levels. In addition to conducting assessments, school psychologists provide services such as academic and behavioral intervention, counseling, teacher consultation, and crisis intervention. However, school psychologists are generally more individual-oriented towards students.
Employment outlook.
Employment for psychologists in the United States is expected to grow faster than most occupations through the year 2014, with anticipated growth of 18–26%. One in four psychologists are employed in educational settings. In the United States, the median salary for psychologists in primary and secondary schools is US$58,360 as of May 2004.
In recent decades the participation of women as professional researchers in North American educational psychology has risen dramatically.
Methods of research.
Educational psychology, as much as any other field of psychology heavily relies on a balance of pure observation and quantitative methods in psychology. The study of education generally combines the studies of history, sociology, and ethics with theoretical approaches. Smeyers and Depaepe explain that historically, the study of education and child rearing have been associated with the interests of policymakers and practitioners within the educational field, however, the recent shift to sociology and psychology has opened the door for new findings in education as a social science. Now being its own academic discipline, educational psychology has proven to be helpful for social science researchers.
Quantitative research is the backing to most observable phenomenon in psychology. This involves observing, creating, and understanding a distribution of data based upon the studies subject matter. Researchers use particular variables to interpret their data distributions from their research and employ statistics as a way of creating data tables and analyzing their data. Psychology has moved from the "common sense" reputations initially posed by Thomas Reid to the methodology approach comparing independent and dependent variables through natural observation, experiments, or combinations of the two. Though results are still, with statistical methods, objectively true based upon significance variables or p- values.
Further reading.
Among the most prominent journals in educational psychology are:

</doc>
<doc id="10333" url="https://en.wikipedia.org/wiki?curid=10333" title="EFTPOS">
EFTPOS

EFTPOS (pronounced ) — electronic funds transfer at point of sale — is an electronic payment system involving electronic funds transfers based on the use of payment cards, such as debit or credit cards, at payment terminals located at points of sale. In Australia and New Zealand it is also the brand name of a specific system used for such payments. The Australian and New Zealand systems are country specific and do not interconnect. EFTPOS technology originated in the United States in 1981 and was quickly adopted by other countries.
Debit and credit cards are embossed plastic cards complying with ISO/IEC 7810 ID-1 standard. The cards have an embossed bank card number conforming with the ISO/IEC 7812 numbering standard.
History.
EFTPOS technology originated in the United States in 1981 and was rolled out in 1982. Initially, a number of nationwide systems were set up, such as "Interlink", which were limited to participating correspondent banking relationships, not being linked to each other. Consumers and merchants were slow to accept it, and there was minimal marketing. As a result, growth and market penetration of EFTPOS was minimal up to the turn of the century. Since 2002 the use of EFTPOS has grown significantly, and it has become the standard payment method, displacing the use of cash. Subsequently, networks facilitating the process of money transfer and payment settlement between the consumer and the merchant grew from a small number of nationwide systems to the majority of payment processing transactions. For EFTPOS, US based systems allow the use of debit cards or credit cards.
In a short time, other countries adopted the EFTPOS technology, but these systems too were limited to the national borders. Each country adopted various interbank co-operative models. In New Zealand, Bank of New Zealand started issuing EFTPOS debit cards in 1985 with the first merchant terminals being installed in petrol stations. In Australia, the major Australian banks started issuing debit or EFTPOS cards (each under a different brand name) starting in 1986 and merchants started installing EFTPOS terminals at the same time. Debit cards issued by all banks could be used at all EFTPOS terminals nationally, but debit cards issued in other countries could not. Prior to 1986, the Australian banks organized a widespread uniform credit card, called Bankcard, which had been in existence since 1974. There was a dispute between the banks whether Bankcard (or credit cards in general) should be permitted into the proposed EFTPOS system. At that time several banks were actively promoting MasterCard and Visa credit cards. Store cards and proprietary cards were shut out of the new system.
In recent years, MasterCard and Visa have introduced a debit card which is widely accepted internationally. International transactions are generally in the local currency, requiring a currency exchange by the card company to the currency of the primary account. Other charges may also apply.
Australia.
In Australia, debit and credit cards are the most common non-cash payment methods at “points of sale” (POS) or via ATMs. Not all merchants provide eftpos facilities, but those who wish to accept eftpos payments must enter an agreement with one of the seven merchant service providers, which rent an eftpos terminal to the merchant. The eftpos system in Australia is managed by eftpos Payments Australia Ltd, which also sets the EFTPOS interchange fee. For credit cards to be accepted by a merchant a separate agreement must be entered into with each credit card company, each of which has its own flexible merchant fee rate.
The clearing arrangements for eftpos are regulated by Australian Payments Clearing Association (APCA). The system for ATM and eftpos interchanges is called Consumer Electronic Clearing System (CECS) also called CS3. CECS required authorisations from the Australian Competition and Consumer Commission (ACCC), which was obtained in 2001 and reaffirmed in 2009. ATM and EFTPOS clearances are the made under individual bilateral arrangements between the institutions involved.
Debit cards.
Australian financial institutions provide their customers a plastic card, which can be used as a debit card or as an ATM card, and sometimes as a credit card. The card merely provides the means by which a customer's linked bank or other accounts can be accessed using an EFTPOS terminal or ATM. These cards can also be used on some vending machines and other automatic payment mechanisms, such as ticket vending machines. Australian debit cards cannot be used for online and telephone banking transactions, unless they are also a credit card.
Each Australian bank has given a different name to its debit cards, such as:
Some banks offer alternative debit card facilities to their customers using the Visa or MasterCard clearance system. For example, St George Bank offers a Visa Debit Card, as does the National Australia Bank. The main difference with regular debit cards is that these cards can be used outside Australia where the respective credit card is accepted.
Those merchants that enter the eftpos payment system must accept debit cards issued by any Australian bank, and some also accept various credit cards and other cards. Some merchants set minimum transaction amounts for eftpos transactions, which can be different for debit and credit card transactions. Some merchant impose a surcharge on the use of eftpos. These can vary between merchants and on the type of card being used, and generally are not imposed on debit card transactions, and widely not on MasterCard and a Visa credit card transactions.
A feature of a debit card is that an eftpos transaction will only be accepted if there is an available credit balance in the bank cheque or savings account linked to the card.
Australian debit cards normally cannot be used outside Australia. They can only be used outside Australia if they carry the MasterCard/Maestro/Cirrus or Visa/Plus or other similar logos, in which case the non-Australian transaction will be processed through those transaction systems. Similarly, non-Australian debit and credit cards can only be used at Australian eftpos terminals or ATMs if they have these logos or the MasterCard or Visa logos. Diners Club and/or American Express cards will be accepted only if the merchant has an agreement with those card companies. The Discover card is accepted in Australia as a Diners Club card [http://uat2.dinersclub.com.au/merchants.htm].
In addition, credit card companies issue prepaid cards which act like generic gift cards, which are anonymous and not linked to any bank accounts. These cards are accepted by merchants who accept credit cards and are processed through the eftpos terminal in the same way as credit cards.
Cash out.
A number of merchants permit customers using a debit card to withdraw cash as part of the EFTPOS transaction. In Australia, this facility (known as debit card cashback in many other countries) is known as "cash out". For the merchant, cash out is a way of reducing their net cash takings, saving on banking of cash. There is no additional cost to the merchant in providing cash out because banks charge a merchant a debit card transaction fee per EFTPOS transaction, and not on the transaction value. Cash out is a facility provided by the merchant, and not the bank, so the merchant can limit or vary how much cash can be withdrawn at a time, or suspend the facility at any time. When available, cash out is convenient for the customer, who can bypass having to visit a bank branch or ATM. Cash out is also cheaper for the customer, since only one bank transaction is involved. For people in some remote areas, cash out may be the only way they can withdraw cash from their personal accounts. However, most merchants who provide the facility set a relatively low limit on cash out, generally $50, and some also charge for the service. Some merchants in Australia only allow cash out with the purchase of goods; other merchants allow cash out whether or not customers buy any goods. Cash out is not available in association with credit card sales because on credit card transactions the merchant is charged a percentage commission based on the transaction value, and also because cash withdrawals are treated differently from purchase transactions by the credit card company. (However, though inconsistent with a merchant's agreement with each credit card company, the merchant may treat a cash withdrawal as part of an ordinary credit card sale.)
Cardholder verification.
EFTPOS transactions involving a debit, credit or prepaid card can be authenticated in a number of ways. The oldest system requires the printing and signing of a receipt, with the merchant verifying the signature on the receipt against the signature on the card. It is not very common in Australia for a merchant to require a cardholder to provide photo identification. More recently, authentication was by a personal identification number (PIN) that was entered into a PIN pad at the terminal and transmitted to the bank for verification. However, financial institutions are in the process of rolling out smart cards with integrated circuits ("chips") that will enable verification of the PIN at the EFTPOS terminal. PIN management is governed by international standard ISO 9564. At ATMs, only PIN verification is available, and all new credit cards are now issued with PINs regardless of whether or not they have a chip. In 2014, banks have indicated that the option for a customer to sign receipts is to be phased out, and that only PIN authentication is to be available. For some merchants, transactions below a specific threshold value (up to $100) can be approved without authentication (either signature or PIN).
As a further security measure, if a user enters an incorrect PIN three times, the card may be locked out of EFTPOS and require reactivation over the phone or at a bank branch. In the case of an ATM, the card will not be returned, and the cardholder will need to visit the branch to retrieve the card, or request a new card to be issued.
All debit cards now have a magnetic stripe on which is encoded the card's service codes, consisting of three-digit values. These codes are used to convey instructions to merchant terminals on how a card should be processed. The first digit indicates if a card can be used internationally or is valid for domestic use only. It is also used to signal if the card is chip-enabled. The second digit indicates if the transaction must be sent online for authorization always or if transactions that are below floor limit can take place without authorization. The third digit is used to indicate the preferred card verification method (e.g. PIN) and the environment where the card can be used (e.g. at point of sale only). Merchant terminals are required to recognize and act on service codes or send all transactions for online authorization.
Contactless smart card.
In the late 2000s, MasterCard and Visa introduced contactless smart debit cards under the brand names MasterCard PayPass and Visa payWave. These payments are made using electronic payment networks separate from the regular eftpos payment networks, and is an alternative to the previous swipe or chip systems. These networks are operated by MasterCard and Visa, and not by the banks as is the eftpos network, through eftpos Payments Australia Limited (ePAL).
These cards are based on EMV technology and contain a RFID chip and antenna loop embedded in the plastic of the card. To pay using this system, a customer passes the card within 4 cm of a reader at a merchant checkout. Using this method, for transactions under $100, the customer does not need to authenticate his or her identity by PIN entry or signature, as on a regular eftpos machine. For transactions over $100, PIN verification is required.
The facility is only available for cards branded with the MasterCard PayPass or Visa payWave logos, indicating that they have the system-permitted embedded chip. ANZ has launched ATM solution based on Visa payWave in 2015, where the consumer tap the card on a reader at the ATM and insert a PIN to finalize the cash withdrawals, and not all merchants offer the facility. Bank debit cards and other credit cards do not currently offer a contactless payment facility. ePAL is developing a contactless payment system for debit cards based on EMV technology as well as an extension of debit cards for use for on-line transactions, and a mobile payment system.
History.
The name and logo for eftpos in Australia were originally owned by the National Australia Bank and were trade marks from 1986 until 1991. The major Australian banks started issuing debit or eftpos cards (each under a different brand name) starting in 1986 and merchants started installing eftpos terminals at the same time.
In April 2009, a company, “eftpos Payments Australia Ltd” (ePal) was formed to manage and promote the eftpos system in Australia. 
ePal regulation commenced in January 2011. The initial members of EFTPOS Payments Australia Ltd were:
In 2006 Commonwealth Bank and MasterCard ran a six-month trial of the contactless smart card system PayPass in Sydney and Wollongong, 
supplementing the traditional EFTPOS swipe or chip system. The system was rolled out across Australia in 2009; other systems being rolled out are Westpac Bank's MasterCard PayPass and Visa payWave branded cards.
In Australia, store cards have been excluded from participation in the eftpos and ATM systems. Consequently, several larger store accounts have entered into co-branding arrangements with credit card networks for the store-based accounts to be widely accepted. This was the case with Coles (previously, Coles-Myer) which co-branded with MasterCard, and David Jones which co-branded with American Express. Woolworths organized its credit card called Everyday Rewards (now Woolworths Money) which initially was partnered with credit provider HSBC Bank, but will change on 26 October 2014 to Macquarie Bank.
Usage.
As of December 2010, there were over 707,000 EFTPOS terminals in Australia and over 28,000 ATMs. Of the terminals, over 60,000 offered cash withdrawals. In 2010, 183 million transactions, worth A$12 billion, were made using Australian EFTPOS terminals per month.
In 2011, these figures increased to 750,000 terminals, with 325,000 individual businesses, processing over 2 billion transactions with combined value of approximately $131 billion for the year.
Network.
The EFT network in Australia is made up of seven proprietary networks in which peers have interchange agreements, making an effective single network. A merchant who wishes to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent the terminal to the merchant. All the merchant's EFTPOS transactions are processed through one of these gateways. Some of these peers are:
Other organisations may have peering agreements with the one or more of the central peers.
The network uses the AS 2805 protocol.
New Zealand.
EFTPOS is highly popular in New Zealand. The system is operated by two providers, Paymark Limited (formerly Electronic Transaction Services Limited) which processes 75% of all electronic transactions in New Zealand, and EFTPOS New Zealand. Although the term eftpos is popularly used to describe the system, EFTPOS is a trademark of EFTPOS New Zealand the smaller of the two providers. Both providers run an interconnected financial network that allows the processing of not only of debit cards at point of sale terminals but also credit cards and charge cards.
History.
The Bank of New Zealand introduced EFTPOS to New Zealand in 1985 through a pilot scheme with petrol stations.
In 1989 the system was officially launched and two providers owned by the major banks now run the system. The largest of the two providers, Paymark Limited (formerly Electronic Transaction Services Limited) is owned equally by ASB Bank, Westpac, Bank of New Zealand and ANZ Bank New Zealand (formerly ANZ National Bank). The second is operated by EFTPOS New Zealand which is fully owned by VeriFone Systems, following its sale by ANZ New Zealand in December 2012.
During July 2006 the five billionth EFTPOS payment was processed, and at the start of 2012 the 10 billionth transaction was processed.
Usage.
EFTPOS is highly popular in New Zealand, and being used for about 60% of all retail transactions.In 2009, there were 200 EFTPOS transactions per person.
Paymark process over 900 million transactions (worth over NZ$48 billion) yearly. More than 75,000 merchants and over 110,000 EFTPOS terminals are connected to Paymark.

</doc>
<doc id="10334" url="https://en.wikipedia.org/wiki?curid=10334" title="Epistle to the Laodiceans">
Epistle to the Laodiceans

The Epistle to the Laodiceans is a possible lost letter of Paul the Apostle, the original existence of which is inferred from an instruction to the church in Colossae to send their letter to the church in Laodicea, and likewise obtain a copy of the letter "from Laodicea" (Greek "ek laodikeas" ἐκ Λαοδικείας).
Several ancient texts purporting to be the missing "Epistle to the Laodiceans" have been known to have existed, most of which are now lost. These were generally considered, both at the time and by modern scholarship, to be attempts to supply a forged copy of a lost document. The exception is a Latin "Epistle to the Laodiceans", which is actually a short compilation of verses from other Pauline epistles, principally Philippians, and on which scholarly opinion is divided as to whether it is the lost Marcionite forgery or alternatively an orthodox replacement of the Marcionite text. In either case it is generally considered a "clumsy forgery" and an attempt to seek to fill the "gap" suggested by Colossians 4:16.
Some ancient sources, such as Hippolytus, and some modern scholars consider that the epistle "from Laodicea" was never a lost epistle, but simply Paul recycling one of his other letters (the most common candidate is the contemporary Epistle to the Ephesians), just as he asks for the copying and forwarding of the Letter to Colossians to Laodicea.
The Colossians 4:16 mention.
Paul, the earliest known Christian author, wrote several letters (or epistles) in Greek to various churches. Paul apparently dictated all his epistles through a secretary (or amanuensis), but wrote the final few paragraphs of each letter by his own hand. Many survived and are included in the New Testament, but others are known to have been lost. The Epistle to the Colossians states "After this letter has been read to you, see that it is also read in the church of the Laodiceans and that you in turn read the letter from Laodicea." The last words can be interpreted as "letter written to the Laodiceans", but also "letter written from Laodicea". The New American Standard Bible (NASB) translates this verse in the latter manner, and translations in other languages such as the Dutch Statenvertaling translate it likewise: "When this letter is read among you, have it also read in the church of the Laodiceans; and you, for your part read my letter (that is coming) from Laodicea." Those who read here "letter written to the Laodiceans" presume that, at the time that the Epistle to the Colossians was written, Paul also had written an epistle to the Laodicean Church.
Identification with the Epistle to the Ephesians.
Some scholars have suggested that this refers to the canonical Epistle to the Ephesians, contending that it was a circular letter (an "encyclical") to be read to many churches in the Laodicean area. Others dispute this view.
The Marcionist Epistle to the Laodiceans.
According to the Muratorian fragment, Marcion's canon contained an epistle called the Epistle to the Laodiceans which is commonly thought to be a forgery written to conform to his own point of view. This is not at all clear, however, since none of the text survives. It is not known what this letter might have contained. Some scholars suggest it may have been the Vulgate epistle described below, while others believe it must have been more explicitly Marcionist in its outlook. Others believe it to be the Epistle to the Ephesians.
The Latin Vulgate Epistle to the Laodiceans.
For centuries some Western Latin Bibles used to contain a small Epistle from Paul to the Laodiceans. The oldest known Bible copy of this epistle is in a Fulda manuscript written for Victor of Capua in 546. It is mentioned by various writers from the fourth century onwards, notably by Pope Gregory the Great, to whose influence may ultimately be due the frequent occurrence of it in Bibles written in England; for it is commoner in English Bibles than in others. John Wycliffe included Paul's letter to the Laodiceans in his Bible translation from the Latin to English. However this epistle is not without controversy because there is no evidence of a Greek text. It contains almost no doctrine, teachings, or narrative not found elsewhere, and its exclusion from the Biblical canon has little effect.
The text was almost unanimously considered pseudepigraphal when the Christian Biblical canon was decided upon, and does not appear in any Greek copies of the Bible at all, nor is it known in Syriac or other versions. Jerome, who wrote the Latin Vulgate translation, wrote in the 4th century, "it is rejected by everyone". However, it evidently gained a certain degree of respect. It appeared in over 100 surviving early Latin copies of the Bible. According to "Biblia Sacra iuxta vulgatum versionem", there are Latin Vulgate manuscripts containing this epistle dating between the 6th and 12th century, including Latin manuscripts F (Codex Fuldensis), M, Q, B, D (Ardmachanus), C, and Lambda.
The apocryphal epistle is generally considered a transparent attempt to supply this supposed lost sacred document. Some scholars suggest that it was created to offset the popularity of the Marcionite epistle.
Wilhelm Schneemelcher's standard work, "New Testament Apocrypha" (Chapter 14 "Apostolic Pseudepigrapha") includes a section on the Latin Epistle to the Laodiceans and a translation of the Latin text.

</doc>
<doc id="10335" url="https://en.wikipedia.org/wiki?curid=10335" title="Extermination camp">
Extermination camp

The German extermination camps or death camps were designed and built by Nazi Germany during World War II (1939–45) to systematically kill millions, primarily by gassing, but also in mass executions and through extreme work under starvation conditions.
The idea of mass extermination with the use of stationary facilities built exclusively for that purpose was a result of earlier Nazi experimentation with the chemically manufactured poison gas during the secretive Action T4 euthanasia programme against German mentally and physically disabled. The technology was adapted, expanded and applied in wartime to unsuspecting victims of many ethnic and national groups; the Jews however were the primary targets accounting for over 90 percent of the extermination camp death toll. This genocide of the Jewish people of Europe was the Third Reich's "Final Solution to the Jewish question". It is now collectively known as the Holocaust.
Extermination camps were also set up by the fascist Ustaše regime of the Independent State of Croatia allied with Germany, carrying out genocidal policy between 1941 and 1945 against Serbs, Jews, Roma and its political opponents.
Background.
The top secret Action T4 euthanasia programme – derived from the ideas of racial hygiene – was initiated by the "SS" in 1939 in order to eliminate the "life unworthy of life" (), a Nazi designation for people who had no right to life. After the invasion of Poland in 1939, the experience gained in the killing of hospital patients led to the creation of extermination camps for the implementation of the Final Solution. By then, the Jews were already confined to new ghettos and interned in Nazi concentration camps along with other targeted groups, including Roma, and the Soviet POWs. The Nazi "Endlösung der Judenfrage" (The Final Solution of the Jewish Question) based on systematic killing of Europe's Jews by gassing began during Operation Reinhard, after the onset of Nazi-Soviet war of June 1941. The adoption of the gassing technology by Nazi Germany was preceded by a wave of hands-on killings carried out by the SS "Einsatzgruppen", who followed the Wehrmacht army during Operation Barbarossa on the Eastern Front.
The camps designed specifically for the mass gassings of Jews were established in the months following the Wannsee Conference chaired by Reinhard Heydrich in January 1942 in which the principle was made clear that the Jews of Europe were to be exterminated. Responsibility for the logistics were to be executed by the programme administrator, Adolf Eichmann.
On 13 October 1941, the SS and Police Leader Odilo Globocnik stationing in Lublin received an oral order from "Reichsführer-SS" Heinrich Himmler – anticipating the fall of Moscow – to start immediate construction work on the killing centre at Bełżec in the General Government territory of occupied Poland. Notably, the order preceded the Wannsee Conference by three months, but the gassings at Kulmhof north of Łódź using gas vans began already in December, under "Sturmbannführer" Herbert Lange. The camp at Bełżec was operational by March 1942, with leadership brought in from Germany under the guise of Organisation Todt (OT). Auschwitz concentration camp was fitted with brand new gassing bunkers in March 1942. Majdanek had them built in September.
Definition.
The Nazis distinguished between extermination and concentration camps, although the terms "extermination camp" ("Vernichtungslager") and "death camp" ("Todeslager") were interchangeable, each referring to camps whose primary function was genocide. "Todeslagers" were designed specifically for the systematic killing of people delivered en masse by the Holocaust trains. The executioners did not expect the prisoners to survive more than a few hours beyond arrival at Belzec, Sobibór, and Treblinka. The Reinhard extermination camps were under Globocnik's direct command; each of them was run by 20 to 35 men from the "SS-Totenkopfverbände" branch of the "Schutzstaffel", augmented by about one hundred Trawnikis – auxiliaries mostly from Soviet Ukraine, and up to one thousand "Sonderkommando" slave labourers each. The Jewish men, women and children were delivered from the ghettos for "special treatment" in an atmosphere of terror by uniformed police battalions from both, Orpo and Schupo.
Death camps differed from concentration camps located in Germany proper, such as Bergen-Belsen, Oranienburg, Ravensbrück, and Sachsenhausen, which were prison camps set up prior to World War II for people defined as 'undesirable'. From March 1936, all Nazi concentration camps were managed by the "SS-Totenkopfverbände" (the Death Head Units, SS-TV), who operated extermination camps from 1941 as well. An SS anatomist, Dr. Johann Kremer, after witnessing the gassing of victims at Birkenau, wrote in his diary on 2 September 1942: "Dante's Inferno seems to me almost a comedy compared to this. They don't call Auschwitz the camp of annihilation for nothing!" The distinction was evident during the Nuremberg trials, when Dieter Wisliceny (a deputy to Adolf Eichmann) was asked to name the "extermination" camps, and he identified Auschwitz and Majdanek as such. Then, when asked "How do you classify the camps Mauthausen, Dachau, and Buchenwald?" he replied, "They were normal concentration camps, from the point of view of the department of Eichmann."
Irrespective of round-ups for extermination camps, the Nazis abducted millions of foreigners for slave labour in other types of camps, which provided perfect cover for the extermination programme. Prisoners represented about a quarter of the total workforce of the Reich, with mortality rates exceeding 75 percent due to starvation, disease, exhaustion, executions and physical brutality.
History.
In the early years of World War II, the Jews were sent primarily to forced labour camps, but from 1942 onward they were mostly deported to the extermination camps under the guise of "resettlement". For political and logistical reasons, the most infamous Nazi German killing factories were built in occupied Poland, where most of the intended victims lived; Poland had the greatest Jewish population in Nazi controlled Europe. On top of that, the new death camps outside the prewar borders of the Third Reich proper could be kept secret from the German civil populace.
Pure extermination camps.
Operationally, there were two types of death camps. Initially, gas vans producing poisonous carbon monoxide exhaust fumes were developed in the occupied Soviet Union (USSR) and at the Chełmno extermination camp in occupied Poland, before being used elsewhere.
The camps at Treblinka, Bełżec, and Sobibór were constructed during Operation Reinhard (October 1941 – November 1943), for the extermination of Poland's Jews. Prisoners were promptly killed upon arrival. Initially, the camps used carbon monoxide gas chambers; at first, the corpses were buried, but then incinerated atop pyres. Later, gas chambers and crematoria were built in Treblinka and Belzec; Zyklon-B was used in Belzec.
Whereas the Auschwitz II (Auschwitz–Birkenau) and Majdanek camps were parts of a labor camp complex, the Operation Reinhard camps and the Chełmno camp were exclusively for the quick extermination of many people (primarily Jews) within hours of their arrival. Some able-bodied prisoners delivered to the death camp were not immediately killed, but were forced into labor units ("Sonderkommando") to work at the extermination process, removing corpses from the gas chambers and burning them. Because the extermination camps were physically small (only several hundred metres long and wide) and equipped with minimal housing and support installations, the Nazis deceived the prisoners upon their arrival, telling them that they were at a temporary transit stop, and soon would continue to an "Arbeitslager" (work camp) farther east.
Concentration and extermination camps.
At the camps of Operation Reinhard including Bełżec, Sobibór, and Treblinka trainloads of prisoners were destined for immediate death in gas chambers built for that purpose. The mass killing facilities were developed at the Majdanek concentration camp, and at Auschwitz II-Birkenau at about the same time. In most other camps prisoners were selected for slave labor first; they were kept alive on starvation rations and made available to work wherever the rulers required. Auschwitz, Majdanek, and Jasenovac were retrofitted with Zyklon-B gas chambers and crematoria as the time went on, remaining operational until war's end in 1945. The Maly Trostenets extermination camp in the USSR initially operated as a prison camp. It became an extermination camp later in the war with victims undergoing mass shootings. This was supplemented with exhaust fume gassing in a van from October 1943.
The Sajmište concentration camp operated by the Nazis in Yugoslavia had a gas van stationed for use from March to June 1942. Once the industrial killings were completed, the van was returned to Berlin. After a refit the van was then sent to Maly Trostinets for use at the camp there. The Janowska concentration camp near Lwow in occupied eastern Poland implemented a selection process. Some prisoners were assigned to work before death. Others were either transported to Belzec or victims of mass shootings on two slopes in the Piaski sand-hills behind the camp. The Warsaw concentration camp was a camp complex of the German concentration camps, possibly including an extermination camp located in German-occupied Warsaw. The various details regarding the camp are very controversial and remain subject of historical research and public debate.
Other means of extermination.
With the support of Nazi Germany and fascist Italy on 10 April 1941 the Independent State of Croatia (NDH) was established, and adopted parallel racial and political doctrines. Death camps were established by the fascist Ustaše government for contributing to the Nazi "final solution" to the "Jewish problem", the killing of Roma people, and the elimination of political opponents, but most significantly to achieve the destruction of the Serbian population of the NDH. The degree of cruelty with which the Serb population was persecuted by Ustaše men shocked even the Germans.
The Jadovno concentration camp was located in a secluded area about from the town of Gospić. It held thousands of Serbs and Jews over a period of 122 days from May to August 1941. Prisoners were usually but by no means exclusively killed by being pushed into deep ravines located near the camp.
The Jasenovac concentration camp complex of five sub-camps replaced Jadovno. Many inmates arriving at Jasenovac were scheduled for systematic extermination. An important criterion for selection was the duration of a prisoner's anticipated detention. Strong men capable of labour and sentenced to less than three years of incarceration were allowed to live. All inmates with indeterminate sentences or sentences of three years or more were immediately scheduled for execution, regardless of their fitness. Some of the mass executions were mechanical following Nazi methodology. Others were performed manually utilising tools such as mallets and agricultural knives and often in conjunction with throwing the victims off the end of a ramp into the River Sava.
Robert Conquest argues that the regime in labour camps in the Soviet Union, principally those in Siberia, was designed to bring about the death of prisoners after extracting 3–6 months' labour from them. Aleksandr Solzhenitsyn concurs with him.
Extermination procedure.
Heinrich Himmler visited the outskirts of Minsk in 1941 to witness a mass shooting. He was told by the commanding officer there that the shootings were proving psychologically damaging to those being asked to pull the triggers. Thus Himmler knew another method of mass killing was required. After the war, the diary of the Auschwitz Commandant, Rudolf Höss, revealed that psychologically "unable to endure wading through blood any longer", many "Einsatzkommandos" – the killers – either went mad or killed themselves.
The Nazis had first used gassing with carbon monoxide cylinders to kill 70,000 disabled people in Germany in what they called a 'euthanasia programme' to disguise that mass murder was taking place. Despite the lethal effects of carbon monoxide, this was seen as unsuitable for use in the East due to the cost of transporting the carbon monoxide in cylinders.
Each extermination camp operated differently, yet each had designs for quick and efficient industrialized killing. While Höss was away on an official journey in late August 1941 his deputy, Karl Fritzsch, tested out an idea. At Auschwitz clothes infected with lice were treated with crystallised prussic acid. The crystals were made to order by the IG Farben chemicals company for which the brand name was Zyklon-B. Once released from their container, Zyklon-B crystals in the air released a lethal cyanide gas. Fritzch tried out the effect of Zyklon B on Soviet POWs, who were locked up in cells in the basement of the bunker for this experiment. Höss on his return was briefed and impressed with the results and this became the camp strategy for extermination as it was also to be at Majdanek. Besides gassing, the camp guards continued killing prisoners via mass shooting, starvation, torture, etc.
Gassings.
SS "Obersturmführer" Kurt Gerstein, of the Institute for Hygiene of the Waffen-SS, during the war told a Swedish diplomat of life in a death camp, of how, on 19 August 1942, he arrived at Belzec extermination camp (which was equipped with carbon monoxide gas chambers) and was shown the unloading of 45 train cars filled with 6,700 Jews, many already dead, but the rest were marched naked to the gas chambers, where:
Auschwitz Camp Commandant Rudolf Höss reported that the first time Zyklon B gas was used on the Jews, many suspected they were to be killed – despite having been deceived into believing they were to be deloused and then returned to the camp. As a result, the Nazis identified and isolated "difficult individuals" who might alert the prisoners, and removed them from the mass – lest they incite revolt among the deceived majority of prisoners en route to the gas chambers. The "difficult" prisoners were led to a site out of view to be killed off discreetly.
A prisoner "Sonderkommando" (Special Detachment) effected in the processes of extermination; they encouraged the Jews to undress without a hint of what was about to happen. They accompanied them into the gas chambers outfitted to appear as shower rooms (with nonworking water nozzles, and tile walls); and remained with the victims until just before the chamber door closed. To psychologically maintain the "calming effect" of the delousing deception, an SS man stood at the door until the end. The "Sonderkommando" talked to the victims about life in the camp to pacify the suspicious ones, and hurried them inside; to that effect, they also assisted the aged and the very young in undressing.
To further persuade the prisoners that nothing harmful was happening, the "Sonderkommando" deceived them with small talk about friends or relations who had arrived in earlier transports. Many young mothers hid their infants beneath their piled clothes fearing that the delousing "disinfectant" might harm them. Camp Commandant Höss reported that the "men of the Special Detachment were particularly on the look-out for this", and encouraged the women to take their children into the "shower room". Likewise, the "Sonderkommando" comforted older children who might cry "because of the strangeness of being undressed in this fashion".
Yet, not every prisoner was deceived by such psychological tactics; Commandant Höss spoke of Jews "who either guessed, or knew, what awaited them, nevertheless ... found the courage to joke with the children, to encourage them, despite the mortal terror visible in their own eyes". Some women would suddenly "give the most terrible shrieks while undressing, or tear their hair, or scream like maniacs"; the "Sonderkommando" immediately took them away for execution by shooting. In such circumstances, others, meaning to save themselves at the gas chamber's threshold, betrayed the identities and "revealed the addresses of those members of their race still in hiding".
Once the door of the filled gas chamber was sealed, pellets of Zyklon B were dropped through special holes in the roof. Regulations required that the Camp Commandant supervise the preparations, the gassing (through a peephole), and the aftermath looting of the corpses. Commandant Höss reported that the gassed victims "showed no signs of convulsion"; the Auschwitz camp physicians attributed that to the "paralyzing effect on the lungs" of the Zyklon-B gas, which killed "before" the victim began suffering convulsions.
As a matter of political training, some high-ranked Nazi Party leaders and SS officers were sent to Auschwitz–Birkenau to witness the gassings; Höss reported that "all were deeply impressed by what they saw ... some ... who had previously spoken most loudly, about the necessity for this extermination, fell silent once they had actually seen the 'final solution of the Jewish problem'." As the Auschwitz Camp Commandant Rudolf Höss justified the extermination by explaining the need for "the iron determination with which we must carry out Hitler's orders"; yet saw that even "Eichmann, who certainly [was tough enough, had no wish to change places with me."
Corpse disposal.
After the gassings, the "Sonderkommando" removed the corpses from the gas chambers, then extracted any gold teeth. Initially, the victims were buried in mass graves, but were later cremated during "Sonderaktion 1005" in all camps of Operation Reinhard.
The "Sonderkommando" were responsible for burning the corpses in the pits, stoking the fires, draining surplus body fat and turning over the "mountain of burning corpses... so that the draught might fan the flames" wrote Commandant Höss in his memoir while in the Polish custody. He was impressed by the diligence of prisoners from the so-called Special Detachment who carried out their duties despite their being well aware that they, too, would meet exactly the same fate in the end. At the Lazaret killing station they held the sick so they would never see the gun while being shot. They did it "in such a matter-of-course manner that they might, themselves, have been the exterminators" wrote Höss. He further said that the men ate and smoked "even when engaged in the grisly job of burning corpses which had been lying for some time in mass graves." They occasionally encountered the corpse of a relative, or saw them entering the gas chambers. According to Höss they were obviously shaken by this but "it never led to any incident." He mentioned the case of a "Sonderkommando" who found the body of his wife, yet continued to drag corpses along "as though nothing had happened."
At Auschwitz, the corpses were incinerated in crematoria and the ashes either buried, scattered, or dumped in the river. At Sobibór, Treblinka, Belzec, and Chełmno, the corpses were incinerated on pyres. The efficiency of industrialised killing at Auschwitz-Birkenau led to the construction of three buildings with crematoria designed by specialists from Topf und Söhne. They handled the body disposal around the clock, day and night, and yet the speed of gassing required that some corpses burn in an open air pit also.
Ustaše camps.
The United States Holocaust Memorial Museum (USHMM) in Washington, DC, presently estimates that the Ustaša regime in Croatia murdered between 77,000 and 99,000 people at their own Jasenovac concentration camp between 1941 and 1945. The Jasenovac Memorial Site quotes a similar figure of between 80,000 and 100,000 victims. The television documentary, "Nazi Collaborators" on Dinko Sakic stated that over 300,000 people were killed at Jasenovac. The Jasenovac mechanical means of mass killing included the use initially of gas vans and later Zyklon B in stationary gas chambers. The Jasenovac guards have also been reported to have cremated living inmates in the crematorium. A notable difference of the Ustaše camps as compared to the German "SS" camps was the widespread use of manual methods in the mass killings. These involved instruments such as mallets and agricultural knives which evolved to often to be done in a manner where still alive victims were thrown off the end of a ramp into the River Sava.
The estimates for the Jadovno concentration camp generally offer a range of 10,000 – 72,000 deaths at the camp over a period of 122 days (May to August 1941). Most commonly Jadovno victims were bound together in a line and the first few victims were murdered with rifle butts or other objects. Afterwards, an entire row of inmates were pushed into the ravine. Hand grenades were hurled inside in order to finish off the victims. Dogs would also be thrown in to feed on the wounded and the dead. Inmates were also killed by machine gunfire, as well as with knives and blunt objects.
Death toll.
The estimated total number of people executed in the Nazi camps in the table below is over three million:
Dismantlement and attempted concealment.
The Nazis attempted to either partly or completely dismantle the extermination camps in order to hide any evidence that people had been murdered. This was an attempt to conceal not only the extermination process but also the buried remains. As a result of the secretive Sonderaktion 1005, camps were dismantled by commandos of condemned prisoners, records destroyed, and mass graves were dug up. Some extermination camps that remained uncleared of evidence were liberated by Soviet troops, who had different standards of documentation and openness than the Western allies.
The notable exception in this is Majdanek. Majdanek was ordered to share a similar fate to the other camps by the Nazi leadership in their attempt to cover up the murderous events. Majdanek though was captured nearly intact. This was due to the rapid advance of the Soviet Red Army during Operation Bagration preventing the SS from destroying most of its infrastructure. Commandant Anton Thernes failed in his task of removing incriminating evidence of war crimes.
Commemoration.
In the post-war period the government of the People's Republic of Poland created monuments at the extermination camp sites. These early monuments mentioned no ethnic, religious, or national particulars of the Nazi victims. The extermination camps sites have been accessible to everyone in recent decades. They are popular destinations for visitors from all over the world, especially the most infamous Nazi death camp, Auschwitz near the town of Oświęcim. In the early 1990s, the Jewish Holocaust organisations debated with the Polish Catholic groups about "What religious symbols of martyrdom are appropriate as memorials in a Nazi death camp such as Auschwitz?" The Jews opposed the placement of Christian memorials such as the Auschwitz cross near Auschwitz I where mostly Poles were killed. The Jewish victims of the Holocaust were mostly killed at Auschwitz II Birkenau.
The March of the Living is organized in Poland annually since 1988. Marchers come from countries as diverse as Estonia, New Zealand, Panama, and Turkey.
The camps and Holocaust denial.
Holocaust deniers are people and organizations who assert that the Holocaust did not occur, or that it did not occur in the historically recognized manner and extent.
Extermination camp research is difficult because of extensive attempts by the SS and Nazi regime to conceal the existence of the extermination camps. The existence of the extermination camps is firmly established by testimonies of camp survivors and Final Solution perpetrators, material evidence (the remaining camps, etc.), Nazi photographs and films of the killings, and camp administration records.
Holocaust deniers often start by pointing out legitimate public misconceptions about the extermination camps. For example, widely published images in America were mostly of typhoid victims and Soviet POWs at the Buchenwald and Dachau concentration camps – the first to be liberated by American troops and the most available imagery in America. In early news reports and for years afterwards these images were often used by the news media somewhat inaccurately in conjunction with descriptions of extermination camps and Jewish suffering. Holocaust deniers, after pointing out such common errors, put it forward as evidence that extermination camps did not exist and the limited evidence about them is mostly a hoax arising out of a deliberate Jewish conspiracy.
Holocaust denial has been thoroughly discredited by scholars and is a criminal offence in nine European countries: Austria, Belgium, the Czech Republic, France, Germany, Lithuania, the Netherlands, Poland, and Switzerland, all of whom besides Switzerland were at one point part of or occupied by the Third Reich.

</doc>
<doc id="10336" url="https://en.wikipedia.org/wiki?curid=10336" title="Enterprise">
Enterprise

Enterprise (occasionally used with the archaic spelling Enterprize) may refer to:

</doc>
<doc id="10338" url="https://en.wikipedia.org/wiki?curid=10338" title="Excommunication">
Excommunication

Excommunication is an institutional act of religious censure used to deprive, suspend, or limit membership in a religious community or to restrict certain rights within it, in particular receiving of the sacraments. Some Protestants use the term disfellowship instead.
The word "excommunication" means putting a specific individual or group out of communion. In some religions, excommunication includes spiritual condemnation of the member or group. Excommunication may involve banishment, shunning, and shaming, depending on the religion, the offense that caused excommunication, or the rules or norms of the religious community. The grave act is often revoked in response to sincere penance, which may be manifested through public recantation, sometimes through the Sacrament of Confession, piety, and/or through mortification of the flesh.
Christianity.
In Jesus says that an offended person should first draw the offender's fault to the offender's attention privately; then, if the offender refuses to listen, bring one or two others, that there may be more than a single witness to the charge; next, if the offender still refuses to listen, bring the matter before the church, and if the offender refuses to listen to the church, treat the offender as "a Gentile and a tax collector".
In , Paul writes to "mark those who cause divisions contrary to the doctrine which ye have learned and avoid them." Also, in , the writer advises believers that "whosoever transgresseth, and abideth not in the doctrine of Christ, hath not God. He that abideth in the doctrine of Christ, he hath both the Father and the Son. If there come any unto you, and bring not this doctrine, receive him not into "your" house [οἰκίαν, residence or abode, or "inmates of the house" (family)], neither bid him God speed: for he that biddeth him God speed is partaker of his evil deeds."
Catholic Church.
Within the Catholic Church, there are differences between the discipline of the majority Latin Church regarding excommunication and that of the Eastern Catholic Churches.
Latin Church.
In Latin Catholic canon law, excommunication is a rarely applied censure and thus a "medicinal penalty" intended to invite the person to change behaviour or attitude, repent, and return to full communion. It is not an "expiatory penalty" designed to make satisfaction for the wrong done, much less a "vindictive penalty" designed solely to punish: "excommunication, which is the gravest penalty of all and the most frequent, is always medicinal", and is "not at all vindictive".
Excommunication can be either "latae sententiae" (automatic, incurred at the moment of committing the offense for which canon law imposes that penalty) or "ferendae sententiae" (incurred only when imposed by a legitimate superior or declared as the sentence of an ecclesiastical court).
According to Bishop Thomas J. Paprocki, "excommunication does not expel the person from the Catholic Church, but simply forbids the excommunicated person from engaging in certain activities..." 
These activities are listed in Canon 1331 §1, and prohibit the individual from any ministerial participation in celebrating the sacrifice of the Eucharist or any other ceremonies of worship; celebrating or receiving the sacraments; or exercising any ecclesiastical offices, ministries, or functions. Therefore, any acts of ecclesiastical governance by the excommunicated person are invalidity, e.g., an excommunicated bishop can neither validly confirm nor ordain.
Under current Catholic canon law, excommunicates remain bound by ecclesiastical obligations such as attending Mass, even though they are barred from receiving the Eucharist and from taking an active part in the liturgy (reading, bringing the offerings, etc.). "Excommunicates lose rights, such as the right to the sacraments, but they are still bound to the obligations of the law; their rights are restored when they are reconciled through the remission of the penalty." They are urged to retain a relationship with the Church, as the goal is to encourage them to repent and return to active participation in its life.
These are the only effects for those who have incurred a "latae sententiae" excommunication. For instance, a priest may not refuse Communion publicly to those who are under an automatic excommunication, as long as it has not been officially declared to have been incurred by them, even if the priest knows that they have incurred it. On the other hand, if the priest knows that excommunication has been imposed on someone or that an automatic excommunication has been declared (and is no longer merely an undeclared automatic excommunication), he is forbidden to administer Holy Communion to that person. (see canon 915).
In the Catholic Church, excommunication is normally resolved by a declaration of repentance, profession of the Creed (if the offense involved heresy) and an Act of Faith, or renewal of obedience (if that was a relevant part of the offending act, i.e., an act of schism) by the excommunicated person and the lifting of the censure (absolution) by a priest or bishop empowered to do this. "The absolution can be in the internal (private) forum only, or also in the external (public) forum, depending on whether scandal would be given if a person were privately absolved and yet publicly considered unrepentant." Since excommunication excludes from reception of the sacraments, absolution from excommunication is required before absolution can be given from the sin that led to the censure. In many cases, the whole process takes place on a single occasion in the privacy of the confessional. For some more serious wrongdoings, absolution from excommunication is reserved to a bishop, another ordinary, or even the Pope. These can delegate a priest to act on their behalf.
During the Middle Ages, formal acts of public excommunication were sometimes accompanied by a ceremony wherein a bell was tolled (as for the dead), the Book of the Gospels was closed, and a candle snuffed out — hence the idiom "to condemn with bell, book, and candle." Such ceremonies are not held today, and instead are simply announced by the bishop and put into a written declaration.
Interdict is a censure similar to excommunication. It too excludes from ministerial functions in public worship and from reception of the sacraments, but not from the exercise of governance.
Eastern Catholic Churches.
In the Eastern Catholic Churches, excommunications is imposed only by decree, never incurred automatically by "latae sententiae" excommunication.
A distinction is made between minor and major excommunication.
Those on whom minor excommunication has been imposed are excluded from receiving the Eucharist and can also be excluded from participating in the Divine Liturgy. They can even be excluded from entering a church when divine worship is being celebrated there. The decree of excommunication must indicate the precise effect of the excommunication and, if required, its duration.
Those under major excommunication are in addition forbidden to receive not only the Eucharist but also the other sacraments, to administer sacraments or sacramentals, to exercise any ecclesiastical offices, ministries, or functions whatsoever, and any such exercise by them is null and void. They are to be removed from participation in the Divine Liturgy and any public celebrations of divine worship. They are forbidden to make use of any privileges granted to them and cannot be given any dignity, office, ministry, or function in the Church, they cannot receive any pension or emoluments associated with these dignities etc., and they are deprived of the right to vote or to be elected.
Eastern Orthodox churches.
In the Eastern Orthodox churches, excommunication is the exclusion of a member from the Eucharist. It is not expulsion from the churches. This can happen for such reasons as not having confessed within that year; excommunication can also be imposed as part of a penitential period. It is generally done with the goal of restoring the member to full communion. Before an excommunication of significant duration is imposed, the bishop is usually consulted. The Orthodox churches do have a means of expulsion, by pronouncing anathema, but this is reserved only for acts of serious and unrepentant heresy. As an example of this, the Second Council of Constantinople in 553, in its eleventh capitula, declared: "If anyone does not anathematize Arius, Eunomius, Macedonius, Apollinarius Nestorius, Eutyches and Origen, as well as their heretical books, and also all other heretics who have already been condemned and anathematized by the holy, catholic and apostolic church and by the four holy synods which have already been mentioned, and also all those who have thought or now think in the same way as the aforesaid heretics and who persist in their error even to death: let him be anathema."
Lutheranism.
Although Lutheranism technically has an excommunication process, some denominations and congregations do not use it. The Lutheran definition, in its earliest and most technical form, would be found in Martin Luther's Small Catechism, defined beginning at Questions No. 277-283, in "The Office of Keys." Luther endeavored to follow the process that Jesus laid out in the 18th chapter of the Gospel of Matthew. According to Luther, excommunication requires:
Beyond this, there is little agreement. Many Lutheran denominations operate under the premise that the entire congregation (as opposed to the pastor alone) must take appropriate steps for excommunication, and there are not always precise rules, to the point where individual congregations often set out rules for excommunicating laymen (as opposed to clergy). For example, churches may sometimes require that a vote must be taken at Sunday services; some congregations require that this vote be unanimous.
The Lutheran process, though rarely used, has created unusual situations in recent years due to its somewhat democratic excommunication process. One example was an effort to get serial killer Dennis Rader excommunicated from his denomination (the Evangelical Lutheran Church in America) by individuals who tried to "lobby" Rader's fellow church members into voting for his excommunication.
Anglican Communion.
Church of England.
The Church of England does not have any specific canons regarding how or why a member can be excommunicated, although it has a canon according to which ecclesiastical burial may be refused to someone "declared excommunicate for some grievous and notorious crime and no man to testify to his repentance".
Episcopal Church of the United States of America.
The ECUSA is in the Anglican Communion, and shares many canons with the Church of England which would determine its policy on excommunication.
Reformed view.
In the Reformed churches, excommunication has generally been seen as the culmination of church discipline, which is one of the three marks of the Church. The Westminster Confession of Faith sees it as the third step after "admonition" and "suspension from the sacrament of the Lord's Supper for a season." Yet, John Calvin argues in his "Institutes of the Christian Religion" that church censures do not "consign those who are excommunicated to perpetual ruin and damnation," but are designed to induce repentance, reconciliation and restoration to communion. Calvin notes, "though ecclesiastical discipline does not allow us to be on familiar and intimate terms with excommunicated persons, still we ought to strive by all possible means to bring them to a better mind, and recover them to the fellowship and unity of the Church."
At least one modern Reformed theologian argues that excommunication is not the final step in the disciplinary process. Jay E. Adams argues that in excommunication, the offender is still seen as a brother, but in the final step they become "as the heathen and tax collector" (Matthew 18:17). Adams writes, "Nowhere in the Bible is excommunication (removal from the fellowship of the Lord's Table, according to Adams) equated with what happens in step 5; rather, step 5 is called "removing from the midst, handing over to Satan," and the like."
Former Yale president and theologian, Jonathan Edwards, addresses the notion of excommunication as "removal from the fellowship of the Lord's Table" in his treatise entitled "The Nature and End of Excommunication". Edwards argues that "Particularly, we are forbidden such a degree of associating ourselves with (excommunicants), as there is in making them our guests at our tables, or in being their guests at their tables; as is manifest in the text, where we are commanded to have no company with them, no not to eat". Edwards insists, "That this respects not eating with them at the Lord's supper, but a common eating, is evident by the words, that the eating here forbidden, is one of the lowest degrees of keeping company, which are forbidden. Keep no company with such a one, saith the apostle, no not to eat — as much as to say, no not in so low a degree as to eat with him. But eating with him at the Lord's supper, is the very highest degree of visible Christian communion. Who can suppose that the apostle meant this: Take heed and have no company with a man, no not so much as in the highest degree of communion that you can have? Besides, the apostle mentions this eating as a way of keeping company which, however, they might hold with the heathen. He tells them, not to keep company with fornicators. Then he informs them, he means not with fornicators of this world, that is, the heathens; but, saith he, “if any man that is called a brother be a fornicator, etc. with such a one keep no company, no not to eat.” This makes it most apparent, that the apostle doth not mean eating at the Lord's table; for so, they might not keep company with the heathens, any more than with an excommunicated person."
Anabaptist tradition.
When believers were baptized and taken into membership of the church by Anabaptists, it was not only done as symbol of cleansing of sin but was also done as a public commitment to identify with Jesus Christ and to conform one's life to the teaching and example of Jesus as understood by the church. Practically, that meant membership in the church entailed a commitment to try to live according to norms of Christian behavior widely held by the Anabaptist tradition.
In the ideal, discipline in the Anabaptist tradition requires the church to confront a notoriously erring and unrepentant church member, first directly in a very small circle and, if no resolution is forthcoming, expanding the circle in steps eventually to include the entire church congregation. If the errant member persists without repentance and rejects even the admonition of the congregation, that person is excommunicated or excluded from church membership. Exclusion from the church is recognition by the congregation that this person has separated himself or herself from the church by way of his or her visible and unrepentant sin. This is done ostensibly as a final resort to protect the integrity of the church. When this occurs, the church is expected to continue to pray for the excluded member and to seek to restore him or her to its fellowship. There was originally no "inherent" expectation to shun (completely sever all ties with) an excluded member, however differences regarding this very issue led to early schisms between different Anabaptist leaders and those who followed them.
Amish.
Jakob Ammann, founder of the Amish sect, believed that the shunning of those under the ban should be systematically practiced among the Swiss Anabaptists as it was in the north and as was outlined in the Dordrecht Confession. Ammann's uncompromising zeal regarding this practice was one of the main disputes that led to the schism between the Anabaptist groups that became the Amish and those that eventually would be called Mennonite. Recently more moderate Amish groups have become less strict in their application of excommunication as a discipline. This has led to splits in several communities, an example of which is the Swartzetruber Amish who split from the main body of Old Order Amish because of the latter's practice of lifting the ban from members who later join other churches. In general, the Amish will excommunicate baptized members for failure to abide by their Ordnung (church rules) as it is interpreted by the local Bishop if certain repeat violations of the Ordnung occur.
Excommunication among the Old Order Amish results in shunning or "the Meidung", the severity of which depends on many factors, such as the family, the local community as well as the type of Amish. Some Amish communities cease shunning after one year if the person joins another church later on, especially if it is another Mennonite church. At the most severe, other members of the congregation are prohibited almost all contact with an excommunicated member including social and business ties between the excommunicant and the congregation, sometimes even marital contact between the excommunicant and spouse remaining in the congregation or family contact between adult children and parents.
Mennonites.
In the Mennonite Church excommunication is rare and is carried out only after many attempts at reconciliation and on someone who is flagrantly and repeatedly violating standards of behavior that the church expects. Occasionally excommunication is also carried against those who repeatedly question the church's behavior and/or who genuinely differ with the church's theology as well, although in almost all cases the dissenter will leave the church before any discipline need be invoked. In either case, the church will attempt reconciliation with the member in private, first one on one and then with a few church leaders. Only if the church's reconciliation attempts are unsuccessful, the congregation formally revokes church membership. Members of the church generally pray for the excluded member.
Some regional conferences (the Mennonite counterpart to dioceses of other denominations) of the Mennonite Church have acted to expel member congregations that have openly welcomed non-celibate homosexuals as members. This internal conflict regarding homosexuality has also been an issue for other moderate denominations, such as the American Baptists and Methodists.
The practice among Old Order Mennonite congregations is more along the lines of Amish, but perhaps less severe typically. An Old Order member who disobeys the "Ordnung" (church regulations) must meet with the leaders of the church. If a church regulation is broken a second time there is a confession in the church. Those who refuse to confess are excommunicated. However upon later confession, the church member will be reinstated. An excommunicated member is placed under the ban. This person is not banned from eating with their own family. Excommunicated persons can still have business dealings with church members and can maintain marital relations with a marriage partner, who remains a church member.
Hutterites.
The separatist, communal, and self-contained Hutterites also use excommunication and shunning as form of church discipline. Since Hutterites have communal ownership of goods, the effects of excommunication could impose a hardship upon the excluded member and family leaving them without employment income and material assets such as a home. However, often arrangements are made to provide material benefits to the family leaving the colony such as an automobile and some transition funds for rent, etc. One Hutterite colony in Manitoba (Canada) had a protracted dispute when leaders attempted to force the departure of a group that had been excommunicated but would not leave. About a dozen lawsuits in both Canada and the United States were filed between the various Hutterite factions and colonies concerning excommunication, shunning, the legitimacy of leadership, communal property rights, and fair division of communal property when factions have separated.
The Church of Jesus Christ of Latter-day Saints.
The Church of Jesus Christ of Latter-day Saints (LDS Church) practices excommunication as penalty for those who commit serious sins, "i.e.", actions that significantly impair the name or moral influence of the church or pose a threat to other people. According to the church leadership "Handbook", the purposes of church discipline are (1) to save the souls of transgressors, (2) to protect the innocent, and (3) to safeguard the purity, integrity, and good name of the church.
The LDS Church also practices the lesser sanctions of private counsel and caution, informal probation, formal probation, and disfellowshipment.
Disfellowshipped is used for serious sins that do not rise to the level of excommunication. Disfellowshipment denies some privileges but does not include a loss of church membership. Once disfellowshipped, persons may not take the sacrament or enter church temples, nor may they offer public prayers or sermons. Disfellowshipped persons may continue to attend most church functions and are permitted to wear temple garments, pay tithes and offerings, and participate in church classes if their conduct is orderly. Disfellowshipment typically lasts for one year, after which one may be reinstated as a member in good standing.
In the more grievous or recalcitrant cases, excommunication becomes a disciplinary option. Excommunication is generally reserved for what are seen as the most serious sins, including committing serious crimes such as murder, child abuse, and incest; committing adultery; involvement in or teaching of polygamy; involvement in homosexual conduct; apostasy; participation in an abortion; teaching false doctrine; or openly criticizing church leaders. A 2006 revision to the "Handbook" states that formally joining another church constitutes apostasy and is an excommunicable offense; however, merely attending another church does not constitute apostasy.
An excommunication can occur only after a formal disciplinary council. Formerly called a "church court," the councils were renamed to avoid focusing on guilt and instead to emphasize the availability of repentance.
The decision to excommunicate a Melchizedek priesthood holder is generally the province of the leadership of a stake. In such a disciplinary council, the stake presidency and stake high council attend. The twelve members of the high council are split in half: one group represents the member in question and is charged with "prevent insult or injustice"; the other group represents the church as a whole. The member under scrutiny is invited to attend the disciplinary proceedings, but the council can go forward without him. In making a decision, the leaders of the high council consult with the stake presidency, but the decision about which discipline is necessary is the stake president's alone. It is possible to appeal a decision of a stake disciplinary council to the church's First Presidency.
For females and for male members not initiated into the Melchizedek priesthood, a ward disciplinary council is held. In such cases, a bishop determines whether excommunication or a lesser sanction is warranted. He does this in consultation with his two counselors, with the bishop making the final determination after prayer. The decision of a ward disciplinary council can be appealed to the stake president.
The following list of variables serves as a general set of guidelines for when excommunication or lesser action may be warranted, beginning with those more likely to result in severe sanction:
Notices of excommunication may be made public, especially in cases of apostasy, where members could be misled; however, the specific reasons for individual excommunications are typically kept confidential and are seldom made public by church leadership.
Those who are excommunicated lose their church membership and the right to partake of the sacrament. Such persons are usually allowed to attend church meetings but participation is limited: they cannot offer public prayers or preach sermons and cannot enter temples. Excommunicated members are also barred from wearing or purchasing temple garments and from paying tithes. Excommunicated members may be re-baptized after a waiting period and sincere repentance, as judged by a series of interviews with church leaders.
Some critics have charged that LDS Church leaders have used the threat of excommunication to silence or punish church members and researchers who disagree with established policy and doctrine, who study or discuss controversial subjects, or who may be involved in disputes with local, stake leaders or general authorities; see, e.g., Brian Evenson, a former BYU professor and writer whose fiction came under criticism from BYU officials and LDS Leadership. Another notable case of excommunication from the LDS Church was the "September Six," a group of intellectuals and professors, five of whom were excommunicated and the sixth disfellowshipped.
However, church policy dictates that local leaders are responsible for excommunication, without influence from church headquarters. The church thus argues that this policy is evidence against any systematic persecution of scholars.
Jehovah's Witnesses.
Jehovah's Witnesses practice a form of excommunication, using the term "disfellowshipping", in cases where a member is believed to have unrepentantly committed one or more of several documented "serious sins". The practice is based on their interpretation of 1 Corinthians 5:11-13 ("quit mixing in company with anyone called a brother that is a fornicator or greedy person or an idolater or a reviler or a drunkard or an extortioner, not even eating with such a man...remove the wicked man from your midst") and 2 John 10 ("never receive him in your home or say a greeting to him"). They interpret these verses to mean that any baptized believer who engages in "gross sins" is to be expelled from the congregation and shunned.
When a member confesses to, or is accused of, a "serious sin", a "judicial committee" of at least three elders is formed. This committee investigates the case and determines the magnitude of the sin committed. If the person is deemed guilty of a disfellowshipping offense, the committee then decides, on the basis of the person's attitude and "works befitting repentance" (), whether the person is to be considered repentant. The "works" may include trying to correct the wrong, making apologies to any offended individuals, and compliance with earlier counsel. If deemed guilty but repentant, the person is not disfellowshipped but is formally "reproved" and has "restrictions" imposed, which preclude the individual from various activities such as presenting talks, offering public prayers or making comments at religious meetings. If the person is deemed guilty and unrepentant, he or she will be disfellowshipped. Unless an appeal is made within seven days, the disfellowshipping is made formal by an announcement at the congregation's next Service Meeting. Appeals are granted to determine if procedural errors are felt to have occurred that may have affected the outcome.
Disfellowshipping is a severing of friendly relationships between all Jehovah's Witnesses and the disfellowshipped person. Interaction with extended family is typically restricted to a minimum, such as presence at the reading of wills and providing essential care for the elderly. Within a household, typical family contact may continue, but without spiritual fellowship such as family Bible study and religious discussions. Parents of disfellowshipped minors living in the family home may continue to attempt to convince the child about the religion's teachings. Jehovah's Witnesses believe that this form of discipline encourages the disfellowshipped individual to conform to biblical standards and prevents the person from influencing other members of the congregation.
Along with breaches of the Witnesses' moral code, openly disagreeing with the teachings Jehovah's Witnesses is considered grounds for shunning. These persons are labeled as "apostates", and are described in Watch Tower Society literature as "mentally diseased". Descriptions of "apostates" appearing in the Witnesses literature have been the subject of investigation in the UK to determine if they violate religious hatred laws. Sociologist Andrew Holden claims many Witnesses who would otherwise defect because of disillusionment with the organization and its teachings, remain affiliated out of fear of being shunned and losing contact with friends and family members. Shunning employs what is known "as relational aggression" in psychological literature. When used by church members and member-spouse parents against excommunicant parents it contains elements of what psychologists call "parental alienation". Extreme shunning may cause trauma to the shunned (and to their dependents) similar to what is studied in the psychology of torture.
Disassociation is a form of shunning where a member expresses verbally or in writing that they do not wish to be associated with Jehovah's Witnesses, rather than for having committed any specific 'sin'. Elders may also decide that an individual has disassociated, without any formal statement by the individual, by actions such as accepting a blood transfusion, or for joining another religion or military organization. Individuals who are deemed by the elders to have disassociated are given no right of appeal.
Each year, congregation elders are instructed to consider meeting with disfellowshipped individuals to determine changed circumstances and encourage them to pursue reinstatement. Reinstatement is not automatic after a certain time period, nor is there a minimum duration; disfellowshipped persons may talk to elders at any time but must apply in writing to be considered for reinstatement into the congregation. Elders consider each case individually, and are instructed to ensure "that sufficient time has passed for the disfellowshipped person to prove that his profession of repentance is genuine." A judicial committee meets with the individual to determine their repentance, and if this is established, the person is reinstated into the congregation and may participate with the congregation in their formal ministry (such as house-to-house preaching), but is prohibited from commenting at meetings or holding any privileges for a period set by the judicial committee. If possible, the same judicial committee members who disfellowshipped the individual are selected for the reinstatement hearing. If the applicant is in a different area, the person will meet with a local judicial committee that will communicate with either the original judicial committee if available or a new one in the original congregation.
A Witness who has been formally reproved or reinstated cannot be appointed to any "special privilege of service" for at least one year. Serious sins involving child sex abuse permanently disqualify the sinner from appointment to any congregational "privilege of service", regardless of whether the sinner was convicted of any secular crime.
Christadelphians.
Similarly to many groups having their origins in the 1830s Restoration Movement, Christadelphians call their form of excommunication "disfellowshipping", though they do not practice "shunning". Disfellowshipping can occur for moral reasons, changing beliefs, or (in some ecclesias) for not attending communion (referred to as "the emblems" or "the breaking of bread").
In such cases, the person involved is usually required to discuss the issues. If they do not conform, the church ('meeting' or 'ecclesia') is recommended by the management committee ("Arranging Brethren") to vote on disfellowshipping the person. These procedures were formulated 1863 onwards by early Christadelphians, and then in 1883 codified by Robert Roberts in "A Guide to the Formation and Conduct of Christadelphian Ecclesias" (colloquially "The Ecclesial Guide"). However Christadelphians justify and apply their practice not only from this document but also from passages such as the exclusion in 1Co.5 and recovery in 2Co.2.
Christadelphians typically avoid the term "excommunication" which many associate with the Catholic Church; and may feel the word carries implications they do not agree with, such as undue condemnation and punishment, as well as failing to recognise the remedial intention of the measure.
In the case of adultery and divorce, the passage of time usually means a member can be restored if he or she wants to be. In the case of ongoing behaviour, cohabitation, homosexual activity, then the terms of the suspension have not been met.
The mechanics of "refellowship" follow the reverse of the original process; the individual makes an application to the "ecclesia", and the "Arranging Brethren" give a recommendation to the members who vote. If the "Arranging Brethren" judge that a vote may divide the ecclesia, or personally upset some members, they may seek to find a third party ecclesia which is willing to "refellowship" the member instead. According to the Ecclesial Guide a third party ecclesia may also take the initiative to "refellowship" another meeting's member. However this cannot be done unilaterally, as this would constitute heteronomy over the autonomy of the original ecclesia's members.
Society of Friends (Quakers).
Among many of the Society of Friends groups (Quakers) one is "read out of meeting" for behaviour inconsistent with the sense of the meeting. However it is the responsibility of each meeting, quarterly meeting, and yearly meeting, to act with respect to their own members. For example, during the Vietnam War many Friends were concerned about Friend Richard Nixon's position on war which seemed at odds with their beliefs; however, it was the responsibility of Nixon's own meeting, the East Whittier Meeting of Whittier, California, to act if indeed that meeting felt the leaning. They did not.
In the 17th century, before the founding of abolitionist societies, Friends who too forcefully tried to convince their coreligionists of the evils of slavery were read out of meeting. Benjamin Lay was read out of the Philadelphia Yearly Meeting for this. During the American Revolution over 400 Friends were read out of meeting for their military participation or support.
Iglesia ni Cristo.
The Iglesia ni Cristo (INC), practices expulsion which is similar to excommunication in Catholicism by expelling members which it deems to have gravely sinned or gone against the teachings and doctrines of the church. The Sanggunian, the church's council has the jurisdiction to expel members from the church. People who were expelled by the church are labeled as "tiwalag" in Tagalog. Among the offences by a member which may be grounds for expulsion is marrying a non-INC member and getting pregnant out of wedlock. An expelled member can be still re-admitted to the church by pledging obedience to the church and its values and teachings.
Unitarian-Universalism.
Unitarian Universalism, being a liberal religious group and a congregational denomination, has a wide diversity of opinions and sentiments. Nonetheless, Unitarian Universalists have had to deal with disruptive individuals. Congregations which had no policies on disruptive individuals have sometimes found themselves having to create such policies, up to (and including) expulsion.
By the late 1990s, several churches were using the West Shore UU Church's policy as a model. If someone is threatening, disruptive, or distracting from the appeal of the church to its membership, a church using this model has three recommended levels of response to the offending individual. While the first level involves dialogue between a committee or clergy member and the offender, the second and third levels involve expulsion, either from the church itself or a church activity.
Buddhism.
There is no direct equivalent to excommunication in Buddhism. However, in the Theravadan monastic community monks can be expelled from monasteries for heresy and/or other acts. In addition, the monks have four vows, called the four defeats, which are abstaining from sexual intercourse, stealing, murder, and refraining from lying about spiritual gains (e.g., having special power or ability to perform miracles). If even one is broken, the monk is automatically a layman again and can never become a monk in his or her current life.
Most Japanese Buddhist sects hold ecclesiastical authority over its followers and have their own rules for expelling members of the sangha, lay or bishopric. The lay Japanese Buddhist organization Sōka Gakkai was expelled from the Nichiren Shoshu sect in 1991 (1997).
Hinduism.
Hinduism has been too diverse to be seen as a monolithic religion, and with a conspicuous absence of any listed dogma or ecclesia (organised church), has no concept of excommunication and hence no Hindu may be ousted from the Hindu religion, although a person may easily lose caste status for a very wide variety of infringements of caste prohibitions. This may or may not be recoverable. However, some of the modern organized sects within Hinduism may practice something equivalent to excommunication today, by ousting a person from their own sect.
In medieval and early-modern times (and sometimes even now) in South Asia, excommunication from one's "caste" ("jati" or "varna") used to be practiced (by the caste-councils) and was often with serious consequences, such as abasement of the person's caste status and even throwing him into the sphere of the untouchables or bhangi. In the 19th century, a Hindu faced excommunication for going abroad, since it was presumed he would be forced to break caste restrictions and, as a result, become polluted.
After excommunication, it would depend upon the caste-council whether they would accept any form of repentance (ritual or otherwise) or not. Such current examples of excommunication in Hinduism are often more political or social rather than religious, for example the excommunication of lower castes for refusing to work as scavengers in Tamil Nadu.
An earlier example of excommunication in Hinduism is that of Shastri Yagnapurushdas, who voluntarily left and was later expelled from the Vadtal Gadi of the Swaminarayan Sampraday by the then Vadtal acharya in 1906. He went on to form his own institution, "Bochasanwasi Swaminarayan Sanstha" or "BSS" (now BAPS) claiming Gunatitanand Swami was the rightful spiritual successor to Swaminarayan.
Islam.
Excommunication as it exists in Christian faiths does not exist in Islam. However, under Sharia law, the penalty for apostasy in Islam is death. The nearest approximation is "takfir", a declaration that an individual or group is "kafir" (or "kuffar" in plural), a non-believer. This does not prevent an individual from taking part in any Islamic rite or ritual, and since the matter of whether a person is "kafir" is a rather subjective matter, a declaration of "takfir" is generally considered null and void if the target refutes it or if the Islamic community in which he or she lives refuses to accept it. 
"Takfir" has usually been practiced through the courts. More recently, cases have taken place where individuals have been considered kuffar. These decisions followed lawsuits against individuals, mainly in response to their writings that some have viewed as anti-Islamic. The most famous cases are of Salman Rushdie, Nasr Abu Zayd, and Nawal El-Saadawi. The repercussions of such cases have included divorce, since under traditional interpretations of Islamic law, Muslim women are not permitted to marry non-Muslim men.
However, "takfir" remains a highly contentious issue in Islam, primarily because there is no universally accepted authority in Islamic law. Indeed, according to classical commentators, the reverse seems to hold true, in that Muhammad reportedly equated the act of declaring someone a "kafir" itself to blasphemy if the accused individual maintained that he was a Muslim.
Ahmadiyya Muslim Community.
Ahmadiyya Muslim Community is a minority sect within Islam. The organization takes "disciplinary actions" against its members who are thought to be violating any religious teachings of the community. These disciplinary actions range from not accepting donations from the members, prohibiting them from taking part in the organizational activities of the community or excommunication in the most severe cases. Dancing on weddings, mixed gender gatherings on weddings and not following "Qaza" board (Community's juidicial board) judgements are a few examples of the actions that can lead to such disciplinary actions.
Judaism.
"Cherem" is the highest ecclesiastical censure in Judaism. It is the total exclusion of a person from the Jewish community. Except for cases in the Charedi community, "cherem" stopped existing after The Enlightenment, when local Jewish communities lost their political autonomy, and Jews were integrated into the gentile nations in which they lived. A "siruv" order, equivalent to a contempt of court, issued by a Rabbinical court may also limit religious participation.
Rabbinical conferences of movements do expel members from time to time, but sometimes choose the lesser penalty of censuring the offending rabbi. Between 2010 and 2015, the Reform Jewish Central Conference of American Rabbis expelled six rabbis, the Orthodox Jewish Rabbinical Council of America expelled three, and the Conservative Jewish Rabbinical Assembly expelled one, suspended three, and caused one to resign without eligibility for reinstatement. While the CCAR and RCA were relatively shy about their reasons for expelling rabbis, the RA was more open about its reasons for kicking rabbis out. Reasons for expulsion from the three conferences include sexual misconduct, failure to comply with ethics investigations, setting up conversion groups without the conference's approval, stealing money from congregations, other financial misconduct, and getting arrested.
Judaism, like Unitarian Universalism, tends towards congregationalism, and so decisions to exclude from a community of worship often depend on the congregation. Congregational bylaws sometimes enable the board of a synagogue to ask individuals to leave or not to enter.

</doc>
<doc id="10339" url="https://en.wikipedia.org/wiki?curid=10339" title="Electrochemical cell">
Electrochemical cell

An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or facilitating chemical reactions through the introduction of electrical energy. A common example of an electrochemical cell is a standard 1.5-volt cell meant for consumer use. This type of device is known as a single Galvanic cell. A "battery" consists of two or more cells, connected in either parallel or series pattern.
Half-cells.
An electrochemical cell consists of two half-cells. Each "half-cell" consists of an electrode and an electrolyte. The two half-cells may use the same electrolyte, or they may use different electrolytes. The chemical reactions in the cell may involve the electrolyte, the electrodes, or an external substance (as in fuel cells that may use hydrogen gas as a reactant). In a full electrochemical cell, species from one half-cell lose electrons (oxidation) to their electrode while species from the other half-cell gain electrons (reduction) from their electrode. 
A "salt bridge" (e.g., filter paper soaked in KNO3 or some other electrolyte) is often employed to provide ionic contact between two half-cells with different electrolytes, yet prevent the solutions from mixing and causing unwanted side reactions. An alternative to a salt bridge is to allow direct contact (and mixing) between the two half-cells, for example in simple electrolysis of water. 
As electrons flow from one half-cell to the other through an external circuit, a difference in charge is established. If no ionic contact were provided, this charge difference would quickly prevent the further flow of electrons. A salt bridge allows the flow of negative or positive ions to maintain a steady-state charge distribution between the oxidation and reduction vessels, while keeping the contents otherwise separate. Other devices for achieving separation of solutions are porous pots and gelled solutions. A porous pot is used in the Bunsen cell (right).
Equilibrium reaction.
Each half-cell has a characteristic voltage. Various choices of substances for each half-cell give different potential differences. Each reaction is undergoing an equilibrium reaction between different oxidation states of the ions: When equilibrium is reached, the cell cannot provide further voltage. In the half-cell that is undergoing oxidation, the closer the equilibrium lies to the ion/atom with the more positive oxidation state the more potential this reaction will provide. Likewise, in the reduction reaction, the closer the equilibrium lies to the ion/atom with the more "negative" oxidation state the higher the potential.
Cell potential.
The cell potential can be predicted through the use of electrode potentials (the voltages of each half-cell). These half-cell potentials are defined relative to the assignment of 0 volts to the standard hydrogen electrode (SHE). (See table of standard electrode potentials). The difference in voltage between electrode potentials gives a prediction for the potential measured. When calculating the difference in voltage, one must first rewrite the half-cell reaction equations to obtain a balanced oxidation-reduction equation.
Note that the cell potential does not change when the reaction is multiplied by a constant.
Cell potentials have a possible range of roughly zero to 6 volts. Cells using water-based electrolytes are usually limited to cell potentials less than about 2.5 volts, because the very powerful oxidizing and reducing agents that would be required to produce a higher cell potential tend to react with the water. Higher cell potentials are possible with cells using other solvents instead of water. For instance, lithium cells with a voltage of 3 volts are commonly available.
The cell potential depends on the concentration of the reactants, as well as their type. As the cell is discharged, the concentration of the reactants decreases, and the cell potential also decreases.

</doc>
<doc id="10340" url="https://en.wikipedia.org/wiki?curid=10340" title="Ecdysis">
Ecdysis

Ecdysis is the moulting of the cuticle in many invertebrates. This process of moulting is the defining feature of the clade Ecdysozoa, comprising the arthropods, nematodes, velvet worms, horsehair worms, tardigrades, and Cephalorhyncha. Since the cuticle of these animals typically forms a largely inelastic exoskeleton, it is shed during growth and a new, larger covering is formed. The remnants of the old, empty exoskeleton are called exuviae.
After moulting, an arthropod is described as "teneral", a "callow"; it is "fresh", pale and soft-bodied. Within one or two hours, the cuticle hardens and darkens following a tanning process analogous to the production of leather. During this short phase the animal expands, since growth is otherwise constrained by the rigidity of the exoskeleton. Growth of the limbs and other parts normally covered by hard exoskeleton is achieved by transfer of body fluids from soft parts before the new skin hardens. A spider with a small abdomen may be undernourished but more probably has recently undergone ecdysis. Some arthropods, especially large insects with tracheal respiration, expand their new exoskeleton by swallowing or otherwise taking in air. The maturation of the structure and colouration of the new exoskeleton might take days or weeks in a long-lived insect; this can make it difficult to identify an individual if it has recently undergone ecdysis. 
Ecdysis allows damaged tissue and missing limbs to be regenerated or substantially re-formed. Complete regeneration may require a series of moults, the stump becoming a little larger with each moult until it is a normal, or near normal, size.
Etymology.
The term "ecdysis" comes from (""), "to take off, strip off".
Process.
In preparation for ecdysis, the arthropod becomes inactive for a period of time, undergoing apolysis or separation of the old exoskeleton from the underlying epidermal cells. For most organisms, the resting period is a stage of preparation during which the secretion of fluid from the moulting glands of the epidermal layer and the loosening of the underpart of the cuticle occur.
Once the old cuticle has separated from the epidermis, a digesting fluid is secreted into the space between them. However, this fluid remains inactive until the upper part of the new cuticle has been formed. Then, by crawling movements, the organism pushes forward in the old integumentary shell, which splits down the back allowing the animal to emerge. Often, this initial crack is caused by a combination of movement and increase in blood pressure within the body, forcing an expansion across its exoskeleton, leading to an eventual crack that allows for certain organisms such as spiders to extricate themselves.
While the old cuticle is being digested, the new layer is secreted. All cuticular structures are shed at ecdysis, including the inner parts of the exoskeleton, which includes terminal linings of the alimentary tract and of the tracheae if they are present.
Insects.
Each stage of development between moults for insects in the taxon endopterygota is called an instar, or stadium, and each stage between moults of insects in the Exopterygota is called a nymph: there may be up to 15 nymphal stages. Endopterygota tend to have only four or five instars. Endopterygotes have more alternatives to moulting, such as expansion of the cuticle and collapse of air sacs to allow growth of internal organs.
The process of moulting in insects begins with the separation of the cuticle from the underlying epidermal cells (apolysis) and ends with the shedding of the old cuticle (ecdysis). In many species it is initiated by an increase in the hormone ecdysone. This hormone causes:
After apolysis the insect is known as a pharate. Moulting fluid is then secreted into the exuvial space between the old cuticle and the epidermis, this contains inactive enzymes which are activated only after the new epicuticle is secreted. This prevents the new procuticle from getting digesting as it is laid down. The lower regions of the old cuticle, the endocuticle and mesocuticle, are then digested by the enzymes and subsequently absorbed. The exocuticle and epicuticle resist digestion and are hence shed at ecdysis.
Spiders.
Spiders generally change their skin for the first time while still inside the egg sac, and the spiderling that emerges broadly resembles the adult. The number of moults varies, both between species and genders, but generally will be between five times and nine times before the spider reaches maturity. Not surprisingly, since males are generally smaller than females, the males of many species mature faster and do not undergo ecdysis as many times as the females before maturing.
Members of the Mygalomorphae are very long-lived, sometimes 20 years or more; they moult annually even after they mature.
Spiders stop feeding at some time before moulting, usually for several days. The physiological processes of releasing the old exoskeleton from the tissues beneath typically cause various colour changes, such as darkening. If the old exoskeleton is not too thick it may be possible to see new structures, such as setae, from outside. However, contact between the nerves and the old exoskeleton is maintained until a very late stage in the process.
The new, teneral exoskeleton has to accommodate a larger frame than the previous instar, while the spider has had to fit into the previous exoskeleton until it has been shed. This means the spider does not fill out the new exoskeleton completely, so it commonly appears somewhat wrinkled.
Most species of spiders hang from silk during the entire process, either dangling from a drop line, or fastening their claws into webbed fibres attached to a suitable base. The discarded, dried exoskeleton typically remains hanging where it was abandoned once the spider has left. 
To open the old exoskeleton, the spider generally contracts its abdomen (opisthosoma) to supply enough fluid to pump into the prosoma with sufficient pressure to crack it open along its lines of weakness. The carapace lifts off from the front, like a helmet, as its surrounding skin ruptures, but it remains attached at the back. Now the spider works its limbs free and typically winds up dangling by a new thread of silk attached to its own exuviae, which in turn hang from the original silk attachment. 
At this point the spider is a callow; it is teneral and vulnerable. As it dangles, its exoskeleton hardens and takes shape. The process may take minutes in small spiders, or some hours in the larger Mygalomorphs. Some spiders, such as some "Synema" species, members of the Thomisidae (crab spiders), mate while the female is still callow, during which time she is unable to eat the male.
Eurypterids.
Eurypterids are a group of chelicerates that became extinct in the late Permian. They underwent ecdysis in a similar manner to extant chelicerates, and most fossils are thought to be of exuviae, rather than cadavers.

</doc>
<doc id="10343" url="https://en.wikipedia.org/wiki?curid=10343" title="Ebor, New South Wales">
Ebor, New South Wales

Ebor is a village on Waterfall Way on the Northern Tablelands, New South Wales, Australia. It is situated about east of Armidale and about a third of the way between Armidale and the coast. At the 2006 census, Ebor and the surrounding area had a population of 160. There is a post office and coffee shop, a hotel/motel and a primary school.
The surrounding district is a noted rich sheep and cattle grazing area. The main tourist attractions are Ebor Falls, Wollomombi Falls, the cool temperate rainforest walks in New England National Park and recreational trout fishing. The Dutton Trout Hatchery on Point Lookout Road was established in 1950 and is one of the largest hatcheries in the state. Visitors can see the various stages of trout development prior to their release in the mountain streams.
History.
Ebor Post Office opened on 2 March 1868, closed in 1869 and reopened in 1910.
Climate.
Ebor is at fairly high altitude, about and by Australian standards, has cold winters with overnight frost and occasional light snow falls. The average rain fall is .

</doc>
<doc id="10344" url="https://en.wikipedia.org/wiki?curid=10344" title="Pre-Islamic period of Afghanistan">
Pre-Islamic period of Afghanistan

Archaeological exploration of the pre-Islamic period of Afghanistan began in Afghanistan in earnest after World War II and proceeded until the late 1970s when the nation was invaded by the Soviet Union. Archaeologists and historians suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world. Urbanized culture has existed in the land from between 3000 and 2000 BC. Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found inside Afghanistan.
After the Indus Valley Civilisation which stretched up to northeast Afghanistan, it was inhabited by the Iranic tribes and controlled by the Medes until about 500 BC when Darius the Great (Darius I) marched with his Persian army to make it part of the Achaemenid Empire. In 330 BC, Alexander the Great of Macedonia invaded the land after defeating Darius III of Persia in the Battle of Gaugamela. Much of Afghanistan became part of the Seleucid Empire followed by the Greco-Bactrian Kingdom. The area south of the Hindu Kush had been given by Seleucus I Nicator to Chandragupta Maurya and became part of the Maurya Empire. The land was inhabited by various tribes and ruled by many different kingdoms for the next two millenniums. Before the arrival of Islam in the 7th century, there were a number of religions practiced in ancient Afghanistan, including Zoroastrianism, Surya worship, Paganism, Hinduism and Buddhism. The Kaffirstan region, in the Hindu Kush, was not converted until the 19th century.
Prehistoric era.
Louis Dupree, the University of Pennsylvania, the Smithsonian Institution and others suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world. 
Afghanistan seems in prehistory, as well as in ancient and modern times, to have been connected by culture and trade with the neighbouring regions. Urban civilization, which includes modern-day Afghanistan, North India, and Pakistan, may have begun as early as 3000 to 2000 BC. Archaeological finds indicate the possible beginnings of the Bronze Age, which would ultimately spread throughout the ancient world from Afghanistan. It is also believed that the region had early trade contacts with Mesopotamia.
Indus Valley Civilisation.
The Indus Valley Civilisation (IVC) was a Bronze Age civilisation (3300-1300 BCE; mature period 2600-1900 BCE) extending from what today is northwest Pakistan to northwest India and northeast Afghanistan. An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan. Apart from Shortughai is Mundigak another notable site. There are several smaller IVC colonies to be found in Afghanistan.
Aryans and the Medes rule (1500 BC–551 BC).
Between 2000–1200 BC, a branch of Indo-European-speaking tribes known as the Aryans began migrating into the region. This is part of a dispute in regards to the Aryan invasion theory. They appear to have split into Iranic peoples, Nuristani, and Indo-Aryan groups at an early stage, possibly between 1500 and 1000 BC in what is today Afghanistan or much earlier as eastern remnants of the Indo-Aryans drifted much further west as with the Mitanni. The Iranians dominated the modern day plateau, while the Indo-Aryans ultimately headed towards the Indian subcontinent. The Avesta is believed to have been composed possibly as early as 1800 BC and written in ancient Ariana (Aryana), the earliest name of Afghanistan which indicates an early link with today's Iranian tribes to the west, or adjacent regions in Central Asia or northeastern Iran in the 6th century BC. Due to the similarity between early Avestan and Sanskrit (and other related early Indo-European languages such as Latin and Ancient Greek), it is believed that the split between the old Persians and Indo-Aryan tribes had taken place at least by 1000 BC. There are striking similarities between Avestan and Sanskrit, which may support the notion that the split was contemporary with the Indo-Aryans living in Afghanistan at a very early stage. Also, the Avesta itself divides into Old and New sections and neither mention the Medes who are known to have ruled Afghanistan starting around 700 BC. This suggests an early time-frame for the Avesta that has yet to be exactly determined as most academics believe it was written over the course of centuries if not millennia. Much of the archaeological data comes from the Bactria-Margiana Archaeological Complex (BMAC and Indus Valley Civilization) that probably played a key role in early Aryanic civilization in Afghanistan.
The Medes, a Western Persian people, arrived from what is today Kurdistan sometime around the 700s BC and came to dominate most of ancient Afghanistan. They were an early tribe that forged the first empire on the present Iranian plateau and were rivals of the Persians whom they initially dominated in the province of Fars to the south. Median domination of parts of far off Afghanistan would last until the Persians challenged and ultimately replaced them from rule.
Achaemenid invasion and Zoroastrianism (550 BC–331 BC).
The city of Bactria (which later became Balkh), is believed to have been the home of Zarathustra, who founded the Zoroastrian religion. The Avesta refers to eastern Bactria as being the home of the Zoroastrian faith, but this can be a reference to either a region in modern Afghanistan or Border line of Afghan-Pakistan. Regardless of the debate as to where Zoroaster was from, Zoroastrianism spread to become one of the world's most influential religions and became the main faith of the old Aryan people for centuries. It also remained the official religion of Persia until the defeat of the Sassanian ruler Yazdegerd III—over a thousand years after its founding—by Muslim Arabs. In what is today southern Iran, the Persians emerged to challenge Median supremacy on the Iranian plateau. By 550 BC, the Persians had replaced Median rule with their own dominion and even began to expand past previous Median imperial borders. Both Gandhara and Kamboja Mahajanapadas of the Buddhist texts soon fell a prey to the Achaemenian Dynasty during the reign of Achaemenid, Cyrus the Great (558–530 BC), or in the first year of Darius I, marking the region or of the eastern-most provinces of the empire, located partly in nowadays Afghanistan. According to Pliny's evidence, Cyrus the Great (Cyrus II) had destroyed Kapisa in Capiscene which was a Kamboja city. The former region of Gandhara and Kamboja (upper Indus) had constituted seventh satrapy of the Achaemenid Empire and annually contributed 170 talents of gold dust as a tribute to the Achaemenids.
Bactria had a special position in old Afghanistan, being the capital of a vice-kingdom. By the 4th century BC, Persian control of outlying areas and the internal cohesion of the empire had become somewhat tenuous. Although distant provinces like Bactriana had often been restless under Achaemenid rule, Bactrian troops nevertheless fought in the decisive Battle of Gaugamela in 330 BC against the advancing armies of Alexander the Great. The Achaemenids were decisively defeated by Alexander and retreated from his advancing army of Greco-Macedonians and their allies. Darius III, the last Achaemenid ruler, tried to flee to Bactria but was assassinated by a subordinate lord, the Bactrian-born Bessus, who proclaimed himself the new ruler of Persia as Artaxerxes (V). Bessus was unable to mount a successful resistance to the growing military might of Alexander's army so he fled to his native Bactria, where he attempted to rally local tribes to his side but was instead turned over to Alexander who proceeded to have him tortured and executed for having committed regicide.
Alexander the Great to Greco-Bactrian rule (330 BC–ca. 150 BC).
Moving thousands of kilometers eastward from recently subdued Persia, the Macedonian leader Alexander the Great, encountered fierce resistance from the local tribes of Aria (satrapy), Drangiana, Arachosia (South and Eastern Afghanistan, North-West Pakistan) and Bactria (North and Central Afghanistan).
Upon Alexander's death in 323 BC, his empire, which had never been politically consolidated, broke apart as his companions began to divide it amongst themselves. Alexander's cavalry commander, Seleucus, took nominal control of the eastern lands and founded the Seleucid dynasty. Under the Seleucids, as under Alexander, Greek colonists and soldiers colonized Bactria, roughly corresponding to modern Afghanistan's borders. However, the majority of Macedonian soldiers of Alexander the Great wanted to leave the east and return home to Greece. Later, Seleucus sought to guard his eastern frontier and moved Ionian Greeks (also known as Yavanas to many local groups) to Bactria in the 3rd century BC.
Maurya Empire.
While the Diadochi were warring amongst themselves, the Mauryan Empire was developing in the northern part of the Indian subcontinent. The founder of the empire, Chandragupta Maurya, confronted a Macedonian invasion force led by Seleucus I in 305 BC and following a brief conflict, an agreement was reached as Seleucus ceded Gandhara and Arachosia (centered around ancient Kandahar) and areas south of Bagram (corresponding to the extreme south-east of modern Afghanistan) to the Mauryans. During the 120 years of the Mauryans in southern Afghanistan, Buddhism was introduced and eventually become a major religion alongside Zoroastrianism and local pagan beliefs. The ancient Grand Trunk Road was built linking what is now Kabul to various cities in the Punjab and the Gangetic Plain. Commerce, art, and architecture (seen especially in the construction of stupas) developed during this period. It reached its high point under Emperor Ashoka whose edicts, roads, and rest stops were found throughout the subcontinent. Although the vast majority of them throughout the subcontinent were written in Prakrit, Afghanistan is notable for the inclusion of 2 Greek and Aramaic ones alongside the court language of the Mauryans.
Inscriptions made by the Mauryan Emperor Ashoka, a fragment of Edict 13 in Greek, as well as a full Edict, written in both Greek and Aramaic has been discovered in Kandahar. It is said to be written in excellent Classical Greek, using sophisticated philosophical terms. In this Edict, Ashoka uses the word Eusebeia ("Piety") as the Greek translation for the ubiquitous "Dharma" of his other Edicts written in Prakrit:
The last ruler in the region was probably Subhagasena (Sophagasenus of Polybius), who, in all probability, belonged to the Ashvaka (q.v.) background.
Greco-Bactrians.
In the middle of the 3rd century BC, an independent, Hellenistic state was declared in Bactria and eventually the control of the Seleucids and Mauryans was overthrown in western and southern Afghanistan. Graeco-Bactrian rule spread until it included a large territory which stretched from Turkmenistan in the west to the Punjab in India in the east by about 170 BC. Graeco-Bactrian rule was eventually defeated by a combination of internecine disputes that plagued Greek and Hellenized rulers to the west, continual conflict with Indian kingdoms, as well as the pressure of two groups of nomadic invaders from Central Asia—the Parthians and Sakas.
Kushan Empire (150 BC–300 AD).
In the 3rd and 2nd centuries BC, the Parthians, a nomadic Iranian peoples, arrived in Western Asia. While they made large inroads into the modern-day territory of Afghanistan, about 100 years later another Indo-European group from the north—the Kushans (a subgroup of the tribe called the Yuezhi by the Chinese)—entered the region of Afghanistan and established an empire lasting almost four centuries, which would dominate most of the Afghanistan region.
The Kushan Empire spread from the Kabul River valley to defeat other Central Asian tribes that had previously conquered parts of the northern central Iranian Plateau once ruled by the Parthians. By the middle of the 1st century BC, the Kushans' base of control became Afghanistan and their empire spanned from the north of the Pamir mountains to the Ganges river valley in India. Early in the 2nd century under Kanishka, the most powerful of the Kushan rulers, the empire reached its greatest geographic and cultural breadth to become a center of literature and art. Kanishka extended Kushan control to the mouth of the Indus River on the Arabian Sea, into Kashmir, and into what is today the Chinese-controlled area north of Tibet. Kanishka was a patron of religion and the arts. It was during his reign that Buddhism, which was promoted in northern India earlier by the Mauryan emperor Ashoka (c. 260 BC–232 BC), reached its zenith in Central Asia. Though the Kushanas supported local Buddhists and Hindus as well as the worship of various local deities.
Sassanian invasion (300–650).
In the 3rd century, Kushan control fragmented into semi-independent kingdoms that became easy targets for conquest by the rising Iranian dynasty, the Sassanians (c. 224–561) which annexed Afghanistan by 300 AD. In these far off eastern-most territories, they established vassal kings as rulers, known as the Kushanshahs. Sassanian control was tenuous at times as numerous challenges from Central Asian tribes led to instability and constant warfare in the region.
The disunited Kushan and Sassanian kingdoms were in a poor position to meet the threat of a new wave of nomadic, Indo-European invaders from the north. The Hephthalites (or White Huns) swept out of Central Asia around the 4th century into Bactria and to the south, overwhelming the last of the Kushan and Sassanian kingdoms. Some have speculated that the name "Afghanistan land of the Afghans" derives from which could be an adjective such as brave, chivlarious, valour, which was to use for the people in today's Afghanistan. Historians believe that Hepthalite control continued for a century and was marked by constant warfare with the Sassanians to the west who exerted nominal control over the region.
By the middle of the 6th century the Hephthalites were defeated in the territories north of the Amu Darya (the Oxus River of antiquity) by another group of Central Asian nomads, the Göktürks, and by the resurgent Sassanians in the lands south of the Amu Darya. It was the ruler of western Göktürks, Sijin (a.k.a. Sinjibu, Silzibul and Yandu Muchu Khan) who led the forces against the Hepthalites who were defeated at the Battle of Chach (Tashkent) and at the Battle of Bukhara.
Kabul Shahi.
The Shahi dynasties ruled portions of the Kabul Valley (in eastern Afghanistan) and the old province of Gandhara (northern Pakistan and Kashmir) from the decline of the Kushan Empire in the 3rd century to the early 9th century. They are split into two eras the Buddhist-Shahis and the later Hindu-Shahis with the change-over occurring around 870, and ruled up until the Islamic conquest of Afghanistan.
When Xuanzang visited the region early in the 7th century, the Kabul region was ruled by a Kshatriya king, who is identified as the "Shahi Khingal", and whose name has been found in an inscription found in Gardez. The Turkic Shahi regency was overthrown and replaced by a Mohyal Shahi dynasty of Brahmins who began the first phase of the Brahmana Hindu Shahi dynasty.
These Hindu kings of Kabul and Gandhara may have had links to some ruling families in neighboring Kashmir and other areas to the east. The Shahis were rulers of predominantly Buddhist, Zoroastrian, Hindu and Muslim populations and were thus patrons of numerous faiths, and various artifacts and coins from their rule have been found that display their multicultural domain. In 964 AD, the last Mohyal Shahi was succeeded by the Janjua overlord, Jayapala, of the Panduvanshi dynasty. The last Shahi emperors Jayapala, Anandapala and Tirlochanpala fought the Muslim Ghaznavids of Ghazna and were gradually defeated. Their remaining army were eventually exiled into northern India.
Archaeological remnants.
Most of the Zoroastrian, Greek, Hellenistic, Buddhist, Hindu and other indigenous cultures were replaced by the coming of Islam and little influence remains in Afghanistan today. Along ancient trade routes, however, stone monuments of the once flourishing Buddhist culture did exist as reminders of the past. The two massive sandstone Buddhas of Bamyan, thirty-five and fifty-three meters high overlooked the ancient route through Bamyan to Balkh and dated from the 3rd and 5th centuries. They survived until 2001, when they were destroyed by the Taliban. In this and other key places in Afghanistan, archaeologists have located frescoes, stucco decorations, statuary, and rare objects from as far away as China, Phoenicia, and Rome, which were crafted as early as the 2nd century and bear witness to the influence of these ancient civilizations upon Afghanistan.
One of the early Buddhist schools, the Mahāsāṃghika-Lokottaravāda, were known to be prominent in the area of Bamiyan. The Chinese Buddhist monk Xuanzang visited a Lokottaravāda monastery in the 7th century CE, at Bamiyan, Afghanistan, and this monastery site has since been rediscovered by archaeologists. Birchbark and palm leaf manuscripts of texts in this monastery's collection, including Mahāyāna sūtras, have been discovered at the site, and these are now located in the Schøyen Collection. Some manuscripts are in the Gāndhārī language and Kharoṣṭhī script, while others are in Sanskrit and written in forms of the Gupta script. Manuscripts and fragments that have survived from this monastery's collection include well-known Buddhist texts such as the "Mahāparinirvāṇa Sūtra" (from the "Āgamas"), the "Diamond Sūtra" ("Vajracchedikā Prajñāpāramitā"), the "Medicine Buddha Sūtra", and the "Śrīmālādevī Siṃhanāda Sūtra".
In 2010, reports stated that about 42 Buddhist relics have been discovered in the Logar Province of Afghanistan, which is south of Kabul. Some of these items date back to the 2nd century according to Archaeologists. The items included two Buddhist temples (Stupas), Buddha statues, frescos, silver and gold coins and precious beads.

</doc>
<doc id="10346" url="https://en.wikipedia.org/wiki?curid=10346" title="Gravitational redshift">
Gravitational redshift

In astrophysics, gravitational redshift or Einstein shift is the process by which electromagnetic radiation originating from a source that is in a gravitational field is reduced in frequency, or redshifted, when observed in a region of a weaker gravitational field. This is a direct result of gravitational time dilation - as one moves away from a source of gravitational field, the rate at which time passes is increased relative to the case when one is near the source. As frequency is inverse of time (specifically, time required for completing one wave oscillation), frequency of the electromagnetic radiation is reduced in an area of a lower gravitational field (i.e., a higher gravitational potential). There is a corresponding reduction in energy when electromagnetic radiation is red-shifted, as given by Planck's relation, due to the electromagnetic radiation propagating in opposition to the gravitational gradient. There also exists a corresponding blueshift when electromagnetic radiation propagates from an area of a weaker gravitational field to an area of a stronger gravitational field.
If applied to optical wavelengths, this manifests itself as a change in the colour of visible light as the wavelength of the light is increased toward the red part of the light spectrum. Since frequency and wavelength are inversely proportional, this is equivalent to saying that the frequency of the light is reduced towards the red part of the light spectrum, giving this phenomenon the name redshift.
Definition.
Redshift is often denoted with the dimensionless variable formula_1, defined as the fractional change of the wavelength
formula_2
where
formula_3 is the wavelength of the electromagnetic radiation (photon) as measured by the observer.
formula_4 is the wavelength of the electromagnetic radiation (photon) when measured at the source of emission.
The gravitational redshift of a photon can be calculated in the framework of general relativity (using the Schwarzschild metric) as
formula_5
with the Schwarzschild radius
formula_6,
where formula_7 denotes Newton's gravitational constant, formula_8 the mass of the gravitating body, formula_9 the speed of light, and formula_10 the distance between the center of mass of the gravitating body and the point at which the photon is emitted. The redshift is not defined for photons emitted inside the Schwarzschild radius, the distance from the body where the escape velocity is greater than the speed of light. Therefore, this formula only applies when formula_10 is larger as formula_12. When the photon is emitted at a distance equal to the Schwarzschild radius, the redshift will be infinitely large and it can't escape to any finite distance from this Schwarzschild sphere! When the photon is emitted at an infinitely large distance, there is no redshift.
In the Newtonian limit, i.e. when formula_10 is sufficiently large compared to the Schwarzschild radius formula_12, the redshift can be approximated by a binomial expansion to become
formula_15
The redshift formula for the frequency formula_16 (and therefore also for the energy formula_17 of a photon) can simply deduced from the wavelength-formula above to be
formula_18
with formula_19 the emitted frequency at the emission point and formula_20 the frequency at distance formula_21 from the center of mass of the gravitating body causing this gravitational potential. Moreover, we get from the law of energy conservation formula_22 the general case for a photon of frequency formula_23 emitted at distance formula_24 to observer distance formula_25 (measured as distances from the gravitational center of mass) the equation
formula_26
as long as formula_27 holds.
History.
The gravitational weakening of light from high-gravity stars was predicted by John Michell in 1783 and Pierre-Simon Laplace in 1796, using Isaac Newton's concept of light corpuscles (see: emission theory) and who predicted that some stars would have a gravity so strong that light would not be able to escape. The effect of gravity on light was then explored by Johann Georg von Soldner (1801), who calculated the amount of deflection of a light ray by the sun, arriving at the Newtonian answer which is half the value predicted by general relativity. All of this early work assumed that light could slow down and fall, which was inconsistent with the modern understanding of light waves.
Once it became accepted that light was an electromagnetic wave, it was clear that the frequency of light should not change from place to place, since waves from a source with a fixed frequency keep the same frequency everywhere. One way around this conclusion would be if time itself were altered—if clocks at different points had different rates.
This was precisely Einstein's conclusion in 1911. He considered an accelerating box, and noted that according to the special theory of relativity, the clock rate at the bottom of the box was slower than the clock rate at the top. Nowadays, this can be easily shown in accelerated coordinates. The metric tensor in units where the speed of light is one is:
and for an observer at a constant value of r, the rate at which a clock ticks, R(r), is the square root of the time coefficient, R(r)=r. The acceleration at position r is equal to the curvature of the hyperbola at fixed r, and like the curvature of the nested circles in polar coordinates, it is equal to 1/r.
So at a fixed value of g, the fractional rate of change of the clock-rate, the percentage change in the ticking at the top of an accelerating box vs at the bottom, is:
The rate is faster at larger values of R, away from the apparent direction of acceleration. The rate is zero at r=0, which is the location of the acceleration horizon.
Using the principle of equivalence, Einstein concluded that the same thing holds in any gravitational field, that the rate of clocks R at different heights was altered according to the gravitational field g. When g is slowly varying, it gives the fractional rate of change of the ticking rate. If the ticking rate is everywhere almost this same, the fractional rate of change is the same as the absolute rate of change, so that:
Since the rate of clocks and the gravitational potential have the same derivative, they are the same up to a constant. The constant is chosen to make the clock rate at infinity equal to 1. Since the gravitational potential is zero at infinity:
where the speed of light has been restored to make the gravitational potential dimensionless.
The coefficient of the formula_32 in the metric tensor is the square of the clock rate, which for small values of the potential is given by keeping only the linear term:
and the full metric tensor is:
where again the c's have been restored. This expression is correct in the full theory of general relativity, to lowest order in the gravitational field, and ignoring the variation of the space-space and space-time components of the metric tensor, which only affect fast moving objects.
Using this approximation, Einstein reproduced the incorrect Newtonian value for the deflection of light in 1909. But since a light beam is a fast moving object, the space-space components contribute too. After constructing the full theory of general relativity in 1916, Einstein solved for the space-space components in a post-Newtonian approximation, and calculated the correct amount of light deflection – double the Newtonian value. Einstein's prediction was confirmed by many experiments, starting with Arthur Eddington's 1919 solar eclipse expedition.
The changing rates of clocks allowed Einstein to conclude that light waves change frequency as they move, and the frequency/energy relationship for photons allowed him to see that this was best interpreted as the effect of the gravitational field on the mass–energy of the photon. To calculate the changes in frequency in a nearly static gravitational field, only the time component of the metric tensor is important, and the lowest order approximation is accurate enough for ordinary stars and planets, which are much bigger than their Schwarzschild radius.
Initial verification.
A number of experimenters initially claimed to have identified the effect using astronomical measurements, and the effect was eventually considered to have been finally identified in the spectral lines of the star Sirius B by W.S. Adams in 1925. However, measurements of the effect before the 1960s have been critiqued by ("e.g.", by C.M. Will), and the effect is now considered to have been definitively verified by the experiments of Pound, Rebka and Snider between 1959 and 1965.
The Pound–Rebka experiment of 1959 measured the gravitational redshift in spectral lines using a terrestrial 57Fe gamma source over a vertical height of 22.5 metres. 
A commonly cited experimental verification is the more accurate Pound–Snider experiment of 1965.
James W. Brault, a graduate student of Robert Dicke at Princeton University, measured the gravitational redshift of the sun using optical methods in 1962.
More information can be seen at Tests of general relativity.
In 2011 the group of Radek Wojtak of the Niels Bohr Institute at the University of Copenhagen collected data from 8000 galaxy clusters and found that the light coming from the cluster centers tended to be red-shifted compared to the cluster edges, confirming the energy loss due to gravity.
Application.
Gravitational redshift is studied in many areas of astrophysical research.
Exact Solutions.
A table of exact solutions of the Einstein field equations consists of the following:
The more often used exact equation for gravitational redshift applies to the case outside of a non-rotating, uncharged mass which is spherically symmetric. The equation is:
formula_36, where
Gravitational redshift versus gravitational time dilation.
When using special relativity's relativistic Doppler relationships to calculate the change in energy and frequency (assuming no complicating route-dependent effects such as those caused by the frame-dragging of rotating black holes), then the gravitational redshift and blueshift frequency ratios are the inverse of each other, suggesting that the "seen" frequency-change corresponds to the actual difference in underlying clockrate. Route-dependence due to frame-dragging may come into play, which would invalidate this idea and complicate the process of determining globally agreed differences in underlying clock rate.
While gravitational redshift refers to what is seen, gravitational time dilation refers to what is deduced to be "really" happening once observational effects are taken into account.

</doc>
<doc id="10350" url="https://en.wikipedia.org/wiki?curid=10350" title="Easter Rising">
Easter Rising

The Easter Rising (), also known as the Easter Rebellion, was an armed insurrection in Ireland during Easter Week, April 1916. The Rising was launched by Irish republicans to end British rule in Ireland and establish an independent Irish Republic while the United Kingdom was heavily engaged in World War I. It was the most significant uprising in Ireland since the rebellion of 1798, and the first armed action of the Irish revolutionary period.
Organised by a seven-man Military Council of the Irish Republican Brotherhood, the Rising began on Easter Monday, 24 April 1916, and lasted for six days. Members of the Irish Volunteers — led by schoolmaster and Irish language activist Patrick Pearse, joined by the smaller Irish Citizen Army of James Connolly and 200 women of Cumann na mBan — seized key locations in Dublin and proclaimed an Irish Republic. The British Army brought in thousands of reinforcements as well as artillery and a gunboat. There was fierce street fighting on the routes into the city centre, where the rebels put up stiff resistance, slowing the British advance and inflicting heavy casualties. Elsewhere in Dublin, the fighting mainly consisted of sniping and long-range gun battles. The main rebel positions were gradually surrounded and bombarded with artillery. There were isolated actions in other parts of Ireland, with attacks on the Royal Irish Constabulary barracks at Ashbourne, County Meath and in County Galway, and the seizure of the town of Enniscorthy, County Wexford. Germany agreed to send an arms shipment to the rebels, but the British had intercepted it just before the Rising began. Volunteer leader Eoin MacNeill had then issued a countermand in a bid to halt the Rising, which greatly reduced the number of rebels who mobilised.
With much greater numbers and heavier weapons, the British Army suppressed the Rising. Pearse agreed to an unconditional surrender on Saturday 29 April, although sporadic fighting continued until Sunday, when word reached the other rebel positions. After the surrender the country remained under martial law. About 3,500 people were taken prisoner by the British, many of whom had played no part in the Rising, and 1,800 of them were sent to internment camps or prisons in Britain. Most of the leaders of the Rising were executed following courts-martial. The Rising brought physical force republicanism back to the forefront of Irish politics, which for nearly 50 years had been dominated by constitutional nationalism. It, and the British reaction to it, led to increased popular support for Irish independence. In December 1918, republicans, represented by the reconstituted Sinn Féin party, won a landslide victory in the general election to the British Parliament. They did not take their seats, but instead convened the First Dáil and declared the independence of the Irish Republic, which led to the War of Independence.
Almost 500 people were killed in the Easter Rising. About 54% were civilians, 30% were British military and police, and 16% were Irish rebels. More than 2,600 were wounded. Many of the civilians were killed as a result of the British using artillery and heavy machine guns, or mistaking civilians for rebels. Others were caught in the crossfire in a crowded city. The shelling and the fires it caused left parts of inner city Dublin in ruins.
Background.
The Acts of Union 1800 united the Kingdom of Great Britain and the Kingdom of Ireland as the United Kingdom of Great Britain and Ireland, abolishing the Irish Parliament and giving Ireland representation in the British Parliament. The Irish Parliament that passed the Act of Union was not representative of the Irish people. Although the vast majority of the Irish population was Catholic, only Protestants could sit in the Irish Parliament and only landowning men could vote. Many MPs were persuaded to vote for the union through bribery. From early on, many Irish nationalists opposed the union as the ensuing exploitation and impoverishment of the island led to a high level of depopulation. Opposition took various forms: constitutional (the Repeal Association; the Home Rule League), social (disestablishment of the Church of Ireland; the Land League) and revolutionary (Rebellion of 1848; Fenian Rising). The Irish Home Rule movement sought to achieve self-government for Ireland, within the United Kingdom. In 1886, the Irish Parliamentary Party (IPP) under Charles Stewart Parnell succeeded in having the First Home Rule Bill introduced in the British parliament, but it was defeated. The Second Home Rule Bill of 1893 was passed by the House of Commons but rejected by the House of Lords.
After the fall of Parnell, younger and more radical nationalists became disillusioned with parliamentary politics and turned toward more extreme forms of separatism. The Gaelic Athletic Association, the Gaelic League and the cultural revival under W. B. Yeats and Lady Augusta Gregory, together with the new political thinking of Arthur Griffith expressed in his newspaper "Sinn Féin" and organisations such as the National Council and the Sinn Féin League, led many Irish people to identify with the idea of an independent Gaelic Ireland. This was sometimes referred to by the generic term "Sinn Féin", particularly by the authorities.
The Third Home Rule Bill was introduced by British Prime Minister H. H. Asquith in 1912, sparking the Home Rule Crisis. Although nationalists, who were in a majority in the country, supported home rule, Protestant unionists, led by Sir Edward Carson, opposed it, as they did not want to be ruled by a Catholic-dominated Irish government. To prevent home rule, in January 1913 they formed the Ulster Volunteer Force (UVF), the first paramilitary group of 20th-century Ireland.
In response, Irish nationalists formed a rival paramilitary group, the Irish Volunteers, in November 1913. The Irish Republican Brotherhood (IRB) was a driving force behind the Irish Volunteers and attempted to control it. Its leader was Eoin MacNeill, who was not an IRB member. The Irish Volunteers' stated goal was "to secure and to maintain the rights and liberties common to all the people of Ireland". It included people with a range of political views, and was open to "all able-bodied Irishmen without distinction of creed, politics or social group". Another militant group, the Irish Citizen Army, was formed by trade unionists as a result of the Dublin Lock-out of that year. When the Irish Volunteers smuggled rifles into Dublin, the British Army attempted to stop them and fired into a crowd of civilians. However, British Army officers threatened to resign if they were ordered to take action against the UVF. By 1914, Ireland seemed to be on the brink of a civil war.
The crisis was ended in August that year by the outbreak of World War I and Ireland's involvement in it. The Home Rule Bill was enacted, but its implementation was postponed by a suspensory act until the end of the war. A Scottish Home Rule Bill, which passed its second reading in the British parliament in May 1914, also lapsed after the outbreak of war.
Although many Irishmen had volunteered for Irish regiments and divisions of the New British Army at the outbreak of World War I, the growing likelihood of enforced conscription created a backlash. The British government began suggesting that it would only implement home rule in exchange for Irish conscription. This outraged the Irish parties at Westminster, including the IPP, the All-for-Ireland League and others, who walked out in protest and returned to Ireland to organise opposition.
Planning the Rising.
The Supreme Council of the IRB met on 5 September 1914, just over a month after the British government had declared war on Germany. At this meeting, they decided to stage an uprising before the war ended and to secure help from Germany. Responsibility for the planning of the rising was given to Tom Clarke and Seán MacDermott ("Mac Diarmada"). The Irish Volunteers—the smaller of the two forces resulting from the September 1914 split over support for the British war effort—set up a "headquarters staff" that included Patrick Pearse as Director of Military Organisation, Joseph Plunkett as Director of Military Operations and Thomas MacDonagh as Director of Training. Éamonn Ceannt was later added as Director of Communications.
In May 1915, Clarke and MacDermott established a Military Committee or Military Council within the IRB, consisting of Pearse, Plunkett and Ceannt, to draw up plans for a rising. Clarke and MacDermott joined it shortly after. The Military Council was able to promote its own policies and personnel independently of both the Volunteer Executive and the IRB Executive. Although the Volunteer and IRB leaders were not against a rising in principle, they were of the opinion that it was not opportune at that moment. Volunteer Chief-of-Staff Eoin MacNeill, supported a rising only if the British government attempted to suppress the Volunteers or introduce conscription, and if such a rising had some chance of success. IRB President Denis McCullough and prominent IRB member Bulmer Hobson held similar views. The Military Council kept its plans secret, so as to prevent the British authorities learning of the plans, and to thwart those within the organisation who might try to stop the rising. IRB members held officer rank in the Volunteers throughout the country and took their orders from the Military Council, not from MacNeill.
After the war began, Roger Casement and Clan na Gael leader John Devoy met the German Ambassador to the United States, Johann Heinrich von Bernstorff, to discuss German backing for an uprising. Casement went to Germany and began negotiations with the German government and military. He persuaded the Germans to announce their support for Irish independence in November 1914. Casement also attempted to recruit an Irish Brigade, made up of Irish prisoners of war, which would be armed and sent to Ireland to join the uprising. However, only 56 men volunteered. Plunkett joined Casement in Germany the following year. Together, Plunkett and Casement presented a plan (the 'Ireland Report') in which a German expeditionary force would land on the west coast of Ireland, while a rising in Dublin diverted the British forces so that the Germans, with the help of local Volunteers, could secure the line of the River Shannon, before advancing on the capital. The German military rejected the plan, but agreed to ship arms and ammunition to the Volunteers.
James Connolly—head of the Irish Citizen Army (ICA), a group of armed socialist trade union men and women—was unaware of the IRB's plans, and threatened to start a rebellion on his own if other parties failed to act. If they had done it alone, the IRB and the Volunteers would possibly have come to their aid; however, the IRB leaders met with Connolly in January 1916 and convinced him to join forces with them. They agreed that they would launch a rising together at Easter and made Connolly the sixth member of the Military Council. Thomas MacDonagh would later become the seventh and final member.
The death of the old Fenian leader Jeremiah O'Donovan Rossa in New York in August 1915 was an opportunity to mount a spectacular demonstration. His body was sent to Ireland for burial in Glasnevin Cemetery, with the Volunteers in charge of arrangements. Huge crowds lined the route and gathered at the graveside. Pearse made a dramatic funeral oration, a rallying call to republicans, which ended with the words "Ireland unfree shall never be at peace".
Build-up to Easter Week.
In early April, Pearse issued orders to the Irish Volunteers for three days of "parades and manoeuvres" beginning on Easter Sunday. He had the authority to do this, as the Volunteers' Director of Organisation. The idea was that IRB members within the organisation would know these were orders to begin the rising, while men such as MacNeill and the British authorities would take it at face value.
On 9 April, the German Navy dispatched a ship for County Kerry. Disguised as a Norwegian ship called the "Aud", it was loaded with 20,000 rifles, one million rounds of ammunition, and explosives. Casement also left for Ireland aboard the German submarine "U-19". He was disappointed with the level of support offered by the Germans and he intended to stop or at least postpone the rising.
On Wednesday 19 April, Alderman Tom Kelly, a Sinn Féin member of Dublin Corporation, read out at a meeting of the Corporation a document supposedly leaked from Dublin Castle, revealing that the British authorities planned to shortly arrest leaders of the Irish Volunteers, Sinn Féin and the Gaelic League, and occupy their premises. Although the British authorities said the "Castle Document" was fake, MacNeill ordered the Volunteers to prepare to resist. Unbeknownst to MacNeill, the document had been forged by the Military Council to persuade moderates of the need for their planned uprising. It was an edited version of a real document outlining British plans in the event of conscription. That same day, the Military Council informed senior Volunteer officers that the rising would begin on Easter Sunday. However, it chose not to inform the rank-and-file, or moderates such as MacNeill, until the last minute.
The following day, MacNeill got wind that a rising was about to be launched and threatened to do everything he could to prevent it, short of informing the British. MacNeill was briefly persuaded to go along with some sort of action when Mac Diarmada revealed to him that a German arms shipment was about to land in County Kerry. MacNeill believed that when the British learned of the shipment they would immediately suppress the Volunteers, thus the Volunteers would be justified in taking defensive action, including the planned manoeuvres.
The "Aud" and the "U-19" reached the coast of Kerry on Good Friday, 21 April. This was earlier than the Volunteers expected and so none were there to meet the vessels. The Royal Navy had known about the arms shipment and intercepted the "Aud", prompting the captain to scuttle the ship. Furthermore, Casement was captured shortly after he landed at Banna Strand.
When MacNeill learned that the arms shipment had been lost, he reverted to his original position. With the support of other leaders of like mind, notably Bulmer Hobson and The O'Rahilly, he issued a countermand to all Volunteers, cancelling all actions for Sunday. This coundermanding order was relayed to Volunteer officers and printed in the Sunday morning newspapers. It succeeded in putting the rising off for only a day, although it greatly reduced the number of Volunteers who turned out.
British Naval Intelligence had been aware of the arms shipment, Casement's return, and the Easter date for the rising through radio messages between Germany and its embassy in the United States that were intercepted by the Royal Navy and deciphered in Room 40 of the Admiralty. The information was passed to the Under-Secretary for Ireland, Sir Matthew Nathan, on 17 April, but without revealing its source, and Nathan was doubtful about its accuracy. When news reached Dublin of the capture of the "Aud" and the arrest of Casement, Nathan conferred with the Lord Lieutenant, Lord Wimborne. Nathan proposed to raid Liberty Hall, headquarters of the Citizen Army, and Volunteer properties at Father Matthew Park and at Kimmage, but Wimborne insisted on wholesale arrests of the leaders. It was decided to postpone action until after Easter Monday, and in the meantime Nathan telegraphed the Chief Secretary, Augustine Birrell, in London seeking his approval. By the time Birrell cabled his reply authorising the action, at noon on Monday 24 April 1916, the Rising had already begun.
On the morning of Easter Sunday, 23 April, the Military Council met at Liberty Hall to discuss what to do in light of MacNeill's countermanding order. They decided that the Rising would go ahead the following day, Easter Monday, and that the Irish Volunteers and Irish Citizen Army would go into action as the 'Army of the Irish Republic'. They elected Pearse as Commander-in-Chief and Connolly as Commandant of the Dublin Brigade. Messengers were then sent to all units informing them of the new orders.
The Rising in Dublin.
Easter Monday.
On the morning of Monday 24 April, about 1,200 members of the Irish Volunteers and Irish Citizen Army mustered at several locations in central Dublin. Among them were members of the all-female Cumann na mBan. Some wore Irish Volunteer and Citizen Army uniforms, while others wore civilian clothes with a yellow Irish Volunteer armband, military hats, and bandoliers. They were armed mostly with rifles (especially 1871 Mausers), but also with shotguns, revolvers, a few Mauser C96 semi-automatic pistols, and grenades. The number of Volunteers who mobilized was much smaller than expected. This was due to MacNeill's countermanding order, and the fact that the new orders had been sent so soon beforehand. However, several hundred Volunteers joined the Rising after it began.
Shortly before midday, the rebels began to seize important sites in central Dublin. The rebels' plan was to hold Dublin city centre. This was a large, oval-shaped area bounded by two canals: the Grand to the south and the Royal to the north, with the River Liffey running through the middle. On the southern and western edges of this district were five British Army barracks. Most of the rebel's positions had been chosen to defend against counter-attacks from these barracks. The rebels took the positions with ease. Civilians were evacuated and policemen were ejected or taken prisoner. Windows and doors were barricaded, food and supplies were secured, and first aid posts were set up. Barricades were erected on the streets to hinder British Army movement.
A joint force of about 400 Volunteers and Citizen Army gathered at Liberty Hall under the command of Commandant James Connolly. This was the headquarters battalion, and it also included Commander-in-Chief Patrick Pearse, as well as Tom Clarke, Seán MacDermott and Joseph Plunkett. They marched to the General Post Office (GPO) on O'Connell Street, Dublin's main thoroughfare, occupied the building and hoisted two republican flags. Pearse stood outside and read the Proclamation of the Irish Republic. Copies of the Proclamation were also pasted on walls and handed out to bystanders by Volunteers and newsboys. The GPO would be the rebels' headquarters for most of the Rising. Volunteers from the GPO also occupied other buildings on the street, including buildings overlooking O'Connell Bridge. They took over a wireless telegraph station and sent out a radio broadcast in Morse code, announcing that an Irish Republic had been declared. This was the first radio broadcast in Ireland.
Elsewhere, some of the headquarters battalion under Michael Mallin occupied St Stephen's Green, where they dug trenches and barricaded the surrounding roads. The 1st battalion, under Edward 'Ned' Daly, occupied the Four Courts and surrounding buildings, while a company under Seán Heuston occupied the Mendicity Institution, across the River Liffey from the Four Courts. The 2nd battalion, under Thomas MacDonagh, occupied Jacob's biscuit factory. The 3rd battalion, under Éamon de Valera, occupied Boland's Mill and surrounding buildings. The 4th battalion, under Éamonn Ceannt, occupied the South Dublin Union and the distillery on Marrowbone Lane. From each of these 'garrisons', small units of rebels established outposts in the surrounding area.
The rebels also attempted to cut transport and communication links. As well as erecting roadblocks, they took control of various bridges and cut telephone and telegraph wires. Westland Row and Harcourt Street railway stations were occupied, though the latter only briefly. The railway line was cut at Fairview and the line was damaged by bombs at Amiens Street, Broadstone, Kingsbridge and Lansdowne Road.
Around midday, a small team of Volunteers and Fianna Éireann members swiftly captured the Magazine Fort in the Phoenix Park and disarmed the guards. The goal was to seize weapons and blow up the ammunition store to signal that the Rising had begun. They seized weapons and planted explosives, but the blast was not big enough to be heard across the city. The 23-year-old son of the fort's commander was fatally shot when he ran to raise the alarm.
A contingent under Seán Connolly occupied Dublin City Hall and adjacent buildings. They attempted to seize neighbouring Dublin Castle, the heart of British rule in Ireland. As they approached the gate a lone police sentry, James O'Brien, attempted to stop them and was shot dead by Connolly. According to some accounts, he was the first casualty of the Rising. The rebels overpowered the soldiers in the guardroom, but failed to press further. The British Army's chief intelligence officer, Major Ivon Price, fired on the rebels while the Under-Secretary for Ireland, Sir Matthew Nathan, helped shut the castle gates. Unbeknownst to the rebels, the Castle was lightly guarded and could have been taken with ease. The rebels instead laid siege to the Castle from City Hall. Fierce fighting erupted there after British reinforcements arrived. The rebels on the roof exchanged fire with soldiers on the street. Seán Connolly was shot dead by a sniper, becoming the first rebel casualty. By the following morning, British forces had re-captured City Hall and taken the rebels prisoner.
The rebels did not attempt to take some other key locations, notably Trinity College, in the heart of the city centre and defended by only a handful of armed unionist students. The failure to occupy strategic locations was attributed to lack of manpower. In at least two incidents, at Jacob's and Stephen's Green, the Volunteers and Citizen Army shot dead civilians trying to attack them or dismantle their barricades. Elsewhere, they hit civilians with their rifle butts to drive them off.
The British military were caught totally unprepared by the rebellion and their response of the first day was generally un-coordinated. Two troops of British cavalry were sent to investigate what was happening. They took fire and casualties from rebel forces at the GPO and at the Four Courts. As one troop passed Nelson's Pillar, the rebels opened fire from the GPO, killing three cavalrymen and two horses and fatally wounding a fourth man. The cavalrymen retreated and were withdrawn to barracks. On Mount Street, a group of Volunteer Training Corps men stumbled upon the rebel position and four were killed before they reached Beggars Bush barracks.
The only substantial combat of the first day of the Rising took place at the South Dublin Union where a piquet from the Royal Irish Regiment encountered an outpost of Éamonn Ceannt's force at the northwestern corner of the South Dublin Union. The British troops, after taking some casualties, managed to regroup and launch several assaults on the position before they forced their way inside and the small rebel force in the tin huts at the eastern end of the Union surrendered. However, the Union complex as a whole remained in rebel hands. A nurse in uniform, Margaret Keogh, was shot dead by British soldiers at the Union. She is believed to have been the first civilian killed in the Rising.
Three unarmed Dublin Metropolitan Police were shot dead on the first day of the Rising and their Commissioner pulled them off the streets. Partly as a result of the police withdrawal, a wave of looting broke out in the city centre, especially in the O'Connell Street area. A total of 425 people were arrested after the Rising for looting.
Tuesday and Wednesday.
Lord Wimborne, the Lord Lieutenant, declared martial law on Tuesday evening and handed over civil power to Brigadier-General William Lowe. British forces initially put their efforts into securing the approaches to Dublin Castle and isolating the rebel headquarters, which they believed was in Liberty Hall. The British commander, Lowe, worked slowly, unsure of the size of the force he was up against, and with only 1,269 troops in the city when he arrived from the Curragh Camp in the early hours of Tuesday 25 April. City Hall was taken from the rebel unit that had attacked Dublin Castle on Tuesday morning.
In the early hours of Tuesday, 120 British soldiers, with machine-guns, occupied two buildings overlooking St Stephen's Green: the Shelbourne Hotel and United Services Club. At dawn they opened fire on the Citizen Army occupying the green. The rebels returned fire, but were forced to retreat to the Royal College of Surgeons building. They remained there for the rest of the week, exchanging fire with British forces.
Fighting erupted along the northern edge of the city centre on Tuesday afternoon. In the northeast, British troops left Amiens St station in an armoured train, to secure and repair a section of damaged tracks. They were attacked by rebels who had taken up position at Annesley Bridge. After a two-hour battle, the British were forced to retreat and several soldiers were captured. At Phibsborough, in the northwest, rebels had occupied buildings and erected barricades at junctions on the North Circular Road. The British summoned 18-pounder field artillery from Athlone and shelled the rebel positions, destroying the barricades. After a fierce firefight, the rebels withdrew. They later made an unsuccessful attack on troops at Broadstone railway station.
That afternoon, Pearse, walked out into O'Connell Street with a small escort and stood in front of Nelson's Pillar. As a large crowd gathered, he read out a ", calling on them to support the Rising.
The rebels had failed to take either of Dublin's two main train stations or either of its ports, at Dublin Port and Kingstown. As a result, during the following week, the British were able to bring in thousands of reinforcements from England and from their garrisons at the Curragh and Belfast. By the end of the week, British strength stood at over 16,000 men. Their firepower was provided by field artillery which they positioned on the northside of the city at Phibsborough and at Trinity College, and by the patrol vessel "Helga", which sailed up the River Liffey, having been summoned from the port at Kingstown. On Wednesday, 26 April, the guns at Trinity College and "Helga" shelled Liberty Hall, and the Trinity College guns then began firing at rebel positions, first at Boland's Mill and then in O'Connell Street. Some rebel commanders, particularly James Connolly, did not believe that the British would shell the 'second city' of the British Empire.
The principal rebel positions at the GPO, the Four Courts, Jacob's Factory and Boland's Mill saw little combat. The British surrounded and bombarded them rather than assault them directly. One Volunteer in the GPO recalled, "we did practically no shooting as there was no target". However, where the insurgents dominated the routes by which the British tried to funnel reinforcements into the city, there was fierce fighting.
On Wednesday morning, hundreds of British troops encircled the Mendicity Institute, which was occupied by 26 Volunteers under Seán Heuston. British troops advanced on the building, supported by snipers and machine gun fire, but the Volunteers put up stiff resistance. Eventually, the troops got close enough to hurl grenades into the building, some of which the rebels threw back. Exhausted and almost out of ammunition, Heuston's men became the first rebel position to surrender. Heuston had been ordered to hold his position for a few hours, to delay the British, but had held on for three days.
Reinforcements were sent to Dublin from England, and disembarked at Kingstown on the morning of Wednesday 26 April. Heavy fighting occurred at the rebel-held positions around the Grand Canal as these troops advanced towards Dublin. More than 1,000 Sherwood Foresters were repeatedly caught in a cross-fire trying to cross the canal at Mount Street Bridge. Seventeen Volunteers were able to severely disrupt the British advance, killing or wounding 240 men. Despite there being alternative routes across the canal nearby, General Lowe ordered repeated frontal assaults on the Mount Street position. The British eventually took the position, which had not been reinforced by the nearby rebel garrison at Boland's Mills, on Thursday, but the fighting there inflicted up to two thirds of their casualties for the entire week for a cost of just four dead Volunteers. It had taken nearly nine hours for the British to advance .
Thursday to Saturday.
The rebel position at the South Dublin Union (site of the present day St. James's Hospital) and Marrowbone Lane, further west along the canal, also inflicted heavy losses on British troops. The South Dublin Union was a large complex of buildings and there was vicious fighting around and inside the buildings. Cathal Brugha, a rebel officer, distinguished himself in this action and was badly wounded. By the end of the week, the British had taken some of the buildings in the Union, but others remained in rebel hands. British troops also took casualties in unsuccessful frontal assaults on the Marrowbone Lane Distillery.
The third major scene of fighting during the week was in the area of North King Street, north of the Four Courts. The rebels had established strong outposts in the area, occupying numerous small buildings and barricading the streets. From Thursday to Saturday, the British made repeated attempts to take the area, in what was some of the fiercest fighting of the Rising. As the troops moved in, the rebels continually opened fire from windows and behind chimneys and barricades. At one point, a platoon led by Major Sheppard made a bayonet charge on one of the barricades, but was cut down by rebel fire. The British employed machine guns and attempted to avoid direct fire by using makeshift armoured trucks, and by mouse-holing through the inside walls of terraced houses to get near the rebel positions. By the time of the rebel headquarters' surrender on Saturday, the South Staffordshire Regiment under Colonel Taylor had advanced only down the street at a cost of 11 dead and 28 wounded. The enraged troops broke into the houses along the street and shot or bayoneted 15 unarmed male civilians whom they accused of being rebel fighters.
Elsewhere, at Portobello Barracks, an officer named Bowen Colthurst summarily executed six civilians, including the pacifist nationalist activist, Francis Sheehy-Skeffington. These instances of British troops killing Irish civilians would later be highly controversial in Ireland.
Surrender.
The headquarters garrison at the GPO, after days of shelling, was forced to abandon their headquarters when fire caused by the shells spread to the GPO. Connolly had been incapacitated by a bullet wound to the ankle and had passed command on to Pearse. The O'Rahilly was killed in a sortie from the GPO. They tunnelled through the walls of the neighbouring buildings in order to evacuate the Post Office without coming under fire and took up a new position in 16 Moore Street.
On Saturday 29 April, from this new headquarters, after realising that they could not break out of this position without further loss of civilian life, Pearse issued an order for all companies to surrender. Pearse surrendered unconditionally to Brigadier-General Lowe. The surrender document read:
The other posts surrendered only after Pearse's surrender order, carried by nurse Elizabeth O'Farrell, reached them. Sporadic fighting therefore continued until Sunday, when word of the surrender was got to the other rebel garrisons. Command of British forces had passed from Lowe to General John Maxwell, who arrived in Dublin just in time to take the surrender. Maxwell was made temporary military governor of Ireland.
The Rising outside Dublin.
Irish Volunteer units mobilised on Easter Sunday in several places outside of Dublin, but because of Eoin MacNeill's countermanding order, most of them returned home without fighting. In addition, because of the interception of the German arms aboard the "Aud", the provincial Volunteer units were very poorly armed.
In the south, around 1,200 Volunteers mustered in Cork, under Tomás Mac Curtain, on the Sunday, but they dispersed on Wednesday after receiving nine contradictory orders by dispatch from the Volunteer leadership in Dublin. At their Sheares Street headquarters, some of the Volunteers engaged in a standoff with British forces. Much to the anger of many Volunteers, MacCurtain, under pressure from Catholic clergy, agreed to surrender his men's arms to the British. The only violence in Cork occurred when the RIC attempted to raid the home of the Kent family. The Kent brothers, who were Volunteers, engaged in a three-hour firefight with the RIC. An RIC officer and one of the brothers were killed, while another brother was later executed.
In the north, Volunteer companies were mobilised in County Tyrone at Coalisland (including 132 men from Belfast led by IRB President Dennis McCullough) and Carrickmore, under the leadership of Patrick McCartan. They also mobilised at Creeslough, County Donegal under Daniel Kelly and James McNulty. However, in part because of the confusion caused by the countermanding order, the Volunteers in these locations dispersed without fighting.
Fingal.
In Fingal (or north County Dublin), about 60 Volunteers mobilised near Swords. They belonged to the 5th Battalion of the Dublin Brigade (also known as the Fingal Battalion), and were led by Thomas Ashe and his second in command, Richard Mulcahy. Unlike the rebels elsewhere, the Fingal Battalion successfully employed guerrilla tactics. They set up camp and Ashe split the battalion into four sections: three would undertake operations while the fourth was kept in reserve, guarding camp and foraging for food. The Volunteers moved against the RIC barracks in Swords, Donabate and Garristown, forcing the RIC to surrender and seizing all the weapons. They also damaged railway lines and cut telgraph wires. The railway line at Blanchardstown was bombed to prevent a troop train reaching Dublin. This derailed a cattle train, which had been sent ahead of the troop train.
The only large-scale engagement of the Rising, outside Dublin city, was at Ashbourne. On Friday, about 35 Fingal Volunteers surrounded the Ashbourne RIC barracks and called on it to surrender, but the RIC responded with a volley of gunfire. A firefight followed, and the RIC surrendered after the Volunteers attacked the building with a homemade grenade. Before the surrender could be taken, up to sixty RIC men arrived in a convoy, sparking a five-hour gun battle, in which eight RIC men were killed and 18 wounded. Two Volunteers were also killed and five wounded, and a civilian was fatally shot. The RIC surrendered and were disarmed. Ashe let them go after warning them not to fight against the Irish Republic again. Ashe's men camped at Kilsalaghan near Dublin until they received orders to surrender on Saturday. The Fingal Battalion's tactics during the Rising foreshadowed those of the IRA during the War of Independence that followed.
Volunteer contingents also mobilised nearby in counties Meath and Louth, but proved unable to link up with the North Dublin unit until after it had surrendered. In County Louth, Volunteers shot dead an RIC man near the village of Castlebellingham on 24 April, in an incident in which 15 RIC men were also taken prisoner.
Enniscorthy.
In County Wexford, 100–200 Volunteers—led by Robert Brennan, Séamus Doyle and Seán Etchingham—took over the town of Enniscorthy on Thursday 27 April until Sunday. Volunteer officer Paul Galligan had cycled 200 km from rebel headquarters in Dublin with orders to mobilise. They blocked all roads into the town and made a brief attack on the RIC barracks, but chose to blockade it rather than attempt to capture it. They flew the tricolour over the Athenaeum building, which they had made their headquarters, and paraded uniformed in the streets. They also occupied Vinegar Hill, where the United Irishmen had made a last stand in the 1798 rebellion. The public largely supported the rebels and many local men offered to join them.
By Saturday, up to 1,000 rebels had been mobilised, and a detachment was sent to occupy the nearby village of Ferns. In Wexford, the British assembled a column of 1,000 soldiers (including the Connaught Rangers), two field guns and a 4.7 inch naval gun on a makeshift armoured train. On Sunday, the British sent messengers to Enniscorthy, informing the rebels of Pearse's surrender order. However, the Volunteer officers were skeptical. Two of them were escorted by the British to Arbour Hill Prison, where Pearse confirmed the surrender order.
Galway.
In County Galway, 600–700 Volunteers mobilised on Tuesday under Liam Mellows. His plan was to "bottle up the British garrison and divert the British from concentrating on Dublin". However, his men were poorly armed, with only 25 rifles, 60 revolvers, 300 shotguns and some homemade grenades – many of them only had pikes. Most of the action took place in a rural area to the east of Galway city. They made unsuccessful attacks on the RIC barracks at Clarinbridge and Oranmore, captured several officers, and bombed a bridge and railway line, before taking up position near Athenry. There was also a skirmish between rebels and an RIC mobile patrol at Carnmore crossroads. A constable, Patrick Whelan, was shot dead after he had called to the rebels: "Surrender, boys, I know ye all".
On Wednesday, arrived in Galway Bay and shelled the countryside on the northeastern edge of Galway. The rebels retreated southeast to Moyode, an abandoned country house and estate. From here they set up lookout posts and sent out scouting parties. On Friday, landed 200 Royal Marines and began shelling the countryside near the rebel position. The rebels retreated further south to Limepark, another abandoned country house. Deeming the situation to be hopeless, they dispersed on Saturday morning. Many went home and were arrested following the rising, while others, including Mellows, went "on the run". By the time British reinforcements arrived in the west, the rising there had already disintegrated.
Casualties.
The Easter Rising resulted in at least 485 deaths, according to the Glasnevin Trust.
Of those killed:
More than 2,600 were wounded; including at least 2,200 civilians and rebels, at least 370 British soldiers and 29 policemen. All 16 police fatalities and 22 of the British soldiers killed were Irishmen. About 40 of those killed were children (i.e. under 17 years old), four of whom were members of the rebel forces.
The number of casualties each day steadily rose, with 55 killed on Monday and 78 killed on Saturday. The British Army suffered their biggest losses in the Battle of Mount Street Bridge on Wednesday, when at least 30 soldiers were killed. The rebels also suffered their biggest losses on that day. The RIC suffered most of their casualties in the Battle of Ashbourne on Friday.
The majority of the casualties, both killed and wounded, were civilians. Most of the civilian casualties, and most of the casualties overall, were caused by the British Army. This was due to the British using artillery, incendiary shells and heavy machine guns in built-up areas, as well as their "inability to discern rebels from civilians". One Royal Irish Regiment officer recalled, "they regarded, not unreasonably, everyone they saw as an enemy, and fired at anything that moved". Many other civilians were killed when caught in the crossfire. Both sides, British and rebel, also shot civilians deliberately on occasion; for not obeying orders (such as to stop at checkpoints), for assaulting or attempting to hinder them, and for looting. There were also instances of British troops killing unarmed civilians out of revenge or frustration: notably in the North King Street Massacre, where fifteen were killed, and at Portobello Barracks, where six were shot. Furthermore, there were incidents of friendly fire. On 29 April, the Royal Dublin Fusiliers under Sgt Robert Flood shot dead two British officers and two Irish civilian employees of the Guinness brewery after mistaking them for rebels. The sergeant was court-martialled for murder but acquitted.
According to historian Fearghal McGarry, the rebels attempted to avoid needless bloodshed. Desmond Ryan stated that Volunteers were told "no firing was to take place except under orders or to repel attack". Aside from the engagement at Ashbourne, policemen and unarmed soldiers were not systematically targeted, and a large group of policemen were allowed to stand at Nelson's Pillar throughout Monday. McGarry writes that the Irish Citizen Army "were more ruthless than Volunteers when it came to shooting policemen" and attributes this to the "acrimonious legacy" of the Dublin Lock-out.
The vast majority of the Irish casualties were buried in Glasnevin Cemetery in the aftermath of the fighting. British families came to Dublin Castle in May 1916 to reclaim the bodies of British soldiers, and funerals were arranged. Soldiers whose bodies were not claimed were given military funerals in Grangegorman Military Cemetery.
Aftermath.
Arrests and executions.
General Maxwell quickly signalled his intention "to arrest all dangerous Sinn Feiners", including "those who have taken an active part in the movement although not in the present rebellion", reflecting the popular belief that Sinn Féin, a separatist organisation that was neither militant nor republican, was behind the Rising.
A total of 3,430 men and 79 women were arrested, although most were subsequently released. When the RIC attempted to arrest members of the Kent family in County Cork on 2 May, a Head Constable and one of the Kent brothers were killed in the ensuing gun battle. The other brothers were arrested. Most of those arrested were held at Richmond Barracks, Dublin.
A series of courts-martial began on 2 May, in which 187 people were tried, most of them at Richmond Barracks. The president of the courts-martial was Charles Blackader. Controversially, Maxwell decided that the courts-martial would be held in secret and without a defence, which Crown law officers later ruled to have been illegal. Some of those who conducted the trials had commanded British troops involved in suppressing the Rising, a conflict of interest that the Military Manual prohibited. Only one of those tried by courts-martial was a woman, Constance Markievicz. Ninety were sentenced to death. Fifteen of those (including all seven signatories of the Proclamation) had their sentences confirmed by Maxwell and were executed by firing squad at Kilmainham Gaol between 3 and 12 May. Among them was the seriously wounded Connolly, who was shot while tied to a chair because of his shattered ankle. Maxwell stated that only the "ringleaders" and those proven to have committed "coldblooded murder" would be executed. However, the evidence presented was weak, and some of those executed were not leaders and did not kill anyone: Willie Pearse described himself as "a personal attaché to my brother, Patrick Pearse"; John MacBride had not even been aware of the Rising until it began, but had fought against the British in the Boer War fifteen years before; Thomas Kent did not come out at all—he was executed for the killing of a police officer during the raid on his house the week after the Rising. The most prominent leader to escape execution was Éamon de Valera, Commandant of the 3rd Battalion, who did so partly because of his American birth.
Most of the executions took place over a nine-day period:
As the executions went on, the Irish public grew increasingly hostile towards the British and sympathetic to the rebels. After the first three executions, John Redmond, leader of the moderate Irish Parliamentary Party, said in the British Parliament that the rising "happily, seems to be over. It has been dealt with with firmness, which was not only right, but it was the duty of the Government to so deal with it". However, he urged the Government "not to show undue hardship or severity to the great masses of those who are implicated". As the executions continued, Redmond pleaded with Prime Minister H. H. Asquith to stop them, warning that "if more executions take place in Ireland, the position will become impossible for any constitutional party". Ulster Unionist Party leader Edward Carson expressed similar views. Redmond's deputy, John Dillon, made an impassioned speech in parliament, saying "thousands of people […] who ten days ago were bitterly opposed to the whole of the Sinn Fein movement and to the rebellion, are now becoming infuriated against the Government on account of these executions". He said "it is not murderers who are being executed; it is insurgents who have fought a clean fight, a brave fight, however misguided". Dillon was heckled by English MPs. The British Government itself had also become concerned at the reaction to the executions, and at the way the courts-martial were being carried out. Asquith had warned Maxwell that "a large number of executions would […] sow the seeds of lasting trouble in Ireland". After Connolly's execution, Maxwell bowed to pressure and had the other death sentences commuted to penal servitude.
1,836 men were interned at internment camps and prisons in England and Wales under Regulation 14B of the Defence of the Realm Act 1914. Many of them, like Arthur Griffith, had little or nothing to do with the Rising. Camps such as Frongoch internment camp became "Universities of Revolution" where future leaders like Michael Collins, Terence McSwiney and J. J. O'Connell began to plan the coming struggle for independence.
Sir Roger Casement was tried in London for high treason and hanged at Pentonville Prison on 3 August.
Claims of British atrocities.
After the Rising, claims of atrocities carried out by British troops began to emerge. Although they did not receive as much attention as the executions, they sparked outrage among the Irish public and were raised by Irish MPs in Parliament.
One incident was the 'Portobello killings'. On Tuesday 25 April, Dubliner Francis Sheehy-Skeffington, a pacifist nationalist activist, had been arrested by British soldiers. Captain John Bowen-Colthurst then took him with a British raiding party as a hostage and human shield. On Rathmines Road he stopped a boy named James Coade, whom he shot dead. His troops then destroyed a tobacconist's shop with grenades and seized journalists Thomas Dickson and Patrick MacIntyre. The next morning, Colthurst had Skeffington and the two journalists shot by firing squad in Portobello Barracks. The bodies were then buried there. Later that day he shot a Labour Party councillor, Richard O'Carroll. When Major Sir Francis Vane learned of the killings he telephoned his superiors in Dublin Castle, but no action was taken. Vane informed Herbert Kitchener, who told General Maxwell to arrest Colthurst, but Maxwell refused. Colthurst was eventually arrested and court-martialled in June. He was found guilty of murder but insane, and detained for twenty months at Broadmoor. Public and political pressure led to a public inquiry, which reached similar conclusions. Major Vane was discharged "owing to his action in the Skeffington murder case".
The other incident was the 'North King Street massacre'. On the night of 28–29 April, British soldiers of the South Staffordshire Regiment, under Colonel Henry Taylor, had burst into houses on North King Street and killed 15 male civilians whom they accused of being rebels. The soldiers shot or bayoneted the victims, then secretly buried some of them in cellars or back yards after robbing them. The area saw some of the fiercest fighting of the Rising and the British had taken heavy casualties for little gain. General Maxwell attempted to excuse the killings and argued that the rebels were ultimately responsible. He claimed that "the rebels wore no uniform" and that the people of North King Street were rebel sympathizers. Maxwell concluded that such incidents "are absolutely unavoidable in such a business as this" and that "Under the circumstance the troops [...] behaved with the greatest restraint". A private brief, prepared for the Prime Minister, said the soldiers "had orders not to take any prisoners" but took it to mean they were to shoot any suspected rebel. The City Coroner's inquest found that soldiers had killed "unarmed and unoffending" residents. The military court of inquiry ruled that no specific soldiers could be held responsible, and no action was taken.
These killings, and the British response to them, helped sway Irish public opinion against the British.
Inquiry.
A Royal Commission was set up to enquire into the causes of the Rising. It began hearings on 18 May under the chairmanship of Lord Hardinge of Penshurst. The Commission heard evidence from Sir Matthew Nathan, Augustine Birrell, Lord Wimborne, Sir Neville Chamberlain (Inspector-General of the Royal Irish Constabulary), General Lovick Friend, Major Ivor Price of Military Intelligence and others. The report, published on 26 June, was critical of the Dublin administration, saying that "Ireland for several years had been administered on the principle that it was safer and more expedient to leave the law in abeyance if collision with any faction of the Irish people could thereby be avoided." Birrell and Nathan had resigned immediately after the Rising. Wimborne had also reluctantly resigned, recalled to London by Lloyd George, but was re-appointed in late 1917. Chamberlain resigned soon after.
Reaction of the Dublin public.
At first, many Dubliners were bewildered by the outbreak of the Rising. James Stephens, who was in Dublin during the week, thought, "None of these people were prepared for Insurrection. The thing had been sprung on them so suddenly they were unable to take sides."
There was great hostility towards the Volunteers in some parts of the city. Historian Keith Jeffery noted that most of the opposition came from people whose relatives were in the British Army and who depended on their Army allowances. Those most openly hostile to the Volunteers were the "separation women" (so-called because they were paid "separation money" by the British government), whose husbands and sons were fighting in the British Army in World War I. There was also hostility from unionists. Supporters of the Irish Parliamentary Party also felt the rebellion was a betrayal of their party. When occupying positions in the South Dublin Union and Jacob's factory, the rebels got involved in physical confrontations with civilians who tried to tear down the rebel barricades and prevent them taking over buildings. The Volunteers shot and clubbed a number of civilians who assaulted them or tried to dismantle their barricades.
That the Rising resulted in a great deal of death and destruction, as well as disrupting food supplies, also contributed to the antagonism toward the rebels. After the surrender, the Volunteers were hissed at, pelted with refuse, and denounced as "murderers" and "starvers of the people". Volunteer Robert Holland for example remembered being "subjected to very ugly remarks and cat-calls from the poorer classes" as they marched to surrender. He also reported being abused by people he knew as he was marched through the Kilmainham area into captivity and said the British troops saved them from being manhandled by the crowd.
However, some Dubliners expressed support for the rebels. Canadian journalist and writer Frederick Arthur McKenzie wrote that in poorer areas, "there was a vast amount of sympathy with the rebels, particularly after the rebels were defeated". He wrote of crowds cheering a column of rebel prisoners as it passed, with one woman remarking "Shure, we cheer them. Why shouldn't we? Aren't they our own flesh and blood?". At Boland's Mill, the defeated rebels were met with a large crowd, "many weeping and expressing sympathy and sorrow, all of them friendly and kind". Other onlookers were sympathetic but watched in silence. Christopher M. Kennedy notes that "those who sympathized with the rebels would, out of fear for their own safety, keep their opinions to themselves". Áine Ceannt witnessed British soldiers arresting a woman who cheered the captured rebels. An RIC District Inspector's report stated: "Martial law, of course, prevents any expression of it; but a strong undercurrent of disloyalty exists". Thomas Johnson, the Labour leader, thought there was, "no sign of sympathy for the rebels, but general admiration for their courage and strategy".
The aftermath of the Rising, and in particular the British reaction to it, helped sway a large section of Irish nationalist opinion away from hostility or ambivalence and towards support for the rebels of Easter 1916. Dublin businessman and Quaker James G. Douglas, for example, hitherto a Home Ruler, wrote that his political outlook changed radically during the course of the Rising because of the British military occupation of the city and that he became convinced that parliamentary methods would not be enough to remove the British presence.
Rise of Sinn Féin.
A meeting called by Count Plunkett on 19 April 1917 led to the formation of a broad political movement under the banner of Sinn Féin which was formalised at the Sinn Féin Ard Fheis of 25 October 1917. The Conscription Crisis of 1918 further intensified public support for Sinn Féin before the general elections to the British Parliament on 14 December 1918, which resulted in a landslide victory for Sinn Féin, winning 75 seats out of 105, whose MPs gathered in Dublin on 21 January 1919 to form Dáil Éireann and adopt the Declaration of Independence.
Legacy.
Shortly after the Easter Rising, poet Francis Ledwidge wrote ""O’Connell Street"" and ""Lament for the Poets of 1916"", which both describe his sense of loss and an expression of holding the same "dreams", as the Easter Rising's Irish Republicans. He would also go on to write "lament for Thomas MacDonagh" for his fallen friend and fellow Irish Volunteer.
A few months after the Easter Rising, W. B. Yeats commemorated some of the fallen figures of the Irish Republican movement, as well as his torn emotions regarding these events, in the poem "Easter, 1916".
Some of the survivors of the Rising went on to become leaders of the independent Irish state. Those who were executed were venerated by many as martyrs; their graves in Dublin's former military prison of Arbour Hill became a national monument and the Proclamation text was taught in schools. An annual commemorative military parade was held each year on Easter Sunday, culminating in a huge national celebration on the 50th anniversary in 1966. RTÉ, the Irish national broadcaster, as one of its first major undertakings made a series of commemorative programmes for the 1966 anniversary of the Rising. Roibéárd Ó Faracháin, head of programming said, "While still seeking historical truth, the emphasis will be on homage, on salutation."
With the outbreak of the Troubles in Northern Ireland, government, academics and the media began to revise the country's militant past, and particularly the Easter Rising. The coalition government of 1973–77, in particular the Minister for Posts and Telegraphs, Conor Cruise O'Brien, began to promote the view that the violence of 1916 was essentially no different from the violence then taking place in the streets of Belfast and Derry.
O'Brien and others asserted that the Rising was doomed to military defeat from the outset, and that it failed to account for the determination of Ulster Unionists to remain in the United Kingdom.
Irish republicans continue to venerate the Rising and its leaders with murals in republican areas of Belfast and other towns celebrating the actions of Pearse and his comrades, and annual parades in remembrance of the Rising. The Irish government, however, discontinued its annual parade in Dublin in the early 1970s, and in 1976 it took the unprecedented step of proscribing (under the Offences against the State Act) a 1916 commemoration ceremony at the GPO organised by Sinn Féin and the Republican commemoration Committee. A Labour Party TD, David Thornley, embarrassed the government (of which Labour was a member) by appearing on the platform at the ceremony, along with Máire Comerford, who had fought in the Rising, and Fiona Plunkett, sister of Joseph Plunkett.
With the advent of a Provisional IRA ceasefire and the beginning of what became known as the Peace Process during the 1990s, the official view of the Rising grew more positive and in 1996 an 80th anniversary commemoration at the Garden of Remembrance in Dublin was attended by the Taoiseach and leader of Fine Gael, John Bruton. In 2005, the Taoiseach, Bertie Ahern, announced the government's intention to resume the military parade past the GPO from Easter 2006, and to form a committee to plan centenary celebrations in 2016. The 90th anniversary was celebrated with a military parade in Dublin on Easter Sunday, 2006, attended by the President of Ireland, the Taoiseach and the Lord Mayor of Dublin. There is now an annual ceremony at Easter attended by relatives of those who fought, by the President, the Taoiseach, ministers, senators and TDs, and by usually large and respectful crowds.
In December 2014 Dublin City Council approved a proposal to create a historical path commemorating the Rising, similar to the Freedom Trail in Boston. Lord Mayor of Dublin Christy Burke announced that the council had committed to building the trail, marking it with a green line or bricks, with brass plates marking the related historic sites such as the Rotunda and the General Post Office.
Date of commemoration.
The Easter Rising lasted from Easter Monday 24 April 1916 to Easter Saturday 29 April 1916. Annual commemorations, rather than taking place on 24–29 April, are typically based on the date of Easter, which is a moveable feast. For example, the annual military parade is on Easter Sunday; the date of coming into force of the Republic of Ireland Act 1948 was symbolically chosen as Easter Monday (18 April) 1949. The official programme of centenary events in 2016 climaxes from 25 March (Good Friday) to 2 April (Easter Saturday) with other events earlier and later in the year taking place on the calendrical anniversaries.

</doc>
<doc id="10353" url="https://en.wikipedia.org/wiki?curid=10353" title="Eschrichtiidae">
Eschrichtiidae

Eschrichtiidae or the gray whales is a family of baleen whale (suborder Mysticeti) with a single extant species, the gray whale ("Eschrichtius robustus"). The family, however, also includes three described fossil genera: "Archaeschrichtius" and "Eschrichtioides" from the Miocene and Pliocene of Italy respectively, and "Gricetoides" from the Pliocene of North Carolina. The names of the extant genus and the family honours Danish zoologist Daniel Eschricht.
Taxonomic history.
A number of 18th century authors described the gray whale as "Balaena gibbosa", the "whale with six bosses", apparently based on a brief note by :
The gray whale was first described as a distinct species by based on a subfossil found in the brackish Baltic Sea, apparently a specimen from the now extinct north Atlantic population. Lilljeborg, however, identified it as ""Balaenoptera robusta"", a species of rorqual. realized that the rib and scapula of the specimen was different from those of any known rorquals, and therefore erected a new genus for it, "Eschrichtius".
Things got increasingly confused as 19th century scientists introduced new species at an alarming rate (e.g. "Eschrichtius pusillus", "E. expansus", "E. priscus", "E. mysticetoides"), often based on fragmentary specimens, and taxonomists started to use several generic and specific names interchangeably and not always correctly (e.g. "Agalephus gobbosus", "Balaenoptera robustus", "Agalephus gibbosus"). Things got even worse in the 1930s when it was finally realised that the extinct Atlantic population was the same species as the extant Pacific population, and the new combination "Eschrichtius gibbosus" was proposed.
In his morphological analysis, found that eschrichtiids and Cetotheriidae ("Cetotherium", "Mixocetus" and "Metopocetus") form a monophyletic sister group of Balaenopteridae.
A specimen from the Late Pliocene of northern Italy, named ""Cetotherium" gastaldii" by and renamed ""Balaenoptera" gastaldii" by , was identified as a basal eschrichtiid by who recombined it to "Eschrichtioides gastaldii".
Evolution.
Fossils of Eschrichtiidae have been found in all major oceanic basins in the Northern Hemisphere, and the family is believed date back to the Late Miocene. Today, gray whales are only present in the northern Pacific, but a population was also present in the northern Atlantic before being driven to extinction by European whalers three centuries ago.
Fossil eschrichtiids from before the Holocene are rare compared to other fossil mysticetes. The only Pleistocene fossil from the Pacific referred to "E. eschrichtius" is a partial skeleton and an associated skull from California, estimated to be about 200 thousand years old. However, a late Pliocene fossil from Hokkaido, Japan, referred to "Eschrichtius" sp. is estimated to be and a similar unnamed fossil has been reported from California.
In their description of "Archaeschrichtius ruggieroi" from the late Miocene of Italy, argued that eschrichtiids most likely originated in the Mediterranean Basin about and remained there, either permanently or intermittently, at least until the Early Pliocene (5–3 Mya), (but see Messinian salinity crisis.)

</doc>
<doc id="10354" url="https://en.wikipedia.org/wiki?curid=10354" title="Edmund I">
Edmund I

Edmund I (; 921 – 26 May 946), called "the Elder", "the Deed-doer", "the Just", or "the Magnificent", was King of the English from 939 until his death. He was a son of Edward the Elder and half-brother of Æthelstan. Æthelstan died on 27 October 939, and Edmund succeeded him as king.
Military threats.
Edmund came to the throne as the son of Edward the Elder, grandson of Alfred the Great, great-grandson of Æthelwulf of Wessex, great-great grandson of Egbert of Wessex and great-great-great grandson of Ealhmund of Kent. Shortly after his proclamation as king, he had to face several military threats. King Olaf III Guthfrithson conquered Northumbria and invaded the Midlands; when Olaf died in 942, Edmund reconquered the Midlands. In 943, Edmund became the god-father of King Olaf of York. In 944, Edmund was successful in reconquering Northumbria. In the same year, his ally Olaf of York lost his throne and left for Dublin in Ireland. Olaf became the king of Dublin as Amlaíb Cuarán and continued to be allied to his god-father. In 945, Edmund conquered Strathclyde but ceded the territory to King Malcolm I of Scotland in exchange for a treaty of mutual military support. Edmund thus established a policy of safe borders and peaceful relationships with Scotland. During his reign, the revival of monasteries in England began.
Louis IV of France.
One of Edmund's last political movements of which there is some knowledge is his role in the restoration of Louis IV of France to the throne. Louis, son of Charles the Simple and Edmund's half-sister Eadgifu, had resided at the West-Saxon court for some time until 936, when he returned to be crowned King of France. In the summer of 945, he was captured by the Norsemen of Rouen and subsequently released to Duke Hugh the Great, who held him in custody. The chronicler Richerus claims that Eadgifu wrote letters both to Edmund and to Otto I, Holy Roman Emperor in which she requested support for her son. Edmund responded to her plea by sending angry threats to Hugh. Flodoard's "Annales", one of Richerus' sources, report:
Death and succession.
On 26 May 946, Edmund was murdered by Leofa, an exiled thief, while attending St Augustine's Day mass in Pucklechurch (South Gloucestershire). John of Worcester and William of Malmesbury add some lively detail by suggesting that Edmund had been feasting with his nobles, when he spotted Leofa in the crowd. He attacked the intruder in person, but in the event, Leofa killed him. Leofa was killed on the spot by those present. A recent article re-examines Edmund's death and dismisses the later chronicle accounts as fiction. It suggests the king was the victim of a political assassination.
Edmund's sister Eadgyth, the wife of Otto I, Holy Roman Emperor, died earlier the same year, as Flodoard's "Annales" for 946 report.
Edmund was succeeded as king by his brother Eadred, king from 946 until 955. Edmund's sons later ruled England as:

</doc>
<doc id="10356" url="https://en.wikipedia.org/wiki?curid=10356" title="Endothermic process">
Endothermic process

The term endothermic process describes a process or reaction in which the system absorbs energy from its surroundings; usually, but not always, in the form of heat. The term was coined by Marcellin Berthelot from the Greek roots "endo-", derived from the word "endon" (ἔνδον) meaning "within" and the root "therm" (θερμ-) meaning "hot." The intended sense is that of a reaction that depends on absorbing heat if it is to proceed. The opposite of an endothermic process is an exothermic process, one that releases, "gives out" energy in the form of (usually, but not always) heat. Thus in each term (endothermic & exothermic) the prefix refers to where heat goes as the reaction occurs, though in reality it only refers to where the energy goes, without necessarily being in the form of heat.
The concept is frequently applied in physical sciences to, for example, chemical reactions, where thermal energy (heat) is converted to chemical bond energy.
Endothermic (and exothermic) analysis only accounts for the enthalpy change (∆H) of a reaction. The full energy analysis of a reaction is the Gibbs free energy (∆G), which includes an entropy (∆S) and temperature term in addition to the enthalpy. A reaction will be a spontaneous process at a certain temperature if the products have a lower Gibbs free energy (an exergonic reaction) even if the enthalpy of the products is higher. Entropy and enthalpy are different terms, so the change in entropic energy can overcome an opposite change in enthalpic energy and make an endothermic reaction favorable.

</doc>
<doc id="10357" url="https://en.wikipedia.org/wiki?curid=10357" title="Earle Page">
Earle Page

Sir Earle Christmas Grafton Page, (8 August 188020 December 1961) was an Australian politician who served as the 11th Prime Minister of Australia in 1939. With 41 years, 361 days in Parliament, he is the third-longest serving federal parliamentarian in Australian history, behind only Billy Hughes and Philip Ruddock.
Early life.
Born in Grafton, New South Wales, Page was educated at Sydney Boys High School and the University of Sydney, where he graduated in medicine at the top of his year in 1901.
Page worked at Sydney's Royal Prince Alfred Hospital, where he met Ethel Blunt, a nurse, whom he married in 1906.
Pre-political career.
In 1903, Page joined a private practice in Grafton; and, in 1904, he became one of the first people in the country to own a car. He practised in Sydney and Grafton before joining the Australian Army as a medical officer in the First World War, serving in Egypt.
After the war, Page went into farming and was elected Mayor of Grafton.
Political career.
In 1919 Page was elected to the House of Representatives from Cowper in northeastern New South Wales. He ran as a candidate of the Farmers and Settlers Association of New South Wales, one of several farmers' groups that won seats in that election. Shortly before parliament opened in 1920, the Farmers and Settlers Association merged with several other rural-based parties to form the Country Party. He became the party's leader in 1921, ousting William McWilliams. The young party found itself with the balance of power in the House after the 1922 election. The Nationalist government of Billy Hughes lost its majority, and could not govern without Country Party support. However, the Country Party had been formed partly due to discontent with Hughes' rural policy, and Page's animosity toward Hughes was such that he would not even consider supporting him. When it was apparent that the Nationalists would have to turn to the Country Party in order to stay in office, Page demanded and got Hughes' resignation as the price for supporting the Nationalist government.
Page then began negotiations with Hughes' successor as leader of the Nationalists, Stanley Bruce. His terms were stiff; he wanted his Country Party to have five seats in an 11-man cabinet, including the post of Treasurer and the second rank in the ministry for himself. These demands were unprecedented for such a new party in a Westminster system. Nonetheless, as the Country Party was the Nationalists' only realistic coalition partner, Bruce accepted Page's terms. For all intents and purposes, Page was the first Deputy Prime Minister of Australia (a title that did not officially exist until 1968). Since then, the leader of the Country/National Party has been the second-ranking member in nearly every non-Labor government.
On 30 January 1924, as Acting Prime Minister, Page chaired the first meeting of Federal Cabinet ever held in Canberra, at Yarralumla House. This was still three years before the opening of Parliament House and Canberra becoming the National Capital.
Page continued his professional medical practice. On 22 October 1924, he had to tell his best friend, Thomas Shorten Cole (1870–1957), the news that his wife Mary Ann Crane had just died on the operating table from complications of intestinal or stomach cancer, reputed by their daughter Dorothy May Cole to be "the worst day of his life".
He was a strong believer in orthodox finance and conservative policies, except where the welfare of farmers was concerned: then he was happy to see government money spent freely. He was also a "high protectionist": a supporter of high tariff barriers to protect Australian rural industries.
The Bruce-Page government was defeated by Labor in 1929 (with Bruce losing his own seat), and Page went into opposition. In 1931, a group of dissident Labor MPs led by Joseph Lyons merged with the Nationalists to form the United Australia Party, with Lyons as leader. Page and the Country Party continued in coalition with the UAP. The UAP-Country Coalition won a comprehensive victory in the 1931 election. However, the UAP was in a strong enough position (only four seats short of a majority) that Lyons was able to form an exclusively UAP minority government with confidence and supply support from the Country Party—to date, the last time that the party has not had any posts in a non-Labor government. In 1934, however, the UAP suffered an eight-seat swing, forcing Lyons to take the Country Party back into his government in a full-fledged Coalition. Page became Minister for Commerce. He was made a Knight Grand Cross of the Order of St Michael and St George (GCMG) in the New Year’s Day Honours of 1938. While nine Australian Prime Ministers were knighted (and Bruce was elevated to the peerage), Page is the only one who was knighted before becoming Prime Minister.
Prime Minister.
When Lyons died suddenly in 1939, the Governor-General Lord Gowrie appointed Page as caretaker Prime Minister. He held the office for three weeks until the UAP elected former deputy leader Robert Menzies as its new leader, and hence Prime Minister. 
Page had been close to Lyons, but disliked Menzies, whom he charged publicly with having been disloyal to Lyons. When Menzies was elected UAP leader, Page refused to serve under him, and made an extraordinary personal attack on him in the House, accusing him not only of ministerial incompetence but of physical cowardice (for failing to enlist during World War I). His party soon rebelled, though, and Page was deposed as Country Party leader and replaced by Archie Cameron.
In 1940 Page and Menzies patched up their differences for the sake of the war effort, and Page returned to the cabinet as Minister for Commerce. Nevertheless, Page's accusations were not forgotten and were occasionally raised in parliament by Menzies' opponents (notably Eddie Ward). In 1941, the government fell; and Page spent the eight years of the Curtin and Chifley Labor governments on the opposition backbench. He was made a Companion of Honour (CH) in June 1942.
New states.
In 1949 Page put forward a discussion paper on the redrawing of state boundaries. He proposed that Australia would be divided into 12 states. Queensland would be split into four states: Eden-Monaro and East Gippsland would become another state, Mount Gambier to Mildura and Cape Otway another state, and the Northern Territory divided into two. Page had a particular interest in forming a new state of New England, in his own area of northeast New South Wales. 
Return to the ministry.
Menzies returned to the Prime Ministership in 1949, and Page was made Minister for Health. He held this post until 1956, when he was 76, then retired to the backbench. Upon the death of Billy Hughes in October 1952, Page became the Father of the House of Representatives and Father of the Parliament. 
Family.
Page and his wife Ethel had five children: a daughter Mary born in 1909 and four sons. Ethel Page died in 1958.
On 20 July 1959, Page married his secretary, Jean Thomas. The second Lady Page, died on 20 June 2011. Page's grandson, Don Page, was a National MP in the NSW Parliament and served as Deputy Leader of the NSW Nationals from 2003 to 2007. Another grandson is Canberra poet Geoff Page. His nephew was Robert Page, a soldier and war hero.
Earle Charles Page.
Page's oldest son, Earle Charles Page, was born in 1911 at South Grafton and attended Newington College (1922–1927), during the headmastership of the Rev Dr Charles Prescott. At Newington, he won the Wigram Allen Scholarship in 1927, awarded by Sir George Wigram Allen, for general proficiency, with Dave Cowper receiving it for classics and Bill Morrow for mathematics in the same year. In 1932, Page graduated in veterinary science from the University of Sydney.
Earle Charles Page played Rugby Union and was selected for Combined Australian Universities and, as a reserve, for NSW.
Earle Charles Page was managing a rural property for his father at the time of his death in 1933 by lightning strike.
The other Pages boys were Donald (b. 1912), Iven (b. 1914) and Douglas (b. 1916).
Later life and death.
Page was the first Chancellor of the University of New England, which was established in 1954.
By the 1961 election, Page was gravely ill, suffering from lung cancer. Although he was too sick to actively campaign, Page refused to even consider retiring from Parliament and soldiered on for his 17th general election. In one of the great upsets of Australian electoral history, he lost his seat to Labor challenger Frank McGuren, whom he had defeated soundly in 1958. Page had gone into the election holding Cowper with what appeared to be an insurmountable 11-point majority, but McGuren managed to win the seat on a swing of 13%.
Page had campaigned sporadically before going to Royal Prince Alfred Hospital in Sydney for emergency surgery. He went comatose a few days before the election and never regained consciousness. He died on 20 December, 11 days after the election, without ever knowing that he had been defeated.
Page had represented Cowper for just four days short of 42 years, making him the longest-serving Australian federal parliamentarian who represented the same seat throughout his career. Only Billy Hughes and Philip Ruddock have served in Parliament longer than Page, but Hughes represented four different seats in two states while Ruddock has represented three seats in New South Wales.

</doc>
<doc id="10358" url="https://en.wikipedia.org/wiki?curid=10358" title="Ephrem the Syrian">
Ephrem the Syrian

Ephrem the Syrian(or Ephrem the Syriac; ("Mār Aprêm Sûryāyâ"); Greek: ; ; ca. 306 – 373) was a Syriac deacon and a prolific Syriac-language hymnographer and theologian of the 4th century from the region of Assyria. His works are hailed by Christians throughout the world, and many denominations venerate him as a saint. He has been declared a Doctor of the Church in Roman Catholicism. He is especially beloved in the Syriac Orthodox Church.
Ephrem wrote a wide variety of hymns, poems, and sermons in verse, as well as prose biblical exegesis. These were works of practical theology for the edification of the church in troubled times. So popular were his works, that, for centuries after his death, Christian authors wrote hundreds of pseudepigraphal works in his name. Ephrem's works witness to an early form of Christianity in which Western ideas take little part. He has been called the most significant of all of the fathers of the Syriac-speaking church tradition.
Life.
Ephrem was born around the year 306 in the city of Nisibis (now Nusaybin in Turkey, on the border with Assyria, in Roman Mesopotamia, then recently acquired by the Roman Empire). 
Internal evidence from Ephrem's hymnody suggests that both his parents were part of the growing Christian community in the city, although later hagiographers wrote that his father was a pagan priest. Numerous languages were spoken in the Nisibis of Ephrem's day, mostly dialects of Aramaic. The Christian community used the Syriac dialect. The culture included pagan religions, Judaism and early Christian sects.
Jacob, the second bishop of Nisibis, was appointed in 308, and Ephrem grew up under his leadership of the community. Jacob of Nisibis is recorded as a signatory at the First Council of Nicea in 325. Ephrem was baptized as a youth and almost certainly became a son of the covenant, an unusual form of syriac proto-monasticism. Jacob appointed Ephrem as a teacher (Syriac "malp̄ānâ", a title that still carries great respect for Syriac Christians). He was ordained as a deacon either at his baptism or later. He began to compose hymns and write biblical commentaries as part of his educational office. In his hymns, he sometimes refers to himself as a "herdsman" (, "‘allānâ"), to his bishop as the "shepherd" (, "rā‘yâ"), and to his community as a 'fold' (, "dayrâ"). Ephrem is popularly credited as the founder of the School of Nisibis, which, in later centuries, was the centre of learning of the Syriac Orthodox Church.
In 337, Emperor Constantine I, who had legalised and promoted the practice of Christianity in the Roman Empire, died. Seizing on this opportunity, Shapur II of Persia began a series of attacks into Roman North Mesopotamia. Nisibis was besieged in 338, 346 and 350. During the first siege, Ephrem credits Bishop Jacob as defending the city with his prayers. In the third siege, of 350, Shapur rerouted the River Mygdonius to undermine the walls of Nisibis. The Nisibenes quickly repaired the walls while the Persian elephant cavalry became bogged down in the wet ground. Ephrem celebrated what he saw as the miraculous salvation of the city in a hymn that portrayed Nisibis as being like Noah's Ark, floating to safety on the flood.
One important physical link to Ephrem's lifetime is the baptistery of Nisibis. The inscription tells that it was constructed under Bishop Vologeses in 359. In that year, Shapur attacked again. The cities around Nisibis were destroyed one by one, and their citizens killed or deported. Constantius II was unable to respond; the campaign of Julian in 363 ended with his death in battle. His army elected Jovian as the new emperor, and to rescue his army, he was forced to surrender Nisibis to Persia (also in 363) and to permit the expulsion of the entire Christian population.
Ephrem, with the others, went first to Amida (Diyarbakır), eventually settling in Edessa (modern Şanlıurfa) in 363. Ephrem, in his late fifties, applied himself to ministry in his new church and seems to have continued his work as a teacher, perhaps in the School of Edessa. Edessa had always been at the heart of the Syriac-speaking world, and the city was full of rival philosophies and religions. Ephrem comments that orthodox Nicene Christians were simply called "Palutians" in Edessa, after a former bishop. Arians, Marcionites, Manichees, Bardaisanites and various gnostic sects proclaimed themselves as the true church. In this confusion, Ephrem wrote a great number of hymns defending Nicene orthodoxy. A later Syriac writer, Jacob of Serugh, wrote that Ephrem rehearsed all-female choirs to sing his hymns set to Syriac folk tunes in the forum of Edessa. After a ten-year residency in Edessa, in his sixties, Ephrem succumbed to the plague as he ministered to its victims. The most reliable date for his death is 9 June 373.
Writings.
Over four hundred hymns composed by Ephrem still exist. Granted that some have been lost, Ephrem's productivity is not in doubt. The church historian Sozomen credits Ephrem with having written over three million lines. Ephrem combines in his writing a threefold heritage: he draws on the models and methods of early Rabbinic Judaism, he engages skillfully with Greek science and philosophy, and he delights in the Mesopotamian/Persian tradition of mystery symbolism.
The most important of his works are his lyric, teaching hymns (, "madrāšê"). These hymns are full of rich, poetic imagery drawn from biblical sources, folk tradition, and other religions and philosophies. The madrāšê are written in stanzas of syllabic verse and employ over fifty different metrical schemes. Each madrāšâ had its "qālâ" (), a traditional tune identified by its opening line. All of these qālê are now lost. It seems that Bardaisan and Mani composed madrāšê, and Ephrem felt that the medium was a suitable tool to use against their claims. The madrāšê are gathered into various hymn cycles. Each group has a title — "Carmina Nisibena", "On Faith", "On Paradise", "On Virginity", "Against Heresies" — but some of these titles do not do justice to the entirety of the collection (for instance, only the first half of the "Carmina Nisibena" is about Nisibis). Each madrāšâ usually had a refrain (, "‘ûnîṯâ"), which was repeated after each stanza. Later writers have suggested that the madrāšê were sung by all-women choirs with an accompanying lyre.
Particularly influential were his "Hymns Against Heresies". Ephrem used these to warn his flock of the heresies that threatened to divide the early church. He lamented that the faithful were "tossed to and fro and carried around with every wind of doctrine, by the cunning of men, by their craftiness and deceitful wiles." He devised hymns laden with doctrinal details to inoculate right-thinking Christians against heresies such as docetism. The "Hymns Against Heresies" employ colourful metaphors to describe the Incarnation of Christ as fully human and divine. Ephrem asserts that Christ's unity of humanity and divinity represents peace, perfection and salvation; in contrast, docetism and other heresies sought to divide or reduce Christ's nature and, in doing so, rend and devalue Christ's followers with their false teachings.
Ephrem also wrote verse homilies (, "mêmrê"). These sermons in poetry are far fewer in number than the madrāšê. The mêmrê were written in a heptosyllabic couplets (pairs of lines of seven syllables each).
The third category of Ephrem's writings is his prose work. He wrote a biblical commentary on the Diatessaron (the single gospel harmony of the early Syriac church), the Syriac original of which was found in 1957. His "Commentary on Genesis and Exodus" is an exegesis of Genesis and Exodus. Some fragments exist in Armenian of his commentaries on the Acts of the Apostles and Pauline Epistles.
He also wrote refutations against Bardaisan, Mani, Marcion and others.
Ephrem is attributed with writing hagiographies such as The Life of Saint Mary the Harlot, though this credit is called into question.
Ephrem wrote exclusively in the Syriac language, but translations of his writings exist in Armenian, Coptic, Georgian, Greek and other languages. Some of his works are only extant in translation (particularly in Armenian). Syriac churches still use many of Ephrem's hymns as part of the annual cycle of worship. However, most of these liturgical hymns are edited and conflated versions of the originals.
The most complete, critical text of authentic Ephrem was compiled between 1955 and 1979 by Dom Edmund Beck, OSB, as part of the "Corpus Scriptorum Christianorum Orientalium".
Greek Ephrem.
Ephrem's artful meditations on the symbols of Christian faith and his stand against heresy made him a popular source of inspiration throughout the church. This occurred to the extent that there is a huge corpus of Ephrem pseudepigraphy and legendary hagiography. Some of these compositions are in verse, often a version of Ephrem's heptosyllabic couplets. Most of these works are considerably later compositions in Greek. Students of Ephrem often refer to this corpus as having a single, imaginary author called "Greek Ephrem", or "Ephraem Graecus" (as opposed to the real Ephrem the Syrian). This is not to say that all texts ascribed to Ephrem in Greek are by others, but many are. Although Greek compositions are the main source of pseudepigraphal material, there are also works in Latin, Slavonic and Arabic. There has been very little critical examination of these works, and many are still treasured by churches as authentic.
The best known of these writings is the "Prayer of Saint Ephrem", which is recited at every service during Great Lent and other fasting periods in Eastern Christianity.
Veneration as a saint.
Soon after Ephrem's death, legendary accounts of his life began to circulate. One of the earlier "modifications" is the statement that Ephrem's father was a pagan priest of Abnil or Abizal. However, internal evidence from his authentic writings suggest that he was raised by Christian parents. This legend may be anti-pagan polemic or may reflect his father's status prior to converting to Christianity. 
The second legend attached to Ephrem is that he was a monk. In Ephrem's day, monasticism was in its infancy in Egypt. He seems to have been a part of the members of the covenant, a close-knit, urban community of Christians that had "covenanted" themselves to service and had refrained from sexual activity. Some of the Syriac terms that Ephrem used to describe his community were later used to describe monastic communities, but the assertion that he was monk is anachronistic. Later hagiographers often painted a picture of Ephrem as an extreme ascetic, but the internal evidence of his authentic writings show him to have had a very active role, both within his church community and through witness to those outside of it.
Ephrem is venerated as an example of monastic discipline in Eastern Christianity. In the Eastern Orthodox scheme of hagiography, Ephrem is counted as a Venerable Father (i.e., a sainted Monk). His feast day is celebrated on 28 January and on the Saturday of the Venerable Fathers (Cheesefare Saturday), which is the Saturday before the beginning of Great Lent.
Ephrem is popularly believed to have taken legendary journeys. In one of these he visits Basil of Caesarea. This links the Syrian Ephrem with the Cappadocian Fathers and is an important theological bridge between the spiritual view of the two, who held much in common. Ephrem is also supposed to have visited Saint Pishoy in the monasteries of Scetes in Egypt. As with the legendary visit with Basil, this visit is a theological bridge between the origins of monasticism and its spread throughout the church.
On 5 October 1920, Pope Benedict XV proclaimed Ephrem a Doctor of the Church ("Doctor of the Syrians"). This proclamation was made before critical editions of Ephrem's authentic writings were available.
The most popular title for Ephrem is "Harp of the Spirit" (Syriac: , "Kenārâ d-Rûḥâ"). He is also referred to as the Deacon of Edessa, the Sun of the Syrians and a Pillar of the Church.
His Roman Catholic feast day of 9 June conforms to his date of death. For 48 years (1920–1969), it was on 18 June.
Ephrem is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on June 10.
Quotations.
About Ephrem:
By Ephrem:

</doc>
<doc id="10359" url="https://en.wikipedia.org/wiki?curid=10359" title="Amiga Enhanced Chip Set">
Amiga Enhanced Chip Set

The Enhanced Chip Set (ECS) is the second generation of the Amiga computer's chipset, offering minor improvements over the original chipset (OCS) design. ECS was introduced in 1990 with the launch of the Amiga 3000. Amigas produced from 1990 onwards featured a mix of OCS and ECS chips, such as later versions of the Amiga 500 and the Commodore CDTV. Other ECS models were the Amiga 500+ in 1991 and lastly the Amiga 600 in 1992. 
Notable improvements were the "Super Agnus" and the "Hires Denise" chips. The sound and floppy controller chip, "Paula", remained unchanged from the OCS design. Super Agnus supports 2 MB of CHIP RAM, whereas the original Agnus/"Fat Agnus" and subsequent "Fatter Agnus" can address 512 KB and 1 MB, respectively. The ECS Denise chip offers "Productivity" (640×480 non-interlaced) and "SuperHires" (1280×200 or 1280×256) display modes (also available in interlaced mode), which are however limited to only 4 on-screen colors. Essentially, a 35 ns pixel mode was added plus the ability to run arbitrary horizontal and vertical scan rates. This made other display modes possible, but only the aforementioned modes were supported originally out of the box. For example, the Linux Amiga framebuffer device driver allows the use of several other display modes. Other improvements were the ability of the blitter to copy regions larger than 1024×1024 pixels in one operation and the ability to display sprites in border regions (outside of any display window where bitplanes are shown). ECS also allows software switching between NTSC and PAL video modes.
These improvements largely favored application software, which benefited from higher resolution and VGA-like display modes, rather than gaming software. As an incremental update, ECS was intended to be backward compatible with software designed for OCS machines, though some pre-ECS games were found to be incompatible. Additionally, features from the improved Kickstart 2 operating system were used in subsequent software, and since these two technologies largely overlap, some users misjudged the significance of ECS. It is possible to upgrade some OCS machines, such as the Amiga 500, to obtain partial or full ECS functionality by replacing OCS chips with ECS versions. ECS was followed by the third generation AGA chipset with the launch of the Amiga 4000 in 1992.

</doc>
<doc id="10361" url="https://en.wikipedia.org/wiki?curid=10361" title="European Space Operations Centre">
European Space Operations Centre

The European Space Operations Centre (ESOC) serves as the main mission control centre for the European Space Agency (ESA) and is located in Darmstadt, Germany. ESOC's primary function is the operation of unmanned spacecraft on behalf of ESA and the launch and early orbit phases (LEOP) of ESA and third-party missions. The Centre is also responsible for a range of operations-related activities within ESA and in cooperation with ESA's industry and international partners, including ground systems engineering, software development, flight dynamics and navigation, development of mission control tools and techniques and space debris studies.
ESOC's current major activities comprise operating planetary and solar missions, such as Mars Express and Rosetta, astronomy & fundamental physics missions, such as Gaia (spacecraft) and XMM Newton, and Earth observation missions such as CryoSat2 and Swarm (spacecraft).
ESOC is responsible for developing, operating and maintaining ESA's European Tracking (ESTRACK) Network of ground stations. Teams at the Centre are also involved in research and development related to advanced mission control concepts and Space Situational Awareness, and standardisation activities related to frequency management; mission operations; tracking, telemetry and telecommanding; and space debris.
Missions.
ESOC's current missions comprise the following:
Planetary and solar missions
Astronomy and fundamental physics missions
Earth observation missions
In addition, the ground segment and mission control teams for several missions are in preparation and training, including:
ESTRACK.
ESOC hosts the control centre for the Agency's European Tracking ESTRACK station network. The core network comprises 9 stations in seven countries: Kourou (French Guiana), Maspalomas, Villafranca and Cebreros (Spain), Redu (Belgium), Santa Maria (Portugal), Kiruna (Sweden), Malargüe (Argentina) and New Norcia (Australia). Operators are on duty at ESOC 24 hours/day, year round, to conduct tracking passes, uploading telecommands and downloading telemetry and data.
Activities.
In addition to 'pure' mission operations, a number of other activities take place at the Centre, most of which are directly related to ESA's broader space operations activities.
History.
The European Space Operations Centre was formally inaugurated in Darmstadt, Germany, on 8 September 1967 by the then-Minister of Research of the Federal Republic of Germany, Gerhard Stoltenberg. Its role was to provide satellite control for the European Space Research Organisation (ESRO), which is today known as its successor organisation, the European Space Agency (ESA).
The 90-person ESOC facility was, as it is today, located on the west side of Darmstadt; it employed the staff and resources previously allocated to the European Space Data Centre (ESDAC), which had been established in 1963 to conduct orbit calculations. These were augmented by mission control staff transferred from ESTEC to operate satellites and manage the ESTRACK tracking station network.
Within just eight months, ESOC, as part of ESRO, was already operating its first mission, ESRO-2B, a scientific research satellite and the first of many operated from ESOC for ESRO, and later ESA.
By July 2012, ESOC had operated over 56 missions spanning science, Earth observation, orbiting observatories, meteorology and space physics.
Location and expansion.
ESOC is located on the west side of the city of Darmstadt, some from the main train station, at Robert-Bosch-Straße 5. In 2011, ESA announced the first phase of the ESOC II modernisation and expansion project valued at €60 million. The new construction will be located across Robert-Bosch-Straße, opposite the current centre.
Employees.
At ESOC, ESA employs approximately 800, comprising some 250 permanent staff and about 550 contractors. Staff from ESOC are routinely dispatched to work at other ESA establishments, ESTRACK stations, the ATV Control Centre (Toulouse), the Columbus Control Centre (Oberpfaffenhofen) and at partner facilities in several countries.

</doc>
<doc id="10363" url="https://en.wikipedia.org/wiki?curid=10363" title="European Space Agency">
European Space Agency

The European Space Agency (ESA) is an intergovernmental organisation dedicated to the exploration of space, with 22 member states. Established in 1975 and headquartered in Paris, France, ESA has a worldwide staff of about 2,000 and an annual budget of about €5.25 billion / US$5.77 billion (2016).
ESA's space flight programme includes human spaceflight, mainly through the participation in the International Space Station programme, the launch and operations of unmanned exploration missions to other planets and the Moon, Earth observation, science, telecommunication as well as maintaining a major spaceport, the Guiana Space Centre at Kourou, French Guiana, and designing launch vehicles. The main European launch vehicle Ariane 5 is operated through Arianespace with ESA sharing in the costs of launching and further developing this launch vehicle.
Its facilities are distributed among the following 5 research centres:
History.
Foundation.
After World War II, many European scientists left Western Europe in order to work with the United States. Although the 1950s boom made it possible for Western European countries to invest in research and specifically in space-related activities, Western European scientists realized solely national projects would not be able to compete with the two main superpowers. In 1958, only months after the Sputnik shock, Edoardo Amaldi and Pierre Auger, two prominent members of the Western European scientific community at that time, met to discuss the foundation of a common Western European space agency. The meeting was attended by scientific representatives from eight countries, including Harrie Massey (UK).
The Western European nations decided to have two different agencies, one concerned with developing a launch system, ELDO (European Launch Development Organization), and the precursor of the European Space Agency, ESRO (European Space Research Organisation). The latter was established on 20 March 1964 by an agreement signed on 14 June 1962. From 1968 to 1972, ESRO launched seven research satellites.
ESA in its current form was founded with the ESA Convention in 1975, when ESRO was merged with ELDO. ESA has 10 founding member states: Belgium, Denmark, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland and the United Kingdom. These signed the ESA Convention in 1975 and deposited the instruments of ratification by 1980, when the convention came into force. During this interval the agency functioned in a de facto fashion. ESA launched its first major scientific mission in 1975, Cos-B, a space probe monitoring gamma-ray emissions in the universe first worked on by ESRO.
Later activities.
ESA joined NASA in the IUE, the world's first high-orbit telescope, which was launched in 1978 and operated very successfully for 18 years. A number of successful Earth-orbit projects followed, and in 1986 ESA began Giotto, its first deep-space mission, to study the comets Halley and Grigg–Skjellerup. Hipparcos, a star-mapping mission, was launched in 1989 and in the 1990s SOHO, Ulysses and the Hubble Space Telescope were all jointly carried out with NASA. Recent scientific missions in cooperation with NASA include the Cassini–Huygens space probe, to which ESA contributed by building the Titan landing module Huygens.
As the successor of ELDO, ESA has also constructed rockets for scientific and commercial payloads. Ariane 1, launched in 1979, brought mostly commercial payloads into orbit from 1984 onward. The next two developments of the Ariane rocket were intermediate stages in the development of a more advanced launch system, the Ariane 4, which operated between 1988 and 2003 and established ESA as the world leader in commercial space launches in the 1990s. Although the succeeding Ariane 5 experienced a failure on its first flight, it has since firmly established itself within the heavily competitive commercial space launch market with 56 successful launches as of September 2011. The successor launch vehicle of Ariane 5, the Ariane 6 is already in the definition stage and is envisioned to enter service in the 2020s.
The beginning of the new millennium saw ESA become, along with agencies like NASA, JAXA, ISRO, CSA and Roscosmos, one of the major participants in scientific space research. Although ESA had relied on cooperation with NASA in previous decades, especially the 1990s, changed circumstances (such as tough legal restrictions on information sharing by the United States military) led to decisions to rely more on itself and on cooperation with Russia. A 2011 press issue thus stated:
Most notable for its new self-confidence are ESA's own recent successful missions SMART-1, a probe testing cutting-edge new space propulsion technology, the Mars Express and Venus Express missions as well as the development of the Ariane 5 rocket and its role in the ISS partnership. ESA maintains its scientific and research projects mainly for astronomy-space missions such as Corot, launched on 27 December 2006, a milestone in the search for extra-solar planets.
Mission.
The treaty establishing the European Space Agency reads:
ESA is responsible for setting a unified space and related industrial policy, recommending space objectives to the member states, and integrating national programs like satellite development, into the European program as much as possible.
Jean-Jacques Dordain ESA's Director General (2003-2015) outlined the European Space Agency's mission in a 2003 interview:
Activities and programmes.
ESA describes its work in two overlapping ways: 
Activities.
According the ESA website the activities are:
Programmes.
Mandatory.
Every member country must contribute to these programmes listed according to [http://www.czechspaceportal.cz/en/section-2/space-policy-eu---esa/mandatory-activities/]:
Optional.
Depending on their individual choices the countries can contribute to the following programmes listed according to [http://www.esa.int/About_Us/Ministerial_Council_2012/FACT_SHEET] :
Member states, funding and budget.
Membership and contribution to ESA.
ESA is an intergovernmental organisation of 22 member states. Member states participate to varying degrees in the mandatory (25% of total expenditures in 2008) and optional space programmes (75% of total expenditures in 2008). The 2008 budget amounted to €3.0 billion the 2009 budget to €3.6 billion. The total budget amounted to about €3.7 billion in 2010, €3.99 billion in 2011, €4.02 billion in 2012, €4.28 billion in 2013 and €4.10 billion in 2014. Languages generally used are English and French. Additionally, official documents are also provided in German and documents regarding the Spacelab are also provided in Italian. If found appropriate, the agency may conduct its correspondence in any language of a member state.
The following table lists all the member states and adjunct members, their ESA convention ratification dates, and their contributions in 2015:
Associate members.
Currently the only associated member of ESA is Canada. Previously associated members were Austria, Norway and Finland, all of which later joined ESA as full members.
Canada.
Since 1 January 1979, Canada has had the special status of a Cooperating State within ESA. By virtue of this accord, the Canadian Space Agency takes part in ESA's deliberative bodies and decision-making and also in ESA's programmes and activities. Canadian firms can bid for and receive contracts to work on programmes. The accord has a provision ensuring a fair industrial return to Canada. The most recent Cooperation Agreement was signed on 2010-12-15 with a term extending to 2020. For 2014, Canada's annual assessed contribution to the ESA general budget was 6,059,449.00 Euros (CAD$8,559,050).
Budget appropriation and allocation.
ESA is funded from annual contributions by national governments as well as from an annual contribution by the European Union (EU).
The budget of ESA was €5.250 billion in 2016. Every 3–4 years, ESA member states agree on a budget plan for several years at an ESA member states conference. This plan can be amended in future years, however provides the major guideline for ESA for several years. The 2016 budget allocations for major areas of ESA activity are shown in the chart on the right.
Countries typically have their own space programmes that differ in how they operate organisationally and financially with ESA. For example, the French space agency CNES has a total budget of €2015 million, of which €755 million is paid as direct financial contribution to ESA. Several space-related projects are joint projects between national space agencies and ESA (e.g. COROT). Also, ESA is not the only European governmental space organisation (for example European Union Satellite Centre).
Enlargement.
After the decision of the ESA Council of 21/22 March 2001, the procedure for accession of the European states was detailed as described the document titled "The Plan for European Co-operating States (PECS)".
Nations that want to become a full member of ESA do so in 3 stages. First a Cooperation Agreement is signed between the country and ESA. In this stage, the country has very limited financial responsibilities. If a country wants to cooperate more fully with ESA, it signs a European Cooperating State (ECS) Agreement. The ECS Agreement makes companies based in the country eligible for participation in ESA procurements. The country can also participate in all ESA programmes, except for the Basic Technology Research Programme. While the financial contribution of the country concerned increases, it is still much lower than that of a full member state. The agreement is normally followed by a Plan For European Cooperating State (or PECS Charter). This is a 5-year programme of basic research and development activities aimed at improving the nation's space industry capacity. At the end of the 5-year period, the country can either begin negotiations to become a full member state or an associated state or sign a new PECS Charter. Many countries, most of which joined the EU in both 2004 and 2007, have started to cooperate with ESA on various levels:
During the Ministerial Meeting in December 2014, ESA ministers approved a resolution calling for discussions to begin with Israel, Australia and South Africa on future association agreements. The ministers noted that “concrete cooperation is at an advanced stage” with these nations and that “prospects for mutual benefits are existing”.
A separate space exploration strategy resolution calls for further cooperation with the United States, Russia and China on "LEO exploration, including a continuation of ISS cooperation and the development of a robust plan for the coordinated use of space transportation vehicles and systems for exploration purposes, participation in robotic missions for the exploration of the Moon, the robotic exploration of Mars, leading to a broad Mars Sample Return mission in which Europe should be involved as a full partner, and human missions beyond LEO in the longer term."
EU and the European Space Agency.
The political perspective of the European Union (EU) was to make ESA an agency of the EU by 2014, although this date was not met. The EU is already the largest single donor to ESA's budget and non-ESA EU states are observers at ESA.
The only current EU member state that has not signed an ESA Cooperation Agreement is Croatia. In December 2014, the ESA Ministerial Council authorized officials to begin discussions to establish formal cooperation with Croatia.
Launch vehicle fleet.
ESA has a fleet of different launch vehicles in service with which it competes in all sectors of the launch market. ESA's fleet consists of three major rocket designs: Ariane 5, Soyuz-2 and Vega. Rocket launches are carried out by Arianespace, which has 23 shareholders representing the industry that manufactures the Ariane 5 as well as CNES, at ESA's Guiana Space Centre. Because many communication satellites have equatorial orbits, launches from French Guiana are able to take larger payloads into space than from spaceports at higher latitudes. In addition, equatorial launches give spacecraft an extra 'push' of nearly 500 m/s due to the higher rotational velocity of the Earth at the equator compared to near the Earth's poles where rotational velocity approaches zero.
Ariane 5.
The Ariane 5 rocket is ESA's primary launcher. It has been in service since 1997 and replaced Ariane 4. Two different variants are currently in use. The heaviest and most used version, the , delivers two communications satellites of up to 10 tonnes into GTO. It failed during its first test flight in 2002, but has since made 71 consecutive successful flights (as of March 2016). The other version, , was used to launch the Automated Transfer Vehicle (ATV) to the International Space Station (ISS) and will be used to launch four Galileo navigational satellites at a time.
In November 2012, ESA agreed to build an upgraded variant called Ariane 5#Ariane 5 ME (Mid-life Evolution) which will increase payload capacity to 11.5 tonnes to GTO and feature a restartable second stage to allow more complex missions. Ariane 5 ME is scheduled to fly in 2018. Some of its new features will also be adopted by the next-generation launcher, Ariane 6, planned to replace Ariane 5 in the 2020s.
ESA's Ariane 1, 2, 3 and 4 launchers (the last of which was ESA's long-time workhorse) have been retired.
Soyuz.
Soyuz-2 (also called the Soyuz-ST or Soyuz-STK) is a Russian medium payload launcher (ca. 3 metric tons to GTO) which was brought into ESA service in October 2011. ESA entered into a €340 million joint venture with the Russian Federal Space Agency over the use of the Soyuz launcher. Under the agreement, the Russian agency manufactures Soyuz rocket parts for ESA, which are then shipped to French Guiana for assembly.
ESA benefits because it gains a medium payload launcher, complementing its fleet while saving on development costs. In addition, the Soyuz rocket—which has been the Russian's space launch workhorse for some 40 years—is proven technology with a very good safety record. Russia benefits in that it gets access to the Kourou launch site. Due to its proximity to the equator, launching from Kourou rather than Baikonur nearly doubles Soyuz's payload to GTO (3.0 tonnes vs. 1.7 tonnes).
Soyuz first launched from Kourou on 21 October 2011, and successfully placed two Galileo satellites into orbit 23,222 kilometres above Earth.
Vega.
Vega is ESA's carrier for small satellites. Developed by seven ESA members led by Italy, it is capable of carrying a payload with a mass of between 300 and 1500 kg to an altitude of 700 km, for low polar orbit. Its maiden launch from Kourou was on 13 February 2012.
The rocket has three solid propulsion stages and a liquid propulsion upper stage (the AVUM) for accurate orbital insertion and the ability to place multiple payloads into different orbits.
Ariane launch vehicle development funding.
Historically, the Ariane family rockets have been funded primarily "with money contributed by ESA governments seeking to participate in the program rather than through competitive industry bids. This meant that governments commit multiyear funding to the development with the expectation of a roughly 90% return on investment in the form of industrial workshare." ESA is proposing changes to this scheme by moving to competitive bids for the development of the Ariane 6.
Human space flight.
History.
At the time ESA was formed, its main goals did not encompass human space flight; rather it considered itself to be primarily a scientific research organisation for unmanned space exploration in contrast to its American and Soviet counterparts. It is therefore not surprising that the first non-Soviet European in space was not an ESA astronaut on a European space craft; it was Czechoslovak Vladimír Remek who in 1978 became the first non-Soviet European in space (the first European in space being Yuri Gagarin of the Soviet Union) — on a Soviet Soyuz spacecraft, followed by the Pole Mirosław Hermaszewski and East German Sigmund Jähn in the same year. This Soviet co-operation programme, known as Intercosmos, primarily involved the participation of Eastern bloc countries. In 1982, however, Jean-Loup Chrétien became the first non-Communist Bloc astronaut on a flight to the Soviet Salyut 7 space station.
Because Chrétien did not officially fly into space as an ESA astronaut, but rather as a member of the French CNES astronaut corps, the German Ulf Merbold is considered the first ESA astronaut to fly into space. He participated in the STS-9 Space Shuttle mission that included the first use of the European-built Spacelab in 1983. STS-9 marked the beginning of an extensive ESA/NASA joint partnership that included dozens of space flights of ESA astronauts in the following years. Some of these missions with Spacelab were fully funded and organizationally and scientifically controlled by ESA (such as two missions by Germany and one by Japan) with European astronauts as full crew members rather than guests on board. Beside paying for Spacelab flights and seats on the shuttles, ESA continued its human space flight co-operation with the Soviet Union and later Russia, including numerous visits to Mir.
During the latter half of the 1980s, European human space flights changed from being the exception to routine and therefore, in 1990, the European Astronaut Centre in Cologne, Germany was established. It selects and trains prospective astronauts and is responsible for the co-ordination with international partners, especially with regard to the International Space Station. As of 2006, the ESA astronaut corps officially included twelve members, including nationals from most large European countries except the United Kingdom.
In the summer of 2008, ESA started to recruit new astronauts so that final selection would be due in spring 2009. Almost 10,000 people registered as astronaut candidates before registration ended in June 2008. 8,413 fulfilled the initial application criteria. Of the applicants, 918 were chosen to take part in the first stage of psychological testing, which narrowed down the field to 192. After two-stage psychological tests and medical evaluation in early 2009, as well as formal interviews, six new members of the European Astronaut Corps were selected - five men and one woman.
Astronaut Corps.
The astronauts of the European Space Agency are:
Crew vehicles.
In the 1980s, France pressed for an independent European crew launch vehicle. Around 1978 it was decided to pursue a reusable spacecraft model and starting in November 1987 a project to create a mini-shuttle by the name of Hermes was introduced. The craft was comparable to early proposals for the Space Shuttle and consisted of a small reusable spaceship that would carry 3 to 5 astronauts and 3 to 4 metric tons of payload for scientific experiments. With a total maximum weight of 21 metric tons it would have been launched on the Ariane 5 rocket, which was being developed at that time. It was planned solely for use in Low-Earth orbit space flights. The planning and pre-development phase concluded in 1991; however, the production phase was never fully implemented because at that time the political landscape had changed significantly. With the fall of the Soviet Union ESA looked forward to cooperation with Russia to build a next-generation space vehicle. Thus the Hermes programme was cancelled in 1995 after about 3 billion dollars had been spent. The Columbus space station programme had a similar fate.
In the 21st century, ESA started new programmes in order to create its own crew vehicles, most notable among its various projects and proposals is Hopper, whose prototype by EADS, called Phoenix, has already been tested. While projects such as Hopper are neither concrete nor to be realised within the next decade, other possibilities for human spaceflight in cooperation with the Russian Space Agency have emerged. Following talks with the Russian Space Agency in 2004 and June 2005, a cooperation between ESA and the Russian Space Agency was announced to jointly work on the Russian-designed Kliper, a reusable spacecraft that would be available for space travel beyond LEO (e.g. the moon or even Mars). It was speculated that Europe would finance part of it. However, a €50 million participation study for Kliper, which was expected to be approved in December 2005, was finally not approved by the ESA member states. The Russian state tender for the Kliper project was subsequently cancelled in the summer of 2006.
In June 2006, ESA member states granted 15 million to the Crew Space Transportation System (CSTS) study, a two-year study to design a spacecraft capable of going beyond Low-Earth orbit based on the current Soyuz design. This project is pursued with Roskosmos instead of the previously cancelled Kliper proposal. A decision on the actual implementation and construction of the CSTS spacecraft is contemplated for 2008, with the major design decisions being made before the summer of 2007.
In mid-2009 EADS Astrium was awarded a €21 million study into designing a crew vehicle based on the European ATV which is believed to now be the basis of the Advanced Crew Transportation System design.
In November 2012, ESA decided to join NASA's Orion programme. The ATV would form the basis of a propulsion unit for NASA's new manned spacecraft. ESA may also seek to work with NASA on Orion's launch system as well in order to secure a seat on the spacecraft for its own astronauts.
In September 2014, ESA signed an agreement with Sierra Nevada Corporation for cooperation in Dream Chaser project. Further studies on the Dream Chaser for European Utilization or DC4EU project were funded, including the feasibility of launching a Europeanized Dream Chaser onboard Ariane 5.
Cooperation with other countries and organisations.
ESA has signed cooperation agreements with the following states that currently neither plan to integrate as tightly with ESA institutions as Canada, nor envision future membership of ESA: Argentina, Brazil, China, India (for the Chandrayan mission), Russia and Turkey.
Additionally, ESA has joint projects with the European Union, NASA of the United States and is participating in the International Space Station together with the United States (NASA), Russia and Japan (JAXA).
European Union.
ESA is not an agency or body of the European Union (EU), and has non-EU countries Switzerland and Norway as members. There are however ties between the two, with various agreements in place and being worked on, to define the legal status of ESA with regard to the EU.
There are common goals between ESA and the EU. ESA has an EU liaison office in Brussels. On certain projects, the EU and ESA cooperate, such as the upcoming Galileo satellite navigation system. Space policy has since December 2009 been an area for voting in the European Council. Under the European Space Policy of 2007, the EU, ESA and its Member States committed themselves to increasing coordination of their activities and programmes and to organising their respective roles relating to space.
The Lisbon Treaty of 2009 reinforces the case for space in Europe and strengthens the role of ESA as an R&D space agency. Article 189 of the Treaty gives the EU a mandate to elaborate a European space policy and take related measures, and provides that the EU should establish appropriate relations with ESA.
Former Italian astronaut Umberto Guidoni, during his tenure as a Member of the European Parliament from 2004 to 2009, stressed the importance of the European Union as a driving force for space exploration, "since other players are coming up such as India and China it is becoming ever more important that Europeans can have an independent access to space. We have to invest more into space research and technology in order to have an industry capable of competing with other international players."
The first EU-ESA International Conference on Human Space Exploration took place in Prague on 22 and 23 October 2009. A road map which would lead to a common vision and strategic planning in the area of space exploration was discussed. Ministers from all 29 EU and ESA members as well as members of parliament were in attendance.
NASA.
ESA has a long history of collaboration with NASA. Since ESA's astronaut corps was formed, the Space Shuttle has been the primary launch vehicle used by ESA's astronauts to get into space through partnership programmes with NASA. In the 1980s and 1990s, the Spacelab programme was an ESA-NASA joint research programme that had ESA develop and manufacture orbital labs for the Space Shuttle for several flights on which ESA participate with astronauts in experiments.
In robotic science mission and exploration missions, NASA has been ESA's main partner. Cassini–Huygens was a joint NASA-ESA mission, along with the Infrared Space Observatory, INTEGRAL, SOHO, and others. Also, the Hubble space telescope is a joint project of NASA and ESA. Future ESA-NASA joint projects include the James Webb Space Telescope and the proposed Laser Interferometer Space Antenna. NASA has committed to provide support to ESA's proposed MarcoPolo-R mission to return an asteroid sample to Earth for further analysis. NASA and ESA will also likely join together for a Mars Sample Return Mission.
Cooperation with other space agencies.
Since China has started to invest more money into space activities, the Chinese Space Agency has sought international partnerships. ESA is, beside the Russian Space Agency, one of its most important partners. Recently the two space agencies cooperated in the development of the Double Star Mission.
ESA entered into a major joint venture with Russia in the form of the CSTS, the preparation of French Guiana spaceport for launches of Soyuz-2 rockets and other projects. With India, ESA agreed to send instruments into space aboard the ISRO's Chandrayaan-1 in 2008. ESA is also cooperating with Japan, the most notable current project in collaboration with JAXA is the BepiColombo mission to Mercury.
Speaking to reporters at an air show near Moscow in August 2011, ESA head Jean-Jacques Dordain said ESA and Russia's Roskosmos space agency would "carry out the first flight to Mars together."
International Space Station.
With regard to the International Space Station (ISS) ESA is not represented by all of its member states: 10 of the 21 ESA member states currently participate in the project. ESA is taking part in the construction and operation of the ISS with contributions such as Columbus, a science laboratory module that was brought into orbit by NASA's STS-122 Space Shuttle mission and the Cupola observatory module that was completed in July 2005 by Alenia Spazio for ESA. The current estimates for the ISS are approaching €100 billion in total (development, construction and 10 years of maintaining the station) of which ESA has committed to paying €8 billion. About 90% of the costs of ESA's ISS share will be contributed by Germany (41%), France (28%) and Italy (20%). German ESA astronaut Thomas Reiter was the first long-term ISS crew member.
ESA has developed the Automated Transfer Vehicle (ATV) for ISS resupply. Each ATV has a cargo capacity of . The first ATV, "Jules Verne", was launched on 9 March 2008 and on 3 April 2008 successfully docked with the ISS. This manoeuvre, considered a major technical feat, involved using automated systems to allow the ATV to track the ISS, moving at 27,000 km/h, and attach itself with an accuracy of 2 cm.
As of 2013, the spacecraft establishing supply links to the ISS are the Russian Progress and Soyuz, European ATV, Japanese Kounotori (HTV), and the USA COTS program vehicles Dragon and Cygnus.
European Life and Physical Sciences research on board the International Space Station (ISS) is mainly based on the European Programme for Life and Physical Sciences in Space programme that was initiated in 2001.
Miscellaneous.
Languages.
According to Annex 1, Resolution No. 8 of the "ESA Convention and Council Rules of Procedure", English, French and German may be used in all meetings of the Agency, with interpretation provided into these three languages. All official documents are available in English and French with all documents concerning the ESA Council being available in German as well.
ESA and the EU institutions.
The EU flag is the one to be flown in space during missions (for example it was flown by ESA's Andre Kuipers during Delta mission)
The Commission is increasingly working together towards common objectives. Some 20 per cent of the funds managed by ESA now originate from the supranational budget of the European Union.
However, in recent years the ties between ESA and the European institutions have been reinforced by the increasing role that space plays in supporting Europe’s social, political and economic policies.
The legal basis for the EU/ESA cooperation is provided by a Framework Agreement which entered into force in May 2004. According to this agreement, the European Commission and ESA coordinate their actions through the Joint Secretariat, a small team of EC’s administrators and ESA executive. The Member States of the two organisations meet at ministerial level in the Space Council, which is a concomitant meeting of the EU and ESA Councils, prepared by Member States representatives in the High-level Space Policy Group (HSPG).
ESA maintains a liaison office in Brussels to facilitate relations with the European institutions.
Guaranteeing European access to space.
In May 2007, the 29 European countries expressed their support for the European Space Policy in a resolution of the Space Council, unifying the approach of ESA with those of the European Union and their member states.
Prepared jointly by the European Commission and ESA’s Director General, the European Space Policy sets out a basic vision and strategy for the space sector and addresses issues such as security and defence, access to space and exploration.
Through this resolution, the EU, ESA and their Member States all commit to increasing coordination of their activities and programmes and their respective roles relating to space.

</doc>
<doc id="10365" url="https://en.wikipedia.org/wiki?curid=10365" title="Embouchure">
Embouchure

The embouchure is the use of facial muscles and the shaping of the lips to the mouthpiece of woodwind instruments or the mouthpiece of the brass instruments.
The word is of French origin and is related to the root "bouche" (fr.), 'mouth'.
The proper embouchure allows the instrumentalist to play the instrument at its full range with a full, clear tone and without strain or damage to one's muscles.
Brass embouchure.
While performing on a brass instrument, the sound is produced by the player buzzing his or her lips into a mouthpiece. Pitches are changed in part through altering the amount of muscular contraction in the lip formation. The performer's use of the air, tightening of cheek and jaw muscles, as well as tongue manipulation can affect how the embouchure works.
Even today, many brass pedagogues take a rigid approach to teaching how a brass player's embouchure should function. Many of these authors also disagree with each other regarding which technique is correct. Research suggests efficient brass embouchures depend on the player using the method that suits that player's particular anatomy (see below). Individual differences in dental structure, lip shape and size, jaw shape and the degree of jaw malocclusion, and other anatomical factors will affect whether a particular embouchure technique will be effective or not .
In 1962, Philip Farkas hypothesized that the air stream traveling through the lip aperture should be directed straight down the shank of the mouthpiece. He believed that it would be illogical to "violently deflect" the air stream downward at the point of where the air moves past the lips. In this text, Farkas also recommends that the lower jaw be protruded so that the upper and lower teeth are aligned.
In 1970, Farkas published a second text which contradicted his earlier writing. Out of 40 subjects, Farkas showed that 39 subjects directed the air downward to varying degrees and 1 subject directed the air in an upward direction at various degrees. The lower jaw position seen in these photographs show more variation from his earlier text as well.
This supports what was written by trombonist and brass pedagogue Donald S. Reinhardt in 1942. In 1972, Reinhardt described and labeled different embouchure patterns according to such characteristics as mouthpiece placement and the general direction of the air stream as it travels past the lips. According to this later text, players who place the mouthpiece higher on the lips, so that more upper lip is inside the mouthpiece, will direct the air downwards to varying degrees while playing. Performers who place the mouthpiece lower, so that more lower lip is inside the mouthpiece, will direct the air to varying degrees in an upward manner. In order for the performer to be successful, the air stream direction and mouthpiece placement need to be personalized based on individual anatomical differences. Lloyd Leno confirmed the existence of both upstream and downstream embouchures.
More controversial was Reinhardt's description and recommendations regarding a phenomenon he termed a "pivot". According to Reinhardt, a successful brass embouchure depends on a motion wherein the performer moves both the mouthpiece and lips as a single unit along the teeth in an upward and downward direction. As the performer ascends in pitch, he or she will either move the lips and mouthpiece together slightly up towards the nose or pull them down together slightly towards the chin, and use the opposite motion to descend in pitch. Whether the player uses one general pivot direction or the other, and the degree to which the motion is performed, depends on the performer's anatomical features and stage of development. The placement of the mouthpiece upon the lips doesn't change, but rather the relationship of the rim and lips to the teeth. While the angle of the instrument may change as this motion follows the shape of the teeth and placement of the jaw, contrary to what many brass performers and teachers believe, the angle of the instrument does not actually constitute the motion Reinhardt advised as a pivot.
Later research supports Reinhardt's claim that this motion exists and might be advisable for brass performers to adopt. John Froelich describes how mouthpiece pressure towards the lips (vertical forces) and shear pressure (horizontal forces) functioned in three test groups, student trombonists, professional trombonists, and professional symphonic trombonists. Froelich noted that the symphonic trombonists used the least amount of both direct and shear forces and recommends this model be followed. Other research notes that virtually all brass performers rely upon the upward and downward embouchure motion. Other authors and pedagogues remain skeptical about the necessity of this motion, but scientific evidence supporting this view has not been sufficiently developed at this time to support this view.
Some noted brass pedagogues prefer to instruct the use of the embouchure from a less analytical point of view. Arnold Jacobs, a tubist and well-regarded brass teacher, believed that it was best for the student to focus on his or her use of the air and musical expression to allow the embouchure to develop naturally on its own. Other instructors, such as Carmine Caruso, believed that the brass player's embouchure could best be developed through coordination exercises and drills that bring all the muscles into balance that focus the student's attention on his or her time perception. Still other authors who have differing approaches to embouchure development include Louis Maggio, Jeff Smiley, Jerome Callet. and Clint McLaughlin.
Farkas embouchure.
Most professional performers, as well as instructors, use a combination called a puckered smile. Farkas told people to blow as if they were trying to cool soup. Raphael Mendez advised saying the letter "M". The skin under your lower lip will be taut with no air pocket. Your lips do not overlap nor do they roll in or out. The corners of the mouth are held firmly in place. To play with an extended range you should use a pivot, tongue arch and lip to lip compression.
According to Farkas the mouthpiece should have 2/3 upper lip and 1/3 lower lip (French horn), 2/3 lower lip and 1/3 upper lip (trumpet and cornet), and more latitude for lower brass (trombone, baritone, and tuba). For trumpet, some also advocate 1/2 upper lip and 1/2 lower lip . Farkas claimed placement was more important for the instruments with smaller mouthpieces. Your lips should not overlap each other, nor should they roll in or out. The mouth corners should be held firm. Farkas speculated that the horn should be held in a downward angle to allow the air stream to go straight into the mouthpiece, although his later text shows that air stream direction actually is either upstream or downstream and is dependent upon the ratio of upper or lower lip inside the mouthpiece, not the horn angle. Farkas advised to moisten the outside of your lips, then form your embouchure and gently place the mouthpiece on it. He also recommended there must be a gap of ⅓ inch or so between your teeth so that the air flows freely.
Arban vs. Saint-Jacome.
Arban and Saint-Jacome were both cornet soloists and authors of well respected and still used method books. Arban stated undogmatically that he believed the mouthpiece should be placed 1/3 on the top lip. St. Jacome to the contrary said dogmatically that the mouthpiece should be placed "two-thirds for the upper and the rest for the under according to all professors and one-third for the upper and two-thirds for the under according to one sole individual, whom I shall not name."
(Subset) Buzzing embouchure.
The Farkas set is the basis of most lip buzzing embouchures. Mendez did teach lip buzzing and got great results. One can initiate this type of buzz by using the same sensation as spitting seeds, but maintaining a continued flow of air. This technique assists the development of the Farkas approach by preventing the player from using an aperture that is too open.
Stevens-Costello embouchure.
Stevens-Costello embouchure has its origins in the William Costello embouchure and was further developed by Roy Stevens. It uses a slight rolling in of both lips and touching evenly all the way across. It also uses mouthpiece placement of about 40% to 50% top lip and 60% to 50% lower lip. The teeth will be about 1/4 to 1/2 inch apart and the teeth are parallel or the jaw slightly forward.
There is relative mouthpiece pressure to the given air column. One exercise to practice the proper weight to air relationship is the palm exercise where you hold your horn by laying it on its side in the palm of your hand. Do not grasp it. Place your lips on the mouthpiece and blow utilizing the weight of the horn in establishing a sound.
Maggio embouchure.
A puckered embouchure, used by most players, and sometimes used by jazz players for extremely high "screamer" notes. Maggio claimed that the pucker embouchure gives more endurance than some systems. Carlton MacBeth is the main proponent of the pucker embouchure. The Maggio system was established because Louis Maggio had sustained an injury which prevented him from playing. In this system you cushion the lips by extending them or puckering (like a monkey). This puckering enables the players to overcome physical malformations. It also lets the player play for an extended time in the upper register. The pucker can make it easy to use to open an aperture. Lots of very soft practice can help overcome this. Claude Gordon was student of Louis Maggio and Herbert L. Clarke and systematized the concepts of these teachers. Claude Gordon made use of pedal tones for embouchure development as did Maggio and Herbert L. Clarke. All three stressed that the mouthpiece should be placed higher on the top lip for a more free vibration of the lips.
Tongue-controlled embouchure.
This embouchure method, advocated by a minority of brass pedagogues such as Jerome Callet, has not yet been sufficiently researched to support the claims that this system is the most effective approach for all brass performers.
Advocates of Callet's approach believe that this method was recommended and taught by the great brass instructors of the early 20th Century. Two French trumpet technique books, authored by Jean-Baptiste Arban, and St. Jacome, were translated into English for use by American players. According to some, due to a misunderstanding arising from differences in pronunciation between French and English, the commonly used brass embouchure in Europe was incorrectly interpreted. Callet attributes this difference in embouchure technique as the reason the great players of the past were able to play at the level of technical virtuosity which they did, although the increased difficulty of contemporary compositions for brass seem to indicate that the level of brass technique achieved by today's performers equals or even exceeds that of most performers from the late 19th and early 20th centuries.
Callet's method of brass embouchure consists of the tongue remaining forward and through the teeth at all times. The corners of the mouth always remain relaxed, and only a small amount of air is used. The top and bottom lips curl inward and grip the forward tongue. The tongue will force the teeth, and subsequently the throat, wide open, supposedly resulting in a bigger, more open sound. The forward tongue resists the pressure of the mouthpiece, controls the flow of air for lower and higher notes, and protects the lips and teeth from damage or injury from mouthpiece pressure. Because of the importance of the tongue in this method many refer to this as a "tongue-controlled embouchure." This technique facilitates the use of a smaller mouthpiece, and larger bore instruments. It results in improved intonation and stronger harmonically related partials across the player's range.
Woodwind embouchure.
Flute embouchure.
A variety of transverse flute embouchures are employed by professional flautists, though the most natural form is perfectly symmetrical, the corners of the mouth relaxed (i.e. not smiling), the lower lip placed along and at a short distance from the embouchure hole. It must be stressed, however, that achieving a symmetrical, or perfectly centred blowing hole ought not to be an end in itself. Indeed Marcel Moyse, the owner of the next quotation, did not play with a symmetrical embouchure.
The end-blown xiao, kaval, shakuhachi and hocchiku flutes demand especially difficult embouchures, sometimes requiring many lessons before any sound can be produced.
The embouchure is an important element to tone production. The right embouchure, developed with "time, patience, and intelligent work", will produce a beautiful sound and a correct intonation. The embouchure is produced with the muscles around the lips: principally the orbicularis oris muscle and the depressor anguli oris, whilst avoiding activiation of zygomaticus major, which will produce a smile, flattening the top lip against the maxillary (upper jaw) teeth. Beginner flute-players tend to suffer fatigue in these muscles, and notably struggle to use the depressor muscle, which necessarily helps to keep the top lip directing the flow of air across the embouchure hole. These muscles have to be properly warmed up and exercised before practicing. Tone development exercises including long notes and harmonics must be done as part of the warm up every day.
Some further adjustments to the embouchure are necessary when moving from the transverse orchestral flute to the piccolo. With the piccolo, it becomes necessary to place the near side of the embouchure hole slightly higher on the lower lip, i.e. above the lip margin, and greater muscle tone from the lip muscles is needed to keep the stream/pressure of air directed across the smaller embouchure hole, particularly when playing in higher piccolo registers.
Reed instrument embouchure.
With the woodwinds, aside from the flute, piccolo, and recorder, the sound is generated by a reed and not with the lips. The embouchure is therefore based on sealing the area around the reed and the mouthpiece. This serves to prevent air from escaping while simultaneously supporting the reed allowing it to vibrate, and to constrict the reed preventing it from vibrating too much. With woodwinds, it is important to ensure that the mouthpiece is not placed too far into the mouth, which would result in too much vibration (no control), often creating a sound an octave (or harmonic twelfth for the clarinet) above the intended note. If the mouthpiece is not placed far enough into the mouth, no noise will be generated, as the reed will not vibrate.
The embouchure for single reed woodwinds like the clarinet and saxophone is formed by resting the reed upon the bottom lip, which rests on the teeth and is supported by the chin muscles and the buccinator muscles on the sides of the mouth. The top teeth then rest on top of the mouthpiece. In both saxophone and clarinet playing, the corners of the mouth are brought inwards (similar to a drawstring bag) in order to create a seal. With the less common double-lip embouchure, the top lip is placed under (around) the top teeth. In both instances, the position of the tongue in the mouth plays a vital role in focusing and accelerating the air stream blown by the player. This results in a more mature and full sound, rich in overtones.
The double reed woodwinds, the oboe and bassoon, have no mouthpiece. Instead the reed is two pieces of cane extending from a metal tube (oboe - staple) or placed on a bocal (bassoon, English horn). The reed is placed directly on the lips and then played like the double-lip embouchure described above. Compared to the single reed woodwinds, the reed is very small and subtle changes in the embouchure can have a dramatic effect on tuning, tone and pitch control.

</doc>
<doc id="10368" url="https://en.wikipedia.org/wiki?curid=10368" title="The Elephant 6 Recording Company">
The Elephant 6 Recording Company

The Elephant 6 Recording Company (or simply Elephant 6) is a collective of American musicians who spawned many notable independent bands of the 1990s, including the Apples in Stereo, the Olivia Tremor Control, Neutral Milk Hotel, Beulah, Elf Power, of Montreal, The Minders, and Circulatory System. They are marked by a shared admiration of 1960s pop music. 
History.
Foundation.
The collective was officially founded in Denver, Colorado by childhood friends Robert Schneider, Bill Doss, Will Cullen Hart and Jeff Mangum, along with founding Apples in Stereo members Jim McIntyre and Hilarie Sidney. Schneider, Doss, Hart and Mangum grew up making music and sharing cassette tapes in Ruston, Louisiana while attending high school together. They started several bands and pet projects; Doss and Hart with the Olivia Tremor Control (then called Synthetic Flying Machine), Mangum with Neutral Milk Hotel, and Schneider with the Apples in Stereo. Together, they held a mutual admiration for the music of the 1960s, especially the Beach Boys, considering the unfinished "Smile" project to be the era's "Holy Grail". 
Wishing to emulate the Beach Boys' Brother Records, Schneider created the Elephant 6 record label when he moved to Denver, Colorado in late 1991 to attend the University of Colorado at Boulder. According to Schneider, Hart coined the name "Elephant 6" and Schneider added "Recording Company." In Denver, Schneider and new friends founded The Apples (which eventually became the Apples in Stereo). Recorded at the newly built Pet Sounds Studio, and released in June 1993, The Apples' "Tidal Wave" 7" EP was the first Elephant 6 products. Hart designed the Art Nouveau-inspired Elephant 6 logo for the label of The Apples EP.
Doss had moved to Athens, Georgia where he joined Hart and Mangum in Synthetic Flying Machine, which became the Olivia Tremor Control. They released "California Demise" as their first recording, and the Elephant 6's second. Afterwards, the base for The Elephant 6 moved from Denver to Athens.
Success, disbandment, and continuation.
Several Elephant 6 projects began to find commercial success in the late 1990s, including Beulah, the Minders, Elf Power, Dressy Bessy, the Music Tapes, and of Montreal, as well as the founding bands. Most of the bands subsequently signed with major record labels; E6 as an entity slowly deteriorated until the collective called it quits — due to recording difficulties and lack of organization — in 2002. The collective's bands all moved on to various labels and projects of their own.
However, many band members are still friends and even tour together under various guises. Many live together on the Orange Twin Conservation Community in Athens. The term "Elephant 6" has since come to refer to a broad range of bands and spin-off projects that the record label has spawned.
In 2007 The Apples in Stereo featured the Elephant 6 logo on their album "New Magnetic Wonder", announcing "The Elephant 6 Recording Company re-opens our doors and windows, and invites the world: join together with your friends and make something special, something meaningful, something to remember when you are old." This marked the first major release in five years to bear the Elephant 6 logo.
In 2008, the Elephant 6 logo was also used when Julian Koster released his long awaited Music Tapes For Clouds and Tornadoes under the name the Music Tapes. He also assembled The Elephant 6 Holiday Surprise Tour which featured contributions from Will Cullen Hart and Bill Doss of Olivia Tremor Control, Scott Spillane of the Gerbils, Andrew Reiger and Laura Carter of Elf Power, Theo Hilton of Nana Grizol, John Fernandes and Eric Harris of OTC, The Music Tapes and Circulatory System, Robbie Cucchiaro of The Music Tapes, Charlie Johnston and Suzanne Allison of the 63 Crayons, Nesey Gallons, Jeff Mangum, and Peter Erchick. This ensemble tour was widely seen as a resurgence of Elephant 6 as a productive, cohesive entity. The show took place in the UK, curated by Jeff Mangum of Neutral Milk Hotel as part of the All Tomorrow's Parties festival in March 2012 in Minehead, England.
In recent years, Robert Schneider has explored a number of experimental music projects, such as the Teletron mind-controlled synthesizer and Non-Pythagorean scale of his own invention. Indeed, since the inception of Elephant 6, members of the collective showed a strong interest and participation in experimental music, particularly the Olivia Tremor Control and Von Hemmling.

</doc>
<doc id="10369" url="https://en.wikipedia.org/wiki?curid=10369" title="Echolocation">
Echolocation

Echolocation is the use of "sound" as a form of navigation 

</doc>
<doc id="10370" url="https://en.wikipedia.org/wiki?curid=10370" title="Evangelicalism">
Evangelicalism

Evangelicalism, Evangelical Christianity, or Evangelical Protestantism is a worldwide, transdenominational movement within Protestant Christianity maintaining that the essence of the gospel consists in the doctrine of salvation by grace through faith in Jesus Christ's atonement. Evangelicals believe in the centrality of the conversion or "born again" experience in receiving salvation, in the authority of the Bible as God's revelation to humanity, and spreading the Christian message.
The movement gained great momentum in the 18th and 19th centuries with the Great Awakenings in the United Kingdom and North America. The origins of Evangelicalism are usually traced back to English Methodism, the Moravian Church (in particular the theology of its bishop Nicolaus Zinzendorf), and German Lutheran Pietism. Today, Evangelicals may be found in many of the Protestant branches, as well as in Protestant denominations not subsumed to a specific branch. Among leaders and major figures of the Evangelical Protestant movement were John Wesley, George Whitefield, Jonathan Edwards, Billy Graham, Harold John Ockenga, John Stott and Martyn Lloyd-Jones.
There are an estimated 285 million Evangelicals, comprising 13.1% of the total Christian population and 4.1% of the total world population. The Americas, Africa and Asia are home to the majority of Evangelicals. The United States has the largest concentration of Evangelicals. Evangelicalism, a major part of popular Protestantism, is among the most dynamic religious movements in the contemporary world, alongside resurgent Islam. While on the rise globally, the developing world is particularly influenced by its spread.
Usage.
The word "evangelical" has its etymological roots in the Greek word for "gospel" or "good news": ε'υαγγέλιον ("evangelion"), from "eu-" "good" and "angelion" "message". By the English Middle Ages the term had expanded semantically to include not only the message, but also the New Testament which contained the message, as well as more specifically the Gospels which portray the life, death and resurrection of Jesus. The first published use of "evangelical" in English came in 1531 when William Tyndale wrote "He exhorteth them to proceed constantly in the evangelical truth." One year later Sir Thomas More produced the earliest recorded use in reference to a theological distinction when he spoke of "Tyndale his evangelical brother Barns".
During the Reformation, Protestant theologians embraced the label as referring to "gospel truth". Martin Luther referred to the "evangelische Kirche" ("evangelical church") to distinguish Protestants from Catholics in the Roman Catholic Church. Into the 21st century, "evangelical" has continued in use as a synonym for (mainline) Protestant in continental Europe. This usage is reflected in the names of Protestant denominations such as the Evangelical Church in Germany (a union of Lutheran and Reformed churches) and the Evangelical Lutheran Church in America.
In the English-speaking world, "evangelical" became a common label used to describe the series of revival movements that occurred in Britain and North America during the eighteenth and nineteenth centuries. Christian historian David Bebbington writes that, "Although 'evangelical', with a lower-case initial, is occasionally used to mean 'of the gospel', the term 'Evangelical', with a capital letter, is applied to any aspect of the movement beginning in the 1730s." The term may also occur outside any religious context to characterize a generic missionary, reforming, or redeeming impulse or purpose. For example, the "Times Literary Supplement" refers to "the rise and fall of evangelical fervor within the Socialist movement".
Characteristics.
One influential definition of Evangelicalism has been proposed by historian David Bebbington. Bebbington notes four distinctive aspects of Evangelical faith: "conversionism", "biblicism", "crucicentrism", and "activism", noting, "Together they form a quadrilateral of priorities that is the basis of Evangelicalism."
"Conversionism", or belief in the necessity of being "born again", has been a constant theme of Evangelicalism since its beginnings. To Evangelicals, the central message of the gospel is justification by faith in Christ and repentance, or turning away, from sin. Conversion differentiates the Christian from the non-Christian, and the change in life it leads to is marked by both a rejection of sin and a corresponding personal holiness of life. A conversion experience can be emotional, including grief and sorrow for sin followed by great relief at receiving forgiveness. The stress on conversion is further differentiated from other forms of Protestantism by the belief that an assurance of salvation will accompany conversion. Among Evangelicals, individuals have testified to both sudden and gradual conversions.
"Biblicism" is defined as having a reverence for the Bible and a high regard for biblical authority. All Evangelicals believe in biblical inspiration, though they disagree over how this inspiration should be defined. Many Evangelicals believe in biblical inerrancy, while other Evangelicals believe in biblical infallibility.
"Crucicentrism" refers to the attention that Evangelicals give to the Atonement, the saving death and resurrection of Jesus, that offers forgiveness of sins and new life. This is understood most commonly in terms of a substitutionary atonement, in which Christ died as a substitute for sinful humanity by taking on himself the guilt and punishment for sin.
"Activism" describes the tendency towards active expression and sharing of the gospel in diverse ways that include preaching and social action. This aspect of Evangelicalism continues to be seen today in the proliferation of Evangelical voluntary religious groups and parachurch organizations.
Diversity.
As a trans-denominational movement, Evangelicalism occurs in nearly every Protestant denomination and tradition. The Reformed, Baptist, Wesleyan, and Pentecostal traditions have all had strong influence within modern Evangelicalism. Evangelicals are also represented within the Anabaptist, Anglican and Lutheran traditions.
The early 20th century saw the decline of Evangelical influence within mainline Protestantism and the development of Christian fundamentalism as a distinct religious movement. The second half of the century witnessed the development of a new mainstream Evangelical consensus that sought to be more inclusive and more culturally relevant than fundamentalism, while maintaining conservative Protestant teaching. According to professor of world Christianity Brian Stanley, this new postwar consensus is termed "Neo-Evangelicalism", the "New Evangelicalism", or simply "Evangelicalism" in the United States, while in the United Kingdom and in other English-speaking countries it is commonly termed conservative Evangelicalism. Over the years, less conservative Evangelicals have challenged this mainstream consensus to varying degrees, and such movements have been described by a variety of labels, such as progressive, open, post-conservative, and post-evangelical.
Fundamentalism.
Fundamentalism regards biblical inerrancy, the virgin birth of Jesus, penal substitutionary atonement, the literal resurrection of Christ and the Second Coming of Christ as fundamental Christian doctrines. Fundamentalism arose among Evangelicals in the 1920s to combat modernist or liberal theology in mainline Protestant churches. Failing to reform the mainline churches, fundamentalists separated from them and established their own churches, refusing to participate in ecumenical organizations such as the National Council of Churches. They also made separatism (rigid separation from non-fundamentalist churches and culture) a true test of faith. According to historian George Marsden, most fundamentalists are Baptists and dispensationalist.
Mainstream varieties.
Mainstream Evangelicalism is historically divided between two main orientations: confessionalism and revivalism. These two streams have been critical of each other. Confessional Evangelicals have been suspicious of unguarded religious experience, while revivalist Evangelicals have been critical of overly intellectual teaching that (they suspect) stifles vibrant spirituality. In an effort to broaden their appeal, many contemporary Evangelical congregations intentionally avoid identifying with any single form of Evangelicalism. These "generic Evangelicals" are usually theologically and socially conservative, but their churches often present themselves as nondenominational within the broader Evangelical movement.
In the words of Albert Mohler, president of the Southern Baptist Theological Seminary, confessional Evangelicalism refers to "that movement of Christian believers who seek a constant convictional continuity with the theological formulas of the Protestant Reformation". While approving of the Evangelical distinctives proposed by Bebbington, confessional Evangelicals believe that authentic Evangelicalism requires more concrete definition in order to protect the movement from theological liberalism and from heresy. This protection, according to confessional Evangelicals, is found in subscription to the ecumenical creeds and to the Reformation-era confessions of faith (such as the confessions of the Reformed churches). Confessional Evangelicals are represented by conservative Presbyterian churches (emphasizing the Westminster Confession), certain Baptist churches that emphasize historic Baptist confessions like the Second London Confession, Anglicans who emphasize the Thirty-Nine Articles (such as in the Anglican Diocese of Sydney, Australia), and some confessional Lutherans with pietistic convictions.
The emphasis on historic Protestant orthodoxy among confessional Evangelicals stands in direct contrast to an anti-creedal outlook that has exerted its own influence on Evangelicalism, particularly among churches heavily influenced by revivalism and by pietism. Revivalist Evangelicals are represented by some quarters of Methodism, the Wesleyan Holiness churches, the Pentecostal/charismatic churches, some Anabaptist churches, and some Baptists and Presbyterians. Revivalist Evangelicals tend to place greater emphasis on religious experience than their confessional counterparts.
Non-conservative varieties.
Evangelicals dissatisfied with the movement's conservative mainstream have been variously described as progressive Evangelicals, post-conservative Evangelicals, Open Evangelicals and Post-evangelicals. Progressive Evangelicals, also known as the Evangelical left, share theological or social views with other progressive Christians, while also identifying with Evangelicalism. Progressive Evangelicals commonly advocate for women's equality, pacifism and social justice.
As described by Baptist theologian Roger E. Olson, post-conservative Evangelicalism is a theological school of thought that adheres to the four marks of Evangelicalism, while being less rigid and more inclusive of other Christians. According to Olson, post-conservatives believe that doctrine and propositional truth is secondary to spiritual experience shaped by Scripture. Post-conservative Evangelicals seek greater dialogue with other Christian traditions and support the development of a multicultural Evangelical theology that incorporates the voices of women, racial minorities, and Christians in the developing world. Some post-conservative Evangelicals also support Open Theism and the possibility of near universal salvation.
The term "Open Evangelical" refers to a particular Christian school of thought or churchmanship, primarily in the United Kingdom (especially in the Church of England). Open Evangelicals describe their position as combining a traditional Evangelical emphasis on the nature of scriptural authority, the teaching of the ecumenical creeds and other traditional doctrinal teachings, with an approach towards culture and other theological points-of-view which tends to be more inclusive than that taken by other Evangelicals. Some Open Evangelicals aim to take a middle position between conservative and charismatic Evangelicals, while others would combine conservative theological emphases with more liberal social positions.
British author Dave Tomlinson coined the phrase "post-evangelical" to describe a movement comprising various trends of dissatisfaction among Evangelicals. Others use the term with comparable intent, often to distinguish Evangelicals in the so-called emerging church movement from post-evangelicals and anti-Evangelicals. Tomlinson argues that "linguistically, the distinction "evangelical and post-evangelical" resembles the one that sociologists make between the modern and postmodern eras".
History.
Background.
Evangelicalism did not take recognizable form until the 18th century, first in Britain and its North American colonies. Nevertheless, there were earlier developments within the larger Protestant world that preceded and influenced the later evangelical revivals. According to religion scholar, social activist, and politician Randall Balmer, Evangelicalism resulted "from the confluence of Pietism, Presbyterianism, and the vestiges of Puritanism. Evangelicalism picked up the peculiar characteristics from each strain – warmhearted spirituality from the Pietists (for instance), doctrinal precisionism from the Presbyterians, and individualistic introspection from the Puritans". Historian Mark Noll adds to this list High Church Anglicanism, which contributed to Evangelicalism a legacy of "rigorous spirituality and innovative organization".
During the 17th century, Pietism emerged in Europe as a movement for the revival of piety and devotion within the Lutheran church. As a protest against "cold orthodoxy" or an overly formal and rational Christianity, Pietists advocated for an experiential religion that stressed high moral standards for both clergy and lay people. The movement included both Christians who remained in the liturgical, state churches as well as separatist groups who rejected the use of baptismal fonts, altars, pulpits, and confessionals. As Pietism spread, the movement's ideals and aspirations influenced and were absorbed into early Evangelicalism.
The Presbyterian heritage not only gave Evangelicalism a commitment to Protestant orthodoxy but also contributed a revival tradition that stretched back to the 1620s in Scotland and Northern Ireland. Central to this tradition was the communion season, which normally occurred in the summer months. For Presbyterians, celebrations of Holy Communion were infrequent but popular events preceded by several Sundays of preparatory preaching and accompanied with preaching, singing, and prayers.
Puritanism combined Calvinism with teaching that conversion was a prerequisite for church membership and a stress on the study of Scripture by lay people. It took root in New England, where the Congregational church was an established religion. The Half-Way Covenant of 1662 allowed parents who had not testified to a conversion experience to have their children baptized, while reserving Holy Communion for converted church members alone. By the 18th century, Puritanism was in decline and many ministers were alarmed at the loss of religious piety. This concern over declining religious commitment led many people to support evangelical revival.
High Church Anglicanism also exerted influence on early Evangelicalism. High Churchmen were distinguished by their desire to adhere to primitive Christianity. This desire included imitating the faith and ascetic practices of early Christians as well as regularly partaking of Holy Communion. High Churchmen were also enthusiastic organizers of voluntary religious societies. Two of the most prominent were the Society for Promoting Christian Knowledge, which distributed Bibles and other literature and built schools, and the Society for the Propagation of the Gospel in Foreign Parts, which was created to facilitate missionary work in British colonies. Samuel and Susanna Wesley, the parents of John and Charles Wesley, were both devoted advocates of High Churchmanship.
18th century.
In the 1730s, Evangelicalism emerged as a distinct phenomenon out of religious revivals that began in Britain and New England. While religious revivals had occurred within Protestant churches in the past, the evangelical revivals that marked the 18th century were more intense and radical. Evangelical revivalism imbued ordinary men and women with a confidence and enthusiasm for sharing the gospel and converting others outside of the control of established churches, a key discontinuity with the Protestantism of the previous era.
It was developments in the doctrine of assurance that differentiated Evangelicalism from what went before. Bebbington says, "The dynamism of the Evangelical movement was possible only because its adherents were assured in their faith." He goes on:
The first local revival occurred in Northampton, Massachusetts, under the leadership of Congregationalist minister Jonathan Edwards. In the fall of 1734, Edwards preached a sermon series on "Justification By Faith Alone", and the community's response was extraordinary. Signs of religious commitment among the laity increased, especially among the town's young people. The revival ultimately spread to 25 communities in western Massachusetts and central Connecticut until it began to wane by the spring of 1735. Edwards was heavily influenced by Pietism, so much so that one historian has stressed his "American Pietism." One practice clearly copied from European Pietists was the use of small groups divided by age and gender, which met in private homes to conserve and promote the fruits of revival.
At the same time, students at Yale University (at that time Yale College) in New Haven, Connecticut, were also experiencing revival. Among them was Aaron Burr, Sr., who would become a prominent Presbyterian minister and future president of Princeton University. In New Jersey, Gilbert Tennent, another Presbyterian minister, was preaching the evangelical message and urging the Presbyterian Church to stress the necessity of converted ministers.
The spring of 1735 also marked important events in England and Wales. Howell Harris, a Welsh schoolteacher, had a conversion experience on May 25 during a communion service. He described receiving assurance of God's grace after a period of fasting, self-examination, and despair over his sins. Sometime later, Daniel Rowland, the Anglican curate of Llangeitho, Wales, experienced conversion as well. Both men began preaching the evangelical message to large audiences, becoming leaders of the Welsh Methodist revival. At about the same time that Harris experienced conversion in Wales, George Whitefield was converted at Oxford University after his own prolonged spiritual crisis. Whitefield later remarked, "About this time God was pleased to enlighten my soul, and bring me into the knowledge of His free grace, and the necessity of being justified in His sight by "faith only"".
Whitefield's fellow Holy Club member and spiritual mentor, Charles Wesley, reported an evangelical conversion in 1738. In the same week, Charles' brother and future founder of Methodism, John Wesley was also converted after a long period of inward struggle. During this spiritual crisis, John Wesley was directly influenced by Pietism. Two years before his conversion, Wesley had traveled to the newly established colony of Georgia as a missionary for the Society for Promoting Christian Knowledge. He shared his voyage with a group of Moravian Brethren led by August Gottlieb Spangenberg. The Moravians' faith and piety deeply impressed Wesley, especially their belief that it was a normal part of Christian life to have an assurance of one's salvation. Wesley recounted the following exchange with Spangenberg on February 7, 1736:
Wesley finally received the assurance he had been searching for at a meeting of a religious society in London. While listening to a reading from Martin Luther's preface to the Epistle to the Romans, Wesley felt spiritually transformed:
Pietism continued to influence Wesley, who had translated 33 Pietist hymns from German to English. Numerous German Pietist hymns became part of the English Evangelical repertoire. By 1737, Whitefield had become a national celebrity in England where his preaching drew large crowds, especially in London where the Fetter Lane Society had become a center of evangelical activity. Whitfield joined forces with Edwards to "fan the flame of revival" in the Thirteen Colonies in 1739–40. Soon the First Great Awakening stirred Protestants throughout America.
Evangelical preachers emphasized personal salvation and piety more than ritual and tradition. Pamphlets and printed sermons crisscrossed the Atlantic, encouraging the revivalists. The Awakening resulted from powerful preaching that gave listeners a sense of deep personal revelation of their need of salvation by Jesus Christ. Pulling away from ritual and ceremony, the Great Awakening made Christianity intensely personal to the average person by fostering a deep sense of spiritual conviction and redemption, and by encouraging introspection and a commitment to a new standard of personal morality. It reached people who were already church members. It changed their rituals, their piety and their self-awareness. To the evangelical imperatives of Reformation Protestantism, 18th century American Christians added emphases on divine outpourings of the Holy Spirit and conversions that implanted within new believers an intense love for God. Revivals encapsulated those hallmarks and forwarded the newly created Evangelicalism into the early republic.
19th century.
The start of the 19th century saw an increase in missionary work and many of the major missionary societies were founded around this time (see Timeline of Christian missions). Both the Evangelical and high church movements sponsored missionaries.
The Second Great Awakening (which actually began in 1790) was primarily an American revivalist movement and resulted in substantial growth of the Methodist and Baptist churches. Charles Grandison Finney was an important preacher of this period.
In Britain in addition to stressing the traditional Wesleyan combination of "Bible, cross, conversion, and activism," the revivalist movement sought a universal appeal, hoping to include rich and poor, urban and rural, and men and women. Special efforts were made to attract children and to generate literature to spread the revivalist message.
"Christian conscience" was used by the British Evangelical movement to promote social activism. Evangelicals believed activism in government and the social sphere was an essential method in reaching the goal of eliminating sin in a world drenched in wickedness. The Evangelicals in the Clapham Sect included figures such as William Wilberforce who successfully campaigned for the abolition of slavery.
In the late 19th century, the revivalist Holiness movement, based on the doctrine of "entire sanctification," took a more extreme form in rural America and Canada, where it ultimately broke away from institutional Methodism. In urban Britain the Holiness message was less exclusive and censorious.
John Nelson Darby was a 19th-century Irish Anglican minister who devised modern dispensationalism, an innovative Protestant theological interpretation of the Bible that was incorporated in the development of modern Evangelicalism. Cyrus Scofield further promoted the influence of dispensationalism through the explanatory notes to his Scofield Reference Bible. According to scholar Mark S. Sweetnam, who takes a cultural studies perspective, dispensationalism can be defined in terms of its Evangelicalism, its insistence on the literal interpretation of Scripture, its recognition of stages in God's dealings with humanity, its expectation of the imminent return of Christ to rapture His saints, and its focus on both apocalypticism and premillennialism.
Notable figures of the latter half of the 19th century include Charles Spurgeon in London and Dwight L. Moody in Chicago. Their powerful preaching reached very large audiences.
An advanced theological perspective came from the Princeton theologians from the 1850s to the 1920s, such as Charles Hodge, Archibald Alexander and B.B. Warfield.
20th century.
Evangelicalism in the early part of the 20th century was dominated by the Fundamentalist movement after 1910; it rejected liberal theology and emphasized the inerrancy of the Scriptures.
Following the Welsh Revival, the Azusa Street Revival in 1906 began the spread of Pentecostalism in North America.
In the post–World War II period, a split developed between Evangelicals, as they disagreed among themselves about how a Christian ought to respond to an unbelieving world. Many Evangelicals urged that Christians must engage the culture directly and constructively, and they began to express reservations about being known to the world as "fundamentalists". As Kenneth Kantzer put it at the time, the name "fundamentalist" had become "an embarrassment instead of a badge of honor".
The term "neo-evangelicalism" was coined by Harold Ockenga in 1947 to identify a distinct movement within self-identified fundamentalist Christianity at the time, especially in the English-speaking world. It described the mood of positivism and non-militancy that characterized that generation. The new generation of Evangelicals set as their goal to abandon a militant Bible stance. Instead, they would pursue dialogue, intellectualism, non-judgmentalism, and appeasement. They further called for an increased application of the gospel to the sociological, political, and economic areas.
The self-identified fundamentalists also cooperated in separating their "neo-Evangelical" opponents from the "fundamentalist" name, by increasingly seeking to distinguish themselves from the more open group, whom they often characterized derogatorily by Ockenga's term, "neo-Evangelical" or just Evangelical.
The fundamentalists saw the Evangelicals as often being too concerned about social acceptance and intellectual respectability, and being too accommodating to a perverse generation that needed correction. In addition, they saw the efforts of evangelist Billy Graham, who worked with non-Evangelical denominations, such as the Roman Catholics (which they claimed to be heretical), as a mistake.
The post-war period also saw growth of the ecumenical movement and the founding of the World Council of Churches, which was generally regarded with suspicion by the Evangelical community.
In the United Kingdom, John Stott and Martyn Lloyd-Jones emerged as key leaders in Evangelical Christianity.
The charismatic movement began in the 1960s and resulted in Pentecostal theology and practice being introduced into many mainline denominations. New charismatic groups such as the Association of Vineyard Churches and Newfrontiers trace their roots to this period (see also British New Church Movement).
The closing years of the 20th century saw controversial postmodern influences entering some parts of Evangelicalism, particularly with the emerging church movement.
Global statistics.
According to a 2011 Pew Forum study on global Christianity, 285,480,000 or 13.1 percent of all Christians are Evangelicals. The largest concentration of Evangelicals can be found in the United States, with 26.8% of the U.S. population or 94.38 million, the latter being roughly one third of the world's Evangelicals. The next most populous is Brazil, with 26.3% or 51.33 million.
The World Evangelical Alliance is "a network of churches in 129 nations that have each formed an evangelical alliance and over 100 international organizations joining together to give a world-wide identity, voice, and platform to more than 600 million evangelical Christians". The Alliance was formed in 1951 by Evangelicals from 21 countries. It has worked to support its members to work together globally.
The World Christian Database estimates the number of Evangelicals at 300 million, Pentecostals and Charismatics at 600 million and "Great Commission" Christians at 700 million. These groups are not mutually exclusive. Operation World estimates the number of Evangelicals at 550 million.
From 1960 to 2000, the global growth of the number of reported Evangelicals grew three times the world's population rate, and twice that of Islam.
Africa.
In the 21st century, there are Evangelical churches active in Sudan, Angola, Mozambique, Zimbabwe, Malawi, Rwanda, Uganda, Ghana, Kenya, Zambia, South Africa, and Nigeria. They have grown especially since independence came in the 1960s, the strongest movements are based on Pentecostal-charismatic beliefs, and comprise a way of life that has led to upward social mobility and demands for democracy. There is a wide range of theology and organizations, including some sponsored by European missionaries and others that have emerged from African culture such as the Apostolic and Zionist Churches which enlist 40% of black South Africans, and their Aladura counterparts in western Africa.
In Nigeria the Evangelical Church Winning All (formerly "Evangelical Church of West Africa") is the largest church organization with five thousand congregations and over three million members. It sponsors two seminaries and eight Bible colleges, and 1600 missionaries who serve in Nigeria and other countries with the Evangelical Missionary Society (EMS). There have been serious confrontations since 1999 between Muslims and Evangelical Christians standing in opposition to the expansion of Sharia law in northern Nigeria. The confrontation has radicalized and politicized the Christians. Violence has been escalating.
In Kenya, mainstream Evangelical denominations have taken the lead in promoting political activism and backers, with the smaller Evangelical sects of less importance. Daniel arap Moi was president 1978 to 2002 and claimed to be an Evangelical; he proved intolerant of dissent or pluralism or decentralization of power.
The Berlin Missionary Society (BMS) was one of four German Protestant mission societies active in South Africa before 1914. It emerged from the German tradition of Pietism after 1815 and sent its first missionaries to South Africa in 1834. There were few positive reports in the early years, but it was especially active 1859–1914. It was especially strong in the Boer republics. The World War cut off contact with Germany, but the missions continued at a reduced pace. After 1945 the missionaries had to deal with decolonisation across Africa and especially with the apartheid government. At all times the BMS emphasized spiritual inwardness, and values such as morality, hard work and self-discipline. It proved unable to speak and act decisively against injustice and racial discrimination and was disbanded in 1972.
Since 1974, young professionals have been the active proselytizers of Evangelicalism in the cities of Malawi.
In Mozambique, Evangelical Protestant Christianity emerged around 1900 from black migrants whose converted previously in South Africa. They were assisted by European missionaries, but, as industrial workers, they paid for their own churches and proselytizing. They prepared southern Mozambique for the spread of Evangelical Protestantism. During its time as a colonial power in Mozambique, the Catholic Portuguese government tried to counter the spread of Evangelical Protestantism.
Latin America.
In modern Latin America, the word "Evangelical" is often simply a synonym for "Protestant".
Brazil.
Protestantism in Brazil largely originated with German immigrants and British and American missionaries in the 19th century, following up on efforts that began in the 1820s.
In the late nineteenth century, while the vast majority of Brazilians were nominal Catholics, the nation was underserved by priests, and for large numbers their religion was only nominal. The Catholic Church in Brazil was de-established in 1890, and responded by increasing the number of dioceses and the efficiency of its clergy. Many Protestants came from a large German immigrant community, but they were seldom engaged in proselytism and grew mostly by natural increase.
Methodists were active along with Presbyterians and Baptists. The Scottish missionary Dr. Robert Reid Kalley, with support from the Free Church of Scotland, moved to Brazil in 1855, founding the first Evangelical church among the Portuguese-speaking population there in 1856. It was organized according to the Congregational policy as the Igreja Evangélica Fluminense; it became the mother church of Congregationalism in Brazil. The Seventh-day Adventists arrived in 1894, and the YMCA was organized in 1896. The missionaries promoted schools colleges and seminaries, including a liberal arts college in São Paulo, later known as Mackenzie, and an agricultural school in Lavras. The Presbyterian schools in particular later became the nucleus of the governmental system. In 1887 Protestants in Rio de Janeiro formed a hospital. The missionaries largely reached a working-class audience, as the Brazilian upper-class was wedded either to Catholicism or to secularism. By 1914, Protestant churches founded by American missionaries had 47,000 communicants, served by 282 missionaries. In general, these missionaries were more successful than they had been in Mexico, Argentina or elsewhere in Latin America.
There were 700,000 Protestants by 1930, and increasingly they were in charge of their own affairs. In 1930, the Methodist Church of Brazil became independent of the missionary societies and elected its own bishop. Protestants were largely from a working-class, but their religious networks help speed their upward social mobility.
Protestants accounted for fewer than 5% of the population until the 1960s, but grew exponentially by proselytizing and by 2000 made up over 15% of Brazilians affiliated with a church. Pentecostals and charismatic groups account for the vast majority of this expansion.
Pentecostal missionaries arrived early in the 20th century. Pentecostal conversions surged during the 1950s and 1960s, when native Brazilians began founding autonomous churches. The most influential included Brasil Para o Cristo (Brazil for Christ), founded in 1955 by Manoel de Mello. With an emphasis on personal salvation, on God's healing power, and on strict moral codes these groups have developed broad appeal, particularly among the booming urban migrant communities. In Brazil, since the mid-1990's, groups committed to uniting black identity, antiracism, and Evangelical theology have rapidly proliferated. Pentecostalism arrived in Brazil with Swedish and American missionaries in 1911. it grew rapidly, but endured numerous schisms and splits. In some areas the Evangelical Assemblies of God churches have taken a leadership role in politics since the 1960s. They claimed major credit for the election of Fernando Collor de Mello as president of Brazil in 1990.
According to the 2000 Census, 15.4% of the Brazilian population was Protestant. A recent research conducted by the Datafolha institute shows that 25% of Brazilians are Protestants, of which 19% are followers of Pentecostal denominations. The 2010 Census found out that 22.2% were Protestant at that date. Protestant denominations saw a rapid growth in their number of followers since the last decades of the 20th century. They are politically and socially conservative, and emphasize that God's favor translates into business success. The rich and the poor remained traditional Catholics, while most Evangelical Protestants were in the new lower-middle class–known as the "C class" (in a A–E classification system).
Chesnut argues that Pentecostalism has become "one of the principal organizations of the poor," for these churches provide the sort of social network that teach members the skills they need to thrive in a rapidly developing meritocratic society.
One large Evangelical church is the Universal Church of the Kingdom of God (IURD), a neo‐Pentecostal denomination begun in 1977. It now has a presence in many countries, and claims millions of members worldwide.
Guatemala.
Protestants remained a small portion of the population until the late-twentieth century, when various Protestant groups experienced a demographic boom that coincided with the increasing violence of the Guatemalan Civil War. Two Guatemalan heads of state, General Efraín Ríos Montt and Jorge Serrano Elia, have been practicing Evangelical Protestants. They are the only two Protestant heads of state in the history of Latin America. General Montt, an Evangelical from the Pentecostal tradition, came to power through a coup. He escalated the war against leftist guerilla insurgents as a holy war against atheistic forces of evil.
Asia.
Korea.
Protestant missionary activity in Asia was most successful in Korea. American Presbyterians and Methodists arrived in the 1880s and were well received. Between 1907 and 1945, when Korea was a Japanese colony, Christianity became in part an expression of nationalism in opposition to Japan's efforts to promote the Japanese language and the Shinto religion. In 1914, out of 16 million people, there were 86,000 Protestants and 79,000 Catholics; by 1934, the numbers were 168,000 and 147,000. Presbyterian missionaries were especially successful. Since the Korean War (1950–53), many Korean Christians have migrated to the U.S., while those who remained behind have risen sharply in social and economic status. Most Korean Protestant churches in the 21st century emphasize their Evangelical heritage. Korean Protestantism is characterized by theological conservatism coupled with an emotional revivalistic style. Most churches sponsor revival meetings once or twice a year. Missionary work is a high priority, with 13,000 men and women serving in missions across the world, putting Korea in second place just behind the US.
Sukman argues that since 1945, Protestantism has been widely seen by Koreans as the religion of the middle class, youth, intellectuals, urbanites, and modernists. It has been a powerful force supporting South Korea's pursuit of modernity and emulation of the United States, and opposition to the old Japanese colonialism and to the authoritarianism of North Korea. There are 8.6 million adherents to Protestant Christianity (approximately 19% of the Korean population) in which many identify themselves as Evangelicals.
South Korea has been referred as an "evangelical superpower" for being the home to some of the largest and most dynamic Christian churches in the world; South Korea is also second to the U.S. in the number of missionaries sent abroad.
United Kingdom.
There are an estimated 2 million Evangelicals in the UK. According to research performed by the Evangelical Alliance in 2013, 87% of UK evangelicals attend Sunday morning church services every week and 63% attend weekly or fortnightly small groups. An earlier survey conducted in 2012 found that 92% of evangelicals agree it is a Christian's duty to help those in poverty and 45% attend a church which has a fund or scheme that helps people in immediate need, and 42% go to a church that supports or runs a foodbank. 63% believe a tithing, and so give around 10% of their income to their church, Christian organisations and various charities 83% of UK evangelicals believe that the Bible has supreme authority in guiding their beliefs, views and behaviour and 52% read or listen to the Bible daily. The Evangelical Alliance, formed in 1846, was the first ecumenical evangelical body in the world and works to unite evangelicals, helping them listen to, and be heard by, the government, media and society.
United States.
The contemporary North American usage of the term reflects the impact of the Fundamentalist–Modernist Controversy of the early 20th century. Evangelicalism may sometimes be perceived as the middle ground between the theological liberalism of the mainline denominations and the cultural separatism of fundamentalism. Evangelicalism has therefore been described as "the third of the leading strands in American Protestantism, straddl the divide between fundamentalists and liberals". In 2004 Andrew Crouch wrote in "Christianity Today": "The emerging movement is a protest against much of evangelicalism as currently practiced. It is post-evangelical in the way that neo-evangelicalism (in the 1950s) was post-fundamentalist. It would not be unfair to call it postmodern evangelicalism."
While the North American perception has a certain importance in understanding some usage of the term, it by no means dominates a wider global view: elsewhere the fundamentalist debate had less direct influence.
D.W. Cloud wrote: "In the first half of the 20th century, evangelicalism in America was largely synonymous with fundamentalism. George Marsden in "Reforming Fundamentalism" (1995) writes, "There was not a practical distinction between fundamentalist and evangelical: the words were interchangeable" (p. 48). When the National Association of Evangelicals (NAE) formed in 1942, for example, participants included such fundamentalist leaders as Bob Jones, Sr., John R. Rice, Charles Woodbridge, Harry Ironside, and David Otis Fuller."
By the mid-1950s, largely due to the ecumenical evangelism of Billy Graham, the terms "Evangelicalism" and "fundamentalism" began to refer to two different approaches. Fundamentalism aggressively attacked its liberal enemies; Evangelicalism downplayed liberalism and emphasized outreach and conversion of new members.
While some conservative Evangelicals believe the label has "broadened" too much beyond its more limiting traditional distinctives, this trend is nonetheless strong enough to create significant ambiguity in the term. As a result, the dichotomy between "Evangelical" and "mainline" denominations is increasingly complex, particularly with such innovations as the "emergent church" movement.
20th century.
By the 1890s, most American Protestants belonged to Evangelical denominations, except for the high church Episcopalians and German Lutherans. In the early 20th century, a divide opened up between the Fundamentalists and the Mainline Protestant denominations, chiefly over the inerrancy of the Bible. The fundamentalists were those Evangelicals who sought to defend their religious traditions, and feared that modern scientific leanings were leading away from the truth. A favored mode of fighting back was to prohibit the teaching of Darwinism or macro-evolution as fact in the public schools, a movement that reached its peak in the Scopes Trial of 1925, and resumed in the 1980s. The more modernistic Protestants largely abandoned the term "evangelical" and tolerated evolutionary theories in modern science and even in Biblical studies.
Evangelicals held the view that the modernist and liberal parties in the Protestant churches had surrendered their heritage as Evangelicals by accommodating the views and values of secularism. At the same time, the modernists criticized fundamentalists for their separatism and their rejection of the Social Gospel.
During and after World War II, Evangelicals increasingly organized, and expanded their vision to include the entire world. There was a great expansion of Evangelical activity within the United States, "a revival of revivalism." Youth for Christ was formed; it later became the base for Billy Graham's revivals. The National Association of Evangelicals formed in 1942 as a counterpoise to the mainline Federal Council of Churches. In 1942–43, the Old-Fashioned Revival Hour had a record-setting national radio audience.
Even more dramatic was the expansion of international missionary activity by the Evangelicals. They had enthusiasm and self-confidence after the national victory in the world war. Many Evangelicals came from poor rural districts, but wartime and postwar prosperity dramatically increased the funding resources available for missionary work. While mainline Protestant denominations cut back on their missionary activities, from 7000 to 3000 overseas workers between 1935 and 1980, the Evangelicals increased their career foreign missionary force from 12,000 in 1935 to 35,000 in 1980. Meanwhile Europe was falling behind, as North Americans comprised 41% of all the world's Protestant missionaries in 1936, rising to 52% in 1952 and 72% in 1969. The most active denominations were the Assemblies of God, which nearly tripled from 230 missionaries in 1935 to 626 in 1952, and the United Pentecostal Church International, formed in 1945. The Southern Baptists more than doubled from 405 to 855, as did the Church of the Nazarene from 88 to 200. Overseas missionaries began to prepare for the postwar challenge, most notably the Far Eastern Gospel Crusade (FEGC; now named "Send International"). After Nazi Germany and fascist Japan had been destroyed, the newly mobilized Evangelicals were now prepared to combat atheistic communism, secularism, Darwinism, liberalism, Catholicism, and (in overseas missions) paganism.
Meaning of Evangelicalism in the US.
The Institute for the Study of American Evangelicals states:
Demographics.
The 2004 survey of religion and politics in the United States identified the Evangelical percentage of the population at 26.3 percent while Roman Catholics are 22 percent and mainline Protestants make up 16 percent. In the 2007 Statistical Abstract of the United States, the figures for these same groups are 28.6 percent (Evangelical), 24.5 percent (Roman Catholic), and 13.9 percent (mainline Protestant.) The latter figures are based on a 2001 study of the self-described religious identification of the adult population for 1990 and 2001 from the Graduate School and University Center at the City University of New York. A 2008 study showed that in the year 2000 about 9 percent of Americans attended an Evangelical service on any given Sunday. The Economist estimated in May 2012, that "over one-third of Americans, more than 100 M, can be considered evangelical," arguing that the percentage is often undercounted because many black Christians espouse Evangelical theology but prefer to refer to themselves as "born again Christians" rather than "evangelical." These estimated figures given by The Economist agree with those in 2012 from Wheaton College's Institute for the Studies of American Evangelicals.
The movement is highly diverse and encompasses a vast number of people. Because the group is diverse, not all of them use the same terminology for beliefs. For instance, several recent studies and surveys by sociologists and political scientists that utilize more complex definitional parameters have estimated the number of Evangelicals in the U.S. in 2012 at about 30–35% of the population, or roughly between 90 and 100 million people.
The National Association of Evangelicals is a U.S. agency which coordinates cooperative ministry for its member denominations.
Types of Evangelical.
John C. Green, a senior fellow at the Pew Forum on Religion and Public Life, used polling data to separate Evangelicals into three camps, which he labels as traditionalist, centrist and modernist:
Politics.
Christian right.
Evangelical political influence in America was first evident in the 1830s with movements such as abolition of slavery and the prohibition movement, which closed saloons and taverns in state after state until it succeeded nationally in 1919. The Christian right is a coalition of numerous groups of traditionalist and observant church-goers of every kind: especially Catholics on issues such as birth control and abortion, Southern Baptists, Missouri Synod Lutherans and others. Since the early 1980s, the Christian right has been associated with several nonprofit political and issue-oriented organizations including the Moral Majority, the Christian Coalition, Focus on the Family and the Family Research Council.
Christian left.
Evangelical political activists are not all on the right. There is a small group of liberal white Evangelicals. Most African Americans belong to Baptist, Methodist or other denominations that share Evangelical beliefs; they are firmly in the Democratic coalition and (with the possible exception of issues involving abortion and homosexuality) are generally liberal in politics.
The evangelical left or progressive evangelicals are Christians aligned with evangelicalism in the United States who generally function on the left wing of the movement, either politically or theologically or both. While the evangelical left is related to the wider Christian left, those who are part of the latter category are not always viewed as evangelical.
Typically, members of the evangelical left affirm the primary tenets of evangelical theology, such as the doctrines of the Incarnation, atonement, and resurrection, and also see the Bible as a primary authority for the Church. Unlike many evangelicals, however, those on the evangelical left support what are often considered progressive or left wing political policies. They are often, for example, opposed to capital punishment and supportive of gun control and welfare programs. In many cases, they are also pacifists. Theologically they also often support and utilize modern biblical criticism, whereas more conservative evangelicals reject it. Some promote the legalization of same-sex marriage or protection of access to abortion for the society at large without necessarily endorsing the practice themselves.
There is considerable dispute over how to even characterize the various segments of the evangelical theological and political spectra, and whether a singular discernible rift between "right" and "left" is oversimplified. However, to the extent that some simplifications are necessary to discuss any complex issue, it is recognized that modern trends like focusing on non-contentious issues (like poverty) and downplaying hot-button social issues (like abortion) tend to be key distinctives of the modern "evangelical left" or "emergent church" movement.
While members of the evangelical left chiefly reside in mainline denominations, they are often heavily influenced by the Anabaptist social tradition.
Recurrent themes.
Evolution.
Some Evangelical Christians strongly dispute the scientifically accepted Neo-Darwinian theory of evolution and have influenced schools to teach Creationism or "Intelligent design" (the pseudoscientific position that God or another active intelligence intervened at various points over the course of an otherwise uninterrupted natural evolutionary history to produce the complexity and diversity of life observed on Earth). There have been a variety of court cases over the issue. Young Earth Creationist Evangelicals also reject the scientific consensus on the age of the Earth and the cosmos, believing the physical universe to have originated between six and ten thousand years ago, a claim based on a literalistic reading of the first chapters of the Book of Genesis and of other Biblical texts.
Abortion.
Since 1980, a central issue motivating conservative Evangelicals' political activism is abortion. The 1973 decision in "Roe v. Wade" by the Supreme Court, which legalized abortion, proved decisive in bringing together Catholics and Evangelicals in a political coalition, which became known as the Religious Right when it successfully mobilized its voters behind presidential candidate Ronald Reagan in 1980.
Secularism.
In the United States, Supreme Court decisions that outlawed organized prayer in school and restricted church-related schools (for example, preventing them from engaging in racial discrimination while also receiving a tax exemption) also played a role in mobilizing the Religious Right. In addition, questions of sexual morality and homosexuality have been energizing factors—and above all, the notion that "elites" are pushing America into secularism.
Christian nation.
Opponents criticise the Evangelicals, whom they say actually want a Christian America—in other words, for America to be a nation in which Christianity is given a privileged position. Survey data shows that "between 64 and 75 percent do not favor a 'Christian Nation' amendment", though between 60 and 75 percent also believe that Christianity and Political Liberalism are incompatible. Evangelical leaders, in turn, counter that they merely seek freedom from the imposition by national elites of an equally subjective secular worldview, and feel that it is their opponents who are violating their rights.
Media references.
Many films offer differing views on evangelical, End Times and Rapture culture. One that offers a revealing view of the mindset of the Calvinist and premillennial dispensationalist element in evangelicalism is "Good People Go to Hell, Saved People Go to Heaven."
Other issues.
According to recent reports in the "New York Times", some Evangelicals have sought to expand their movement's social agenda to include poverty, combating AIDS in the Third World, and protecting the environment. This is highly contentious within the Evangelical community, since more conservative Evangelicals believe that this trend is compromising important issues and prioritizing popularity and consensus too highly. Personifying this division were the Evangelical leaders James Dobson and Rick Warren, the former who warned of the dangers of a Barack Obama victory in 2008 from his point of view, in contrast with the latter who declined to endorse either major candidate on the grounds that he wanted the church to be less politically divisive and that he agreed substantially with both men.

</doc>
