<doc id="6760" url="https://en.wikipedia.org/wiki?curid=6760" title="Cryonics">
Cryonics

Cryonics (from Greek κρύος 'kryos-' meaning 'cold') is the low-temperature preservation (usually at -196°C) of people who cannot be sustained by contemporary medicine, with the hope that resuscitation and restoration to full health may be possible in the far future. Cryopreservation of humans is not reversible with present technology; hope that medical advances will someday allow cryopreserved people to be revived.
Cryonics is regarded with skepticism within the mainstream scientific community and is not part of normal medical practice. It is not known if it will ever be possible to revive a cryopreserved human being. Cryonics depends on beliefs that death is a process rather than an event, clinical death is a prognosis of death rather than a diagnosis of death, and that the cryonics patient has not experienced information-theoretic death. Such views are at the speculative edge of medicine.
Cryonics procedures can only begin after legal death, and cryonics "patients" are considered legally dead. Cryonics procedures ideally begin within minutes of cardiac arrest, and use cryoprotectants to prevent ice formation during cryopreservation. The first human being to be cryopreserved was Dr. James Bedford in 1967. As of 2014, about 250 people were cryopreserved in the United States, with 1500 more having made arrangements for cryopreservation after their legal death.
Theory.
Long-term memory is stored in cell structures and molecules within the brain. In surgeries on the aortic arch, hypothermia is used to cool the body while the heart is stopped; this is done primarily to spare the brain by slowing its metabolic rate, reducing the need for oxygen, and thus reducing damage from lack of oxygen. The metabolic rate can be reduced by around 50% at 28 °C, by around 80% at 18 °C or profound hypothermia. By keeping the brain at around 25 °C, (considered deep hypothermia), surgeries can stretch to be around a half-hour with very good neurological recovery rates; stretching that to 40 minutes increases the risk of short term and long term neurological damage.
Cryonics goes further than the mainstream consensus that the brain doesn't have to be continuously active to survive or retain memory. Cryonics controversially asserts that a human person survives even within an inactive brain that's been badly damaged provided that original encoding of memory and personality can, in theory, be adequately inferred and reconstituted from structure that remains. Cryonicists argue that as long as brain structure remains intact, there is no fundamental barrier, given our current understanding of physical law, to recovering its information content. Cryonicists consider a cryonics "patient" who has been declared legally dead to possibly still be alive by information theoretic criteria, and argue that true "death" should be defined as irreversible loss of brain information critical to personal identity, rather than inability to resuscitate using current technology. The cryonics argument that death doesn't occur as long as brain structure remains intact and theoretically repairable has received some mainstream medical discussion in the context of the ethical concept of brain death and organ donation.
Cryonics uses temperatures below -130°C, called cryopreservation, in an attempt to preserve enough brain information to permit future revival of the cryopreserved person. Cryopreservation may be accomplished by freezing, freezing with cryoprotectant to reduce ice damage, or by vitrification to avoid ice damage. Even using the best methods, cryopreservation of whole bodies or brains is very damaging and irreversible with current technology.
Cryonics requires future technology to repair or regenerate tissue that is diseased, damaged, or missing. Brain repairs in particular will require analysis at the molecular level. This far future technology is usually assumed to be nanomedicine based on molecular nanotechnology. Biological repair methods or mind uploading have also been proposed.
Practice.
Costs can include payment for medical personnel to be on call for death, vitrification, transportation in dry ice to a preservation facility, and payment into a trust fund intended to cover indefinite storage in liquid nitrogen and future revival costs. As of 2011, U.S. cryopreservation costs can range from $28,000 to $200,000, and are often financed via life insurance. KrioRus, which stores bodies communally in large dewars, charges $12,000 to $36,000 for the procedure. Some patients opt to have only their head, rather than their whole body, cryopreserved. As of 2016, four facilities exist in the world to retain cryopreserved bodies; three are in the U.S., and one is in Russia. As of 2014, about 250 people have been cryogenically preserved in the U.S., and around 1500 more have signed up to be preserved.
Obstacles to success.
Preservation injury.
Long-term preservation of biological tissue can be achieved by cooling to temperatures below -130°C. Immersion in liquid nitrogen at a temperature of -196°C (77 Kelvin) is often used for convenience. Low temperature preservation of tissue is called cryopreservation. Contrary to popular belief, water that freezes during cryopreservation is usually water outside cells, not water inside cells. Cells don't burst during freezing, but instead become dehydrated and compressed between ice crystals that surround them. Intracellular ice formation only occurs if the rate of freezing is faster than the rate of osmotic loss of water to the extracellular space.
Without cryprotectants, cell shrinkage and high salt concentrations during freezing usually prevent frozen cells from functioning again after thawing. In tissues and organs, ice crystals can also disrupt connections between cells that are necessary for organs to function. The difficulties of recovering large animals and their individual organs from a frozen state have been long known. Attempts to recover frozen mammals by simply rewarming them were abandoned by 1957. At present, only cells, tissues, and some small organs can be reversibly cryopreserved.
When used at high concentrations, cryoprotectants can stop ice formation completely. Cooling and solidification without crystal formation is called vitrification. The first cryoprotectant solutions able to vitrify at very slow cooling rates while still being compatible with whole organ survival were developed in the late 1990s by cryobiologists Gregory Fahy and Brian Wowk for the purpose of banking transplantable organs. This has allowed animal brains to be vitrified, warmed back up, and examined for ice damage using light and electron microscopy. No ice crystal damage was found; remaining cellular damage was due to dehydration and toxicity of the cryoprotectant solutions. Large vitrified organs tend to develop fractures during cooling, a problem worsened by the large tissue masses and very low temperatures of cryonics.
The use of vitrification rather than freezing for cryonics was anticipated in 1986, when K. Eric Drexler proposed a technique called "fixation and vitrification", anticipating reversal by molecular nanotechnology. In 2016, Robert L. McIntyre and Gregory Fahy at the cryobiology research company 21st Century Medicine, Inc. won the Small Animal Brain Preservation Prize of the Brain Preservation Foundation by demonstrating to the satisfaction of neuroscientist judges that a particular implementation of fixation and vitrification called "aldehyde-stabilized cryopreservation" could preserve a rabbit brain in "near perfect" condition at -135°C, with the cell membranes, synapses, and intracellular structures intact in electron micrographs. Brain Preservation Foundation President, Ken Hayworth, said, "This result directly answers a main skeptical and scientific criticism against cryonics—that it does not provably preserve the delicate synaptic circuitry of the brain.” However the price paid for perfect preservation as seen by microscopy was tying up all protein molecules with chemical crosslinks, completely eliminating biological viability. Actual cryonics organizations use vitrification without a chemical fixation step, sacrificing some structural preservation quality for supposedly less damage at the molecular level.
While preservation of both structure and function has been possible for brain slices using vitrification, this goal remains elusive for whole brains. In absence of a revived brain, or brain simulation from somehow scanning a preserved brain, the adequacy of present vitrification technology (with or without fixation) for preserving the anatomical and molecular basis of long-term memory as required by cryonics is still unproven.
Outside the cryonics community, many scientists have a blanket skepticism toward existing preservation methods. Cryobiologist Dayong Gao states that "we simply don't know if (subjects have) been damaged to the point where they've 'died' during vitrification because the subjects are now inside liquid nitrogen canisters." Biochemist Ken Storey argues (based on experience with organ transplants), that "even if you only wanted to preserve the brain, it has dozens of different areas, which would need to be cryopreserved using different protocols."
Revival.
Those who believe that revival may someday be possible generally look toward advanced bioengineering, molecular nanotechnology, or nanomedicine as key technologies. Revival would require repairing damage from lack of oxygen, cryoprotectant toxicity, thermal stress (fracturing), freezing in tissues that do not successfully vitrify, and reversing the effects that caused death. In many cases extensive tissue regeneration would be necessary.
According to Cryonics Institute president Ben Best, cryonics revival may be similar to a last in, first out process. People cryopreserved in the future, with better technology, may require less advanced technology to be revived because they will have been cryopreserved with better technology that caused less damage to tissue. In this view, preservation methods would get progressively better until eventually they are demonstrably reversible, after which medicine would begin to reach back and revive people cryopreserved by more primitive methods.
Alternatively, some cryonicists propose that a brain could be electronically scanned and uploaded into a digital computer using hypothetical far-future technology. For some, this raises a philosophical issue: would such an upload "actually be you"; would it be "a new person who is like you but whose conscious experience you don’t have access to"; or would it merely be a "philosophical zombie"?
Legal issues.
Legally, most countries treat preserved individuals as deceased persons because of laws that forbid vitrifying someone who is medically alive. Cryonics providers tend to be treated as medical research institutes. In France, cryonics is not considered a legal mode of body disposal; only burial, cremation, and formal donation to science are allowed. However, bodies may legally be shipped to other countries for cryonic freezing. As of 2015, the Canadian province of British Columbia prohibits the sale of arrangements for body preservation based on cryonics. In Russia, cryonics falls outside both the medical industry and the funeral services industry, making it easier in Russia than in the U.S. to get hospitals and morgues to release cryonics candidates.
Ethics.
Writing in Bioethics, David Shaw examines the ethical status of cryonics. The arguments against it include changing the concept of death, the expense of preservation and revival, lack of scientific advancement to permit revival, temptation to use premature euthanasia, and failure due to catastrophe. Arguments in favor of cryonics include the potential benefit to society, the prospect of immortality, and the benefits associated with avoiding death. Shaw explores the relatively minor expense and the potential payoff, and applies it to an adapted version of Pascal's Wager.
History.
20th century.
In 1922 Alexander Yaroslavsky, member of Russian immortalists-biocosmists movement, wrote "Anabiosys Poem". However, the modern era of cryonics began in 1962 when Michigan college physics teacher Robert Ettinger proposed in a privately published book, "The Prospect of Immortality", that freezing people may be a way to reach future medical technology. (The book was republished in 2005 and remains in print.) Even though freezing a person is apparently fatal, Ettinger argued that what appears to be fatal today may be reversible in the future. He applied the same argument to the process of dying itself, saying that the early stages of clinical death may be reversible in the future. Combining these two ideas, he suggested that freezing recently deceased people may be a way to save lives. In 1955 James Lovelock was able to reanimate rats frozen at 0 Celsius using microwave diathermy.
Slightly before Ettinger’s book was complete, Evan Cooper (writing as Nathan Duhring) privately published a book called "Immortality: Physically, Scientifically, Now" that independently suggested the same idea. Cooper founded the Life Extension Society (LES) in 1964 to promote freezing people. Ettinger came to be credited as the originator of cryonics, perhaps because his book was republished by Doubleday in 1964 on recommendation of Isaac Asimov and Fred Pohl, and received more publicity. Ettinger also stayed with the movement longer.
The first person was cryopreserved in 1967 (James Bedford). In the U.S., cryonics took a reputation hit around the 1970s: the Cryonics Society of California, led by a former TV repairman named Robert Nelson with no scientific background, ran out of money to maintain cryopreservation of existing patients; Nelson was sued for allowing nine bodies to decompose.
Demographics.
Cryonicists are predominantly nonreligious white males; according to the New York Times, among living cryonicists, men outnumber women by about three to one. According to The Guardian, most cryonicists used to be "young, male and geeky", but recent demographics may have shifted a bit toward whole families.
In 2015 Du Hong, a 61-year-old female writer of children's literature, became the first known Chinese person to be cryopreserved.
Public reception.
Some scientists have expressed skepticism about cryonics in media sources, however the number of peer-reviewed papers on cryonics is limited because its speculative aspects place it outside of the focus of most academic fields. While most neuroscientists agree that all the subtleties of a human mind are contained in its anatomical structure, few neuroscientists will comment directly upon the topic of cryonics due to its speculative nature. Individuals who intend to be frozen are often "looked at as a bunch of kooks", despite many of them being scientists and doctors.
At the extreme, some people are openly hostile to the idea of cryonics.
According to cryonicist Aschwin de Wolf and others, cryonics can often produce intense hostility from spouses who are not cryonicists. James Hughes, the executive director of the pro-life-extension Institute for Ethics and Emerging Technologies, chooses not to personally sign up for cryonics, calling it a worthy experiment but stating laconically that "I value my relationship with my wife."
Cryobiologist Dayong Gao states that "People can always have hope that things will change in the future, but there is no scientific foundation supporting cryonics at this time." Alcor disagrees, stating that "There are no known credible technical arguments that lead one to conclude that cryonics, carried out under good conditions today, would not work."
Many people assert there would be no point in being revived in the far future, if their friends and families are dead. While it's universally agreed that "personal identity" is uninterrupted when brain activity temporarily ceases during incidents of accidental drowning (where people have been restored to normal functioning after being completely submerged in cold water for up to 66 minutes), some people express concern that a centuries-long cryopreservation might interrupt their conception of personal identity, such that the revived person would "not be you".
In fiction.
Suspended animation is a popular theme in science fiction and fantasy settings, appearing in comic books, films, literature, and television. A survey in Germany found that about half of the respondents were familiar with cryonics, and about half of those familiar with cryonics had learned of the subject from films or television. Some commonly known examples of cryonics being used in popular culture include, Vanilla Sky, Futurama, and Nip/Tuck.
Famous people.
Famous people who are cryopreserved.
Among the cryopreserved are James Bedford, Dick Clair, L. Stephen Coles (in 2014), Thomas K. Donaldson, FM-2030, Hal Finney (in 2014), Jerry Leaf, John-Henry Williams, and Ted Williams.
Famous people associated with cryonics but who were not cryopreserved.
The urban legend suggesting Walt Disney was cryopreserved is false; he was cremated and interred at Forest Lawn Memorial Park Cemetery. Robert A. Heinlein, who wrote enthusiastically of the concept in "The Door into Summer" (serialized in 1956), was cremated and had his ashes distributed over the Pacific Ocean. Timothy Leary was a long-time cryonics advocate and signed up with a major cryonics provider, but he changed his mind shortly before his death and was not cryopreserved.

</doc>
<doc id="6761" url="https://en.wikipedia.org/wiki?curid=6761" title="Unitary patent">
Unitary patent

The European patent with unitary effect (EPUE), more commonly known as the unitary patent, is a new type of European patent in advanced stage of adoption which would be valid in participating member states of the European Union. Unitary effect can be registered for a European patent upon grant, replacing validation of the European patent in the individual countries concerned. The unitary effect means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection—which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.
Formal agreement on the two EU regulations that made the unitary patent possible through enhanced cooperation at EU level was reached between the European Council and European Parliament on 17 December 2012. The legality of the two regulations was however challenged by Spain and Italy, who filed in total four actions for annulment, two of which were rejected, and two of which are currently pending before the European Court of Justice. All EU member states except Spain and Croatia participate in the enhanced cooperation. Unitary effect of newly granted European patents can be requested, from the date the related Unified Patent Court Agreement enters into force for the first group of ratifiers, and will extend to those participating member states for which the UPC Agreement had entered into force upon the registration of unitary effect.
The negotiations which resulted in the unitary patent can be traced back to various initiatives dating to the 1970s. At different times, the project, or very similar projects, have been referred to as the "European Union patent" (the name used in the EU treaties, which serve as the legal basis for EU competency), "EU patent", "Community patent", "European Community Patent", "EC patent" and "COMPAT".
By not requiring translations into a language of each contracting state, and by requiring the payment of only a single renewal fee for the group of contracting states, the unitary patent aims to be cheaper than European patents. Instead, unitary patents will be accepted in English, French, or German with no further translation required after grant. Machine translations will be provided, but will be, in the words of the regulation, "for information purposes only and should not have any legal effect". The maintenance fees, with a single fee for the whole area, are also expected to be lower compared to renewal fees for the whole area but the fees have yet to be announced.
The proposed unitary patent will be a particular type of European patent, granted under the European Patent Convention. A European patent, once granted, becomes a "bundle of nationally enforceable patents", in the states which are designated by the applicant, and the unitary effect would effectively create a single enforceable region in a subgroup of those 38 states, which may coexist with nationally enforceable patents ("classical" patents) in the remaining states. "Classical", non-unitary European patents hold exclusively for single countries and require the filing of a translation in some contracting states, in accordance with .
Background.
Legislative history.
In 2009, three draft documents were published regarding a community patent: a European patent in which the European Community was designated:
Based on those documents, the European Council requested on 6 July 2009 an opinion from the Court of Justice of the European Union, regarding the compatibility of the envisioned Agreement with EU law: "‘Is the envisaged agreement creating a Unified Patent Litigation System (currently named European and Community Patents Court) compatible with the provisions of the Treaty establishing the European Community?’"
In December 2010, the use of the enhanced co-operation procedure, under which of the Treaty on the Functioning of the European Union provides that a group of member states of the European Union can choose to co-operate on a specific topic, was proposed by twelve Member States to set up a unitary patent applicable in all participating European Union Member States. The use of this procedure had only been used once in the past, for harmonising rules regarding the applicable law in divorce across several EU Member States.
In early 2011, the procedure leading to the enhanced co-operation was reported to be progressing. Twenty-five Member States had written to the European Commission requesting to participate, with Spain and Italy remaining outside, primarily on the basis of ongoing concerns over translation issues. On 15 February, the European Parliament approved the use of the enhanced co-operation procedure for unitary patent protection by a vote of 471 to 160. and on 10 March 2011 the Council gave their authorisation. Two days earlier, on 8 March 2011, the Court of Justice of the European Union had issued its opinion, stating that the draft Agreement creating the European and Community Patent Court would be incompatible with EU law. The same day, the Hungarian Presidency of the Council insisted that this opinion would not affect the enhanced co-operation procedure.
In November 2011, negotiations on the enhanced co-operation system were reportedly advancing rapidly—too fast, in some views. It was announced that implementation required an enabling European Regulation, and a Court agreement between the states that elect to take part. The European Parliament approved the continuation of negotiations in September. A draft of the agreement was issued on 11 November 2011 and was open to all member states of the European Union, but not to other European Patent Convention states. However, serious criticisms of the proposal remained mostly unresolved. A meeting of the Competitiveness Council on 5 December failed to agree on the final text. In particular, there was no agreement on where the Central Division of a Unified Patent Court should be located, "with London, Munich and Paris the candidate cities."
The Polish Presidency acknowledged on 16 December 2011 the failure to reach an agreement "on the question of the location of the seat of the central division." The Danish Presidency therefore inherited the issue. According to the President of the European Commission in January 2012, the only question remaining to be settled was the location of the Central Division of the Court. However, evidence presented to the UK House of Commons European Scrutiny Committee in February suggested that the position was more complicated. At an EU summit at the end of January 2012, participants agreed to press on and finalise the system by June. On 26 April, Herman Van Rompuy, President of the European Council, wrote to members of the Council, saying "This important file has been discussed for many years and we are now very close to a final deal... This deal is needed now, because this is an issue of crucial importance for innovation and growth. I very much hope that the last outstanding issue will be sorted out at the May Competitiveness Council. If not, I will take it up at the June European Council." The Competitiveness Council met on 30 May and failed to reach agreement.
A compromise agreement on the seat(s) of the unified court was eventually reached at the June European Council (28–29 June 2012), splitting the central division according to technology between Paris (the main seat), London and Munich. However, on 2 July 2012, the European Parliament decided to postpone the vote following a move by the European Council to modify the arrangements previously approved by MEPs in negotiations with the European Council. The modification was considered controversial and included the deletion of three key articles (6–8) of the legislation, seeking to reduce the competence of the European Union Court of Justice in unitary patent litigation. On 9 July 2012, the Committee on Legal Affairs of the European Parliament debated the patent package following the decisions adopted by the General Council on 28–29 June 2012 in camera in the presence of MEP Bernhard Rapkay. A later press release by Rapkay quoted from a legal opinion submitted by the Legal Service of the European Parliament, which affirmed the concerns of MEPs to approve the decision of a recent EU summit to delete said articles as it "nullifies central aspects of a substantive patent protection". A Europe-wide uniform protection of intellectual property would thus not exist with the consequence that the requirements of the corresponding EU treaty would not be met and that the European Court of Justice could therefore invalidate the legislation. By the end of 2012 a new compromise was reached between the European Parliament and the European Council, including a limited role for the European Court of Justice. The Unified Court will apply the Unified Patent Court Agreement, which is considered national patent law from an EU law point of view, but still is equal for each participant. the [http://www.legislation.gov.uk/ukdsi/2016/9780111142899 draft statutory instrument aimed at implementation of the Unified Court and UPC in the UK provides for different infringement laws for: European patents (unitary or not) litigated through the Unified Court; European patents (UK) litigated before UK courts; and national patents]. The legislation for the enhanced co-operation mechanism was approved by the European Parliament on 11 December 2012 and the regulations were signed by the European Council and European Parliament officials on 17 December 2012.
On 30 May 2011, Italy and Spain challenged the Council's authorisation of the use of enhanced co-operation to introduce the trilingual (English, French, German) system for the unitary patent, which they viewed as discriminatory to their languages, with the CJEU on the grounds that it did not comply with the EU treaties. In January 2013, Advocate General Yves Bot delivered his recommendation that the court reject the complaint. Suggestions by the Advocate General are advisory only, but are generally followed by the court. The case was dismissed by the court in April 2013, however Spain launched two new challenges with the EUCJ in March 2013 against the regulations implementing the unitary patent package. The court hearing for both cases was scheduled for 1 July 2014. Advocate-General Yves Bot published his opinion on 18 November 2014, suggesting that both actions be dismissed ( and ). The court handed down its decisions on 5 May 2015 as and fully dismissing the Spanish claims. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.
European patents.
European patents are granted in accordance with the provisions of the European Patent Convention, via a unified procedure before the European Patent Office. A single patent application, in one language, may be filed at the European Patent Office or at a national patent office of certain Contracting States. Patentable inventions, according to the EPC, are "any inventions, in all fields of technology, providing that they are new, involve an inventive step, and are susceptible of industrial application."
In contrast to the unified character of a European patent application, a granted European patent has, in effect, no unitary character, except for the centralized opposition procedure (which can be initiated within 9 months from grant, by somebody else than the patent proprietor), and the centralized limitation and revocation procedures (which can only be instituted by the patent proprietor). In other words, a European patent in one Contracting State, i.e. a "national" European patent, is effectively independent of the same European patent in each other Contracting State, except for the opposition, limitation and revocation procedures. The enforcement of a European patent is dealt with by national law. The abandonment, revocation or limitation of the European patent in one state does not affect the European patent in other states.
The EPC provides however the possibility for a group of member states to allow European patents to have a unitary character also after grant. Until now, only Liechtenstein and Switzerland have opted to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).
Upon filing of a European patent application, all 38 Contracting States are automatically designated, unless, when filing the application, the applicant withdraws one or more designations. This may be important to avoid conflicts with national (non European patent) applications. Designations may also be withdrawn after filing, at any time before grant. Upon grant, a European patent has immediate effect in all designated States, but to remain effective, yearly renewal fees have to be paid, in each State, and in certain countries translation requirements have to be met.
Legal basis and implementation.
Three instruments were proposed for the implementation of the unitary patent:
The system is based on EU law as well as the European Patent Convention (EPC). provides the legal basis for establishing a common system of patents for Parties to the EPC. Previously, only Liechtenstein and Switzerland had used this possibility to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).
Regulations regarding the unitary patent.
The first two regulations were approved by the European Parliament on 11 December 2012, with future application set for the 25 member states then participating in the enhanced cooperation for a unitary patent (all current EU member states except Croatia, Italy and Spain). The instruments were adopted as regulations EU 1257/2012 and 1260/2012 on 17 December 2012, and entered into force in January 2013. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.
As of July 2015, neither of the two remaining non-participants in the Unitary Patent (Spain and Croatia) had requested the European Commission to participate.
Although formally the Regulations will apply to all 26 participating states from the moment the UPC Agreement enters into force for the first group of ratifiers, the unitary effect of newly granted unitary patents will only extend to those of the 26 states where the UPC Agreement has entered into force.
Role of the European Patent Office.
Some administrative tasks relating to the European patents with unitary effect will be performed by the European Patent Office. These tasks include the collection of renewal fees and registration of unitary effect upon grant, exclusive licenses and statements that licenses are available to any person. Decisions of the European Patent Office regarding the unitary patent are open to appeal to the Unified Patent Court, rather than to the EPO Boards of Appeal.
Translation requirements.
For a unitary patent ultimately no translation will be required, which significantly reduces the cost for protection in the whole area. However, during a transition period of maximum 12 years one translation needs to be provided, either into English if the application is in French or German, or into any EU official language if the application is in English. In addition, machine translations will be provided, which will be, in the words of the regulation, "for information purposes only and should not have any legal effect".
In several contracting states, for "national" European patents a translation has to be filed within a three-month time limit after the publication of grant in the European Patent Bulletin under , otherwise the patent is considered never to have existed (void ab initio) in that state. For the 20 parties to the London Agreement, this requirement has already been abolished or reduced (e.g. by dispensing with the requirement if the patent is available in English, and/or only requiring translation of the claims). Translation requirements for the participating states in the enhanced cooperation for a unitary patent are shown below:
Unitary patent as an object of property.
Article 7 of Regulation 1257/2012 provides that, as an object of property, a European patent with unitary effect will be treated "in its entirety and in all participating Member States as a national patent of the participating Member State in which that patent has unitary effect and in which the applicant had her/his residence or principal place of business or, by default, had a place of business on the date of filing the application for the European patent." When the applicant had no domicile in a participating Member State, German law will apply. Ullrich has the criticized the system, which is similar to the Community Trademark and the Community Design, as being "in conflict with both the purpose of the creation of unitary patent protection and with primary EU law."
Implementation of the regulations at the EPO.
In January 2013, after the two regulations about the unitary patent had entered into force, but before the regulations applied, the participating member states in the unitary patent established (as member states of the European Patent Convention) a Select Committee of the Administrative Council of the European Patent Organisation in order to prepare the work for implementation of the provisions. The committee held its inaugural meeting on 20 March 2013. The work of the Select Committee has to proceed in parallel to the work of the Preparatory Committee for the creation of the Unified Patent Court. Implementation of the Unitary Patent—including the legal, administrative and financial measures—shall be completed in due time before the entry into operation of the Unified Patent Court. In May 2015, the communicated target date for completion of the remaining preparatory work of the Select Committee was 30 June 2015. The Committee reached agreement on the level of the renewal fees June 2015, while it stated that a decision on the distribution of those fees among member states was in Autumn 2015.
Agreement on a Unified Patent Court.
The Agreement on a Unified Patent Court provides the legal basis for the Unified Patent Court: a patent court for European patents (with and without unitary effect), with jurisdiction in those countries where the Agreement is in effect. In addition to regulations regarding the court structure, it also contains substantive provisions relating to the right to prevent use of an invention and allowed use by non patent proprietors (e.g. for private non-commercial use), preliminary and permanent injunctions.
Parties.
The Agreement was signed on 19 February 2013 by 24 EU member states, including all states then participating in the enhanced co-operation measures except Bulgaria and Poland, while Italy, which did not originally join the enhanced co-operation measures but subsequently signed up, did sign the UPC agreement. The agreement remains open to accession for all remaining EU member states, and Bulgaria signed the agreement on 5 March. Meanwhile, Poland decided to wait to see how the new patent system works before joining due to concerns that it would harm their economy. States which do not participate in the unitary patent regulations can still become parties to the UPC agreement, which would allow the new court to handle European patents validated in the country. Entry into force for the UPC will take place after 13 states (including Germany, France and the United Kingdom as the three states with the most patents in force) have ratified the Agreement. As of November 2015, the agreement has been ratified by 8 states (including 1 of the required ratifiers: France).
Jurisdiction.
The Unified Patent Court will have exclusive jurisdiction in infringement and revocation proceedings involving European patents with unitary effect, and during a transition period non-exclusive jurisdiction (that the patent holder can be opt out from) regarding European patents without unitary effect in the states where the Agreement applies. It furthermore has jurisdiction to hear cases against decisions of the European Patent Office regarding unitary patents. As a court of several member states of the European Union it may (Court of First Instance) or must (Court of Appeal) ask prejudicial questions to the European Court of Justice when the interpretation of EU law (including the two unitary patent regulations, but excluding the UPC Agreement) is not obvious.
Organization.
The court would have two divisions: a court of first instance and a court of appeal. The court of appeal and the registry would have their seats in Luxembourg, while the central division of the court of first instance would have its seat in Paris. The central division would have thematic branches in London and Munich. The court of first instance may further have local and regional divisions in all member states that wish to set up such divisions.
Geographical scope of and request for unitary effect.
While the regulations formally apply to all 26 member states participating in the enhanced cooperation for a unitary patent, from the date the UPC agreement has entered into force for the first group of ratifiers, unitary patents will only extend to the territory of those participating member states where the UPC Agreement had entered into force when the unitary effect was registered. If the unitary effect territory subsequently expands to additional participating member states for which the UPC Agreement later enters into force, this will be reflected for all subsequently registered unitary patents, but the territorial scope of the unitary effect of existing unitary patents will not be extended to these states.
Unitary effect can be requested up to one month after grant of the European patent, with retroactive effect from the date of grant. However, according to the "Draft Rules Relating to Unitary Patent Protection", unitary effect would be registered only if the European patent has been granted with the same set of claims for all the 26 participating member states in the regulations, whether the unitary effect applies to them or not. European patents automatically become a bundle of "national" European patents upon grant. Upon the grant of unitary effect, the "national" European patents will retroactively be considered to never have existed in the territories where the unitary patent has effect. The unitary effect does not affect "national" European patents in states where the unitary patent does not apply. Any "national" European patents applying outside the "unitary effect" zone will co-exist with the unitary patent.
Special territories of participating member states.
As the unitary patent is introduced by an EU regulation, it is expected to not only be valid in the mainland territory of the participating member states that are party to the UPC, but also in those of their special territories that are part of the European Union. As of April 2014, this includes the following fourteen territories:
In addition to the territories above, the European Patent Convention has been extended by three member states participating in the enhanced cooperation for a unitary patent to cover some of their dependent territories outside the European Union:
Among the dependencies in the second list, the Isle of Man intends to apply the unitary patent. Draft Dutch implementation provision provide for the "Dutch European Patent" to exist in parallel with the unitary patent: while the unitary patent will cover the European territory, the Dutch EP will be valid in Caribbean Netherlands, Curacao and Sint Maarten and be subject to regular translation and renewal requirements.
Costs.
The renewal fees of the unitary patent range from 32 Euro in the second year to 4855 in the 20th year as is based on the cumulative renewal fees of Germany, France, the UK and the Netherlands, the 4 states in which most European patents are in force.
Translation requirements as well as the requirement to pay yearly patent fees in all countries in which a European patent is designated, presently renders the European patent system costly in the European Union. In an impact assessment the European Commission estimated that the costs of obtaining a patent in all 27 EU countries would drop from over 32 000 euro (mainly due to translation costs) to 6 500 euro (for the combination of an EU, Spanish and Italian patent) due to introduction of the EU patent. Per capita costs of an EU patent were estimated at just 6 euro/million in the original 25 participating countries (and 12 euro/million in the 27 EU countries for protection with an EU, Italian and Spanish patent).
How the EU Commission has presented the expected cost savings has however been sharply criticized as exaggerated and based on unrealistic assumptions. The EU Commission has notably considered the costs for validating a European patent in 27 countries while in reality only about 1% of all granted European patents are currently validated in all 27 contracting states. Based on more realistic assumptions, the cost savings are expected to be much lower than actually claimed by the Commission.
Earlier attempts.
1970s and 1980s: proposed Community Patent Convention.
Work on a Community patent started in the 1970s, but the resulting Community Patent Convention (CPC) was a failure.
The "Luxembourg Conference on the Community Patent" took place in 1975 and the Convention for the European Patent for the common market, or (Luxembourg) Community Patent Convention (CPC), was signed at Luxembourg on 15 December 1975, by the 9 member states of the European Economic Community at that time. However, the CPC never entered into force. It was not ratified by enough countries.
Fourteen years later, the Agreement relating to Community patents was made at Luxembourg on 15 December 1989. It attempted to revive the CPC project, but also failed. This Agreement consisted of an amended version of the original Community Patent Convention. Twelve states signed the Agreement: Belgium, Denmark, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, and United Kingdom. All of those states would need to have ratified the Agreement to cause it to enter into force, but only seven did so: Denmark, France, Germany, Greece, Luxembourg, the Netherlands, and United Kingdom.
Nevertheless, a majority of member states of the EEC at that time introduced some harmonisation into their national patent laws in anticipation of the entry in force of the CPC. A more substantive harmonisation took place at around the same time to take account of the European Patent Convention and the Strasbourg Convention.
2000 to 2004: EU Regulation proposal.
In 2000, renewed efforts from the European Union resulted in a Community Patent Regulation proposal, sometimes abbreviated as CPR. It provides that the patent, once it has been granted by the European Patent Office (EPO) in one of its procedural languages (English, German or French) and published in that language, with a translation of the claims into the two other procedural languages, will be valid without any further translation. This proposal is aimed to achieve a considerable reduction in translation costs.
Nevertheless, additional translations could become necessary in legal proceedings against a suspected infringer. In such a situation, a suspected infringer who has been unable to consult the text of the patent in the official language of the Member State in which he is domiciled, is presumed, until proven otherwise, not to have knowingly infringed the patent. To protect a suspected infringer who, in such a situation, has not acted in a deliberate manner, it is provided that the proprietor of the patent will not be able to obtain damages in respect of the period prior to the translation of the patent being notified to the infringer.
The proposed Community Patent Regulation should also establish a court holding exclusive jurisdiction to invalidate issued patents; thus, a Community Patent's validity will be the same in all EU member states. This court will be attached to the present European Court of Justice and Court of First Instance through use of provisions in the Treaty of Nice.
Discussion regarding the Community patent had made clear progress in 2003 when a political agreement was reached on 3 March 2003. However, one year later in March 2004 under the Irish presidency, the Competitiveness Council failed to agree on the details of the Regulation. In particular the time delays for translating the claims and the authentic text of the claims in case of an infringement remained problematic issues throughout discussions and in the end proved insoluble.
In view of the difficulties in reaching an agreement on the community patent, other legal agreements have been proposed outside the European Union legal framework to reduce the cost of translation (of patents when granted) and litigation, namely the London Agreement, which entered into force on 1 May 2008—and which has reduced the number of countries requiring translation of European patents granted nowadays under the European Patent Convention, and the corresponding costs to obtain a European patent—and the European Patent Litigation Agreement (EPLA), a proposal that has now lapsed.
Reactions to the failure.
After the council in March 2004, EU Commissioner Frits Bolkestein said that "The failure to agree on the Community Patent I am afraid undermines the credibility of the whole enterprise to make Europe the most competitive economy in the world by 2010." Adding:
Jonathan Todd, Commission's Internal Market spokesman, declared:
European Commission President Romano Prodi, asked to evaluate his five-year term, cites as his weak point the failure of many EU governments to implement the "Lisbon Agenda", agreed in 2001. In particular, he cited the failure to agree on a Europewide patent, or even the languages to be used for such a patent, "because member states did not accept a change in the rules; they were not coherent".
Support for the Regulation.
There is support for the Community patent from various quarters. From the point of view of the European Commission the Community Patent is an essential step towards creating a level playing field for trade within the European Union. For smaller businesses, if the Community patent achieves its aim of providing a relatively inexpensive way of obtaining patent protection across a wide trading area, then there is also support. 
For larger businesses, however, other issues come into play, which have tended to dilute overall support. In general, these businesses recognise that the current European Patent system provides the best possible protection given the need to satisfy national sovereignty requirements such as regarding translation and enforcement. The Community Patent proposal was generally supported if it would do away with both of these issues, but there was some concern about the level of competence of the proposed European Patent Court. A business would be reluctant to obtain a Europe-wide patent if it ran the risk of being revoked by an inexperienced judge. Also, the question of translations would not go away – unless the users of the system could see significant change in the position of some of the countries holding out for more of a patent specification to be translated on grant or before enforcement, it was understood that larger businesses (the bulk of the users of the patent system) would be unlikely to move away from the tried and tested European Patent.
Since 2005: stalemate and new debate.
Thus, in 2005, the Community patent looked unlikely to be implemented in the near future. However, on 16 January 2006 the European Commission "launched a public consultation on how future action in patent policy to create an EU-wide system of protection can best take account of stakeholders' needs." The Community patent was one of the issues the consultation focused on. More than 2500 replies were received. According to the European Commission, the consultation showed that there is widespread support for the Community patent but not at any cost, and "in particular not on the basis of the Common Political Approach reached by EU Ministers in 2003".
In February 2007, EU Commissioner Charlie McCreevy was quoted as saying:
The European Commission released a white paper in April 2007 seeking to "improve the patent system in Europe and revitalise the debate on this issue." On 18 April 2007, at the European Patent Forum in Munich, Germany, Günter Verheugen, Vice-President of the European Commission, said that his proposal to support the European economy was "to have the London Agreement ratified by all member states, and to have a European patent judiciary set up, in order to achieve rapid implementation of the Community patent, which is indispensable". He further said that he believed this could be done within five years.
In October 2007, the Portuguese presidency of the Council of the European Union proposed an EU patent jurisdiction, "borrowing heavily from the rejected draft European Patent Litigation Agreement (EPLA)". In November 2007, EU ministers were reported to have made some progress towards a community patent legal system, with "some specific results" expected in 2008.
In 2008, the idea of using machine translations to translate patents was proposed to solve the language issue, which is partially responsible for blocking progress on the community patent. Meanwhile, European Commissioner for Enterprise and Industry Günter Verheugen declared at the European Patent Forum in May 2008 that there was an "urgent need" for a community patent.
Agreement in December 2009, and language issue.
In December 2009, it was reported that the Swedish EU presidency had achieved a breakthrough in negotiations concerning the community patent. The breakthrough was reported to involve setting up a single patent court for the EU, however ministers conceded much work remained to be done before the community patent would become a reality.
According to the agreed plan, the EU would accede to the European Patent Convention as a contracting state, and patents granted by the European Patent Office will, when validated for the EU, have unitary effect in the territory of the European Union. On 10 November 2010, it was announced that no agreement had been reached and that, "in spite of the progress made, Competitiveness Council of the European Union had fallen short of unanimity by a small margin," with commentators reporting that the Spanish representative, citing the aim to avoid any discrimination, had "re-iterated at length the stubborn rejection of the Madrid Government of taking the 'Munich' three languages regime (English, German, French) of the European Patent Convention (EPC) as a basis for a future EU Patent."

</doc>
<doc id="6762" url="https://en.wikipedia.org/wiki?curid=6762" title="Companies law">
Companies law

Company law (or the law of business associations) is the field of law concerning companies and other business organizations. This includes corporations, partnerships and other associations which usually carry on some form of economic or charitable activity. The most prominent kind of company, usually referred to as a "corporation", is a "juristic person", i.e. it has separate legal personality, and those who invest money into the business have limited liability for any losses the company makes, governed by corporate law. The largest companies are usually publicly listed on stock exchanges around the world. Even single individuals, also known as sole traders may incorporate themselves and limit their liability in order to carry on a business. All different forms of companies depend on the particular law of the particular country in which they reside.
The law of business organizations originally derived from the common law of England, but has evolved significantly in the 20th century. In common law countries today, the most commonly addressed forms are:
The proprietary limited company is a statutory business form in several countries, including Australia.
Many countries have forms of business entity unique to that country, although there are equivalents elsewhere. Examples are the limited liability company (LLC) and the limited liability limited partnership (LLLP) in the United States.
Other types of business organizations, such as cooperatives, credit unions and publicly owned enterprises, can be established with purposes that parallel, supersede, or even replace the profit maximization mandate of business corporations.
For a country-by-country listing of officially recognized forms of business organization, see Types of business entity.
There are various types of company that can be formed in different jurisdictions, but the most common forms of company are:
There are, however, many specific categories of corporations and other business organizations which may be formed in various countries and jurisdictions throughout the world.
United States company law.
In the United States, corporations are generally incorporated, or organized, under the laws of a particular state. The corporate law of a corporation's state of incorporation generally governs that corporation's internal governance (even if the corporation's operations take place outside of that state). The corporate laws of the various states differ - in some cases significantly - from state to state. Because of these differences, corporate lawyers are often consulted in an effort to determine the most appropriate or advantageous state in which to incorporate, and a majority of public companies in the U.S. are Delaware corporations. The federal laws of Nigeria and local law may also be applicable sources of corporate law.
Company law theory.
“A corporation is described to be a person in a political capacity created by the law, to endure in perpetual succession.” Americans in the 1790s knew of a variety of corporations established for various purposes, including those of commerce, education, and religion. As the law of corporations was articulated by the Supreme Court under Chief Justice Marshall, over the first several decades of the new American state, emphasis fell, in a way which seems natural to us today, upon commercial corporations. Nonetheless, Wilson believed that, in all cases, corporations “should be erected with caution, and inspected with care.” The actions of corporations were clearly circumscribed: “To every corporation a name must be assigned; and by that name alone it can perform legal acts.” For non-binding external actions or transactions, corporations enjoyed the same latitude as private individuals; but it was with an eye to internal affairs that many saw principal advantage in incorporation. The power of making by-laws was “tacitly annexed to corporations by the very act of their establishment.” While they must not directly contradict the overarching laws of the land, the central or local government cannot be expected to regulate toward the peculiar circumstances of a given body, and so “they are invested with authority to make regulations for the management of their own interests and affairs.”
The question then arises: if corporations are to be inspected with care, what - if not the commercial or social conduct, or the by-laws - is to be inspected – and by whom? Do corporations have duties? Yes: “The general duties of every corporation may be collected from the nature and design of its institution: it should act agreeably to its nature, and fulfill the purposes for which it was formed.” Who sees that corporations are living up to those duties? “The law has provided proper persons with proper powers to visit those institutions, and to correct every irregularity, which may arise within them.” The Common Law provided for inspection by the court of king’s bench. In 1790, at least, “the powers of the court of king's bench vested in the supreme court of Pennsylvania.” As for the dissolution of corporations, there seems not to have been much question that a corporation might “surrender its legal existence into the hands of that power, from which it was received. From such a surrender, the dissolution of the body corporate ensues.” Nor does there seem to have been much question that by “a judgment of forfeiture against a corporation itself, it may be dissolved.” However, Supreme Court Justice Wilson, lecturing in his unofficial capacity, at least, suggests not his displeasure with the doctrine that corporate dissolution cannot be predicated “by a judgment of ouster against individuals. God forbid ― such is the sentiment of Mr. Justice Wilmot ― that the rights of the body should be lost or destroyed by the offenses of the members.”
As theorists such as Ronald Coase have pointed out, all business organizations represent an attempt to avoid certain costs associated with doing business. Each is meant to facilitate the contribution of specific resources - investment capital, knowledge, relationships, and so forth - towards a venture which will prove profitable to all contributors. Except for the partnership, all business forms are designed to provide limited liability to both members of the organization and external investors. Business organizations originated with agency law, which permits an agent to act on behalf of a principal, in exchange for the principal assuming equal liability for the wrongful acts committed by the agent. For this reason, all partners in a typical general partnership may be held liable for the wrongs committed by one partner. Those forms that provide limited liability are able to do so because the state provides a mechanism by which businesses that follow certain guidelines will be able to escape the full liability imposed under agency law. The state provides these forms because it has an interest in the strength of the companies that provide jobs and services therein, but also has an interest in monitoring and regulating their behavior.
Company law study.
Law schools typically offer either a single upper level course on business organizations, or offer several courses covering different aspects of this area of law. The area of study examines issues such as how each major form of business entity may be formed, operated, and dissolved; the degree to which limited liability protects investors; the extent to which a business can be held liable for the acts of an agent of the business; the relative advantages and disadvantages of different types of business organizations, and the structures established by governments to monitor the buying and selling of ownership interests in large corporations.
The basic theory behind all business organizations is that, by combining certain functions within a single entity, a business (usually called a firm by economists) can operate more efficiently, and thereby realize a greater profit. Governments seek to facilitate investment in profitable operations by creating rules that protect investors in a business from being held personally liable for debts incurred by that business, either through mismanagement, or because of wrongful acts committed by the business.

</doc>
<doc id="6763" url="https://en.wikipedia.org/wiki?curid=6763" title="Cistron">
Cistron

A cistron is a gene. The term cistron is used to emphasize that genes exhibit a specific behavior in a cis-trans test; distinct positions (or loci) within a genome are cistronic.
For example, suppose a mutation at a chromosome position formula_1 is responsible for a recessive trait in a diploid organism (where chromosomes come in pairs). We say that the mutation is recessive because the organism will exhibit the wild type phenotype (ordinary trait) unless both chromosomes of a pair have the mutation (homozygous mutation). Similarly, suppose a mutation at another position, formula_2, is responsible for the same recessive trait. The positions formula_1 and formula_2 are said to be within the same cistron when an organism that has the mutation at formula_1 on one chromosome and has the mutation at position formula_2 on the paired chromosome exhibits the recessive trait even though the organism is not homozygous for either mutation. When instead the wild type trait is expressed, the positions are said to belong to distinct cistrons / genes.
For example, an operon is a stretch of DNA that is transcribed to create a contiguous segment of RNA, but contains more than one cistron / gene. The operon is said to be polycistronic, whereas ordinary genes are said to be monocistronic.

</doc>
<doc id="6766" url="https://en.wikipedia.org/wiki?curid=6766" title="Commonwealth">
Commonwealth

Commonwealth is a traditional English term for a political community founded for the common good. Historically it has sometimes been synonymous with "republic".
The English noun "commonwealth" in the sense meaning "public welfare; general good or advantage" dates from the 15th century. The original phrase "the common-wealth" or "the common weal" (echoed in the modern synonym "public weal") comes from the old meaning of "wealth", which is "well-being", and is itself a loose translation of the Latin res publica (republic). The term literally meant "common well-being". In the 17th century the definition of "commonwealth" expanded from its original sense of "public welfare" or "commonweal" to mean "a state in which the supreme power is vested in the people; a republic or democratic state". "Better things were done, and better managed ... under a Commonwealth than under a King." Pepys, "Diary" (1667) 
Three countries – Australia, the Bahamas, and Dominica – have the official title "Commonwealth", as do four US states and two US territories. More recently, the term has been used for fraternal associations of some sovereign nations, most notably the Commonwealth of Nations, an association primarily of former members of the British Empire, which is often referred to as simply "the Commonwealth".
Historical use.
Rome.
Translations of Roman writers' works to English have on occasion translated ""Res publica"", and variants thereof, to "the commonwealth", a term referring to the Roman state as a whole.
England.
The Commonwealth of England was the official name of the political unit ("de facto" military rule in the name of parliamentary supremacy) that replaced the Kingdom of England (after the English Civil War) from 1649–53 and 1659–60, under the rule of Oliver Cromwell and his son and successor Richard. From 1653 to 1659, although still legally known as a Commonwealth, the republic, united with the former Kingdom of Scotland, operated under different institutions (at times as a "de facto" monarchy) and is known by historians as the Protectorate. In a British context, it is sometimes referred to as the "Old Commonwealth".
Iceland.
The Icelandic Commonwealth or the Icelandic Free State (Icelandic: Þjóðveldið) was the state existing in Iceland between the establishment of the Althing in 930 and the pledge of fealty to the Norwegian king in 1262. It was initially established by a public consisting largely of recent immigrants from Norway who had fled the unification of that country under King Harald Fairhair.
Poland–Lithuania.
"Republic" is still an alternative translation of the traditional name of the Polish–Lithuanian Commonwealth. Wincenty Kadłubek (Vincent Kadlubo, 1160–1223) used for the first time the original Latin term "res publica" in the context of Poland in his "Chronicles of the Kings and Princes of Poland". The name was used officially for the confederal country formed by Poland and Lithuania 1569–1795.
It is also often referred as "Nobles' Commonwealth" (1505–1795, i.e., before the union). In contemporary political doctrine of Polish–Lithuanian Commonwealth, "our state is a Republic (Commonwealth) under presidency of the King". The commonwealth introduced a doctrine of religious tolerance called Warsaw Confederation, had its own parliament "Sejm" (although elections were restricted to the nobility and elected kings, who were bound to certain contracts "Pacta conventa" from the beginning of the reign).
"A commonwealth of good counsaile" was the title of the 1607 English translation of the work of Wawrzyniec Grzymała Goślicki "De optimo senatore" that presented to English readers many of the ideas present in the political system of the Polish–Lithuanian Commonwealth.
Current use.
National level.
Australia.
"Commonwealth" was first proposed as a term for a federation of the six Australian crown colonies at the 1891 constitutional convention in Sydney. Its adoption was initially controversial, as it was associated by some with the republicanism of Oliver Cromwell (see above), but it was retained in all subsequent drafts of the constitution. The term was finally incorporated into law in the "Commonwealth of Australia Constitution Act 1901", which established the federation. Australia operates under a federal system, in which power is divided between the federal (national) government and the state governments (the successors of the six colonies). So, in an Australian context, the term "Commonwealth" (capitalised) refers to the federal government, and "Commonwealth of Australia" is the official name of the country.
The Bahamas.
The Bahamas uses the official style "Commonwealth of The Bahamas".
Dominica.
The small Caribbean republic of Dominica has used the official style "Commonwealth of Dominica" since 1970.
Subnational level.
The term "commonwealth" has one of two political meanings within the United States:
U.S. states.
Four states in the United States officially designate themselves as "commonwealths". All four were original colonies or parts thereof (Kentucky was originally a part of the land grant of the Colony of Virginia) and share a strong influence of colonial common law in some of their laws and institutions. The four are:
U.S. insular areas.
"Commonwealth" is also used in the United States to describe the political relationship between the United States and the overseas unincorporated territories:
International bodies.
Commonwealth of Nations.
The Commonwealth of Nations—formerly the British Commonwealth—is a voluntary association of 53 independent sovereign states, most of which were once part of the British Empire. The Commonwealth's membership includes both republics and monarchies. The head of the Commonwealth of Nations is Queen Elizabeth II, who reigns as monarch directly in 16 member states known as Commonwealth realms.
Commonwealth of Independent States.
The Commonwealth of Independent States (CIS) is a loose alliance or confederation consisting of 10 of the 15 former Soviet Republics, the exceptions being Turkmenistan (a CIS associate member), Lithuania, Latvia, Estonia, and Georgia. Georgia left the CIS in August 2008 after a clash with Russia over South Ossetia. Its creation signalled the dissolution of the Soviet Union, its purpose being to "allow a civilised divorce" between the Soviet Republics. The CIS has developed as a forum by which the member-states can co-operate in economics, defence, and foreign policy.
Proposed use.
United Kingdom.
Labour MP Tony Benn sponsored a "Commonwealth of Britain Bill" several times between 1991 and 2001, intended to abolish the monarchy and establish a British republic. It never reached second reading.

</doc>
<doc id="6767" url="https://en.wikipedia.org/wiki?curid=6767" title="Commodore 1541">
Commodore 1541

The Commodore 1541 (also known as the CBM 1541 and VIC-1541) is a floppy disk drive which was made by Commodore International for the Commodore 64 (C64), Commodore's most popular home computer. The best-known floppy disk drive for the C64, the 1541 is a single-sided 170-kilobyte drive for 5¼" disks. The 1541 directly followed the Commodore 1540 (meant for the VIC-20).
The disk drive uses group code recording (GCR) and contains a MOS Technology 6502 microprocessor, doubling as a disk controller and on-board disk operating system processor. The number of sectors per track varies from 17 to 21 (an early implementation of zone bit recording). The drive's built-in disk operating system is CBM DOS 2.6.
History.
Introduction.
The 1541 was priced at under at its introduction. A C64 plus a 1541 cost about $900, while an Apple II with no disk drive cost $1295. The first 1541 drives produced in 1982 have a label on the front reading VIC-1541 and have an off-white case to match the VIC-20. In 1983, the 1541 was switched to having the familiar beige case and a front label reading simply "1541" along with rainbow stripes to match the Commodore 64.
By 1983 a 1541 sold for $300 or less, much less than the price of other computers' disk drives. After a brutal home-computer price war that Commodore began, the C64 and 1541 together cost under $500. The drive became very popular, and became difficult to find. The company claimed that the shortage occurred because 90% of C64 owners bought the 1541 compared to its 30% expectation, but the press discussed what "Creative Computing" described as "an absolutely alarming return rate" because of defects. The magazine reported in March 1984 that it received three defective drives in two weeks, and "Compute!'s Gazette" reported in December 1983 that four of the magazine's seven drives had failed; "COMPUTE! Publications sorely needs additional 1541s for in-house use, yet we can't find any to buy. After numerous phone calls over several days, we were able to locate only two units in the entire continental United States", reportedly because of Commodore's attempt to resolve a manufacturing issue that caused the high failures.
The early (1982–83) 1541s have a spring-eject mechanism (Alps drive), and the disks often fail to release. This style of drive has the popular nickname "Toaster Drive", because it requires the use of a knife or other hard thin object to pry out the stuck media just like a piece of toast stuck in an actual toaster (though this is inadvisable with actual toasters). This was fixed later when Commodore changed the vendor of the drive mechanism (Mitsumi) and adopted the flip-lever Newtronics mechanism, greatly improving reliability. In addition, Commodore made the drive's controller board smaller and reduced its chip count compared to the early 1541s (which had a large PCB running the length of the case, with dozens of TTL chips). The beige-case Newtronics 1541 was produced from 1984-86.
Versions and third-party clones.
All but the very earliest non-II model 1541s can use either the Alps or Newtronics mechanism. Visually, the first models, of the "-VIC-1541" denomination, have an off-white color like the VIC-20 and VIC-1540. Then, to match the look of the C64, CBM changed the drive's color to brown-beige and the name to "Commodore 1541".
The 1541's numerous shortcomings opened a market for a number of third-party clones of the disk drive, a situation that continued for the lifetime of the C64. Well-known clones are the "Oceanic OC-118" a.k.a. "Excelerator+", the MSD Super Disk single and dual drives, the "Enhancer 2000", the "Indus GT", and "CMD" 's "FD-2000" and "FD-4000". Nevertheless, the 1541 became the first disk drive to see widespread use in the home and Commodore sold millions of the units.
In 1986, Commodore released the 1541C, a revised version that offered quieter and slightly more reliable operation and a light beige case matching the color scheme of the Commodore 64C. It was replaced in 1988 by the 1541-II, which uses an external power supply to provide cooler operation and allows the drive to have a smaller desktop footprint (the power supply "brick" being placed elsewhere, typically on the floor). Later ROM revisions fixed assorted problems, including a software bug that made the save-and-replace command unusable.
Successors.
The Commodore 1570 is an upgrade from the 1541 for use with the Commodore 128, available in Europe. It offers MFM capability for accessing CP/M disks, improved speed, and somewhat quieter operation, but was only manufactured until Commodore got its production lines going with the 1571, the double-sided drive. Finally, the small, external-power-supply-based, MFM-based Commodore 1581 3½" drive was made, giving 800 KB access to the C128 and C64. By this time, however, many CBM users had shifted their attention to the 16/32-bit Amiga, and the 1581 was mostly sold to remaining GEOS users.
Design.
Hardware.
The 1541 does not have dip switches to change the device number. If a user added more than one drive to a system the user had to open the case and cut a trace in the circuit board to permanently change the drive's device number, or hand-wire an external switch to allow it to be changed externally. It was also possible to change the number temporarily from the operating system.
The pre-II 1541s also have an internal power source, which generate much heat. The heat generation was a frequent source of humour. For example, "Compute!" stated in 1988 that "Commodore 64s used to be a favorite with amateur and professional chefs since they could compute and cook on top of their 1500-series disk drives at the same time". A series of humorous tips in "MikroBitti" in 1989 said "When programming late, coffee and kebab keep nicely warm on top of the 1541." The "MikroBitti" review of the 1541-II said that its external power source "should end the jokes about toasters".
The drive-head mechanism installed in the early production years is notoriously easy to misalign. The most common cause of the 1541's drive head knocking and subsequent misalignment is copy-protection schemes on commercial software. The main cause of the problem is that the disk drive itself does not feature any means of detecting when the read/write head reaches track zero. Accordingly, when a disk is formatted or a disk error occurs, the unit tries to move the head 40 times in the direction of track zero (although the 1541 DOS only uses 35 tracks, the drive mechanism itself is a 40-track unit, so this ensured track zero would be reached no matter where the head was before). Once track zero is reached, every further attempt to move the head in that direction would cause it to be rammed against a solid stop: for example, if the head happened to be on track 18 (where the directory is located) before this procedure, the head would be actually moved 18 times, and then rammed against the stop 22 times. This ramming gives the characteristic "machine gun" noise and sooner or later throws the head out of alignment.
A defective head-alignment part likely caused many of the reliability issues in early 1541 drives; one dealer told "Compute!s Gazette" in 1983 that the part had caused all but three of several hundred drive failures that he had repaired. The drives were so unreliable that "Info" magazine joked, "Sometimes it seems as if one of the original design specs ... must have said 'Mean time between failure: 10 accesses.'". Users can realign the drive themselves with a software program and a calibration disk. What the user would do is remove the drive from its case and then loosen the screws holding the stepper motor that moved the head, then with the calibration disk in the drive gently turn the stepper motor back and forth until the program shows a good alignment. The screws are then tightened and the drive is put back into its case.
A third-party fix for the 1541 appeared where the solid head stop was replaced by a sprung stop, giving the head a much easier life. The later 1571 drive (which is 1541-compatible) incorporates track-zero detection by photo-interrupter and is thus immune to the problem. Also, a software solution, which resides in the drive controller's ROM, prevents the rereads from occurring, though this could cause problems when genuine errors did occur.
Interface.
The 1541 uses a proprietary serialized derivative of the IEEE-488 parallel interface, which Commodore used on their previous disk drives for the PET/CBM range of personal and business computers. To ensure a ready supply of inexpensive cabling for its home computer peripherals, Commodore chose standard DIN connectors for the serial interface. Disk drives and other peripherals such as printers connected to the computer via a daisy chain setup, necessitating only a single connector on the computer itself.
Throughput and software.
Initially, Commodore intended to use a hardware shift register (one component of the 6522 VIA) to maintain relatively brisk drive speeds with the new serial interface. However, a hardware bug with this chip prevented the initial design from working as anticipated, and the ROM code was hastily rewritten to handle the entire operation in software. According to Jim Butterfield, this causes a speed reduction by a factor of five.
As implemented on the VIC-20 and Commodore 64, CBM DOS transfers only about 300 bytes per second - compare the 300-baud data rate of the Commodore Datasette storage system - which translates to about 20 minutes to copy one disk—10 minutes of reading time, and 10 minutes of writing time. However, since both the computer and the drive can easily be reprogrammed, third parties quickly wrote more efficient firmware that would speed up drive operations drastically. Without hardware modifications, some "fast loader" utilities managed to achieve speeds of up to 4 kB/s. The most common of these products are the Epyx FastLoad, the Final Cartridge, and the Action Replay plug-in ROM cartridges, which all have machine code monitor and disk editor software on board as well. The popular Commodore computer magazines of the era also entered the arena with type-in fast-load utilities, with "Compute!'s Gazette" publishing "TurboDisk" in 1985 and "RUN" publishing "Sizzle" in 1987.
Even though each 1541 has its own on-board disk controller and disk operating system, it is not normally possible for a user to command two 1541 drives to copy a disk (one drive reading and the other writing) as with older dual drives like the 4040 and 8050 that were often found with the PET computer, and which the 1541 is backward-compatible with (it can read 4040 disks but not write to them since its internal operating system is similar enough for reading but not for writing). Unfortunately, however, the routines in the previous disk operating system to enable disk copying were removed for the 1541 as it was intended to be a stand-alone unit. Originally, to copy from drive to drive, software running on the C64 was needed and it would first read from one drive into computer memory, then write out to the other. Only later when first, Fast Hack'em, then other disk backup programs, were released, was true drive-to-drive copying possible for a pair of 1541s. The user could then unplug the C64 from the drives (i.e. from the first drive in the daisy chain) and do something else with the computer as the drives proceeded to copy the entire disk. This is not a recommended practice as disconnecting the serial lead from a powered drive and/or computer can result in destruction of one or both of the port chips in the disk drive.
Media.
Each side of 170 kB is split into 683 sectors on 35 tracks, each of the sectors holding 256 bytes; the file system made each sector individually rewritable.
However, one track is reserved by DOS for directory and file allocation information (the BAM, block availability map). And since for normal files, two bytes of each physical sector are used by DOS as a pointer to the next physical track and sector of the file, only 254 out of the 256 bytes of a block are used for file contents.
If the disk side was not otherwise prepared with a custom format, (e.g. for data disks), 664 blocks would be free after formatting, giving 664 × 254 = 168,656 bytes (or almost 165 kB) for user data.
By using custom formatting and load/save routines (sometimes included in third-party DOSes, see below), all of the mechanically possible 40 tracks can be used. The reason Commodore decided not to use the upper five tracks by default (or at least more than 35) was the bad quality of some of the drive mechanisms which did not always work reliably at the highest tracks. So by reducing the number of tracks used and thus capacity, it was possible to further reduce cost - in contrast to single-density drives used e.g. in IBM PC computers of the day which save 180 kB on one side (by using a 40-track format). The 1983 Apple FileWare minifloppy drives use double-sided media, higher track pitch, and variable motor speed to achieve a storage capacity of 871 kB, or 435 kB per side.
The 1541 does not have an index hole sensor, making it straightforward to use the reverse side of a disk by flipping it. A disk can be converted to a "flippy disk" by simply cutting/punching a notch on the left-hand side, causing the drive to recognize both sides as writable. This would effectively double the storage capacity. The notch can be made with scissors, a knife, hole punch, or a disk notcher tool that is specifically designed for this task. Most soft-sectored and all hard-sectored drives would have also required an extra cut-out for the index hole — a harder modification.
Tracks 36-42 are non-standard. The bitrate is the raw one between the read/write head and signal circuitry so actual useful data rate is a factor 5/4 less due to GCR encoding.
The 1541 disk typically has 35 tracks. Track 18 is reserved; the remaining tracks are available for data storage. The header is on 18/0 (track 18, sector 0) along with the BAM (block availability map), and the directory starts on 18/1 (track 18, sector 1). The file interleave is 10 blocks, while the directory interleave is 3 blocks.
Header contents: The header is similar to other Commodore disk headers, the structural differences being the BAM offset ($04) and size, and the label+ID+type offset ($90).
Uses.
Early copy protection schemes deliberately introduced read errors on the disk, the software refusing to load unless the correct error message is returned. The general idea was that simple disk-copy programs are incapable of copying the errors. When one of these errors is encountered, the disk drive (as do many floppy disk drives) will attempt one or more reread attempts after first resetting the head to track zero. Few of these schemes had much deterrent effect, as various software companies soon released "nibbler" utilities that enabled protected disks to be copied and, in some cases, the protection removed.
Commodore copy protection sometimes depends on specific hardware configurations. "Gunship", for example, does not load if a second disk drive or printer is connected to the computer.

</doc>
<doc id="6769" url="https://en.wikipedia.org/wiki?curid=6769" title="Commodore 1581">
Commodore 1581

The Commodore 1581 is a 3½-inch double-sided double-density floppy disk drive that was first made by Commodore Business Machines (CBM) in 1987, primarily for its C64 and C128 home/personal computers. The drive stores 800 kilobytes using an MFM encoding but format different from MS-DOS (720 kB), Amiga (880 kB), and Mac Plus (800 kB) formats. With special software it's possible to read C1581 disks on an x86 PC system, and likewise, read MS-DOS and other formats of disks in the C1581 (using Big Blue Reader), provided that the PC or other floppy handles the size format. This capability was most frequently used to read MS-DOS disks. The drive was released in the summer of 1987 and quickly became popular with bulletin board system (BBS) operators and other users.
Like the 1541 and 1571, the 1581 has an onboard MOS Technology 6502 CPU with its own ROM and RAM, and uses a serial version of the IEEE-488 interface. Inexplicably, the drive's ROM contains commands for parallel use, although no parallel interface was available. Unlike the 1571, which is nearly 100% backward-compatible with the 1541, the 1581 has limited compatibility with Commodore's earlier drives. Although it responds to the same DOS commands, most disk utilities written prior to 1987—most notably fast loaders—are so 1541-specific that they do not work with the 1581.
The version of Commodore DOS built into the 1581 added support for partitions, which could also function as fixed-allocation subdirectories. PC-style subdirectories were rejected as being too difficult to work with in terms of block availability maps, then still much in vogue, and which for some time had been the traditional way of inquiring into block availability. When used together with the C128, it implements faster burst mode access than the Commodore 1571 5¼" drive. When using the 1581 together with the C64, however, it is almost as slow as the 1541 drive, due to limitations of the C64's ROM code. The 1581 provides a total of 3160 blocks free when formatted (a block being equal to 256 bytes). The number of permitted directory entries was also increased, to 296 entries. With a storage capacity of 800 kB, the 1581 is the highest-capacity serial-bus drive that was ever made by Commodore (the 1-MB SFD-1001 uses the parallel IEEE-488), and the only 3½" one. However, starting in 1991, Creative Micro Designs (CMD) made the FD-2000 high density (1.6 MB) and FD-4000 extended density (3.2 MB) 3½" drives, both of which offered not only a 1581-emulation mode but also 1541- and 1571-compatibility modes.
Like the 1541 and 1571, a nearly identical job queue is available to the user in zero page (except for job 0), providing for exceptional degrees of compatibility.
Unlike the cases of the 1541 and 1571, the low-level disk format used by the 1581 is similar enough to the MS-DOS format as the 1581 is built around a WD1770 FM/MFM floppy controller chip. PC floppy controllers directly connected via the ISA-bus or onboard, but not standalone USB floppy drives, are able to deal with the 1581 format without need for any special tricks. Thus, utilities to format, read, and write 1581-format disks in standard PC floppy drives under Linux and Microsoft Windows exist. This controller chip, however, was the seat of some early problems with 1581 drives when the first production runs were recalled due to a high failure rate; the problem was quickly corrected. Later versions of the 1581 drive have a smaller, more streamlined-looking external power supply provided with them.
Specifications.
1581 Image Layout.
The 1581 disk has 80 logical tracks, each with 40 logical sectors (the actual physical layout of the diskette is abstracted and managed by a hardware translation layer). The directory starts on 40/3 (track 40, sector 3). The disk header is on 40/0, and the BAM (block availability map) resides on 40/1 and 40/2.
Header Contents
BAM Contents, 40/1
BAM Contents, 40/2

</doc>
<doc id="6771" url="https://en.wikipedia.org/wiki?curid=6771" title="College football">
College football

College football is American football played by teams of student athletes fielded by American universities, colleges, and military academies, or Canadian football played by teams of student athletes fielded by Canadian universities. It was through college football play that American football rules first gained popularity in the United States. No minor league farm organizations exist in American football. Therefore, college football is generally considered to be the second tier of American football in the United States; one step ahead of high school competition, and one step below professional competition. It is in college football where a player's performance directly impacts his chances of playing professional football. The best collegiate players will typically declare for the professional draft after 3–4 years of collegiate competition; with the NFL holding its annual draft every spring. 255 players are selected annually. Those not selected can still attempt to land an NFL roster spot as an undrafted free agent.
History.
Even after the emergence of the professional National Football League (NFL), college football remained extremely popular throughout the U.S. 
Although the college game has a much larger margin for talent than its pro counterpart, the sheer number of fans following major colleges provides a financial equalizer for the game, with Division I programs – the highest level – playing in huge stadiums, six of which have seating capacity exceeding 100,000. In many cases, college stadiums employ bench-style seating, as opposed to individual seats with backs and arm rests. This allows them to seat more fans in a given amount of space than the typical professional stadium, which tends to have more features and comforts for fans. (Only two stadiums owned by U.S. colleges or universities – Papa John's Cardinal Stadium at the University of Louisville and FAU Stadium at Florida Atlantic University – consist entirely of chairback seating.)
College athletes, unlike players in the NFL, are not permitted by the NCAA to be paid salaries. Colleges are only allowed to provide non-monetary compensation such as athletic scholarships that provide for tuition, housing, and books.
Rugby Football in England and Canada.
Modern North American football has its origins in various games, all known as "football", played at public schools in England in the mid-19th century. By the 1840s, students at Rugby School were playing a game in which players were able to pick up the ball and run with it, a sport later known as Rugby football. The game was taken to Canada by British soldiers stationed there and was soon being played at Canadian colleges.
The first documented gridiron football match was a game played at University College, a college of the University of Toronto, November 9, 1861. One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.
In 1864, at Trinity College, also a college of the University of Toronto, F. Barlow Cumberland and Frederick A. Bethune devised rules based on rugby football. Modern Canadian football is widely regarded as having originated with a game played in Montreal, in 1865, when British Army officers played local civilians. The game gradually gained a following, and the Montreal Football Club was formed in 1868, the first recorded non-university football club in Canada.
American college football.
Early games appear to have had much in common with the traditional "mob football" played in England. The games remained largely unorganized until the 19th century, when intramural games of football began to be played on college campuses. Each school played its own variety of football. Princeton University students played a game called "ballown" as early as 1820. A Harvard tradition known as "Bloody Monday" began in 1827, which consisted of a mass ballgame between the freshman and sophomore classes. In 1860, both the town police and the college authorities agreed the Bloody Monday had to go. The Harvard students responded by going into mourning for a mock figure called "Football Fightum", for whom they conducted funeral rites. The authorities held firm and it was a dozen years before football was once again played at Harvard. Dartmouth played its own version called "Old division football", the rules of which were first published in 1871, though the game dates to at least the 1830s. All of these games, and others, shared certain commonalities. They remained largely "mob" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common. The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale, under pressure from the city of New Haven, banned the play of all forms of football in 1860.
American football historian Parke H. Davis described the period between 1869 and 1875 as the 'Pioneer Period'; the years 1876–93 he called the 'Period of the American Intercollegiate Football Association'; and the years 1894–1933 he dubbed the 'Period of Rules Committees and Conferences'.
Early games.
On November 6, 1869, Rutgers University faced Princeton University (then known as the College of New Jersey) in the first-ever game of intercollegiate football. It was played with a round ball and, like all early games, used a set of rules suggested by Rutgers captain William J. Leggett, based on the Football Association's first set of rules, which were an early attempt by the former pupils of England's public schools, to unify the rules of their public schools games and create a universal and standardized set of rules for the game of football and bore little resemblance to the American game which would be developed in the following decades. It is still usually regarded as the first game of college football. The game was played at a Rutgers field. Two teams of 25 players attempted to score by kicking the ball into the opposing team's goal. Throwing or carrying the ball was not allowed, but there was plenty of physical contact between players. The first team to reach six goals was declared the winner. Rutgers won by a score of six to four. A rematch was played at Princeton a week later under Princeton's own set of rules (one notable difference was the awarding of a "free kick" to any player that caught the ball on the fly, which was a feature adopted from the Football Association's rules; the fair catch kick rule has survived through to modern American game). Princeton won that game by a score of 8 - 0. Columbia joined the series in 1870, and by 1872 several schools were fielding intercollegiate teams, including Yale and Stevens Institute of Technology.
Columbia University was the third school to field a team. The Lions traveled from New York City to New Brunswick on November 12, 1870 and were defeated by Rutgers 6 to 3. The game suffered from disorganization and the players kicked and battled each other as much as the ball. Later in 1870, Princeton and Rutgers played again with Princeton defeating Rutgers 6-0. This game's violence caused such an outcry that no games at all were played in 1871. Football came back in 1872, when Columbia played Yale for the first time. The Yale team was coached and captained by David Schley Schaff, who had learned to play football while attending Rugby school. Schaff himself was injured and unable to the play the game, but Yale won the game 3-0 nonetheless. Later in 1872, Stevens Tech became the fifth school to field a team. Stevens lost to Columbia, but beat both New York University and City College of New York during the following year.
By 1873, the college students playing football had made significant efforts to standardize their fledgling game. Teams had been scaled down from 25 players to 20. The only way to score was still to bat or kick the ball through the opposing team's goal, and the game was played in two 45 minute halves on fields 140 yards long and 70 yards wide. On October 20, 1873, representatives from Yale, Columbia, Princeton, and Rutgers met at the Fifth Avenue Hotel in New York City to codify the first set of intercollegiate football rules. Before this meeting, each school had its own set of rules and games were usually played using the home team's own particular code. At this meeting, a list of rules, based more on the Football Association's rules than the rules of the recently founded Rugby Football Union, was drawn up for intercollegiate football games.
Harvard–McGill (1874).
Old "Football Fightum" had been resurrected at Harvard in 1872, when Harvard resumed playing football. Harvard, however, preferred to play a rougher version of football called "the Boston Game" in which the kicking of a round ball was the most prominent feature though a player could run with the ball, pass it, or dribble it (known as "babying"). The man with the ball could be tackled, although hitting, tripping, "hacking" (shin-kicking) and other unnecessary roughness was prohibited. There was no limit to the number of players, but there were typically ten to fifteen per side. A player could carry the ball only when being pursued.
As a result of this, Harvard refused to attend the rules conference organized by Rutgers, Princeton and Columbia at the Fifth Avenue Hotel in New York City on October 20, 1873 to agree on a set of rules and regulations that would allow them to play a form of football that was essentially Association football; and continued to play under its own code. While Harvard's voluntary absence from the meeting made it hard for them to schedule games against other American universities, it agreed to a challenge to play the rugby team of McGill University, from Montreal, in a two-game series. It was agreed that two games would be played on Harvard's Jarvis baseball field in Cambridge, Massachusetts on May 14 and 15, 1874: one to be played under Harvard rules, another under the stricter rugby regulations of McGill. Jarvis Field was at the time a patch of land at the northern point of the Harvard campus, bordered by Everett and Jarvis Streets to the north and south, and Oxford Street and Massachusetts Avenue to the east and west. The game was won by Tufts 1-0 Harvard beat McGill in the "Boston Game" on the Thursday and held McGill to a 0-0 tie on the Friday. The Harvard students took to the rugby rules and adopted them as their own, The games featured a round ball instead of a rugby-style oblong ball. This series of games represents an important milestone in the development of the modern game of American football. In October 1874, the Harvard team once again traveled to Montreal to play McGill in rugby, where they won by three tries.
Inasmuch as Rugby football had been transplanted to Canada from England, the McGill team played under a set of rules which allowed a player to pick up the ball and run with it whenever he wished. Another rule, unique to McGill, was to count tries (the act of grounding the football past the opposing team's goal line; it is important to note that there was no end zone during this time), as well as goals, in the scoring. In the Rugby rules of the time, a try only provided the attempt to kick a free goal from the field. If the kick was missed, the try did not score any points itself.
Harvard–Tufts, Harvard–Yale (1875).
Harvard quickly took a liking to the rugby game, and its use of the try which, until that time, was not used in American football. The try would later evolve into the score known as the touchdown. On June 4, 1875, Harvard faced Tufts University in the first game between two American colleges played under rules similar to the McGill/Harvard contest, which was won by Tufts. The rules included each side fielding 11 men at any given time, the ball was advanced by kicking or carrying it, and tackles of the ball carrier stopped play.
Further elated by the excitement of McGill's version of football, Harvard challenged its closest rival, Yale, to which the Bulldogs accepted. The two teams agreed to play under a set of rules called the "Concessionary Rules", which involved Harvard conceding something to Yale's soccer and Yale conceding a great deal to Harvard's rugby. They decided to play with 15 players on each team. On November 13, 1875, Yale and Harvard played each other for the first time ever, where Harvard won 4-0. At the first The Game (as the annual contest between Harvard and Yale came to be named) the future "father of American football" Walter Camp was among the 2000 spectators in attendance. Walter, who would enroll at Yale the next year, was torn between an admiration for Harvard's style of play and the misery of the Yale defeat, and became determined to avenge Yale's defeat. Spectators from Princeton also carried the game back home, where it quickly became the most popular version of football.
On November 23, 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit House in Springfield, Massachusetts to standardize a new code of rules based on the rugby game first introduced to Harvard by McGill University in 1874. Three of the schools—Harvard, Columbia, and Princeton—formed the Intercollegiate Football Association, as a result of the meeting. Yale initially refused to join this association because of a disagreement over the number of players to be allowed per team (relenting in 1879) and Rutgers were not invited to the meeting. The rules that they agreed upon were essentially those of rugby union at the time with the exception that points be awarded for scoring a try, not just the conversion afterwards (extra point). Incidentally, rugby was to make a similar change to its scoring system 10 years later.
Walter Camp: Father of American football.
Walter Camp is widely considered to be the most important figure in the development of American football. As a youth, he excelled in sports like track, baseball, and association football, and after enrolling at Yale in 1876, he earned varsity honors in every sport the school offered.
Following the introduction of rugby-style rules to American football, Camp became a fixture at the Massasoit House conventions where rules were debated and changed. Dissatisfied with what seemed to him to be a disorganized mob, he proposed his first rule change at the first meeting he attended in 1878: a reduction from fifteen players to eleven. The motion was rejected at that time but passed in 1880. The effect was to open up the game and emphasize speed over strength. Camp's most famous change, the establishment of the line of scrimmage and the snap from center to quarterback, was also passed in 1880. Originally, the snap was executed with the foot of the center. Later changes made it possible to snap the ball with the hands, either through the air or by a direct hand-to-hand pass. Rugby league followed Camp's example, and in 1906 introduced the play-the-ball rule, which greatly resembled Camp's early scrimmage and center-snap rules. In 1966, Rugby league introduced a four-tackle rule based on Camp's early down-and-distance rules.
Camp's new scrimmage rules revolutionized the game, though not always as intended. Princeton, in particular, used scrimmage play to slow the game, making incremental progress towards the end zone during each down. Rather than increase scoring, which had been Camp's original intent, the rule was exploited to maintain control of the ball for the entire game, resulting in slow, unexciting contests. At the 1882 rules meeting, Camp proposed that a team be required to advance the ball a minimum of five yards within three downs. These down-and-distance rules, combined with the establishment of the line of scrimmage, transformed the game from a variation of rugby football into the distinct sport of American football.
Camp was central to several more significant rule changes that came to define American football. In 1881, the field was reduced in size to its modern dimensions of 120 by 53 yards (109.7 by 48.8 meters). Several times in 1883, Camp tinkered with the scoring rules, finally arriving at four points for a touchdown, two points for kicks after touchdowns, two points for safeties, and five for field goals. Camp's innovations in the area of point scoring influenced rugby union's move to point scoring in 1890. In 1887, game time was set at two halves of 45 minutes each. Also in 1887, two paid officials—a referee and an umpire—were mandated for each game. A year later, the rules were changed to allow tackling below the waist, and in 1889, the officials were given whistles and stopwatches.
After leaving Yale in 1882, Camp was employed by the New Haven Clock Company until his death in 1925. Though no longer a player, he remained a fixture at annual rules meetings for most of his life, and he personally selected an annual All-American team every year from 1889 through 1924. The Walter Camp Football Foundation continues to select All-American teams in his honor.
Expansion.
College football expanded greatly during the last two decades of the 19th century. Several major rivalries date from this time period.
November 1890 was an active time in the sport. In Baldwin City, Kansas, on November 22, 1890, college football was first played in the state of Kansas. Baker beat Kansas 22–9. On the 27th, Vanderbilt played Nashville (Peabody) at Athletic Park and won 40–0. It was the first time organized football played in the state of Tennessee. The 29th also saw the first instance of the Army–Navy Game. Navy won 24–0.
East.
Rutgers was first to extend the reach of the game. An intercollegiate game was first played in the state of New York when Rutgers played Columbia on November 2, 1872. It was also the first scoreless tie in the history of the fledgling sport. Yale football starts the same year and has its first match against Columbia, the nearest college to play football. It took place at Hamilton Park in New Haven and was the first game in New England. The game was essentially soccer with 20-man sides, played on a field 400 by 250 feet. Yale wins 3-0, Tommy Sherman scoring the first goal and Lew Irwin the other two.
After the first game against Harvard, Tufts took its squad to Bates College in Lewiston, Maine for the first football game played in Maine. This occurred on November 6, 1875.
Penn's Athletic Association was looking to pick "a twenty" to play a game of football against Columbia. This "twenty" never played Columbia, but did play twice against Princeton. Princeton won both games 6 to 0. The first of these happened on November 11, 1876 in Philadelphia and was the first intercollegiate game in the state of Pennsylvania.
Brown enters the intercollegiate game in 1878.
The first game where one team scored over 100 points happened on October 25, 1884 when Yale routed Dartmouth 113–0. It was also the first time one team scored over 100 points and the opposing team was shut out. The next week, Princeton outscored Lafayette by 140 to 0.
The first intercollegiate game in the state of Vermont happened on November 6, 1886 between Dartmouth and Vermont at Burlington, Vermont. Dartmouth won 91 to 0.
The first nighttime football game was played in Mansfield, Pennsylvania on September 28, 1892 between Mansfield State Normal and Wyoming Seminary and ended at halftime in a 0–0 tie. The Army-Navy game of 1893 saw the first documented use of a football helmet by a player in a game. Joseph M. Reeves had a crude leather helmet made by a shoemaker in Annapolis and wore it in the game after being warned by his doctor that he risked death if he continued to play football after suffering an earlier kick to the head.
Mid West.
In 1879, the University of Michigan became the first school west of Pennsylvania to establish a college football team. On May 30, 1879 Michigan beat Racine College 1–0 in a game played in Chicago. The "Chicago Daily Tribune" called it "the first rugby-football game to be played west of the Alleghenies." Other Midwestern schools soon followed suit, including the University of Chicago, Northwestern University, and the University of Minnesota. The first western team to travel east was the 1881 Michigan team, which played at Harvard, Yale and Princeton. The nation's first college football league, the Intercollegiate Conference of Faculty Representatives (also known as the Western Conference), a precursor to the Big Ten Conference, was founded in 1895.
Led by coach Fielding H. Yost, Michigan became the first "western" national power. From 1901 to 1905, Michigan had a 56-game undefeated streak that included a 1902 trip to play in the first college football bowl game, which later became the Rose Bowl Game. During this streak, Michigan scored 2,831 points while allowing only 40.
Organized intercollegiate football was first played in the state of Minnesota on September 30, 1882 when Hamline was convinced to play Minnesota. Minnesota won 2 to 0. It was the first game west of the Mississippi River.
November 30, 1905, saw Chicago defeat Michigan 2 to 0. Dubbed "The First Greatest Game of the Century," broke Michigan's 56-game unbeaten streak and marked the end of the "Point-a-Minute" years.
South.
Organized intercollegiate football was first played in the state of Virginia and the south on November 2, 1873 in Lexington between Washington and Lee and VMI. Washington and Lee won 4–2. Some industrious students of the two schools organized a game for October 23, 1869 – but it was rained out. Students of the University of Virginia were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim it organized a game against Washington and Lee College in 1871; but no record has been found of the score of this contest. Due to scantness of records of the prior matches some will claim Virginia v. Pantops Academy November 13, 1887 as the first game in Virginia.
On April 9, 1880 at Stoll Field, Transylvania University (then called Kentucky University) beat Centre College by the score of 13¾–0 in what is often considered the first recorded game played in the South. The first game of "scientific football" in the South was the first instance of the Victory Bell rivalry between North Carolina and Duke (then known as Trinity College) held on Thanksgiving Day, 1888, at the North Carolina State Fairgrounds in Raleigh, North Carolina.
On November 13, 1887 the Virginia Cavaliers and Pantops Academy fought to a scoreless tie in the first organized football game in the state of Virginia. Students at UVA were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim that some industrious ones organized a game against Washington and Lee College in 1871, just two years after Rutgers and Princeton's historic first game in 1869. But no record has been found of the score of this contest. Washington and Lee also claims a 4 to 2 win over VMI in 1873.
On October 18, 1888, the Wake Forest Demon Deacons defeated the North Carolina Tar Heels 6 to 4 in the first intercollegiate game in the state of North Carolina.
On December 14, 1889, Wofford defeated Furman 5 to 1 in the first intercollegiate game in the state of South Carolina. The game featured no uniforms, no positions, and the rules were formulated before the game.
January 30, 1892 saw the first football game played in the Deep South when the Georgia Bulldogs defeated Mercer 50–0 at Herty Field.
The beginnings of the contemporary Southeastern Conference and Atlantic Coast Conference start in 1894. The Southern Intercollegiate Athletic Association (SIAA) was founded on December 21, 1894, by Dr. William Dudley, a chemistry professor at Vanderbilt. The original members were Alabama, Auburn, Georgia, Georgia Tech, North Carolina, , and Vanderbilt. Clemson, Cumberland, Kentucky, LSU, Mercer, Mississippi, Mississippi A&M (Mississippi State), Southwestern Presbyterian University, Tennessee, Texas, Tulane, and the University of Nashville joined the following year in 1895 as invited charter members. The conference was originally formed for "the development and purification of college athletics throughout the South".
It is thought that the first forward pass in football occurred on October 26, 1895 in a game between Georgia and North Carolina when, out of desperation, the ball was thrown by the North Carolina back Joel Whitaker instead of punted and George Stephens caught the ball. On November 9, 1895 John Heisman executed a hidden ball trick utilizing quarterback Reynolds Tichenor to get Auburn's only touchdown in a 6 to 9 loss to Vanderbilt. It was the first game in the south decided by a field goal. Heisman later used the trick against Pop Warner's Georgia team. Warner picked up the trick and later used it at Cornell against Penn State in 1897. He then used it in 1903 at Carlisle against Harvard and garnered national attention.
The 1899 Sewanee Tigers are one of the all-time great teams of the early sport. The team went 12–0, outscoring opponents 322 to 10. Known as the "Iron Men", with just 13 men they had a six-day road trip with five shutout wins over Texas A&M; Texas; Tulane; LSU; and Ole Miss. It is recalled memorably with the phrase "... and on the seventh day they rested." Grantland Rice called them "the most durable football team I ever saw."
Organized intercollegiate football was first played in the state of Florida in 1901. A 7-game series between intramural teams from Stetson and Forbes occurred in 1894. The first intercollegiate game between official varsity teams was played on November 22, 1901. Stetson beat Florida Agricultural College at Lake City, one of the four forerunners of the University of Florida, 6-0, in a game played as part of the Jacksonville Fair.
On September 27, 1902, Georgetown beat Navy 4 to 0. It is claimed by Georgetown authorities as the game with the first ever "roving center" or linebacker when Percy Given stood up, in contrast to the usual tale of Germany Schulz. The first linebacker in the South is often considered to be Frank Juhan.
On Thanksgiving Day 1903, a game was scheduled in Montgomery, Alabama between the best teams from each region of the Southern Intercollegiate Athletic Association for an "SIAA championship game", pitting Cumberland against Heisman's Clemson. The game ended in an 11–11 tie causing many teams to claim the title. Heisman pressed hardest for Cumberland to get the claim of champion. It was his last game as Clemson head coach.
1904 saw big coaching hires in the south: Mike Donahue at Auburn, John Heisman at Georgia Tech, and Dan McGugin at Vanderbilt were all hired that year. Both Donahue and McGugin just came from the north that year, Donahue from Yale and McGugin from Michigan, and were among the initial inductees of the College Football Hall of Fame. The undefeated 1904 Vanderbilt team scored an average of 52.7 points per game, the most in college football that season, and allowed just four points.
Southwest.
The first college football game in Oklahoma Territory occurred on November 7, 1895 when the 'Oklahoma City Terrors' defeated the Oklahoma Sooners 34 to 0. The Terrors were a mix of Methodist college students and high schoolers. The Sooners did not manage a single first down. By next season, Oklahoma coach John A. Harts had left to prospect for gold in the Arctic. Organized football was first played in the territory on November 29, 1894 between the Oklahoma City Terrors and Oklahoma City High School. The high school won 24 to 0.
Pacific Coast.
The University of Southern California first fielded an American football team in 1888. Playing its first game on November 14 of that year against the Alliance Athletic Club, in which USC gained a 16–0 victory. Frank Suffel and Henry H. Goddard were playing coaches for the first team which was put together by quarterback Arthur Carroll; who in turn volunteered to make the pants for the team and later became a tailor. USC faced its first collegiate opponent the following year in fall 1889, playing St. Vincent's College to a 40–0 victory. In 1893, USC joined the Intercollegiate Football Association of Southern California (the forerunner of the SCIAC), which was composed of USC, Occidental College, Throop Polytechnic Institute (Cal Tech), and Chaffey College. Pomona College was invited to enter, but declined to do so. An invitation was also extended to Los Angeles High School.
In 1891, the first Stanford football team was hastily organized and played a four-game season beginning in January 1892 with no official head coach. Following the season, Stanford captain John Whittemore wrote to Yale coach Walter Camp asking him to recommend a coach for Stanford. To Whittemore's surprise, Camp agreed to coach the team himself, on the condition that he finish the season at Yale first. As a result of Camp's late arrival, Stanford played just three official games, against San Francisco's Olympic Club and rival California. The team also played exhibition games against two Los Angeles area teams that Stanford does not include in official results. Camp returned to the East Coast following the season, then returned to coach Stanford in 1894 and 1895.
On 25 December 1894, Amos Alonzo Stagg's Chicago Maroons agreed to play Camp's Stanford football team in San Francisco in the first postseason intersectional contest, foreshadowing the modern bowl game. Future president Herbert Hoover was Stanford's student financial manager. Chicago won 24 to 4. Stanford won a rematch in Los Angeles on December 29 by 12 to 0.
The Big Game between Stanford and California is the oldest college football rivalry in the West. The first game was played on San Francisco's Haight Street Grounds on March 19, 1892 with Stanford winning 14–10. The term "Big Game" was first used in 1900, when it was played on Thanksgiving Day in San Francisco. During that game, a large group of men and boys, who were observing from the roof of the nearby S.F. and Pacific Glass Works, fell into the fiery interior of the building when the roof collapsed, resulting in 13 dead and 78 injured. On December 4, 1900, the last victim of the disaster (Fred Lilly) died, bringing the death toll to 22; and, to this day, the "Thanksgiving Day Disaster" remains the deadliest accident to kill spectators at a U.S. sporting event.
The University of Oregon began playing American football in 1894 and played its first game on March 24, 1894, defeating Albany College 44–3 under head coach Cal Young. Cal Young left after that first game and J.A. Church took over the coaching position in the fall for the rest of the season. Oregon finished the season with two additional losses and a tie, but went undefeated the following season, winning all four of its games under head coach Percy Benson. In 1899, the Oregon football team left the state for the first time, playing the California Golden Bears in Berkeley, California.
American football at Oregon State University started in 1893 shortly after athletics were initially authorized at the college. Athletics were banned at the school in May 1892, but when the strict school president, Benjamin Arnold, died, President John Bloss reversed the ban. Bloss's son William started the first team, on which he served as both coach and quarterback. The team's first game was an easy 63-0 defeat over the home team, Albany College.
In May 1900, Yost was hired as the football coach at Stanford University, and, after traveling home to West Virginia, he arrived in Palo Alto, California, on August 21, 1900. Yost led the 1900 Stanford team to a 7–2–1, outscoring opponents 154 to 20. The next year in 1901, Yost was hired by Charles A. Baird as the head football coach for the Michigan Wolverines football team. On 1 January 1902, Yost's dominating 1901 Michigan Wolverines football team agreed to play a 3–1–2 team from Stanford University in the inaugural "Tournament East-West football game" what is now known as the "Rose Bowl Game" by a score of 49–0 after Stanford captain Ralph Fisher requested to quit with eight minutes remaining.
The 1905 season marked the first meeting between Stanford and USC. Consequently, Stanford is USC's oldest existing rival. The Big Game between Stanford and Cal on November 11, 1905 was the first played at Stanford Field, with Stanford winning 12–5.
In 1906, citing concerns about the violence in American Football, universities on the West Coast, led by California and Stanford, replaced the sport with rugby union. At the time, the future of American football was very much in doubt and these schools believed that rugby union would eventually be adopted nationwide. Other schools fllowed suit and also made the switch included Nevada, St. Mary's, Santa Clara, and USC (in 1911). However, due to the perception that West Coast football was inferior to the game played on the East Coast anyway, East Coast and Midwest teams shrugged off the loss of the teams and continued playing American football. With no nationwide movement, the available pool of rugby teams to play remained small. The schools scheduled games against local club teams and reached out to rugby union powers in Australia, New Zealand, and especially, due to its proximity, Canada. The annual Big Game between Stanford and California continued as rugby, with the winner invited by the British Columbia Rugby Union to a tournament in Vancouver over the Christmas holidays, with the winner of that tournament receiving the Cooper Keith Trophy.
Durung 12 seasons of playing rugby union, Stanford was remarkably successful: the team had three undefeated seasons, three one-loss seasons, and an overall record of 94 wins, 20 losses, and 3 ties for a winning percentage of .816. However, after a few years, the school began to feel the isolation of its newly adopted sport, which was not spreading as many had hoped. Students and alumni began to clamor for a return to American football to allow wider intercollegiate competition. The pressure at rival California was stronger (especially as the school had not been as successful in the Big Game as they had hoped), and in 1915 California returned to American football. As reasons for the change, the school cited rule change back to American football, the overwhelming desire of students and supporters to play American football, interest in playing other East Coast and Midwest schools, and a patriotic desire to play an "American" game. California's return to American football increased the pressure on Stanford to also change back in order to maintain the rivalry. Stanford played its 1915, 1916, and 1917 "Big Games" as rugby union against Santa Clara and California's football "Big Game" in those years was against Washington, but both schools desired to restore the old traditions. The onset of American involvement in World War I gave Stanford an out: in 1918, the Stanford campus was designated as the Students' Army Training Corps headquarters for all of California, Nevada, and Utah, and the commanding officer, Sam M. Parker, decreed that American football was the appropriate athletic activity to train soldiers and rugby union was dropped.
Mountain West.
The University of Colorado began playing American football in 1890. Colorado found much success in its early years, winning eight Colorado Football Association Championships (1894–97, 1901–08).
The following was taken from the "Silver & Gold" newspaper of December 16, 1898. It was a recollection of the birth of Colorado football written by one of CU's original gridders, John C. Nixon, also the school's second captain. It appears here in its original form:
In 1909, the Rocky Mountain Athletic Conference was founded, featuring four members, Colorado, Colorado College, Colorado School of Mines, and Colorado Agricultural College. The University of Denver and the University of Utah joined the RMAC in 1910. For its first thirty years, the RMAC was considered a major conference equivalent to today's Division I, before 7 larger members left and formed the Mountain States Conference (also called the Skyline Conference).
Violence, formation of NCAA.
College football increased in popularity through the remainder of the 19th and early 20th century. It also became increasingly violent. Between 1890 and 1905, 330 college athletes died as a direct result of injuries sustained on the football field. These deaths could be attributed to the mass formations and gang tackling that characterized the sport in its early years.
The 1894 Harvard-Yale game, known as the "Hampden Park Blood Bath", resulted in crippling injuries for four players; the contest was suspended until 1897. The annual Army-Navy game was suspended from 1894 to 1898 for similar reasons. One of the major problems was the popularity of mass-formations like the flying wedge, in which a large number of offensive players charged as a unit against a similarly arranged defense. The resultant collisions often led to serious injuries and sometimes even death. Georgia fullback Richard Von Albade Gammon notably died on the field from concussions received against Virginia in 1897, causing Georgia, Georgia Tech, and Mercer to suspend their football programs.
The situation came to a head in 1905 when there were 19 fatalities nationwide. President Theodore Roosevelt reportedly threatened to shut down the game if drastic changes were not made. However, the threat by Roosevelt to eliminate football is disputed by sports historians. What is absolutely certain is that on October 9, 1905, Roosevelt held a meeting of football representatives from Harvard, Yale, and Princeton. Though he lectured on eliminating and reducing injuries, he never threatened to ban football. He also lacked the authority to abolish football and was, in fact, actually a fan of the sport and wanted to preserve it. The President's sons were also playing football at the college and secondary levels at the time.
Meanwhile, John H. Outland held an experimental game in Wichita, Kansas that reduced the number of scrimmage plays to earn a first down from four to three in an attempt to reduce injuries. The "Los Angeles Times" reported an increase in punts and considered the game much safer than regular play but that the new rule was not "conducive to the sport". In 1906, President Roosevelt organized a meeting among thirteen school leaders at the White House to find solutions to make the sport safer for the athletes. Because the college officials could not agree upon a change in rules, it was decided over the course of several subsequent meetings that an external governing body should be responsible. Finally, on December 28, 1905, 62 schools met in New York City to discuss rule changes to make the game safer. As a result of this meeting, the Intercollegiate Athletic Association of the United States was formed in 1906. The IAAUS was the original rule making body of college football, but would go on to sponsor championships in other sports. The IAAUS would get its current name of National Collegiate Athletic Association (NCAA) in 1910, which still sets rules governing the sport.
The rules committee considered widening the playing field to "open up" the game, but Harvard Stadium (the first large permanent football stadium) had recently been built at great expense; it would be rendered useless by a wider field. The rules committee legalized the forward pass instead. Though it was underutilized for years, this proved to be one of the most important rule changes in the establishment of the modern game. Another rule change banned "mass momentum" plays (many of which, like the infamous "flying wedge", were sometimes literally deadly).
Modernization and innovation (1906–1930).
As a result of the 1905–1906 reforms, mass formation plays became illegal and forward passes legal. Bradbury Robinson, playing for visionary coach Eddie Cochems at Saint Louis University, threw the first legal pass in a September 5, 1906, game against Carroll College at Waukesha. Other important changes, formally adopted in 1910, were the requirements that at least seven offensive players be on the line of scrimmage at the time of the snap, that there be no pushing or pulling, and that interlocking interference (arms linked or hands on belts and uniforms) was not allowed. These changes greatly reduced the potential for collision injuries. Several coaches emerged who took advantage of these sweeping changes. Amos Alonzo Stagg introduced such innovations as the huddle, the tackling dummy, and the pre-snap shift. Other coaches, such as Pop Warner and Knute Rockne, introduced new strategies that still remain part of the game.
Besides these coaching innovations, several rules changes during the first third of the 20th century had a profound impact on the game, mostly in opening up the passing game. In 1914, the first roughing-the-passer penalty was implemented. In 1918, the rules on eligible receivers were loosened to allow eligible players to catch the ball anywhere on the field—previously strict rules were in place allowing passes to only certain areas of the field. Scoring rules also changed during this time: field goals were lowered to three points in 1909 and touchdowns raised to six points in 1912.
Star players that emerged in the early 20th century include Jim Thorpe, Red Grange, and Bronko Nagurski; these three made the transition to the fledgling NFL and helped turn it into a successful league. Sportswriter Grantland Rice helped popularize the sport with his poetic descriptions of games and colorful nicknames for the game's biggest players, including Notre Dame's "Four Horsemen" backfield and Fordham University's linemen, known as the "Seven Blocks of Granite".
In 1907 at Champaign, Illinois Chicago and Illinois played in the first game to have a halftime show featuring a marching band. Chicago won 42–6. On November 25, 1911 Kansas and Missouri played the first homecoming football game. The game was "broadcast" play-by-play over telegraph to at least 1,000 fans in Lawrence, Kansas. It ended in a 3–3 tie. The game between West Virginia and Pittsburgh on October 8, 1921, saw the first live radio broadcast of a college football game when Harold W. Arlin announced that year's Backyard Brawl played at Forbes Field on KDKA. Pitt won 21–13. On October 28, 1922, Princeton and Chicago played the first game to be nationally broadcast on radio. Princeton won 21–18 in a hotly contested game which had Princeton dubbed the "Team of Destiny."
Rise of the South.
One publication claims "The first scouting done in the South was in 1905, when Dan McGugin and Captain Innis Brown, of Vanderbilt went to Atlanta to see Sewanee play Georgia Tech." Fuzzy Woodruff claims Davidson was the first in the south to throw a legal forward pass in 1906. The following season saw Vanderbilt execute a double pass play to set up the touchdown that beat Sewanee in a meeting of unbeatens for the SIAA championship. Grantland Rice cited this event as the greatest thrill he ever witnessed in his years of watching sports. Vanderbilt coach Dan McGugin in "Spalding's Football Guides summation of the season in the SIAA wrote "The standing. First, Vanderbilt; second, Sewanee, a might good second;" and that Aubrey Lanier "came near winning the Vanderbilt game by his brilliant dashes after receiving punts." Bob Blake threw the final pass to center Stein Stone, catching it near the goal amongst defenders. Honus Craig then ran in the winning touchdown.
Heisman shift.
Utilizing the "jump shift" offense, John Heisman's Georgia Tech Golden Tornado won 222 to 0 over Cumberland on October 7, 1916, at Grant Field in the most lopsided victory in college football history. Tech went on a 33-game winning streak during this period. The 1917 team was the first national champion from the South, led by a powerful backfield. It also had the first two players from the Deep South selected first-team All-American in Walker Carpenter and Everett Strupper. Pop Warner's Pittsburgh Panthers were also undefeated, but declined a challenge by Heisman to a game. When Heisman left Tech after 1919, his shift was still employed by protege William Alexander.
Notable intersectional games.
In 1906 Vanderbilt defeated Carlisle 4 to 0, the result of a Bob Blake field goal. In 1907 Vanderbilt fought Navy to a 6 to 6 tie. In 1910 Vanderbilt held defending national champion Yale to a scoreless tie.
Helping Georgia Tech's claim to a title in 1917, the Auburn Tigers held undefeated, Chic Harley-led Big Ten champion Ohio State to a scoreless tie the week before Georgia Tech beat the Tigers 68 to 7. The next season, with many players gone due to World War I, a game was finally scheduled at Forbes Field with Pittsburgh. The Panthers, led by freshman Tom Davies, defeated Georgia Tech 32 to 0. Tech center Bum Day was the first player on a Southern team ever selected first-team All-American by Walter Camp.
1917 saw the rise of another Southern team in Centre of Danville, Kentucky. In 1921 Bo McMillin-led Centre upset defending national champion Harvard 6 to 0 in what is widely considered one of the greatest upsets in college football history. The next year Vanderbilt fought Michigan to a scoreless tie at the inaugural game at Dudley Field (now Vanderbilt Stadium), the first stadium in the South made exclusively for college football. Michigan coach Fielding Yost and Vanderbilt coach Dan McGugin were brothers-in-law, and the latter the protege of the former. The game featured the season's two best defenses and included a goal line stand by Vanderbilt to preserve the tie. Its result was "a great surprise to the sporting world." Commodore fans celebrated by throwing some 3,000 seat cushions onto the field. The game features prominently in Vanderbilt's history. That same year, Alabama upset Penn 9 to 7.
Vanderbilt's line coach then was Wallace Wade, who in 1925 coached Alabama to the south's first Rose Bowl victory. This game is commonly referred to as "the game that changed the south." Wade followed up the next season with an undefeated record and Rose Bowl tie. Georgia's 1927 "dream and wonder team" defeated Yale for the first time. Georgia Tech, led by Heisman protege William Alexander, gave the dream and wonder team its only loss, and the next year were national and Rose Bowl champions. The Rose Bowl included Roy Riegels' wrong-way run. On October 12, 1929, Yale lost to Georgia in Sanford Stadium in its first trip to the south. Wade's Alabama again won a national championship and Rose Bowl in 1930.
Coaches of the era.
Glenn "Pop" Warner.
Glenn "Pop" Warner coached at several schools throughout his career, including the University of Georgia, Cornell University, University of Pittsburgh, Stanford University, and Temple University. One of his most famous stints was at the Carlisle Indian Industrial School, where he coached Jim Thorpe, who went on to become the first president of the National Football League, an Olympic Gold Medalist, and is widely considered one of the best overall athletes in history. Warner wrote one of the first important books of football strategy, "Football for Coaches and Players", published in 1927. Though the shift was invented by Stagg, Warner's single wing and double wing formations greatly improved upon it; for almost 40 years, these were among the most important formations in football. As part of his single and double wing formations, Warner was one of the first coaches to effectively utilize the forward pass. Among his other innovations are modern blocking schemes, the three-point stance, and the reverse play. The youth football league, Pop Warner Little Scholars, was named in his honor.
Knute Rockne.
Knute Rockne rose to prominence in 1913 as an end for the University of Notre Dame, then a largely unknown Midwestern Catholic school. When Army scheduled Notre Dame as a warm-up game, they thought little of the small school. Rockne and quarterback Gus Dorais made innovative use of the forward pass, still at that point a relatively unused weapon, to defeat Army 35–13 and helped establish the school as a national power. Rockne returned to coach the team in 1918, and devised the powerful Notre Dame Box offense, based on Warner's single wing. He is credited with being the first major coach to emphasize offense over defense. Rockne is also credited with popularizing and perfecting the forward pass, a seldom used play at the time. The 1924 team featured the Four Horsemen backfield. In 1927, his complex shifts led directly to a rule change whereby all offensive players had to stop for a full second before the ball could be snapped. Rather than simply a regional team, Rockne's "Fighting Irish" became famous for barnstorming and played any team at any location. It was during Rockne's tenure that the annual Notre Dame-University of Southern California rivalry began. He led his team to an impressive 105–12–5 record before his premature death in a plane crash in 1931. He was so famous at that point that his funeral was broadcast nationally on radio.
From a regional to a national sport (1930–1958).
In the early 1930s, the college game continued to grow, particularly in the South, bolstered by fierce rivalries such as the "South's Oldest Rivalry", between Virginia and North Carolina and the "Deep South's Oldest Rivalry", between Georgia and Auburn. Although before the mid-1920s most national powers came from the Northeast or the Midwest, the trend changed when several teams from the South and the West Coast achieved national success. Wallace William Wade's 1925 Alabama team won the 1926 Rose Bowl after receiving its first national title and William Alexander's 1928 Georgia Tech team defeated California in the 1929 Rose Bowl. College football quickly became the most popular spectator sport in the South.
Several major modern college football conferences rose to prominence during this time period. The Southwest Athletic Conference had been founded in 1915. Consisting mostly of schools from Texas, the conference saw back-to-back national champions with Texas Christian University (TCU) in 1938 and Texas A&M in 1939. The Pacific Coast Conference (PCC), a precursor to the Pac-12 Conference (Pac-12), had its own back-to-back champion in the University of Southern California which was awarded the title in 1931 and 1932. The Southeastern Conference (SEC) formed in 1932 and consisted mostly of schools in the Deep South. As in previous decades, the Big Ten continued to dominate in the 1930s and 1940s, with Minnesota winning 5 titles between 1934 and 1941, and Michigan (1933, 1947, and 1948) and Ohio State (1942) also winning titles.
As it grew beyond its regional affiliations in the 1930s, college football garnered increased national attention. Four new bowl games were created: the Orange Bowl, Sugar Bowl, the Sun Bowl in 1935, and the Cotton Bowl in 1937. In lieu of an actual national championship, these bowl games, along with the earlier Rose Bowl, provided a way to match up teams from distant regions of the country that did not otherwise play. In 1936, the Associated Press began its weekly poll of prominent sports writers, ranking all of the nation's college football teams. Since there was no national championship game, the final version of the AP poll was used to determine who 
was crowned the National Champion of college football.
The 1930s saw growth in the passing game. Though some coaches, such as General Robert Neyland at Tennessee, continued to eschew its use, several rules changes to the game had a profound effect on teams' ability to throw the ball. In 1934, the rules committee removed two major penalties—a loss of five yards for a second incomplete pass in any series of downs and a loss of possession for an incomplete pass in the end zone—and shrunk the circumference of the ball, making it easier to grip and throw. Players who became famous for taking advantage of the easier passing game included Alabama end Don Hutson and TCU passer "Slingin" Sammy Baugh.
In 1935, New York City's Downtown Athletic Club awarded the first Heisman Trophy to University of Chicago halfback Jay Berwanger, who was also the first ever NFL Draft pick in 1936. The trophy was designed by sculptor Frank Eliscu and modeled after New York University player Ed Smith. The trophy recognizes the nation's "most outstanding" college football player and has become one of the most coveted awards in all of American sports.
During World War II, college football players enlisted in the armed forces, some playing in Europe during the war. As most of these players had eligibility left on their college careers, some of them returned to college at West Point, bringing Army back-to-back national titles in 1944 and 1945 under coach Red Blaik. Doc Blanchard (known as "Mr. Inside") and Glenn Davis (known as "Mr. Outside") both won the Heisman Trophy, in 1945 and 1946 respectively. On the coaching staff of those 1944–1946 Army teams was future Pro Football Hall of Fame coach Vince Lombardi.
The 1950s saw the rise of yet more dynasties and power programs. Oklahoma, under coach Bud Wilkinson, won three national titles (1950, 1955, 1956) and all ten Big Eight Conference championships in the decade while building a record 47-game winning streak. Woody Hayes led Ohio State to two national titles, in 1954 and 1957, and dominated the Big Ten conference, winning three Big Ten titles—more than any other school. Wilkinson and Hayes, along with Robert Neyland of Tennessee, oversaw a revival of the running game in the 1950s. Passing numbers dropped from an average of 18.9 attempts in 1951 to 13.6 attempts in 1955, while teams averaged just shy of 50 running plays per game. Nine out of ten Heisman trophy winners in the 1950s were runners. Notre Dame, one of the biggest passing teams of the decade, saw a substantial decline in success; the 1950s were the only decade between 1920 and 1990 when the team did not win at least a share of the national title. Paul Hornung, Notre Dame quarterback, did, however, win the Heisman in 1956, becoming the only player from a losing team ever to do so.
Modern college football (since 1958).
Following the enormous success of the 1958 NFL Championship Game, college football no longer enjoyed the same popularity as the NFL, at least on a national level. While both games benefited from the advent of television, since the late 1950s, the NFL has become a nationally popular sport while college football has maintained strong regional ties.
As professional football became a national television phenomenon, college football did as well. In the 1950s, Notre Dame, which had a large national following, formed its own network to broadcast its games, but by and large the sport still retained a mostly regional following. In 1952, the NCAA claimed all television broadcasting rights for the games of its member institutions, and it alone negotiated television rights. This situation continued until 1984, when several schools brought a suit under the Sherman Antitrust Act; the Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals. ABC Sports began broadcasting a national Game of the Week in 1966, bringing key matchups and rivalries to a national audience for the first time.
New formations and play sets continued to be developed. Emory Bellard, an assistant coach under Darrell Royal at the University of Texas, developed a three-back option style offense known as the wishbone. The wishbone is a run-heavy offense that depends on the quarterback making last second decisions on when and to whom to hand or pitch the ball to. Royal went on to teach the offense to other coaches, including Bear Bryant at Alabama, Chuck Fairbanks at Oklahoma and Pepper Rodgers at UCLA; who all adapted and developed it to their own tastes. The strategic opposite of the wishbone is the spread offense, developed by professional and college coaches throughout the 1960s and 1970s. Though some schools play a run-based version of the spread, its most common use is as a passing offense designed to "spread" the field both horizontally and vertically. Some teams have managed to adapt with the times to keep winning consistently. In the rankings of the most victorious programs, Michigan, Texas, and Notre Dame are ranked first, second, and third in total wins.
Growth of bowl games.
In 1940, for the highest level of college football, there were only five bowl games (Rose, Orange, Sugar, Sun, and Cotton). By 1950, three more had joined that number and in 1970, there were still only eight major college bowl games. The number grew to eleven in 1976. At the birth of cable television and cable sports networks like ESPN, there were fifteen bowls in 1980. With more national venues and increased available revenue, the bowls saw an explosive growth throughout the 1980s and 1990s. In the thirty years from 1950 to 1980, seven bowl games were added to the schedule. From 1980 to 2008, an additional 20 bowl games were added to the schedule. Some have criticized this growth, claiming that the increased number of games has diluted the significance of playing in a bowl game. Yet others have countered that the increased number of games has increased exposure and revenue for a greater number of schools, and see it as a positive development.
With the growth of bowl games, it became difficult to determine a national champion in a fair and equitable manner. As conferences became contractually bound to certain bowl games (a situation known as a tie-in), match-ups that guaranteed a consensus national champion became increasingly rare. In 1992, seven conferences and independent Notre Dame formed the Bowl Coalition, which attempted to arrange an annual No.1 versus No.2 matchup based on the final AP poll standings. The Coalition lasted for three years; however, several scheduling issues prevented much success; tie-ins still took precedence in several cases. For example, the Big Eight and SEC champions could never meet, since they were contractually bound to different bowl games. The coalition also excluded the Rose Bowl, arguably the most prestigious game in the nation, and two major conferences—the Pac-10 and Big Ten—meaning that it had limited success. In 1995, the Coalition was replaced by the Bowl Alliance, which reduced the number of bowl games to host a national championship game to three—the Fiesta, Sugar, and Orange Bowls—and the participating conferences to five—the ACC, SEC, Southwest, Big Eight, and Big East. It was agreed that the No.1 and No.2 ranked teams gave up their prior bowl tie-ins and were guaranteed to meet in the national championship game, which rotated between the three participating bowls. The system still did not include the Big Ten, Pac-10, or the Rose Bowl, and thus still lacked the legitimacy of a true national championship.
Bowl Championship Series.
In 1998, a new system was put into place called the Bowl Championship Series. For the first time, it included all major conferences (ACC, Big East, Big 12, Big Ten, Pac-10, and SEC) and four major bowl games (Rose, Orange, Sugar and Fiesta). The champions of these six conferences, along with two "at-large" selections, were invited to play in the four bowl games. Each year, one of the four bowl games served as a national championship game. Also, a complex system of human polls, computer rankings, and strength of schedule calculations was instituted to rank schools. Based on this ranking system, the No.1 and No.2 teams met each year in the national championship game. Traditional tie-ins were maintained for schools and bowls not part of the national championship. For example, in years when not a part of the national championship, the Rose Bowl still hosted the Big Ten and Pac-10 champions.
The system continued to change, as the formula for ranking teams was tweaked from year to year. At-large teams could be chosen from any of the Division I conferences, though only one selection—Utah in 2005—came from a non-BCS affiliated conference. Starting with the 2006 season, a fifth game—simply called the BCS National Championship Game—was added to the schedule, to be played at the site of one of the four BCS bowl games on a rotating basis, one week after the regular bowl game. This opened up the BCS to two additional at-large teams. Also, rules were changed to add the champions of five additional conferences (Conference USA the Mid-American Conference [MAC, the Mountain West Conference the Sun Belt Conference and the Western Athletic Conference [WAC), provided that said champion ranked in the top twelve in the final BCS rankings, or was within the top 16 of the BCS rankings and ranked higher than the champion of at least one of the "BCS conferences" (also known as "AQ" conferences, for Automatic Qualifying). Several times since this rule change was implemented, schools from non-AQ conferences have played in BCS bowl games. In 2009, Boise State played TCU in the Fiesta Bowl, the first time two schools from non-BCS conferences played each other in a BCS bowl game. The last team from the non-AQ ranks to reach a BCS bowl game in the BCS era was Northern Illinois in 2012, which played in (and lost) the 2013 Orange Bowl.
College Football Playoff.
The longtime resistance to a playoff system at the FBS level finally ended with the creation of the College Football Playoff (CFP) beginning with the 2014 season. The CFP is a Plus-One system, a concept that became popular as a BCS alternative following controversies in 2003 and 2004. The CFP is a four-team tournament whose participants are chosen and seeded by a 13-member selection committee. The semifinals are hosted by two of a group of six traditional bowl games often called the "New Year's Six", with semifinal hosting rotating annually among three pairs of games in the following order: Rose/Sugar, Orange/Cotton, and Fiesta/Peach. The two semifinal winners then advance to the College Football Playoff National Championship, whose host is determined by open bidding several years in advance.
The establishment of the CFP followed a tumultuous period of conference realignment in Division I. The WAC, after seeing all but two of its football members leave, dropped football after the 2012 season. The Big East split into two leagues in 2013; the schools that did not play FBS football reorganized as a new non-football Big East Conference, while the FBS member schools that remained in the original structure joined with several new members and became the American Athletic Conference. The American retained the Big East's automatic BCS bowl bid for the 2013 season, but lost this status in the CFP era.
The 10 FBS conferences are formally and popularly divided into two groups:
Official rules and notable rule distinctions.
Although rules for the high school, college, and NFL games are generally consistent, there are several minor differences. The NCAA Football Rules Committee determines the playing rules for Division I (both Bowl and Championship Subdivisions), II, and III games (the National Association of Intercollegiate Athletics (NAIA) is a separate organization, but uses the NCAA rules).
Organization.
College teams mostly play other similarly sized schools through the NCAA's divisional system. Division I generally consists of the major collegiate athletic powers with larger budgets, more elaborate facilities, and (with the exception of a few conferences such as the Pioneer Football League) more athletic scholarships. Division II primarily consists of smaller public and private institutions that offer fewer scholarships than those in Division I. Division III institutions also field teams, but do not offer any scholarships.
Football teams in Division I are further divided into the Bowl Subdivision (consisting of the largest programs) and the Championship Subdivision. The Bowl Subdivision has historically not used an organized tournament to determine its champion, and instead teams compete in post-season bowl games. That changed with the debut of the four-team College Football Playoff at the end of the 2014 season.
Teams in each of these four divisions are further divided into various regional conferences.
Several organizations operate college football programs outside the jurisdiction of the NCAA:
A college that fields a team in the NCAA is not restricted from fielding teams in club or sprint football, and several colleges field two teams, a varsity (NCAA) squad and a club or sprint squad (no schools, as of 2015, field both club "and" sprint teams at the same time).
Playoff games.
Started in the 2014 season, four Division I FBS teams are selected at the end of regular season to compete in a playoff for the FBS national championship. The inaugural champion was Ohio State University. The College Football Playoff replaced the Bowl Championship Series, which had been used as the selection method to determine the national championship game participants since in the 1998 season.
At the Division I FCS level, the teams participate in a 24-team playoff (most recently expanded from 20 teams in 2013) to determine the national championship. Under the current playoff structure, the top eight teams are all seeded, and receive a bye week in the first round. The highest seed receives automatic home field advantage. Starting in 2013, non-seeded teams can only host a playoff game if both teams involved are unseeded; in such a matchup, the schools must bid for the right to host the game. Selection for the playoffs is determined by a selection committee, although usually a team must have a 7-4 record to even be considered. Losses to an FBS team count against their playoff eligibility, while wins against a Division II opponent do not count towards playoff consideration. Thus, only Division I wins (whether FBS, FCS, or FCS non-scholarship) are considered for playoff selection. The Division I National Championship game is held in Frisco, Texas.
Division II and Division III of the NCAA also participate in their own respective playoffs, crowning national champions at the end of the season. The National Association of Intercollegiate Athletics also holds a playoff.
Bowl games.
Unlike other college football divisions and most other sports—collegiate or professional—the Football Bowl Subdivision, formerly known as Division I-A college football, has historically not employed a playoff system to determine a champion. Instead, it has a series of postseason "bowl games". The annual National Champion in the Football Bowl Subdivision is then instead traditionally determined by a vote of sports writers and other non-players.
This system has been challenged often, beginning with an NCAA committee proposal in 1979 to have a four-team playoff following the bowl games. However, little headway was made in instituting a playoff tournament until 2014, given the entrenched vested economic interests in the various bowls. Although the NCAA publishes lists of claimed FBS-level national champions in its official publications, it has never recognized an official FBS national championship; this policy continues even after the establishment of the College Football Playoff (which is not directly run by the NCAA) in 2014. As a result, the official Division I National Champion is the winner of the Football Championship Subdivision, as it is the highest level of football with an NCAA-administered championship tournament.
The first bowl game was the 1902 Rose Bowl, played between Michigan and Stanford; Michigan won 49-0. It ended when Stanford requested and Michigan agreed to end it with 8 minutes on the clock. That game was so lopsided that the game was not played annually until 1916, when the Tournament of Roses decided to reattempt the postseason game. The term "bowl" originates from the shape of the Rose Bowl stadium in Pasadena, California, which was built in 1923 and resembled the Yale Bowl, built in 1915. This is where the name came into use, as it became known as the Rose Bowl Game. Other games came along and used the term "bowl", whether the stadium was shaped like a bowl or not.
At the Division I FBS level, teams must earn the right to be bowl eligible by winning at least 6 games during the season (teams that play 13 games in a season, which is allowed for Hawaii and any of its home opponents, must win 7 games). They are then invited to a bowl game based on their conference ranking and the tie-ins that the conference has to each bowl game. For the 2009 season, there were 34 bowl games, so 68 of the 120 Division I FBS teams were invited to play at a bowl. These games are played from mid-December to early January and most of the later bowl games are typically considered more prestigious.
After the Bowl Championship Series, additional all-star bowl games round out the post-season schedule through the beginning of February.
Division I FBS National Championship Games.
Partly as a compromise between both bowl game and playoff supporters, the NCAA created the Bowl Championship Series (BCS) in 1998 in order to create a definitive National Championship game for college football. The series included the four most prominent bowl games (Rose Bowl, Orange Bowl, Sugar Bowl, Fiesta Bowl), while the National Championship game rotated each year between one of these venues. The BCS system was slightly adjusted in 2006, as the NCAA added a fifth game to the series, called the National Championship Game. This allowed the four other BCS bowls to use their normal selection process to select the teams in their games while the top two teams in the BCS rankings would play in the new National Championship Game.
The BCS selection committee used a complicated, and often controversial, computer system to rank all Division 1-FBS teams and the top two teams at the end of the season played for the National Championship. This computer system, which factored in newspaper polls, online polls, coaches' polls, strength of schedule, and various other factors of a team's season, led to much dispute over whether the two best teams in the country were being selected to play in the National Championship Game.
The BCS ended after the 2013 season and, since the 2014 season, the FBS national champion has been determined by a four-team tournament known as the College Football Playoff (CFP). A selection committee of college football experts decides the participating teams. Six major bowl games (the Rose, Sugar, Cotton, Orange, Peach, and Fiesta) rotate on a three-year cycle as semifinal games, with the winners advancing to the College Football Playoff National Championship. This arrangement is contractually locked in until the 2026 season.
Controversy.
College football is a controversial institution within American higher education, where the amount of money involved—what people will pay for the entertainment provided—is a corrupting factor within universities that they are usually ill-equipped to deal with. According to William E. Kirwan, chancellor of the University of Maryland System and co-director of the Knight Commission on Intercollegiate Athletics, "We've reached a point where big-time intercollegiate athletics is undermining the integrity of our institutions, diverting presidents and institutions from their main purpose." Football coaches often make more than the presidents of their universities which employ them. Athletes are alleged to receive preferential treatment both in academics and when they run afoul of the law. Although in theory football is an extra-curricular activity engaged in as a sideline by students, it generates a substantial profit, from which the athletes receive no direct benefit. There has been serious discussion about making student-athletes university employees to allow them to be paid.
College football outside the United States.
Canadian football, which parallels American football, is played by collegiate teams in Canada under the auspices of Canadian Interuniversity Sport. (Unlike in the United States, no junior colleges play football in Canada, and the sanctioning body for junior college athletics in Canada, CCAA, does not sanction the sport.) However, amateur football outside of colleges is played in Canada, such as in the Canadian Junior Football League. Organized competition in American football also exists at the collegiate level in Mexico (ONEFA), the UK (British Universities American Football League), Japan (Japan American Football Association, Koshien Bowl), and South Korea (Korea American Football Association).

</doc>
<doc id="6773" url="https://en.wikipedia.org/wiki?curid=6773" title="Ciprofloxacin">
Ciprofloxacin

Ciprofloxacin is an antibiotic used to treat a number of bacterial infections. This includes bone and joint infections, intra abdominal infections, certain type of infectious diarrhea, respiratory tract infections, skin infections, typhoid fever, and urinary tract infections, among others. For some infections it is used in addition to other antibiotics. It can be taken by mouth or used intravenously.
Common side effects include nausea, vomiting, diarrhea, and rash. Ciprofloxacin increases the risk of tendon rupture and worsening muscle weakness in people with the neurological disorder myasthenia gravis. Rates of side effects appear to be higher than some groups of antibiotics such as cephalosporins but lower than others such as clindamycin. Studies in other animals raise concerns regarding use in pregnancy. No problems were identified; however, in the children of a small number of women who took the medication. It appears to be safe during breastfeeding. It is a second-generation fluoroquinolone with a broad spectrum of activity that usually results in the death of the bacteria.
Ciprofloxacin was introduced in 1987. It is on the World Health Organization's List of Essential Medicines, the most important medications needed in a basic health system. It is available as a generic medication and not very expensive. Wholesale it costs between 0.03 and 0.13 USD a dose. In the United States it is sold for about 0.40 USD per dose.
Medical uses.
Ciprofloxacin is used to treat a wide variety of infections, including infections of bones and joints, endocarditis, gastroenteritis, malignant otitis externa, respiratory tract infections, cellulitis, urinary tract infections, prostatitis, anthrax, and chancroid.
Ciprofloxacin only treats bacterial infections; it does not treat viral infections such as the common cold. For certain uses including acute sinusitis, lower respiratory tract infections and uncomplicated gonorrhea, ciprofloxacin is not considered a first-line agent.
Ciprofloxacin occupies an important role in treatment guidelines issued by major medical societies for the treatment of serious infections, especially those likely to be caused by Gram-negative bacteria, including "Pseudomonas aeruginosa". For example, ciprofloxacin in combination with metronidazole is one of several first-line antibiotic regimens recommended by the Infectious Diseases Society of America for the treatment of community-acquired abdominal infections in adults. It also features prominently in treatment guidelines for acute pyelonephritis, complicated or hospital-acquired urinary tract infection, acute or chronic prostatitis, certain types of endocarditis, certain skin infections, and prosthetic joint infections.
In other cases, treatment guidelines are more restrictive, recommending in most cases that older, narrower-spectrum drugs be used as first-line therapy for less severe infections to minimize fluoroquinolone-resistance development. For example, the Infectious Diseases Society of America recommends the use of ciprofloxacin and other fluoroquinolones in urinary tract infections be reserved to cases of proven or expected resistance to narrower-spectrum drugs such as nitrofurantoin or trimethoprim/sulfamethoxazole. The European Association of Urology recommends ciprofloxacin as an alternative regimen for the treatment of uncomplicated urinary tract infections, but cautions that the potential for “adverse events have to be considered”.
Although approved by regulatory authorities for the treatment of respiratory infections, ciprofloxacin is not recommended for respiratory infections by most treatment guidelines due in part to its modest activity against the common respiratory pathogen "Streptococcus pneumoniae". "Respiratory quinolones" such as levofloxacin, having greater activity against this pathogen, are recommended as first line agents for the treatment of community-acquired pneumonia in patients with important co-morbidities and in patients requiring hospitalization (Infectious Diseases Society of America 2007). Similarly, ciprofloxacin is not recommended as a first-line treatment for acute sinusitis.
Ciprofloxacin is approved for the treatment of gonorrhea in many countries, but this recommendation is widely regarded as obsolete due to resistance development.
Pregnancy.
In the United States ciprofloxacin is pregnancy category C. This category includes drugs for which no adequate and well-controlled studies in human pregnancy exist, and for which animal studies have suggested the potential for harm to the fetus, but potential benefits may warrant use of the drug in pregnant women
despite potential risks. An expert review of published data on experiences with ciprofloxacin use during pregnancy by the Teratogen Information System concluded therapeutic doses during 
pregnancy are unlikely to pose a substantial teratogenic risk (quantity and quality of data=fair), but the data are insufficient to state no risk exists.
Two small post-marketing epidemiology studies of mostly short-term, first-trimester exposure found that fluoroquinolones did not increase risk of major malformations, spontaneous abortions, premature birth, or low birth weight. The label notes, however, that these studies are insufficient to reliably evaluate the definitive safety or risk of less common defects by ciprofloxacin in pregnant women and their developing fetuses.
Breastfeeding.
Fluoroquinolones have been reported as present in a mother's milk and thus passed on to the nursing child. The U.S. FDA recommends that because of the risk of serious adverse reactions (including articular damage) in infants nursing from mothers taking ciprofloxacin, a decision should be made whether to discontinue nursing or discontinue the drug, taking into account the importance of the drug to the mother.
Children.
Oral and intravenous ciprofloxacin are approved by the FDA for use in children for only two indications due to the risk of permanent injury to the musculoskeletal system:
1) Inhalational anthrax (postexposure)
2) Complicated urinary tract infections and pyelonephritis due to "Escherichia coli", but never as first-line agents. Current recommendations by the American Academy of Pediatrics note the systemic use of ciprofloxacin in children should be restricted to infections caused by multidrug-resistant pathogens or when no safe or effective alternatives are available.
Spectrum of activity.
Its spectrum of activity includes most strains of bacterial pathogens responsible for respiratory, urinary tract, gastrointestinal, and abdominal infections, including Gram-negative ("Escherichia coli", "Haemophilus influenzae", "Klebsiella pneumoniae", "Legionella pneumophila", "Moraxella catarrhalis", "Proteus mirabilis", and "Pseudomonas aeruginosa"), and Gram-positive (methicillin-sensitive, but not methicillin-resistant "Staphylococcus aureus", "Streptococcus pneumoniae", "Staphylococcus epidermidis", "Enterococcus faecalis", and "Streptococcus pyogenes") bacterial pathogens.
Bacterial resistance.
As a result of its widespread use to treat minor infections readily treatable with older, narrower spectrum antibiotics, many bacteria have developed resistance to this drug in recent years, leaving it significantly less effective than it would have been otherwise.
Resistance to ciprofloxacin and other fluoroquinolones may evolve rapidly, even during a course of treatment. Numerous pathogens, including enterococci, "Streptococcus pyogenes" and "Klebsiella pneumoniae" (quinolone-resistant) now exhibit resistance. Widespread veterinary usage of the fluoroquinolones, particularly in Europe, has been implicated. Meanwhile, some "Burkholderia cepacia", "Clostridium innocuum" and "Enterococcus faecium" strains have developed resistance to ciprofloxacin to varying degrees.
Fluoroquinolones had become the class of antibiotics most commonly prescribed to adults in 2002. Nearly half (42%) of those prescriptions in the U.S. were for conditions not approved by the FDA, such as acute bronchitis, otitis media, and acute upper respiratory tract infection, according to a study supported in part by the Agency for Healthcare Research and Quality. Additionally, they were commonly prescribed for medical conditions that were not even bacterial to begin with, such as viral infections, or those to which no proven benefit existed.
Contraindications.
Contraindications include:
Ciprofloxacin is also considered to be contraindicated in children (except for the indications outlined above), in pregnancy, to nursing mothers, and in people with epilepsy or other seizure disorders.
Side effects.
Rates of side effects appear to be higher than with some groups of antibiotics such as cephalosporins but lower than with others such as clindamycin. Compared to other antibiotics some studies find a higher rate of side effects while others find no difference.
In pre-approval clinical trials of ciprofloxacin most of the adverse events reported were described as mild or moderate in severity, abated soon after the drug was discontinued, and required no treatment. Ciprofloxacin was discontinued because of an adverse event in 1% of people treated with the medication by mouth. The most frequently reported drug-related events, from trials of all formulations, all dosages, all drug-therapy durations, and for all indications, were nausea (2.5%), diarrhea (1.6%), abnormal liver function tests (1.3%), vomiting (1%), and rash (1%). Other adverse events occurred at rates of <1%.
Tendinitis.
The black box warning on the U.S. FDA-approved ciprofloxacin label warns of an increased risk of tendinitis and tendon rupture, especially in people who are older than 60 years, people who also use corticosteroids, and people with kidney, lung, or heart transplants. Tendon rupture can occur during therapy or even months after discontinuation of the drug. A case control study performed using a UK medical care database found that fluoroquinolone use was associated with a 1.9-fold increase in tendon problems. The relative risk increased to 3.2 in those over 60 years of age and to 6.2 in those over the age of 60 who were also taking corticosteroids. Among the 46,766 quinolone users in the study, 38 (0.1%) cases of Achilles tendon rupture were identified. A study performed using an Italian healthcare database reached qualitatively similar conclusions.
Nervous system.
The 2013 FDA label warns of nervous system effects. Ciprofloxacin, like other fluoroquinolones, is known to trigger seizures or lower the seizure threshold, and may cause other central nervous system side effects. Headache, dizziness, and insomnia have been reported as occurring fairly commonly in postapproval review articles, along with a much lower incidence of serious CNS side effects such as tremors, psychosis, anxiety, hallucinations, paranoia, and suicide attempts, especially at higher doses. Like other fluoroquinolones, it is also known to cause peripheral neuropathy that may be irreversible, such as weakness, burning pain, tingling, or numbness.
Cancer.
Ciprofloxacin is active in six of eight "in vitro" assays used as rapid screens for the detection of genotoxic effects, but is not active in "in vivo" assays of genotoxicity. Long-term carcinogenicity studies in rats and mice resulted in no carcinogenic or tumorigenic effects due to ciprofloxacin at daily oral dose levels up to 250 and 750 mg/kg to rats and mice, respectively (about 1.7 and 2.5 times the highest recommended therapeutic dose based upon mg/m2). Results from photo co-carcinogenicity testing indicate ciprofloxacin does not reduce the time to appearance of UV-induced skin tumors as compared to vehicle control.
Other.
The other black box warning is that ciprofloxacin should not be used in patients with myasthenia gravis due to possible exacerbation of muscle weakness which may lead to breathing problems resulting in death or ventilator support. Fluoroquinolones are known to block neuromuscular transmission. 
"Clostridium difficile"-associated diarrhea is a serious adverse effect of ciprofloxacin and other fluoroquinolones; it is unclear whether the risk is higher than with other broad-spectrum antibiotics.
A wide range of rare but potentially fatal side effects spontaneously reported to the U.S. FDA or the subject of case reports published in medical journals includes, but is not limited to, toxic epidermal necrolysis, Stevens-Johnson syndrome, heart arrhythmias ("torsades de pointes" or QT prolongation), allergic pneumonitis, bone marrow suppression, hepatitis or liver failure, and sensitivity to light. The drug should be discontinued if a rash, jaundice, or other sign of hypersentitivity occur.
Children and the elderly are at a much greater risk of experiencing adverse reactions.
Overdose.
Overdose of ciprofloxacin may result in reversible renal toxicity. Treatment of overdose includes emptying of the stomach by induced vomiting or gastric lavage, as well as administration of antacids containing magnesium, aluminum, or calcium to reduce drug absorption. Renal function and urinary pH should be monitored. Important support includes adequate hydration and urine acidification if necessary to prevent crystalluria. Hemodialysis or peritoneal dialysis can only remove less than 10% of ciprofloxacin. Ciprofloxacin may be quantified in plasma or serum to monitor for drug accumulation in patients with hepatic dysfunction or to confirm a diagnosis of poisoning in acute overdose victims.
Interactions.
Ciprofloxacin interacts with certain foods and several other drugs leading to undesirable increases or decreases in the serum levels or distribution of one or both drugs.
Ciprofloxacin should not be taken with antacids containing magnesium or aluminum, highly buffered drugs (sevelamer, lanthanum carbonate, sucralfate, didanosine), or with supplements containing calcium, iron, or zinc. It should be taken two hours before or six hours after these products. Magnesium or aluminum antacids turn ciprofloxacin into insoluble salts that are not readily absorbed by the intestinal tract, reducing peak serum concentrations by 90% or more, leading to therapeutic failure. Additionally, it should not be taken with dairy products or calcium-fortified juices alone, as peak serum concentration and the area under the serum concentration-time curve can be reduced up to 40%. However, ciprofloxacin may be taken with dairy products or calcium-fortified juices as part of a meal.
Ciprofloxacin inhibits the drug-metabolizing enzyme CYP1A2 and thereby can reduce the clearance of drugs metabolized by that enzyme. CYP1A2 substrates that exhibit increased serum levels in ciprofloxacin-treated patients include tizanidine, theophylline, caffeine, methylxanthines, clozapine, olanzapine, and ropinirole. Co-administration of ciprofloxacin with the CYP1A2 substrate tizanidine (Zanaflex) is contraindicated due to a 583% increase in the peak serum concentrations of tizanidine when administered with ciprofloxacin as compared to administration of tizanidine alone. Use of ciprofloxacin is cautioned in patients on theophylline due to its narrow therapeutic index. The authors of one review recommended that patients being treated with ciprofloxacin reduce their caffeine intake. Evidence for significant interactions with several other CYP1A2 substrates such as cyclosporine is equivocal or conflicting.
The Committee on Safety of Medicines and the FDA warn that central nervous system adverse effects, including seizure risk, may be increased when NSAIDs are combined with quinolones. The mechanism for this interaction may involve a synergistic increased antagonism of GABA neurotransmission.
Altered serum levels of the antiepileptic drugs phenytoin and carbamazepine (increased and decreased) have been reported in patients receiving concomitant ciprofloxacin.
Mechanism of action.
Ciprofloxacin is a broad-spectrum antibiotic of the fluoroquinolone class. It is active against both Gram-positive and Gram-negative bacteria. It functions by inhibiting DNA gyrase, and a type II topoisomerase, topoisomerase IV, necessary to separate bacterial DNA, thereby inhibiting cell division.
Pharmacokinetics.
Ciprofloxacin for systemic administration is available as immediate-release tablets, extended-release tablets, an oral suspension, and as a solution for intravenous administration. 
When administered over one hour as an intravenous infusion, ciprofloxacin rapidly distributes into the tissues, with levels in some tissues exceeding those in the serum. Penetration into the central nervous system is relatively modest, with cerebrospinal fluid levels normally less than 10% of peak serum concentrations. The serum half-life of ciprofloxacin is about 4–6 hours, with 50-70% of an administered dose being excreted in the urine as unmetabolized drug. An additional 10% is excreted in urine as metabolites. Urinary excretion is virtually complete 24 hours after administration. Dose adjustment is required in the elderly and in those with renal impairment.
Ciprofloxacin is weakly bound to serum proteins (20-40%), but is an inhibitor of the drug-metabolizing enzyme cytochrome P450 1A2, which leads to the potential for clinically important drug interactions with drugs metabolized by that enzyme.
Ciprofloxacin is about 70% orally available when administered orally, so a slightly higher dose is needed to achieve the same exposure when switching from IV to oral administration. A 750-mg immediate-release oral tablet given every 12 hours produces about the same area under the serum concentration curve (AUC) and peak serum concentration (Cmax) as a 400-mg dose given every 8 hours IV. 
The extended release oral tablets allow once-daily administration by releasing the drug more slowly in the gastrointestinal tract. These tablets contain 35% of the administered dose in an immediate-release form and 65% in a slow-release matrix. Maximum serum concentrations are achieved between 1 and 4 hours after administration. Compared to the 250- and 500-mg immediate-release tablets, the 500-mg and 1000-mg XR tablets provide higher Cmax, but the 24‑hour AUCs are equivalent.
Ciprofloxacin immediate-release tablets contain ciprofloxacin as the hydrochloride salt, and the XR tablets contain a mixture of the hydrochloride salt as the free base.
Chemical properties.
Ciprofloxacin is 1-cyclopropyl-6-fluoro-1,4-dihydro-4-oxo-7-(1-piperazinyl)-3-quinolinecarboxylic acid. Its empirical formula is C17H18FN3O3 and its molecular weight is 331.4 g/mol. It is a faintly yellowish to light yellow crystalline substance.
Ciprofloxacin hydrochloride (USP) is the monohydrochloride monohydrate salt of ciprofloxacin. It is a faintly yellowish to light yellow crystalline substance with a molecular weight of 385.8 g/mol. Its empirical formula is C17H18FN3O3HCl•H2O.
Usage.
Ciprofloxacin is the most widely used of the second-generation quinolones. In 2010, over 20 million prescriptions were written, making it the 35th-most commonly prescribed generic drug and the 5th-most commonly prescribed antibacterial in the U.S.
History.
The first members of the quinolone antibacterial class were relatively low-potency drugs such as nalidixic acid, used mainly in the treatment of urinary tract infections owing to their renal excretion and propensity to be concentrated in urine. In 1979, the publication of a patent filed by the pharmaceutical arm of Kyorin Seiyaku Kabushiki Kaisha disclosed the discovery of norfloxacin, and the demonstration that certain structural modifications including the attachment of a fluorine atom to the quinolone ring leads to dramatically enhanced antibacterial potency. In the aftermath of this disclosure, several other pharmaceutical companies initiated research and development programs with the goal of discovering additional antibacterial agents of the fluoroquinolone class.
The fluoroquinolone program at Bayer focused on examining the effects of very minor changes to the norfloxacin structure. In 1983, the company published "in vitro" potency data for ciprofloxacin, a fluoroquinolone antibacterial having a chemical structure differing from that of norfloxacin by the presence of a single carbon atom. This small change led to a two- to 10-fold increase in potency against most strains of Gram-negative bacteria. Importantly, this structural change led to a four-fold improvement in activity against the important Gram-negative pathogen "Pseudomonas aeruginosa", making ciprofloxacin one of the most potent known drugs for the treatment of this intrinsically antibiotic-resistant pathogen.
The oral tablet form of ciprofloxacin was approved in October 1987, just one year after the approval of norfloxacin. In 1991, the intravenous formulation was introduced. Ciprofloxacin sales reached a peak of about 2 billion euros in 2001, representing 34% of Bayer’s total pharmaceutical revenues, before Bayer's patent expired in 2004, after which annual sales have averaged around €200 million.
Society and culture.
Cost.
It is available as a generic medication and not very expensive. Wholesale it costs between 0.03 and 0.13 USD a dose. In the United States it is sold for about 0.40 USD per dose.
Generic equivalents.
On 24 October 2001, the Prescription Access Litigation (PAL) project filed suit to dissolve an agreement between Bayer and three of its competitors which produced generic versions of drugs (Barr Laboratories, Rugby Laboratories, and Hoechst-Marion-Roussel) that PAL claimed was blocking access to adequate supplies and cheaper, generic versions of ciprofloxacin. The plaintiffs charged that Bayer Corporation, a unit of Bayer AG, had unlawfully paid the three competing companies a total of $200 million to prevent cheaper, generic versions of ciprofloxacin from being brought to the market, as well as manipulating its price and supply. Numerous other consumer advocacy groups joined the lawsuit. On 15 October 2008, five years after Bayer's patent had expired, the United States District Court for the Eastern District of New York granted Bayer's and the other defendants' motion for summary judgment, holding that any anticompetitive effects caused by the settlement agreements between Bayer and its codefendants were within the exclusionary zone of the patent and thus could not be redressed by federal antitrust law, in effect upholding Bayer's agreement with its competitors.
Available forms.
Ciprofloxacin for systemic administration is available as immediate-release tablets, as extended-release tablets, as an oral suspension, and as a solution for intravenous infusion. It is also available for local administration as eye drops and ear drops.
Litigation.
A class action was filed against Bayer AG on behalf of employees of the Brentwood Post Office in Washington, D.C., and workers at the U.S. Capitol, along with employees of American Media, Inc. in Florida and postal workers in general who alleged they suffered serious adverse effects from taking ciprofloxacin (Cipro) in the aftermath of the anthrax attacks in 2001. The action alleged Bayer failed to warn class members of the potential side effects of the drug, thereby violating the Pennsylvania Unfair Trade Practices and Consumer Protection Laws. The class action was defeated and the litigation abandoned by the plaintiffs.
A similar action was filed in 2003 in New Jersey by four New Jersey postal workers but was withdrawn for lack of grounds, as workers had been informed of the risks of cipro when they were given the option of taking the drug.

</doc>
<doc id="6774" url="https://en.wikipedia.org/wiki?curid=6774" title="Consubstantiation">
Consubstantiation

Consubstantiation is a theological doctrine that (like Transubstantiation) attempts to describe the nature of the Christian Eucharist in concrete metaphysical terms. It holds that during the sacrament, the fundamental "substance" of the body and blood of Christ are present "alongside" the substance of the bread and wine, which remain present.
Use.
The doctrine of consubstantiation is often held in contrast to the doctrine of transubstantiation. While some Lutherans use the term "consubstantiation" to describe their doctrine, many reject it as not accurately reflecting the eucharistic doctrine of Martin Luther, the sacramental union. Lutherans reject the concept of consubstantiation because it replaces what they believe to be the biblical doctrine with a philosophical construct and implies, in their view, a natural, local inclusion of the body and blood of Christ in the consecrated bread and wine of the eucharist.
In England in the late 14th century, there was a political and religious movement known as Lollardy. Among much broader goals, the Lollards affirmed a form of consubstantiation—that the Eucharist remained physically bread and wine, while becoming spiritually the body and blood of Christ. Lollardy survived up until the time of the English Reformation.
Literary critic Kenneth Burke's dramatism takes this concept and utilizes it in secular rhetorical theory to look at the dialectic of unity and difference within the context of logology.

</doc>
<doc id="6775" url="https://en.wikipedia.org/wiki?curid=6775" title="Chlorophyta">
Chlorophyta

Chlorophyta is a division of green algae, informally called chlorophytes. The name is used in two very different senses, so care is needed to determine the use by a particular author. In older classification systems, it refers to a highly paraphyletic group of "all" the green algae within the green plants (Viridiplantae) and thus includes about 7,000 species of mostly aquatic photosynthetic eukaryotic organisms. In newer classifications, it refers to one of the two clades making up the Viridiplantae, which are the chlorophytes and the streptophytes. The clade Streptophyta consists of two divisions, the Charophyta and the Embryophyta. In this sense the Chlorophyta includes only about 4,300 species.
Like the land plants (bryophytes and tracheophytes), green algae contain chlorophyll a and chlorophyll b and store food as starch in their plastids.
The division contains both unicellular and multicellular species. While most species live in freshwater habitats and a large number in marine habitats, other species are adapted to a wide range of environments. Watermelon snow, or "Chlamydomonas nivalis", of the class Chlorophyceae, lives on summer alpine snowfields. Others live attached to rocks or woody parts of trees. "Monostroma kuroshiensis", an edible green alga cultivated worldwide and most expensive among green algae, belongs to this group. Some lichens are symbiotic relationships between fungi and green algae.
Members of the Chlorophyta also form symbiotic relationships with protozoa, sponges, and cnidarians. All are flagellated, and these have an advantage of motility. Some conduct sexual reproduction, which is oogamous or isogamous.
Ecology.
Species of Chlorophyta (treated as what is now considered one of the two main clades of Viridiplantae) are common inhabitants of marine, freshwater and terrestrial environments. Several species have adapted to specialised and extreme environments, such as deserts, arctic environments, hypersaline habitats, marine deep waters and deep-sea hydrothermal vents. 
Classifications.
Characteristics used for the classification of Chlorophyta are: type of zoid, mitosis (karyokynesis), cytokinesis, organization level, life cycle, type of gametes, cell wall polysaccharides and more recently genetic data.
Leliaert "et al". 2012.
Simplified phylogeny of the Chlorophyta, according to Leliaert "et al". 2012. Note that many algae previously classified in Chlorophyta are placed here in Streptophyta.
Pombert "et al". 2005.
A possible classification when Chlorophyta refers to one of the two clades of the Viridiplantae is shown below.
Hoek, Mann and Jahns 1995.
Classification of the Chlorophyta, treated as all green algae, according to Hoek, Mann and Jahns 1995.
In a note added in proof, an alternative classification is presented for the algae of the class Chlorophyceae:
Bold and Wynne 1985.
Classification of the Chlorophyta and Charophyta according to Bold and Wynne 1985.
Mattox & Stewart 1984.
Classification of the Chlorophyta according to Mattox & Stewart 1984:
Fott 1971.
Classification of the Chlorophyta according to Fott 1971.
Round 1971.
Classification of the Chlorophyta and related algae according to Round 1971.
Smith 1938.
Classification of the Chlorophyta according to Smith 1938:

</doc>
<doc id="6776" url="https://en.wikipedia.org/wiki?curid=6776" title="Capybara">
Capybara

The capybara ("Hydrochoerus hydrochaeris") is a large rodent of the genus "Hydrochoerus" of which the only other extant member is the lesser capybara ("Hydrochoerus isthmius"). The capybara is the largest rodent in the world. Close relatives are guinea pigs and rock cavies, and it is more distantly related to the agouti, chinchillas, and the coypu. Native to South America, the capybara inhabits savannas and dense forests and lives near bodies of water. It is a highly social species and can be found in groups as large as 100 individuals, but usually lives in groups of 10–20 individuals. The capybara is not a threatened species and is hunted for its meat and hide and also for a grease from its thick fatty skin which is used in the pharmaceutical trade.
Etymology.
Its common name is derived from Tupi "ka'apiûara", a complex agglutination of "kaá" (leaf) + "píi" (slender) + "ú" (eat) + "ara" (a suffix for agent nouns), meaning "one who eats slender leaves", or "grass-eater".
The scientific name, both "hydrochoerus" and "hydrochaeris", comes from Greek ὕδωρ ("hydor" = water) + χοίρος ("choiros" = pig, hog).
Classification and phylogeny.
The capybara and the lesser capybara belong to the subfamily Hydrochoerinae along with the rock cavies. The living capybaras and their extinct relatives were previously classified in their own family Hydrochoeridae. Since 2002, molecular phylogenetic studies have recognized a close relationship between "Hydrochoerus" and "Kerodon" supporting placement of both genera in a subfamily of Caviidae. Paleontological classifications have yet to incorporate this new taxonomy and continue to use Hydrochoeridae for all capybaras, while using Hydrochoerinae for the living genus and its closest fossil relatives, such as "Neochoerus". The taxonomy of fossil hydrochoerines is also in a state of flux. In recent years, the diversity of fossil hydrochoerines has been substantially reduced. This is largely due to the recognition that capybara molar teeth show strong variation in shape over the life of an individual. In one instance, material once referred to four genera and seven species on the basis of differences in molar shape is now thought to represent differently aged individuals of a single species, "Cardiatherium paranense".
Description.
The capybara has a heavy, barrel-shaped body and short head, with reddish-brown fur on the upper part of its body that turns yellowish-brown underneath. Its sweat glands can be found in the surface of the hairy portions of its skin, an unusual trait among rodents. The animal lacks under hair, and guard hair differs little from over hair. Adult capybaras grow to in length, stand tall at the withers, and typically weigh , with an average in the Venezuelan llanos of . The top recorded weights are for a wild female from Brazil and for a wild male from Uruguay. The dental formula is . Capybaras have slightly webbed feet and vestigial tails. Their hind legs are slightly longer than their forelegs; they have three toes on their rear feet and four toes on their front feet. Their muzzles are blunt, with nostrils, and the eyes and ears are near the top of their heads. Females are slightly heavier than males.
Its karyotype has 2n = 66 and FN = 102.
Ecology.
Capybaras are semi-aquatic mammals found throughout almost all countries of South America (except Chile). They live in densely forested areas near bodies of water, such as lakes, rivers, swamps, ponds, and marshes, as well as flooded savannah and along rivers in tropical forest. Capybara have flourished in cattle ranches. They roam in home ranges averaging 10 hectares (25 acres) in high-density populations.
Many escapees from captivity can also be found in similar watery habitats around the world. Sightings are fairly common in Florida, although a breeding population has not yet been confirmed. In 2011, one was spotted in the Central Coast of California.
Diet and predation.
Capybaras are herbivores, grazing mainly on grasses and aquatic plants, as well as fruit and tree bark. They are very selective feeders and will feed on the leaves of one species and disregard other species surrounding it. They eat a greater variety of plants during the dry season, as fewer plants are available. While they eat grass during the wet season, they have to switch to more abundant reeds during the dry season. Plants that capybaras eat during the summer lose their nutritional value in the winter and therefore are not consumed at that time. The capybara's jaw hinge is not perpendicular and they thus chew food by grinding back-and-forth rather than side-to-side. Capybaras are autocoprophagous, meaning they eat their own feces as a source of bacterial gut flora, to help digest the cellulose in the grass that forms their normal diet, and to extract the maximum protein and vitamins from their food. They may also regurgitate food to masticate again, similar to cud-chewing by a cow. As is the case with other rodents, the front teeth of capybaras grow continually to compensate for the constant wear from eating grasses; their cheek teeth also grow continuously.
Like its cousin the guinea pig, the capybara does not have the capacity to synthesize vitamin C, and capybaras not supplemented with vitamin C in captivity have been reported to develop gum disease as a sign of scurvy.
They can have a life span of 8–10 years on average, but live less than four years in the wild, as they are "a favourite food of jaguar, puma, ocelot, eagle and caiman". The capybara is also the preferred prey of the anaconda.
Social organization.
Capybaras are gregarious. While they sometimes live solitarily, they are more commonly found in groups that average 10–20 individuals, with two to four adult males, four to seven adult females, and the remainder juveniles. Capybara groups can consist of as many as 50 or 100 individuals during the dry season when the animals gather around available water sources. Males establish social bonds, dominance, or, general group census. They can make dog-like barks when threatened or when females are herding young.
Capybaras have two types of scent glands; a morillo, located on the snout, and anal glands. Both sexes have these glands, but males have much larger morillos and use their anal glands more frequently. The anal glands of males are also lined with detachable hairs. A crystalline form of scent secretion is coated on these hairs and is released when in contact with objects like plants. These hairs have a longer-lasting scent mark and are tasted by other capybaras. Capybara scent-mark by rubbing their morillo on an object, or by walking over scrub and marking it with their anal glands. Capybara can spread their scent further by urinating; however, females usually mark without urinating and scent-mark less frequently than males overall. Females mark more often during the wet season when they are in estrus. In addition to objects, males will also scent-mark females.
Reproduction.
When in estrus, the female's scent changes subtly and nearby males begin pursuit. In addition, a female will alert males she is in estrus by whistling though her nose. During mating, the female has the advantage and mating choice. Capybaras mate only in water, and if a female does not want to mate with a certain male, she will either submerge or leave the water. Dominant males are highly protective of the females, but they usually cannot prevent all the subordinates from copulating. The larger the group, the harder it is for the male to watch all the females. Dominant males secure significantly more matings than each subordinate, but subordinate males, as a class, are responsible for more matings than each dominant male. The lifespan of the capybara's sperm is longer than that of other rodents.
Capybara gestation is 130–150 days, and usually produces a litter of four capybara babies, but may produce between one and eight in a single litter. Birth is on land and the female will rejoin the group within a few hours of delivering the newborn capybaras, which will join the group as soon as they are mobile. Within a week, the young can eat grass, but will continue to suckle—from any female in the group—until weaned at about 16 weeks. The young will form a group within the main group. Alloparenting has been observed in this species. Breeding peaks between April and May in Venezuela and between October and November in Mato Grosso, Brazil.
Activities.
Though quite agile on land (capable of running as fast as a horse), capybaras are equally at home in the water. They are excellent swimmers, and can remain completely submerged for up to five minutes, an ability they use to evade predators. Capybaras can sleep in water, keeping only their noses out of the water. As temperatures increase during the day, they wallow in water and then graze during the late afternoon and early evening. They also spend a lot of time wallowing in mud. They rest around midnight and then continue to graze before dawn.
Conservation and human interaction.
Capybaras are not considered a threatened species; their population is stable throughout most of their South American range, though in some areas hunting has reduced their numbers.
Capybaras are hunted for their meat and pelts in some areas, and otherwise killed by humans who see their grazing as competition for livestock. In some areas, they are farmed, which has the effect of ensuring the wetland habitats are protected. Their survival is aided by their ability to breed rapidly.
Capybaras have adapted well to the urbanization in South America. They can be found in many areas in zoos and parks, and may live for 12 years in captivity. Capybaras are gentle and will usually allow humans to pet and hand-feed them, but physical contact is normally discouraged as their ticks can be vectors to Rocky Mountain spotted fever.
The European Association of Zoos and Aquaria (EAZA) tasked Drusillas Park in Alfriston, Sussex to keep the studbook for capybaras, to monitor captive populations in Europe. The studbook includes information about all births, deaths and movements of capybaras, as well as how they are related.
Capybaras are farmed for meat and skins in South America. The meat is considered unsuitable to eat in some areas, while in other areas it is considered an important source of protein. In parts of South America, especially in Venezuela, capybara meat is popular during Lent and Holy Week as the Catholic Church previously gave a special dispensation that allows for its consumption while other meats are generally forbidden.
Although it is illegal in some states, capybaras are occasionally kept as pets in the United States.

</doc>
<doc id="6777" url="https://en.wikipedia.org/wiki?curid=6777" title="Computer animation">
Computer animation

Computer animation, or CGI animation, is the process used for generating animated images. The more general term computer-generated imagery encompasses both static scenes and dynamic images, while computer animation "only" refers to the moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well.
Computer animation is essentially a digital successor to the stop motion techniques used in traditional animation with 3D models and frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other more physically based processes, constructing miniatures for effects shots or hiring extras for crowd scenes, and because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it, but advanced slightly in time (usually at a rate of 24 or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.
For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.
For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.
Explanation.
To trick the eye and the brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second or faster. (A frame is one complete image.) With rates above 75-120 frames per second, no improvement in realism or smoothness is perceivable due to the way the eye and the brain both process images. At rates below 12 frames per second, most people can detect jerkiness associated with the drawing of new images that detracts from the illusion of realistic movement. Conventional hand-drawn cartoon animation often uses 15 frames per second in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. To produce more realistic imagery, computer animation demands higher frame rates.
Films seen in theaters in the United States run at 24 frames per second, which is sufficient to create the illusion of continuous movement. For high resolution, adapters are used.
History.
Early digital computer animation was developed at Bell Telephone Laboratories in the 1960s by Edward E. Zajac, Frank W. Sinden, Kenneth C. Knowlton, and A. Michael Noll. Other digital animation was also practiced at the Lawrence Livermore National Laboratory.
An early step in the history of computer animation was the sequel to the 1973 film "Westworld," a science-fiction film about a society in which robots live and work among humans. The sequel, "Futureworld" (1976), used the 3D wire-frame imagery, which featured a computer-animated hand and face both created by University of Utah graduates Edwin Catmull and Fred Parke. This imagery originally appeared in their student film "A Computer Animated Hand", which they completed in 1971.
Developments in CGI technologies are reported each year at SIGGRAPH, an annual conference on computer graphics and interactive techniques that is attended by thousands of computer professionals each year. Developers of computer games and 3D video cards strive to achieve the same visual quality on personal computers in real-time as is possible for CGI films and animation. With the rapid advancement of real-time rendering quality, artists began to use game engines to render non-interactive movies, which led to the art form Machinima.
The very first full length computer animated television series was "ReBoot", which debuted in September 1994; the series followed the adventures of characters who lived inside a computer. The first feature-length computer animated film was "Toy Story" (1995), which was made by Pixar. It followed an adventure centered around toys and their owners. This groundbreaking film was also the first of many fully computer-animated movies.
Computer animation helped to create blockbuster films, "Toy Story 3" (2010), "Avatar" (2009), "Shrek 2" (2004), "Cars 2" (2011), "Life of Pi" (2012), "Frozen" (2013), and "Inside Out" (2015).
Animation methods.
In most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure. The position of each segment of the skeletal model is defined by animation variables, or Avars for short. In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, with facial features (though other methods for facial animation exist). The character "Woody" in "Toy Story", for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame.
There are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly. Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or tween between them in a process called keyframing. Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.
In contrast, a newer method called motion capture makes use of live action footage. When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated. His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.
Each method has its advantages and as of 2007, games and films are using either or both of these methods in productions. Keyframe animation can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor. For example, in the 2006 film "", Bill Nighy provided the performance for the character Davy Jones. Even though Nighy doesn't appear in the movie himself, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed what can be done throughout the conventional costuming.
Modeling.
3D computer animation combines 3D models of objects and programmed or hand "keyframed" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with "textures" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as rigging, the virtual marionette is given various controllers and handles for controlling movement. Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.
3D models rigged for animation may contain thousands of control points — for example, "Woody" from "Toy Story" uses 700 specialized animation controllers. Rhythm and Hues Studios labored for two years to create Aslan in the movie "", which had about 1,851 controllers (742 in the face alone). In the 2004 film "The Day After Tomorrow", designers had to design forces of extreme weather with the help of video references and accurate meteorological facts. For the 2005 remake of "King Kong", actor Andy Serkis was used to help designers pinpoint the gorilla's prime location in the shots and used his expressions to model "human" characteristics onto the creature. Serkis had earlier provided the voice and performance for Gollum in J. R. R. Tolkien's "The Lord of the Rings" trilogy.
Equipment.
Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can take a lot of time on an ordinary home computer. Because of this, video game animators tend to use low resolution and low polygon count renders so that the graphics can be rendered in real time on a home computer. Photorealistic animation would be impractical in this context.
Professional animators of movies, television, and video sequences on computer games make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used. Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. A large number of workstations (known as a "render farm") are networked together to effectively act as a giant computer. The result is a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically-advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation.
Facial animation.
The realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables. Historically speaking, the first SIGGRAPH tutorials on "State of the art in Facial Animation" in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.
The Facial Action Coding System (with 46 "action units", "lip bite" or "squint"), which had been developed in 1976, became a popular basis for many systems. As early as 2001, MPEG-4 included 68 Face Animation Parameters (FAPs) for lips, jaws, etc., and the field has made significant progress since then and the use of facial microexpression has increased.
In some cases, an affective space, the PAD emotional state model, can be used to assign specific emotions to the faces of avatars. In this approach, the PAD model is used as a high level emotional space and the lower level space is the MPEG-4 Facial Animation Parameters (FAP). A mid-level Partial Expression Parameters (PEP) space is then used to in a two-level structure – the PAD-PEP mapping and the PEP-FAP translation model.
Realism.
Realism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph or make the characters' animation believable and lifelike. Computer animation can also be realistic with or without the photorealistic rendering.
One of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Many animated films instead feature characters who are anthropomorphic animals ("Finding Nemo", "Ice Age", "Bolt", "Madagascar", "Over the Hedge", "Rio", "Kung Fu Panda", "Alpha and Omega", "Zootopia"), machines ("Cars", "WALL-E", "Robots"), insects ("Antz", "A Bug's Life", "The Ant Bully", "Bee Movie"), fantasy creatures and characters ("Monsters, Inc.", "Shrek", "TMNT", "Brave", "Epic"), or humans with non-realistic, cartoon-like proportions ("Despicable Me", "Up", "Megamind", "", "Planet 51", "Hotel Transylvania", "Cloudy with a Chance of Meatballs"). Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where the human audience (up to a point) tends to have an increasingly negative, emotional response as a human replica looks and acts more and more human. Also, some materials that commonly appear in a scene (such as cloth, foliage, fluids, and hair) have proven more difficult to faithfully recreate and animate than others. Consequently, special software and techniques have been developed to better simulate these specific elements.
In theory, realistic computer animation can reach a point where it is indistinguishable from real action captured on film. When computer animation achieves this level of realism, it may have major repercussions for the film industry.
The goal of computer animation is not always to emulate live action as closely as possible. For example, animation was used in the Nautilus Productions documentary "Mystery Mardi Gras Shipwreck" to model a remotely operated underwater vehicle (ROV) and the "Mardi Gras" archaeological site in of water in the Gulf of Mexico. Computer animation can also be tailored to mimic or substitute for other kinds of animation, traditional stop-motion animation (as shown in "Flushed Away" or "The Lego Movie"). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.
Films.
CGI short films have been produced as independent animation since 1976, although the popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation. The first completely computer-animated television series was "ReBoot" in 1994, and the first completely computer-animated movie was "Toy Story" (1995).
Amateur animation.
The popularity of websites that allow members to upload their own movies for others to view has created a growing community of amateur computer animators. With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open source animation software applications exist as well. A popular amateur approach to animation is via the animated GIF format, which can be uploaded and seen on the web easily.
Detailed examples and pseudocode.
In 2D computer animation, moving objects are often referred to as "sprites." A sprite is an image that has a location associated with it. The location of the sprite is changed slightly, between each displayed frame, to make the sprite appear to move. The following pseudocode makes a sprite move from left to right:
Computer animation uses different techniques to produce animations. Most frequently, sophisticated mathematics is used to manipulate complex three-dimensional polygons, apply "textures", lighting and other effects to the polygons and finally rendering the complete image. A sophisticated graphical user interface may be used to create the animation and arrange its choreography. Another technique called constructive solid geometry defines objects by conducting boolean operations on regular shapes, and has the advantage that animations may be accurately produced at any resolution.
Computer-assisted vs computer-generated.
"To animate means "to give life to" and there are two basic ways that animators commonly do this."
Computer-assisted animation is usually classed as two-dimensional (2D) animation. Creators drawings either hand drawn (pencil to paper) or interactively drawn(drawn on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package the creator will place drawings into different key frames which fundamentally create an outline of the most important movements. The computer will then fill in all the " in-between frames", commonly known as Tweening. Computer-assisted animation is basically using new technologies to cut down the time scale that traditional animation could take, but still having the elements of traditional drawings of characters or objects.
Two examples of films using computer-assisted animation are "Beauty and the Beast" and "Antz".
Computer-generated animation is known as 3-dimensional (3D) animation. Creators will design an object or character with an X,Y and Z axis. Unlike the traditional way of animation no pencil to paper drawings create the way computer generated animation works. The object or character created will then be taken into a software, key framing and tweening are also carried out in computer generated animation but are also a lot of techniques used that do not relate to traditional animation. Animators can break physical laws by using mathematical algorithms to cheat, mass, force and gravity rulings. Fundamentally, time scale and quality could be said to be a preferred way to produce animation as they are two major things that are enhanced by using computer generated animation. Another great aspect of CGA is the fact you can create a flock of creatures to act independently when created as a group. An animal's fur can be programmed to wave in the wind and lie flat when it rains instead of programming each strand of hair separately.
A few examples of computer-generated animation movies are "Toy Story", "Tangled", "Frozen", "Inside Out", "Shrek", and "Finding Nemo".
External links.
Animated images on "Wikimedia Commons":

</doc>
<doc id="6778" url="https://en.wikipedia.org/wiki?curid=6778" title="Ceawlin of Wessex">
Ceawlin of Wessex

Ceawlin (also spelled Ceaulin and Caelin, died "ca." 593) was a King of Wessex. He may have been the son of Cynric of Wessex and the grandson of Cerdic of Wessex, whom the "Anglo-Saxon Chronicle" represents as the leader of the first group of Saxons to come to the land which later became Wessex. Ceawlin was active during the last years of the Anglo-Saxon expansion, with little of southern England remaining in the control of the native Britons by the time of his death.
The chronology of Ceawlin's life is highly uncertain. The historical accuracy and dating of many of the events in the later "Anglo-Saxon Chronicle" have been called into question, and his reign is variously listed as lasting seven, seventeen, or thirty-two years. The "Chronicle" records several battles of Ceawlin's between the years 556 and 592, including the first record of a battle between different groups of Anglo-Saxons, and indicates that under Ceawlin Wessex acquired significant territory, some of which was later to be lost to other Anglo-Saxon kingdoms. Ceawlin is also named as one of the eight ""bretwaldas"", a title given in the "Chronicle" to eight rulers who had overlordship over southern Britain, although the extent of Ceawlin's control is not known.
Ceawlin died in 593, having been deposed the year before, possibly by his successor, Ceol. He is recorded in various sources as having two sons, Cutha and Cuthwine, but the genealogies in which this information is found are known to be unreliable.
Historical context.
The history of the sub-Roman period in Britain is poorly sourced and the subject of a number of important disagreements among historians. It appears, however, that in the fifth century raids on Britain by continental peoples developed into migrations. The newcomers included Angles, Saxons, Jutes, and Frisians. These peoples captured territory in the east and south of England, but at about the end of the fifth century, a British victory at the battle of Mons Badonicus halted the Anglo-Saxon advance for fifty years. Near the year 550, however, the British began to lose ground once more, and within twenty-five years, it appears that control of almost all of southern England was in the hands of the invaders.
The peace following the battle of Mons Badonicus is attested partly by Gildas, a monk, who wrote "De Excidio et Conquestu Britanniae" or "On the Ruin and Conquest of Britain" during the middle of the sixth century. This essay is a polemic against corruption and Gildas provides little in the way of names and dates. He appears, however, to state that peace had lasted from the year of his birth to the time he was writing. The "Anglo-Saxon Chronicle" is the other main source that bears on this period, in particular in an entry for the year 827 that records a list of the kings who bore the title ""bretwalda"", or "Britain-ruler". That list shows a gap in the early sixth century that matches Gildas's version of events.
Ceawlin's reign belongs to the period of Anglo-Saxon expansion at the end of the sixth century. Though there are many unanswered questions about the chronology and activities of the early West Saxon rulers, it is clear that Ceawlin was one of the key figures in the final Anglo-Saxon conquest of southern Britain.
Early West Saxon sources.
The two main written sources for early West Saxon history are the "Anglo-Saxon Chronicle" and the West Saxon Genealogical Regnal List. The "Chronicle" is a set of annals which were compiled near the year 890, during the reign of King Alfred the Great of Wessex. They record earlier material for the older entries, which were assembled from earlier annals that no longer survive, as well as, from saga material that might have been transmitted orally. The "Chronicle" dates the arrival of the future "West Saxons" in Britain to 495, when Cerdic and his son, Cynric, land at "Cerdices ora", or Cerdic's shore. Almost twenty annals describing Cerdic's campaigns, and those of his descendants appear interspersed through the next hundred years of entries in the "Chronicle". Although these annals provide most of what is known about Ceawlin, the historicity of many of the entries is uncertain.
The West Saxon Genealogical Regnal List is a list of rulers of Wessex, including the lengths of their reigns. It survives in several forms, including as a preface to the manuscript of the "Chronicle". As with "Chronicle", the list was compiled during the reign of Alfred the Great, and both the list and the "Chronicle" are influenced by the desire of their writers to use a single line of descent to trace the lineage of the Kings of Wessex through Cerdic to Gewis, a descendant of Woden and the legendary ancestor of the West Saxons. The result served the political purposes of the scribe, but is riddled with contradictions for historians.
The contradictions may be seen clearly by calculating dates by different methods from the various sources. The first event in West Saxon history, the date of which can be regarded as reasonably certain, is the baptism of Cynegils, which occurred in the late 630s, perhaps as late as 640. The "Chronicle" dates Cerdic's arrival to 495, but adding up the lengths of the reigns as given in the West Saxon Genealogical Regnal List, leads to the conclusion that Cerdic's reign might have started in 532, a difference of 37 years. Neither 495 nor 532 may be treated as reliable, however, the latter date relies on the presumption that the Regnal List is correct in presenting the Kings of Wessex as having succeeded one another, with no omitted kings, no joint kingships, and that the durations of the reigns are correct as given. None of these presumptions may be made safely.
The sources also are inconsistent on the length of Ceawlin's reign. The "Chronicle" gives it as thirty-two years, from 560 to 592, but the Regnal Lists disagree: different versions give it as seven or seventeen years. A recent detailed study of the Regnal List dates the arrival of the West Saxons in England to 538, and favours seven years as the most likely length of Ceawlin's reign, with dates of 581–588 proposed. The sources do agree that Ceawlin is the son of Cynric and he usually is named as the father of Cuthwine. There is one discrepancy in this case: the entry for 685 in the version of the "Chronicle" assigns Ceawlin a son, Cutha, but in the 855 entry in the same manuscript, Cutha is listed as the son of Cuthwine. Cutha also is named as Ceawlin's brother in the [E and versions of the "Chronicle", in the 571 and 568 entries, respectively.
Whether Ceawlin is a descendant of Cerdic is a matter of debate. Subgroupings of different West Saxon lineages give the impression of separate groups, of which Ceawlin's line is one. Some of the problems in the Wessex genealogies may have come about because of efforts to integrate Ceawlin's line with the other lineages: it was very important to the West Saxons to be able to trace their ancestors back to Cerdic. Another reason for doubting the literal nature of these early genealogies is that the etymology of the names of several early members of the dynasty do not appear to be Germanic, as would be expected in the names of leaders of an apparently Anglo-Saxon dynasty. The name Ceawlin is one of the names that do not have convincing Anglo-Saxon etymologies; it seems more likely to be of native British origin.
The earliest sources do not use the term "West Saxon". According to Bede's "Ecclesiastical History of the English People", the term is interchangeable with the Gewisse. The term "West Saxon" appears only in the late seventh century, after the reign of Cædwalla.
West Saxon expansion.
Ultimately, the kingdom of Wessex occupied the southwest of England, but the initial stages in this expansion are not apparent from the sources. Cerdic's landing, whenever it is to be dated, seems to have been near the Isle of Wight, and the annals record the conquest of the island in 530. In 534, according to the "Chronicle", Cerdic died and his son Cynric took the throne; the "Chronicle" adds that "they gave the Isle of Wight to their nephews, Stuf and Wihtgar". These records are in direct conflict with Bede, who states that the Isle of Wight was settled by Jutes, not Saxons; the archaeological record is somewhat in favour of Bede on this.
Subsequent entries in the "Chronicle" give details of some of the battles by which the West Saxons won their kingdom. Ceawlin's campaigns are not given as near the coast. They range along the Thames valley and beyond, as far as Surrey in the east and the mouth of the Severn in the west. Ceawlin clearly is part of the West Saxon expansion, but the military history of the period is difficult to understand. In what follows the dates are as given in the "Chronicle", although as noted above, these are earlier than now thought accurate.
556: Beran byrg.
The first record of a battle fought by Ceawlin is in 556, when he and his father, Cynric, fought the native Britons at "Beran byrg", or Bera's Stronghold. This now is identified as Barbury Castle, an Iron Age hill fort in Wiltshire, near Swindon. Cynric would have been king of Wessex at this time.
568: Wibbandun.
The first battle Ceawlin fought as king is dated by the "Chronicle" to 568, when he and Cutha fought with Æthelberht, the king of Kent. The entry says "Here Ceawlin and Cutha fought against Aethelberht and drove him into Kent; and they killed two ealdormen, Oslaf and Cnebba, on Wibbandun." The location of "Wibbandun", which can be translated as "Wibba's Mount", has not been identified definitely; it was at one time thought to be Wimbledon, but this now is known to be incorrect. This battle is notable as the first recorded conflict between the invading peoples: previous battles recorded in the "Chronicle" are between the Anglo-Saxons and the native Britons.
There are multiple examples of joint kingship in Anglo-Saxon history, and this may be another: it is not clear what Cutha's relationship to Ceawlin is, but it certainly is possible he was also a king. The annal for 577, below, is another possible example.
571: Bedcanford.
The annal for 571 reads: "Here Cuthwulf fought against the Britons at Bedcanford, and took four settlements: Limbury and Aylesbury, Benson and Eynsham; and in the same year he passed away." Cuthwulf's relationship with Ceawlin is unknown, but the alliteration common to Anglo-Saxon royal families suggests Cuthwulf may be part of the West Saxon royal line. The location of the battle itself is unidentified. It has been suggested that it was Bedford, but what is known of the early history of Bedford's names, does not support this. This battle is of interest because it is surprising that an area so far east should still be in Briton hands this late: there is ample archaeological evidence of early Saxon and Anglian presence in the Midlands, and historians generally have interpreted Gildas's "De Excidio" as implying that the Britons had lost control of this area by the mid-sixth century. One possible explanation is, that this annal records a reconquest of land that was lost to the Britons in the campaigns ending in the battle of Mons Badonicus.
577: The lower Severn.
The annal for 577 reads "Here Cuthwine and Ceawlin fought against the Britons, and they killed three kings, Coinmail and Condidan and Farinmail, in the place which is called Dyrham, and took three cities: Gloucester and Cirencester and Bath." This entry is all that is known of these Briton kings; their names are in an archaic form that makes it very likely that this annal derives from a much older written source. The battle itself has long been regarded as a key moment in the Saxon advance, since in reaching the Bristol Channel, the West Saxons divided the Britons west of the Severn from land communication with those in the peninsula to the south of the Channel. Wessex almost certainly lost this territory to Penda of Mercia in 628, when the "Chronicle" records that "Cynegils and Cwichelm fought against Penda at Cirencester and then came to an agreement."
It is possible that when Ceawlin and Cuthwine took Bath, they found the Roman baths still operating to some extent. Nennius, a ninth-century historian, mentions a "Hot Lake" in the land of the Hwicce, which was along the Severn, and adds "It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot". Bede also describes hot baths in the geographical introduction to the "Ecclesiastical History" in terms very similar to those of Nennius.
Wansdyke, an early medieval defensive linear earthwork, runs from south of Bristol to near Marlborough, Wiltshire, passing not far from Bath. It probably was built in the fifth or sixth centuries, perhaps by Ceawlin.
584: Fethan leag.
Ceawlin's last recorded victory is in 584. The entry reads "Here Ceawlin and Cutha fought against the Britons at the place which is named Fethan leag, and Cutha was killed; and Ceawlin took many towns and countless war-loot, and in anger he turned back to his own ." There is a wood named "Fethelée" mentioned in a twelfth-century document that relates to Stoke Lyne, in Oxfordshire, and it now is thought that the battle of Fethan leag must have been fought in this area.
The phrase "in anger he turned back to his own" probably indicates that this annal is drawn from saga material, as perhaps are all of the early Wessex annals. It also has been used to argue that perhaps, Ceawlin did not win the battle and that the chronicler chose not to record the outcome fully – a king does not usually come home "in anger" after taking "many towns and countless war-loot". It may be that Ceawlin's overlordship of the southern Britons came to an end with this battle.
Bretwaldaship.
About 731, Bede, a Northumbrian monk and chronicler, wrote a work called the "Ecclesiastical History of the English People". The work was not primarily a secular history, but Bede provides much information about the history of the Anglo-Saxons, including a list early in the history of seven kings who, he said, held "imperium" over the other kingdoms south of the Humber. The usual translation for "imperium" is "overlordship". Bede names Ceawlin as the second on the list, although he spells it "Caelin", and adds that he was "known in the speech of his own people as Ceaulin". Bede also makes it clear that Ceawlin was not a Christian—Bede mentions a later king, Æthelberht of Kent, as "the first to enter the kingdom of heaven".
The "Anglo-Saxon Chronicle," in an entry for the year 827, repeats Bede's list, adds Egbert of Wessex, and also mentions that they were known as "bretwalda", or "Britain-ruler". A great deal of scholarly attention has been given to the meaning of this word. It has been described as a term "of encomiastic poetry", but there also is evidence that it implied a definite role of military leadership.
Bede says that these kings had authority "south of the Humber", but the span of control, at least of the earlier bretwaldas, likely was less than this. In Ceawlin's case the range of control is hard to determine accurately, but Bede's inclusion of Ceawlin in the list of kings who held "imperium", and the list of battles he is recorded as having won, indicate an energetic and successful leader who, from a base in the upper Thames valley, dominated much of the surrounding area and held overlordship over the southern Britons for some period. Despite Ceawlin's military successes, the northern conquests he made could not always be retained: Mercia took much of the upper Thames valley, and the north-eastern towns won in 571 were among territory subsequently under the control of Kent and Mercia at different times.
Bede's concept of the power of these overlords also must be regarded as the product of his eighth-century viewpoint. When the "Ecclesiastical History" was written, Æthelbald of Mercia dominated the English south of the Humber, and Bede's view of the earlier kings was doubtless strongly coloured by the state of England at that time. For the earlier "bretwaldas", such as Ælle and Ceawlin, there must be some element of anachronism in Bede's description. It also is possible that Bede only meant to refer to power over Anglo-Saxon kingdoms, not the native Britons.
Ceawlin is the second king in Bede's list. All the subsequent bretwaldas followed more or less consecutively, but there is a long gap, perhaps fifty years, between Ælle of Sussex, the first bretwalda, and Ceawlin. The lack of gaps between the overlordships of the later bretwaldas has been used to make an argument for Ceawlin's dates matching the later entries in the "Chronicle" with reasonable accuracy. According to this analysis, the next bretwalda, Æthelberht of Kent, must have been already a dominant king by the time Pope Gregory the Great wrote to him in 601, since Gregory would have not written to an underking. Ceawlin defeated Æthelberht in 568 according to the "Chronicle". Æthelberht's dates are a matter of debate, but recent scholarly consensus has his reign starting no earlier than 580. The 568 date for the battle at Wibbandun is thought to be unlikely because of the assertion in various versions of the West Saxon Genealogical Regnal List that Ceawlin's reign lasted either seven or seventeen years. If this battle is placed near the year 590, before Æthelberht has established himself as a powerful king, then the subsequent annals relating to Ceawlin's defeat and death may be reasonably close to the correct date. In any case, the battle with Æthelberht is unlikely to have been more than a few years on either side of 590. The gap between Ælle and Ceawlin, on the other hand, has been taken as supporting evidence for the story told by Gildas in "De Excidio" of a peace lasting a generation or more following a Briton victory at Mons Badonicus.
Æthelberht of Kent succeeds Ceawlin on the list of bretwaldas, but the reigns may overlap somewhat: recent evaluations give Ceawlin a likely reign of 581–588, and place Æthelberht's accession near to the year 589, but these analyses are no more than scholarly guesses. Ceawlin's eclipse in 592, probably by Ceol, may have been the occasion for Æthelberht to rise to prominence; Æthelberht very likely was the dominant Anglo-Saxon king by 597. Æthelberht's rise may have been earlier: the 584 annal, even if it records a victory, is the last victory of Ceawlin's in the "Chronicle", and the period after that may have been one of Æthelberht's ascent and Ceawlin's decline.
Wessex at Ceawlin's death.
Ceawlin lost the throne of Wessex in 592. The annal for that year reads, in part: "Here there was great slaughter at Woden's Barrow, and Ceawlin was driven out." Woden's Barrow is a tumulus, now called Adam's Grave, at Alton Priors, Wiltshire. No details of his opponent are given. The medieval chronicler William of Malmesbury, writing in about 1120, says that it was "the Angles and the British conspiring together". Alternatively, it may have been Ceol, who is supposed to have been the next king of Wessex, ruling for six years according to the West Saxon Genealogical Regnal List. According to the "Anglo-Saxon Chronicle", Ceawlin died the following year. The relevant part of the annal reads: "Here Ceawlin and Cwichelm and Crida perished." Nothing more is known of Cwichelm and Crida, although they may have been members of the Wessex royal house – their names fit the alliterative pattern common to royal houses of the time.
According to the Regnal List, Ceol was a son of Cutha, who was a son of Cynric; and Ceolwulf, his brother, reigned for seventeen years after him. It is possible that some fragmentation of control among the West Saxons occurred at Ceawlin's death: Ceol and Ceolwulf may have been based in Wiltshire, as opposed to the upper Thames valley. This split also may have contributed to Æthelberht's ability to rise to dominance in southern England. The West Saxons remained influential in military terms, however: the "Chronicle" and Bede record continued military activity against Essex and Sussex within twenty or thirty years of Ceawlin's death.

</doc>
<doc id="6779" url="https://en.wikipedia.org/wiki?curid=6779" title="Christchurch (disambiguation)">
Christchurch (disambiguation)

Christchurch is the third largest urban area in New Zealand. 
Christchurch may also refer to:

</doc>
<doc id="6780" url="https://en.wikipedia.org/wiki?curid=6780" title="CD-R">
CD-R

CD-R (Compact Disc-Recordable) is a digital optical disc storage format. A CD-R disc is a compact disc that can be written once and read arbitrarily many times.
CD-R disks (CD-Rs) are readable by most plain CD readers, i.e., CD readers manufactured prior to the introduction of CD-R. This is an advantage over CD-RW, which can be re-written but cannot be played on many plain CD readers.
History.
The CD-R,originally named CD Write-Once (WO), specification was first published in 1988 by Philips and Sony in the 'Orange Book'. The Orange Book consists of several parts, furnishing details of the CD-WO, CD-MO (Magneto-Optic), and CD-RW (ReWritable). The latest editions have abandoned the use of the term "CD-WO" in favor of "CD-R", while "CD-MO" were used very little. Written CD-Rs and CD-RWs are, in the aspect of low-level encoding and data format, fully compatible with the audio CD ("Red Book" CD-DA) and data CD ("Yellow Book" CD-ROM) standards. (Note that the Yellow Book standard for CD-ROM only specifies a high-level data format and refers to the Red Book for all physical format and low-level code details, such as track pitch, linear bit density, and bitstream encoding.) This means they use Eight-to-Fourteen Modulation, CIRC error correction, and, for CD-ROM, the third error correction layer defined in the Yellow Book. Properly written CD-R discs on blanks of less than 80 minutes length are fully compatible with the audio CD and CD-ROM standards in all details including physical specifications. 80 minute CD-R discs marginally violate the Red Book physical format specifications, and longer discs are noncompliant. CD-RW discs have lower reflectivity than CD-R or pressed (non-writable) CDs and for this reason cannot meet the Red Book standard (or come close). Some hardware compatible with Red Book CDs may have difficulty reading CD-Rs and, because of their lower reflectivity, especially CD-RWs. To the extent that CD hardware can read extended-length discs or CD-RW discs, it is because that hardware has capability beyond the minimum required by the Red Book and Yellow Book standards (the hardware is more capable than it needs to be to bear the Compact Disc logo).
CD-R recording systems available in 1990 were similar to the washing machine-sized Meridian CD Publisher, based on the two-piece rack mount Yamaha PDS audio recorder costing $35,000, not including the required external ECC circuitry for data encoding, SCSI hard drive subsystem, and MS-DOS control computer. By 1992, the cost of typical recorders was down to $10–12,000, and in September 1995, Hewlett-Packard introduced its model 4020i manufactured by Philips, which, at $995, was the first recorder to cost less than $1000.
The dye materials developed by Taiyo Yuden made it possible for CD-R discs to be compatible with Audio CD and CD-ROM discs.
Initially, in the United States, there was a market separation between "music" CD-Rs and "data" CD-Rs, the former being several times more expensive than the latter due to industry copyright arrangements with the RIAA. Physically, there is no difference between the discs save for the Disc Application Flag that identifies their type: standalone audio recorders will only accept "music" CD-Rs to enforce the RIAA arrangement, while computer CD-R drives can use either type of media to burn either type of content.
Physical characteristics.
A standard CD-R is a thick disc made of polycarbonate about 120 mm (4.7 in) or 80 mm (3.150 in) diameter. The 120 mm disc has a storage capacity of 74 minutes of audio or 650 Megabytes of data. CD-R/RWs are available with capacities of 80 minutes of audio or 737,280,000 bytes (700 MiB), which they achieve by molding the disc at the tightest allowable tolerances specified in the Orange Book CD-R/CD-RW standards. The engineering margin that was reserved for manufacturing tolerance has been used for data capacity instead, leaving no tolerance for manufacturing; for these discs to be truly compliant with the Orange Book standard, the manufacturing process must be perfect .
Despite the foregoing, most CD-Rs on the market have an 80-minute capacity. There are also 90 minute/790 MiB and 99 minute/870 MiB discs, although they are less common (and depart from the Orange Book standard outright). Also, due to the limitations of the data structures in the ATIP (see below), 90 and 99 minute blanks will identify as 80 minute ones. (As the ATIP is part of the Orange Book standard, it is natural that its design does not support some nonstandard disc configurations.) Therefore, in order to use the additional capacity, these discs have to be burned using "overburn" options in the CD recording software. (Overburning itself is so named because it is outside the written standards, but, due to market demand, it has nonetheless become a de facto standard function in most CD writing drives and software for them.)
Some drives use special techniques, such as Plextor's GigaRec or Sanyo's HD-BURN, to write more data onto a given disc; these techniques are inherently deviations from the Compact Disc (Red, Yellow, and/or Orange Book) standards, making the recorded discs proprietary-formatted and not fully compatible with standard CD players and drives. However, in certain applications where discs will not be distributed or exchanged outside a private group and will not be archived for a long time, a proprietary format may be an acceptable way to obtain greater capacity (up to 1.2 GiB with GigaRec or 1.8 GiB with HD-BURN on 99 minute media). The greatest risk in using such a proprietary data storage format, assuming that it works reliably as designed, is that it may be difficult or impossible to repair or replace the hardware used to read the media if it fails, is damaged, or is lost after its original vendor discontinues it.
Nothing in the Red, Yellow or Orange Book standards prohibits disc reading/writing devices from having the capacity to read or write discs beyond the Compact Disc standards. The standards do require discs to meet precise requirements in order to be called Compact Discs, but the other discs may be called by other names; if this were not true, no DVD drive could legally bear the Compact Disc logo. While disc players and drives may have capabilities beyond the standards, enabling them to read and write nonstandard discs, there is no assurance, in the absence of explicit additional manufacturer specifications beyond normal Compact Disc logo certification, that any particular player or drive will perform beyond the standards at all or consistently. Furthermore, if the same device with no explicit performance specs beyond the Compact Disc logo initially handles nonstandard discs reliably, there is no assurance that it will not later stop doing so, and in that case, there is no assurance that it can be made to do so again by service or adjustment. Therefore, discs with capacities larger than 650 MB, and especially those larger than 700 MB, are less interchangeable among players/drives than standard discs and are not very suitable for archival use, as their readability on future equipment, or even on the same equipment at a future time, is not assured, even under the assumption that the discs will not degrade at all.
The polycarbonate disc contains a spiral groove, called the "pregroove" (because it is molded in before data are written to the disc), to guide the laser beam upon writing and reading information. The pregroove is molded into the top side of the polycarbonate disc, where the pits and lands would be molded if it were a pressed (nonrecordable) Red Book CD; the bottom side, which faces the laser beam in the player or drive, is flat and smooth. The polycarbonate disc is coated on the pregroove side with a very thin layer of organic dye. Then, on top of the dye is coated a thin, reflecting layer of silver, a silver alloy, or gold. Finally, a protective coating of a photo-polymerizable lacquer is applied on top of the metal reflector and cured with UV-light.
A blank CD-R is not "empty"; the pregroove has a wobble (the ATIP), which helps the writing laser to stay on track and to write the data to the disc at a constant rate. Maintaining a constant rate is essential to ensure proper size and spacing of the pits and lands burned into the dye layer. As well as providing timing information, the ATIP (absolute time in pregroove) is also a data track containing information about the CD-R manufacturer, the dye used and media information (disc length and so on). The pregroove is not destroyed when the data are written to the CD-R, a point which some copy protection schemes use to distinguish copies from an original CD.
There are three basic formulations of dye used in CD-Rs:
There are many hybrid variations of the dye formulations, such as Formazan by Kodak (a hybrid of cyanine and phthalocyanine).
Unfortunately, many manufacturers have added additional coloring to disguise their unstable cyanine CD-Rs in the past, so the formulation of a disc cannot be determined based purely on its color. Similarly, a gold reflective layer does not guarantee use of phthalocyanine dye. The quality of the disc is also not only dependent on the dye used, it is also influenced by sealing, the top layer, the reflective layer, and the polycarbonate. Simply choosing a disc based on its dye type may be problematic. Furthermore, correct power calibration of the laser in the writer, as well as correct timing of the laser pulses, stable disc speed, and so on, is critical to not only the immediate readability but the longevity of the recorded disc, so for archiving it is important to have not only a high quality disc but a high quality writer. In fact, a high quality writer may produce adequate results with medium quality media, but high quality media cannot compensate for a mediocre writer, and discs written by such a writer cannot achieve their maximum potential archival lifetime.
Speed.
These times only include the actual optical writing pass over the disc. For most disc recording operations, additional time is used for overhead processes, such as organizing the files and tracks, which adds to the theoretical minimum total time required to produce a disc. (An exception might be making a disk from a prepared ISO image, for which the overhead would likely be trivial.) At the lowest write speeds, this overhead takes so much less time than the actual disc writing pass that it may be negligible, but at higher write speeds, the overhead time becomes a larger proportion of the overall time taken to produce a finished disc and may add significantly to it.
Also, above 20× speed, drives use a Zoned-CLV or CAV strategy, where the advertised maximum speed is only reached near the outer rim of the disc. This is not taken into account by the above table. (If this were not done, the faster rotation that would be required at the inner tracks could cause the disc to fracture and/or could cause excessive vibration which would make accurate and successful writing impossible.)
Writing methods.
The blank disc has a pre-groove track onto which the data are written. The pre-groove track, which also contains timing information, ensures that the recorder follows the same spiral
path as a conventional CD. A CD recorder writes data to a CD-R disc by pulsing its laser to heat areas of the organic dye layer. The writing process does not produce indentations (pits); instead, the heat permanently changes the optical properties of the dye, changing the reflectivity of those areas. Using a low laser power, so as not to further alter the dye, the disc is read back in the same way as a CD-ROM. However, the reflected light is modulated not by pits, but by the alternating regions of heated and unaltered dye. The change of the intensity of the reflected laser radiation is transformed into an electrical signal, from which the digital information is recovered ("decoded"). Once a section of a CD-R is written, it cannot be erased or rewritten, unlike a CD-RW. A CD-R can be recorded in multiple sessions.
A CD recorder can write to a CD-R using several methods including:
With careful examination, the written and unwritten areas can be distinguished by the naked eye. CD-Rs are written from the center outwards, so the written area appears as an inner band with slightly different shading.
Lifespan.
Real-life (not accelerated aging) tests have revealed that some CD-Rs degrade quickly even if stored normally. The quality of a CD-R disc has a large and direct influence on longevity—low quality discs should not be expected to last very long. According to research conducted by J. Perdereau, CD-Rs are expected to have an average life expectancy of 10 years. Branding isn't a reliable guide to quality, because many brands (major as well as no name) do not manufacture their own discs. Instead they are sourced from different manufacturers of varying quality. For best results, the actual manufacturer and material components of each batch of discs should be verified.
As well as degradation of the dye, failure of a CD-R can be due to the reflective surface. While silver is less expensive and more widely used, it is more prone to oxidation resulting in a non-reflecting surface. Gold on the other hand, although more expensive and no longer widely used, is an inert material, so gold-based CD-Rs do not suffer from this problem. Manufacturers have estimated the longevity of gold-based CD-Rs to be as high as 100 years.
Labeling.
It is recommended if using adhesive-backed paper labels that the labels be specially made for CD-Rs. A balanced CD vibrates only slightly when rotated at high speed. Bad or improperly made labels, or labels applied off-center, unbalance the CD and can cause it to vibrate when it spins, which causes read errors and even risks damaging the drive.
A professional alternative to CD labels is pre-printed CDs using a 5-color silkscreen or offset press. Using a permanent marker pen is also a common practice. However, solvents from such pens can affect the dye layer.
Disposal.
Data confidentiality.
Since CD-Rs in general cannot be logically erased to any degree, the disposal of CD-Rs presents a possible security issue if they contain sensitive / private data. Destroying the data requires physically destroying the disc or data layer. Heating the disc in a microwave oven for 10–15 seconds effectively destroys the data layer by causing arcing in the metal reflective layer, but this same arcing may cause damage or excessive wear to the microwave oven. Many office paper shredders are also designed to shred CDs.
Some recent burners (Plextor, LiteOn) support erase operations on -R media, by "overwriting" the stored data with strong laser power, although the erased area cannot be overwritten with new data.
Recycling.
The polycarbonate material and possible gold or silver in the reflective layer would make CD-Rs highly recyclable. However, the polycarbonate is of very little value and the quantity of precious metals is so small that it is not profitable to recover them. Consequently, recyclers that accept CD-Rs typically do not offer compensation for donating or transporting the materials.

</doc>
<doc id="6781" url="https://en.wikipedia.org/wiki?curid=6781" title="Cytosol">
Cytosol

The cytosol or cytoplasmic matrix is the liquid found inside cells. It constitutes most of the intracellular fluid (ICF). It is separated into compartments by membranes. For example, the mitochondrial matrix separates the mitochondrion into many compartments.
In the eukaryotic cell, the cytosol is within the cell membrane and is part of the cytoplasm, which also comprises the mitochondria, plastids, and other organelles (but not their internal fluids and structures); the cell nucleus is separate. The cytosol is thus a liquid matrix around the organelles. In prokaryotes, most of the chemical reactions of metabolism take place in the cytosol, while a few take place in membranes or in the periplasmic space. In eukaryotes, while many metabolic pathways still occur in the cytosol, others are contained within organelles.
The cytosol is a complex mixture of substances dissolved in water. Although water forms the large majority of the cytosol, its structure and properties within cells is not well understood. The concentrations of ions such as sodium and potassium are different in the cytosol than in the extracellular fluid; these differences in ion levels are important in processes such as osmoregulation, cell signaling, and the generation of action potentials in excitable cells such as endocrine, nerve and muscle cells. The cytosol also contains large amounts of macromolecules, which can alter how molecules behave, through macromolecular crowding.
Although it was once thought to be a simple solution of molecules, the cytosol has multiple levels of organization. These include concentration gradients of small molecules such as calcium, large complexes of enzymes that act together to carry out metabolic pathways, and protein complexes such as proteasomes and carboxysomes that enclose and separate parts of the cytosol.
Definition.
The term cytosol was first introduced in 1965 by H.A. Lardy, and initially referred to the liquid that was produced by breaking cells apart and pelleting all the insoluble components by ultracentrifugation. Such a soluble cell extract is not identical to the soluble part of the cell cytoplasm and is usually called a cytoplasmic fraction. The term "cytosol" is now used to refer to the liquid phase of the cytoplasm in an intact cell. This excludes any part of the cytoplasm that is contained within organelles. Due to the possibility of confusion between the use of the word "cytosol" to refer to both extracts of cells and the soluble part of the cytoplasm in intact cells, the phrase "aqueous cytoplasm" has been used to describe the liquid contents of the cytoplasm of living cells.
Properties and composition.
The proportion of cell volume that is cytosol varies: for example while this compartment forms the bulk of cell structure in bacteria, in plant cells the main compartment is the large central vacuole. The cytosol consists mostly of water, dissolved ions, small molecules, and large water-soluble molecules (such as proteins). The majority of these non-protein molecules have a molecular mass of less than 300 Da. This mixture of small molecules is extraordinarily complex, as the variety of molecules that are involved in metabolism (the metabolites) is immense. For example, up to 200,000 different small molecules might be made in plants, although not all these will be present in the same species, or in a single cell. Estimates of the number of metabolites in single cells such as "E. coli" and baker's yeast predict that under 1,000 are made.
Water.
Most of the cytosol is water, which makes up about 70% of the total volume of a typical cell. The pH of the intracellular fluid is 7.4. while human cytosolic pH ranges between 7.0 - 7.4, and is usually higher if a cell is growing. The viscosity of cytoplasm is roughly the same as pure water, although diffusion of small molecules through this liquid is about fourfold slower than in pure water, due mostly to collisions with the large numbers of macromolecules in the cytosol. Studies in the brine shrimp have examined how water affects cell functions; these saw that a 20% reduction in the amount of water in a cell inhibits metabolism, with metabolism decreasing progressively as the cell dries out and all metabolic activity halting when the water level reaches 70% below normal.
Although water is vital for life, the structure of this water in the cytosol is not well understood, mostly because methods such as nuclear magnetic resonance spectroscopy only give information on the average structure of water, and cannot measure local variations at the microscopic scale. Even the structure of pure water is poorly understood, due to the ability of water to form structures such as water clusters through hydrogen bonds.
The classic view of water in cells is that about 5% of this water is strongly bound in by solutes or macromolecules as water of solvation, while the majority has the same structure as pure water. This water of solvation is not active in osmosis and may have different solvent properties, so that some dissolved molecules are excluded, while others become concentrated. However, others argue that the effects of the high concentrations of macromolecules in cells extend throughout the cytosol and that water in cells behaves very differently from the water in dilute solutions. These ideas include the proposal that cells contain zones of low and high-density water, which could have widespread effects on the structures and functions of the other parts of the cell. However, the use of advanced nuclear magnetic resonance methods to directly measure the mobility of water in living cells contradicts this idea, as it suggests that 85% of cell water acts like that pure water, while the remainder is less mobile and probably bound to macromolecules.
Ions.
The concentrations of the other ions in cytosol are quite different from those in extracellular fluid and the cytosol also contains much higher amounts of charged macromolecules such as proteins and nucleic acids than the outside of the cell structure.
In contrast to extracellular fluid, cytosol has a high concentration of potassium ions and a low concentration of sodium ions. This difference in ion concentrations is critical for osmoregulation, since if the ion levels were the same inside a cell as outside, water would enter constantly by osmosis - since the levels of macromolecules inside cells are higher than their levels outside. Instead, sodium ions are expelled and potassium ions taken up by the Na⁺/K⁺-ATPase, potassium ions then flow down their concentration gradient through potassium-selection ion channels, this loss of positive charge creates a negative membrane potential. To balance this potential difference, negative chloride ions also exit the cell, through selective chloride channels. The loss of sodium and chloride ions compensates for the osmotic effect of the higher concentration of organic molecules inside the cell.
Cells can deal with even larger osmotic changes by accumulating osmoprotectants such as betaines or trehalose in their cytosol. Some of these molecules can allow cells to survive being completely dried out and allow an organism to enter a state of suspended animation called cryptobiosis. In this state the cytosol and osmoprotectants become a glass-like solid that helps stabilize proteins and cell membranes from the damaging effects of desiccation.
The low concentration of calcium in the cytosol allows calcium ions to function as a second messenger in calcium signaling. Here, a signal such as a hormone or an action potential opens calcium channels so that calcium floods into the cytosol. This sudden increase in cytosolic calcium activates other signalling molecules, such as calmodulin and protein kinase C. Other ions such as chloride and potassium may also have signaling functions in the cytosol, but these are not well understood.
Macromolecules.
Protein molecules that do not bind to cell membranes or the cytoskeleton are dissolved in the cytosol. The amount of protein in cells is extremely high, and approaches 200 mg/ml, occupying about 20-30% of the volume of the cytosol. However, measuring precisely how much protein is dissolved in cytosol in intact cells is difficult, since some proteins appear to be weakly associated with membranes or organelles in whole cells and are released into solution upon cell lysis. Indeed, in experiments where the plasma membrane of cells were carefully disrupted using saponin, without damaging the other cell membranes, only about one quarter of cell protein was released. These cells were also able to synthesize proteins if given ATP and amino acids, implying that many of the enzymes in cytosol are bound to the cytoskeleton. However, the idea that the majority of the proteins in cells are tightly bound in a network called the microtrabecular lattice is now seen as unlikely.
In prokaryotes the cytosol contains the cell's genome, within a structure known as a nucleoid. This is an irregular mass of DNA and associated proteins that control the transcription and replication of the bacterial chromosome and plasmids. In eukaryotes the genome is held within the cell nucleus, which is separated from the cytosol by nuclear pores that block the free diffusion of any molecule larger than about 10 nanometres in diameter.
This high concentration of macromolecules in cytosol causes an effect called macromolecular crowding, which is when the effective concentration of other macromolecules is increased, since they have less volume to move in. This crowding effect can produce large changes in both the rates and the position of chemical equilibrium of reactions in the cytosol. It is particularly important in its ability to alter dissociation constants by favoring the association of macromolecules, such as when multiple proteins come together to form protein complexes, or when DNA-binding proteins bind to their targets in the genome.
Organization.
Although the components of the cytosol are not separated into regions by cell membranes, these components do not always mix randomly and several levels of organization can localize specific molecules to defined sites within the cytosol.
Concentration gradients.
Although small molecules diffuse rapidly in the cytosol, concentration gradients can still be produced within this compartment. A well-studied example of these are the "calcium sparks" that are produced for a short period in the region around an open calcium channel. These are about 2 micrometres in diameter and last for only a few milliseconds, although several sparks can merge to form larger gradients, called "calcium waves". Concentration gradients of other small molecules, such as oxygen and adenosine triphosphate may be produced in cells around clusters of mitochondria, although these are less well understood.
Protein complexes.
Proteins can associate to form protein complexes, these often contain a set of proteins with similar functions, such as enzymes that carry out several steps in the same metabolic pathway. This organization can allow substrate channeling, which is when the product of one enzyme is passed directly to the next enzyme in a pathway without being released into solution. Channeling can make a pathway more rapid and efficient than it would be if the enzymes were randomly distributed in the cytosol, and can also prevent the release of unstable reaction intermediates. Although a wide variety of metabolic pathways involve enzymes that are tightly bound to each other, others may involve more loosely associated complexes that are very difficult to study outside the cell. Consequently, the importance of these complexes for metabolism in general remains unclear.
Protein compartments.
Some protein complexes contain a large central cavity that is isolated from the remainder of the cytosol. One example of such an enclosed compartment is the proteasome. Here, a set of subunits form a hollow barrel containing proteases that degrade cytosolic proteins. Since these would be damaging if they mixed freely with the remainder of the cytosol, the barrel is capped by a set of regulatory proteins that recognize proteins with a signal directing them for degradation (a ubiquitin tag) and feed them into the proteolytic cavity.
Another large class of protein compartments are bacterial microcompartments, which are made of a protein shell that encapsulates various enzymes. These compartments are typically about 100-200 nanometres across and made of interlocking proteins. A well-understood example is the carboxysome, which contains enzymes involved in carbon fixation such as RuBisCO.
Cytoskeletal sieving.
Although the cytoskeleton is not part of the cytosol, the presence of this network of filaments restricts the diffusion of large particles in the cell. For example, in several studies tracer particles larger than about 25 nanometres (about the size of a ribosome) were excluded from parts of the cytosol around the edges of the cell and next to the nucleus. These "excluding compartments" may contain a much denser meshwork of actin fibres than the remainder of the cytosol. These microdomains could influence the distribution of large structures such as ribosomes and organelles within the cytosol by excluding them from some areas and concentrating them in others.
Function.
The cytosol has no single function and is instead the site of multiple cell processes. Examples of these processes include signal transduction from the cell membrane to sites within the cell, such as the cell nucleus, or organelles. This compartment is also the site of many of the processes of cytokinesis, after the breakdown of the nuclear membrane in mitosis. Another major function of cytosol is to transport metabolites from their site of production to where they are used. This is relatively simple for water-soluble molecules, such as amino acids, which can diffuse rapidly through the cytosol. However, hydrophobic molecules, such as fatty acids or sterols, can be transported through the cytosol by specific binding proteins, which shuttle these molecules between cell membranes. Molecules taken into the cell by endocytosis or on their way to be secreted can also be transported through the cytosol inside vesicles, which are small spheres of lipids that are moved along the cytoskeleton by motor proteins.
The cytosol is the site of most metabolism in prokaryotes, and a large proportion of the metabolism of eukaryotes. For instance, in mammals about half of the proteins in the cell are localized to the cytosol. The most complete data are available in yeast, where metabolic reconstructions indicate that the majority of both metabolic processes and metabolites occur in the cytosol. Major metabolic pathways that occur in the cytosol in animals are protein biosynthesis, the pentose phosphate pathway, glycolysis and gluconeogenesis. The localization of pathways can be different in other organisms, for instance fatty acid synthesis occurs in chloroplasts in plants and in apicoplasts in apicomplexa.

</doc>
<doc id="6782" url="https://en.wikipedia.org/wiki?curid=6782" title="Compound">
Compound

Compound may refer to:
Compound may also refer to:

</doc>
<doc id="6784" url="https://en.wikipedia.org/wiki?curid=6784" title="Citizenship">
Citizenship

Citizenship is the status of a person recognized under the custom or law as being a member of a country. A person may have multiple citizenships and a person who does not have citizenship of any state is said to be stateless.
Nationality is often used as a synonym for citizenship in English – notably in international law – although the term is sometimes understood as denoting a person's membership of a nation (a large ethnic group). In some countries, e.g. the United States, the United Kingdom, "nationality" and "citizenship" can have different meanings (for more information, see Nationality#Nationality versus citizenship).
Determining factors.
A person can be a citizen for several reasons. Usually citizenship of the place of birth is automatic; in other cases an application may be required. Each country has their own policies and regulations which change the criteria of who is issued citizenship.
History.
Polis.
Many thinkers point to the concept of citizenship beginning in the early city-states of ancient Greece, although others see it as primarily a modern phenomenon dating back only a few hundred years and, for mankind, that the concept of citizenship arose with the first laws. "Polis" meant both the political assembly of the city-state as well as the entire society. Citizenship has generally been identified as a western phenomenon. There is a general view that citizenship in ancient times was a simpler relation than modern forms of citizenship, although this view has come under scrutiny. The relation of citizenship has not been a fixed or static relation, but constantly changed within each society, and that according to one view, citizenship might "really have worked" only at select periods during certain times, such as when the Athenian politician Solon made reforms in the early Athenian state.
Historian Geoffrey Hosking in his 2005 "Modern Scholar" lecture course suggested that citizenship in ancient Greece arose from an appreciation for the importance of freedom. Hosking explained:
Slavery permitted slaveowners to have substantial free time, and enabled participation in public life. Polis citizenship was marked by exclusivity. Inequality of status was widespread; citizens had a higher status than non-citizens, such as women, slaves or barbarians. The first form of citizenship was based on the way people lived in the ancient Greek times, in small-scale organic communities of the polis. Citizenship was not seen as a separate activity from the private life of the individual person, in the sense that there was not a distinction between public and private life. The obligations of citizenship were deeply connected into one's everyday life in the polis. These small-scale organic communities were generally seen as a new development in world history, in contrast to the established ancient civilizations of Egypt or Persia, or the hunter-gatherer bands elsewhere. From the viewpoint of the ancient Greeks, a person's public life was not separated from their private life, and Greeks did not distinguish between the two worlds according to the modern western conception. The obligations of citizenship were deeply connected with everyday life. To be truly human, one had to be an active citizen to the community, which Aristotle famously expressed: "To take no part in the running of the community's affairs is to be either a beast or a god!" This form of citizenship was based on obligations of citizens towards the community, rather than rights given to the citizens of the community. This was not a problem because they all had a strong affinity with the polis; their own destiny and the destiny of the community were strongly linked. Also, citizens of the polis saw obligations to the community as an opportunity to be virtuous, it was a source of honour and respect. In Athens, citizens were both ruler and ruled, important political and judicial offices were rotated and all citizens had the right to speak and vote in the political assembly.
Roman ideas.
In the Roman Empire, citizenship expanded from small-scale communities to the entire empire. Romans realized that granting citizenship to people from all over the empire legitimized Roman rule over conquered areas. Roman citizenship was no longer a status of political agency; it had been reduced to a judicial safeguard and the expression of rule and law. Rome carried forth Greek ideas of citizenship such as the principles of equality under the law, civic participation in government, and notions that "no one citizen should have too much power for too long", but Rome offered relatively generous terms to its captives, including chances for lesser forms of citizenship. If Greek citizenship was an "emancipation from the world of things", the Roman sense increasingly reflected the fact that citizens could act upon material things as well as other citizens, in the sense of buying or selling property, possessions, titles, goods. One historian explained:
Roman citizenship reflected a struggle between the upper-class patrician interests against the lower-order working groups known as the plebeian class. A citizen came to be understood as a person "free to act by law, free to ask and expect the law's protection, a citizen of such and such a legal community, of such and such a legal standing in that community". Citizenship meant having rights to have possessions, immunities, expectations, which were "available in many kinds and degrees, available or unavailable to many kinds of person for many kinds of reason". And the law, itself, was a kind of bond uniting people. Roman citizenship was more impersonal, universal, multiform, having different degrees and applications.
Middle Ages.
During the European Middle Ages, citizenship was usually associated with cities and towns, and applied mainly to middle class folk. Titles such as burgher, grand burgher (German "Großbürger") and bourgeoisie denoted political affiliation and identity in relation to a particular locality, as well as membership in a mercantile or trading class; thus, individuals of respectable means and socioeconomic status were interchangeable with citizens.
During this era, members of the nobility had a range of privileges above commoners (see aristocracy), though political upheavals and reforms, beginning most prominently with the French Revolution, abolished privileges and created an egalitarian concept of citizenship.
Renaissance.
During the Renaissance, people transitioned from being subjects of a king or queen to being citizens of a city and later to a nation. Each city had its own law, courts, and independent administration. And being a citizen often meant being subject to the city's law in addition to having power in some instances to help choose officials. City dwellers who had fought alongside nobles in battles to defend their cities were no longer content with having a subordinate social status, but demanded a greater role in the form of citizenship. Membership in guilds was an indirect form of citizenship in that it helped their members succeed financially. The rise of citizenship was linked to the rise of republicanism, according to one account, since independent citizens meant that kings had less power. Citizenship became an idealized, almost abstract, concept, and did not signify a submissive relation with a lord or count, but rather indicated the bond between a person and the state in the rather abstract sense of having rights and duties.
Modern times.
The modern idea of citizenship still respects the idea of political participation, but it is usually done through "elaborate systems of political representation at a distance" such as representative democracy. Modern citizenship is much more passive; action is delegated to others; citizenship is often a constraint on acting, not an impetus to act. Nevertheless, citizens are usually aware of their obligations to authorities, and are aware that these bonds often limit what they can do.
Different senses.
Citizenship status, under social contract theory, carries with it both rights and duties. In this sense, citizenship was described as "a bundle of rights -- primarily, political participation in the life of the community, the right to vote, and the right to receive certain protection from the community, as well as obligations." Citizenship is seen by most scholars as culture-specific, in the sense that the meaning of the term varies considerably from culture to culture, and over time. In China, for example, there is a cultural politics of citizenship which could be called "peopleship". How citizenship is understood depends on the person making the determination. The relation of citizenship has never been fixed or static, but constantly changes within each society. While citizenship has varied considerably throughout history, and within societies over time, there are some common elements but they vary considerably as well. As a bond, citizenship extends beyond basic kinship ties to unite people of different genetic backgrounds. It usually signifies membership in a political body. It is often based on, or was a result of, some form of military service or expectation of future service. It usually involves some form of political participation, but this can vary from token acts to active service in government. Citizenship is a status in society. It is an ideal state as well. It generally describes a person with legal rights within a given political order. It almost always has an element of exclusion, meaning that some people are not citizens, and that this distinction can sometimes be very important, or not important, depending on a particular society. Citizenship as a concept is generally hard to isolate intellectually and compare with related political notions, since it relates to many other aspects of society such as the family, military service, the individual, freedom, religion, ideas of right and wrong, ethnicity, and patterns for how a person should behave in society. When there are many different groups within a nation, citizenship may be the only real bond which unites everybody as equals without discrimination—it is a "broad bond" linking "a person with the state" and gives people a universal identity as a legal member of a specific nation.
Modern citizenship has often been looked at as two competing underlying ideas:
Scholars suggest that the concept of citizenship contains many unresolved issues, sometimes called tensions, existing within the relation, that continue to reflect uncertainty about what citizenship is supposed to mean. Some unresolved issues regarding citizenship include questions about what is the proper balance between duties and rights. Another is a question about what is the proper balance between "political citizenship" versus "social citizenship". Some thinkers see benefits with people being absent from public affairs, since too much participation such as revolution can be destructive, yet too little participation such as total apathy can be problematic as well. Citizenship can be seen as a special elite status, and it can also be seen as a democratizing force and something that everybody has; the concept can include both senses. According to sociologist Arthur Stinchcombe, citizenship is based on the extent that a person can control one's own destiny within the group in the sense of being able to influence the government of the group. One last distinction within citizenship is the so-called "consent descent" distinction, and this issue addresses whether citizenship is a fundamental matter determined by a person "choosing" to belong to a particular nation––by his or her "consent"––or is citizenship a matter of where a person was born––that is, by his or her "descent".
International.
Some intergovernmental organizations have extended the concept and terminology associated with citizenship to the international level, where it is applied to the totality of the citizens of their constituent countries combined. Citizenship at this level is a secondary concept, with rights deriving from national citizenship.
European Union.
The Maastricht Treaty introduced the concept of citizenship of the European Union. Article 17 (1) of the Treaty on European Union stated that: Citizenship of the Union is hereby established. Every person holding the nationality of a Member State shall be a citizen of the Union. Citizenship of the Union shall be additional to and not replace national citizenship.
An agreement known as the amended EC Treaty established certain minimal rights for European Union citizens. Article 12 of the amended EC Treaty guaranteed a general right of non-discrimination within the scope of the Treaty. Article 18 provided a limited right to free movement and residence in Member States other than that of which the European Union citizen is a national. Articles 18-21 and 225 provide certain political rights.
Union citizens have also extensive rights to move in order to exercise economic activity in any of the Member States which predate the introduction of Union citizenship.
Commonwealth.
The concept of "Commonwealth Citizenship" has been in place ever since the establishment of the Commonwealth of Nations. As with the EU, one holds Commonwealth citizenship only by being a citizen of a Commonwealth member state. This form of citizenship offers certain privileges within some Commonwealth countries:
Although Ireland was excluded from the Commonwealth in 1949 because it declared itself a republic, Ireland is generally treated as if it were still a member. Legislation often specifically provides for equal treatment between Commonwealth countries and Ireland and refers to "Commonwealth countries and Ireland". Ireland's citizens are not classified as foreign nationals in the United Kingdom.
Canada departed from the principle of nationality being defined in terms of allegiance in 1921. In 1935 the Irish Free State was the first to introduce its own citizenship. However, Irish citizens were still treated as subjects of the Crown, and they are still not regarded as foreign, even though Ireland is not a member of the Commonwealth. The Canadian Citizenship Act of 1947 provided for a distinct Canadian Citizenship, automatically conferred upon most individuals born in Canada, with some exceptions, and defined the conditions under which one could become a naturalized citizen. The concept of Commonwealth citizenship was introduced in 1948 in the British Nationality Act 1948. Other dominions adopted this principle such as New Zealand, by way of the British Nationality and New Zealand Citizenship Act of 1948.
Subnational.
Citizenship most usually relates to membership of the nation state, but the term can also apply at the subnational level. Subnational entities may impose requirements, of residency or otherwise, which permit citizens to participate in the political life of that entity, or to enjoy benefits provided by the government of that entity. But in such cases, those eligible are also sometimes seen as "citizens" of the relevant state, province, or region. An example of this is how the fundamental basis of Swiss citizenship is citizenship of an individual commune, from which follows citizenship of a canton and of the Confederation. Another example is Åland where the residents enjoy a special provincial citizenship within Finland, "hembygdsrätt".
The United States has a federal system in which a person is a citizen of their specific state of residence, such as New Jersey or California, as well as a citizen of the United States. State constitutions may grant certain rights above and beyond what are granted under the United States Constitution and may impose their own obligations including the sovereign right of taxation and military service; each state maintains at least one military force subject to national militia transfer service, the state's national guard, and some states maintain a second military force not subject to nationalization.
Education.
"Active citizenship" is the philosophy that citizens should work towards the betterment of their community through economic participation, public, volunteer work, and other such efforts to improve life for all citizens. In this vein, schools in some countries provide citizenship education (subject). By the time children reach secondary education there is an emphasis on such unconventional subjects to be included in academic curriculum. While the diagram on citizenship to the right is rather facile and depth-less, it is simplified to explain the general model of citizenship that is taught to many secondary school pupils. The idea behind this model within education is to instill in young pupils that their actions (i.e. their vote) have an impact on collective citizenship and thus in turn them.
United Kingdom.
Citizenship is offered as a General Certificate of Secondary Education (GCSE) course in many schools in the United Kingdom. As well as teaching knowledge about democracy, parliament, government, the justice system, human rights and the UK's relations with the wider world, students participate in active citizenship, often involving a social action or social enterprise in their local community.
Ireland.
It is taught in Ireland as an exam subject for the Junior Certificate. It is known as Civic, Social and Political Education (CSPE). A new Leaving Certificate exam subject with the working title 'Politics & Society' is being developed by the National Council for Curriculum and Assessment (NCCA) and is expected to be introduced to the curriculum sometime after 2012.

</doc>
<doc id="6787" url="https://en.wikipedia.org/wiki?curid=6787" title="Chiapas">
Chiapas

Chiapas (), officially Free and Sovereign State of Chiapas (), is one of the 31 states that, with the Federal District, make up the 32 federal entities of Mexico. It is divided into 122 municipalities and its capital city is Tuxtla Gutiérrez. Other important population centers in Chiapas include San Cristóbal de las Casas, Comitán, Tapachula and Arriaga. Located in Southwestern Mexico, it is the southernmost state, bordered by the states of Oaxaca to the west, Veracruz to the northwest and Tabasco to the north, and by the Petén, Quiché, Huehuetenango and San Marcos departments of Guatemala to the east and southeast. Chiapas has a coastline along the Pacific Ocean to the south.
In general, Chiapas has a humid, tropical climate. In the north, in the area bordering Tabasco, near Teapa, rainfall can average more than per year. In the past, natural vegetation at this region was lowland, tall perennial rainforest, but this vegetation has been destroyed almost completely to give way to agriculture and ranching. Rainfall decreases moving towards the Pacific Ocean, but it is still abundant enough to allow the farming of bananas and many other tropical crops near Tapachula. On the several parallel "sierras" or mountain ranges running along the center of Chiapas, climate can be quite temperate and foggy, allowing the development of cloud forests like those of the Reserva de la Biosfera el Triunfo, home to a handful of resplendent quetzals and horned guans.
Chiapas is home to the ancient Mayan ruins of Palenque, Yaxchilán, Bonampak, and Chinkultic. It is also home to one of the largest indigenous populations in the country with twelve federally recognized ethnicities. Much of the state’s history is centered on the subjugation of these peoples with occasional rebellions. The last of these rebellions was the 1994 Zapatista uprising, which succeeded in obtaining new rights for indigenous people.
History.
The official name of the state is Chiapas. The name derives from "Chiapan" or "Tepechiapan" the name of an indigenous population. The term is from Nahuatl and has been translated to mean "sage seed hill" and "water below the hill." After the Spanish arrived, they established two cities called Chiapas de los Indios and Chiapas de los Españoles, with the name of Provincia de Chiapas for the area around the cities. The first coat of arms for the state was created in 1535 as that of the Ciudad Real (San Cristobal de las Casas). The modern coat of arms was created by Chiapas painter Javier Vargas Ballinas.
Pre-Columbian Era.
Hunter gatherers began to occupy the central valley of the state around 7000 BCE, but little is known about their lives. The oldest archaeological remains in the seat are located at the Santa Elena Ranch in Ocozocoautla whose finds include tools and weapons made of stone and bone. It also includes burials. In the pre Classic period from 1800 BCE to 300 CE, agricultural villages appeared all over the state although hunter gather groups would persist for long after the era.
Recent excavations in the Soconusco region of the state indicate that the oldest civilization to appear in what is now modern Chiapas is that of the Mokaya, which were cultivating corn and living in houses as early as 1500 BCE, making them one of the oldest in Mesoamerica. There is speculation that these were the forefathers of the Olmec, migrating across the Grijalva Valley and onto the coastal plain of the Gulf of Mexico to the north, which was Olmec territory. One of these people's ancient cities is now the archeological site of Chiapa de Corzo, in which was found the oldest calendar known on a piece of ceramic with a date of 36 BCE. This is three hundred years before the Mayans developed their calendar. The descendants of Mokaya are the Mixe-Zoque.
During the pre Classic era, it is known that most of Chiapas was not Olmec, but had close relations with them, especially the Olmecs of the Isthmus of Tehuantepec. Olmec-influenced sculpture can be found in Chiapas and products from the state including amber, magnetite, and ilmenite were exported to Olmec lands. The Olmecs came to what is now the northwest of the state looking for amber with one of the main evidences for this called the Simojovel Ax.
Mayan civilization began in the pre-Classic period as well, but did not come into prominence until the Classic period (300-900 CE). Development of this culture was agricultural villages during the pre-Classic period with city building during the Classic as social stratification became more complex. The Mayans built cities on the Yucatán Peninsula and west into Guatemala. In Chiapas, Mayan sites are concentrated along the state's borders with Tabasco and Guatemala, near Mayan sites in those entities. Most of this area belongs to the Lacandon Jungle.
Mayan civilization in the Lacandon area is marked by rising exploitation of rain forest resources, rigid social stratification, fervent local identity, waging war against neighboring peoples. At its height, it had large cities, a writing system, and development of scientific knowledge, such as mathematics and astronomy. Cities were centered on large political and ceremonial structures elaborately decorated with murals and inscriptions. Among these cities are Palenque, Bonampak, Yaxchilan, Chinkultic, Toniná and Tenón. The Mayan civilization had extensive trade networks and large markets trading in goods such as animal skins, indigo, amber, vanilla and quetzal feathers. It is not known what ended the civilization but theories range from over population size, natural disasters, disease, and loss of natural resources through over exploitation or climate change.
Nearly all Mayan cities collapsed around the same time, 900 CE. From then until 1500 CE, social organization of the region fragmented into much smaller units and social structure became much less complex. There was some influence from the rising powers of central Mexico but two main indigenous groups emerged during this time, the Zoques and the various Mayan descendents. The Chiapans, for whom the state is named, migrated into the center of the state during this time and settled around Chiapa de Corzo, the old Mixe–Zoque stronghold. There is evidence that the Aztecs appeared in the center of the state around Chiapa de Corza in the 15th century, but were unable to displace the native Chiapa tribe. However, they had enough influence so that the name of this area and of the state would come from Nahuatl.
Colonial period.
When the Spanish arrived in the 16th century, they found the indigenous peoples divided into Mayan and non-Mayan, with the latter dominated by the Zoques and Chiapa. The first contact between Spaniards and the people of Chiapas came in 1522, when Hernán Cortés sent tax collectors to the area after Aztec Empire was subdued. The first military incursion was headed by Luis Marín, who arrived in 1523. For three years, Marín was able to subjugate a number of the local peoples, but met with fierce resistance from the Tzotzils in the highlands. The Spanish colonial government then sent a new expedition under Diego de Mazariegos. Mazariegos had more success than his predecessor, but many indigenous preferred to commit suicide rather than submit to the Spanish. One famous example of this is the Battle of Tepetchia, where many jumped to their deaths in the Sumidero Canyon.
Indigenous resistance was weakened by continual warfare with the Spaniards as well as disease, and by 1530, almost all of the indigenous peoples of the area had been subdued with the exception of the Lacandons in the deep jungles who actively resisted until 1695. However, the main two groups, the Tzotzils and Tzeltals of the central highlands were subdued enough to establish the first Spanish city, today called San Cristóbal de las Casas, in 1528. It was one of two settlements initially called Villa Real de Chiapa de los Españoles and the other called Chiapa de los Indios.
Soon after, the encomienda system was introduced, which reduced most of the indigenous population to serfs and many even as slaves, paid as a form of tribute. The conquistadors brought previously unknown diseases. This, as well as overwork on plantations, dramatically decreased the indigenous population. The Spanish also established missions, mostly under the Dominicans, with the Diocese of Chiapas established in 1538 by Pope Paul III. The Dominican evangelizers became early advocates of the indigenous' plight, with Bartolomé de las Casas winning a battle with the passing of a law in 1542 for their protection. This order also worked to make sure that communities would keep their indigenous name with a saint’s prefix leading to names such as San Juan Chamula and San Lorenzo Zinacantán. He also advocated adapting the teaching of Christianity to indigenous language and culture. The encomienda system that had perpetrated much of the abuse of the indigenous peoples fell away by the end of the 16th century, and was replaced by haciendas. However, the use and misuse of Indian labor remained a large part of Chiapas politics into modern times. This treatment and tribute payments would create an undercurrent of resentment in the indigenous population that passed on from generation to generation. One uprising against high tribute payments occurs in the Tzeltal communities in the Los Alto region in 1712. Soon, the Tzoltzils and Ch’ols joined the Tzeltales in rebellion, but within a year, the government was able to extinguish the rebellion.
As of 1778, Thomas Kitchin described Chiapas as "the metropolis of the original Mexicans," with a population of approximately 20,000, and consisting mainly of indigenous peoples. The Spanish introduced new crops such as sugar cane, wheat, barley and indigo as main economic staples along native ones such as corn, cotton, cacao and beans. Livestock such as cattle, horses and sheep were introduced as well. Regions would specialize in certain crops and animals depending on local conditions and for many of these regions, communication and travel were difficult. Most Europeans and their descendents tended to concentrate in cities such as Ciudad Real, Comitán, Chiapa and Tuxtla. Intermixing of the races was prohibited by colonial law but by the end of the 17th century there was a significant mestizo population. Added to this was a population of African slaves brought in by the Spanish in the middle of the 16th century due to the loss of native workforce.
Initially, "Chiapas" referred to the first two cities established by the Spanish in what is now the center of the state and the area surrounding them. Two other regions were also established, the Soconusco and Tuxtla, all under the regional colonial government of Guatemala. Chiapas, Soconusco and Tuxla regions were united to the first time as an "intendencia" during the Bourbon Reforms in 1790 as an administrative region under the name of Chiapas. However, within this intendencia, the division between Chiapas and Soconusco regions would remain strong and have consequences at the end of the colonial period.
Era of Independence.
Since the colonial period, Chiapas had been relatively isolated from colonial authorities in Mexico City and regional authorities in Guatemala. One reason for this was the rugged terrain but the other was that much of Chiapas was not attractive to the Spanish for its lack of mineral wealth, large areas of arable land, and easy access to markets. This isolation spared it from battles related to Independence. José María Morelos y Pavón did enter the city of Tonalá but incurred no resistance. The only other insurgent activity was the publication of a newspaper called "El Pararrayos" by Matías de Córdova in San Cristóbal de las Casas.
Following the end of Spanish rule in New Spain, it was unclear what new political arrangements would emerge. The isolation of Chiapas from centers of power, along with the strong internal divisions in the intendencia caused a political crisis after royal government collapsed in Mexico City in 1821, ending the Mexican War of Independence. During this war, a group of influential Chiapas merchants and ranchers sought the establishment of the Free State of Chiapas. This group became known as the "La Familia Chiapaneca". However, this alliance did not last with the lowlands preferring inclusion among the new republics of Central America and the highlands annexation to Mexico. In 1821, a number of cities in Chiapas, starting in Comitán, declared the state's separation from the Spanish empire. In 1823, Guatemala became part of the United Provinces of Central America, which united to form a federal republic that would last from 1823 to 1839. With the exception of the pro-Mexican Ciudad Real (San Cristóbal) and some others, many Chiapanecan towns and villages favored a Chiapas independent of Mexico and some favored unification with Guatemala.
Elites in highland cities pushed for incorporation into Mexico. In 1822, then Emperor Agustín de Iturbide decreed that Chiapas was part of Mexico. In 1823, the Junta General de Gobierno was held and Chiapas declared independence again. In July 1824, the Soconusco District of southwestern Chiapas split off from Chiapas, announcing that it would join the Central American Federation. In September of the same year, a referendum was held on whether the intendencia would join Central America or Mexico, with many of the elite endorsing union with Mexico. This referendum ended in favor of incorporation with Mexico (allegedly through manipulation of the elite in the highlands), but the Soconusco region maintained a neutral status until 1842, when Oaxacans under General Antonio López de Santa Anna occupied the area, and declared it reincorporated into Mexico. Elites of the area would not accept this until 1844. Guatemala would not recognize Mexico's annexation of the Soconusco region until 1895 even though a final border between Chiapas and the country was finalized until 1882. The State of Chiapas was officially declared in 1824, with its first constitution in 1826. Ciudad Real was renamed San Cristóbal de las Casas in 1828.
In the decades after the official end of the war, the provinces of Chiapas and Soconusco unified, with power concentrated in San Cristóbal de las Casas. The state's society evolved into three distinct spheres: indigenous peoples, mestizos from the farms and haciendas and the Spanish colonial cities. Most of the political struggles were between the latter two groups especially over who would control the indigenous labor force. Economically, the state lost one of its main crops, indigo, to synthetic dyes. There was a small experiment with democracy in the form of "open city councils" but it was short lived because voting was heavily rigged.
The Universidad Pontificia y Literaria de Chiapas was founded in 1826, with Mexico's second teacher’s college founded in the state in 1828.
Era of the Liberal Reform.
With the ouster of conservative Antonio López de Santa Anna, Mexican liberals came to power. The Reform War (1858-1861) fought between Liberals, who favored federalism and sought economic development, decreased power of the Roman Catholic Church, and Mexican army, and Conservatives, who favored centralized autocratic government, retention of elite privileges, did not lead to any military battles in the state. Despite that it strongly affected Chiapas politics. In Chiapas, the Liberal-Conservative division had its own twist. Much of the division between the highland and lowland ruling families was for whom the Indians should work for and for how long as the main shortage was of labor. These families split into Liberals in the lowlands, who wanted further reform and Conservatives in the highlands who still wanted to keep some of the traditional colonial and church privileges. For most of the early and mid 19th century, Conservatives held most of the power and were concentrated in the larger cites of San Cristóbal de las Casas, Chiapa (de Corzo), Tuxtla and Comitán. As Liberals gained the upper hand nationally in the mid-19th century, one Liberal politician Ángel Albino Corzo gained control of the state. Corzo became the primary exponent of Liberal ideas in the southeast of Mexico and defended the Palenque and Pichucalco areas from annexation by Tabasco. However, Corzo's rule would end in 1875, when he opposed the regime of Porfirio Díaz.
Liberal land reforms would have negative effects on the state's indigenous population unlike in other areas of the country. Liberal governments expropriated lands that were previously held by the Spanish Crown and Catholic Church in order to sell them into private hands. This was not only motivated by ideology, but also due to the need to raise money. However, many of these lands had been in a kind of "trust" with the local indigenous populations, who worked them. Liberal reforms took away this arrangement and many of these lands fell into the hands of large landholders who when made the local Indian population work for three to five days a week just for the right to continue to cultivate the lands. This requirement caused many to leave and look for employment elsewhere. Most became "free" workers on other farms, but they were often paid only with food and basic necessities from the farm shop. If this was not enough, these workers became indebted to these same shops and then unable to leave.
The opening up of these lands also allowed many whites and mestizos (often called Ladinos in Chiapas) to encroach on what had been exclusively indigenous communities in the state. These communities had had almost no contact with the Ladino world, except for a priest. The new Ladino landowners occupied their acquired lands as well as others, such as shopkeepers, opened up businesses in the center of Indian communities. In 1848, a group of Tzeltals plotted to kill the new mestizos in their midst, but this plan was discovered, and was punished by th removal of large number of the community’s male members. The changing social order had severe negative effects on the indigenous population with alcoholism spreadings, leading to more debts as it was expensive. The struggles between Conservatives and Liberals nationally disrupted commerce and confused power relations between Indian communities and Ladino authorities. It also resulted in some brief respites for Indians during times when the instability led to uncollected taxes.
One other effect that Liberal land reforms had was the start of coffee plantations, especially in the Soconusco region. One reason for this push in this area was that Mexico was still working to strengthen its claim on the area against Guatemala’s claims on the region. The land reforms brought colonists from other areas of the country as well as foreigners from England, the United States and France. These foreign immigrants would introduce coffee production to the areas, as well as modern machineray and professional administration of coffee plantations. Eventually, this production of coffee would become the state's most important crop.
Although the Liberals had mostly triumphed in the state and the rest of the country by the 1860s, Conservatives still held considerable power in Chiapas. Liberal politicians sought to solidify their power among the indigenous groups by weakening the Roman Catholic Church. The more radical of these even allowed indigenous groups the religious freedoms to return to a number of native rituals and beliefs such as pilgrimages to natural shrines such as mountains and waterfalls.
This culminated in the Chiapas "caste war", which was an uprising the Tzotzils beginning in 1868. The basis of the uprising was the establishment of the "three stones cult" in Tzajahemal. Agustina Gómez Checheb was a girl tending her father’s sheep when three stones fell from the sky. Collecting them, she put them on her father’s altar and soon claimed that the stone communicated with her. Word of this soon spread and the "talking stones" of Tzajahemel soon became a local indigenous pilgrimage site. The cult was taken over by one pilgrim, Pedro Díaz Cuzcat, who also claimed to be able to communicate with the stones, and had knowledge of Catholic ritual, becoming a kind of priest. However, this challenged the traditional Catholic faith and non Indians began to denounce the cult. Stories about the cult include embellishments such as the crucifixion of a young Indian boy.
This led to the arrest of Checheb and Cuzcat in December 1868. This caused resentment among the Tzotzils. Although the Liberals had earlier supported the cult, Liberal landowners had also lost control of much of their Indian labor and Liberal politicians were having a harder time collecting taxes from indigenous communities. An Indian army gathered at Zontehuitz then attacked various villages and haciendas. By the following June the city of San Cristóbal was surrounded by several thousand Indians, who offered the exchanged of several Ladino captives for their religious leaders and stones. Chiapas governor Dominguéz come to San Cristóbal with about three hundred heavily armed men, who then attacked the Indian force armed only with sticks and machetes. The indigenous force was quickly dispersed and routed with government troops pursuing pockets of guerrilla resistance in the mountains until 1870. The event effectively returned control of the indigenous workforce back to the highland elite.
Porfiriato, 1876-1911.
The Porfirio Díaz era at the end of the 19th century and beginning of the 20th was initially thwarted by regional bosses called caciques, bolstered by a wave of Spanish and mestizo farmers who migrated to the state and added to the elite group of wealthy landowning families. There was some technological progress such as a highway from San Cristóbal to the Oaxaca border and the first telephone line in the 1880s, but Porfirian era economic reforms would not begin until 1891 with Governor Emilio Rabasa. This governor took on the local and regional caciques and centralized power into the state capital, which he moved from San Cristóbal de las Casas to Tuxtla in 1892. He modernized public administration, transportation and promoted education. Rabasa also introduced telegraph, limited public schooling, sanitation and road construction, including a route from San Cristóbal to Tuxtla then Oaxaca, which signaled the beginning of favoritism of development in the central valley over the highlands. He also changed state policies to favor foreign investment, favored large land mass consolidation for the production of cash crops such as henequen, rubber, guayule, cochineal and coffee. Agricultural production boomed, especially coffee, which induced the construction of port facilities in Tonalá. The economic expansion and investment in roads also increased access to tropical commodities such as hardwoods, rubber and chicle.
These still required cheap and steady labor to be provided by the indigenous population. By the end of the 19th century, the four main indigenous groups, Tzeltals, Tzotzils, Tojolabals and Ch’ols were living in "reducciones" or reservations, isolated from one another. Conditions on the farms of the Porfirian era was serfdom, as bad if not worse than for other indigenous and mestizo populations leading to the Mexican Revolution. While this coming event would affect the state, Chiapas did not follow the uprisings in other areas that would end the Porfirian era.
Japanese immigration to Mexico began in 1897 when the first thirty five migrants arrived in Chiapas to work on coffee farms, so that Mexico was the first Latin American country to receive organized Japanese immigration. Although this colony ultimately failed, there remains a small Japanese community in Acacoyagua, Chiapas.
Early 20th century to 1960.
In the early 20th century and into the Mexican Revolution, the production of coffee was particularly important but labor-intensive. This would lead to a practice called "enganche" (hook), where recruiter would lure workers with advanced pay and other incentives such as alcohol and then trap them with debts for travel and other items to be worked off. This practice would lead to a kind of indentured servitude and uprisings in areas of the state, although they never led to large rebel armies as in other parts of Mexico.
A small war broke out between Tuxtla Gutiérrez and San Cristobal in 1911. San Cristóbal, allied with San Juan Chamula, tried to regain the state's capital but the effort failed. San Cristóbal de las Casas, who had a very limited budget, to the extent that it had to ally with San Juan Chamula, and Tuxtla Gutierrez, which was enough only a small ragtag army to beat overwhelmingly the army helped by chamulas from San Cristóbal. There were three years of peace after that until troops allied with "First Chief" of the revolutionary Constitutionalist forces, Venustiano Carranza, entered in 1914 taking over the government, with the aim of imposing the "Ley de Obreros" (Workers' Law) to address injustices against the state's mostly indigenous workers. Conservatives responded violently months later when they were certain the Carranza forces would take their lands. This was mostly by way of guerrilla actions headed by farm owners who called themselves the "Mapaches". This action continued for six years, until President Carranza was assassinated in 1920 and revolutionary general Álvaro Obregón became president of Mexico. This allowed the Mapaches to gain political power in the state and effectively stop many of the social reforms occurring in other parts of Mexico.
The Mapaches continued to fight against socialists and communists in Mexico from 1920 to 1936, to maintain their control over the state. In general, elite landowners also allied with the nationally dominant party founded by Plutarco Elías Calles following the assassination of president-elect Obregón in 1928; that party was renamed the Institutional Revolutionary Party in 1946. Through that alliance, they could block land reform in this way as well. The Mapaches were first defeated in 1925 when an alliance of socialists and former Carranza loyalists had Carlos A. Vidal selected as governor, although he was assassinated two years later. The last of the Mapache resistance was over come in the early 1930s by Governor Victorico Grajales, who pursued President Lázaro Cárdenas' social and economic policies including persecution of the Catholic Church. These policies would have some success in redistributing lands and organizing indigenous workers but the state would remain relatively isolated for the rest of the 20th century.
The territory was reorganized into municipalities in 1916. The current state constitution was written in 1921.
There was political stability from the 1940s to the early 1970s; however, regionalism regained with people thinking of themselves as from their local city or municipality over the state. This regionalism impeded the economy as local authorities restrained outside goods. For this reason, construction of highways and communications were pushed to help with economic development. Most of the work was done around Tuxtla Gutiérrez and Tapachula. This included the Sureste railroad connecting northern municipalities such as Pichucalco, Salto de Agua, Palenque, Catazajá and La Libertad. The Cristobal Colon highway linked Tuxtla to the Guatemalan border. Other highways included El Escopetazo to Pichucalco, a highway between San Cristóbal and Palenque with branches to Cuxtepeques and La Frailesca. This helped to integrate the state's economy, but it also permitted the political rise of communal land owners called ejidatarios.
Mid-20th century to 1990.
In the mid-20th century, the state experienced a significant rise in population, which outstripped local resources, especially land in the highland areas. Since the 1930s, many indigenous and mestizos have migrated from the highland areas into the Lacandon Jungle with the populations of Altamirano, Las Margaritas, Ocosingo and Palenque rising from less than 11,000 in 1920 to over 376,000 in 2000. These migrants came to the jungle area to clear forest and grow crops and raise livestock, especially cattle. Economic development in general raised the output of the state, especially in agriculture, but it had the effect of deforesting many areas, especially the Lacandon. Added to this was there was still serf like conditions for many workers and insufficient educational infrastructure. Population continued to increase faster than the economy could absorb There were some attempts to resettle peasant farmers onto non cultivated lands, but they were met with resistance. President Gustavo Díaz Ordaz awarded a land grant to the town of Venustiano Carranza in 1967, but that land was already being used by cattle-ranchers who refused to leave. The peasants tried to take over the land anyway, but when violence broke out, they were forcibly removed. In Chiapas poor farmland and severe poverty afflict the Mayan Indians which led to unsuccessful non violent protests and eventually armed struggle started by the Zapatista Nationial Liberation Army in January 1994.
These events began to lead to political crises in the 1970s, with more frequent land invasions and takeovers of municipal halls. This was the beginning of a process that would lead to the emergence of the Zapatista movement in the 1990s. Another important factor to this movement would be the role of the Catholic Church from the 1960s to the 1980s. In 1960, Samuel Ruiz became the bishop of the Diocese of Chiapas, centered in San Cristóbal. He supported and worked with Marist priests and nuns following an ideology called liberation theology. In 1974, he organized a statewide "Indian Congress" with representatives from the Tzeltal, Tzotzil, Tojolabal and Ch'ol peoples from 327 communities as well as Marists and the Maoist People's Union. This congress was the first of its kind with the goal of uniting the indigenous peoples politically. These efforts were also supported by leftist organizations from outside Mexico, especially to form unions of ejido organizations. These unions would later form the base of the EZLN organization. One reason for the Church's efforts to reach out to the indigenous population was that starting in the 1970s, a shift began from traditional Catholic affiliation to Protestant, Evangelical and other Christian sects.
The 1980s saw a large wave of refugees coming into the state from Central America as a number of these countries, especially Guatemala, were in the midst of violent political turmoil. The Chiapas/Guatemala border had been relatively porous with people traveling back and forth easily in the 19th and 20th centuries, much like the Mexico/U.S. border around the same time. This is in spite of tensions caused by Mexico's annexation of the Soconusco region in the 19th century. The border between Mexico and Guatemala had been traditionally poorly guarded, due to diplomatic considerations, lack of resources and pressure from landowners who need cheap labor sources.
The arrival of thousands of refugees from Central America stressed Mexico's relationship with Guatemala, at one point coming close to war as well as a politically destabilized Chiapas. Although Mexico is not a signatory to the UN Convention Relating to the Status of Refugees, international pressure forced the government to grant official protection to at least some of the refugees. Camps were established in Chiapas and other southern states, and mostly housed Mayan peoples. However, most Central American refugees from that time never received any official status, estimated by church and charity groups at about half a million from El Salvador alone. The Mexican government resisted direct international intervention in the camps, but eventually relented somewhat because of finances. By 1984, there were 92 camps with 46,000 refugees in Chiapas, concentrated in three areas, mostly near the Guatemalan border. To make matters worse, the Guatemalan army conducted raids into camps on Mexican territories with significant casualties, terrifying the refugees and local populations. From within Mexico, refugees faced threats by local governments who threatened to deport them, legally or not, and local paramilitary groups funded by those worried about the political situation in Central American spilling over into the state. The official government response was to militarize the areas around the camps, which limited international access and migration into Mexico from Central America was restricted. By 1990, it was estimated that there were over 200,000 Guatemalans and half a million from El Salvador, almost all peasant farmers and most under age twenty.
In the 1980s, the politization of the indigenous and rural populations of the state began in the 1960s and 1970s continued. In 1980, several ejido (communal land organizations) joined to form the Union of Ejidal Unions and United Peasants of Chiapas, generally called the Union of Unions or UU. It had a membership of 12,000 families from over 180 communities. By 1988, this organization joined with other to form the ARIC-Union of Unions (ARIC-UU) and took over much of the Lacandon Jungle portion of the state. Most of the members of these organization were from Protestant and Evangelical sects as well as "Word of God" Catholics affiliated with the political movements of the Diocese of Chiapas. What they held in common was indigenous identity vis-à-vis the non-indigenous, using the old 19th century "caste war" word "Ladino" for them.
Neoliberalism and the EZLN.
The adoption of neoliberalism by the Mexican federal government clashed with the leftist political ideals of these groups, especially as the reforms began to have negative economic effects on poor farmers, especially small-scale indigenous coffee growers. This would coalese into the Zapatista movement in the 1990s. Although the Zapatista movement couched its demands and cast is role in response to contemporary issues, especially in its opposition to neoliberalism, it is one of a long line of peasant and indigenous uprisings that have occurred in the state since the colonial era. This is reflected in its indigenous vs. Ladino character. However, the movement was an economic one as well. Although rich in resources, much of the local population of the state, especially in rural areas, did not benefit from this. In the 1990s, two thirds of the states residents did not have sewage service, only a third had electricity and half did not have potable water. Over half of the schools offered education only to the third grade and most dropped out by the end of first grade. These grievances, which were strongest in the San Cristóbal and Lacandon Jungle areas, were taken up by a small leftist guerrilla band led by a man called only "Subcomandante Marcos."
This small band, called the Zapatista Army of National Liberation (Ejército Zapatista de Liberación Nacional, EZLN), came to the world's attention when on January 1, 1994, the day the NAFTA treaty went into effect, EZLN forces occupied and took over the towns of San Cristobal de las Casas, Las Margaritas, Altamirano, Ocosingo and three others. They read their proclamation of revolt to the world and then laid siege to a nearby military base, capturing weapons and releasing many prisoners from the jails. This action followed previous protests in the state in opposition to neoliberal economic policies.
Although it has been estimated at having no more than 300 armed guerrilla members, the EZLN paralyzed the Mexican government, as it could not afford the political risks of direct confrontation. The major reason for this was that the rebellion caught the attention of the national and world press, as Marcos made full use of the then-new Internet to get the group's message out, putting the spotlight on indigenous issues in Mexico in general. It was also actively supported by opposition press in Mexico City, especially "La Jornada". However, these elements did provoke the rebellion to go national. Many blamed the unrest on infiltration of leftists among the large Central American refugee population in Chiapas, and the rebellion opened up splits in the countryside with those supporting and opposing EZLN. Zapatista sympathizers have included mostly Protestants and Word of God Catholics, versus those "traditionalist" Catholics who practiced a syncretic form of Catholicism and indigenous beliefs. This split had existed in Chiapas since the 1970s, with the latter group supported by the caciques and others in the traditional power structure. Protestants and Word of God Catholics (allied directly with the bishopric in San Cristóbal) tended to oppose traditional power structures.
The reaction of the Bishop of Chiapas Samuel Ruiz and the Diocese of Chiapas was to offer to mediate between the rebels and authorities. However, because of this diocese's activism since the 1960s, authorities accused the clergy of being involved with the rebels. There was some ambiguity about the relationship between Ruiz and Marcos and it was a constant feature of news coverage, with many in official circles using such to discredit Ruiz. Eventually, the activities of the Zapatistas began to worry the Roman Catholic Church in general and upstage the diocese's attempts to re establish itself among Chiapan indigenous communities against Protestant evangelization. This would lead to a breach between the Church and the Zapatistas.
The Zapatista story remained in headlines for a number of years. One reason for this was the December 1997 massacre of forty-five Tzotzil peasants, mostly women and children in the Zapatista-controlled village of Acteal in the Chenhaló municipality just north of San Cristóbal. This allowed many media outlets in Mexico to step up their criticisms of the government. However, the massacre was not done by the government but by other civilians, which shows how the emergence of the Zapatista movement had divided indigenous groups.
Despite this, the armed conflict was brief, mostly because the Zapatistas did not try to gain traditional political power like many other guerilla movements. Its focus was more on trying to manipulate public opinion in order to obtain concessions from the government. This has linked the Zapatistas to other indigenous and identity-politics movements that arose in the late 20th century. The main concession that the group received was the San Andrés Accords, also known as the Law on Indian Rights and Culture. The Accords appear to grant certain indigenous zones autonomy, but this is against the Mexican constitution, so its legitimacy has been questioned. Zapatista declarations since the mid-1990s have called for a new constitution. To the present, the government has not found a solution to this problem. The revolt also pressed the government to institute anti poverty programs such as "Progresa" later called "Oportunidades" and the "Puebla-Panama Plan" aimed to increase trade between southern Mexico and Central America.
As of the first decade of the 2000s, the Zapatista movement remained popular in many indigenous communities. The uprising gave indigenous peoples a more active role in the state's politics. However, it did not solve the economic issues that many peasant farmers face, especially the lack of land to cultivate. This problem has been at crisis proportions since the 1970s, and the government's reaction has been to encourage peasant farmers—mostly indigenous—to migrate into the sparsely populated Lacandon Jungle, a trend since earlier in the century.
From the 1970s on, some 100,000 people set up homes in this rainforest area, with many being recognized as ejidos, or communal land holding organizations. These migrants included Tzeltals, Tojolabals, Ch'ols and mestizos, mostly farming corn and beans and raising livestock. However, the government changed policies in the late 1980s with establishment of the Montes Azules Biosphere Reserve as much of the Lacandon Jungle had been destroyed or severely damaged. While armed resistance had wound down, the Zapatistas have remained a strong political force, especially around San Cristóbal and the Lacandon Jungle, its traditional bases. Since the Accords, they have shifted focus in gaining autonomy for the communities they control.
Since the 1994 uprising, migration into the Lacandon Jungle has significantly increased including illegal settlements and cutting in the protected biosphere reserve. These actions are supported by the Zapatistas as part of indigenous rights, but it has put them in conflict with international environmental groups and the indigenous inhabitants of the rainforest area, the Lacandons. Environmental groups state that the settlements pose grave risks to what remains of the Lacandon, while the Zapatistas accuse them of being fronts for the government, who want to open the rainforest up to multinational corporations. Added to this is the possibility that there are significant oil and gas deposits under this area as well.
The Zapatista movement has had some successes. The agricultural sector of the economy now favors ejidos and other commonly owned land. There have been some other gains economically as well. In the last decades of the 20th century, Chiapas's traditional agricultural economy has diversified somewhat with the construction of more roads and better infrastructure by the federal and state governments. At this time, tourism has become important in some areas of the state, especially in San Cristóbal de las Casas and Palenque. Its economy is important to Mexico as a whole as well, producing coffee, corn, cacao, tobacco, sugar, fruit, vegetable and honey for export. It is also a key state for the nation's petrochemical and hydroelectric industries. A significant percentage of PEMEX's drilling and refining is based in Chiapas and Tabasco, and fifty five percent of the nations hydroelectric energy is produced in Chiapas.
However, Chiapas remains one of the poorest states in Mexico. Ninety-four of its 111 municipalities have a large percentage of the population living in poverty. In areas such as Ocosingo, Altamirano and Las Margaritas, the towns where the Zapatistas first came into prominence in 1994, 48% of the adults are illiterate. Chiapas is still considered isolated and distant from the rest of Mexico, both culturally and geographically. It has significantly underdeveloped infrastructure compared to the rest of the country and its significant indigenous population with isolationist tendencies keep the state distinct culturally. Cultural stratification, neglect and lack of investment by the Mexican federal government has exacerbated this problem.
Geography.
Political geography.
Chiapas is located in the south east of Mexico, bordering the states of Tabasco, Veracruz and Oaxaca with the Pacific Ocean to the south and Guatemala to the east. It has a territory of 74,415 km2, the eighth largest state in Mexico. The state consists of 118 municipalities organized into nine political regions called Center, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa. There are 18 cities, twelve towns (villas) and 111 pueblos (villages). Major cities include Tuxtla Gutiérrez, San Cristóbal de las Casas, Tapachula, Palenque, Comitán, and Chiapa de Corzo.
Geographical regions.
The state has a complex geography with seven distinct regions according to the Mullerried classification system. These include the Pacific Coast Plains, the Sierra Madre de Chiapas, the Central Depression, the Central Highlands, the Eastern Mountains, the Northern Mountains and the Gulf Coast Plains. The Pacific Coast Plains is a strip of land parallel to the ocean. It is composed mostly of sediment from the mountains that border it on the northern side. It is uniformly flat, and stretches from the Bernal Mountain south to Tonalá. It has deep salty soils due to its proximity to the sea. It has mostly deciduous rainforest although most has been converted to pasture for cattle and fields for crops. It has numerous estuaries with mangroves and other aquatic vegetation.
The Sierra Madre de Chiapas runs parallel to the Pacific coastline of the state, northwest to southeast as a continuation of the Sierra Madre del Sur. This area has the highest altitudes in Chiapas including the Tacaná Volcano, which rises 4,093 meters above sea level. Most of these mountains are volcanic in origin although the nucleus is metamorphic rock. It has a wide range of climates but little arable land. It is mostly covered in middle altitude rainforest, high altitude rainforest, and forests of oaks and pines. The mountains partially block rain clouds from the Pacific, a process known as Orographic lift, which creates a particularly rich coastal region called the Soconusco. The main commercial center of the sierra is the town of Motozintla, also near the Guatemalan border.
The Central Depression is in the center of the state. It is an extensive semi flat area bordered by the Sierra Madre de Chiapas, the Central Highlands and the Northern Mountains. Within the depression there are a number of distinct valleys. The climate here can be very hot and humid in the summer, especially due to the large volume of rain received in July and August. The original vegetation was lowland deciduous rainforest with some rainforest of middle altitudes and some oaks above 1500masl.
The Central Highlands, also referred to as Los Altos, are mountains oriented from northwest to southeast with altitudes ranging from twelve to sixteen hundred meters above sea level. The western highlands are displaced faults, while the eastern highlands are mainly folds of sedimentary formations—mainly limestone, shale, and sandstone. These mountains, along the Sierra Madre of Chiapas become the Cuchumatanes where they extend over the border into Guatemala. Its topography is mountainous with many narrow valleys and karst formations called uvalas or poljés, depending on the size. Most of the rock is limestone allowing for a number of formations such as caves and sinkholes. There are also some isolated pockets of volcanic rock with the tallest peaks being the Tzontehuitz and Huitepec volcanos. There are no significant surface water systems as they are almost all underground. The original vegetation was forest of oak and pine but these have been heavily damaged. The highlands climate in the Koeppen modified classification system for Mexico is humid temperate C(m) and subhumid temperate C (w 2 ) (w). This climate exhibits a summer rainy season and a dry winter, with possibilities of frost from December to March. The Central Highlands have been the population center of Chiapas since the Conquest. European epidemics were hindered by the tierra fría climate, allowing the indigenous peoples in the highlands to retain their large numbers.
The Eastern Mountains (Montañas del Oriente) are in the east of the state, formed by various parallel mountain chains mostly made of limestone and sandstone. Its altitude varies from 500 to 1500 masl. This area receives moisture from the Gulf of Mexico with abundant rainfall and exuberant vegetation, which creates the Lacandon Jungle, one of the most important rainforests in Mexico. The Northern Mountains (Montañas del Norte) are in the north of the state. They separate the flatlands of the Gulf Coast Plains from the Central Depression. Its rock is mostly limestone. These mountains also receive large amounts of rainfall with moisture from the Gulf of Mexico giving it a mostly hot and humid climate with rains year round. In the highest elevations around 1800 masl, temperatures are somewhat cooler and do experience a winter. The terrain is rugged with small valleys whose natural vegetation is high altitude rainforest.
The Gulf Coast Plains (Llanura Costera del Golfo) stretch into Chiapas from the state of Tabasco, which gives it the alternate name of the Tabasqueña Plains. These plains are found only in the extreme north of the state. The terrain is flat and prone to flooding during the rainy season as it was built by sediments deposited by rivers and streams heading to the Gulf.
Lacandon Jungle.
The Lacandon Jungle is situated in north eastern Chiapas, centered on a series of canyonlike valleys called the Cañadas, between smaller mountain ridges oriented from northwest to southeast. The ecosystem covers an area of approximately 1.9 million hectares extending from Chiapas into northern Guatemala and southern Yucatán Peninsula and into Belize. This area contains as much as 25% of Mexico's total species diversity, most of which has not been researched. It has a predominantly hot and humid climate (Am w" i g) with most rain falling from summer to part of fall, with an average of between 2300 and 2600 mm per year. There is a short dry season from March to May. The predominate wild vegetation is perennial high rainforest. The Lacandon comprises a biosphere reserve (Montes Azules); four natural protected areas (Bonampak, Yaxchilan, Chan Kin, and Lacantum); and the communal reserve (La Cojolita), which functions as a biological corridor with the area of Petén in Guatemala. Flowing within the Rainforest is the Usumacinta River, considered to be one of the largest rivers in Mexico and seventh largest in the world based on volume of water.
During the 20th century, the Lacandon has had a dramatic increase in population and along with it, severe deforestation. The population of municipalities in this area, Altamirano, Las Margaritas, Ocosingo and Palenque have risen from 11,000 in 1920 to over 376,000 in 2000. Migrants include Ch'ol, Tzeltal, Tzotzil, Tojolabal indigenous peoples along with mestizos, Guatemalan refugees and others. Most of these migrants are peasant farmers, who cut forest to plant crops. However, the soil of this area cannot support annual crop farming for more than three or four harvents. The increase in population and the need to move on to new lands has pitted migrants against each other, the native Lacandon people, and the various ecological reserves for land. It is estimated that only ten percent of the original Lacandon rainforest in Mexico remains, with the rest strip-mined, logged and farmed. It once stretched over a large part of eastern Chiapas but all that remains is along the northern edge of the Guatemalan border. Of this remaining portion, Mexico is losing over five percent each year.
The best preserved portion of the Lacandon is within the Montes Azules Biosphere Reserve. It is centered on what was a commercial logging grant by the Porfirio Díaz government, which the government later nationalized. However, this nationalization and conversion into a reserve has made it one of the most contested lands in Chiapas, with the already existing ejidos and other settlements within the park along with new arrivals squatting on the land.
Soconusco.
The Soconusco region encompasses a coastal plain and a mountain range with elevations of up to 2000 meters above sea levels paralleling the Pacific Coast. The highest peak in Chiapas is the Tacaná Volcano at 4,800 meters above sea level. In accordance with an 1882 treaty, the dividing line between Mexico and Guatemala goes right over the summit of this volcano. The climate is tropical, with a number of rivers and evergreen forests in the mountains. This is Chiapas’ major coffee producing area, as it has the best soils and climate for coffee.
Before the arrival of the Spanish, this area was the principal source of cocoa seeds in the Aztec empire, which they used as currency, and for the highly prized quetzal feathers used by the nobility. It would become the first area to produce coffee, introduced by an Italian entrepreneur on the La Chacara farm. Coffee is cultivated on the slopes of these mountains mostly between 600 and 1200 masl. Mexico produces about 4 million sacks of green coffee each year, fifth in the world behind Brazil, Colombia, Indonesia and Vietnam. Most producers are small with plots of land under five hectares. From November to January, the annual crop is harvested and processed employing thousands of seasonal workers. Lately, a number of coffee haciendas have been developing tourism infrastructure as well.
Environment and protected areas.
Chiapas is located in the tropical belt of the planet, but the climate is moderated in many areas by altitude. For this reason, there are hot, semi-hot, temperate and even cold climates. There are areas with abundant rainfall year round along with those that receive most of their rain from May to October with a dry season from November to April. The mountain areas affect wind and moisture flow over the state, concentrating moisture in certain areas of the state. They also are responsible for some cloud-covered rainforest areas in the Sierra Madre.
Chiapas' rainforests are home to thousands of animals and plants, some of which cannot be found anywhere else in the world. Natural vegetation varies from lowland to highland tropical forest, pine and oak forests in the highest altitudes and plains area with some grassland. Chiapas is ranked second in forest resources in Mexico with valued woods such as pine, cypress, "Liquidambar", oak, cedar, mahogany and more. The Lacandon Jungle is one of the last major tropical rainforests in the northern hemisphere with an extension of . It contains about sixty percent of Mexico’s tropical tree species, 3,500 species of plants, 1,157 species of invertebrates and over 500 of vertebrate species. Chiapas has one of the greatest diversities in wildlife in the Americas. There are more than 100 species of amphibians, 700 species of birds, fifty of mammals and just over 200 species of reptiles. In the hot lowlands, there are armadillos, monkeys, pelicans, wild boar, jaguars, crocodiles, iguanas and many others. In the temperate regions there are species such as bobcats, salamanders, a large red lizard Abronia lythrochila, weasels, opossums, deer, ocelots and bats. The coastal areas have large quantities of fish, turtles, and crustaceans, with many species in danger of extinction or endangered as they are endemic only to this area. The total biodiversity of the state is estimated at over 50,000 species of plants and animals. The diversity of species is not limited to the hot lowlands but in the higher altitudes as well with mesophile forests, oak/pine forests in the Los Altos, Northern Mountains and Sierra Madre and the extensive estuaries and mangrove wetlands along the coast.
Chiapas has about thirty percent of Mexico’s fresh water resources. The Sierra Madre divides them into those that flow to the Pacific and those that flow to the Gulf of Mexico. Most of the first are short rivers and streams; most longer ones flow to the Gulf. Most Pacific side rivers do not drain directly into this ocean but into lagoons and estuaries. The two largest rivers are the Grijalva and the Usumacinta, with both part of the same system. The Grijalva has four dams built on it the Belisario Dominguez (La Angostura); Manuel Moreno Torres (Chicoasén); Nezahualcóyotl (Malpaso); and Angel Albino Corzo (Peñitas). The Usumacinta divides the state from Guatemala and is the longest river in Central America. In total, the state has of surface waters, of coastline, control of of ocean, of estuaries and ten lake systems. Laguna Miramar is a lake in the Montes Azules reserve and the largest in the Lacandon Jungle at 40 km in diameter. The color of its waters varies from indigo to emerald green and in ancient times, there were settlements on its islands and its caves on the shoreline. The Catazajá Lake is 28 km north of the city of Palenque. It is formed by rainwater captured as it makes it way to the Usumacinta River. It contains wildlife such as manatees and iguanas and it is surrounded by rainforest. Fishing on this lake is an ancient tradition and the lake has an annual bass fishing tournament. The Welib Já Waterfall is located on the road between Palenque and Bonampak.
The state has thirty-six protected areas at the state and federal levels along with 67 areas protected by various municipalities. The Sumidero Canyon National Park was decreed in 1980 with an extension of . It extends over two of the regions of the state, the Central Depression and the Central Highlands over the municipalities of Tuxtla Gutiérrez, Nuevo Usumacinta, Chiapa de Corzo and San Fernando. The canyon has steep and vertical sides that rise to up to 1000 meters from the river below with mostly tropical rainforest but some areas with xerophile vegetation such as cactus can be found. The river below, which has cut the canyon over the course of twelve million years, is called the Grijalva. The canyon is emblematic for the state as it is featured in the state seal. The Sumidero Canyon was once the site of a battle between the Spaniards and Chiapanecan Indians. Many Chiapanecans chose to throw themselves from the high edges of the canyon rather than be defeated by Spanish forces. Today, the canyon is a popular destination for ecotourism. Visitors often take boat trips down the river that runs through the canyon and enjoy the area's natural beauty including the many birds and abundant vegetation.
The Montes Azules Integral Biosphere Reserve was decreed in 1978. It is located in the northeast of the state in the Lacandon Jungle. It covers in the municipalities of Maravilla Tenejapa, Ocosingo and Las Margaritas. It conserves highland perennial rainforest. The jungle is in the Usumacinta River basin east of the Chiapas Highlands. It is recognized by the United Nations Environment Programme for its global biological and cultural significance. In 1992, the Lacantun Reserve, which includes the Classic Maya archaeological sites of Yaxchilan and Bonampak, was added to the biosphere reserve.
Agua Azul Waterfall Protection Area is in the Northern Mountains in the municipality of Tumbalá. It covers an area of of rainforest and pine-oak forest, centered on the waterfalls it is named after. It is located in an area locally called the "Mountains of Water", as many rivers flow through there on their way to the Gulf of Mexico. The rugged terrain encourages waterfalls with large pools at the bottom, that the falling water has carved into the sedimentary rock and limestone. Agua Azul is one of the best known in the state. The waters of the Agua Azul River emerge from a cave that forms a natural bridge of thirty meters and five small waterfalls in succession, all with pools of water at the bottom. In addition to Agua Azul, the area has other attractions—such as the Shumuljá River, which contains rapids and waterfalls, the Misol Há Waterfall with a thirty-meter drop, the Bolón Ajau Waterfall with a fourteen-meter drop, the Gallito Copetón rapids, the Blacquiazules Waterfalls, and a section of calm water called the Agua Clara.
The El Ocote Biosphere Reserve was decreed in 1982 located in the Northern Mountains at the boundary with the Sierra Madre del Sur in the municipalities of Ocozocoautla, Cintalapa and Tecpatán. It has a surface area of and preserves a rainforest area with karst formations. The Lagunas de Montebello National Park was decreed in 1959 and consists of near the Guatemalan border in the municipalities of La Independencia and La Trinitaria. It contains two of the most threatened ecosystems in Mexico the "cloud rainforest" and the Soconusco rainforest. The El Triunfo Biosphere Reserve, decreed in 1990, is located in the Sierra Madre de Chiapas in the municipalities of Acacoyagua, Ángel Albino Corzo, Montecristo de Guerrero, La Concordia, Mapastepec, Pijijiapan, Siltepec and Villa Corzo near the Pacific Ocean with . It conserves areas of tropical rainforest and many freshwater systems endemic to Central America. It is home to around 400 species of birds including several rare species such as the horned guan, the quetzal and the azure-rumped tanager. The Palenque National Forest is centered on the archaeological site of the same name and was decreed in 1981. It is located in the municipality of Palenque where the Northern Mountains meet the Gulf Coast Plain. It extends over of tropical rainforest. The Laguna Bélgica Conservation Zone is located in the north west of the state in the municipality of Ocozocoautla. It covers forty-two hectares centered on the Bélgica Lake. The El Zapotal Ecological Center was established in 1980. Nahá – Metzabok is an area in the Lacandon Jungle whose name means "place of the black lord" in Nahuatl. It extends over and in 2010, it was included in the World Network of Biosphere Reserves. Two main communities in the area are called Nahá and Metzabok. They were established in the 1940s, but the oldest communities in the area belong to the Lacandon people. The area has large numbers of wildlife including endangered species such as eagles, quetzals and jaguars.
Demographics.
General statistics.
As of 2010, the population is 4,796,580, the eighth most populous state in Mexico. The 20th century saw large population growth in Chiapas. From fewer than one million inhabitants in 1940, the state had about two million in 1980, and over 4 million in 2005. Overcrowded land in the highlands was relieved when the rainforest to the east was subject to land reform. Cattle ranchers, loggers, and subsistence farmers migrated to the rain forest area. The population of the Lacandon was only one thousand people in 1950, but by the mid-1990s this had increased to 200 thousand. As of 2010, 78% lives in urban communities with 22% in rural communities. While birthrates are still high in the state, they have come down in recent decades from 7.4 per woman in 1950. However, these rates still mean significant population growth in raw numbers. About half of the state's population is under age 20, with an average age of 19. In 2005, there were 924,967 households, 81% headed by men and the rest by women. Most households were nuclear families (70.7%) with 22.1% consisting of extended families.
More migrate out of Chiapas than migrate in, with emigrants leaving for Tabasco, Oaxaca, Veracruz, State of Mexico and the Federal District primarily.
While Catholics remain the majority, their numbers have dropped as many have converted to Protestant denominations in recent decades. The National Presbyterian Church in Mexico has a large followers in Chiapas; some estimate that 40% of the population are followers of the Presbyterian church.
There are a number of people in the state with African features. These are the descendents of slaves brought to the state in the 16th century. There are also those with predominantly European features who are the descendents of the original Spanish colonizers as well as later immigrants to Mexico. The latter mostly came at the end of the 19th and early 20th century under the Porfirio Díaz regime to start plantations.
Indigenous population.
Numbers and influence.
Over the history of Chiapas, there have been three main indigenous groups: the Mixes-Zoques, the Mayas and the Chiapa . Today, there are an estimated fifty-six linguistic groups. As of the 2005 Census, there were 957,255 people who spoke an indigenous language out of a total population of about 3.5 million. Of this one million, one third do not speak Spanish. Out of Chiapas' 111 municipios, ninety-nine have significant indigenous populations. Twenty two municipalities have indigenous populations over 90 percent, and 36 municipalities have native populations exceeding 50 percent. However, despite population growth in indigenous villages, the percentage of indigenous to non indigenous continues to fall with less than 35% indigenous. Indian populations are concentrated in a few areas, with the largest concentration of indigenous-language-speaking individuals is living in five of Chiapas's nine economic regions: Los Altos, Selva, Norte, Fronteriza, and Sierra. The remaining four regions, Centro, Frailesca, Soconusco, and Costa, have populations that are considered to be dominantly mestizo .
The state has about 13.5% of all of Mexico's indigenous population, and it has been ranked among the ten "most indianized" states, with only Campeche, Oaxaca, Quintana Roo and Yucatán having been ranked above it between 1930 and the present. These indigenous peoples have been historically resistant to assimilation into the broader Mexican society, with it best seen in the retention rates of indigenous languages and the historic demands for autonomy over geographic areas as well as cultural domains. Much of the latter has been prominent since the Zapatista uprising in 1994.
Most of Chiapas' indigenous groups are descended from the Mayans, speaking languages that are closely related to one another, belonging to the Western Maya language group. The state was part of a large region dominated by the Mayans during the Classic period. The most numerous of these Mayan groups include the Tzeltal, Tzotzil, Ch'ol, Zoque, Tojolabal, Lacandon and Mam, which have traits in common such as syncretic religious practices, and social structure based on kinship. The most common Western Maya languages are Tzeltal and Tzotzil along with Chontal, Ch’ol, Tojolabal, Chuj, Kanjobal, Acatec, Jacaltec and Motozintlec.
Twelve of Mexico's officially recognized native peoples live in the state have conserved their language, customs, history dress and traditions to a significant degree. The primary groups include the Tzeltal, Tzotzil, Ch'ol, Tojolabal, Zoque, Chuj, Kanjobal, Mam, Jacalteco, Mochó Cakchiquel and Lacandon. Most indigenous communities are found in the municipalities of the Centro, Altos, Norte and Selva regions, with many having indigenous populations of over fifty percent. These include Bochil, Sitalá, Pantepec, Simojovel to those with over ninety percent indigenous such as San Juan Cancuc, Huixtán, Tenejapa, Tila, Oxchuc, Tapalapa, Zinacantán, Mitontic, Ocotepec, Chamula, and Chalchihuitán. The most numerous indigenous communities are the Tzeltal and Tzotzil peoples, who number about 400,000 each, together accounting for about half of the state's indigenous population. The next most numerous are the Ch’ol with about 200,000 people and the Tojolabal and Zoques, who number about 50,000 each. The top 3 municipalities in Chiapas with indigenous language speakers 3 years of age and older are: Ocosingo (133,811), Chilon (96,567), and San Juan Chamula (69,475). These three municipalities accounted for 24.8% (299,853) of all indigenous language speakers 3 years or older in the state of Chiapas, out of a total of 1,209,057 indigenous language speakers 3 years or older.
Although most indigenous language speakers are bilingual, especially in the younger generations, many of these languages have shown resilience. Four of Chiapas' indigenous languages Tzeltal, Tzotzil, Tojolabal and Chol are high-vitality languages, meaning that a high percentage of these ethnicities speak the language and that there is a high rate of monolingualism in it. It is used in over 80% of homes. Zoque is considered to be of medium-vitality with a rate of bilingualism of over 70% and home use somewhere between 65% and 80%. Maya is considered to be of low-vitality with almost all of its speakers bilingual with Spanish. The most spoken indigenous languages as of 2010 are Tzeltal with 461,236 speakers, Tzotzil with 417,462, Ch’ol with 191,947 and Zoque with 53,839. In total, there are 1,141,499 who speak an indigenous language or 27% of the total population. Of these 14% do not speak Spanish. Studies done between 1930 and 2000 have indicated that Spanish is not dramatically displacing these languages. In raw number, speakers of these languages are increasing, especially among groups with a long history of resistance to Spanish/Mexican domination. Language maintenance has been strongest in areas related to where the Zapatista uprising took plaza such as the municipalities of Altamirano, Chamula, Chanal, Larráinzar, Las Margaritas, Ocosingo, Palenque, Sabanilla, San Cristóbal de Las Casas and Simojovel.
The state's rich indigenous tradition along with its associated political uprisings, especially that of 1994, has great interest from other parts of Mexico and abroad. It has been especially appealing to a variety of academics including many anthropologists, archeologists, historians, psychologists and sociologists. The concept of "mestizo" or mixed indigenous European heritage became important to Mexico's identity by the time of Independence, but Chiapas has kept its indigenous identity to the present day. Since the 1970s, this has been supported by the Mexican government as it has shifted from cultural policies that favor a "multicultural" identity for the country. One major exception to the separatist, indigenous identity has been the case of the Chiapa people, from whom the state's name comes, who have mostly been assimilated and intermarried into the mestizo population.
Most Indigenous communities have economies based primarily on traditional agriculture such as the cultivation and processing of corn, beans and coffee as a cash crop and in the last decade, many have begun producing sugarcane and jatropha for refinement into biodiesel and ethanol for automobile fuel. The raising of livestock, particularly chicken and turkey and to a lesser extent beef and farmed fish is also a major economic activity. Many indigenous, in particular the Maya are employed in the production of traditional clothing, fabrics, textiles, wood items, artworks and traditional goods such as jade and amber works. Tourism has provided a number of a these communities with markets for their handcrafts and works, some of which are very profitable.
San Cristóbal de las Casas and San Juan Chamula maintain a strong indigenous identity. On market day, many indigenous people from rural areas come into San Cristóbal to buy and sell mostly items for everyday use such as fruit, vegetables, animals, cloth, consumer goods and tools. San Juan Chamula is considered to be a center of indigenous culture, especially its elaborate festivals of Carnival and Day of Saint John. It was common for politicians, especially during Institutional Revolutionary Party's dominance to visit here during election campaigns and dress in indigenous clothing and carry a carved walking stick, a traditional sign of power. Relations between the indigenous ethnic groups is complicated. While there have been inter ethnic political activism such as that promoted by the Diocese of Chiapas in the 1970s and the Zapatista movement in the 1990s, there has been inter-indigenous conflict as well. Much of this has been based on religion, pitting those of the traditional Catholic/indigenous beliefs who support the traditional power structure against Protestants, Evangelicals and Word of God Catholics (directly allied with the Diocese) who tend to oppose it. This is particularly significant problem among the Tzeltals and Tzotzils. Starting in the 1970s, traditional leaders in San Juan Chamula began expelling dissidents from their homes and land, amounting to about 20,000 indigenous forced to leave over a thirty-year period. It continues to be a serious social problem although authorities downplay it. Recently there has been political, social and ethnic conflict between the Tzotzil who are more urbanized and have a significant number of Protestant practitioners and the Tzeltal who are predominantly Catholic and live in smaller farming communities. Many Protestant Tzotzil have accused the Tzeltal of ethnic discrimination and intimidation due to their religious beliefs and the Tzeltal have in return accused the Tzotzil of singling them out for discrimination.
Clothing, especially women’s clothing, varies by indigenous group. For example, women in Ocosingo tend to wear a blouse with a round collar embroidered with flowers and a black skirt decorated with ribbons and tied with a cloth belt. The Lacandon people tend to wear a simple white tunic. They also make a ceremonial tunic from bark, decorated with astronomy symbols. In Tenejapa, women wear a huipil embroidered with Mayan fretwork along with a black wool rebozo. Men wear short pants, embroidered at the bottom.
Tzeltals.
The Tzeltals call themselves Winik atel, which means "working men." This is the largest ethnicity in the state, mostly living southeast of San Cristóbal with the largest number in Amatenango. Today, there are about 500,000 Tzeltal Indians in Chiapas. Tzeltal Mayan, part of the Mayan language family, today is spoken by about 375,000 people making it the fourth-largest language group in Mexico. There are two main dialects; highland (or Oxchuc) and lowland (or Bachajonteco) . This language, along with Tzotzil, is from the Tzeltalan subdivision of the Mayan language family. Lexico-statistical studies indicate that these two languages probably became differentiated from one another around 1200 Most children are bilingual in the language and Spanish although many of their grandparents are monolingual Tzeltal speakers.
Each Tzeltal community constitutes a distinct social and cultural unit with its own well-defined lands, wearing apparel, kinship system, politico-religious organization, economic resources, crafts, and other cultural features. Women are distinguished by a black skirt with a wool belt and an undyed cotton bloused embroidered with flowers. Their hair is tied with ribbons and covered with a cloth. Most men do not use traditional attire. Agriculture is the basic economic activity of the Tzeltal people. Traditional Mesoamerican crops such as maize, beans, squash, and chili peppers are the most important, but a variety of other crops, including wheat, manioc, sweet potatoes, cotton, chayote, some fruits, other vegetables, and coffee.
Tzotzils.
Tzotzil speakers number just slightly less than theTzeltals at 226,000, although those of the ethnicity are probably higher. Tzotzils are found in the highlands or Los Altos and spread out towards the northeast near the border with Tabasco. However, Tzotzil communities can be found in almost every municipality of the state. They are concentrated in Chamula, Zinacantán, Chenalhó, and Simojovel. Their language is closely related to Tzeltal and distantly related to Yucatec Mayan and Lacandon. Men dress in short pants tied with a red cotton belt and a shirt that hangs down to their knees. They also wear leather huaraches and a hat decorated with ribbons. The women wear a red or blue skirt, a short huipil as a blouse, and use a chal or rebozo to carry babies and bundles. Tzotzil communities are governed by a katinab who is selected for life by the leaders of each neighborhood. The Tzotzils are also known for their continued use of the temazcal for hygiene and medicinal purposes.
Ch’ols.
The Ch’ols of Chiapas migrated to the northwest of the state starting about 2,000 years ago, when they were concentrated in Guatemala and Honduras. Those Ch’ols who remained in the south are distinguished by the name Chortís. Chiapas Ch’ols are closely related to the Chontal in Tabasco as well. Choles are found in Tila, Tumbalá, Sabanilla, Palenque, and Salto de Agua, with an estimated population of about 115,000 people. The Ch’ol language belongs to the Maya family and is related to Tzeltal, Tzotzil, Lacandon, Tojolabal, and Yucatec Mayan. There are three varieties of Chol (spoken in Tila, Tumbalá, and Sabanilla), all mutually intelligible. Over half of speakers are monolingual in the Chol language. Women wear a long navy blue or black skirt with a white blouse heavily embroidered with bright colors and a sash with a red ribbon. The men only occasionally use traditional dress for events such as the feast of the Virgin of Guadalupe. This dress usually includes pants, shirts and huipils made of undyed cotton, with leather huaraches, a carrying sack and a hat. The fundamental economic activity of the Ch’ols is agriculture. They primarily cultivate corn and beans, as well as sugar cane, rice, coffee, and some fruits. They have Catholic beliefs strongly influenced by native ones. Harvests are celebrated on the Feast of Saint Rose on 30 August.
Tojolabals.
The Totolabals are estimated at 35,000 in the highlands. According to oral tradition, the Tojolabales came north from Guatemala. The largest community is Ingeniero González de León in the La Cañada region, an hour outside the municipal seat of Las Margaritas. Tojolabales are also found in Comitán, Trinitaria, Altamirano and La Independencia. This area is filled with rolling hills with a temperate and moist climate. There are fast moving rivers and jungle vegetation. Tojolabal is related to Kanjobal, but also to Tzeltal and Tzotzil. However, most of the youngest of this ethnicity speak Spanish. Women dress traditionally from childhood with brightly colored skirts decorated with lace or ribbons and a blouse decorated with small ribbons, and they cover their heads with kerchiefs. They embroider many of their own clothes but do not sell them. Married women arrange their hair in two braids and single women wear it loose decorated with ribbons. Men no longer wear traditional garb daily as it is considered too expensive to make.
Zoques.
The Zoques are found in 3,000 square kilometers the center and west of the state scattered among hundreds of communities. These were one of the first native peoples of Chiapas, with archeological ruins tied to them dating back as far as 3500 BCE. Their language is not Mayan but rather related to Mixe, which is found in Oaxaca and Veracruz. By the time the Spanish arrived, they had been reduced in number and territory. Their ancient capital was Quechula, which was covered with water by the creation of the Malpaso Dam, along with the ruins of Guelegas, which was first buried by an eruption of the Chichonal volcano. There are still Zoque ruins at Janepaguay, the Ocozocuautla and La Ciénega valleys.
Lacandons.
The Lacandons are one of the smallest native indigenous groups of the state with a population estimated between 600 and 1000. They are mostly located in the communities of Lacanjá, Chansayab and Mensabak in the Lacandon Jungle. They live near the ruins of Bonampak and Yaxchilan and local lore states that the gods resided here when they lived on Earth. They inhabit about a million hectares of rainforest but from the 16th century to the present, migrants have taken over the area, most of which are indigenous from other areas of Chiapas. This dramatically altered their lifestyle and worldview. Traditional Lacandon shelters are huts made with fonds and wood with an earthen floor, but this has mostly given way to modern structures.
Mochós.
The Mochós or Motozintlecos are concentrated in the municipality of Motozintla on the Guatemalan border. According to anthropologists, these people are an "urban" ethnicity as they are mostly found in the neighborhoods of the municipal seat. Other communities can be found near the Tacaná volcano, and in the municipalities of Tuzantán and Belisario Dominguez. The name "Mochó" comes from a response many gave the Spanish whom they could not understand and means "I don't know." This community is in the process of disappearing as their numbers shrink.
Mams.
The Mams are a Mayan ethnicity that numbers about 20,000 found in thirty municipalities, especially Tapachula, Motozintla, El Porvenir, Cacahoatán and Amatenango in the southeastern Sierra Madre of Chiapas. The Mame language is one of the most ancient Mayan languages with 5,450 Mame speakers were tallied in Chiapas in the 2000 census. These people first migrated to the border region between Chiapas and Guatemala at the end of the nineteenth century, establishing scattered settlements. In the 1960s, several hundred migrated to the Lacandon rain forest near the confluence of the Santo Domingo and Jataté Rivers. Those who live in Chiapas are referred to locally as the "Mexican Mam (or Mame)" to differientiate them from those in Guatemala. Most live around the Tacaná volcano, which the Mams call "our mother" as it is considered to be the source of the fertility of the area's fields. The masculine deity is the Tajumulco volcano, which is in Guatemala.
Guatemalan migrant groups.
In the last decades of the 20th century, Chiapas received a large number of indigenous refugees, especially from Guatemala, many of whom remain in the state. These have added ethnicities such as the Kekchi, Chuj, Ixil, Kanjobal, K'iche' and Cakchikel to the population. The Kanjobal mainly live along the border between Chiapas and Guatemala, with almost 5,800 speakers of the language tallied in the 2000 census. It is believed that a significant number of these Kanjobal-speakers may have been born in Guatemala and immigrated to Chiapas, maintaining strong cultural ties to the neighboring nation.
Economy.
Economic indicators.
Chiapas accounts for 1.73% of Mexico's GDP. The primary sector, agriculture, produces 15.2% of the states GDP. The secondary sector, mostly energy production, but also commerce, services and tourism, accounts for 21.8%. The percentage of the GDP by commerce in services is rising while that of agriculture is falling. The state is divided into nine economic regions. These regions were established in the 1980s in order to facilitate statewide economic planning. Many of these regions are based on state and federal highway systems. These include Centro, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa.
Despite being rich in resources, Chiapas, along with Oaxaca and Guerrero, lags behind the rest of the country in almost all socioeconomic indicators. , there were 889,420 residential units, with 71% having running water, 77.3% having sewerage, and 93.6% having electricity. Construction of these units is varied from modern construction of block and concrete to those constructed of wood and laminate. Because of it high economic marginalization, more people migrate from Chiapas than migrate to it. Most of its socioeconomic indicators are the lowest in the country including income, education, health and housing. It has a significantly higher percentage of illiteracy than the rest of the country although that situation has improved since the 1970s when over 45% were illiterate and in the 1980s when about 32% were. The tropical climate presents health challenges, with most illnesses related to the gastro-intestinal tract and parasites. As of 2005, the state has 1,138 medical facilities: 1098 outpatient and 40 inpatient. Most are run by IMSS and ISSSTE and other government agencies. The implementation of NAFTA has had negative effects on the economy, often by lowering prices for agricultural products. It has also worked to make the southern states of Mexico poorer in comparison to those in the north with over 90% of the poorest municipalities in the south of the country. As of 2006, 31.8% work in communal services, social services and personal services. 18.4% work in financial services, insurance and real estate, 10.7% work in commerce, restaurants and hotels, 9.8% work in construction, 8.9% in utilities, 7.8% in transportation, 3.4% in industry (excluding handcrafts), and 8.4% in agriculture.
Although until the 1960s, many indigenous communities were considered by scholars to be autonomous and economically isolated, this was never the case. Economic conditions began forcing many to migrate to work, especially in agriculture for non- indigenous. However, unlike many other migrant workers, most indigenous in Chiapas have remained strongly tied to their home communities. A study as early as the 1970s showed that 77 percent of heads of household migrated outside of the Chamula municipality as local land did not produce sufficiently to support families. In the 1970s, cuts in the price of corn forced many large landowners to convert their fields into pasture for cattle, displacing many hired laborers as cattle required less work. These agricultural laborers began to work for the government on infrastructure projects financed by oil revenue. It is estimated that in the 1980s to 1990s as many as 100,000 indigenous people moved from the mountain areas into cities in Chiapas, with some moving out of the state to Mexico City, Cancún and Villahermosa in search of employment.
Agriculture, livestock, forestry and fishing.
Agriculture, livestock, forestry and fishing employ over 53% of the state’s population; however, its productivity is considered to be low. Agriculture includes both seasonal and perennial plants. Major crops include corn, beans, sorghum, soybeans, peanuts, sesame seeds, coffee, cacao, sugar cane, mangos, bananas, and palm oil. These crops take up 95% of the cultivated land in the state and 90% of the agricultural production. Only four percent of fields are irrigated with the rest dependent on rainfall either seasonally or year round. Chiapas ranks second among the Mexican states in the production of cacao, the product used to make chocolate, and is responsible for about 60 percent of Mexico's total coffee output. The production of bananas, cacao and corn make Chiapas Mexico's second largest agricultural producer overall.
Coffee is the state's most important cash crop with a history from the 19th century. The crop was introduced in 1846 by Jeronimo Manchinelli who brought 1,500 seedlings from Guatemala on his farm La Chacara. This was followed by a number of other farms as well. Coffee production intensified during the regime of Porfirio Díaz and the Europeans who came to own many of the large farms in the area. By 1892, there were 22 coffee farms in the region, among them Nueva Alemania, Hamburgo, Chiripa, Irlanda, Argovia, San Francisco, and Linda Vista in the Soconusco region. Since then coffee production has grown and diversified to include large plantations, the use and free and forced labor and a significant sector of small producers. While most coffee is grown in the Soconusco, other areas grow it, including the municipalities of Oxchuc, Pantheló, El Bosque, Tenejapa, Chenalhó, Larráinzar, and Chalchihuitán, with around six thousand producers. It also includes organic coffee producers with 18 million tons grown annually 60,000 producers. One third of these producers are indigenous women and other peasant farmers who grow the coffee under the shade of native trees without the use of agro chemicals. Some of this coffee is even grown in environmentally protected areas such as the El Triunfo reserve, where ejidos with 14,000 people grow the coffee and sell it to cooperativers who sell it to companies such as Starbucks, but the main market is Europe. Some growers have created cooperatives of their own to cut out the middleman.
Ranching occupies about three million hectares of natural and induced pasture, with about 52% of all pasture induced. Most livestock is done by families using traditional methods. Most important are meat and dairy cattle, followed by pigs and domestic fowl. These three account for 93% of the value of production. Annual milk production in Chiapas totals about 180 million liters per year. The state's cattle production, along with timber from the Lacandon Jungle and energy output gives it a certain amount of economic clouts compared to other states in the region.
Forestry is mostly based on conifers and common tropical species producing 186,858 m3 per year at a value of 54,511,000 pesos. Exploited non-wood species include the Camedor palm tree for its fronds. The fishing industry is underdeveloped but includes the capture of wild species as well as fish farming. Fish production is generated both from the ocean as well as the many freshwater rivers and lakes. In 2002, 28,582 tons of fish valued at 441.2 million pesos was produced. Species include tuna, shark, shrimp, mojarra and crab.
Industry and energy.
The state's abundant rivers and streams have been dammed to provide about fifty five percent of the country's hydroelectric energy. Much of this is sent to other states accounting for over six percent of all of Mexico's energy output. Main power stations are located at Malpaso, La Angostura, Chicoasén and Peñitas, which produce about eight percent of Mexico's hydroelectric energy. Manuel Moreno Torres plant on the Grijalva River the most productive in Mexico. All of the hydroelectric plants are owned and operated by the Federal Electricity Commission (Comisión Federal de Electricidad, CFE).
Chiapas is rich in petroleum reserves. Oil production began during the 1980s and Chiapas has become the fourth largest producer of crude oil and natural gas among the Mexican states. Many reserves are yet untapped, but between 1984 and 1992, PEMEX drilled nineteen oil wells in the Lacandona Jungle. Currently, petroleum reserves are found in the municipalities of Juárez, Ostuacán, Pichucalco and Reforma in the north of the state with 116 wells accounting for about 6.5% of the country's oil production. It also provides about a quarter of the country’s natural gas. This production equals of natural gas and 17,565,000 barrels of oil per year.
Industry is limited to small and micro enterprises and include auto parts, bottling, fruit packing, coffee and chocolate processing, production of lime, bricks and other construction materials, sugar mills, furniture making, textiles, printing and the production of handcrafts. The two largest enterprises is the Comisión Federal de Electricidad and a Petróleos Mexicanos refinery. Chiapas opened its first assembly plant in 2002, a fact that highlights the historical lack of industry in this area.
Handcrafts.
Chiapas is one of the states that produces a wide variety of handcrafts and folk art in Mexico. One reason for this is its many indigenous ethnicities who produce traditional items out of identity as well as commercial reasons. One commercial reason is the market for crafts provided by the tourism industry. Another is that most indigenous communities can no longer provide for their own needs through agriculture. The need to generate outside income has led to many indigenous women producing crafts communally, which has not only had economic benefits but also involved them in the political process as well. Unlike many other states, Chiapas has a wide variety of wood resources such as cedar and mahogany as well as plant species such as reeds, ixtle and palm. It also has minerals such as obsidian, amber, jade and several types of clay and animals for the production of leather, dyes from various insects used to create the colors associated with the region. Items include various types of handcrafted clothing, dishes, jars, furniture, roof tiles, toys, musical instruments, tools and more.
Chiapas’ most important handcraft is textiles, most of which is cloth woven on a backstrap loom. Indigenous girls often learn how to sew and embroider before they learn how to speak Spanish. They are also taught how to make natural dyes from insects, and weaving techniques. Many of the items produced are still for day-to-day use, often dyed in bright colors with intricate embroidery. They include skirts, belts, rebozos, blouses, huipils and shoulder wraps called chals. Designs are in red, yellow, turquoise blue, purple, pink, green and various pastels and decorated with designs such as flowers, butterflies, and birds, all based on local flora and fauna. Commercially, indigenous textiles are most often found in San Cristóbal de las Casas, San Juan Chamula and Zinacantán. The best textiles are considered to be from Magdalenas, Larráinzar, Venustiano Carranza and Sibaca.
One of the main minerals of the state is amber, much of which is 25 million years old, with quality comparable to that found in the Dominican Republic. Chiapan amber has a number of unique qualities, including much that is clear all the way through and some with fossilized insects and plants. Most Chiapan amber is worked into jewelry including pendants, rings and necklaces. Colors vary from white to yellow/orange to a deep red, but there are also green and pink tones as well. Since pre-Hispanic times, native peoples have believed amber to have healing and protective qualities. The largest amber mine is in Simojovel, a small village 130 km from Tuxtla Gutiérrez, which produces 95% of Chiapas' amber. Other mines are found in Huitiupán, Totolapa, El Bosque, Pueblo Nuevo Solistahuacán, Pantelhó and San Andrés Duraznal. According to the Museum of Amber in San Cristóbal, almost 300 kg of amber is extracted per month from the state. Prices vary depending on quality and color.
The major center for ceramics in the state is the city of Amatenango del Valle, with its barro blanco (white clay) pottery. The most traditional ceramic in Amatenango and Aguacatenango is a type of large jar called a cantaro used to transport water and other liquids. Many pieces created from this clay are ornamental as well as traditional pieces for everyday use such as comals, dishes, storage containers and flowerpots. All pieces here are made by hand using techniques that go back centuries. Other communities that produce ceramics include Chiapa de Corzo, Tonalá, Ocuilpa, Suchiapa and San Cristóbal de las Casas.
Wood crafts in the state center on furniture, brightly painted sculptures and toys. The Tzotzils of San Juan de Chamula are known for their sculptures as well as for their sturdy furniture. Sculptures are made from woods such as cedar, mahogany and strawberry tree. Another town noted for their sculptures is Tecpatán. The making lacquer to use in the decoration of wooden and other items goes back to the colonial period. The best-known area for this type of work, called "laca" is Chiapa de Corzo, which has a museum dedicated to it. One reason this type of decoration became popular in the state was that it protected items from the constant humidity of the climate. Much of the laca in Chiapa de Corzo is made in the traditional way with natural pigments and sands to cover gourds, dipping spoons, chests, niches and furniture. It is also used to create the Parachicos masks.
Traditional Mexican toys, which have all but disappeared in the rest of Mexico, are still readily found here and include the cajita de la serpiente, yo yos, ball in cup and more. Other wooden items include masks, cooking utensils, and tools. One famous toy is the "muñecos zapatistas" (Zapatista dolls), which are based on the revolutionary group that emerged in the 1990s.
Tourism and general commerce/services.
Ninety four percent of the state's commercial outlets are small retail stores with about 6% wholesalers. There are 111 municipal markets, 55 tianguis, three wholesale food markets and 173 large vendors of staple products. The service sector is the most important to the economy, with mostly commerce, warehousing and tourism.
Tourism brings large numbers of visitors to the state each year. Most of Chiapas' tourism is based on its culture, colonial cities and ecology. The state has a total of 491 ranked hotels with 12,122 rooms. There are also 780 other establishments catering primarily to tourism, such as services and restaurants.
There are three main tourist routes: the Maya Route, the Colonial Route and the Coffee Route. The Maya Route runs along the border with Guatemala in the Lacandon Jungle and includes the sites of Palenque, Bonampak, Yaxchilan along with the natural attractions of Agua Azul Waterfalls, Misol-Há Waterfall, and the Catazajá Lake. Palenque is the most important of these sites, and one of the most important tourist destinations in the state. Yaxchilan was a Mayan city along the Usumacinta River. It developed between 350 and 810 CE. Bonampak is known for its well preserved murals. These Mayan sites have made the state an attraction for international tourism. These sites contain a large number of structures, most of which date back thousands of years, especially to the sixth century. In addition to the sites on the Mayan Route, there are others within the state away from the border such as Toniná, near the city of Ocosingo.
The Colonial Route is mostly in the central highlands with a significant number of churches, monasteries and other structures from the colonial period along with some from the 19th century and even into the early 20th. The most important city on this route is San Cristóbal de las Casas, located in the Los Altos region in the Jovel Valley. The historic center of the city is filled with tiled roofs, patios with flowers, balconies, Baroque facades along with Neoclassical and Moorish designs. It is centered on a main plaza surrounded by the cathedral, the municipal palace, the Portales commercial area and the San Nicolás church. In addition, it has museums dedicated to the state’s indigenous cultures, one to amber and one to jade, both of which have been mined in the state. Other attractions along this route include Comitán de Domínguez and Chiapa de Corzo, along with small indigenous communities such as San Juan Chamula. The state capital of Tuxtla Gutiérrez does not have many colonial era structures left, but it lies near the area's most famous natural attraction of the Sumidero Canyon. This canyon is popular with tourists who take boat tours into it on the Grijalva River to see such features such as caves (La Cueva del Hombre, La Cueva del Silencio) and the Christmas Tree, which is a rock and plant formation on the side of one of the canyon walls created by a seasonal waterfall.
The Coffee Route begins in Tapachula and follows a mountainous road into the Suconusco regopm. The route passes through Puerto Chiapas, a port with modern infrastructure for shipping exports and receiving international cruises. The route visits a number of coffee plantations, such as Hamburgo, Chiripa, Violetas, Santa Rita, Lindavista, Perú-París, San Antonio Chicarras and Rancho Alegre. These haciendas provide visitors with the opportunity to see how coffee is grown and initially processed on these farms. They also offer a number of ecotourism activities such as mountain climbing, rafting, rappelling and mountain biking. There are also tours into the jungle vegetation and the Tacaná Volcano. In addition to coffee, the region also produces most of Chiapas’ soybeans, bananas and cacao.
The state has a large number of ecological attractions most of which are connected to water. The main beaches on the coastline include Puerto Arista, Boca del Cielo, Playa Linda, Playa Aventuras, Playa Azul and Santa Brigida. Others are based on the state's lakes and rivers. Laguna Verde is a lake in the Coapilla municipality. The lake is generally green but its tones constantly change through the day depending on how the sun strikes it. In the early morning and evening hours there can also be blue and ochre tones as well. The El Chiflón Waterfall is part of an ecotourism center located in a valley with reeds, sugarcane, mountains and rainforest. It is formed by the San Vicente River and has pools of water at the bottom popular for swimming. The Las Nubes Ecotourism center is located in the Las Margaritas municipality near the Guatemalan border. The area features a number of turquoise blue waterfalls with bridges and lookout points set up to see them up close.
Still others are based on conservation, local culture and other features. The Las Guacamayas Ecotourism Center is located in the Lacandon Jungle on the edge of the Montes Azules reserve. It is centered on the conservation of the red macaw, which is in danger of extinction. The Tziscao Ecotourism Center is centered on a lake with various tones. It is located inside the Lagunas de Montebello National Park, with kayaking, mountain biking and archery. Lacanjá Chansayab is located in the interior of the Lacandon Jungle and a major Lacandon people community. It has some activities associated with ecotourism such as mountain biking, hiking and cabins. The Grutas de Rancho Nuevo Ecotourism Center is centered on a set of caves in which appear capricious forms of stalagmite and stalactites. There is also horseback riding as well.
Culture.
Architecture.
Architecture in the state begins with the archeological sites of the Mayans and other groups who established color schemes and other details that echo in later structures. After the Spanish subdued the area, the building of Spanish style cities began, especially in the highland areas.
Many of the colonial era buildings area related to Dominicans who came from Seville. This Spanish city had much Arabic influence in its architecture. This Arabic influence was transferred to form part of the colonial architecture in Chiapas, especially for structures dating from the 16th to 18th centuries. However, there are a number of architectural styles and influences present in Chiapas colonial structures, including colors and patterns from Oaxaca and Central America along with indigenous ones from Chiapas.
The main colonial structures are the cathedral and Santo Domingo church of San Cristóbal, the Santo Domingo monastery and La Pila in Chiapa de Corzo. The San Cristóbal cathedral has a Baroque facade that was begun in the 16th century but by the time it was finished in the 17th, it had a mix of Spanish, Arabic, and indigenous influences. It is one of the most elaborately decorated in Mexico.
The churches and former monasteries of Santo Domingo, La Merced and San Francisco have ornamentation similar to that of the cathedral. The main structures in Chiapa de Corzo are the Santo Domingo monastery and the La Pila fountain. Santo Domingo has indigenous decorative details such as double headed eagles as well as a statue of the founding monk. In San Cristóbal, the Diego de Mazariegos house has a Plateresque facade, while that of Francisco de Montejo, built later in the 18th century has a mix of Baroque and Neoclassical. Art Deco structures can be found in San Cristóbal and Tapachula in public buildings as well as a number of rural coffee plantations from the Porfirio Díaz era.
Art and literature.
Art in Chiapas is based on the use of color and has strong indigenous influence. This dates back to cave paintings such as those found in Sima de las Cotorras near Tuxtla Gutiérrez and the caverns of Rancho Nuevo where human remains and offerings were also found. The best-known pre Hispanic artwork is the Maya murals of Bonampak, which are the only Mesoamerican murals to have been preserved for over 1500 years. In general, Mayan artwork stands out for its precise depiction of faces and its narrative form. Indigenous forms derive from this background and continue into the colonial period with the use of indigenous color schemes in churches and into modern structures such as the municipal palace in Tapachula. Since the colonial period, the state has produced a large number of painter and sculptures. Noted 20th-century artists include Lázaro Gómez, Ramiro Jiménez Chacón, Héctor Ventura Cruz, Máximo Prado Pozo, and Gabriel Gallegos Ramos.
The two best-known poets from the state include Jaime Sabines and Rosario Castellanos, both from prominent Chiapan families. The first was a merchant and diplomat and the second was a teacher, diplomat, theatre director and the director of the Instituto Nacional Indigenista. Jaime Sabines is widely regarded as Mexico’s most influential contemporary poet. His work celebrates everyday people in common settings.
Music.
The most important instrument in the state is the marimba. In the pre Hispanic period, indigenous peoples had already been producing music with wooden instruments. The marimba was introduced by African slaves brought to Chiapas by the Spanish. However, it achieved its widespread popularity in the early 20th century due to the formation of the Cuarteto Marimbistico de los Hermanos Gómez in 1918, who popularized the instrument and the popular music they play not only in Chiapas but in various parts of Mexico and into the United States. Along with Cuban Juan Arozamena, they composed the piece "Las chiapanecas" considered to be the unofficial anthem of the state. In the 1940s, they were also featured in a number of Mexican films. Marimbas are constructed in Venustiano Carranza, Chiapas de Corzo and Tuxtla Gutiérrez.
Cuisine.
Like the rest of Mesoamerica, the basic diet has been based on corn and Chiapas cooking retains strong indigenous influence. One important ingredient is chipilin, a fragrant and strongly flavored herb and hoja santa, the large anise-scented leaves used in much of southern Mexican cuisine. Chiapan dishes do not incorporate many chili peppers as part of their dishes. Rather, chili peppers are most often found in the condiments. One reason for that is that a local chili pepper, called the simojovel, is far too hot to use except very sparingly. Chiapan cuisine tends to rely more on slightly sweet seasonings in their main dishes such as cinnamon, plantains, prunes and pineapple are often found in meat and poultry dishes.
Tamales are a major part of the diet and often include chipilín mixed into the dough and hoja santa, within the tamale itself or used to wrap it. One tamale native to the state is the "picte", a fresh sweet corn tamale. Tamales juacanes are filled with a mixture of black beans, dried shrimp, and pumpkin seeds.
Meats are centered on the European introduced beef, pork and chicken as many native game animals are in danger of extinction. Meat dishes are frequently accompanied by vegetables such as squash, chayote and carrots. Black beans are the favored type. Beef is favored, especially a thin cut called tasajo usually served in a sauce. Pepita con tasajo is a common dish at festivals especially in Chiapa de Corzo. It consists of a squash seed based sauced over reconstituted and shredded dried beef. As a cattle raising area, beef dishes in Palenque are particularly good. Pux-Xaxé is a stew with beef organ meats and mole sauce made with tomato, chili bolita and corn flour. Tzispolá is a beef broth with chunks of meat, chickpeas, cabbage and various types of chili peppers. Pork dishes include cochito, which is pork in an adobo sauce. In Chiapa de Corzo, their version is cochito horneado, which is a roast suckling pig flavored with adobo. Seafood is a strong component in many dishes along the coast. Turula is dried shrimp with tomatoes. Sausages, ham and other cold cuts are most often made and consumed in the highlands.
In addition to meat dishes, there is chirmol, a cooked tomato sauced flavored with chili pepper, onion and cilantro and zats, butterfly caterpillars from the Altos de Chiapas that are boiled in salted water, then sautéed in lard and eaten with tortillas, limes, and green chili pepper.
Sopa de pan consists of layers of bread and vegetables covered with a broth seasoned with saffron and other flavorings. A Comitán speciality is hearts of palm salad in vinaigrette and Palenque is known for many versions of fried plaintains, including filled with black beans or cheese.
Cheese making is important, especially in the municipalities of Ocosingo, Rayon and Pijijiapan. Ocosingo has its own self-named variety, which is shipped to restaurants and gourmet shops in various parts of the country. Regional sweets include crystallized fruit, coconut candies, flan and compotes. San Cristobal is noted for its sweets, as well as chocolates, coffee and baked goods.
While Chiapas is known for good coffee, there are a number of other local beverages. The oldest is pozol, originally the name for a fermented corn dough. This dough has its origins in the pre Hispanic period. To make the beverage, the dough is dissolved in water and usually flavored with cocoa and sugar, but sometimes it is left to ferment further. It is then served very cold with lots of ice. Taxcalate is a drink made from a powder of toasted corn, achiote, cinnamon and sugar prepared with milk or water. Pumbo is a beverage made with pineapple, club soda, vodka, sugar syrup and lots of ice. Posh is a drink distilled from sugar cane.
Religion.
Like in the rest of Mexico, Christianity was imposed on the native population by the Spanish conquistadors. Catholic beliefs were mixed with indigenous ones to form what is now called "traditionalist" Catholic belief. The Diocese of Chiapas comprises almost the entire state, and centered on San Cristobal de las Casas. It was founded in 1538 by Pope Paul III to evangelize the area with its most famous bishop of that time Bartolomé de las Casas. Evangelization focused on grouping indigenous peoples into communities centered on a church. This bishop not only had these people evangelized in their own language, he worked to introduce many of the crafts still practiced today. While still a majority, only sixty-eight percent of Chiapas residents profess the Catholic faith as of 2010, compared to 83% of the rest of the country.
Many indigenous people mix Christianity with Indian beliefs. One particular area where this is strong is the central highlands in small communities such as San Juan Chamula. In one church in San Cristobal, Mayan rites including the sacrifice of animals is permitted inside the church to ask for good health or to "ward off the evil eye."
Starting in the 1970s, there has been a shift away from traditional Catholic affiliation to Protestant, Evangelical and other Christian denominations. Presbyterians and Pentecostals attracted a large number of converts, with percentages of Protestants in the state rising from five percent in 1970 to twenty-one percent in 2000. This shift has had a political component as well, with those making the switch tending to identify across ethnic boundaries, especially across indigenous ethnic boundaries and being against the traditional power structure. The National Presbyterian Church in Mexico is particularly strong in Chiapas, the state can be described as one of the strongholds of the denomination.
To counter this, the Diocese of Chiapas began to actively re-evangelize among the indigenous populations, and working on their behalf politically as well, following an ideology called liberation theology. Those attracted by this movement call themselves "Word of God" Catholics and identify directly with the Diocese, rather than with local Catholic authorities. Both Protestants and Word of God Catholics tend to oppose traditional cacique leadership and often worked to prohibit the sale of alcohol. The latter had the effect of attracting many women to both movements.
The growing number of Protestants, Evangelicals and Word of God Catholics challenging traditional authority has caused religious strife in a number of indigenous communities. Tensions have been strong, at times, especially in rural areas such as San Juan Chamula. Tension among the groups reached its peak in the 1990s with a large number of people injured during open clashes. In the 1970s, caciques began to expel dissidents from their communities for challenging their power, initially with the use of violence. By 2000, more than 20,000 people had been displaced, but state and federal authorities did not act to stop the expulsions. Today, the situation has quieted but the tension remains, especially in very isolated communities.
Archeology.
The earliest population of Chiapas was in the coastal Soconusco region, where the Chantuto peoples appeared, going back to 5500 BC. This was the oldest Mesoamerican culture discovered to date.
The largest and best-known archeological sites in Chiapas belong to the Mayan civilization. Apart from a few works by Franciscan friars, knowledge of Maya civilisation largely disappeared after the Spanish Conquest. In the mid-19th century, John Lloyd Stephens and Frederick Catherwood traveled though the sites in Chiapas and other Mayan areas and published their writings and illustrations. This led to serious work on the culture including the deciphering of its hieroglyphic writing.
In Chiapas, principal Mayan sites include Palenque, Toniná, Bonampak, Chinkoltic and Tenam Puentes, all or near in the Lacandon Jungle. They are technically more advanced than earlier Olmec sites, which can best be seen in the detailed sculpting and novel construction techniques, including structures of four stories in height. Mayan sites are not only noted for large numbers of structures, but also for glyphs, other inscriptions, and artwork that has provided a relatively complete history of many of the sites.
Palenque is the most important Mayan and archeological site. Tthough much smaller than the huge sites at Tikal or Copán, Palenque contains some of the finest architecture, sculpture and stucco reliefs the Mayans ever produced. The history of the Palenque site begins in 431 with its height under Pakal I (615-683), Chan-Bahlum II (684-702) and Kan-Xul who reigned between 702 and 721. However, the power of Palenque would be lost by the end of the century. Pakal’s tomb was not discovered inside the Temple of Inscriptions until 1949. Today, Palenque is a World Heritage Site and one of the best-known sites in Mexico.
Yaxchilan flourished in the 8th and 9th centuries. The site contains impressive ruins, with palaces and temples bordering a large plaza upon a terrace above the Usumacinta River. The architectural remains extend across the higher terraces and the hills to the south of the river, overlooking both the river itself and the lowlands beyond. Yaxchilan is known for the large quantity of excellent sculpture at the site, such as the monolithic carved stelae and the narrative stone reliefs carved on lintels spanning the temple doorways. Over 120 inscriptions have been identified on the various monuments from the site. The major groups are the Central Acropolis, the West Acropolis and the South Acropolis. The South Acropolis occupies the highest part of the site. The site is aligned with relation to the Usumacinta River, at times causing unconventional orientation of the major structures, such as the two ballcourts.
The city of Bonampak features some of the finest remaining Maya murals. The realistically rendered paintings depict human sacrifices, musicians and scenes of the royal court. In fact the name means “painted murals.” It is centered on a large plaza and has a stairway that leads to the Acropolis. There are also a number of notable steles.
Toniná is near the city of Ocosingo with its main features being the Casa de Piedra (House of Stone) and Acropolis. The latter is a series of seven platforms with various temples and steles. This site was a ceremonial center that flourished between 600 and 900 CE.
Pre-Mayan cultures.
While the Mayan sites are the best-known, there are a number of other important sites in the state, including many older than the Maya civilization.
The oldest sites are in the coastal Soconusco region. This includes the Mokaya culture, the oldest ceramic culture of Mesoamerica. Later, Paso de la Amada became important. Many of these sites are in Mazatan, Chiapas area.
Izapa became an important pre-Mayan site as well.
There are also other ancient sites including Tapachula and Tepcatán, and Pijijiapan. These sites contain numerous embankments and foundations that once lay beneath pyramids and other buildings. Some of these buildings have disappeared and others have been covered by jungle for about 3,000 years, unexplored.
Pijijiapan and Izapa are on the Pacific coast and were the most important pre Hispanic cities for about 1,000 years, as the most important commercial centers between the Mexican Plateau and Central America. Sima de las Cotorras is a sinkhole 140 meters deep with a diameter of 160 meters in the municipality of Ocozocoautla. It contains ancient cave paintings depicting warriors, animals and more. It is best known as a breeding area for parrots, thousands of which leave the area at once at dawn and return at dusk. The state as its Museo Regional de Antropologia e Historia located in Tuxtla Gutiérrez focusing on the pre Hispanic peoples of the state with a room dedicated to its history from the colonial period.
Education.
The average number of years of schooling is 6.7, which is the beginning of middle school, compared to the Mexico average of 8.6. 16.5% have no schooling at all, 59.6% have only primary school/secondary school, 13.7% finish high school or technical school and 9.8% go to university. Eighteen out of every 100 people 15 years or older cannot read or write, compared to 7/100 nationally. Most of Chiapas’ illiterate population are indigenous women, who are often prevented from going to school. School absenteeism and dropout rates are highest among indigenous girls.
There are an estimated 1.4 million students in the state from preschool on up. The state has about 61,000 teachers and just over 17,000 centers of educations. Preschool and primary schools are divided into modalities called general, indigenous, private and community educations sponsored by CONAFE. Middle school is divided into technical, telesecundaria (distance education) and classes for working adults. About 98% of the student population of the state is in state schools. Higher levels of education include "professional medio" (vocational training), general high school and technology-focused high school. At this level, 89% of students are in public schools. There are 105 universities and similar institutions with 58 public and 47 private serving over 60,500 students.
The state university is the Universidad Autónoma de Chiapas (UNACH). It was begun when an organization to establish a state level institution was formed in 1965, with the university itself opening its doors ten years later in 1975. The university project was partially supported by UNESCO in Mexico. It integrated older schools such as the Escuela de Derecho (Law School), which originated in 1679; the Escuela de Ingeniería Civil (School of Civil Engineering), founded in 1966; and the Escuela de Comercio y Administración, which was located in Tuxtla Gutiérrez.
Infrastructure.
The state has approximately 22,517 km of highway with 10,857 federally maintained and 11,660 maintained by the state. Almost all of these kilometers are paved. Major highways include the Las Choapas-Raudales-Ocozocoautla, which links the state to Oaxaca, Veracruz, Puebla and Mexico City. Major airports include Llano San Juan in Ocozocoautla, Francisco Sarabia National Airport (which was replaced by Ángel Albino Corzo International Airport) in Tuxtla Gutiérrez and Corazón de María Airport (which closed in 2010) in San Cristóbal de las Casas. These are used for domestic flights with the airports in Palenque and Tapachula providing international service into Guatemala. There are 22 other airfields in twelve other municipalities. Rail lines extend over 547.8 km. There are two major lines: one in the north of the state that links the center and southeast of the country, and the Costa Panamericana route, which runs from Oaxaca to the Guatemalan border.
There are thirty six AM radio stations and sixteen FM stations. There are thirty seven local television stations and sixty six repeaters.
Chiapas' main port is just outside the city of Tapachula called the Puerto Chiapas. It faces 3,361 meters of ocean, with 3,060 m2 of warehouse space. Next to it, there is an industrial park that covers 2,340,000 m2. Puerto Chiapas has 60,000 m2 of area with a capacity to receive 1,800 containers as well as refrigerated containers. The port serves the state of Chiapas and northern Guatemala. Puerto Chiapas serves to import and export products across the Pacific to Asia, the United States, Canada and South America. It also has connections with the Panama Canal. There is an international airport located eleven km away as well as a railroad terminal ending at the port proper. Over the past five years the port has grown with its newest addition being a terminal for cruise ships with tours to the Izapa site, the Coffee Route, the city of Tapachula, Pozuelos Lake and an Artesanal Chocolate Tour. Principal exports through the port include banana and banana trees, corn, fertilizer and tuna.
Sports.
The capital of the state has a professional football soccer team called Chiapas F.C., located in Tuxtla Gutiérrez. This team made Mexico's first division in 2002. The team changed its symbol and colors to orange and black in 2010. It participated in the Copa Libertadores de América in 2011.

</doc>
<doc id="6788" url="https://en.wikipedia.org/wiki?curid=6788" title="Chrysler Building">
Chrysler Building

The Chrysler Building is an Art Deco-style skyscraper located on the East Side of Midtown Manhattan in New York City, at the intersection of 42nd Street and Lexington Avenue in the Turtle Bay neighborhood. At , the structure was the world's tallest building for 11 months before it was surpassed by the Empire State Building in 1931.
It is the tallest brick building in the world, albeit with a steel frame. After the destruction of the World Trade Center, it was again the second-tallest building in New York City until December 2007, when the spire was raised on the 1,200-foot (365.8 m) Bank of America Tower, pushing the Chrysler Building into third position. In addition, The New York Times Building, which opened in 2007, is exactly level with the Chrysler Building in height. Both buildings were then pushed into fourth position, when the under-construction One World Trade Center surpassed their height, and then to fifth position by 432 Park Avenue which was completed in 2015.
The Chrysler Building is a classic example of Art Deco architecture and considered by many contemporary architects to be one of the finest buildings in New York City. In 2007, it was ranked ninth on the "List of America's Favorite Architecture" by the American Institute of Architects. It was the headquarters of the Chrysler Corporation from 1930 until the mid-1950s. Although the building was built and designed specifically for the car manufacturer, the corporation did not pay for the construction of it and never owned it, as Walter P. Chrysler decided to pay for it himself, so that his children could inherit it.
History.
The Chrysler Building was designed by architect William Van Alen for a project of Walter P. Chrysler. When the ground breaking occurred on September 19, 1928, there was an intense competition in New York City to build the world's tallest skyscraper. Despite a frantic pace (the building was built at an average rate of four floors per week), no workers died during the construction of this skyscraper.
Design beginnings.
Van Alen's original design for the skyscraper called for a decorative jewel-like glass crown. It also featured a base in which the showroom windows were tripled in height and topped by 12 stories with glass-wrapped corners, creating an impression that the tower appeared physically and visually light as if floating in mid-air. The height of the skyscraper was also originally designed to be . However, the design proved to be too advanced and costly for building contractor William H. Reynolds, who disapproved of Van Alen's original plan. The design and lease were then sold to Walter P. Chrysler, who worked with Van Alen and redesigned the skyscraper for additional stories; it was eventually revised to be tall. As Walter Chrysler was the chairman of the Chrysler Corporation and intended to make the building into Chrysler's headquarters, various architectural details and especially the building's gargoyles were modeled after Chrysler automobile products like the hood ornaments of the Plymouth; they exemplify the machine age in the 1920s ("see below").
Construction.
Construction commenced on September 19, 1928. In total, 391,881 rivets were used and approximately 3,826,000 bricks were manually laid, to create the non-loadbearing walls of the skyscraper. Contractors, builders and engineers were joined by other building-services experts to coordinate construction.
Prior to its completion, the building stood about even with a rival project at 40 Wall Street, designed by H. Craig Severance. Severance increased the height of his project and then publicly claimed the title of the world's tallest building. (This distinction excluded structures that were not fully habitable, such as the Eiffel Tower.) In response, Van Alen obtained permission for a long spire and had it secretly constructed inside the frame of the building. The spire was delivered to the site in four different sections. On October 23, 1929, the bottom section of the spire was hoisted to the top of the building's dome and lowered into the 66th floor of the building. The other remaining sections of the spire were hoisted and riveted to the first one in sequential order in just 90 minutes.
Completion.
Upon completion on May 27, 1930, the added height of the spire allowed the Chrysler Building to surpass 40 Wall Street as the tallest building in the world and the Eiffel Tower as the tallest structure. It was the first man-made structure to stand taller than . Van Alen's satisfaction in these accomplishments was likely muted by Walter Chrysler's later refusal to pay the balance of his architectural fee. Less than a year after it opened to the public on May 27, 1930, the Chrysler Building was surpassed in height by the Empire State Building, but the Chrysler Building is still the world's tallest steel-supported brick building. As of November 2, 2011, the building's height was surpassed by the under construction One World Trade Center at the height of 1,106 feet.
Property.
The east building wall of the base out of which the tower rises runs at a slant to the Manhattan street grid, following a property line that predated the Commissioners' Plan of 1811. The land on which the Chrysler Building stands was donated to The Cooper Union for the Advancement of Science and Art in 1902. The land was originally leased to William H. Reynolds, but, when he was unable to raise money for the project, the building and the development rights to the land were acquired by Walter P. Chrysler in 1928. Contrary to popular belief, the Chrysler Corporation was never involved in the construction or ownership of the Chrysler Building, although it was built and designed for the corporation and served as its headquarters until the mid-1950s. It was a project of Walter P. Chrysler for his children.
The ownership of the building has changed several times. The Chrysler family sold the building in 1953 to William Zeckendorf, and in 1957, it was purchased by Sol Goldman and Alex DiLorenzo, and owned by Massachusetts Mutual Life Insurance Company. The lobby was refurbished and the facade renovated in 1978–1979. The building was bought by Jack Kent Cooke in 1979. The spire underwent a restoration that was completed in 1995. In 1998, Tishman Speyer Properties and the Travelers Insurance Group bought the Chrysler Building and the adjoining Kent Building in 1997 for about $220 million (equal to $ million in ) from a consortium of banks and the estate of Jack Kent Cooke. Tishman Speyer Properties had negotiated a 150-year lease on the land from the Cooper Union and the college continues to own both the land under the Chrysler Building and the building itself. Cooper Union's name is on the deed.
In 2001, a 75% stake in the building management contract was sold, for US$300 million (equal to $ million in ), to TMW, the German arm of an Atlanta-based investment fund. On June 11, 2008 it was reported that the Abu Dhabi Investment Council was in negotiations to buy TMW's 75% economic interest, and a 15% interest from Tishman Speyer Properties in the building, and a share of the Trylons retail structure next door for US$800 million. On July 9, 2008 it was announced that the transaction had been completed, and that the Abu Dhabi Investment Council was now the 90% owner of the building.
Architecture.
The Chrysler Building is considered a leading example of Art Deco architecture. The corners of the 61st floor are graced with eagles; on the 31st floor, the corner ornamentation are replicas of the 1929 Chrysler radiator caps. The building is constructed of masonry, with a steel frame, and metal cladding. The building currently contains a total of 3,862 windows on its facades. Inside, there are four banks of 8 elevators designed by the Otis Elevator Corporation. The building was declared a National Historic Landmark in 1976, and a New York City Landmark in 1978.
The Chrysler Building is also renowned and recognized for its terraced crown. Composed of seven radiating terraced arches, Van Alen's design of the crown is a cruciform groin vault constructed into seven concentric members with transitioning setbacks, mounted up one behind another. The stainless-steel cladding is ribbed and riveted in a radiating sunburst pattern with many triangular vaulted windows, transitioning into smaller segments of the seven narrow setbacks of the facade of the terraced crown. The entire crown is clad with silvery "Enduro KA-2" metal, an austenitic stainless steel developed in Germany by Krupp and marketed under the trade name "Nirosta" (a German acronym for "nichtrostender Stahl", meaning "non-rusting steel").
When the building first opened, it contained a public viewing gallery on the 71st floor, which was closed to the public in 1945. This floor is now the highest occupied floor of the Chrysler Building, it was occupied by an office space management firm in 1986. The private Cloud Club occupied a three-floor high space from the 66th–68th floors, but closed in the late 1970s. Above the 71st floor, the stories of the building are designed mostly for exterior appearance, functioning mainly as landings for the stairway to the spire. These top stories are very narrow with low, sloped ceilings, and are useful only for holding radio-broadcasting and other mechanical and electrical equipment. Television station WCBS-TV (Channel 2) originally transmitted from the top of the Chrysler in the 1940s and early 1950s, before moving to the Empire State Building. For many years, WPAT-FM and WTFM (now WKTU) also used the Chrysler Building as a transmission site, but they also moved to the Empire State Building by the 1970s. There are currently no commercial broadcast stations located at the Chrysler Building.
There are two sets of lighting in the top spires and decoration. The first are the V-shaped lighting inserts in the steel of the building itself. Added later were groups of floodlights that are on mast arms directed back at the building. This allows the top of the building to be lit in many colors for special occasions.
Representation.
The Chrysler Building has been shown in several movies that take place in New York. In the summer of 2005, New York's own Skyscraper Museum asked one hundred architects, builders, critics, engineers, historians, and scholars, among others, to choose their 10 favorites among 25 New York towers. The Chrysler Building came in first place as 90% of them placed the building in their top-10 favorite buildings.
The Chrysler Building's distinctive profile has inspired similar skyscrapers worldwide, including One Liberty Place in Philadelphia.
References.
Notes
Further reading

</doc>
<doc id="6794" url="https://en.wikipedia.org/wiki?curid=6794" title="Comet Shoemaker–Levy 9">
Comet Shoemaker–Levy 9

Comet Shoemaker–Levy 9 (formally designated D/1993 F2) was a comet that broke apart in July 1992 and collided with Jupiter in July 1994, providing the first direct observation of an extraterrestrial collision of Solar System objects. This generated a large amount of coverage in the popular media, and the comet was closely observed by astronomers worldwide. The collision provided new information about Jupiter and highlighted its role in reducing space debris in the inner Solar System.
The comet was discovered by astronomers Carolyn and Eugene M. Shoemaker and David Levy. Shoemaker–Levy 9 had been captured by Jupiter and was orbiting the planet at the time. It was located on the night of March 24, 1993 in a photograph taken with the Schmidt telescope at the Palomar Observatory in California. It was the first comet observed to be orbiting a planet, and had probably been captured by Jupiter around 20 – 30 years earlier.
Calculations showed that its unusual fragmented form was due to a previous closer approach to Jupiter in July 1992. At that time, the orbit of Shoemaker–Levy 9 passed within Jupiter's Roche limit, and Jupiter's tidal forces had acted to pull apart the comet. The comet was later observed as a series of fragments ranging up to in diameter. These fragments collided with Jupiter's southern hemisphere between July 16 and July 22, 1994 at a speed of approximately or . The prominent scars from the impacts were more easily visible than the Great Red Spot and persisted for many months.
Discovery.
While conducting a program of observations designed to uncover near-Earth objects, the Shoemakers and Levy discovered Comet Shoemaker–Levy 9 on the night of March 24, 1993 in a photograph taken with the Schmidt telescope at the Palomar Observatory in California. The comet was thus a serendipitous discovery, but one that quickly overshadowed the results from their main observing program.
Comet Shoemaker–Levy 9 was the ninth periodic comet (a comet whose orbital period is 200 years or less) discovered by the Shoemakers and Levy, hence its name. It was their eleventh comet discovery overall including their discovery of two non-periodic comets, which use a different nomenclature. The discovery was announced in IAU Circular 5725 on March 27, 1993.
The discovery image gave the first hint that comet Shoemaker–Levy 9 was an unusual comet, as it appeared to show multiple nuclei in an elongated region about 50 arcseconds long and 10 arcseconds wide. Brian G. Marsden of the Central Bureau for Astronomical Telegrams noted that the comet lay only about 4 degrees from Jupiter as seen from Earth, and that while this could of course be a line of sight effect, its apparent motion in the sky suggested that it was physically close to the giant planet. Because of this, he suggested that the Shoemakers and David Levy had discovered the fragments of a comet that had been disrupted by Jupiter's gravity.
Jupiter-orbiting comet.
Orbital studies of the new comet soon revealed that it was orbiting Jupiter rather than the Sun, unlike all other comets known at the time. Its orbit around Jupiter was very loosely bound, with a period of about 2 years and an apoapsis (the point in the orbit farthest from the planet) of . Its orbit around the planet was highly eccentric ("e" = 0.9986).
Tracing back the comet's orbital motion revealed that it had been orbiting Jupiter for some time. It seems most likely that it was captured from a solar orbit in the early 1970s, although the capture may have occurred as early as the mid-1960s. Several other observers found images of the comet in precovery images obtained before March 24, including Kin Endate from a photograph exposed on March 15, S. Otomo on March 17, and a team led by Eleanor Helin from images on March 19. No precovery images dating back to earlier than March 1993 have been found. Before the comet was captured by Jupiter, it was probably a short-period comet with an aphelion just inside Jupiter's orbit, and a perihelion interior to the asteroid belt.
The volume of space within which an object can be said to orbit Jupiter is defined by Jupiter's Hill sphere (also called the Roche sphere). When the comet passed Jupiter in the late 1960s or early 1970s, it happened to be near its aphelion, and found itself slightly within Jupiter's Hill sphere. Jupiter's gravity nudged the comet towards it. Because the comet's motion with respect to Jupiter was very small, it fell almost straight toward Jupiter, which is why it ended up on a Jupiter-centric orbit of very high eccentricitythat is to say, the ellipse was nearly flattened out.
The comet had apparently passed extremely close to Jupiter on July 7, 1992, just over above the planet's cloud topsa smaller distance than Jupiter's radius of , and well within the orbit of Jupiter's innermost moon Metis and the planet's Roche limit, inside which tidal forces are strong enough to disrupt a body held together only by gravity. Although the comet had approached Jupiter closely before, the July 7 encounter seemed to be by far the closest, and the fragmentation of the comet is thought to have occurred at this time. Each fragment of the comet was denoted by a letter of the alphabet, from "fragment A" through to "fragment W", a practice already established from previously observed broken-up comets.
More exciting for planetary astronomers was that the best orbital calculations suggested that the comet would pass within of the center of Jupiter, a distance smaller than the planet's radius, meaning that there was an extremely high probability that SL9 would collide with Jupiter in July 1994. Studies suggested that the train of nuclei would plow into Jupiter's atmosphere over a period of about five days.
Predictions for the collision.
The discovery that the comet was likely to collide with Jupiter caused great excitement within the astronomical community and beyond, as astronomers had never before seen two significant Solar System bodies collide. Intense studies of the comet were undertaken, and as its orbit became more accurately established, the possibility of a collision became a certainty. The collision would provide a unique opportunity for scientists to look inside Jupiter's atmosphere, as the collisions were expected to cause eruptions of material from the layers normally hidden beneath the clouds.
Astronomers estimated that the visible fragments of SL9 ranged in size from a few hundred metres to two kilometres across, suggesting that the original comet may have had a nucleus up to acrosssomewhat larger than Comet Hyakutake, which became very bright when it passed close to the Earth in 1996. One of the great debates in advance of the impact was whether the effects of the impact of such small bodies would be noticeable from Earth, apart from a flash as they disintegrated like giant meteors. The most optimistic prediction was that large, asymmetric ballistic fireballs would rise above the limb of Jupiter and into sunlight to be visible from Earth. 
Other suggested effects of the impacts were seismic waves travelling across the planet, an increase in stratospheric haze on the planet due to dust from the impacts, and an increase in the mass of the Jovian ring system. However, given that observing such a collision was completely unprecedented, astronomers were cautious with their predictions of what the event might reveal.
Impacts.
Anticipation grew as the predicted date for the collisions approached, and astronomers trained terrestrial telescopes on Jupiter. Several space observatories did the same, including the Hubble Space Telescope, the ROSAT X-ray-observing satellite, and significantly the "Galileo" spacecraft, then on its way to a rendezvous with Jupiter scheduled for 1995. While the impacts took place on the side of Jupiter hidden from Earth, "Galileo", then at a distance of 1.6 AU from the planet, was able to see the impacts as they occurred. Jupiter's rapid rotation brought the impact sites into view for terrestrial observers a few minutes after the collisions.
Two other satellites made observations at the time of the impact: the Ulysses spacecraft, primarily designed for solar observations, was pointed towards Jupiter from its location 2.6 AU away, and the distant "Voyager 2" probe, some 44 AU from Jupiter and on its way out of the Solar System following its encounter with Neptune in 1989, was programmed to look for radio emission in the 1–390 kHz range.
The first impact occurred at 20:13 UTC on July 16, 1994, when fragment A of the nucleus entered Jupiter's southern hemisphere at a speed of about 60 km/s. Instruments on "Galileo" detected a fireball which reached a peak temperature of about 24,000 K, compared to the typical Jovian cloudtop temperature of about 130 K, before expanding and cooling rapidly to about 1500 K after 40 s. The plume from the fireball quickly reached a height of over 3,000 km. A few minutes after the impact fireball was detected, "Galileo" measured renewed heating, probably due to ejected material falling back onto the planet. Earth-based observers detected the fireball rising over the limb of the planet shortly after the initial impact.
Despite published predictions, astronomers had not expected to see the fireballs from the impacts and did not have any idea in advance how visible the other atmospheric effects of the impacts would be from Earth. Observers soon saw a huge dark spot after the first impact. The spot was visible even in very small telescopes, and was about (one Earth radius) across. This and subsequent dark spots were thought to have been caused by debris from the impacts, and were markedly asymmetric, forming crescent shapes in front of the direction of impact.
Over the next 6 days, 21 distinct impacts were observed, with the largest coming on July 18 at 07:33 UTC when fragment G struck Jupiter. This impact created a giant dark spot over 12,000 km across, and was estimated to have released an energy equivalent to 6,000,000 megatons of TNT (600 times the world's nuclear arsenal). Two impacts 12 hours apart on July 19 created impact marks of similar size to that caused by fragment G, and impacts continued until July 22, when fragment W struck the planet.
Observations and discoveries.
Chemical studies.
Observers hoped that the impacts would give them a first glimpse of Jupiter beneath the cloud tops, as lower material was exposed by the comet fragments punching through the upper atmosphere. Spectroscopic studies revealed absorption lines in the Jovian spectrum due to diatomic sulfur (S2) and carbon disulfide (CS2), the first detection of either in Jupiter, and only the second detection of S2 in any astronomical object. Other molecules detected included ammonia (NH3) and hydrogen sulfide (H2S). The amount of sulfur implied by the quantities of these compounds was much greater than the amount that would be expected in a small cometary nucleus, showing that material from within Jupiter was being revealed. Oxygen-bearing molecules such as sulfur dioxide were not detected, to the surprise of astronomers.
As well as these molecules, emission from heavy atoms such as iron, magnesium and silicon was detected, with abundances consistent with what would be found in a cometary nucleus. While substantial water was detected spectroscopically, it was not as much as predicted beforehand, meaning that either the water layer thought to exist below the clouds was thinner than predicted, or that the cometary fragments did not penetrate deeply enough. The relatively low levels of water were later confirmed by Galileo's atmospheric probe, which explored Jupiter's atmosphere directly.
Waves.
As predicted beforehand, the collisions generated enormous waves which swept across the planet at speeds of and were observed for over two hours after the largest impacts. The waves were thought to be travelling within a stable layer acting as a waveguide, and some scientists believed the stable layer must lie within the hypothesised tropospheric water cloud. However, other evidence seemed to indicate that the cometary fragments had not reached the water layer, and the waves were instead propagating within the stratosphere.
Other observations.
Radio observations revealed a sharp increase in continuum emission at a wavelength of 21 cm after the largest impacts, which peaked at 120% of the normal emission from the planet. This was thought to be due to synchrotron radiation, caused by the injection of relativistic electronselectrons with velocities near the speed of lightinto the Jovian magnetosphere by the impacts.
About an hour after fragment K entered Jupiter, observers recorded auroral emission near the impact region, as well as at the antipode of the impact site with respect to Jupiter's strong magnetic field. The cause of these emissions was difficult to establish due to a lack of knowledge of Jupiter's internal magnetic field and of the geometry of the impact sites. One possible explanation was that upwardly accelerating shock waves from the impact accelerated charged particles enough to cause auroral emission, a phenomenon more typically associated with fast-moving solar wind particles striking a planetary atmosphere near a magnetic pole.
Some astronomers had suggested that the impacts might have a noticeable effect on the Io torus, a torus of high-energy particles connecting Jupiter with the highly volcanic moon Io. High resolution spectroscopic studies found that variations in the ion density, rotational velocity, and temperatures at the time of impact and afterwards were within the normal limits.
Post-impact analysis.
Several models were devised to compute the density and size of Shoemaker–Levy 9. Its average density was calculated to be about 0.5 g cm−3; the breakup of a much less dense comet would not have resembled the observed string of objects. The size of the parent comet was calculated to be about 1.8 km in diameter. These predictions were among the few that were actually confirmed by subsequent observation.
One of the surprises of the impacts was the small amount of water revealed compared to prior predictions. Before the impact, models of Jupiter's atmosphere had indicated that the break-up of the largest fragments would occur at atmospheric pressures of anywhere from 30 kilopascals to a few tens of megapascals (from 0.3 to a few hundred bar), with some predictions that the comet would penetrate a layer of water and create a bluish shroud over that region of Jupiter.
Astronomers did not observe large amounts of water following the collisions, and later impact studies found that fragmentation and destruction of the cometary fragments in an 'airburst' probably occurred at much higher altitudes than previously expected, with even the largest fragments being destroyed when the pressure reached , well above the expected depth of the water layer. The smaller fragments were probably destroyed before they even reached the cloud layer.
Longer-term effects.
The visible scars from the impacts could be seen on Jupiter for many months. They were extremely prominent, and observers described them as more easily visible even than the Great Red Spot. A search of historical observations revealed that the spots were probably the most prominent transient features ever seen on the planet, and that while the Great Red Spot is notable for its striking color, no spots of the size and darkness of those caused by the SL9 impacts have ever been recorded before.
Spectroscopic observers found that ammonia and carbon disulfide persisted in the atmosphere for at least fourteen months after the collisions, with a considerable amount of ammonia being present in the stratosphere as opposed to its normal location in the troposphere.
Counterintuitively, the atmospheric temperature dropped to normal levels much more quickly at the larger impact sites than at the smaller sites: at the larger impact sites, temperatures were elevated over a region wide, but dropped back to normal levels within a week of the impact. At smaller sites, temperatures 10 K higher than the surroundings persisted for almost two weeks. Global stratospheric temperatures rose immediately after the impacts, then fell to below pre-impact temperatures 2–3 weeks afterwards, before rising slowly to normal temperatures.
Frequency of impacts.
SL9 is not unique in having orbited Jupiter for a time; five comets, (including 82P/Gehrels, 147P/Kushida–Muramatsu, and 111P/Helin–Roman–Crockett) are known to have been temporarily captured by the planet.
Cometary orbits around Jupiter are unstable, as they will be highly elliptical and likely to be strongly perturbed by the Sun's gravity at apojove (the furthest point on the orbit from the planet).
By far the most massive planet in the Solar System, Jupiter can capture objects relatively frequently, but the size of SL9 makes it a rarity: one post-impact study estimated that comets 0.3 km in diameter impact the planet once in approximately 500 years and those in diameter do so just once in every 6,000 years.
There is very strong evidence that comets have previously been fragmented and collided with Jupiter and its satellites. During the Voyager missions to the planet, planetary scientists identified 13 crater chains on Callisto and three on Ganymede, the origin of which was initially a mystery. Crater chains seen on the Moon often radiate from large craters, and are thought to be caused by secondary impacts of the original ejecta, but the chains on the Jovian moons did not lead back to a larger crater. The impact of SL9 strongly implied that the chains were due to trains of disrupted cometary fragments crashing into the satellites.
Impact of July 19, 2009.
On July 19, 2009, a new black spot about the size of the Pacific Ocean appeared in Jupiter's southern hemisphere. Thermal infrared measurements showed the impact site was warm and spectroscopic analysis detected the production of excess hot ammonia and silica-rich dust in the upper regions of Jupiter's atmosphere. Scientists have concluded that another impact event had occurred, but this time a more compact and strong object, probably a small undiscovered asteroid, was the cause.
Jupiter as a "cosmic vacuum cleaner".
The impact of SL9 highlighted Jupiter's role as a "cosmic vacuum cleaner" (or in deference to the ancients' planetary correspondences to the major organs in the human body, a "cosmic liver") for the inner Solar System. The planet's strong gravitational influence leads to many small comets and asteroids colliding with the planet, and the rate of cometary impacts on Jupiter is thought to be between 2000-8000 times higher than the rate on Earth. 
The extinction of the dinosaurs at the end of the Cretaceous period is generally believed to have been caused by the Cretaceous–Paleogene impact event which created the Chicxulub crater, demonstrating that impacts are a serious threat to life on Earth. Astronomers have speculated that without Jupiter to mop up potential impactors, extinction events might have been more frequent on Earth, and complex life might not have been able to develop. This is part of the argument used in the Rare Earth hypothesis.
In 2009, it was shown that the presence of a smaller planet at Jupiter's position in the Solar System might increase the impact rate of comets on the Earth significantly. A planet of Jupiter's mass still seems to provide increased protection against asteroids, but the total effect on all orbital bodies within the Solar System is unclear. Computer simulations in 2016 have continued to erode the theory.

</doc>
<doc id="6796" url="https://en.wikipedia.org/wiki?curid=6796" title="Ceres Brewery">
Ceres Brewery

Ceres Brewery was a brewery company located in Aarhus, Denmark. It was part of Royal Unibrew. The factories in central Aarhus, was closed in 2008 and the grounds are now being redeveloped into a new neighbourhood of the city, known as CeresByen (The CeresCity).
History.
Ceres Brewery was founded by a grocer named Malthe Conrad Lottrup, with help from the chemists A. S. Aagard and Knud Redelien, as the city's seventh brewery. It was named after the Roman goddess Ceres, and its opening was announced in the local newspaper, "Stiftstidende", in 1856.
The brewery was successful, and Lottrup became one of the most prominent people of Aarhus. After ten years, he expanded the brewery, adding a grand new building as his own private residence, where he entertained other local figures.
Lottrup's son-in-law, Laurits Christian Meulengracht, took over the running of the brewery after that, and was in charge for nearly thirty years, expanding it further. He then sold it to another brewery, Østjyske Bryggerier A/S.
The brewery gained more esteem in 1914, when it was made "Purveyor to the Royal Danish Court".
In 2008 the factory closed because the brewery could not live up to the expectations from its owner Royal Unibrew.

</doc>
<doc id="6799" url="https://en.wikipedia.org/wiki?curid=6799" title="COBOL">
COBOL

COBOL (, an acronym for "common business-oriented language") is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages. Most programming in COBOL is now purely to maintain existing applications.
COBOL was designed in 1959, by CODASYL and was partly based on previous programming language design work by Grace Hopper, commonly referred to as "the (grand)mother of COBOL". It was created as part of a US Department of Defense effort to create a portable programming language for data processing. Intended as a stopgap, the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption. It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is "ISO/IEC 1989:2014".
COBOL has an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like , COBOL has a more English-like syntax (in this case, ).
COBOL code is split into four divisions (identification, environment, data and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.
Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for businessmen, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.
COBOL has been criticized throughout its life, however, for its verbosity, design process and poor support for structured programming, which resulted in monolithic and incomprehensible programs.
History and specification.
Background.
In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.
In April 1959, representatives from academia, computer users and manufacturers met at the University of Pennsylvania to organize a formal meeting on common business languages. Representatives among others, included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.
The group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD, who thought that they "thoroughly understood" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.
Phillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.
COBOL 60.
On May 28 and 29 of 1959 (exactly one year after the Zürich ALGOL 58 meeting), a meeting was held at the Pentagon to discuss the creation of a common programming language for business. It was attended by 41 people and was chaired by Phillips. The Department of Defense was concerned about whether it could run the same data processing programs on different computers. FORTRAN, the only mainstream language at the time, lacked the features needed to write such programs.
Representatives enthusiastically described a language that could work in a wide variety of environments, from banking and insurance to utilities and inventory control. They agreed unanimously that more people should be able to program and that the new language should not be restricted by the limitations of contemporary technology. A majority agreed that the language should make maximal use of English, be capable of change, be machine-independent and be easy to use, even at the expense of power.
The meeting resulted in the creation of a steering committee and short-, intermediate- and long-range committees. The short-range committee was given to September (three months) to produce specifications for an interim language, which would then be improved upon by the other committees. Their official mission, however, was to identify the strengths and weaknesses of existing programming languages and did not explicitly direct them to create a new language.
The deadline was met with disbelief by the short-range committee.
One member, Betty Holberton, described the three-month deadline as "gross optimism" and doubted that the language really would be a stopgap.
The steering committee met on June 4 and agreed to name the entire activity as the "Committee on Data Systems Languages", or CODASYL, and to form an executive committee.
The short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology). The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.
The committee mainly examined the FLOW-MATIC, AIMACO and COMTRAN programming languages.
The FLOW-MATIC language was particularly influential because it had been implemented and because AIMACO was a derivative of it with only minor changes.
FLOW-MATIC's inventor, Grace Hopper, also served as a technical adviser to the committee. FLOW-MATIC's major contributions to COBOL were long variable names, English words for commands and the separation of data descriptions and instructions.
IBM's COMTRAN language, invented by Bob Bemer, was regarded as a competitor to FLOW-MATIC by a short-range committee made up of colleagues of Grace Hopper.
Some of its features were not incorporated into COBOL so that it would not look like IBM had dominated the design process, and Jean Sammet said in 1981 that there had been a "strong anti-IBM bias" from some committee members (herself included).
In one case, after Roy Goldfinger, author of the COMTRAN manual and intermediate-range committee member, attended a subcommittee meeting to support his language and encourage the use of algebraic expressions, Grace Hopper sent a memo to the short-range committee reiterating Sperry Rand's efforts to create a language based on English.
In 1980, Grace Hopper commented that "COBOL 60 is 95% FLOW-MATIC" and that COMTRAN had had an "extremely small" influence. Furthermore, she said that she would claim that work was influenced by both FLOW-MATIC and COMTRAN only to "keep other people happy they wouldn't try to knock us out".
Features from COMTRAN incorporated into COBOL included formulas, the clause, an improved codice_1 statement, which obviated the need for GO TOs, and a more robust file management system.
The usefulness of the committee's work was subject of great debate. While some members thought the language had too many compromises and was the result of design by committee, others felt it was better than the three languages examined. Some felt the language was too complex; others, too simple.
Controversial features included those some considered useless or too advanced for data processing users. Such features included boolean expressions, formulas and table "" (indices). Another point of controversy was whether to make keywords context-sensitive and the effect that would have on readability. Although context-sensitive keywords were rejected, the approach was later used in PL/I and partially in COBOL from 2002. Little consideration was given to interactivity, interaction with operating systems (few existed at that time) and functions (thought of as purely mathematical and of no use in data processing).
The specifications were presented to the Executive Committee on September 4. They fell short of expectations: Joseph Wegstein noted that "it contains rough spots and requires some additions", and Bob Bemer later described them as a "hodgepodge". The subcommittee was given until December to improve it.
At a mid-September meeting, the committee discussed the new language's name. Suggestions included "BUSY" (Business System), "INFOSYL" (Information System Language) and "COCOSYL" (Common Computer Systems Language). The name "COBOL" was suggested by Bob Bemer.
In October, the intermediate-range committee received copies of the FACT language specification created by Roy Nutt. Its features impressed the committee so much that they passed a resolution to base COBOL on it.
This was a blow to the short-range committee, who had made good progress on the specification. Despite being technically superior, FACT had not been created with portability in mind or through manufacturer and user consensus. It also lacked a demonstrable implementation, allowing supporters of a FLOW-MATIC-based COBOL to overturn the resolution. RCA representative Howard Bromberg also blocked FACT, so that RCA's work on a COBOL implementation would not go to waste.
It soon became apparent that the committee was too large for any further progress to be made quickly. A frustrated Howard Bromberg bought a $15 tombstone with "COBOL" engraved on it and sent it to Charles Phillips to demonstrate his displeasure.
A sub-committee was formed to analyze existing languages and was made up of six individuals:
The sub-committee did most of the work creating the specification, leaving the short-range committee to review and modify their work before producing the finished specification.
The specifications were approved by the Executive Committee on January 3, 1960, and sent to the government printing office, which printed these as "COBOL 60". The language's stated objectives were to allow efficient, portable programs to be easily written, to allow users to move to new systems with minimal effort and cost, and to be suitable for inexperienced programmers.
The CODASYL Executive Committee later created the COBOL Maintenance Committee to answer questions from users and vendors and to improve and expand the specifications.
During 1960, the list of manufacturers planning to build COBOL compilers grew. By September, five more manufacturers had joined CODASYL (Bendix, Control Data Corporation, General Electric (GE), National Cash Register and Philco), and all represented manufacturers had announced COBOL compilers. GE and IBM planned to integrate COBOL into their own languages, GECOM and COMTRAN, respectively. In contrast, International Computers and Tabulators planned to replace their language, CODEL, with COBOL.
Meanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.
On December 6 and 7, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.
The relative influences of which languages were used continues to this day in the recommended advisory printed in all COBOL reference manuals:
COBOL-61 to COBOL-65.
Many logical flaws were found in "COBOL 60", leading GE's Charles Katz to warn that it could not be interpreted unambiguously. A reluctant short-term committee enacted a total cleanup and, by March 1963, it was reported that COBOL's syntax was as definable as ALGOL's, although semantic ambiguities remained.
Early COBOL compilers were primitive and slow. A 1962 US Navy evaluation found compilation speeds of 3–11 statements per minute. By mid-1964, they had increased to 11–1000 statements per minute. It was observed that increasing memory would drastically increase speed and that compilation costs varied wildly: costs per statement were between $0.23 and $18.91.
In late 1962, IBM announced that COBOL would be their primary development language and that development of COMTRAN would cease.
The COBOL specification was revised three times in the five years after its publication.
COBOL-60 was replaced in 1961 by COBOL-61. This was then replaced by the COBOL-61 Extended specifications in 1963, which introduced the sort and report writer facilities.
The added facilities corrected flaws identified by Honeywell in late 1959 in a letter to the short-range committee.
COBOL Edition 1965 brought further clarifications to the specifications and introduced facilities for handling mass storage files and tables.
COBOL-68.
Efforts began to standardize COBOL to overcome incompatibilities between versions. In late 1962, both ISO and the United States of America Standards Institute (now ANSI) formed groups to create standards. ANSI produced "USA Standard COBOL X3.23" in August 1968, which became the cornerstone for later versions. This version was known as American National Standard (ANS) COBOL and was adopted by ISO in 1972.
COBOL-74.
By 1970, COBOL had become the most widely used programming language in the world.
Independently of the ANSI committee, the CODASYL Programming Language Committee was working on improving the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and file merging facilities as well as improved string-handling and library inclusion features.
Although CODASYL was independent of the ANSI committee, the "CODASYL Journal of Development" was used by ANSI to identify features that were popular enough to warrant implementing.
The Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.
The Programming Language Committee was not well-known, however. The vice-president, William Rinehuls, complained that two-thirds of the COBOL community did not know of the committee's existence. It was also poor, lacking the funds to make public documents, such as minutes of meetings and change proposals, freely available.
In 1974, ANSI published a revised version of (ANS) COBOL, containing new features such as file organizations, the statement and the segmentation module.
Deleted features included the statement, the statement (which was replaced by ) and the implementer-defined random access module (which was superseded by the new sequential and relative I/O modules). These made up 44 changes, which rendered existing statements incompatible with the new standard.
The report writer was slated to be removed from COBOL, but was reinstated before the standard was published. ISO later adopted the updated standard in 1978.
COBOL-85.
In June 1978, work began on revising COBOL-74. The proposed standard (commonly called COBOL-80) differed significantly from the previous one, causing concerns about incompatibility and conversion costs. In January 1981, Joseph T. Brophy, Senior Vice-President of Travelers Insurance, threatened to sue the standard committee because it was not upwards compatible with COBOL-74. Mr. Brophy described previous conversions of their 40-million-line code base as "non-productive" and a "complete waste of our programmer resources".
Later that year, the Data Processing Management Association (DPMA) said it was "strongly opposed" to the new standard, citing "prohibitive" conversion costs and enhancements that were "forced on the user".
During the first public review period, the committee received 2,200 responses, of which 1,700 were negative form letters.
Other responses were detailed analyses of the effect COBOL-80 would have on their systems; conversion costs were predicted to be at least 50 cents per line of code. Fewer than a dozen of the responses were in favor of the proposed standard.
In 1983, the DPMA withdrew its opposition to the standard, citing the responsiveness of the committee to public concerns. In the same year, a National Bureau of Standards study concluded that the proposed standard would present few problems. A year later, a COBOL-80 compiler was released to DEC VAX users, who noted that conversion of COBOL-74 programs posed few problems. The new codice_2 statement and inline codice_3 were particularly well received and improved productivity, thanks to simplified control flow and debugging.
The second public review drew another 1,000 (mainly negative) responses, while the last drew just 25, by which time many concerns had been addressed.
In late 1985, ANSI published the revised standard. Sixty features were changed or deprecated and many were added, such as:
The standard was adopted by ISO the same year. Two amendments followed in 1989 and 1993, the first introducing intrinsic functions and the other providing corrections. ISO adopted the amendments in 1991 and 1994 respectively, before subsequently taking primary ownership and development of the standard.
COBOL 2002 and object-oriented COBOL.
In 1997, Gartner Group estimated that there were a total of 200 billion lines of COBOL in existence, which ran 80% of all business programs.
In the early 1990s, work began on adding object-orientation in the next full revision of COBOL. Object-oriented features were taken from C++ and Smalltalk.
The initial estimate was to have this revision completed by 1997, and an ISO Committee Draft (CD) was available by 1997. Some vendors (including Micro Focus, Fujitsu, and IBM) introduced object-oriented syntax based on drafts of the full revision. The final approved ISO standard was approved and published in late 2002.
Fujitsu/GTSoftware, Micro Focus and RainCode introduced object-oriented COBOL compilers targeting the .NET Framework.
There were many other new features, many of which had been in the "CODASYL COBOL Journal of Development" since 1978 and had missed the opportunity to be included in COBOL-85. These other features included:
Three corrigenda were published for the standard: two in 2006 and one in 2009.
COBOL 2014.
Between 2003 and 2009, three technical reports were produced describing object finalization, XML processing and collection classes for COBOL.
COBOL 2002 suffered from poor support: no compilers completely supported the standard. Micro Focus found that it was due to a lack of user demand for the new features and due to the abolition of the NIST test suite, which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.
COBOL 2014 includes the following changes:
Legacy.
COBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, VME, Unix, OpenVMS and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code and 5 billion lines more being written annually.
Near the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields. After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.
The authors said that the survey data suggest "a gradual decline in the importance of Cobol in application development over the 10 years unless ... integration with other languages and technologies can be adopted".
In 2006 and 2012, "Computerworld" surveys found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software. 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it was cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.
Features.
Syntax.
COBOL has an English-like syntax, which is used to describe nearly everything in a program. For example, a condition can be expressed as   or more concisely as    or  . More complex conditions can be "abbreviated" by removing repeated conditions and variables. For example,    can be shortened to . As a consequence of this English-like syntax, COBOL has over 300 keywords. Some of the keywords are simple alternative or pluralized spellings of the same word, which provides for more English-like statements and clauses; e.g., the and keywords can be used interchangeably, as can and , and and .
Each COBOL program is made up of four basic lexical items: words, literals, picture character-strings (see ) and separators. Words include reserved words and user-defined identifiers. They are up to 31 characters long and may include letters, digits, hyphens and underscores. Literals include numerals (e.g. ) and strings (e.g. ). Separators include the space character and commas and semi-colons followed by a space.
A COBOL program is split into four divisions: the identification division, the environment division, the data division and the procedure division. The identification division specifies the name and type of the source element and is where classes and interfaces are specified. The environment division specifies any program features that depend on the system running it, such as files and character sets. The data division is used to declare variables and parameters. The procedure division contains the program's statements. Each division is sub-divided into sections, which are made up of paragraphs.
Code format.
COBOL can be written in two formats: fixed (the default) or free. In fixed-format, code must be aligned to fit in certain areas. Until COBOL 2002, these were:
In COBOL 2002, Areas A and B were merged and extended to column 255, and the program name area was removed.
COBOL 2002 also introduced free-format code. Free-format code can be placed in any column of the file, as in newer programming languages. Comments are specified using codice_12, which can be placed anywhere and can also be used in fixed-format source code. Continuation lines are not present, and the codice_13 directive replaces the codice_14 indicator.
Identification division.
The identification division identifies the following code entity and contains the definition of a class or interface.
Object-oriented programming.
Classes and interfaces have been in COBOL since 2002. Classes have factory objects, containing class methods and variables, and instance objects, containing instance methods and variables. Inheritance and interfaces provide polymorphism. Support for generic programming is provided through parameterized classes, which can be instantiated to use any class or interface. Objects are stored as references which may be restricted to a certain type. There are two ways of calling a method: the statement, which acts similarly to , or through inline method invocation, which is analogous to using functions.
<syntaxhighlight lang="cobolfree">
INVOKE my-class "foo" RETURNING var
MOVE my-class::"foo" TO var *> Inline method invocation
</syntaxhighlight>
COBOL does not provide a way to hide methods. Class data can be hidden, however, by declaring it without a property (programming) clause, which leaves the user with no way to access it. Method overloading was added in COBOL 2014.
Environment division.
The environment division contains the configuration section and the input-output section. The configuration section is used to specify variable features such
as currency signs, locales and character sets. The input-output section contains file-related information.
Files.
COBOL supports three file formats, or ': sequential, indexed and relative. In sequential files, records are contiguous and must be traversed sequentially, similarly to a linked list. Indexed files have one or more indexes which allow records to be randomly accessed and which can be sorted on them. Each record must have a unique key, but other, ', record keys need not be unique. Implementations of indexed files vary between vendors, although common implementations, such as C‑ISAM and VSAM, are based on IBM's ISAM. Relative files, like indexed files, have a unique record key, but they do not have alternate keys. A relative record's key is its ordinal position; for example, the 10th record has a key of 10. This means that creating a record with a key of 5 may require the creation of (empty) preceding records. Relative files also allow for both sequential and random access.
A common non-standard extension is the "" organization, used to process text files. Records in a file are terminated by a newline and may be of varying length.
Data division.
The data division is split into six sections which declare different items: the file section, for file records; the working-storage section, for static variables; the local-storage section, for automatic variables; the linkage section, for parameters and the return value; the report section and the screen section, for text-based user interfaces.
Aggregated data.
Data items in COBOL are declared hierarchically through the use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level data items, with a level-number of 1, are called '. Items that have subordinate aggregate data are called '; those that do not are called "". Level-numbers used to describe standard data items are between 1 and 49.
<syntaxhighlight lang="cobol">
</syntaxhighlight>
In the above example, elementary item and group item are subordinate to the record , while elementary items , , and are part of the group item .
Subordinate items can be disambiguated with the (or ) keyword. For example, consider the example code above along with the following example:
<syntaxhighlight lang="cobol">
</syntaxhighlight>
The names , , and are ambiguous by themselves, since more than one data item is defined with those names. To specify a particular data item, for instance one of the items contained within the group, the programmer would use (or the equivalent ). (This syntax is similar to the "dot notation" supported by most contemporary languages.)
Other data levels.
A level-number of 66 is used to declare a re-grouping of previously defined items, irrespective of how those items are structured. This data level, also referred to by the associated , is rarely used and, circa 1988, was usually found in old programs. Its ability to ignore the hierarchical and logical structure data meant its use was not recommended and many installations forbade its use.
<syntaxhighlight lang="cobol">
</syntaxhighlight>
A 77 level-number indicates the item is stand-alone, and in such situations is equivalent to the level-number 01. For example, the following code declares two 77-level data items, and , which are non-group data items that are independent of (not subordinate to) any other data items:
<syntaxhighlight lang="cobol">
</syntaxhighlight>
An 88 level-number declares a "" (a so-called 88-level) which is true when its parent data item contains one of the values specified in it clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the data item. When the data item contains a value of , the condition-name is true, whereas when it contains a value of or , the condition-name is true. If the data item contains some other value, both of the condition-names are false.
<syntaxhighlight lang="cobol">
</syntaxhighlight>
Data types.
Standard COBOL provides the following data types:
Type safety is variable in COBOL. Numeric data is converted between different representations and sizes silently and alphanumeric data can be placed in any data item that can be stored as a string, including numeric and group data. In contrast, object references and pointers may only be assigned from items of the same type and their values may be restricted to a certain type.
PICTURE clause.
A (or ) clause is a string of characters, each of which represents a portion of the data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a indicates a decimal digit, and an indicates that the item is signed. Other picture characters (called ' and ' characters) specify how an item should be formatted. For example, a series of characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, is equivalent to . Picture specifications containing only digit () and sign () characters define purely ' data items, while picture specifications containing alphabetic () or alphanumeric () characters define ' data items. The presence of other formatting characters define ' or ' data items.
USAGE clause.
The clause declares the format data is stored in. Depending on the data type, it can either complement or be used instead of a clause. While it can be used to declare pointers and object references, it is mostly geared towards specifying numeric types. These numeric formats are:
Report writer.
The report writer is a declarative facility for creating reports. The programmer need only specify the report layout and the data required to produce it, freeing them from having to write code to handle things like page breaks, data formatting, and headings and footings.
Reports are associated with report files, which are files which may only be written to through report writer statements.
<syntaxhighlight lang="cobol">
</syntaxhighlight>
Each report is defined in the report section of the data division. A report is split into report groups which define the report's headings, footings and details. Reports work around hierarchical "". Control breaks occur when a key variable changes it value; for example, when creating a report detailing customers' orders, a control break could occur when the program reaches a different customer's orders. Here is an example report description for a report which gives a salesperson's sales and which warns of any invalid records:
<syntaxhighlight lang="cobol">
</syntaxhighlight>
The above report description describes the following layout:
Four statements control the report writer: , which prepares the report writer for printing; , which prints a report group; , which suppresses the printing of a report group; and , which terminates report processing. For the above sales report example, the procedure division might look like this:
<syntaxhighlight lang="cobol">
</syntaxhighlight>
Procedure division.
Procedures.
The sections and paragraphs in the procedure division (collectively called procedures) can be used as labels and as simple subroutines. Unlike in other divisions, paragraphs do not need to be in sections.
Execution goes down through the procedures of a program until it is terminated.
To use procedures as subroutines, the verb is used. This transfers control to the specified range of procedures and returns only upon reaching the end.
Unusual control flow can trigger "", which cause control in performed procedures to return at unexpected times to unexpected locations. Procedures can be reached in three ways: they can be called with , jumped to from a or through execution "falling through" the bottom of an above paragraph. Combinations of these invoke undefined behavior, creating mines. Specifically, mines occur when execution of a range of procedures would cause control flow to go past the last statement of a range of procedures already being performed.
For example, in the code in the adjacent image, a mine is tripped at the end of when the screen is invalid. When the screen is invalid, control jumps to the section, which, when done, performs . This recursion triggers undefined behavior as there are now two overlapping ranges of procedures being performed. The mine is then triggered upon reaching the end of and means control could return to one of two locations:
Statements.
COBOL 2014 has 47 statements (also called ""), which can be grouped into the following broad categories: control flow, I/O, data manipulation and the report writer. The report writer statements are covered in the report writer section.
Control flow.
COBOL's conditional statements are and . is a switch-like statement with the added capability of evaluating multiple values and conditions. This can be used to implement decision tables. For example, the following might be used to control a CNC lathe: 
<syntaxhighlight lang="cobolfree">
EVALUATE TRUE ALSO desired-speed ALSO current-speed
END-EVALUATE
</syntaxhighlight>
The statement is used to define loops which are executed until a condition is true (not , unlike other languages). It is also used to call procedures or ranges of procedures (see the procedures section for more details). and call subprograms and methods, respectively. The name of the subprogram/method is contained in a string which may be a literal or a data item. Parameters can be passed by reference, by content (where a copy is passed by reference) or by value (but only if a prototype is available).
The statement is a return statement and the statement stops the program. The statement has six different formats: it can be used as a return statement, a break statement, a continue statement, an end marker or to leave a procedure.
Exceptions are raised by a statement and caught with a handler, or "", defined in the portion of the procedure division. Declaratives are sections beginning with a statement which specify the errors to handle. Exceptions can be names or objects. is used in a declarative to jump to the statement after the one that raised the exception or to a procedure outside the . Unlike other languages, uncaught exceptions may not terminate the program and the program can proceed unaffected.
I/O.
File I/O is handled by the self-describing , , , and statements along with a further three: , which updates a record; , which selects subsequent records to access by finding a record with a certain key; and , which releases a lock on the last record accessed.
User interaction is done using and .
Data manipulation.
The following verbs manipulate data:
Files and tables are sorted using and the verb merges and sorts files. The verb provides records to sort and retrieves sorted records in order.
Scope termination.
Some statements, such as and , may themselves contain statements. Such statements may be terminated in two ways: by a period (""), which terminates "all" unterminated statements contained, or by a scope terminator, which terminates the nearest matching open statement.
<syntaxhighlight lang="cobolfree">
IF invalid-record
IF invalid-record
END-IF
</syntaxhighlight>
Nested statements terminated with a period are a common source of bugs. For example, examine the following code:
<syntaxhighlight lang="cobolfree">
IF x
</syntaxhighlight>
Here, the intent is to display codice_18 and codice_19 if condition codice_20 is true. However, codice_19 will be displayed whatever the value of codice_20 because the codice_1 statement is terminated by an erroneous period after .
Another bug is a result of the dangling else problem, when two codice_1 statements can associate with an codice_25.
<syntaxhighlight lang="cobolfree">
IF x
ELSE
</syntaxhighlight>
In the above fragment, the codice_25 associates with the    statement instead of the    statement, causing a bug. Prior to the introduction of explicit scope terminators, preventing it would require    to be placed after the inner codice_1.
Self-modifying code.
The original COBOL specification supported the infamous    statement, for which many compilers generated self-modifying code. codice_28 and codice_29 are procedure labels, and the single    statement in procedure codice_28 executed after such an statement means    instead. Many compilers still support it,
but it was deemed obsolete in the COBOL 1985 standard and deleted in 2002.
Hello, world.
A "Hello, world" program in COBOL:
<syntaxhighlight lang="cobol">
</syntaxhighlight>
HELLO, WORLD.
When the – now famous – "Hello, World!" program example in "The C Programming Language" was first published in 1978 a similar mainframe COBOL program sample would have been submitted through JCL, very likely using a punch card reader, and 80 column punch cards. The listing below, "with an empty DATA DIVISION", was tested using GNU/Linux and the System/370 Hercules emulator running MVS 3.8J. The JCL, written in July 2015, is derived from the Hercules tutorials and samples hosted by Jay Moseley. In keeping with COBOL programming of that era, HELLO, WORLD is displayed in all capital letters.
<syntaxhighlight lang="cobolfree">
//COBUCLG JOB (001),'COBOL BASE TEST', 00010000
// CLASS=A,MSGCLASS=A,MSGLEVEL=(1,1) 00020000
//BASETEST EXEC COBUCLG 00030000
//COB.SYSIN DD * 00040000
//LKED.SYSLIB DD DSNAME=SYS1.COBLIB,DISP=SHR 00190000
// DD DSNAME=SYS1.LINKLIB,DISP=SHR 00200000
//GO.SYSPRINT DD SYSOUT=A 00210000
// 00220000
</syntaxhighlight>
After submitting the JCL, the MVS console displayed:
<syntaxhighlight lang="text" highlight="10">
</syntaxhighlight>
"Line 10 of the console listing above is highlighted for effect, the highlighting is not part of the actual console output".
The associated compiler listing generated over four pages of technical detail and job run information, for the single line of output from the 14 lines of COBOL.
Criticism and defense.
Lack of structure.
In the 1970s, programmers began moving away from unstructured spaghetti code to the structured programming paradigm. In his letter to an editor in 1975 entitled "How do we tell truths that might hurt?" which was critical of several of COBOL's contemporaries, computer scientist and Turing Award recipient Edsger Dijkstra remarked that "The use of COBOL cripples the mind; its teaching should, therefore, be regarded as a criminal offense."
In his dissenting response to Dijkstra's article and the above "offensive statement," computer scientist Howard E. Tompkins defended structured COBOL: "COBOL programs with convoluted control flow indeed tend to 'cripple the mind'," but this was because "There are too many such business application programs written by programmers that have never had the benefit of structured COBOL taught well..."
One cause of spaghetti code was the statement. Attempts to remove s from COBOL code, however, resulted in convoluted programs and reduced code quality. s were largely replaced by the statement and procedures, which promoted modular programming and gave easy access to powerful looping facilities. However, could only be used with procedures so loop bodies were not located where they were used, making programs harder to understand.
COBOL programs were infamous for being monolithic and lacking modularization.
COBOL code could only be modularized through procedures, which were found to be inadequate for large systems. It was impossible to restrict access to data, meaning a procedure could access and modify data item. Furthermore, there was no way to pass parameters to a procedure, an omission Jean Sammet regarded as the committee's biggest mistake.
Another complication stemmed from the ability to a specified sequence of procedures. This meant that control could jump to and return from any procedure, creating convoluted control flow and permitting a programmer to break the "single entry, single exit" rule.
This situation improved as COBOL adopted more features. COBOL-74 added subprograms, giving programmers the ability to control the data each part of the program could access. COBOL-85 then added nested subprograms, allowing programmers to hide subprograms. Further control over data and code came in 2002 when object-oriented programming, user-defined functions and user-defined data types were included.
Compatibility issues.
COBOL was intended to a be a highly portable, "common" language. However, by 2001, around 300 dialects had been created.
COBOL-85 was not fully compatible with earlier versions, and its development was controversial. Joseph T. Brophy, the CIO of Travelers Insurance, spearheaded an effort to inform COBOL users of the heavy reprogramming costs of implementing the new standard. As a result, the ANSI COBOL Committee received more than 2,200 letters from the public, mostly negative, requiring the committee to make changes. On the other hand, conversion to COBOL-85 was thought to increase productivity in future years, thus justifying the conversion costs.
Verbose syntax.
COBOL syntax has often been criticized for its verbosity. Proponents say that this was intended to make the code self-documenting, easing program maintenance. COBOL was also intended to be easy for programmers to learn and use, while still being readable to non-technical staff such as managers.
The desire for readability led to the use of English-like syntax and structural elements, such as nouns, verbs, clauses, sentences, sections, and divisions. Yet by 1984, maintainers of COBOL programs were struggling to deal with "incomprehensible" code and the main changes in COBOL-85 were there to help ease maintenance.
Jean Sammet, a short-range committee member, noted that "little attempt was made to cater to the professional programmer, in fact people whose main interest is programming tend to be very unhappy with COBOL" which she attributed to COBOL's verbose syntax.
Isolation from the computer science community.
The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled. Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing. The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new Backus–Naur form because few committee members had heard of it. This resulted in "severe" criticism.
Later, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear (with Richard D. Irwin publishing a college textbook on COBOL in 1966). By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress. University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a "trade school" nature. Donald Nelson, chair of the CODASYL COBOL committee, said in 1984 that "academics ... hate COBOL" and that computer science graduates "had 'hate COBOL' drilled into them". A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.
In contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.
Concerns about the design process.
Doubts have been raised about the competence of the standards committee. Short-term committee member Howard Bromberg said that there was "little control" over the development process and that it was "plagued by discontinuity of personnel and ... a lack of talent." Jean Sammet and Jerome Garfunkel also noted that changes introduced in one revision of the standard would be reverted in the next, due as much to changes in who was in the standard committee as to objective evidence.
COBOL standards have repeatedly suffered from delays: COBOL-85 arrived five years later than hoped,
COBOL 2002 was five years late,
and COBOL 2014 was six years late.
To combat delays, the standard committee allowed the creation of optional addenda which would add features more quickly than by waiting for the next standard revision. However, some committee members raised concerns about incompatibilities between implementations and frequent modifications of the standard.
Influences on other languages.
COBOL's data structures influenced subsequent programming languages. Its record and file structure influenced PL/I and Pascal, and the codice_31 clause was a predecessor to Pascal's variant records. Explicit file structure definitions preceded the development of database management systems and aggregated data was a significant advance over Fortran's arrays.
COBOL's facility, although considered "primitive",
influenced the development of include directives.
The focus on portability and standardization meant programs written in COBOL could be portable and facilitated the spread of the language to a wide variety of hardware platforms and operating systems. Additionally, the well-defined division structure restricts the definition of external references to the Environment Division, which simplifies platform changes in particular.

</doc>
<doc id="6801" url="https://en.wikipedia.org/wiki?curid=6801" title="Crew">
Crew

A crew is a body or a class of people who work at a common activity, generally in a structured or hierarchical organization. A location in which a crew works is called a crewyard or a workyard. The word has nautical resonances: the tasks involved in operating a ship, particularly a sailing ship, providing numerous specialities within a ship's crew, often organised with a chain of command. Traditional nautical usage strongly distinguishes officers from crew, though the two groups combined form the ship's company. Members of a crew are often referred to by the title "Crewman".
"Crew" also refers to the sport of rowing, where teams row competitively in racing shells.
"Crew" is used colloquially to refer to a small, tight-knit group of friends or associates engaged in criminal activity. Also used in reference to the traditional "unit" of criminals under the supervision of a caporegime in the American Mafia. However, the term is not specific to (Mafia-affiliated) organized crime. "Crew" can also refer simply to a group of friends, unrelated to crime or violence.

</doc>
<doc id="6803" url="https://en.wikipedia.org/wiki?curid=6803" title="CCD">
CCD

CCD can stand for:

</doc>
