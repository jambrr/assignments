<doc id="6804" url="https://en.wikipedia.org/wiki?curid=6804" title="Charge-coupled device">
Charge-coupled device

A charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated, for example conversion into a digital value. This is achieved by "shifting" the signals between stages within the device one at a time. CCDs move charge between capacitive "bins" in the device, with the shift allowing for the transfer of charge between bins.
The CCD is a major piece of technology in digital imaging. In a CCD image sensor, pixels are represented by p-doped MOS capacitors. These capacitors are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges. Although CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data is required. In applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors (CMOS) are generally used; the large quality advantage CCDs enjoyed early on has narrowed over time.
History.
The charge-coupled device was invented in 1969 at AT&T Bell Labs by Willard Boyle and George E. Smith.
The lab was working on semiconductor bubble memory when Boyle and Smith conceived of the design of what they termed, in their notebook, "Charge 'Bubble' Devices".
The device could be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s. The first patent () on the application of CCDs to imaging was assigned to Michael Tompsett.
The initial paper describing the concept listed possible uses as a memory, a delay line, and an imaging device. The first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds.
The first working CCD made with integrated circuit technology was a simple 8-bit shift register. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device.
Development of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.
Several companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2-D 100 x 100 pixel device. Steven Sasson, an electrical engineer working for Kodak, invented the first digital still camera using a Fairchild CCD in 1975. The first KH-11 KENNAN reconnaissance satellite equipped with charge-coupled device array ( pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony also started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982; subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution.
In January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics, for their invention of the CCD concept.
Michael Tompsett was awarded the 2010 National Medal of Technology and Innovation for pioneering work and electronic technologies including the design and development of the first charge coupled device (CCD) imagers. He was also awarded the 2012 IEEE Edison Medal "For pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers".
Basics of operation.
In a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).
An image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.
Detailed physics of operation.
Charge generation.
Before the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly "p"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of a "n" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.
Then, when electron–hole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:
The last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 105 electrons per pixel.
Design and manufacturing.
The photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly "p" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:
This thin layer (= 0.2–0.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2–3 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.
Later in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.
Channel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or "charge carrying", regions.
Channel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).
The clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the p–n junction and will collect and move the charge packets beneath the gates—and within the channels—of the device.
CCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.
Another version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.
Architecture.
The CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.
In a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.
With a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.
The interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.
The choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.
CCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.
Most common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1 μm. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.
Cooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.
Use in astronomy.
Due to the high quantum efficiencies of CCDs (for a quantum efficiency of 100%, one count equals one photon), linearity of their outputs, ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.
Thermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the "dark frame" average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD.
The Hubble Space Telescope, in particular, has a highly developed series of steps (“data reduction pipeline”) to convert the raw CCD data to useful images.
CCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.
An interesting unusual astronomical application of CCDs, called "drift-scanning", uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to produce the largest uniform survey of the sky yet accomplished.
In addition to astronomy, CCDs are also used in astronomical analytical instrumentation such as spectrometers.
Color cameras.
Digital color cameras generally use a Bayer mask over the CCD. Each square of four pixels has one filtered red, one blue, and two green (the human eye is more sensitive to green than either red or blue). The result of this is that luminance information is collected at every pixel, but the color resolution is lower than the luminance resolution.
Better color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (and therefore higher light sensitivity for a given aperture size). This is because in a 3CCD device most of the light entering the aperture is captured by a sensor, while a Bayer mask absorbs a high proportion (about 2/3) of the light falling on each CCD pixel.
For still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).
Sensor sizes.
Sensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8″ or 2/3″ called the optical format. This measurement actually originates back in the 1950s and the time of Vidicon tubes.
Electron-multiplying CCD.
An electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by e2v Ltd., GB, L3CCD or Impactron CCD, a product offered by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small ("P" < 2%), but as the number of elements is large (N > 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. It is to be noted that the use of avalanche breakdown for amplification of photo charges had already been described in the in 1973 by George E. Smith/Bell Telephone Laboratories.
EMCCDs show a similar sensitivity to Intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the "exact" gain that has been applied to a pixel's charge is impossible to know. At high gains (> 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron — or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:
formula_2 if formula_3
where "P" is the probability of getting "n" output electrons given "m" input electrons and a total mean multiplication register gain of "g".
Because of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system — using either thermoelectric cooling or liquid nitrogen — to cool the chip down to temperatures in the range of . This cooling system unfortunately adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.
The low-light capabilities of EMCCDs primarily find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-Pérot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.
In terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.
Frame transfer CCD.
The frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.
The normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as "vertical smear" and cause a strong light source to create a vertical line above and below its exact location. In addition, the CCD cannot be used to collect light while it is being read out. Unfortunately, a faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.
A frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.
The disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.
Intensified charge-coupled device.
An intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.
An image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.
An image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called "gating" and therefore ICCDs are also called gateable CCD cameras.
Besides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.
ICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand, EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around 170 K. This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.
ICCDs are used in night vision devices and in a large variety of scientific applications.
Blooming.
When a CCD exposure is long enough, eventually the electrons that collect in the "bins" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.
Some anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.
James M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity.

</doc>
<doc id="6806" url="https://en.wikipedia.org/wiki?curid=6806" title="Computer memory">
Computer memory

In computing, memory refers to the computer hardware devices used to store information for immediate use in a computer; it is synonymous with the term "primary storage". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access program and data storage but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage, through a memory management technique called "virtual memory". An archaic synonym for memory is store.
The term "memory", meaning "primary storage" or "main memory", is often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory areal density than DRAM.
Most semiconductor memory is organized into memory cells or bistable flip-flops, each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multiple bits per cell (called MLC, Multiple Level Cell). The memory cells are grouped into words of fixed word length, for example 1, 2, 4, 8, 16, 32, 64 or 128 bit. Each word can be accessed by a binary address of "N" bit, making it possible to store 2 raised by "N" words in the memory. This implies that processor registers normally are not considered as memory, since they only store one word and do not include an addressing mechanism.
Typical secondary storage devices are hard disk drives and solid-state drives.
History.
In the early 1940s, memory technology mostly permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of octal-base radio vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits which were held in the vacuum tube accumulators.
The next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.
Two alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams would invent the Williams tube, which would be the first random access computer memory. The Williams tube would prove more capacious than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube would nevertheless prove to be frustratingly sensitive to environmental disturbances.
Efforts began in the late 1940s to find non-volatile memory. Jay Forrester, Jan A. Rajchman and An Wang developed magnetic core memory, which allowed for recall of memory after power loss. Magnetic core memory would become the dominant form of memory until the development of transistor-based memory in the late 1960s.
Developments in technology and economies of scale have made possible so-called Very Large Memory (VLM) computers.
The term "memory" when used with reference to computers generally refers to Random Access Memory or RAM.
Volatile memory.
Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). SRAM retains its contents as long as the power is connected and is easy for interfacing, but uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.
SRAM is not worthwhile for desktop system memory, where DRAM dominates, but is used for their cache memories. SRAM is commonplace in small embedded systems, which might only need tens of kilobytes or less. Forthcoming volatile memory technologies that aim at replacing or competing with SRAM and DRAM include Z-RAM and A-RAM.
Non-volatile memory.
Non-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
Forthcoming non-volatile memory technologies include FeRAM, CBRAM, PRAM, SONOS, RRAM, racetrack memory, NRAM, 3D XPoint, and millipede memory.
Management of memory.
Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.
Nearly everything a computer programmer does requires him or her to consider how to manage memory. Even storing a number in memory requires the programmer to specify how the memory should store it.
Memory management bugs.
Improper management of memory is a common cause of bugs, including the following types:
Early computer systems.
In early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.
This approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.
Virtual memory.
Virtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides what physical location to place the memory in.
This offers several advantages. Computer programmers no longer need to worry about where the memory is physically stored or whether the user's computer will have enough memory. It also allows multiple types of memory to be used. For example, some memory can be stored in physical RAM chips while other memory is stored on a hard drive. This drastically increases the amount of memory available to programs. The operating system will place actively used memory in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving memory from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.
Virtual memory systems usually include protected memory, but this is not always the case.
Protected memory.
Protected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.
Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers.
Protected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated. This way, only the offending program crashes, and other programs are not affected by the error.
Protected memory systems almost always include virtual memory as well.

</doc>
<doc id="6809" url="https://en.wikipedia.org/wiki?curid=6809" title="CDC">
CDC

CDC may refer to:

</doc>
<doc id="6811" url="https://en.wikipedia.org/wiki?curid=6811" title="Centers for Disease Control and Prevention">
Centers for Disease Control and Prevention

The Centers for Disease Control and Prevention (CDC) is the leading national public health institute of the United States. The CDC is a federal agency under the Department of Health and Human Services and is headquartered in unincorporated DeKalb County, Georgia, a few miles northeast of the Atlanta city limits.
Its main goal is to protect public health and safety through the control and prevention of disease, injury, and disability. The CDC focuses national attention on developing and applying disease control and prevention. It especially focuses its attention on infectious disease, food borne pathogens, environmental health, occupational safety and health, health promotion, injury prevention and educational activities designed to improve the health of United States citizens. In addition, the CDC researches and provides information on non-infectious diseases such as obesity and diabetes and is a founding member of the International Association of National Public Health Institutes.
History.
The Communicable Disease Center was founded July 1, 1946, as the successor to the World War II Malaria Control in War Areas program of the Office of National Defense Malaria Control Activities. Preceding its founding, organizations with global influence in malaria control were the Malaria Commission of the League of Nations and the Rockefeller Foundation. The Rockefeller Foundation greatly supported malaria control, sought to have the governments take over some of its efforts, and collaborated with the agency.
The new agency was a branch of the U.S. Public Health Service and Atlanta was chosen as the location because malaria was endemic in the Southern United States. The agency changed names (see infobox on top) before adopting the name "Communicable Disease Center" in 1946. Offices were located on the sixth floor of the Volunteer Building on Peachtree Street. With a budget at the time of about $1 million, 59 percent of its personnel were engaged in mosquito abatement and habitat control with the objective of control and eradication of malaria in the United States (see National Malaria Eradication Program).
Among its 369 employees, the main jobs at CDC were originally entomology and engineering. In CDC's initial years, more than six and a half million homes were sprayed, mostly with DDT. In 1946, there were only seven medical officers on duty and an early organization chart was drawn, somewhat fancifully, in the shape of a mosquito. Under Joseph Walter Mountin, the CDC continued to advocate for public health issues and pushed to extend its responsibilities to many other communicable diseases. In 1947, the CDC made a token payment of $10 to Emory University for of land on Clifton Road in DeKalb County, still the home of CDC headquarters today. CDC employees collected the money to make the purchase. The benefactor behind the “gift” was Robert W. Woodruff, chairman of the board of The Coca-Cola Company. Woodruff had a long-time interest in malaria control, which had been a problem in areas where he went hunting. The same year, the PHS transferred its San Francisco based plague laboratory into the CDC as the Epidemiology Division, and a new Veterinary Diseases Division was established. An Epidemic Intelligence Service (EIS) was established in 1951, originally due to biological warfare concerns arising from the Korean War; it evolved into two-year postgraduate training program in epidemiology, and a prototype for Field Epidemiology Training Programs (FETP), now found in numerous countries, reflecting CDC's influence in promoting this model internationally.
The mission of CDC expanded beyond its original focus on malaria to include sexually transmitted diseases when the Venereal Disease Division of the U.S. Public Health Service (PHS) was transferred to the CDC in 1957. Shortly thereafter, Tuberculosis Control was transferred (in 1960) to the CDC from PHS, and then in 1963 the Immunization program was established.
It became the "National Communicable Disease Center (NCDC)" effective July 1, 1967. The organization was renamed the "Center for Disease Control (CDC)" on June 24, 1970, and "Centers for Disease Control" effective October 14, 1980. An act of the United States Congress appended the words "and Prevention" to the name effective October 27, 1992. However, Congress directed that the initialism "CDC" be retained because of its name recognition.
Currently the CDC focus has broadened to include chronic diseases, disabilities, injury control, workplace hazards, environmental health threats, and terrorism preparedness. CDC combats emerging diseases and other health risks, including birth defects, West Nile virus, obesity, avian, swine, and pandemic flu, E. coli, and bioterrorism, to name a few. The organization would also prove to be an important factor in preventing the abuse of penicillin.
In May 1994 the CDC admitted having sent several biological warfare agents to the Iraqi government from 1984 through 1989, including Botulinum toxin, West Nile virus, Yersinia pestis and Dengue fever virus.
On April 21, 2005, then-CDC Director Julie Gerberding, formally announced the reorganization of CDC to "confront the challenges of 21st-century health threats". The four Coordinating Centers — established under the G. W. Bush Administration and Gerberding — "diminished the influence of national centers under umbrella", and were ordered cut under the Obama Administration in 2009.
The CDC's Biosafety Level 4 laboratories are among only about a dozen such facilities in the country, as well as one of only two official repositories of smallpox in the world. The second smallpox store resides at the State Research Center of Virology and Biotechnology VECTOR in the Russian Federation. The CDC revealed in 2014 that it had discovered several misplaced smallpox samples and also that lab workers had potentially been infected with anthrax.
Organization.
The CDC is organized into "Centers, Institutes, and Offices" (CIOs) which allow it to be responsive and effective in its interface with public health concerns. Each organizational unit implements the agency's response in a particular area of expertise. Within "Offices" are Centers, Divisions, and Branches.
CIOs are
Budget and workforce.
CDC’s FY2014 budget is $6.9 billion.
As of 2008, staff numbered approximately 15,000 (including 6,000 contractors and 840 Commissioned Corps officers) in 170 occupations. Eighty percent have earned bachelor's degrees or higher; almost half have advanced degrees (a master's degree or a doctorate such as a PhD, D.O., or M.D.). CDC job titles include engineer, entomologist, epidemiologist, biologist, physician, veterinarian, behaviorial scientist, nurse, medical technologist, economist, public health advisor, health communicator, toxicologist, chemist, computer scientist, and statistician.
In addition to its Atlanta headquarters, the CDC has other locations in the United States and Puerto Rico. Those locations include Anchorage; Cleveland; Cincinnati; Fort Collins; Hyattsville; Morgantown; Pittsburgh; Research Triangle Park; San Juan, Puerto Rico; Spokane, Washington; Detroit; and Washington, D.C. The CDC also conducts the Behavioral Risk Factor Surveillance System, the world’s largest, on-going telephone health survey system.
The CDC offers grants that help many organizations each year bring health, safety and awareness to surrounding communities throughout the entire United States. As a government-run department, the Centers for Disease Control and Prevention awards over 85 percent of its annual budget through these grants to accomplish its ultimate goal of disease control and quality health for all.
The CDC operates the Public Health Associate Program (PHAP), a two-year paid fellowship for recent college graduates to work in public health agencies all over the United States. PHAP was founded in 2007 and currently has 159 associates in 34 states.
Directors.
The President of the United States appoints the director of the CDC and the appointment does not require Senate confirmation. The director serves at the pleasure of the President and may be fired at any time. Sixteen directors have served the CDC or its predecessor agencies.
Foundation.
The CDC Foundation operates independently from CDC as a private, nonprofit 501(c)(3) organization incorporated in the State of Georgia. The creation of the Foundation was authorized by section 399F of the Public Health Service Act to support the mission of CDC in partnership with the private sector, including organizations, foundations, businesses, educational groups, and individuals.
Diseases.
Influenza.
The CDC has launched campaigns targeting the transmission of influenza, including the H1N1 swine flu. The CDC has launched websites including to educate people in proper hygiene.
Other infectious diseases.
The CDC's website (see below) has information on other infectious diseases, including smallpox, measles, and others. The CDC runs a program that protects the public from rare and dangerous substances such as anthrax and the Ebola virus. The program, called the Select Agents Program, calls for inspections of labs in the U.S. that work with dangerous pathogens.
During the 2014 Ebola outbreak in West Africa, the CDC helped coordinate the return of two infected American aid workers for treatment at Emory University Hospital, the home of a special unit to handle highly infectious diseases.
As a response to the 2014 Ebola outbreak, the U.S. House of Representatives proposed and passed a Continuing Appropriations Resolution to allocate up to $30,000,000 towards CDCP's efforts to fight the virus.
Non-infectious diseases.
The CDC also works on non-infectious diseases, including obesity.
Popular culture and controversies.
Historically, the CDC has been relatively free of political manipulation.
Tuskegee Study of Untreated Syphilis in the Negro Male.
For 15 years, the CDC had direct oversight over the "Tuskegee Study of Untreated Syphilis in the Negro Male". In the study, which lasted from 1932 to 1972, a group of African American men (nearly 400 of whom had syphilis) were studied to learn more about the disease. Notably, the disease was left untreated in the research subjects and they never gave their informed consent to serve as research subjects. The Tuskegee Study was initiated in 1932 by the Public Health Service. The CDC took over the study in 1957.
The CDC's response to the AIDS crisis in the 1980s has been criticized for promoting some public health policies that harmed HIV+ people and for providing ineffective public education.
The agency's response to the 2001 anthrax attacks was criticized for ineffective communication with other public health agencies and with the public.
CDC zombie apocalypse outreach campaign.
On May 16, 2011, the Centers for Disease Control and Prevention's blog published an article instructing the public on what to do to prepare for a zombie invasion. While the article did not claim that such a scenario was possible, it did use the popular culture appeal as a means of urging citizens to prepare for all potential hazards, such as earthquakes, tornadoes, and floods.
According to David Daigle, the Associate Director for Communications, Public Health Preparedness and Response, the idea arose when his team was discussing their upcoming hurricane information campaign and Daigle mused that "we say pretty much the same things every year, in the same way, and I just wonder how many people are paying attention." A social media employee mentioned that the subject of zombies had come up a lot on Twitter when she had been tweeting about the Fukushima Daiichi nuclear disaster and radiation. The team realized that a campaign like this would most likely reach a different audience from the one that normally pays attention to hurricane preparedness warnings and went to work on the zombie campaign, launching it right before hurricane season began. "The whole idea was, if you're prepared for a zombie apocalypse, you're prepared for pretty much anything," said Daigle.
Once the blog article became popular, the CDC announced an open contest for YouTube submissions of the most creative and effective videos covering preparedness for a zombie apocalypse (or apocalypse of any kind), to be judged by the "CDC Zombie Task Force". Submissions were open until October 11, 2011. They also released a zombie themed graphic novella available on their website. Zombie-themed educational materials for teachers are available on the site.
Gun violence.
One area of current partisan dispute related to CDC funding is studying gun violence. In the immediate aftermath of Charleston, South Carolina massacre in 2015, the US House of Representatives Appropriations Committee rejected an amendment, supported by Democrats but opposed by most Republicans, which would have allowed the Centers for Disease Control and Prevention to study the underlying causes of gun violence.
See also.
For similar agencies elsewhere, see the list of national public health agencies.

</doc>
<doc id="6813" url="https://en.wikipedia.org/wiki?curid=6813" title="Chandrasekhar limit">
Chandrasekhar limit

The Chandrasekhar limit () is the maximum mass of a stable white dwarf star. The limit was first indicated in papers published by Wilhelm Anderson and E. C. Stoner, and was named after Subrahmanyan Chandrasekhar, the Indian astrophysicist who independently discovered and improved upon the accuracy of the calculation in 1930, at the age of 19, in India. This limit was initially ignored by the community of scientists because such a limit would logically require the existence of black holes, which were considered a scientific impossibility at the time. White dwarfs resist gravitational collapse primarily through electron degeneracy pressure. (By comparison, main sequence stars resist collapse through thermal pressure.) The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction. Consequently, white dwarfs with masses greater than the limit would be subject to further gravitational collapse, evolving into a different type of stellar remnant, such as a neutron star or black hole. (However, white dwarfs generally avoid this fate by exploding before they undergo collapse.) Those with masses under the limit remain stable as white dwarfs.
The currently accepted value of the limit is about 1.39 formula_1 (2.765 × 1030 kg).
Physics.
Electron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons will increase upon compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure.
In the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form formula_2, where "P" is the pressure, formula_3 is the mass density, and formula_4 is a constant. Solving the hydrostatic equation then leads to a model white dwarf which is a polytrope of index 3/2 and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass.
As the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses. The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form formula_5. This will yield a polytrope of index 3, which will have a total mass, "M"limit say, depending only on K2.
For a fully relativistic treatment, the equation of state used will interpolate between the equations formula_2 for small ρ and formula_5 for large ρ.
When this is done, the model radius still decreases with mass, but becomes zero at "M"limit. This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. μe has been set equal to 2.
Radius is measured in standard solar radii or kilometers, and mass in standard solar masses.
Calculated values for the limit will vary depending on the nuclear composition of the mass. Chandrasekhar, eq. (36), eq. (58), eq. (43) gives the following expression, based on the equation of state for an ideal Fermi gas:
where:
As formula_11 is the Planck mass, the limit is of the order of
A more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle Schrödinger equation.
History.
In 1926, the British physicist Ralph H. Fowler observed that the relationship among the density, energy and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei which obeyed Fermi–Dirac statistics. This Fermi gas model was then used by the British physicist E. C. Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming them to be homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately 1.37 kg. In 1930, Stoner derived the internal energy–density equation of state for a Fermi gas, and was then able to treat the mass–radius relationship in a fully relativistic manner, giving a limiting mass of approximately (for μe=2.5) 2.19 · 1030 kg. Stoner went on to derive the pressure–density equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community.
A series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930,
where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved
the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture. This value was also computed in 1932 by the Soviet physicist Lev Davidovich Landau, who, however, did not apply it to white dwarfs.
Chandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Stanley Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied:
Eddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law P=K1ρ5/3 universally applicable, even for large ρ. Although Bohr, Fowler, Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar., pp. 110–111 Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of "Empire of the Stars", Arthur I. Miller's biography of Chandrasekhar. In Miller's view:
Applications.
The core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process will be exhausted, and the core will collapse, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse.
If a main-sequence star is not too massive (less than approximately 8 solar masses), it will eventually shed enough mass to form a white dwarf having mass below the Chandrasekhar limit, which will consist of the former core of the star. For more-massive stars, electron degeneracy pressure will not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities will destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos., pp. 1046–1047. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy which is on the order of 1046 joules (100 foes). Most of this energy is carried away by the emitted neutrinos. This process is believed to be responsible for supernovae of types Ib, Ic, and II.
Type Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbon–oxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass. As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation which disrupts the star and causes the supernova., §5.1.2
A strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, MV is approximately -19.3, with a standard deviation of no more than 0.3., (1) A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy.
Super-Chandrasekhar mass supernovae.
In April 2003, the Supernova Legacy Survey observed a type Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4 billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf which grew to twice the mass of the Sun before exploding. They believe that the star, dubbed the "Champagne Supernova" by University of Oklahoma astronomer David R. Branch, may have been spinning so fast that centrifugal force allowed it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type Ia supernovae as standard candles.
Since the observation of the Champagne Supernova in 2003, more very bright type Ia supernovae have been observed that are thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4–2.8 solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely.
Tolman–Oppenheimer–Volkoff limit.
After a supernova explosion, a neutron star may be left behind. Like white dwarfs these objects are extremely compact and are supported by degeneracy pressure, but a neutron star is so massive and compressed that electrons and protons have combined to form neutrons, and the star is thus supported by neutron degeneracy pressure instead of electron degeneracy pressure. The limit of neutron degeneracy pressure, analogous to the Chandrasekhar limit, is known as the Tolman–Oppenheimer–Volkoff limit.

</doc>
<doc id="6814" url="https://en.wikipedia.org/wiki?curid=6814" title="Congregationalist polity">
Congregationalist polity

Congregationalist polity, often known as congregationalism, is a system of church governance ("ecclesiastical polity") in which every local church congregation is independent, ecclesiastically sovereign, or "autonomous". Its first articulation in writing is the Cambridge Platform of 1648 in New England. Among those major Protestant Christian traditions that employ congregationalism are those Congregational Churches known by the "Congregationalist" name that descended from the Independent wing of the Anglo-American Puritan movement of the 17th century, Quakerism, the Baptist churches, and most of the groups brought about by the Anabaptist movement in Germany that migrated to the U.S. in the late 18th century. More recent generations have witnessed also a growing number of non-denominational churches, which are most often congregationalist in their governance. In Christianity, congregationalism is distinguished most clearly from episcopal polity, which is governance by a hierarchy of bishops. But it is also distinct from presbyterian polity, in which higher assemblies of congregational representatives can exercise considerable authority over individual congregations.
Congregationalism is not limited only to organization of Christian congregations; the principles of congregationalism have been inherited by the Unitarian Universalist Association and the Canadian Unitarian Council. Jewish synagogues, many Sikh Gurdwaras and most Islamic mosques in the U.S. operate under congregational government, with no hierarchies.
Basic form.
The term "congregationalist polity" describes a form of church governance that is based on the local congregation. Each local congregation is independent and self-supporting, governed by its own members. Some band into loose voluntary associations with other congregations that share similar beliefs (e.g., the Willow Creek Association and the American Unitarian Association). Others join "conventions", such as the Southern Baptist Convention, the National Baptist Convention or the American Baptist Churches USA (formerly the Northern Baptist Convention). In Quaker Congregationalism, monthly meetings, which are the most basic unit of administration, may be organized into larger Quarterly meetings or Yearly Meetings. Monthly, quarterly, or yearly meetings may also be associated with large "umbrella" associations such as Friends General Conference or Friends United Meeting. These conventions generally provide stronger ties between congregations, including some doctrinal direction and pooling of financial resources. Congregations that belong to associations and conventions are still independently governed. Most non-denominational churches are organized along congregationalist lines. Many do not see these voluntary associations as "denominations", because they "believe that there is no church other than the local church, and denominations are in variance to Scripture."
Congregational church.
The earmarks of Congregationalism can be traced back to the Pilgrim societies of the United States in the early 17th century. Congregationalism expressed the viewpoint that (1) every local church is a full realization in miniature of the entire Church of Jesus Christ; and (2) the Church, while on earth, besides the local church, can only be invisible and ideal. While other theories may insist on the truth of the former, the latter precept of congregationalism gives the entire theory a unique character among plans of church government. There is no other reference than the local congregation for the "visible church" in Congregationalism. And yet, the connection of all Christians is also asserted, albeit in a way that defenders of this view usually decline, often intentionally, to elaborate more clearly or consistently. This first, foundational principle by which congregationalism is guided results in confining it to operate with the consent of each gathering of believers.
Although "congregational rule" may seem to suggest that pure democracy reigns in congregational churches, this is seldom the case. It is granted, with few exceptions (namely in some Anabaptist churches), that God has given the government of the Church into the hands of an ordained ministry. What makes congregationalism unique is its system of checks and balances, which constrains the authority of the clergy, the lay officers, and the members.
Most importantly, the boundaries of the powers of the ministers and church officers are set by clear and constant reminders of the freedoms guaranteed by the Gospel to the laity, collectively and individually. With that freedom comes the responsibility upon each member to govern himself or herself under Christ. This requires lay people to exercise great charity and patience in debating issues with one another and to seek the glory and service of God as the foremost consideration in all of their decisions.
The authority of all of the people, including the officers, is limited in the local congregation by a definition of union, or a covenant, by which the terms of their cooperation together are spelled out and agreed to. This might be something as minimal as a charter specifying a handful of doctrines and behavioral expectations, or even a statement only guaranteeing specific freedoms. Or, it may be a constitution describing a comprehensive doctrinal system and specifying terms under which the local church is connected to other local churches, to which participating congregations give their assent. In congregationalism, rather uniquely, the church is understood to be a truly voluntary association.
Finally, the congregational theory strictly forbids ministers from ruling their local churches by themselves. Not only does the minister serve by the approval of the congregation, but committees further constrain the pastor from exercising power without consent by either the particular committee, or the entire congregation. It is a contradiction of the congregational principle if a minister makes decisions concerning the congregation without the vote of these other officers.
The other officers may be called "deacons", "elders" or "session" (borrowing Presbyterian terminology), or even "vestry" (borrowing the Anglican term) — it is not their label that is important to the theory, but rather their lay status and their equal vote, together with the pastor, in deciding the issues of the church. While other forms of church government are more likely to define "tyranny" as "the imposition of unjust rule", a congregationally governed church would more likely define tyranny as "transgression of liberty" or equivalently, "rule by one man". To a congregationalist, no abuse of authority is worse than the concentration of all decisive power in the hands of one ruling body, or one person.
Following this sentiment, congregationalism has evolved over time to include even more participation of the congregation, more kinds of lay committees to whom various tasks are apportioned, and more decisions subject to the vote of the entire membership.
One of the most notable characteristics of New England (or British)-heritage Congregationalism has been its consistent leadership role in the formation of "unions" with other churches. Such sentiments especially grew strong in the late 19th and early 20th centuries, when ecumenism evolved out of a liberal, non-sectarian perspective on relations to other Christian groups that accompanied the relaxation of Calvinist stringencies held by earlier generations. The congregationalist theory of independence within a union has been a cornerstone of most ecumenical movements since the 18th century.
Baptist churches.
Most Baptists hold that no church or ecclesiastical organization has inherent authority over a Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control.
Exceptions to this local form of local governance include a few churches that submit to the leadership of a body of elders, as well as the Episcopal Baptists that have an Episcopal system.
Independent Baptist churches have no formal organizational structure above the level of the local congregation. More generally among Baptists, a variety of parachurch agencies and evangelical educational institutions may be supported generously or not at all, depending entirely upon the local congregation's customs and predilections. Usually doctrinal conformity is held as a first consideration when a church makes a decision to grant or decline financial contributions to such agencies, which are legally external and separate from the congregations they serve. These practices also find currency among non-denominational fundamentalist or charismatic fellowships, many of which derive from Baptist origins, culturally if not theologically.
Most Southern Baptist and National Baptist congregations, by contrast, generally relate more closely to external groups such as mission agencies and educational institutions than do those of independent persuasion. However, they adhere to a very similar ecclesiology, refusing to permit outside control or oversight of local affairs.
Churches of Christ.
Church government is congregational rather than denominational. Churches of Christ purposefully have no central headquarters, councils, or other organizational structure above the local church level. Rather, the independent congregations are a network with each congregation participating at its own discretion in various means of service and fellowship with other congregations (see Sponsoring church (Churches of Christ)). Churches of Christ are linked by their shared commitment to restoration principles.
Congregations are generally overseen by a plurality of elders (also known in some congregations as shepherds, bishops, or pastors) who are sometimes assisted in the administration of various works by deacons. Elders are generally seen as responsible for the spiritual welfare of the congregation, while deacons are seen as responsible for the non-spiritual needs of the church. Deacons serve under the supervision of the elders, and are often assigned to direct specific ministries. Successful service as a deacon is often seen as preparation for the eldership. Elders and deacons are chosen by the congregation based on the qualifications found in and . Congregations look for elders who have a mature enough understanding of scripture to enable them to supervise the minister and to teach, as well as to perform "governance" functions. In lieu of willing men who meet these qualifications, congregations are sometimes overseen by the congregation's men in general.
While the early Restoration Movement had a tradition of itinerant preachers rather than "located Preachers", during the 20th century a long-term, formally trained congregational minister became the norm among Churches of Christ. Ministers are understood to serve under the oversight of the elders. While the presence of a long-term professional minister has sometimes created "significant "de facto" ministerial authority" and led to conflict between the minister and the elders, the eldership has remained the "ultimate locus of authority in the congregation".
Churches of Christ hold to the priesthood of all believers. No special titles are used for preachers or ministers that would identify them as "clergy". Churches of Christ emphasize that there is no distinction between "clergy" and "laity" and that every member has a gift and a role to play in accomplishing the work of the church.

</doc>
<doc id="6816" url="https://en.wikipedia.org/wiki?curid=6816" title="Cavalry">
Cavalry

Cavalry (from French "cavalerie", cf. "cheval" 'horse') or horsemen were soldiers or warriors who fought mounted on horseback. Cavalry were historically the most mobile of the combat arms. An individual soldier in the cavalry is known by a number of designations such as cavalryman, horseman, dragoon, or trooper.
The designation of cavalry was not usually given to any military force that used other animals, such as camels, mules or elephants. Infantry who moved on horseback, but dismounted to fight on foot, were known in the 17th and early 18th centuries as dragoons, a class of mounted infantry which later evolved into cavalry proper while retaining their historic title.
From earliest times cavalry had the advantage of improved mobility, and a man fighting from horseback also had the advantages of greater height, speed, and inertial mass over an opponent on foot. Another element of horse mounted warfare is the psychological impact a mounted soldier can inflict on an opponent.
The speed, mobility and shock value of the cavalry was greatly appreciated and exploited in armed forces in the Ancient and Middle Ages; some forces were mostly cavalry, particularly in nomadic societies of Asia, notably the Mongol armies. In Europe cavalry became increasingly armoured (heavy), and eventually became known for the mounted knights. During the 17th century cavalry in Europe lost most of its armor, ineffective against the muskets and cannon which were coming into use, and by the mid-19th century armor had mainly fallen into disuse, although some regiments retained a small thickened cuirass that offered protection against lances and sabres and some protection against shot.
In the period between the World Wars, many cavalry units were converted into motorized infantry and mechanized infantry units, or reformed as tank troops. However, some cavalry still served during World War II, notably in the Red Army, the Mongolian People's Army, the Royal Italian Army, the Romanian Army and the Polish Land Forces. Most cavalry units that are horse-mounted in modern armies serve in purely ceremonial roles, or as mounted infantry in difficult terrain such as mountains or heavily forested areas. Modern usage of the term generally refers to specialist units equipped with tanks ("armored cavalry") or aircraft ("air cavalry").
Role of cavalry.
In many modern armies, the term "cavalry" is still often used to refer to units that are a combat arm of the armed forces which in the past filled the traditional horse-borne land combat light cavalry roles. These include scouting, skirmishing with enemy reconnaissance elements to deny them knowledge of own disposition of troops, forward security, offensive reconnaissance by combat, defensive screening of friendly forces during retrograde movement, retreat, restoration of command and control, deception, battle handover and passage of lines, relief in place, linkup, breakout operations, and raiding. The shock role, traditionally filled by heavy cavalry, is generally filled by units with the "armored" designation.
History.
Origins.
Before the Iron Age, the role of cavalry on the battlefield was largely performed by light chariots. The chariot originated with the Sintashta-Petrovka culture in Central Asia and spread by nomadic or semi-nomadic Indo-Iranians. The chariot was quickly adopted by settled peoples both as a military technology and an object of ceremonial status, especially by the pharaohs of the New Kingdom of Egypt as well as the Assyrian army and Babylonian royalty.
The power of mobility given by mounted units was recognized early on, but was offset by the difficulty of raising large forces and by the inability of horses (then mostly small) to carry heavy armor. Cavalry techniques were an innovation of equestrian nomads of the Central Asian and Iranian steppe and pastoralist tribes such as the Persian Parthians and Sarmatians.
The photograph above left shows Assyrian cavalry from reliefs of 865–860 BC. At this time, the men had no spurs, saddles, saddle cloths, or stirrups. Fighting from the back of a horse was much more difficult than mere riding. The cavalry acted in pairs; the reins of the mounted archer were controlled by his neighbour's hand. Even at this early time, cavalry used swords, shields, and bows. The sculpture implies two types of cavalry, but this might be a simplification by the artist. Later images of Assyrian cavalry show saddle cloths as primitive saddles, allowing each archer to control his own horse.
As early as 490 BC a breed of large horses was bred in the Nisaean plain in Media to carry men with increasing amounts of armour (Herodotus 7,40 & 9,20). But large horses were still very exceptional at this time. Excepting a few ineffective attempts to revive scythed chariots, and continuing far eastern use, the use of chariots in battle was obsolete in civilized nations by the time of the Persian defeat at the hands of Alexander the Great, but chariots remained in use for ceremonial purposes such as carrying the victorious general in a Roman triumph, or for racing. The southern Britons met Julius Caesar with chariots in 55 and 54 BC, but by the time of the Roman conquest of Britain a century later chariots were mostly obsolete, even in Britannia. However, the last mention of chariot use in battle was at Mons Graupius, in 84 AD. Chariots remained in use in ancient China throughout the Warring States period.
Ancient Greece: city-states, Thebes, Thessaly and Macedonia.
During the classical Greek period cavalry were usually limited to those citizens who could afford expensive war-horses. Three types of cavalry became common: light cavalry, whose riders, armed with javelins, could harass and skirmish; heavy cavalry, whose troopers, using lances, had the ability to close with their opponents; and finally those whose equipment allowed them to fight either on horseback or foot. The role of horsemen did however remain secondary to that of the hoplites or heavy infantry who comprised the main strength of the citizen levies of the various city states.
Cavalry played a relatively minor role in ancient Greek city-states, with conflicts decided by massed armored infantry. However, Thebes produced Pelopidas, her first great cavalry commander, whose tactics and skills were absorbed by Phillip II of Macedon when Phillip was a guest-hostage in Thebes. Thessaly was widely known for producing competent cavalrymen, and later experiences in wars both with and against the Persians taught the Greeks the value of cavalry in skirmishing and pursuit. The Athenian author and soldier Xenophon in particular advocated the creation of a small but well-trained cavalry force; to that end, he wrote several manuals on horsemanship and cavalry operations.
The Macedonian Kingdom in the north, on the other hand, developed a strong cavalry force that culminated in the "hetairoi" (Companion cavalry) of Philip II of Macedon and Alexander the Great. In addition to these heavy cavalry, the Macedonian army also employed lighter horsemen called prodromoi for scouting and screening, as well as the Macedonian pike phalanx and various kinds of light infantry. There were also the "Ippiko" (or "Horserider"), Greek "heavy" cavalry, armed with kontos (or cavalry lance), and sword. These wore leather armour or mail plus a helmet. They were medium rather than heavy cavalry, meaning that they were better suited to be scouts, skirmishers, and pursuers rather than front line fighters. This combination of cavalry and infantry helped to break enemy lines and were used effectively to dominate the opponents of the kingdom.
The effectiveness of this combined-arms system was most dramatically demonstrated in Alexander's conquest of Persia, Bactria, and northwestern India.
Roman Republic and Early Empire.
The cavalry in the early Roman Republic remained the preserve of the wealthy landed class known as the "equites"—men who could afford the expense of maintaining a horse in addition to arms and armor heavier than those of the common legions. As the class grew to be more of a social elite instead of a functional property-based military grouping, the Romans began to employ Italian socii for filling the ranks of their cavalry.The weakness of Roman cavalry was demonstrated by Hannibal Barca during the second Punic war where he used his superior mounted forces to win several battles. The most notable of these was the Battle of Cannae, when he inflicted a catastrophic defeat on the Romans. At about the same time the Romans began to recruit foreign auxiliary cavalry from among Gauls, Iberians, and Numidians, the last being highly valued as mounted skirmishers and scouts (see Numidian cavalry). Julius Caesar had a high opinion of his escort of Germanic mixed cavalry, giving rise to the "Cohortes Equitatae". Early emperors maintained an ala of Batavian cavalry as their personal bodyguards until the unit was dismissed by Galba after the Batavian Rebellion.
For the most part, Roman cavalry during the Republic functioned as an adjunct to the legionary infantry and formed only one-fifth of the showing force. This does not mean that its utility should be underestimated, as its strategic role in scouting, skirmishing, and outpost duties was crucial to the Romans' capability to conduct operations over long distances in hostile or unfamiliar territory. On some occasions Roman cavalry also proved its ability to strike a decisive tactical blow against a weakened or unprepared enemy, such as the final charge at the Battle of Aquilonia.
After defeats such as the Battle of Carrhae, the Romans learned the importance of large cavalry formations from the Parthians. They would begin to substantially increase both the numbers and the training standards of the cavalry in their employ, just as nearly a thousand years earlier the first Iranians to reach the Iranian Plateau forced the Assyrians to undertake a similar reform. Nonetheless, the Romans would continue to rely mainly on their heavy infantry supported by auxiliary cavalry.
Late Roman Empire and the Migration Period.
In the army of the late Roman Empire, cavalry played an increasingly important role. The Spatha, the classical sword throughout most of the 1st millennium was adopted as the standard model for the Empire's cavalry forces.
The most widespread employment of heavy cavalry at this time was found in the forces of the Parthians and their Iranian Sassanid successors. Both, but especially the former, were famed for the cataphract (fully armored cavalry armed with lances) even though the majority of their forces consisted of lighter horse archers. The West first encountered this eastern heavy cavalry during the Hellenistic period with further intensive contacts during the eight centuries of the Roman–Persian wars. At first the Parthians' mobility greatly confounded the Romans, whose armoured close-order infantry proved unable to match the speed of the Parthians. However, later the Romans would successfully adapt such heavy armor and cavalry tactics by creating their own units of cataphracts and "clibanarii".
The decline of the Roman infrastructure made it more difficult to field large infantry forces, and during the 4th and 5th centuries cavalry began to take a more dominant role on the European battlefield, also in part made possible by the appearance of new, larger breeds of horses. The replacement of the Roman saddle by variants on the Scythian model, with pommel and cantle, was also a significant factor as was the adoption of stirrups and the concomitant increase in stability of the rider's seat. Armored Cataphracts began to be deployed in eastern Europe and the near East, following the precedents established by Persian forces, as the main striking force of the armies in contrast to the earlier roles of cavalry as scouts, raiders, and outflankers.
The late Roman cavalry tradition and the mounted nobility of the Germanic invaders both contributed to the development of mediaeval knightly cavalry.
Asia.
Central Asia.
Xiongnu, Tujue, Avars, Kipchaks, Mongols, Don Cossacks and the various Turkic peoples are also examples of the horse-mounted groups that managed to gain substantial successes in military conflicts with settled agrarian and urban societies, due to their strategic and tactical mobility. As European states began to assume the character of bureaucratic nation-states supporting professional standing armies, recruitment of these mounted warriors was undertaken in order to fill the strategic roles of scouts and raiders. The best known instance of the continued employment of mounted tribal auxiliaries were the Cossack cavalry regiments of Tsarist Russia. In eastern Europe, Russia, and out onto the steppes, cavalry remained important much longer and dominated the scene of warfare until the early 17th century and even beyond, as the strategic mobility of cavalry was crucial for the semi-nomadic pastoralist lives that many steppe cultures led. Tibetans also had a tradition of cavalry warfare, in several military engagements with the Chinese Tang dynasty (618 – 907 AD).
East Asia.
Further east, the military history of China, specifically northern China, held a long tradition of intense military exchange between Han Chinese infantry forces of the settled dynastic empires and the mounted nomads or "barbarians" of the north. The naval history of China was centered more to the south, where mountains, rivers, and large lakes necessitated the employment of a large and well-kept navy.
In 307 BC, King Wuling of Zhao, the ancient Chinese ruler of the former State of Jin territory, ordered his military commanders and troops to adopt the trousers of the nomads as well as practice the nomads' form of mounted archery to hone their new cavalry skills. Soon afterwards the cavalry tactics employed by the State of Zhao forced their enemies in the other Warring States to adopt the same techniques in order to mount any effective attack against their swift movements on the battlefield.
The adoption of massed cavalry in China also broke the tradition of the chariot-riding Chinese aristocracy in battle, which had been in use since the ancient Shang Dynasty (c. 1600 BC-1050 BC). By this time large Chinese infantry-based armies of 100,000 to 200,000 troops were now buttressed with several hundred thousand mounted cavalry in support or as an effective striking force. The handheld pistol-and-trigger crossbow was invented in China in the 4th century BC; it was written by the Song dynasty scholars Zeng Gongliang, Ding Du, and Yang Weide in their book "Wujing Zongyao" (1044 AD) that massed missile fire by crossbowmen was the most effective defense against enemy cavalry charges.
On many occasions the Chinese studied nomadic cavalry tactics and applied the lessons in creating their own potent cavalry forces, while in others they simply recruited the tribal horsemen wholesale into their armies; and in yet other cases nomadic empires proved eager to enlist Chinese infantry and engineering, as in the case of the Mongol Empire and its sinicized part, the Yuan Dynasty (1279–1368). The Chinese recognized early on during the Han Dynasty (202 BC – 220 AD) that they were at a disadvantage in lacking the number of horses the northern nomadic peoples mustered in their armies. Emperor Wu of Han (r. 141 BC – 87 BC) went to war with the Dayuan for this reason, since the Dayuan were hording a massive amount of tall, strong, Central Asian bred horses in the Hellenized–Greek region of Fergana (established slightly earlier by Alexander the Great). Although experiencing some defeats early on in the campaign, Emperor Wu's war from 104 BC to 102 BC succeeded in gathering the prized tribute of horses from Fergana.
Cavalry tactics in China were enhanced by the invention of the saddle-attached stirrup by at least the 4th century, as the oldest reliable depiction of a rider with paired stirrups was found in a Jin Dynasty tomb of the year 322 AD. The Chinese invention of the horse collar by the 5th century was also a great improvement from the breast harness, allowing the horse to haul greater weight without heavy burden on its skeletal structure.
The horse warfare of Korea was first started during the ancient Korean kingdom Gojoseon. Since at least the 3rd century BC, there was influence of northern nomadic peoples and Yemaek peoples on Korean warfare. By roughly the 1st century BC, the ancient kingdom of Buyeo also had mounted warriors. The cavalry of Goguryeo, one of the Three Kingdoms of Korea, were called "Gaemamusa" (개마무사, 鎧馬武士), and were renowned as a fearsome heavy cavalry force. King Gwanggaeto the Great often led expeditions into the Baekje, Gaya confederacy, Buyeo, Later Yan and against Japanese invaders with his cavalry.
In the 12th century, Jurchen tribes began to violate the Goryeo–Jurchen borders, and eventually invaded Goryeo Korea. After experiencing the invasion by the Jurchen, Korean general Yun Gwan realized that Goryeo lacked efficient cavalry units. He reorganized the Goryeo military into a professional army that would contain decent and well-trained cavalry units. In 1107, the Jurchen were ultimately defeated, and surrendered to Yun Gwan. To mark the victory, General Yun built nine fortresses to the northeast of the Goryeo–Jurchen borders (동북 9성, 東北 九城).
Japan.
The ancient Japanese of the Kofun period also adopted cavalry and equine culture by the 5th century AD. The emergence of the samurai aristocracy led to the development of armoured horse archers, themselves to develop into charging lancer cavalry as gunpowder weapons rendered bows obsolete.
For example, Yabusame.Yabusame (流鏑馬?) is a type of mounted archery in traditional Japanese archery. An archer on a running horse shoots three special "turnip-headed" arrows successively at three wooden targets.
This style of archery has its origins at the beginning of the Kamakura period. Minamoto no Yoritomo became alarmed at the lack of archery skills his samurai had. He organized yabusame as a form of practice.
Nowadays, the best places to see yabusame performed are at the Tsurugaoka Hachiman-gū in Kamakura and Shimogamo Shrine in Kyoto (during Aoi Matsuri in early May). It is also performed in Samukawa and on the beach at Zushi, as well as other locations.
Kasagake or Kasakake (笠懸, かさがけ lit. "hat shooting") is a type of Japanese mounted archery. In contrast to yabusame, the types of targets are various and the archer shoots without stopping the horse. While yabusame has been played as a part of formal ceremonies, kasagake has developed as a game or practice of martial arts, focusing on technical elements of horse archery.
South Asia.
In the Indian subcontinent, cavalry played a major role from the Gupta Dynasty (320-600) period onwards. India has also the oldest evidence for the introduction of toe-stirrups.
Indian literature contains numerous references to the cavalry forces of the Central Asian horse nomads like the Sakas, Kambojas, Yavanas, Pahlavas and Paradas. Numerous Puranic texts refer to a conflict in ancient India (16th century BC) in which the cavalry forces of five nations, called five hordes ("pañca.ganan") or Kṣatriya hordes ("Kṣatriya ganah"), attacked and captured the throne of Ayudhya by dethroning its Vedic King Bahu
The Mahabharata, Ramayana, numerous Puranas and some foreign sources numerously attest that Kamboja cavalry was frequently requisitioned in ancient wars. V. R. Ramachandra Dikshitar writes: "Both the Puranas and the epics agree that the horses of the Sindhu and Kamboja regions were of the finest breed, and that the services of the Kambojas as cavalry troopers were requisitioned in ancient wars". J.A.O.S. writes: "Most famous horses are said to come either from Sindhu or Kamboja; of the latter (i.e. the Kamboja), the Indian epic Mahabharata speaks among the finest horsemen".
Mahabharata (950 c BC) speaks of the esteemed cavalry of the Kambojas, Sakas, Yavanas and Tusharas, all of whom had participated in the Kurukshetra war under the supreme command of Kamboja ruler Sudakshin Kamboj.
Mahabharata and Vishnudharmotari Purana especially styles the Kambojas, Yavansa, Gandharas etc. as "Ashva.yuddha.kushalah" (expert cavalrymen). In the Mahabharata war, the Kamboja cavalry along with that of the Sakas, Yavanas is reported to have been enlisted by the Kuru king Duryodhana of Hastinapura.
Herodotus (484 c BC–425 c BC) attests that the Gandarian mercenaries (i.e. "Gandharans/Kambojans" of Gandari Strapy of Achaemenids) from the 20th strapy of the Achaemenids were recruited in the army of emperor Xerxes I (486-465 BC), which he led against the Hellas. Similarly, the "men of the Mountain Land " from north of Kabol-River equivalent to medieval Kohistan (Pakistan), figure in the army of Darius III against Alexander at Arbela with a cavalry and fifteen elephants. This obviously refers to Kamboja cavalry south of Hindukush.
The Kambojas were famous for their horses, as well as cavalry-men ("asva-yuddha-Kushalah"). On account of their supreme position in horse (Ashva) culture, they were also popularly known as Ashvakas, i.e. the "horsemen" and their land was known as "Home of Horses". They are the Assakenoi and Aspasioi of the Classical writings, and the Ashvakayanas and Ashvayanas in Pāṇini's Ashtadhyayi. The Assakenoi had faced Alexander with 30,000 infantry, 20,000 cavalry and 30 war elephants. Scholars have identified the Assakenoi and Aspasioi clans of Kunar and Swat valleys as a section of the Kambojas. These hardy tribes had offered stubborn resistance to Alexander (326 c BC) during latter's campaign of the Kabul, Kunar and Swat valleys and had even extracted the praise of the Alexander's historians. These highlanders, designated as ""parvatiya Ayudhajivinah"" in Pāṇini's Astadhyayi, were rebellious, fiercely independent and freedom-loving cavalrymen who never easily yielded to any overlord.
The Sanskrit drama "Mudra-rakashas" by "Visakha Dutta" and the Jaina work "Parisishtaparvan" refer to Chandragupta's (320 C BC–298 c BC) alliance with Himalayan king "Parvataka". The Himalayan alliance gave Chandragupta a formidable composite army made up of the cavalry forces of the Shakas, Yavanas, Kambojas, Kiratas, Parasikas and Bahlikas as attested by Mudra-Rakashas (Mudra-Rakshasa 2). These hordes had helped Chandragupta Maurya defeat the ruler of Magadha and placed Vhandragupta on the throne, thus laying the foundations of Mauryan Dynasty in Northern India.
The cavalry of Hunas and the Kambojas is also attested in the Raghu Vamsa epic poem of Sanskrit poet Kalidasa. Raghu of Kalidasa is believed to be Chandragupta II ("Vikaramaditya") (375–413/15 AD), of the well-known Gupta Dynasty.
As late as mediaeval era, the Kamboja cavalry had also formed part of the Gurjara-Pratihara armed forces from the 8th to the 10th centuries AD. They had come to Bengal with the Pratiharas when the latter conquered part of the province.
Ancient Kambojas were constituted into military "Sanghas" and Srenis (Corporations) to manage their political and military affairs, as Arthashastra of Kautiliya as well as the Mahabharata amply attest for us. They are attested to be living as "Ayuddha-jivi" or "Shastr-opajivis" (Nation-in-arms), which also means that the Kamboja cavalry offered its military services to other nations as well. There are numerous references to Kambojas having been requisitioned as cavalry troopers in ancient wars by outside nations.
European Middle Ages.
Although Roman cavalry had no stirrups, their horned saddle allowed the combination of a firm seat with substantial flexibility. But the introduction of the wraparound saddle during the Middle Ages provided greater efficiency in mounted shock combat and the important invention of the stirrup enabled a broader array of attacks to be delivered from the back of a horse. As a greater weight of man and armor could be supported in the saddle, the probability of being dismounted in combat was significantly reduced and the impact of a charge increased.
Finally, the introduction of spurs allowed better control of the mount during the "knightly charge" in full gallop. In western Europe there emerged what is considered the "ultimate" heavy cavalry, the knight. The knights and other similarly equipped mounted men-at-arms charged in close formation, exchanging flexibility for a massive, irresistible first charge.
The mounted men-at-arms quickly became an important force in Western European tactics. Medieval military doctrine employed them as part of a combined-arms force along with various kinds of foot troops; however medieval chroniclers tended to pay undue attention to the knights at the expense of the rank and file, which led early students of military history to suppose that this heavy cavalry was the only force that mattered on medieval European battlefields, which was not the case.
Massed English longbowmen triumphed over French cavalry at Crécy, Poitiers and Agincourt, while at Gisors (1188), Bannockburn (1314), and Laupen (1339), foot-soldiers proved their invulnerability to cavalry charges as long as they held their formation. Once the Swiss developed their pike squares for offensive as well as defensive use, infantry started to become the principal arm. This aggressive new doctrine gave the Swiss victory over a range of adversaries, and their enemies found that the only reliable way to defeat them was by the use of an even more comprehensive combined arms doctrine, as evidenced in the Battle of Marignano. The introduction of missile weapons that required less skill than the longbow, such as the crossbow and hand cannon, also helped remove the focus somewhat from cavalry elites to masses of cheap infantry equipped with easy-to-learn weapons. These missile weapons were very successfully used in the Hussite Wars, in combination with Wagenburg tactics.
This gradual rise in the dominance of infantry led to the adoption of dismounted tactics. From the earliest times knights and mounted men-at-arms had frequently dismounted to handle enemies they could not overcome on horseback, such as in the Battle of the Dyle (891) and the Battle of Bremule (1119), but after the 1350s this trend became more marked with the dismounted men-at-arms fighting as super-heavy infantry with two-handed swords and poleaxes. In any case, warfare in the Middle Ages tended to be dominated by raids and sieges rather than pitched battles, and mounted men-at-arms rarely had any choice other than dismounting when faced with the prospect of assaulting a fortified position.
Greater Middle East.
Arabs.
The Islamic Prophet Muhammad made use of cavalry in many of his military campaigns including the Expedition of Dhu Qarad, and the expedition of Zaid ibn Haritha in al-Is which took place in September, 627 AD, 5th month of 6 AH of the Islamic calendar.
Early organized Arab mounted forces under the Rashidun caliphate comprised a light cavalry armed with lance and sword. Its main role was to attack the enemy flanks and rear. These relatively lightly armored horsemen formed the most effective element of the Muslim armies during the later stages of the Islamic conquest of the Levant. The best use of this lightly armed fast moving cavalry was revealed at the Battle of Yarmouk (636 AD) in which Khalid ibn Walid, knowing the skills of his horsemen, used them to turn the tables at every critical instance of the battle with their ability to engage, disengage, then turn back and attack again from the flank or rear. A strong cavalry regiment was formed by Khalid ibn Walid which included the veterans of the campaign of Iraq and Syria. Early Muslim historians have given it the name "Mutaharrik tulai'a"( متحرك طليعة ), or the Mobile guard. This was used as an advance guard and a strong striking force to route the opposing armies with its greater mobility that give it an upper hand when maneuvering against any Byzantine army. With this mobile striking force, the conquest of Syria was made easy.
The Battle of Talas in 751 AD was a conflict between the Arab Abbasid Caliphate and the Chinese Tang dynasty over the control of Central Asia. Chinese infantry were routed by Arab cavalry near the bank of the River Talas.
Later Mamluks were trained as cavalry soldiers. Mamluks were to follow the dictates of al-furusiyya, a code of conduct that included values like courage and generosity but also doctrine of cavalry tactics, horsemanship, archery and treatment of wounds.
Iran.
Qizilbash, were considered by patrons to be elite cavalry units; or a corps of zealot militants by their opponents.
Renaissance Europe.
Ironically, the rise of infantry in the early 16th century coincided with the "golden age" of heavy cavalry; a French or Spanish army at the beginning of the century could have up to half its numbers made up of various kinds of light and heavy cavalry, whereas in earlier medieval and later 17th-century armies the proportion of cavalry was seldom more than a quarter.
Knighthood largely lost its military functions and became more closely tied to social and economic prestige in an increasingly capitalistic Western society. With the rise of drilled and trained infantry, the mounted men-at-arms, now sometimes called "gendarmes" and often part of the standing army themselves, adopted the same role as in the Hellenistic age, that of delivering a decisive blow once the battle was already engaged, either by charging the enemy in the flank or attacking their commander-in-chief.
From the 1550s onwards, the use of gunpowder weapons solidified infantry's dominance of the battlefield and began to allow true mass armies to develop. This is closely related to the increase in the size of armies throughout the early modern period; heavily armored cavalrymen were expensive to raise and maintain and it took years to replace a skilled horseman or a trained horse, while arquebusiers and later musketeers could be trained and kept in the field at much lower cost, and were much easier to replace.
The Spanish tercio and later formations relegated cavalry to a supporting role. The pistol was specifically developed to try to bring cavalry back into the conflict, together with manoeuvres such as the caracole. The caracole was not particularly successful, however, and the charge (whether with sword, pistol, or lance) remained as the primary mode of employment for many types of European cavalry, although by this time it was delivered in much deeper formations and with greater discipline than before. The demi-lancers and the heavily armored sword-and-pistol reiters were among the types of cavalry whose heyday was in the 16th and 17th centuries, as for the Polish winged hussars, a heavy cavalry force that achieved great success against Swedes, Russians, and Turks.
18th-century Europe and Napoleonic Wars.
Cavalry retained an important role in this age of regularization and standardization across European armies. First and foremost they remained the primary choice for confronting enemy cavalry. Attacking an unbroken infantry force head-on usually resulted in failure, but extended linear infantry formations were vulnerable to flank or rear attacks. Cavalry was important at Blenheim (1704), Rossbach (1757), Eylau and Friedland (1807), remaining significant throughout the Napoleonic Wars.
The greatest cavalry charge of modern history was at the 1807 battle of Eylau, when the entire 11,000-strong French cavalry reserve, led by Maréchal Murat, launched a huge charge on and through the Russian infantry lines. However, in 1815 at the Battle of Waterloo, repeated charges by up to 9,000 French cavalrymen failed to break the line of the British and German infantry, who had formed squares.
Massed infantry was deadly to cavalry, but offered an excellent target for artillery. Once the bombardment had disordered the infantry formation, cavalry were able to rout and pursue the scattered foot soldiers. It was not until individual firearms gained accuracy and improved rates of fire that cavalry was diminished in this role as well. Even then light cavalry remained an indispensable tool for scouting, screening the army's movements, and harassing the enemy's supply lines until military aircraft supplanted them in this role in the early stages of World War I.
19th century.
Europe.
By the 19th century, European cavalry fell into four main categories:
There were cavalry variations for individual nations as well: France had the "chasseurs à cheval"; Germany had the "Jäger zu Pferd"; Bavaria had the "Chevaulegers"; and Russia had Cossacks. Britain, from the mid-18th century, had Light Dragoons as light cavalry and Dragoons, Dragoon Guards and Household Cavalry as heavy cavalry. Only after the end of the Napoleonic wars were the Household Cavalry equipped with cuirasses, and some other regiments were converted to lancers. In the United States Army the cavalry were almost always dragoons. The Imperial Japanese Army had its cavalry uniformed as hussars, but they fought as dragoons.
In the Crimean War, the Charge of the Light Brigade and the Thin Red Line at the Battle of Balaclava showed the vulnerability of cavalry, when deployed without effective support.
United States.
In the early American Civil War the regular United States Army mounted rifle, dragoon, and two existing cavalry regiments were reorganized and renamed cavalry regiments, of which there were six. Over a hundred other federal and state cavalry regiments were organized, but the infantry played a much larger role in many battles due to its larger numbers, lower cost per rifle fielded, and much easier recruitment. However, cavalry saw a role as part of screening forces and in foraging and scouting. The later phases of the war saw the Federal army developing a truly effective cavalry force fighting as scouts, raiders, and, with repeating rifles, as mounted infantry. Noted cavalry commanders included Confederate general J.E.B. Stuart and on the Union side, Philip Sheridan and George Armstrong Custer.
Post Civil War, as the volunteer armies disbanded, the regular army cavalry regiments increased in number from six to ten, among them Custer's U.S. 7th Cavalry Regiment of Little Bighorn fame, and the African-American U.S. 9th Cavalry Regiment and U.S. 10th Cavalry Regiment. The black units, along with others (both cavalry and infantry), collectively became known as the Buffalo Soldiers. According to Robert M. Utley: 
These regiments, which rarely took the field as complete organizations, served throughout the American Indian Wars through the close of the frontier in the 1890s. Volunteer cavalry regiments like the Rough Riders consisted of horsemen such as cowboys, ranchers and other outdoorsmen, that served as a cavalry in the United States Military.
Franco-Prussian War.
During the Franco-Prussian War, at the Battle of Mars-la-Tour in 1870, a Prussian cavalry brigade decisively smashed the centre of the French battle line, after skilfully concealing their approach. This event became known as Von Bredow's Death Ride after the brigade commander Adalbert von Bredow; it would be used in the following decades to argue that massed cavalry charges still had a place on the modern battlefield.
Imperial expansion.
Cavalry found a new role in colonial campaigns (irregular warfare), where modern weapons were lacking and the slow moving infantry-artillery train or fixed fortifications were often ineffective against indigenous insurgents (unless the latter offered a fight on an equal footing, as at Tel-el-Kebir, Omdurman, etc.). Cavalry "flying columns" proved effective, or at least cost-effective, in many campaigns—although an astute native commander (like Samori in western Africa, Shamil in the Caucasus, or any of the better Boer commanders) could turn the tables and use the greater mobility of their cavalry to offset their relative lack of firepower compared with European forces.
The British Indian Army maintained about forty regiments of cavalry, officered by British and manned by Indian sowars (cavalrymen). Among the more famous regiments in the lineages of the modern Indian and Pakistani armies are:
Several of these formations are still active, though they now are armoured formations, for example the Guides Cavalry in Pakistan.
The French Army maintained substantial cavalry forces in Algeria and Morocco from 1830 until the end of the Second World War. Much of the Mediterranean coastal terrain was suitable for mounted action and there was a long established culture of horsemanship amongst the Arab and Berber inhabitants. The French forces included Spahis, Chasseurs d' Afrique, Foreign Legion cavalry and mounted Goumiers. Both Spain and Italy raised cavalry regiments from amongst the indigenous horsemen of their North African territories (see regulares, Italian Spahis and savari respectively).
Imperial Germany employed mounted formations in South West Africa as part of the Schutztruppen (colonial army) garrisoning the territory.
First World War.
Pre-war developments.
At the beginning of the 20th century all armies still maintained substantial cavalry forces, although there was contention over whether their role should revert to that of mounted infantry (the historic dragoon function). Following the experience of the South African War of 1899-1902 (where mounted Boer citizen commandos fighting on foot from cover proved superior to regular cavalry) the British Army withdrew lances for all but ceremonial purposes and placed a new emphasis on training for dismounted action.
In 1908 however the six British lancer regiments in existence resumed use of this impressive but obsolete weapon for active service. In 1882 the Imperial Russian Army converted all its line hussar and lancer regiments to dragoons, with an emphasis on mounted infantry training. In 1910 these regiments reverted to their historic roles, designations and uniforms.
By 1909 official regulations dictating the role of the Imperial German cavalry had been revised to indicate an increasing realization of the realities of modern warfare. The massive cavalry charge in three waves which had previously marked the end of annual maneuvers was discontinued and a new emphasis was placed in training on scouting, raiding and pursuit; rather than main battle involvement.
In spite of significant experience in mounted warfare in Morocco during 1908-14, the French cavalry remained a highly conservative institution. The traditional tactical distinctions between heavy, medium and light cavalry branches were retained. French cuirassiers wore breastplates and plumed helmets unchanged from the Napoleonic period, during the early months of World War I. Dragoons were similarly equipped, though they did not wear cuirasses and did carry lances. Light cavalry were described as being "a blaze of colour". French cavalry of all branches were well mounted and were trained to change position and charge at full gallop.
Opening stages.
In August 1914 all combatant armies still retained substantial numbers of cavalry and the mobile nature of the opening battles on both Eastern and Western Fronts provided a number of instances of traditional cavalry actions, though on a smaller and more scattered scale than those of previous wars. The Imperial German cavalry, while as colourful and traditional as any in peacetime appearance, had adopted a practice of falling back on infantry support when any substantial opposition was encountered. These cautious tactics aroused derision amongst their more conservative French and Russian opponents but proved appropriate to the new nature of warfare. A single attempt by the German army, on 12 August 1914, to use six regiments of massed cavalry to cut off the Belgian field army from Antwerp foundered when they were driven back in disorder by rifle fire. The two German cavalry brigades involved lost 492 men and 843 horses in repeated charges against dismounted Belgian lancers and infantry. Once the front lines stabilised on the Western Front, a combination of barbed wire, machine guns and rapid fire rifles proved deadly to horse mounted troops.
On the Eastern Front a more fluid form of warfare arose from flat open terrain favorable to mounted warfare. On the outbreak of war in 1914 the bulk of the Russian cavalry was deployed at full strength in frontier garrisons and during the period that the main armies were mobilizing scouting and raiding into East Prussia and Austrian Galacia was undertaken by mounted troops trained to fight with sabre and lance in the traditional style. On 21 August 1914 the 4th Austro-Hungarian "Kavalleriedivison" fought a major mounted engagement at Jaroslavic with the Russian 10th Cavalry Division, in what was arguably the final historic battle to involve thousands of horsemen on both sides. While this was the last massed cavalry encounter on the Eastern Front, the absence of good roads limited the use of mechanized transport and even the technologically advanced Imperial German Army continued to deploy up to twenty-four horse-mounted divisions in the East, as late as 1917.
Europe 1915–18.
For the remainder of the War on the Western Front cavalry had virtually no role to play. The British and French armies dismounted many of their cavalry regiments and used them in infantry and other roles: the Life Guards for example spent the last months of the War as a machine gun corps; and the Australian Light Horse served as light infantry during the Gallipoli campaign. In September 1914 cavalry comprised 9.28% of the total manpower of the British Expeditionary Force in France—by July 1918 this proportion had fallen to 1.65%. As early as the first winter of the war most French cavalry regiments had dismounted a squadron each, for service in the trenches. The French cavalry numbered 102,000 in May 1915 but had been reduced to 63,000 by October 1918.
The German Army dismounted nearly all their cavalry in the West, maintaining only one mounted division on that front by January 1917. At the same date however the Central Powers still had twenty-four divisions of horse cavalry active or in reserve along the Russian Front.
Italy entered the war in 1915 with thirty regiments of line cavalry, lancers and light horse. While employed effectively against their Austo-Hungarian counterparts during the initial offensives across the Isonzo River, the Italian mounted forces ceased to have a significant role as the front shifted into mountainous terrain. By 1916 all cavalry machine sections and two complete cavalry divisions had been dismounted and seconded to the infantry.
Some cavalry were retained as mounted troops behind the lines in anticipation of a penetration of the opposing trenches that it seemed would never come. Tanks, introduced on the Western Front by the British in September 1916, had the capacity to achieve such breakthroughs but did not have the reliable range to exploit them. In their first major use at the Battle of Cambrai (1917), the plan was for a cavalry division to follow behind the tanks, however they were not able to cross a canal because a tank had broken the only bridge. It was not until the German Army had been forced to retreat in the Hundred Days Offensive of 1918, that cavalry were again able to operate in their intended role. There was a successful charge by the British 7th Dragoon Guards on the last day of the war.
In the wider spaces of the Eastern Front a more fluid form of warfare continued and there was still a use for mounted troops. Some wide-ranging actions were fought, again mostly in the early months of the war. However, even here the value of cavalry was overrated and the maintenance of large mounted formations at the front by the Russian Army put a major strain on the railway system, to little strategic advantage. In February 1917 the Russian regular cavalry (exclusive of Cossacks) was reduced by nearly a third from its peak number of 200,000, as two squadrons of each regiment were dismounted and incorporated into additional infantry battalions.
Middle East.
In the Middle East, during the Sinai and Palestine Campaign mounted forces (British, Indian, Ottoman, Australian, Arab and New Zealand) retained an important strategic role both as mounted infantry and cavalry.
In Egypt the mounted infantry formations like the New Zealand Mounted Rifles Brigade and Australian Light Horse of ANZAC Mounted Division, operating as mounted infantry, drove German and Ottoman forces back from Romani to Magdhaba and Rafa and out of the Egyptian Sinai Peninsula in 1916.
After a stalemate on the Gaza—Beersheba line between March and October 1917, Beersheba was captured by the Australian Mounted Division's 4th Light Horse Brigade. Their mounted charge succeeded after a coordinated attack by the British Infantry and Yeomanry cavalry and the Australian and New Zealand Light Horse and Mounted Rifles brigades. A series of coordinated attacks by these Egyptian Expeditionary Force infantry and mounted troops were also successful at the Battle of Mughar Ridge, during which the British infantry divisions and the Desert Mounted Corps drove two Ottoman armies back to the Jaffa—Jerusalem line. The infantry with mainly dismounted cavalry and mounted infantry fought in the Judean Hills to eventually almost encircle Jerusalem which was occupied shortly after.
During a pause in operations necessitated by the Spring Offensive in 1918 on the Western Front joint infantry and mounted infantry attacks towards Amman and Es Salt resulted in retreats back to the Jordan Valley which continued to be occupied by mounted divisions during the summer of 1918.
The Australian Mounted Division was armed with swords and in September, after the successful breaching of the Ottoman line on the Mediterranean coast by the British Empire infantry XXI Corps was followed by cavalry attacks by the 4th Cavalry Division, 5th Cavalry Division and Australian Mounted Divisions which almost encircled two Ottoman armies in the Judean Hills forcing their retreat. Meanwhile, Chaytor's Force of infantry and mounted infantry in ANZAC Mounted Division held the Jordan Valley, covering the right flank to later advance eastwards to capture Es Salt and Amman and half of a third Ottoman army. A subsequent pursuit by the 4th Cavalry Division and the Australian Mounted Division followed by the 5th Cavalry Division to Damascus. Armoured cars and 5th Cavalry Division lancers were continuing the pursuit of Ottoman units north of Aleppo when the Armistice of Mudros was signed by the Ottoman Empire.
Post–World War I.
A combination of military conservatism in almost all armies and post-war financial constraints prevented the lessons of 1914-18 being acted on immediately. There was a general reduction in the number of cavalry regiments in the British, French, Italian and other Western armies but it was still argued with conviction (for example in the 1922 edition of the "Encyclopædia Britannica") that mounted troops had a major role to play in future warfare. The 1920s saw an interim period during which cavalry remained as a proud and conspicuous element of all major armies, though much less so than prior to 1914.
Cavalry was extensively used in the Russian Civil War and the Soviet-Polish War. The last major cavalry battle was the Battle of Komarów in 1920, between Poland and the Russian Bolsheviks. Colonial warfare in Morocco, Syria, the Middle East and the North West Frontier of India provided some opportunities for mounted action against enemies lacking advanced weaponry.
The post-war German Army (Reichsheer) was permitted a large proportion of cavalry (18 regiments or 16.4% of total manpower) under the conditions of the Treaty of Versailles.
The British Army mechanised all cavalry regiments between 1929 and 1941, redefining their role from horse to armoured vehicles to form the Royal Armoured Corps together with the Royal Tank Regiment. The U.S. Cavalry abandoned its sabres in 1934 and commenced the conversion of its horsed regiments to mechanized cavalry, starting with the First Regiment of Cavalry in January 1933.
During the 1930s the French Army experimented with integrating mounted and mechanised cavalry units into larger formations. Dragoon regiments were converted to motorised infantry (trucks and motor cycles), and cuirassiers to armoured units; while light cavalry (Chasseurs a' Cheval, Hussars and Spahis) remained as mounted sabre squadrons. The theory was that mixed forces comprising these diverse units could utilise the strengths of each according to circumstances. In practice mounted troops proved unable to keep up with fast moving mechanised units over any distance.
The thirty-nine regiments of the British Indian Army were reduced to twenty-one as the result of a series of amalgamations immediately following World War I. The new establishment remained unchanged until 1936 when three regiments were redesignated as permanent training units, each with six, still mounted, regiments linked to them. In 1938 the process of mechanism began with the conversion of a full cavalry brigade (two Indian regiments and one British) to armoured car and tank units. By the end of 1940 all of the Indian cavalry had been mechanised, receiving light tanks, armoured cars or 15cwt trucks. The last horsed regiment of the Indian Army (other than the Viceregal Bodyguard and some Indian States Forces regiments) was the 19th King George's Own Lancers which had its last mounted parade at Rawalpindi on 28 October 1939. This unit still exists (though in the Pakistan Army) with an armour TOE.
World War II.
While most armies still maintained cavalry units at the outbreak of World War II in 1939, significant mounted action was largely restricted to the Polish, Balkan and Soviet campaigns.
Polish.
A popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This arose from misreporting of a single clash on 1 September near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabres scattered German infantry before being caught in the open by German armoured cars.
Two examples illustrate how the myth developed. First, because motorised vehicles were in short supply, the Poles used horses to pull anti-tank weapons into position. Second, there were a few incidents when Polish cavalry was trapped by German tanks, and attempted to fight free. However, this did not mean that the Polish army chose to attack tanks with horse cavalry. Later, on the Eastern Front, the Red Army did deploy cavalry units effectively against the Germans.
A more correct term would be "mounted infantry" instead of "cavalry", as horses were primarily used as a means of transportation, for which they were very suitable in view of the very poor road conditions in pre-war Poland. Another myth describes Polish cavalry as being armed with both sabres and lances; lances were used for peacetime ceremonial purposes only and the primary weapon of the Polish cavalryman in 1939 was a rifle. Individual equipment did include a sabre, probably because of well-established tradition, and in the case of a melee combat this secondary weapon would probably be more effective than a rifle and bayonet. Moreover, the Polish cavalry brigade order of battle in 1939 included, apart from the mounted soldiers themselves, light and heavy machine guns (wheeled), the Anti-tank rifle, model 35, anti-aircraft weapons, anti tank artillery such as the Bofors 37 mm, also light and scout tanks, etc. The last cavalry vs. cavalry mutual charge in Europe took place in Poland during the battle of Krasnobrod, when Polish and German cavalry units clashed with each other.
The last classical cavalry charge of the war was that made on March 1, 1945 during the Battle of Schoenfeld by the 1st "Warsaw" Independent Cavalry Brigade. Infantry and tanks had been employed to little effect against the German position, both of which floundered in the open wetlands only to be dominated by infantry and antitank fire from the German fortifications on the forward slope of Hill 157, overlooking the wetlands. The Germans had not taken cavalry into consideration when fortifying their position which, combined with the "Warsaw"s swift assault, overran the German anti-tank guns and consolidated into an attack into the village itself, now supported by infantry and tanks.
Greek.
The Italian invasion of Greece in October 1940 saw mounted cavalry used effectively by the Greek defenders along the mountainous frontier with Albania. Three Greek cavalry regiments (two mounted and one partially mechanized) played an important role in the Italian defeat in this difficult terrain.
Soviet.
By the final stages of the war only the Soviet Union was still fielding mounted units in substantial numbers, some in combined mechanized and horse units. The advantage of this approach was that in exploitation mounted infantry could keep pace with advancing tanks. Other factors favouring the retention of mounted forces included the high quality of Russian Cossacks and other horse cavalry; and the relative lack of roads suitable for wheeled vehicles in many parts of the Eastern Front. Another consideration was that the logistic capacity required to support very large motorised forces exceeded that necessary for mounted troops. The main usage of Soviet cavalry involved infiltration through front lines with subsequent deep raids, which disorganised German supply lines. Another role was the pursuit of retreating enemy forces during major frontline operations and breakthroughs.
Italian.
The last mounted sabre charge by Italian cavalry occurred on August 24, 1942 at Isbuscenski (Russia), when a squadron of the Savoia Cavalry Regiment charged the 812th Siberian Infantry Regiment. The remainder of the regiment, together with the Novara Lancers made a dismounted attack in an action that ended with the retreat of the Russians after heavy losses on both sides. The final Italian cavalry action occurred on October 17, 1942 in Poloj (Croatia) by a squadron of the Alexandria Cavalry Regiment against a large group of Yugoslav partisans.
Other Axis.
Romanian, Hungarian and Italian cavalry were dispersed or disbanded following the retreat of the Axis forces from Russia. Germany still maintained some mounted (mixed with bicycles) SS and Cossack units until the last days of the War.
Finnish.
Finland used mounted troops against Russian forces effectively in forested terrain during the winter war. The last Finnish cavalry unit was not disbanded until 1947.
US.
The U.S. Army's last horse cavalry actions were fought during World War II: a) by the 26th Cavalry Regiment—a small mounted regiment of Philippine Scouts which fought the Japanese during the retreat down the Bataan peninsula, until it was effectively destroyed by January 1942; and b) on captured German horses by the mounted reconnaissance section of the U.S. 10th Mountain Division in a spearhead pursuit of the German Army across the Po Valley in Italy in April 1945. The last horsed U.S. Cavalry (the Second Cavalry Division) were dismounted in March 1944.
British Empire.
All British Army cavalry regiments had been mechanised since 1 March 1942 when the Queen's Own Yorkshire Dragoons (Yeomanry) was converted to a motorised role, following mounted service against the Vichy French in Syria the previous year. The final cavalry charge by British Empire forces occurred on 21 March 1942 when a 60 strong patrol of the Burma Frontier Force encountered Japanese infantry near Toungoo airfield in central Burma. The Sikh sowars of the Frontier Force cavalry, led by Captain Arthur Sandeman of The Central India Horse (21st King George V's Own Horse), charged in the old style with sabres and most were killed.
Mongolia.
In the early stages of World War II, mounted units of the Mongolian People's Army were involved in the Battle of Khalkhin Gol against invading Japanese forces. Soviet forces under the command of Georgy Zhukov, together with Mongolian forces, defeated the Japanese Sixth army and effectively ended the Soviet–Japanese Border Wars. After the Soviet–Japanese Neutrality Pact of 1941, Mongolia remained neutral throughout most of the war, but its geographical situation meant that the country served as a buffer between Japanese forces and the Soviet Union. In addition to keeping around 10% of the population under arms, Mongolia provided half a million trained horses for use by the Soviet Army. In 1945 a partially mounted Soviet-Mongolian Cavalry Mechanized Group played a supporting role on the western flank of the Soviet invasion of Manchuria. The last active service seen by cavalry units of the Mongolian Army occurred in 1946–1948, during border clashes between Mongolia and the Republic of China.
Post–World War II to present day.
While most modern "cavalry" units have some historic connection with formerly mounted troops this is not always the case. The modern Irish Defence Force (IDF) includes a "Cavalry Corps" equipped with armoured cars and Scorpion tracked combat reconnaissance vehicles. The IDF has never included horse cavalry since its establishment in 1922 (other than a small mounted escort of Blue Hussars drawn from the Artillery Corps when required for ceremonial occasions). However, the mystique of the cavalry is such that the name has been introduced for what was always a mechanised force.
Some engagements in late 20th and early 21st century guerrilla wars involved mounted troops, particularly against partisan or guerrilla fighters in areas with poor transport infrastructure. Such units were not used as cavalry but rather as mounted infantry. Examples occurred in Afghanistan, Portuguese Africa and Rhodesia. The French Army used existing mounted squadrons of Spahis to a limited extent for patrol work during the Algerian War (1954–62) and the Swiss Army maintained a mounted dragoon regiment for combat purposes until 1973. The Portuguese Army used horse mounted cavalry with some success in the wars of independence in Angola and Mozambique in the 1960s and 1970s. During the 1964-79 Rhodesian Bush War the Rhodesian Army created an elite mounted infantry unit called Grey's Scouts to fight unconventional actions against the rebel forces of Robert Mugabe and Joshua Nkomo. The horse mounted infantry of the Scouts were effective and reportedly feared by their opponents in the rebel African forces. In the 1978 to present Afghan Civil War period there have been several instances of horse mounted combat.
South and Central American armies maintained mounted cavalry for longer than those of Europe, Asia or North America. The Mexican Army included a number of horse mounted cavalry regiments as late as the mid-1990s and the Chilean Army had five such regiments in 1983 as mounted mountain troops.
The Soviet Army retained horse cavalry divisions until 1955, and even at the dissolution of the Soviet Union in 1991, there was an independent horse mounted cavalry squadron in Kyrgyzstan.
Operational horse cavalry.
Today, the Indian Army's 61st Cavalry is reported to be the largest remaining non-ceremonial horse-mounted cavalry in the world. It was raised in 1951 from the amalgamated state cavalry squadrons of Gwailior, Jodhpur, and Mysore. While primarily utilised for ceremonial purposes, the regiment can be deployed for internal security or police roles if required. The 61st Cavalry and the President's Body Guard parade in full dress uniform in New Delhi each year in what is probably the largest assembly of traditional cavalry still to be seen in the world. Both the Indian and the Pakistani armies maintain armoured regiments with the titles of Lancers or Horse, dating back to the 19th century.
As of 2007 the Chinese People's Liberation Army employed two battalions of horse-mounted border guards in Xinjing Military District for border patrol work. The PLA mounted units last saw action during border clashes with Vietnam in the 1970s and 80s, after which most cavalry units were disbanded as part of the major military downsizing of the 1980s.
In the wake of the 2008 Sichuan earthquake, there have been calls to rebuild the army horse inventory for disaster relief in difficult terrain. Subsequent Chinese media reporting confirms that the Chinese Army maintains operational horse cavalry at squadron strength in the Mongolia Autonomous Region for scouting and logistical purposes.
Ceremonial horse cavalry and armored cavalry retaining traditional titles.
Cavalry or mounted gendarmerie units continue to be maintained for purely or primarily ceremonial purposes by the United States, British, Finnish, French, Italian, Danish, Swedish, Dutch, Chilean, Portuguese, Moroccan, Algerian, Nepalese, Nigerian, Venezuelan, Brazilian, Peruvian, Paraguayan, Polish, Argentine, Senegalese, Jordanian, Pakistani, Indian, Spanish, Omani, Thai, Panamanian and Bulgarian armed forces. The Army of the Russian Federation has recently reintroduced a ceremonial mounted squadron wearing historic uniforms.
A number of armoured regiments in the British Army retain the historic designations of Hussars, Dragoons, Light Dragoons, Dragoon Guards, Lancers and Yeomanry. Only the Household Cavalry (consisting of the Life Guards' mounted squadron, The Blues and Royals' mounted squadron, the State Trumpeters of The Household Cavalry and the Household Cavalry Mounted Band) are maintained for mounted (and dismounted) ceremonial duties in London.
The French Army still has regiments with the historic designations of Cuirassiers, Hussars, Chasseurs, Dragoons and Spahis. Only the cavalry of the Republican Guard and a ceremonial "fanfare" detachment of trumpeters for the cavalry/armoured branch as a whole are now mounted.
In the Canadian Army, a number of regular and reserve units have cavalry roots, including The Royal Canadian Hussars (Montreal), the Governor General's Horse Guards, Lord Strathcona's Horse, the Royal Canadian Dragoons, and the South Alberta Light Horse. Of these, only Lord Strathcona's Horse and the Governor General's Horse Guards maintain an official ceremonial horse-mounted cavalry troop or squadron.
Both the Australian and New Zealand armies follow the British practice of maintaining traditional titles (Light Horse or Mounted Rifles) for modern mechanised units. However, neither country retains a horse-mounted unit.
Several armored units of the modern United States Army retain the designation of "Armored cavalry". The United States also has "air cavalry" units equipped with helicopters. The Horse Cavalry Detachment of the U.S. Army's 1st Cavalry Division is made up of active duty soldiers, still functions as an active unit, trained to approximate the weapons, tools, equipment and techniques used by the United States Cavalry in the 1880s.
Non-combat support roles.
The First Troop Philadelphia City Cavalry is a volunteer unit within the Pennsylvania Army National Guard which serves as a combat force when in federal service but acts in a mounted disaster relief role when in state service. In addition, the Parsons' Mounted Cavalry is a Reserve Officer Training Corps unit which forms part of the Corps of Cadets at Texas A&M University.
Some individual U.S. states actively maintain cavalry units as a part of their respective state defense forces. The Maryland Defense Force includes a cavalry unit, Cavalry Troop A, which serves primarily as a ceremonial unit. The unit training includes a saber qualification course based upon the 1926 U.S. Army course. Cavalry Troop A also assists other Maryland agencies as a rural search and rescue asset. In Massachusetts, The National Lancers trace their lineage to a volunteer cavalry militia unit established in 1836 and are currently organized as an official part of the Massachusetts Organized Militia. The National Lancers maintain three units, Troops A, B, and C, which serve in a ceremonial role and assist in search and rescue missions. In July 2004, the National Lancers were ordered into active state service to guard Camp Curtis Guild during the 2004 Democratic National Convention. The Governor's Horse Guard of Connecticut maintains two companies which are trained in urban crowd control.
Light and armored cavalry.
Historically, cavalry was divided into light and armoured cavalry and horse archers. The differences were their role in combat, the size of the mount, and how much armor was worn by the mount and rider.
Early light cavalry (like the auxiliaries of the Roman army) were typically used to scout and skirmish, to cut down retreating infantry, and for defeating enemy missile troops. Armoured cavalry such as the Byzantine cataphract were used as shock troops—they would charge the main body of the enemy and, in many cases, their actions decided the outcome of the battle, hence the later term "battle cavalry".
During the Gunpowder Age, armored cavalry become obsolete. However, many units retained cuirasses and helmets for their protective value against sword and bayonet strikes and the morale boost these provide to the wearers. By this time the main difference between light and battle cavalry was their training; the former was regarded as a tool for harassment and reconnaissance, while the latter was considered best for close-order charges.
Since the development of armored warfare the distinction between light and heavy armor has persisted basically along the same lines. Armored cars and light tanks have adopted the reconnaissance role while medium and heavy tanks are regarded as the decisive shock troops.
Social status.
From the beginning of civilization to the 20th century, ownership of heavy cavalry horses has been a mark of wealth amongst settled peoples. A cavalry horse involves considerable expense in breeding, training, feeding, and equipment, and has very little productive use except as a mode of transport.
For this reason, and because of their often decisive military role, the cavalry has typically been associated with high social status. This was most clearly seen in the feudal system, where a lord was expected to enter combat armored and on horseback and bring with him an entourage of peasants on foot. If landlords and peasants came into conflict, the peasants would be ill-equipped to defeat armored knights.
In later national armies, service as an officer in the cavalry was generally a badge of high social status. For instance prior to 1914 most officers of British cavalry regiments came from a socially privileged background and the considerable expenses associated with their role generally required private means, even after it became possible for officers of the line infantry regiments to live on their pay. Options open to poorer cavalry officers in the various European armies included service with less fashionable (though often highly professional) frontier or colonial units. These included the British Indian cavalry, the Russian Cossacks or the French Chasseurs d' Afrique.
During the 19th and early 20th centuries most monarchies maintained a mounted cavalry element in their royal or imperial guards. These ranged from small units providing ceremonial escorts and palace guards through to large formations intended for active service. The mounted escort of the Spanish Royal Household provided an example of the former and the twelve cavalry regiments of the Prussian Imperial Guard an example of the latter. In either case the officers of such units were likely to be drawn from the aristocracies of their respective societies.
On film.
Some small sense of the noise and power of a cavalry charge can be gained from the 1970 film "Waterloo", which featured some 2000 cavalrymen, some of them cossacks. It included detailed displays of the horsemanship required to manage animal and weapons in large numbers at the gallop (unlike the real battle of Waterloo, where deep mud significantly slowed the horses). The Gary Cooper movie "They Came to Cordura" contains an excellent scene of a cavalry regiment deploying from march to battleline formation. A smaller-scale cavalry charge can be seen in "" (2003); although the finished scene has substantial computer-generated imagery, raw footage and reactions of the riders are shown in the Extended Version DVD Appendices.
Other films that show cavalry actions include:

</doc>
<doc id="6818" url="https://en.wikipedia.org/wiki?curid=6818" title="Citric acid cycle">
Citric acid cycle

The citric acid cycle – also known as the tricarboxylic acid (TCA) cycle or the Krebs cycle – is a series of chemical reactions used by all aerobic organisms to generate energy through the oxidation of acetyl-CoA derived from carbohydrates, fats and proteins into carbon dioxide and chemical energy in the form of guanosine triphosphate (GTP). In addition, the cycle provides precursors of certain amino acids as well as the reducing agent NADH that is used in numerous other biochemical reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest established components of cellular metabolism and may have originated abiogenically.
The name of this metabolic pathway is derived from citric acid (a type of tricarboxylic acid) that is consumed and then regenerated by this sequence of reactions to complete the cycle. In addition, the cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, and produces carbon dioxide as a waste byproduct. The NADH generated by the TCA cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.
In eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria which lack mitochondria, the TCA reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion.
Discovery.
Several of the components and reactions of the citric acid cycle were established in the 1930s by the research of the Nobel laureate Albert Szent-Györgyi, for which he received the Nobel Prize in 1937 for his discoveries pertaining to fumaric acid, a key component of the cycle. The citric acid cycle itself was finally identified in 1937 by Hans Adolf Krebs while at the University of Sheffield, for which he received the Nobel Prize for Physiology or Medicine in 1953.
Evolution.
Components of the TCA cycle were derived from anaerobic bacteria, and the TCA cycle itself may have evolved more than once. Theoretically there are several alternatives to the TCA cycle; however, the TCA cycle appears to be the most efficient. If several TCA alternatives had evolved independently, they all appear to have converged to the TCA cycle.
Overview.
The citric acid cycle is a key metabolic pathway that unifies carbohydrate, fat, and protein metabolism. The reactions of the cycle are carried out by 8 enzymes that completely oxidize acetate, in the form of acetyl-CoA, into two molecules each of carbon dioxide and water. Through catabolism of sugars, fats, and proteins, a two-carbon organic product acetate in the form of acetyl-CoA is produced which enters the citric acid cycle. The reactions of the cycle also convert three equivalents of nicotinamide adenine dinucleotide (NAD+) into three equivalents of reduced NAD+ (NADH), one equivalent of flavin adenine dinucleotide (FAD) into one equivalent of FADH2, and one equivalent each of guanosine diphosphate (GDP) and inorganic phosphate (Pi) into one equivalent of guanosine triphosphate (GTP). The NADH and FADH2 generated by the citric acid cycle are in turn used by the oxidative phosphorylation pathway to generate energy-rich adenosine triphosphate (ATP).
One of the primary sources of acetyl-CoA is from the breakdown of sugars by glycolysis which yield pyruvate that in turn is decarboxylated by the enzyme pyruvate dehydrogenase generating acetyl-CoA according to the following reaction scheme:
The product of this reaction, acetyl-CoA, is the starting point for the citric acid cycle. Acetyl-CoA may also be obtained from the oxidation of fatty acids. Below is a schematic outline of the cycle:
Steps.
Two carbon atoms are oxidized to CO2, the energy from these reactions being transferred to other metabolic processes by GTP (or ATP), and as electrons in NADH and QH2. The NADH generated in the TCA cycle may later donate its electrons oxidative phosphorylation to drive ATP synthesis; FADH2 is covalently attached to succinate dehydrogenase, an enzyme functioning both in the TCA cycle and the mitochondrial electron transport chain in oxidative phosphorylation. FADH2, therefore, facilitates transfer of electrons to coenzyme Q, which is the final electron acceptor of the reaction catalyzed by the Succinate:ubiquinone oxidoreductase complex, also acting as an intermediate in the electron transport chain.
The citric acid cycle is continuously supplied with new carbon in the form of acetyl-CoA, entering at step 0 below.
Mitochondria in animals, including humans, possess two succinyl-CoA synthetases: one that produces GTP from GDP, and another that produces ATP from ADP. Plants have the type that produces ATP (ADP-forming succinyl-CoA synthetase). Several of the enzymes in the cycle may be loosely associated in a multienzyme protein complex within the mitochondrial matrix.
The GTP that is formed by GDP-forming succinyl-CoA synthetase may be utilized by nucleoside-diphosphate kinase to form ATP (the catalyzed reaction is GTP + ADP → GDP + ATP).
Products.
Products of the first turn of the cycle are: "one GTP (or ATP), three NADH, one QH2, two CO2".
Because two acetyl-CoA molecules are produced from each glucose molecule, two cycles are required per glucose molecule. Therefore, at the end of two cycles, the products are: two GTP, six NADH, two QH2, and four CO2"
The above reactions are balanced if Pi represents the H2PO4− ion, ADP and GDP the ADP2− and GDP2− ions, respectively, and ATP and GTP the ATP3− and GTP3− ions, respectively.
The total number of ATP obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38.
Efficiency.
The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent NADH and 2 ATP per FADH2). In eukaryotes, two equivalents of NADH are generated in glycolysis, which takes place in the cytoplasm. Transport of these two equivalents into the mitochondria consumes two equivalents of ATP, thus reducing the net production of ATP to 36. Furthermore, inefficiencies in oxidative phosphorylation due to leakage of protons across the mitochondrial membrane and slippage of the ATP synthase/proton pump commonly reduces the ATP yield from NADH and FADH2 to less than the theoretical maximum yield. The observed yields are, therefore, closer to ~2.5 ATP per NADH and ~1.5 ATP per FADH2, further reducing the total net production of ATP to approximately 30. An assessment of the total ATP yield with newly revised proton-to-ATP ratios provides an estimate of 29.85 ATP per glucose molecule.
Variation.
While the TCA cycle is in general highly conserved, there is significant variability in the enzymes found in different taxa (note that the diagrams on this page are specific to the mammalian pathway variant).
Some differences exist between eukaryotes and prokaryotes. The conversion of D-"threo"-isocitrate to 2-oxoglutarate is catalyzed in eukaryotes by the NAD+-dependent EC 1.1.1.41, while prokaryotes employ the NADP+-dependent EC 1.1.1.42. Similarly, the conversion of ("S")-malate to oxaloacetate is catalyzed in eukaryotes by the NAD+-dependent EC 1.1.1.37, while most prokaryotes utilize a quinone-dependent enzyme, EC 1.1.5.4.
A step with significant variability is the conversion of succinyl-CoA to succinate. Most organisms utilize EC 6.2.1.5, succinate–CoA ligase (ADP-forming) (despite its name, the enzyme operates in the pathway in the direction of ATP formation). In mammals a GTP-forming enzyme, succinate–CoA ligase (GDP-forming) (EC 6.2.1.4) also operates. The level of utilization of each isoform is tissue dependent. In some acetate-producing bacteria, such as "Acetobacter aceti", an entirely different enzyme catalyzes this conversion – EC 2.8.3.18, succinyl-CoA:acetate CoA-transferase. This specialized enzyme links the TCA cycle with acetate metabolism in these organisms. Some bacteria, such as "Helicobacter pylori", employ yet another enzyme for this conversion – succinyl-CoA:acetoacetate CoA-transferase (EC 2.8.3.5).
Some variability also exists at the previous step – the conversion of 2-oxoglutarate to succinyl-CoA. While most organisms utilize the ubiquitous NAD+-dependent 2-oxoglutarate dehydrogenase, some bacteria utilize a ferredoxin-dependent 2-oxoglutarate synthase (EC 1.2.7.3).
Other organisms, including obligately autotrophic and methanotrophic bacteria and archaea, bypass succinyl-CoA entirely, and convert 2-oxoglutarate to succinate via succinate semialdehyde, using EC 4.1.1.71, 2-oxoglutarate decarboxylase, and EC 1.2.1.79, succinate-semialdehyde dehydrogenase.
Regulation.
The regulation of the TCA cycle is largely determined by product inhibition and substrate availability. If the cycle were permitted to run unchecked, large amounts of metabolic energy could be wasted in overproduction of reduced coenzyme such as NADH and ATP. The major eventual substrate of the cycle is ADP which gets converted to ATP. A reduced amount of ADP causes accumulation of precursor NADH which in turn can inhibit a number of enzymes. NADH, a product of all dehydrogenases in the TCA cycle with the exception of succinate dehydrogenase, inhibits pyruvate dehydrogenase, isocitrate dehydrogenase, α-ketoglutarate dehydrogenase, and also citrate synthase. Acetyl-coA inhibits pyruvate dehydrogenase, while succinyl-CoA inhibits alpha-ketoglutarate dehydrogenase and citrate synthase. When tested in vitro with TCA enzymes, ATP inhibits citrate synthase and α-ketoglutarate dehydrogenase; however, ATP levels do not change more than 10% in vivo between rest and vigorous exercise. There is no known allosteric mechanism that can account for large changes in reaction rate from an allosteric effector whose concentration changes less than 10%.
Calcium is used as a regulator. Mitochondrial matrix calcium levels can reach the tens of micromolar levels during cellular activation. It activates pyruvate dehydrogenase phosphatase which in turn activates the pyruvate dehydrogenase complex. Calcium also activates isocitrate dehydrogenase and α-ketoglutarate dehydrogenase. This increases the reaction rate of many of the steps in the cycle, and therefore increases flux throughout the pathway.
Citrate is used for feedback inhibition, as it inhibits phosphofructokinase, an enzyme involved in glycolysis that catalyses formation of fructose 1,6-bisphosphate,a precursor of pyruvate. This prevents a constant high rate of flux when there is an accumulation of citrate and a decrease in substrate for the enzyme.
Recent work has demonstrated an important link between intermediates of the citric acid cycle and the regulation of hypoxia-inducible factors (HIF). HIF plays a role in the regulation of oxygen homeostasis, and is a transcription factor that targets angiogenesis, vascular remodeling, glucose utilization, iron transport and apoptosis. HIF is synthesized consititutively, and hydroxylation of at least one of two critical proline residues mediates their interaction with the von Hippel Lindau E3 ubiquitin ligase complex, which targets them for rapid degradation. This reaction is catalysed by prolyl 4-hydroxylases. Fumarate and succinate have been identified as potent inhibitors of prolyl hydroxylases, thus leading to the stabilisation of HIF.
Major metabolic pathways converging on the TCA cycle.
Several catabolic pathways converge on the TCA cycle. Most of these reactions add intermediates to the TCA cycle, and are therefore known as anaplerotic reactions, from the Greek meaning to “fill up”. These increase the amount of acetyl CoA that the cycle is able to carry, increasing the mitochondrion's capability to carry out respiration if this is otherwise a limiting factor. Processes that remove intermediates from the cycle are termed "cataplerotic" reactions.
In this section and in the next, the citric acid cycle intermediates are indicated in "italics" to distinguish them from other substrates and end-products.
Pyruvate molecules produced by glycolysis are actively transported across the inner mitochondrial membrane, and into the matrix. Here they can be oxidized and combined with coenzyme A to form CO2, "acetyl-CoA", and NADH, as in the normal cycle. 
However, it is also possible for pyruvate to be carboxylated by pyruvate carboxylase to form "oxaloacetate". This latter reaction ”fills up” the amount of "oxaloacetate" in the citric acid cycle, and is therefore an anaplerotic reaction, increasing the cycle’s capacity to metabolize "acetyl-CoA" when the tissue's energy needs (e.g. in muscle) are suddenly increased by activity.
In the citric acid cycle all the intermediates (e.g. "citrate, iso-citrate, alpha-ketoglutarate, succinate, fumarate, malate" and "oxaloacetate") are regenerated during each turn of the cycle. Adding more of any of these intermediates to the mitochondrion therefore means that that additional amount is retained within the cycle, increasing all the other intermediates as one is converted into the other. Hence the addition of any one of them to the cycle has an anaplerotic effect, and its removal has a cataplerotic effect. These anaplerotic and cataplerotic reactions will, during the course of the cycle, increase or decrease the amount of "oxaloacetate" available to combine with "acetyl-CoA" to form "citric acid". This in turn increases or decreases the rate of ATP production by the mitochondrion, and thus the availability of ATP to the cell.
"Acetyl-CoA", on the other hand, derived from pyruvate oxidation, or from the beta-oxidation of fatty acids, is the only fuel to enter the citric acid cycle. With each turn of the cycle one molecule of "acetyl-CoA" is consumed for every molecule of "oxaloacetate" present in the mitochondrial matrix, and is never regenerated. It is the oxidation of the acetate portion of "acetyl-CoA" that produces CO2 and water, with the energy thus released captured in the form of ATP.
In the liver, the carboxylation of cytosolic pyruvate into intra-mitochondrial "oxaloacetate" is an early step in the gluconeogenic pathway which converts lactate and de-aminated alanine into glucose, under the influence of high levels of glucagon and/or epinephrine in the blood. Here the addition of "oxaloacetate" to the mitochondrion does not have a net anaplerotic effect, as another citric acid cycle intermediate ("malate") is immediately removed from the mitochondrion to be converted into cytosolic oxaloacetate, which is ultimately converted into glucose, in a process that is almost the reverse of glycolysis.
In protein catabolism, proteins are broken down by proteases into their constituent amino acids. Their carbon skeletons (i.e. the de-aminated amino acids) may either enter the citric acid cycle as intermediates (e.g. "alpha-ketoglutarate" derived from glutamate or glutamine), having an anaplerotic effect on the cycle, or, in the case of leucine, isoleucine, lysine, phenylalanine, tryptophan, and tyrosine, they are converted into "acetyl-CoA" which can be burned to CO2 and water, or used to form ketone bodies, which too can only be burned in tissues other than the liver where they are formed, or excreted via the urine or breath. These latter amino acids are therefore termed "ketogenic" amino acids, whereas those that enter the citric acid cycle as intermediates can only be cataplerotically removed by entering the gluconeogenic pathway via "malate" which is transported out of the mitochondrion to be converted into cytosolic oxaloacetate and ultimately into glucose. These are the so-called "glucogenic" amino acids. De-aminated alanine, cysteine, glycine, serine, and threonine are converted to pyruvate and can consequently either enter the citric acid cycle as "oxaloacetate" (an anaplerotic reaction) or as "acetyl-CoA" to be disposed of as CO2 and water.
In fat catabolism, triglycerides are hydrolyzed to break them into fatty acids and glycerol. In the liver the glycerol can be converted into glucose via dihydroxyacetone phosphate and glyceraldehyde-3-phosphate by way of gluconeogenesis. In many tissues, especially heart and skeletal muscle tissue, fatty acids are broken down through a process known as beta oxidation, which results in the production of mitochondrial "acetyl-CoA", which can be used in the citric acid cycle. Beta oxidation of fatty acids with an odd number of methylene bridges produces propionyl-CoA, which is then converted into "succinyl-CoA" and fed into the citric acid cycle as an anaplerotic intermediate.
The total energy gained from the complete breakdown of one (six-carbon) molecule of glucose by glycolysis, the formation of 2 "acetyl-CoA" molecules, their catabolism in the citric acid cycle, and oxidative phosphorylation equals about 30 ATP molecules, in eukaryotes. The number of ATP molecules derived from the beta oxidation of a 6 carbon segment of a fatty acid chain, and the subsequent oxidation of the resulting 3 molecules of "acetyl-CoA" is 40.
Citric acid cycle intermediates serve as substrates for biosynthetic processes.
In this subheading, as in the previous one, the TCA intermediates are identified by "italics".
Several of the citric acid cycle intermediates are used for the synthesis of important compounds, which will have significant cataplerotic effects on the cycle. 
"Acetyl-CoA" cannot be transported out of the mitochondrion. To obtain cytosolic acetyl-CoA, "citrate" is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to mitochondrion as "malate" (and then converted back into "oxaloacetate" to transfer more "acetyl-CoA" out of the mitochondrion). The cytosolic acetyl-CoA is used for fatty acid synthesis and the production of cholesterol. Cholesterol can, in turn, be used to synthesize the steroid hormones, bile salts, and vitamin D.
The carbon skeletons of many non-essential amino acids are made from citric acid cycle intermediates. To turn them into amino acids the alpha keto-acids formed from the citric acid cycle intermediates have to acquire their amino groups from glutamate in a transamination reaction, in which pyridoxal phosphate is a cofactor. In this reaction the glutamate is converted into "alpha-ketoglutarate", which is a citric acid cycle intermediate. The intermediates that can provide the carbon skeletons for amino acid synthesis are "oxaloacetate" which forms aspartate and asparagine; and "alpha-ketoglutarate" which forms glutamine, proline, and arginine.
Of these amino acids, aspartate and glutamine are used, together with carbon and nitrogen atoms from other sources, to form the purines that are used as the bases in DNA and RNA, as well as in ATP, AMP, GTP, NAD, FAD and CoA.
The pyrimidines are partly assembled from aspartate (derived from "oxaloacetate"). The pyrimidines, thymine, cytosine and uracil, form the complementary bases to the purine bases in DNA and RNA, and are also components of CTP, UMP, UDP and UTP.
The majority of the carbon atoms in the porphyrins come from the citric acid cycle intermediate, "succinyl-CoA". These molecules are an important component of the hemoproteins, such as hemoglobin, myoglobin and various cytochromes.
During gluconeogenesis mitochondrial "oxaloacetate" is reduced to "malate" which is then transported out of the mitochondrion, to be oxidized back to oxaloacetate in the cytosol. Cytosolic oxaloacetate is then decarboxylated to phosphoenolpyruvate by phosphoenolpyruvate carboxykinase, which is the rate limiting step in the conversion of nearly all the gluconeogenic precursors (such as the glucogenic amino acids and lactate) into glucose by the liver and kidney.
Because the citric acid cycle is involved in both catabolic and anabolic processes, it is known as an amphibole pathway.

</doc>
<doc id="6821" url="https://en.wikipedia.org/wiki?curid=6821" title="Military engineering vehicle">
Military engineering vehicle

A military engineering vehicle is a vehicle built for the construction work or for the transportation of combat engineers on the battlefield. These vehicles may be modified civilian equipment or purpose-built military vehicles.
Types of military engineering vehicles.
Civilian and militarized heavy equipment.
Military engineering can employ a wide variety of heavy equipment in the same or similar ways to how this equipment is used outside the military. Bulldozers, cranes, graders, excavators, dump trucks, loaders, and backhoes all see extensive use by military engineers.
Military engineers may also use civilian heavy equipment which was modified for military applications. Typically, this involves adding armour for protection from battlefield hazards such as artillery, unexploded ordnance, mines, and small arms fire. Often this protection is provided by armour plates and steel jackets. Some examples of armoured civilian heavy equipment are the IDF Caterpillar D9, American D7 TPK, Canadian D6 armoured bulldozer, cranes, graders, excavators, and M35 2-1/2 ton cargo truck.
Militarized heavy equipment may also take on the form of traditional civilian equipment designed and built to unique military specifications. These vehicles typically sacrifice some depth of capability from civilian models in order to gain greater speed and independence from prime movers. Examples of this type of vehicle include high speed backhoes such as the Australian Army's High Mobility Engineering Vehicle (HMEV) from Thales or the Canadian Army's Multi-Purpose Engineer Vehicle (MPEV) from Arva.
"The main article for civilian heavy equipment is:" Heavy equipment (construction)
Armoured engineering vehicle.
Typically based on the platform of a main battle tank, these vehicles go by different names depending upon the country of use or manufacture. In the US the term "combat engineer vehicle (CEV)" is used, in the UK the term "Armoured Vehicle Royal Engineers (AVRE)" is used, while in Canada and other commonwealth nations the term "armoured engineer vehicle (AEV)" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large calibre demolition cannon, augers, winches, excavator arms and cranes or lifting booms.
These vehicles are designed to directly conduct obstacle breaching operations and to conduct other earth-moving and engineering work on the battlefield.
Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle.
It should be noted that while the term "armoured engineer vehicle" is used specifically to describe these multi-purpose tank based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank based engineering vehicles used in the support of mechanized forces. Thus, "armoured engineer vehicle" used generically would refer to AEV, AVLB, Assault Breachers, and so on.
Armoured earth mover.
Lighter and less multi-functional than the AEVs described above, these vehicles are designed to conduct earth-moving work on the battlefield. These vehicles have greater high speed mobility than traditional heavy equipment and are protected against the effects of blast and fragmentation. Good examples are the American M9 ACE and the UK FV180 Combat Engineer Tractor.
Breaching vehicle.
These vehicles are equipped with mechanical or other means for the breaching of man made obstacles. Common types of breaching vehicles include mechanical flails, mine plough vehicles, and mine roller vehicles. In some cases, these vehicles will also mount Mine-clearing line charges. Breaching vehicles may be either converted armoured fighting vehicles or purpose built vehicles. In larger militaries, converted AFV are likely to be used as "assault breachers" while the breached obstacle is still covered by enemy observation and fire, and then purpose built breaching vehicles will create additional lanes for following forces.
Good examples of breaching vehicles include the USMC M1 Assault Breacher Vehicle, the UK Aardvark JSFU, and the Singaporean Trailblazer.
Bridging vehicles.
Several types of military bridging vehicles have been developed. An armoured vehicle-launched bridge (AVLB) is typically a modified tank hull converted to carry a bridge into battle in order to support crossing ditches, small waterways, or other gap obstacles.
Another type of bridging vehicle is the truck launched bridge. The Soviet TMM bridging truck could carry and launch a 10-meter bridge that could be daisy-chained with other TMM bridges to cross larger obstacles. More recent developments have seen the conversion of AVLB and truck launched bridge with launching systems that can be mounted on either tank or truck for bridges that are capable of supporting heavy main battle tanks.
Earlier examples of bridging vehicles include a type in which a converted tank hull is the bridge. On these vehicles, the hull deck comprises the main portion of the tread way while ramps extend from the front and rear of the vehicle to allow other vehicles to climb over the bridging vehicle and cross obstacles. An example of this type of armoured bridging vehicle was the Churchill Ark used in the Second World War.
Combat engineer section carriers.
Another type of CEVs are armoured fighting vehicles which are used to transport sappers (combat engineers) and can be fitted with a bulldozer's blade and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).
Military ferries and amphibious crossing vehicles.
One of the major tasks of military engineering is crossing major rivers. Several military engineering vehicles have been developed in various nations to achieve this task. One of the more common types is the amphibious ferry such as the M3 Amphibious Rig. These vehicles are self-propelled on land, they can transform into raft type ferries when in the water, and often multiple vehicles can connect to form larger rafts or floating bridges. Other types of military ferries, such as the Soviet "Plavayushij Transportyor - Srednyj", are able to load while still on land and transport other vehicles cross country and over water.
In addition to amphibious crossing vehicles, military engineers may also employ several types of boats. Military assault boats are small boats propelled by oars or an outboard motor and used to ferry dismounted infantry across water.
Tank based combat engineering vehicles.
Most CEVs are armoured fighting vehicles that may be based on a tank chassis and have special attachments in order to breach obstacles. Such attachments may include dozer blades, mine rollers, cranes etc. An example of an engineering vehicle of this kind is a bridgelaying tank, which replaces the turret with a segmented hydraulic bridge.
The Hobart's Funnies of the Second World War were a wide variety of armoured vehicles for combat engineering tasks. They were allocated to the initial beachhead assaults by the British and Commonwealth forces in the D-Day landings
Churchill tank.
The British Churchill tank because of its good cross-country performance and capacious interior with side hatches became the most adapted with modifications, the base unit being the AVRE carrying a large demolition gun.

</doc>
<doc id="6822" url="https://en.wikipedia.org/wiki?curid=6822" title="Catalonia">
Catalonia

Catalonia (, , ) is an autonomous community of Spain, located on the northeastern part of the Iberian Peninsula. It is politically designated as a "nationality" by its Statute of Autonomy. Catalonia consists of four provinces: Barcelona, Girona, Lleida, and Tarragona. The capital and largest city is Barcelona, the second-largest city in Spain and the centre of one of the largest metropolitan areas in Europe and the Mediterranean basin.
Catalonia comprises most of the territory of the former Principality of Catalonia, with the remainder now part of France's Pyrénées-Orientales. It is bordered by France and Andorra to the north, the Mediterranean Sea to the east, and the Spanish autonomous communities of Aragon to the west and Valencia to the south. The official languages are Catalan, Spanish, and the Aranese dialect of Occitan.
In the late 8th century, the counties of the March of Gothia and the Hispanic March were established by Francia as feudatory vassals across and near the eastern Pyrenees as a defensive barrier against Muslim invasions. The eastern counties of these marches were united under the rule of the Frankish vassal the Count of Barcelona, and were later called Catalonia. In 1137, Catalonia and the Kingdom of Aragon were united by marriage under the Crown of Aragon, and the Principality of Catalonia became the base for the Crown of Aragon's naval power and expansionism in the Mediterranean. In the later Middle Ages Catalan literature flourished. Between 1469 and 1516, the King of Aragon and the Queen of Castile married and ruled their kingdoms together, retaining all their distinct institutions, Courts (parliament), and constitutions. During the Franco-Spanish War (1635–59), Catalonia revolted (1640–52) against a large and burdensome presence of the Spanish army in its territory, becoming a republic under French protection. Within a brief period France took full control of Catalonia until it was largely reconquered by the Spanish army. Under the terms of the Treaty of the Pyrenees in 1659, which ended the wider Franco-Spanish War, the Spanish Crown ceded the northern parts of Catalonia, mostly incorporated in the county of Roussillon, to France. During the War of the Spanish Succession (1701–14), the Crown of Aragon sided against the Bourbon Philip V of Spain, whose subsequent victory led to the abolition of non-Castilian institutions in all of Spain and the replacement of Latin and other languages (such as Catalan) with Spanish in legal documents.
In the nineteenth century, Catalonia was severely affected by the Napoleonic and Carlist Wars. In the second half of the century Catalonia experienced industrialisation. As wealth from the industrial expansion grew, Catalonia saw a cultural renaissance coupled with incipient nationalism while several workers movements appeared. In 1914, the four Catalan provinces formed a Commonwealth, and with the return of democracy during the Second Spanish Republic (1931–39), the Generalitat of Catalonia was restored as an autonomous government. After the Spanish Civil War, the Francoist dictatorship enacted repressive measures, abolishing Catalan institutions and banning the official use of the Catalan language again. From the late 1950s through to the early 1970s, Catalonia saw rapid economic growth, drawing many workers from across Spain, making Barcelona one of Europe's largest industrial metropolitan areas and Catalonia into a major tourist destination. Since the Spanish transition to democracy (1975–82), Catalonia has gained some political and cultural autonomy and is now one of the most economically dynamic communities of Spain.
Etymology and pronunciation.
The name "Catalunya" (Catalonia)—spelled "Cathalonia", or "Cathalaunia", in Mediaeval Latin—began to be used for the homeland of the Catalans ("Cathalanenses") in the late 11th century and was probably used before as a territorial reference to the group of counties that comprised part of the March of Gothia and March of Hispania under the control of the Count of Barcelona and his relatives. The origin of the name "Catalunya" is subject to diverse interpretations because of a lack of evidence.
One theory suggests that "Catalunya" derives from the name "Gothia" (or "Gauthia") "Launia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, known as "Gothia", whence "Gothland" > "Gothlandia" > "Gothalania" > "Cathalaunia" > "Catalonia" theoretically derived. During the Middle Ages, Byzantine chroniclers claimed that "Catalania" derives from the local medley of Goths with Alans, initially constituting a "Goth-Alania".
Other less plausible theories suggest:
In English, "Catalonia" is pronounced . The native name, "Catalunya", is pronounced in Central Catalan, the most widely spoken variety whose pronunciation is considered standard. The Spanish name is "Cataluña" (), and the Aranese name is "Catalonha" ().
History.
Pre-Roman and Roman period.
In pre-Roman times, the area that is now called Catalonia in the north-east of Iberian Peninsula, like the rest of the Mediterranean side of the peninsula, was populated by the Iberians. Coastal trading colonies were established by the ancient Greeks, who settled around the Roses area. Both Greeks and Carthaginians briefly ruled the territory in the course of the Second Punic War and traded with the surrounding Iberian population.
After the Carthaginian defeat by the Roman Republic, the north-east of Iberia became the first to come under Roman rule and became part of Hispania, the westernmost part of the Roman Empire. Tarraco (modern Tarragona) was one of the most important Roman cities in Hispania and the capital of the province of Tarraconensis.
Middle Ages.
After the fall of the Western Roman Empire, the area was conquered by the Visigoths and was ruled as part of the Visigothic Kingdom for almost two and a half centuries. In 718, it came under Muslim control and became part of Al-Andalus, a province of the Umayyad Caliphate. From the conquest of Roussillon in 760, to the conquest of Barcelona in 801, the Frankish empire took control of the area between Septimania and the Llobregat river from the Muslims and created heavily militarised, self-governing counties. These counties formed part of the Gothic and Hispanic marches, a buffer zone in the south of the Frankish empire in the former province of Septimania and in the northeast of the Iberian Peninsula, to act as a defensive barrier for the Frankish empire against further Muslim invasions from Al-Andalus.
These counties came under the rule of the counts of Barcelona, who were Frankish vassals nominated by the emperor of the Franks, to whom they were feudatories (801–987). The earliest known use of the name "Catalonia" for these counties dates to 1117. During the 9th century, the Count Wifred the Hairy made its title hereditary and founded the dynasty of the House of Barcelona, which ruled Catalonia until 1410.
In 987 Borrell II, Count of Barcelona, did not recognise Hugh Capet as his king, making his successors (from Ramon Borell I to Ramon Berenguer IV) de facto independent of the Carolingian crown. At the start of eleventh century the Catalan Counties suffer an important process of feudalisation, partially controlled by the Peace and Truce Assemblies and by the power and negotiations of the Counts of Barcelona like Ramon Berenguer I. In 1137, Ramon Berenguer IV, Count of Barcelona decided to accept King Ramiro II of Aragon's proposal to marry Queen Petronila, establishing the dynastic union of the County of Barcelona with the Kingdom of Aragon, joining the Crown of Aragon and making the Catalan counties that were united under the county of Barcelona into a principality of the Aragonese Crown.
In 1258, by means of the Treaty of Corbeil, the Count of Barcelona and King of Aragon, of Mallorca and of Valencia, James I of Aragon renounced his family rights and dominions in Occitania and recognised the king of France as heir of the Carolingian Dynasty. The king of France formally relinquished his nominal feudal lordship over all the Catalan counties, excepting the County of Foix despite the opposition of the King of Aragon and Count of Barcelona. This treaty transformed the principality's "de facto" union with Aragon into a "de jure" one and was the origin of the definitive separation between both geographical areas Catalonia and Languedoc.
As a coastal territory, Catalonia became the base of the Aragonese Crown's maritime forces, which spread the power of the Aragonese Crown in the Mediterranean, and made Barcelona into a powerful and wealthy city. In the period 1164–1410 new territories, the Kingdom of Valencia, the Kingdom of Majorca, Sardinia, the Kingdom of Sicily, Corsica and (briefly) the Duchy of Athens, were incorporated into the dynastic domains of the House of Aragon.
At the same time, the Principality of Catalonia developed a complex institutional and political system based in the concept of pact between the estates of the realm and the king. The laws had to be approved in the General Court of Catalonia, one of the first parliamentary bodies of Europe that banned the royal power to create legislation unilaterally (since 1283). The Courts were composed by the three Estates, were presided by the king of Aragon and approved the constitutions, which created a compilation of rights for the whole citizens of the Principality. In order to recapt general taxes, the Courts of 1359 established a permanent representation of deputies, called Deputation of the General (and later sometimes known as Generalitat), that gained an important political power in the next centuries.
The domains of the Aragonese Crown were severely affected by the Black Death pandemic and by later outbreaks of the plague. Between 1347 and 1497 Catalonia lost 37 percent of its population.
In 1410, King Martin I died without surviving descendants. Under the Compromise of Caspe, Ferdinand from the Castilian House of Trastámara received the Crown of Aragon as Ferdinand I of Aragon.
Modern Era.
The grandson of Ferdinand I, Ferdinand II of Aragon and Queen Isabella I of Castile were married in 1469, later taking the title the Catholic Monarchs; subsequently, this event was seen by historiographers as the dawn of a unified Spain. At that point, though united by marriage, the Crowns of Castile and Aragon maintained distinct territories, each kept its own traditional institutions, parliaments and laws. Castile commissioned expeditions to the Americas and benefited from the riches acquired in the Spanish colonisation of the Americas, but in time, also carried the main burden of military expenses of the united Spanish kingdoms. After Isabella's death, Ferdinand II personally ruled both kingdoms.
By virtue of descent from his maternal grandparents, Ferdinand II of Aragon and Isabella I of Castile, in 1516 Charles I of Spain became the first king to rule Castile and Aragon simultaneously by his own right. Following the death of his paternal (House of Habsburg) grandfather, Maximilian I, Holy Roman Emperor, he was also elected Charles V, Holy Roman Emperor, in 1519.
The Catalan Revolt (1640–52) saw Catalonia rebel (briefly as a republic) with French help against the Spanish Crown for overstepping Catalonia's traditional rights during the Thirty Years' War. Most of Catalonia was reconquered by the Spanish monarchy but Catalan rights were recognised. Roussillon was lost to France by the Peace of the Pyrenees (1659).
The most significant conflict concerning the governing monarchy was the War of the Spanish Succession, which began when the childless Charles II of Spain, the last Spanish Habsburg, died without an heir in 1700. Charles II had chosen Philip V of Spain from the French House of Bourbon. Catalonia, like other territories that formed the Crown of Aragon, rose up in support of the Austrian Habsburg pretender Charles VI, Holy Roman Emperor in his claim for the Spanish throne as Charles III of Spain. The fight between the houses of Bourbon and Habsburg for the Spanish Crown split Spain and Europe.
The fall of Barcelona on 11 September 1714 to the Bourbon king Philip V militarily ended the Habsburg claim to the Spanish Crown, which became legal fact in the Treaty of Utrecht. Philip felt that he had been betrayed by the Catalan Courts, as it had initially sworn its loyal to him when he had presided over it in 1701. In retaliation for the betrayal, the first Bourbon king introduced the Nueva Planta decrees that incorporated the territories of the Crown of Aragon, including Catalonia, as provinces under the Crown of Castile in 1716, terminating their separate institutions, laws and rights, within a united kingdom of Spain. During the second half of 18th century Catalonia started a successful process of proto-industrialization.
Industrialisation and beyond.
At the beginning of the nineteenth century Catalonia was severely affected by the Napoleonic and Carlist Wars. In the latter half of the 19th century, it became an industrial center. To this day it remains one of the most industrialised parts of Spain. During those years, Barcelona was the focus of important revolutionary uprisings, while the Catalan language saw a cultural renaissance (the "Renaixença").
In the first third of the 20th century, Catalonia gained and lost varying degrees of autonomy several times. In 1914, the four Catalan provinces were authorized to create a Commonwealth ("Mancomunitat"), without any legislative power or specific autonomy, that was disbanded in 1925 by the dictatorship of Primo de Rivera. After the fall of the dictator and a brief proclamation of the Catalan Republic, it received its first Statute of Autonomy during the Second Spanish Republic (1931), establishing an autonomous body, the Generalitat of Catalonia, that included a parliament, a government and a court of appeal, and the left-wing independentist leader Francesc Macià was elected its first President. This period was marked by political unrest and the preeminence of Revolutionary Catalonia during the Spanish Civil War (1936–39). The Anarchists had been active throughout the early 20th century, achieving the first eight-hour workday in Europe in 1919.
The defeat of the Second Spanish Republic in the Spanish Civil War brought fascist Francisco Franco to power as dictator. His regime imposed linguistic, political and cultural restrictions across Spain. In Catalonia, any kind of public activities associated with Catalan nationalism, republicanism, anarchism, socialism, liberalism, democracy or communism, including the publication of books on those subjects or simply discussion of them in open meetings, was banned. Franco's regime banned the use of Catalan in government-run institutions and during public events, and also the Catalan institutions of self-government were abolished. The pro-Republic of Spain President of Catalonia, Lluís Companys, was taken to Spain from his exile in the German-occupied France, and was tortured and executed in the Montjuïc Castle of Barcelona for the crime of 'military rebellion'.
During later stages of Francoist Spain, certain folkloric and religious celebrations in Catalan resumed and were tolerated. Use of Catalan in the mass media had been forbidden, but was permitted from the early 1950s in the theatre. Publishing in Catalan continued throughout the dictatorship.
The years after the war were extremely hard. Catalonia, like many other parts of Spain, had been devastated by the war. Recovery from the war damage was slow and made more difficult by the international trade embargo against Franco's dictatorial regime. By the late 1950s the country had recovered its pre-war economic levels and in the 1960s was the second fastest growing economy in the world in what became known as the Spanish miracle. During this period there was a spectacular growth of industry and tourism in Catalonia that drew large numbers of workers to the region from across Spain and made the area around Barcelona into one of Europe's largest industrial metropolitan areas.
After Franco's death in 1975, Catalonia voted for the adoption of a democratic Spanish Constitution in 1978, in which Catalonia recovered political and cultural autonomy, restoring the Generalitat from the exile in 1977 and adopting a new Statute of Autonomy in 1979. Today, Catalonia is one of the most economically dynamic communities of Spain. The Catalan capital and largest city, Barcelona, is a major international cultural centre and a major tourist destination. In 1992, Barcelona hosted the Summer Olympic Games.
21st Century.
On 9 November 2015, Catalan lawmakers approved a plan for secession from Spain by 2017 with a vote 72 to 63. The plan was suspended by the Constitutional Court.
Geography.
Climate.
The climate of Catalonia is diverse. The populated areas lying by the coast in Tarragona, Barcelona and Girona provinces feature a Mediterranean climate (Köppen "Csa"). The inland part (including the Lleida province and the inner part of Barcelona province) show a mostly continental Mediterranean climate (Köppen "Csa"). The Pyrenean peaks have a mountain (Köppen "H") or even Alpine climate (Köppen "ET") at the highest summits, while the valleys have a maritime or oceanic climate sub-type (Köppen "Cfb").
In the Mediterranean area, summers are dry and hot with sea breezes, and the maximum temperature is around . Winter is cool or slightly cold depending on the location. It snows frequently in the Pyrenees, and it occasionally snows at lower altitudes, even by the coastline. Spring and autumn are typically the rainiest seasons, except for the Pyrenean valleys, where summer is typically stormy.
The inland part of Catalonia is hotter and drier in summer. Temperature may reach , some days even . Nights are cooler there than at the coast, with the temperature of around . Fog is not uncommon in valleys and plains; it can be especially persistent, with freezing drizzle episodes and subzero temperatures during winter (record from −36 °C), along the Segre and in other river valleys.
Topography.
Catalonia has a marked geographical diversity, if we consider the relatively small size of its territory. The geography is conditioned by the Mediterranean coast, with of coastline, and large relief units of the Pyrenees to the north. The Catalan territory is divided into three main geomorphological units:
The Catalan Pyrenees represent almost half in length of the Pyrenees, as it extends more than 200 km. Traditionally differentiated the Axial Pyrenees (the main part) and the Pre-Pyrenees (southern from the Axial) which are mountainous formations parallel to the main mountain ranges but with lower altitudes, less steep and a different geological formation. The highest mountain of Catalonia, located north of the comarca of Pallars Sobirà is the Pica d'Estats (3,143 m), followed by the Puigpedrós (2,914 m). On the Pre-Pyrenees is located the Serra del Cadí, that separtes the valley of Cerdanya from the Central Depression.
Central Catalan Depression is a plain located between the Pyrenees and Pre-Coastal Mountains. The Depression lands are located between 200 and 600 meters. The plains and the water that descend from the Pyrenees have made it fertile territory for agriculture and there are built numerous irrigation canals. Other important plain is the Empordà, located on the northeast.
The Catalan Mediterranean system is based on two (more or less) parallel ranges to the coast, in a Northwest direction towards the Southwest. These two mountain ranges are the Coastal and the Pre-Coastal. The Coastal Range is minor extent and it has lower altitudes, while the Pre-Coastal is larger in both length and height. The most relevant mountains of this area are Montserrat and the Montseny. Within the ranges are a series of plains, the entities over which form the Coastal and the Pre-Coastal Depressions. The Coastal Depression is located on the East of the Coastal Range towards the coast. The Pre-Coastal, on the other hand, is located in the interior, between the two mountain ranges, and constitutes the basis of the plains of Vallès and Penedès.
Flora and fauna.
Catalonia is a showcase of European landscapes on a small scale. Just over 30,000 square kilometers hosting a variety of substrates, soils, climates, directions, altitudes and distances to the sea. The set is of great ecological diversity and a remarkable wealth of landscapes, habitats and species.
The fauna of Catalonia consists broadly of a combination of a minority of animals endemic from thid land and the majority of animals which are also present in other places. Much of Catalonia enjoys a Mediterranean climate (except mountain areas), which makes many of the animals that live there are adapted to Mediterranean ecosystems. Of mammals, are plentiful wild boar, red fox, as well as the roe deer and, in the Pyrenees, there is located the Pyrenean chamois. Other species such as the bear have been reintroduides recently.
Hydrography.
Most of Catalonia belongs to the Mediterranean Basin. The Catalan hydrographic network consists of two important basins, the one of the Ebro and the one that comprises the internal basins of Catalonia (respectively covering 46.84% and 51.43% of the territory), all of them flow to the Mediterranean. Furthermore, there is the Garona river basin that flows to the Atlantic Ocean, but it only covers 1.73% of the Catalan territory.
The hydrographic network can be divided in two sectors, an occidental slope or Ebre river slope and one oriental slope constituted by minor rivers that flow to the Mediterranean along the Catalan coast. The first slope provides an average of per year, while the second only provides an average of /year. The difference is due to the big contribution of the Ebre river, from which the Segre is an important tributary. Moreover, in Catalonia there is a relative wealth of groundwaters, although there is inequality between "comarques", given the complex geological structure of the territory. In the Pyrenees there are many small lakes, remnants of the ice age. The biggest is the one of Banyoles.
The Catalan coast is almost rectilinear, with a length of and few landforms—the most relevant are the Cap de Creus and the Gulf of Roses to the north and the Ebro Delta to the south. The Catalan Coastal Range hugs the coastline, and it is split into two segments, one between L'Estartit and the town of Blanes (the Costa Brava), and the other at the south, at the Costes del Garraf.
The principal rivers in Catalonia are the Ter, Llobregat, and the Ebre, all of which run into the Mediterranean.
Politics.
After Franco's death in 1975 and the adoption of a democratic constitution in Spain in 1978, Catalonia recovered and extended the powers that it had gained in the Statute of Autonomy of 1932 but lost with the fall of the Second Spanish Republic at the end of the Spanish Civil War in 1939.
This autonomous community has gradually achieved more autonomy since the approval of the Spanish Constitution of 1978. The Generalitat holds exclusive jurisdiction in culture, environment, communications, transportation, commerce, public safety and local government, and shares jurisdiction with the Spanish government in education, health and justice. In all, the current system grants Catalonia with "more self-government than almost any other corner in Europe".
A relatively large sector of the population supports the ideas and policies of Catalan nationalism, a political movement which defends the notion that Catalonia is a separate nation and advocates for either further political autonomy or full independence of Catalonia.
The support for Catalan nationalism ranges from a demand for further autonomy and the federalisation of Spain to the desire for independence from the rest of Spain, expressed by Catalan independentists. The first survey following the Constitutional Court ruling that cut back elements of the 2006 Statute of Autonomy, published by "La Vanguardia" on 18 July 2010, found that 46% of the voters would support independence in a referendum. In February of the same year, a poll by the Open University of Catalonia gave more or less the same results. Other polls have shown lower support for independence, ranging from 40 to 49%.
Since 2011 when the question started to be regularly surveyed by the governmental Center for Public Opinion Studies (CEO), support for Catalan independence has been on the rise. According to the CEO opinion poll from March 2016, 45,5% of Catalans would vote against independence and 45,3% for it, while a 57.2 claim to be "absolutely" or "fairly" in favour of independence.
In hundreds of non-binding local referendums on independence, organised across Catalonia from 13 September 2009, a large majority voted for independence, although critics argued that the polls were mostly held in pro-independence areas. In December 2009, 94% of those voting backed independence from Spain, on a turn-out of 25%. The final local referendum was held in Barcelona, in April 2011. On 11 September 2012, a pro-independence march pulled in a crowd of between 600,000 (according to the Spanish Government), 1.5 million (according to the Guàrdia Urbana de Barcelona), and 2 million (according to its promoters); whereas poll results revealed that half the population of Catalonia supported secession from Spain.
Two major factors were Spain's Constitutional Court's 2010 decision to declare part of the 2006 Statute of Autonomy of Catalonia unconstitutional, as well as the fact that Catalonia contributes 19.49% of the central government’s tax revenue, but only receives 14.03% of central government's spending.
Parties that consider themselves either Catalan nationalist or independentist have been present in all Catalan governments since 1980. The largest Catalan nationalist party, Convergence and Union, ruled Catalonia from 1980 to 2003, and returned to power in the 2010 election. Between 2003 and 2010, a leftist coalition, composed by the Catalan Socialists' Party, the pro-independence Republican Left of Catalonia and the leftist-environmentalist Initiative for Catalonia-Greens, implemented policies that widened Catalan autonomy.
In the November 25, 2012 Catalan parliamentary election, sovereigntist parties supporting a secession referendum gathered 59.01% of the votes and hold 87 of the 135 seats in the Catalan Parliament. Parties supporting independence from the rest of Spain obtained 49.12% of the votes and a majority of 74 seats.
Artur Mas, the president of Catalonia, organised early elections and they took place on September 27, 2015. In these elections, Convergència and Esquerra Republicana decided to join, and they presented themselves under the coalition named "Junts pel sí" (in Catalan, "Together for Yes"). Junts pel sí won 62 seats and was the most voted party, and CUP (Candidatura d'Unitat Popular, far-left and independentist party) won another 10, so the sum of all the independentist forces was 72 seats, reaching an absolute majority of seats for the independentist forces, but not in number of individual votes that was 47,74% of the total.
Statute of Autonomy.
The Statute of Autonomy of Catalonia is the fundamental organic law, second only to the Spanish Constitution from which the Statute originates.
In the Spanish Constitution of 1978 Catalonia, along with the Basque Country and Galicia, was defined as a "nationality". The same constitution gave Catalonia the automatic right to autonomy, which resulted in the Statute of Autonomy of Catalonia of 1979.
Both the 1979 Statute of Autonomy and the current one, approved in 2006, state that "Catalonia, as a nationality, exercises its self-government constituted as an Autonomous Community in accordance with the Constitution and with the Statute of Autonomy of Catalonia, which is its basic institutional law, always under the law in Spain".
The Preamble of the 2006 Statute of Autonomy of Catalonia states that the Parliament of Catalonia has defined Catalonia as a nation, but that "the Spanish Constitution recognizes Catalonia's national reality as a nationality". While the Statute was approved by and sanctioned by both the Catalan and Spanish parliaments, and later by referendum in Catalonia, it has been subject to a legal challenge by the surrounding autonomous communities of Aragon, Balearic Islands and Valencia, as well as by the conservative People's Party. The objections are based on various issues such as disputed cultural heritage but, especially, on the Statute's alleged breaches of the principle of "solidarity between regions" in fiscal and educational matters enshrined by the Constitution.
Spain's Constitutional Court assessed the disputed articles and on 28 June 2010, issued its judgment on the principal allegation of unconstitutionality presented by the People's Party in 2006. The judgment granted clear passage to 182 articles of the 223 that make up the fundamental text. The court approved 73 of the 114 articles that the People's Party had contested, while declaring 14 articles unconstitutional in whole or in part and imposing a restrictive interpretation on 27 others. The court accepted the specific provision that described Catalonia as a "nation", however ruled that it was a historical and cultural term with no legal weight, and that Spain remained the only nation recognised by the constitution.
Government and law.
The Catalan Statute of Autonomy establishes that Catalonia is organised politically through the Generalitat of Catalonia, conformed by the Parliament of Catalonia, the Presidency of the Generalitat, the Government or Executive Council and the other institutions created by the Parliament.
The seat of the Executive Council is the city of Barcelona. Since the restoration of the Generalitat on the return of democracy in Spain, the presidents of Catalonia have been Jordi Pujol i Soley (1980–2003), Pasqual Maragall i Mira (2003–2006), José Montilla (2006–2010), Artur Mas i Gavarró (2010-2016) and Carles Puigdemont, incumbent .
Security forces and Justice.
Catalonia has its own police force, the "Mossos d'Esquadra" (officially called "Mossos d'Esquadra-Policia de la Generalitat de Catalunya"), whose origins date back to the 18th century. Since 1980 they have been under the command of the Generalitat, and since 1994 they have expanded in number in order to replace the national Civil Guard and National Police Corps, which report directly to the Homeland Department of Spain. The national bodies retain personnel within Catalonia to exercise functions of national scope such as overseeing ports, airports, coasts, international borders, custom offices, the identification of documents and arms control, immigration control, terrorism prevention, arms trafficking prevention, amongst others.
Most of the justice system is administered by national judicial institutions, the highest body and last judicial instance in the Catalan jurisdiction, integrating the Spanish judiciary, is the High Court of Justice of Catalonia. The criminal justice system is uniform throughout Spain, while civil law is administered separately within Catalonia.
Navarre, the Basque Country and Catalonia are the Spanish communities with the highest degree of autonomy in terms of law enforcement.
Administrative divisions.
Catalonia is organised territorially into provinces, further subdivided into comarques and municipalities. The 2006 Statute of Autonomy of Catalonia establishes the administrative organisation of three local authorities: vegueries, comarques, and municipalities.
Provinces.
Catalonia is divided administratively into four provinces, the governing body of which is the Provincial Deputation (, ). The four provinces and their populations are:
Municipalities.
There are at present 948 municipalities in Catalonia.
Comarques.
Comarques are entities composed by the municipalities to manage their responsibilities and services. The current regional division has its roots in a decree of the Generalitat de Catalunya of 1936, in effect until 1939, when it was suppressed by Franco. In 1987 the Government adopted the territorial division again and in 1988 three new comarques were added (Alta Ribagorça, Pla d'Urgell and Pla de l'Estany), and in 2015 was created the last comarca, the Moianès. At present there are 41.
The comarca of Val d'Aran (Aran Valley) has a special status and its autonomous government is named "Conselh Generau d'Aran".
Vegueries.
The "vegueria" is a new type of division defined as a specific territorial area for the exercise of government and inter-local cooperation with legal personality. The current Statute of Autonomy states vegueries are intended to supersede provinces in Catalonia, and take over many of functions of the comarques.
The territorial plan of Catalonia ("Pla territorial general de Catalunya") provided six general functional areas, but was amended by Law 24/2001, of 31 December, recognizing the Alt Pirineu i Aran as a new functional area differentiated of Ponent. On 14 July 2010 the Catalan Parliament approved the creation of the functional area of the Penedès.
Economy.
In 2014, the regional GDP of Catalonia was €199,797 million (the highest of Spain) In that year, the GDP growth was 1.4%.
In the last years there has been a negative net relocation rate of companies based in Catalonia moving to other autonomous communities of Spain. In 2014 Calalonia lost 987 companies to other parts of Spain (mainly Madrid), getting 602 new ones from the rest of the country.
Catalonia's long-term credit rating is BB (Non-Investment Grade) according to Standard & Poor's, Ba2 (Non-Investment Grade) according to Moody's, and BBB- (Low Investment Grade) according to Fitch Ratings. Catalonia's rating is tied for worst with between 1 and 5 other autonomous communities of Spain, depending on the rating agency.
In the context of the 2008 financial crisis, Catalonia was expected to suffer a recession amounting to almost a 2% contraction of its regional GDP in 2009. Catalonia's debt in 2012 was the highest of all Spain's autonomous communities, reaching €13,476 million, i.e. 38% of the total debt of the 17 autonomous communities, but in recent years its economy recovered a positive evolution and the GDP grew a 3.3% in 2015.
In 2011, Catalonia ranked the 64th largest country subdivision by GDP (nominal). Catalonia belongs to the organisation Four Motors for Europe.
The distribution of sectors is as follows:
The main tourist destinations in Catalonia are the city of Barcelona, the beaches of the Costa Brava in Girona, the beaches of the Costa del Maresme and Costa del Garraf from Malgrat de Mar to Vilanova i la Geltrú and the Costa Daurada in Tarragona. In the High Pyrenees there are several ski resorts, near Lleida. On 1 November 2012, Catalonia started charging a tourist tax. The revenue is used to promote tourism, and to maintain and upgrade tourism-related infrastructure.
Many savings banks are based in Catalonia, with 10 of the 46 Spanish savings banks having headquarters in the region. This list includes Europe's premier savings bank, La Caixa. The first private bank in Catalonia is Banc Sabadell, ranked fourth among all Spanish private banks.
The stock market of Barcelona, which in 2004 traded almost €205,000 million, is the second largest of Spain after Madrid, and Fira de Barcelona organizes international exhibitions and congresses to do with different sectors of the economy.
The main economic cost for the Catalan families is the purchase of a home. According to data from the Society of Appraisal on 31 December 2005 Catalonia is, after Madrid, the second most expensive region in Spain for housing: 3,397 €/m² on average (see Spanish property bubble).
Transport.
Airports.
Airports in Catalonia are owned and operated by Aena (a Spanish Government entity) except two airports in Lleida which are operated by Aeroports de Catalunya (an entity belonging to the Government of Catalonia).
Ports.
The ports of Barcelona and Tarragona are the two main commercial and passenger ports in Catalonia, owned and operated by Puertos del Estado (a Spanish Government entity).
The other ports of Catalonia (26, such as the ports of Palamós or Vilanova i la Geltrú) are operated and administered by Ports de la Generalitat, a Catalan Government entity.
Roads.
There are of roads throughout Catalonia.
The principal highways are ("Autopista de la Mediterrània") and ("Autovia de la Mediterrània"). They follow the coast from the French border to Valencia, Murcia and Andalusia. The main roads generally radiate from Barcelona. The ("Autopista del Nord-est") and ("Autovia del Nord-est") connect inland and onward to Madrid.
Other major roads are:
Public-own roads in Catalonia are either managed by the autonomous government of Catalonia (e.g., roads) or the Spanish government (e.g., , , roads).
Railways.
Catalonia saw the first railway construction in the Iberian Peninsula in 1848, linking Barcelona with Mataró. Given the topography most lines radiate from Barcelona. The city has both suburban and inter-city services. The main east coast line runs through the province connecting with the SNCF (French Railways) at Portbou on the coast.
There are two publicly owned railway companies operating in Catalonia: the Catalan FGC that operates commuter and regional services, and the Spanish national RENFE that operates long-distance and high-speed rail services (AVE and Avant) and the main commuter and regional service "Rodalies de Catalunya", administered by the Catalan government since 2010.
High-speed rail (AVE) services from Madrid currently reach Lleida, Tarragona and Barcelona. The official opening between Barcelona and Madrid took place 20 February 2008. The journey between Barcelona and Madrid now takes about two-and-a-half hours. A connection to the French high-speed TGV network has been completed, but is awaiting the completion of stations along the route to begin passenger service in April 2013. This new line (currently the LGV Perpignan–Figueres-Vilafant) passes through Girona and Figueres with a tunnel through the Pyrenees. There is a direct train from Barcelona Estació de França to Paris Austerlitz along the older railway tracks.
Demographics.
Catalonia covers an area of with an official population of 7,504,008 (2015), of which non-Spanish immigrants represent about 19% according to the Spanish Statistics Institute (INE) for 2012.
The Urban Region of Barcelona includes 5,217,864 people and covers an area of , and about 1.7 million people live in a radius of from Barcelona. The metropolitan area of the Urban Region includes cities such as L'Hospitalet de Llobregat, Sabadell, Terrassa, Badalona, Santa Coloma de Gramenet and Cornellà de Llobregat.
In 1900, the population of Catalonia was 1,984,115 people and in 1970 it was 5,107,606. That increase was due to the demographic boom in Spain during the 60s and early 70s and also to the large-scale internal migration from the rural interior of Spain to its industrial cities. In Catalonia that wave of internal migration arrived from several regions of Spain, especially Andalusia, Murcia and Extremadura.
Immigrants from other countries settled in Catalonia in the 1990s and 2000s; a large percentage came from Africa and Latin America, and smaller numbers from Asia and Eastern Europe, often settling in urban centers such as Barcelona and industrial areas.
Languages.
According to the linguistic census held by the Government of Catalonia in 2013, Spanish is the most spoken language in Catalonia (46.53% claim Spanish as "their own language"), followed by Catalan (37.26% claim Catalan as "their own language"). In everyday use, 11.95% of the population claim to use both languages equally, whereas 45.92% mainly use Spanish and 35.54% mainly use Catalan. There is a significant difference between the Barcelona metropolitan area (and, to a lesser extent, the Tarragona area), where Spanish is more spoken than Catalan, and the more rural Catalonia, where Catalan clearly prevails over Spanish.
Since the Statute of Autonomy of 1979, Aranese (a dialect of Gascon Occitan) has also been official and subject to special protection in Val d'Aran. This small area of 7,000 inhabitants was the only place where a dialect of Occitan has received full official status. Then, on 9 August 2006, when the new Statute came into force, Occitan became official throughout Catalonia. Occitan is the mother tongue of 22.4% of the population of Val d'Aran. Catalan Sign Language is also officially recognised.
Originating in the historic territory of Catalonia, Catalan has enjoyed special status since the approval of the Statute of Autonomy of 1979 which declares it to be "Catalonia's own language," a term which signifies a language given special legal status within a Spanish territory, or which is historically spoken within a given region. The other languages with official status are Spanish, which has official status throughout Spain, and Aranese Occitan, which enjoys co-official status with Catalan and Spanish in the Val d'Aran.
Although not considered an "official language" in the same way as Catalan, Spanish, and Aranese, Catalan Sign Language, with about 18,000 users in Catalonia, is granted official recognition and support: "The public authorities shall guarantee the use of Catalan sign language and conditions of equality for deaf people who choose to use this language, which shall be the subject of education, protection and respect."
Under the Franco dictatorship, Catalan was excluded from the public education system and all other official use, so that for example families were not allowed to officially register children with Catalan names. Although never completely banned, Catalan language publishing was severely restricted during the early 1940s, with only religious texts and small-run self-published texts being released. Some books were published clandestinely or circumvented the restrictions by showing publishing dates prior to 1936. This policy was changed in 1946, when unrestricted publishing in Catalan resumed.
Rural–urban migration originating in other parts of Spain also reduced the social use of Catalan in urban areas and increased the use of Spanish. Lately, a similar sociolinguistic phenomenon has occurred with foreign immigration. Catalan cultural activity increased in the 1960s and Catalan classes began thanks to the initiative of associations such as Òmnium Cultural.
After the end of Franco's dictatorship, the newly established self-governing democratic institutions in Catalonia embarked on a long-term language policy to increase the use of Catalan and has, since 1983, enforced laws which attempt to protect and extend the use of Catalan. This policy, known as the "linguistic normalisation" ("normalització lingüística" in Catalan, "normalización lingüística" in Spanish) has been supported by the vast majority of Catalan political parties through the last thirty years. Some groups consider these efforts a way to discourage the use of Spanish, whereas some others, including the Catalan government and the European Union consider the policies respectful, or even as an example which "should be disseminated throughout the Union".
Today, Catalan is the main language of the Catalan autonomous government and the other public institutions that fall under its jurisdiction. Basic public education is given in Catalan, except for two hours per week of Spanish medium instruction. Businesses are required to display all information (e.g. menus, posters) in Catalan under penalty of fines. There is no obligation to display this information in either Occitan or Spanish, although there is no restriction on doing so in these or other languages. The use of fines was introduced in a 1997 linguistic law that aims to increase the public use of Catalan and defend the rights of Catalan speakers.
The law ensures that both Catalan and Spanish – being official languages – can be used by the citizens without prejudice in all public and private activities, but primary education can only be taken in Catalan language. The Generalitat uses Catalan in its communications and notifications addressed to the general population, but citizens can also receive information from the Generalitat in Spanish if they so desire. Debates in the Catalan Parliament take place almost exclusively in Catalan and the Catalan public television broadcasts programs only in Catalan.
Due to the intense immigration which Spain in general and Catalonia in particular experienced in the first decade of the 21st century, many foreign languages are spoken in various cultural communities in Catalonia, of which Rif-Berber, Moroccan Arabic, Romanian and Urdu are the most common ones.
Recently, some of these policies have been criticised for trying to promote Catalan by imposing fines on businesses. For example, following the passage of a March 2010 law on Catalan cinema, which establishes that half of the movies shown in Catalan cinemas must be in Catalan, a general strike of 75% of the cinemas took place. These criticisms mostly come from outside Catalonia, especially from conservative, conservative liberal and classical liberal circles of Spanish society. In Catalonia, on the other hand, there is a high social and political consensus on the language policies favoring Catalan, also among Spanish speakers and speakers of other languages. The US government, based on its Human Rights Report, questions the linguistic law and reports irregularities of the rights of the Spanish speakers in Catalonia. On the other hand, such organisations as Plataforma per la Llengua reported different violations of the linguistic rights of the Catalan speakers in Catalonia and the other Catalan-speaking territories in Spain, most of them caused by the institutions of the Spanish government in these territories.
In Catalonia, the Catalan language policy has been challenged by some Catalan intellectuals like Albert Boadella. Since 2006, the liberal Citizens - Party of the Citizenry, currently the main opposition party, has been one of the most consistent critics of the Catalan language policy within Catalonia. The local Catalan branch of the People's Party has a more ambiguous position on the issue: on one hand, it demands a bilingual Catalan–Spanish education and a more balanced language policy that would defend Catalan without favoring it over Spanish, whereas on the other hand, a few local PP politicians have supported in their municipalities measures privileging Catalan over Spanish and it has defended some aspects of the official language policies, sometimes against the positions of its colleagues from other parts of Spain.
Culture.
Art and architecture.
Catalonia has given the world many important figures in the area of the art. Catalan painters internationally known are Salvador Dalí, Joan Miró and Antoni Tàpies. Closely linked with the Catalan pictorial atmosphere, Pablo Picasso lived in Barcelona during his youth, training them as an artist and creating the movement of cubism. Other important artists are Ramon Casas, Josep Maria Subirachs and Marià Fortuny. The most important painting museums of Catalonia are the Teatre-Museu Dalí, Picasso Museum, Fundació Antoni Tàpies, Joan Miró Foundation, the National Art Museum of Catalonia (MNAC), the Barcelona Museum of Contemporary Art (MACBA), the Centre for Contemporary Culture in Barcelona (CCCB) and the CaixaFòrum.
In the area of architecture were developed and adapted to Catalonia different artistic styles prevalent in Europe, leaving footprints in many churches, monasteries and cathedrals, of Romanesque (the best examples of which are located in the northern half of the territory) and Gothic styles. There are some examples of Renaissance architecture, Baroque and Neoclassical. Modernism (Art Noveau) in the late nineteenth century appears as the national art. The world-renowned Catalan architects of this style are Antoni Gaudí, Lluís Domènech i Montaner and Josep Puig i Cadafalch. In the field of architectural rationalism, highlighting Josep Lluís Sert and Torres Clavé.
Monuments and World Heritage Sites.
There are several UNESCO World Heritage Sites in Catalonia:
Literature.
There are two historical moments of splendor of Catalan literature. The first begins with the historiography chronicles of the thirteenth century and the subsequent Golden Age of the fourteenth century. The second moment of splendor began in the nineteenth century with the cultural and political "Renaixença" (Renaissance) represented by writers and poets such as Jacint Verdaguer, Narcís Oller, Joan Maragall and Àngel Guimerà. During the twentieth century were developed the avant-garde movements represented by Josep Carner, Carles Riba, J.V. Foix and others. During the Civil War and the Francoist period the most prominent authors were Josep Pla, Mercè Rodoreda and Salvador Espriu.
After the transition to democracy (1975–1978) and the restoration of the Generalitat (1980), literary life and the editorial market have returned to normality and literary production in Catalan is being bolstered with a number of language policies intended to protect Catalan culture. Besides the aforementioned authors, other relevant 20th-century writers of the Francoist and democracy periods include Joan Brossa, Agustí Bartra, Manuel de Pedrolo, Pere Calders or Quim Monzó. 
Music and dance.
The sardana is considered the most characteristic Catalan popular dance, other groups also practice "Ball de bastons", "moixiganga", galops or "jota" in the southern part. The "Havaneres" are characteristic in some marine localities of the Costa Brava, especially during the summer months when these songs are sung outdoors accompanied by a "cremat" of burned rum. Other music styles born during the 20th century are Catalan rumba, Catalan rock and Nova Cançó.
Media.
Catalonia is the autonomous community, along with Madrid, with more media, both in Catalan and Spanish.
Televisió de Catalunya, which broadcasts entirely in Catalan language, is the main Catalan public TV. It has five channels: TV3, Canal 33, 324, Esport3 and Canal Super3. TV3 compete in audience with the State televisions that broadcast in Catalonia in Spanish language: Televisión Española (with few emissions in Catalan), Tele 5, Antena 3, Cuatro i La Sexta. Other smaller television audience, with remarkable presence are 8TV, television broadcast of the private Grup Godó, Canal Català TV, Barça TV and the local televisions, the greatest exponent of which is Barcelona TV, that also broadcast in Catalan.
The two main Catalan newspapers of general information are "El Periódico de Catalunya" and "La Vanguardia", both with editions in Catalan and Spanish. There are also some important newspapers published only in Catalan, "Ara" and "El Punt Avui", As well as most part of the local press. The Spanish newspapers, such as El País and El Mundo, can be also acquired.
The public Catalunya Ràdio and the private RAC 1 (Grup Godó) are the two main radios of Catalonia, both in Catalan.
Philosophy.
"Seny" is a form of ancestral Catalan wisdom or sensibleness. It involves well-pondered perception of situations, level-headedness, awareness, integrity, and right action. Many Catalans consider seny something unique to their culture, is based on a set of ancestral local customs stemming from the scale of values and social norms of their society.
Sport.
Sport has an important incidence in Catalan life and culture since the beginning of the 20th century and, as a result, it has a well developed sport infraestructure. The main sports are football, basketball, handball, rink hockey, tennis and motorsport. Despiste the fact that the most popular sports are represented outside by the Spanish national teams, Catalonia can officially play as itself in some others. The Catalan Football Federation also periodically fields a national team against international opposition, organizing friendly matches.
The biggest football clubs are FC Barcelona, which has won 5 European Champions leagues, 4 UEFA Cup Winners' Cups, and RCD Espanyol, which has been twice runner-up of the UEFA Cup. Both play in La Liga.
Catalonia hosted many relevant international sport events, such as the 1992 Summer Olympics in Barcelona, and also the 1955 Mediterranean Games or the 2013 World Aquatics Championships.
Symbols.
Catalonia has its own representative and distinctive symbols such as:
Festivals and public holidays.
Castells are one of the main manifestations of Catalan popular culture. The activity consists in constructing human towers by competing "colles castelleres" (teams). This practice originated in the southern part of Catalonia, mainly on the regions of Penedès and the Camp de Tarragona, during the 18th century, and later it was extended along the next two centuries to the rest of the territory. The tradition of els Castells i els Castellers was declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO in 2010.
In the greater celebrations other elements of the Catalan popular culture are usually present: the parades of "gegants" (giants) and "correfocs" of devils and firecrackers. Another traditional celebration in Catalonia is "La Patum de Berga", declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO on 25 November 2005.
In addition to traditional local Catalan culture, traditions from other parts of Spain can be found as a result of migration from other regions, for instance the celebration of the Andalusian Feria de Abril in Catalonia.
On 28 July 2010, Catalonia became the second Spanish territory, after the Canary Islands, to forbid bullfighting. The ban, which went into effect on 1 January 2012, had originated in a popular petition supported by over 180,000 signatures.

</doc>
<doc id="6823" url="https://en.wikipedia.org/wiki?curid=6823" title="Constantine Kanaris">
Constantine Kanaris

Constantine Kanaris or Canaris (; 1793 or 1795September 2, 1877) was a Greek Prime Minister, admiral and politician who in his youth was a freedom fighter in the Greek War of Independence.
Early life.
He was born and grew up on the island of Psara, close to the island of Chios, in the Aegean. His exact year of birth is unknown. The official records of the Hellenic Navy indicate 1795 but modern Greek historians believe that 1793 is more probable.
Constantine was left an orphan at a young age. Having to support himself, he chose to become a seaman like most members of his family since the beginning of the 18th century. He was hired as a boy on the brig of his uncle Dimitris Bourekas.
Military career.
Constantine gained his fame during the Greek War of Independence (1821–1829). Unlike most other prominent figures of the War, he had never been initiated into the Filiki Eteria (Friendly Society), which played a significant role in the revolution against the Ottoman Empire, primarily by secret recruitment of supporters against the Empire.
By early 1821, it had gained enough support to declare a revolution. This declaration seems to have surprised Constantine, who was absent at Odessa. He returned to Psara in haste and was there when the island joined the Revolution on April 10, 1821.
The island formed its own fleet of ships and the famed seamen of Psara, already known for their successful naval combats against pirates and their well-equipped ships, proved to be effective at full naval war. Constantine soon distinguished himself as a fire ship captain.
At Chios, on the moonless night of June 6/June 7, 1822 forces under his command destroyed the flagship of the Turkish admiral Nasuhzade Ali Pasha (or Kara-Ali Pasha) in revenge for the Chios Massacre. The admiral was holding a celebration (Bayram), so Kanaris and his men managed to place a fire ship next to it without being noticed. When the flagship's powder store caught fire, all men aboard were instantly killed. The Ottoman casualties comprised 2000 men, both naval officers and common sailors, as well as Kara-Ali himself.
Constantine led further successful attacks against the Turkish fleet, at Tenedos in November 1822 and at Samos in August 1824. He was famously said to have encouraged himself by murmuring ""Konstantí, you are going to die"" every time he was approaching a Turkish warship on the fire boat he was about to detonate.
Egypt was technically a province of the Ottoman Empire at the time but its viceroy Mohammad Ali (1769–1849), had earned enough power to act independently from the Sultan and had formed his own army and naval fleet. It was headed by his adoptive son Ibrahim Pasha (1789–1848). The latter had hired a number of veteran French officers - who had served under Emperor Napoleon I and were discharged from the French army following his defeat - to help organise the new army. By 1824, it counted 100,000 men and was both better organised and better equipped than the Sultan's army.
Sultan Mahmud II offered to the viceroy the command of Crete, if he agreed to send part of this army against the Greeks. They quickly reached an agreement. The Egyptian army, under the personal command of Ibrahim Pasha, started a campaign in both land and sea against the Greeks.
The Turkish fleet captured Psara on June 21, 1824. A part of the population managed to flee the island, but those who didn't were either sold into slavery or slaughtered. The island was deserted and surviving islanders were scattered through what is now Southern Greece (see Destruction of Psara).
After the destruction of his home island, Constantine continued to lead his men into attacks against the Turks, until the Battle of Navarino of October 20, 1827. Then the Turkish-Egyptian fleet was destroyed by the combined naval forces of Britain, France and Russia.
Following the end of the war and the independence of Greece, Constantine became an officer of the new Greek Navy, reaching the rank of Admiral, and later became a prominent politician.
Political career.
Constantine Kanaris was one of the few with the personal confidence of Ioannis Kapodistrias the first Head of State of independent Greece. Kanaris served as Minister in various governments and then as Prime Minister, in the provisional government, from March 11-April 11, 1844. He served a second term (October 27, 1848December 24, 1849), and as Navy Minister in Mavrokordatos' 1854 cabinet.
In 1862, he was one of the few War of Independence veterans that helped in the bloodless revolution that deposed King Otto of Greece and put Prince William of Denmark on the Greek throne as King George I of Greece. Under George I, he served as a prime minister for a third term (March 17April 28, 1864), fourth term (August 7, 1864February 9, 1865) and fifth and last term (June 7September 14, 1877).
Kanaris died on 2 September 1877 whilst still serving in office as Prime Minister. Following his death his government remained in power until September 14, 1877 without agreeing on a replacement at its head. He was buried in the First Cemetery of Athens, where most Greek prime ministers and celebrated figures are also buried. After his death he was honored as a national hero.
To honour Kanaris, three ships of the Hellenic Navy have been named after him;
Family.
In 1817, he married Despina Maniatis, from a historical family of Psara. They had seven children:
Wilhelm Canaris, a German Admiral, speculated that he might be a descendant of Constantine Kanaris. An official genealogical family history that was researched in 1938 showed that he was unrelated and that his family was from Italy.

</doc>
<doc id="6824" url="https://en.wikipedia.org/wiki?curid=6824" title="Carl Sagan">
Carl Sagan

Carl Edward Sagan (; November 9, 1934 – December 20, 1996) was an American astronomer, cosmologist, astrophysicist, astrobiologist, author, science popularizer, and science communicator in astronomy and other natural sciences. He is best known for his contributions to the scientific research of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation. Sagan assembled the first physical messages sent into space: the Pioneer plaque and the Voyager Golden Record, universal messages that could potentially be understood by any extraterrestrial intelligence that might find them. Sagan argued the now accepted hypothesis that the high surface temperatures of Venus can be attributed to and calculated using the greenhouse effect.
He published more than 600 scientific papers and articles and was author, co-author or editor of more than 20 books. Sagan wrote many popular science books, such as "The Dragons of Eden", "Broca's Brain" and "Pale Blue Dot", and narrated and co-wrote the award-winning 1980 television series "". The most widely watched series in the history of American public television, "Cosmos" has been seen by at least 500 million people across 60 different countries. The book "Cosmos" was published to accompany the series. He also wrote the science fiction novel "Contact", the basis for a 1997 film of the same name. His papers, containing 595,000 items, are archived at The Library of Congress.
Sagan always advocated scientific skeptical inquiry and the scientific method, pioneered exobiology and promoted the Search for Extra-Terrestrial Intelligence (SETI). He spent most of his career as a professor of astronomy at Cornell University, where he directed the Laboratory for Planetary Studies. Sagan and his works received numerous awards and honors, including the NASA Distinguished Public Service Medal, the National Academy of Sciences Public Welfare Medal, the Pulitzer Prize for General Non-Fiction for his book "The Dragons of Eden", and, regarding "Cosmos: A Personal Voyage", two Emmy Awards, the Peabody Award and the Hugo Award. He married three times and had five children. After suffering from myelodysplasia, Sagan died of pneumonia at the age of 62, on December 20, 1996.
Early life.
Carl Sagan was born in Brooklyn, New York. His father, Samuel Sagan, was an immigrant garment worker from Kamianets-Podilskyi, then Russian Empire, in today's Ukraine. His mother, Rachel Molly Gruber, was a housewife from New York. Carl was named in honor of Rachel's biological mother, Chaiya Clara, in Sagan's words, "the mother she never knew."
He had a sister, Carol, and the family lived in a modest apartment near the Atlantic Ocean, in Bensonhurst, a Brooklyn neighborhood. According to Sagan, they were Reform Jews, the most liberal of North American Judaism's four main groups. Both Sagan and his sister agreed that their father was not especially religious, but that their mother "definitely believed in God, and was active in the temple ... and served only kosher meat." During the depths of the Depression, his father worked as a theater usher.
According to biographer Keay Davidson, Sagan's "inner war" was a result of his close relationship with both of his parents, who were in many ways "opposites." Sagan traced his later analytical urges to his mother, a woman who had been extremely poor as a child in New York City during World War I and the 1920s. As a young woman she had held her own intellectual ambitions, but they were frustrated by social restrictions: her poverty, her status as a woman and a wife, and her Jewish ethnicity. Davidson notes that she therefore "worshipped her only son, Carl. He would fulfill her unfulfilled dreams."
However, he claimed that his sense of wonder came from his father. In his free time he gave apples to the poor or helped soothe labor-management tensions within New York's garment industry. Although he was awed by Carl's intellectual abilities, he took his son's inquisitiveness in stride and saw it as part of his growing up. In his later years as a writer and scientist, Sagan would often draw on his childhood memories to illustrate scientific points, as he did in his book, "Shadows of Forgotten Ancestors". Sagan describes his parents' influence on his later thinking:
1939 World's Fair.
Sagan recalls that one of his most defining moments was when his parents took him to the 1939 New York World's Fair when he was four years old. The exhibits became a turning point in his life. He later recalled the moving map of the "America of Tomorrow" exhibit: "It showed beautiful highways and cloverleaves and little General Motors cars all carrying people to skyscrapers, buildings with lovely spires, flying buttresses—and it looked great!" At other exhibits, he remembered how a flashlight that shone on a photoelectric cell created a crackling sound, and how the sound from a tuning fork became a wave on an oscilloscope. He also witnessed the future media technology that would replace radio: television. Sagan wrote:
He also saw one of the Fair's most publicized events, the burial of a time capsule at Flushing Meadows, which contained mementos of the 1930s to be recovered by Earth's descendants in a future millennium. "The time capsule thrilled Carl," writes Davidson. As an adult, Sagan and his colleagues would create similar time capsules—capsules that would be sent out into the galaxy; these were the Pioneer plaque and the "Voyager Golden Record" records, all of which were spinoffs of Sagan's memories of the World's Fair.
World War II.
During World War II Sagan's family worried about the fate of their European relatives. Sagan, however, was generally unaware of the details of the ongoing war. He wrote, "Sure, we had relatives who were caught up in the Holocaust. Hitler was not a popular fellow in our household... But on the other hand, I was fairly insulated from the horrors of the war." His sister, Carol, said that their mother "above all wanted to protect Carl... She had an extraordinarily difficult time dealing with World War II and the Holocaust." Sagan's book, "The Demon-Haunted World" (1996), included his memories of this conflicted period, when his family dealt with the realities of the war in Europe but tried to prevent it from undermining his optimistic spirit.
Inquisitiveness about nature.
Soon after entering elementary school he began to express a strong inquisitiveness about nature. Sagan recalled taking his first trips to the public library alone, at the age of five, when his mother got him a library card. He wanted to learn what stars were, since none of his friends or their parents could give him a clear answer:
At about age six or seven, he and a close friend took trips to the American Museum of Natural History across the East River in Manhattan. While there, they went to the Hayden Planetarium and walked around the museum's exhibits of space objects, such as meteorites, and displays of dinosaurs and animals in natural settings. Sagan writes about those visits:
His parents helped nurture his growing interest in science by buying him chemistry sets and reading materials. His interest in space, however, was his primary focus, especially after reading science fiction stories by writers such as H. G. Wells and Edgar Rice Burroughs, which stirred his imagination about life on other planets such as Mars. According to biographer Ray Spangenburg, these early years as Sagan tried to understand the mysteries of the planets became a "driving force in his life, a continual spark to his intellect, and a quest that would never be forgotten."
In 1947 he discovered "Astounding Science Fiction" magazine, which introduced him to more hard science fiction speculations than those in Burroughs's novels. That same year inaugurated the "flying saucer" mass hysteria with the young Carl suspecting the "discs" might be alien spaceships.
High school years.
Sagan had lived in Bensonhurst where he went to David A. Boody Junior High School. He had his bar mitzvah in Bensonhurst when he turned 13. The following year, 1948, his family moved to the nearby town of Rahway, New Jersey for his father's work, where Sagan then entered Rahway High School. He graduated in 1951. Rahway was an older industrial town, and the Sagans were among its few Jewish families.
Sagan was a straight-A student but was bored due to unchallenging classes and uninspiring teachers. His teachers realized this and tried to convince his parents to send him to a private school, the administrator telling them, "This kid ought to go to a school for gifted children, he has something really remarkable." This they couldn't do, partly because of the cost.
Sagan was made president of the school's chemistry club, and at home he set up his own laboratory. He taught himself about molecules by making cardboard cutouts to help him visualize how molecules were formed: "I found that about as interesting as doing experiments," he said. Sagan remained mostly interested in astronomy as a hobby, and in his junior year made it a career goal after he learned that astronomers were paid for doing what he always enjoyed: "That was a splendid day­­­­—when I began to suspect that if I tried hard I could do astronomy full-time, not just part-time."
Education and scientific career.
He attended the University of Chicago, where he participated in the Ryerson Astronomical Society, received a B.A. degree in self-proclaimed "nothing" with general and special honors in 1954, a B.S. degree in physics in 1955, and an M.S. degree in physics in 1956, before earning a Ph.D. degree in 1960 with the dissertation "Physical Studies of Planets" submitted to the Department of Astronomy and Astrophysics. The title of Sagan's thesis reflects his shared interests with Gerard Kuiper, his dissertation director, who throughout the 1950s had been president of the International Astronomical Union's commission on "Physical Studies of Planets and Satellites". In 1958, the two worked on the classified military Project A119, the secret Air Force plan to detonate a nuclear warhead on the Moon.
During his time as an honors program undergraduate, Sagan worked in the laboratory of the geneticist H. J. Muller and wrote a thesis on the origins of life with physical chemist H. C. Urey. He used the summer months of his graduate studies to work with planetary scientist Gerard Kuiper, physicist George Gamow, and chemist Melvin Calvin. From 1960 to 1962 Sagan was a Miller Fellow at the University of California, Berkeley. From 1962 to 1968, he worked at the Smithsonian Astrophysical Observatory in Cambridge, Massachusetts. At the same time, he worked with geneticist Joshua Lederberg.
Sagan lectured and did research at Harvard University until 1968, when he moved to Cornell University in Ithaca, New York, after being denied tenure at Harvard. It has been suggested that Sagan was denied tenure in part because of his publicized scientific advocacy, which some scientists perceived as being self-promotion; Harold Urey wrote a letter to the tenure committee recommending against tenure for Sagan. He became a full professor at Cornell in 1971, and directed the Laboratory for Planetary Studies there. From 1972 to 1981, Sagan was associate director of the Center for Radiophysics and Space Research (CRSR) at Cornell.
Sagan was associated with the U.S. space program from its inception. From the 1950s onward, he worked as an advisor to NASA, where one of his duties included briefing the Apollo astronauts before their flights to the Moon. Sagan contributed to many of the robotic spacecraft missions that explored the Solar System, arranging experiments on many of the expeditions. He conceived the idea of adding an unalterable and universal message on spacecraft destined to leave the Solar System that could potentially be understood by any extraterrestrial intelligence that might find it. Sagan assembled the first physical message that was sent into space: a gold-anodized plaque, attached to the space probe Pioneer 10, launched in 1972. Pioneer 11, also carrying another copy of the plaque, was launched the following year. He continued to refine his designs; the most elaborate message he helped to develop and assemble was the Voyager Golden Record that was sent out with the Voyager space probes in 1977. Sagan often challenged the decisions to fund the Space Shuttle and the International Space Station at the expense of further robotic missions.
Sagan taught a course on critical thinking at Cornell University, until he died in 1996 from pneumonia, a few months after learning that he was in remission of myelodysplastic syndrome.
Scientific achievements.
Former student David Morrison describes Sagan as "an 'idea person' and a master of intuitive physical arguments and 'back of the envelope' calculations," and Gerard Kuiper said that "Some persons work best in specializing on a major program in the laboratory; others are best in liaison between sciences. Dr. Sagan belongs in the latter group."
Sagan's contributions were central to the discovery of the high surface temperatures of the planet Venus. In the early 1960s no one knew for certain the basic conditions of that planet's surface, and Sagan listed the possibilities in a report later depicted for popularization in a Time–Life book, "Planets". His own view was that Venus was dry and very hot as opposed to the balmy paradise others had imagined. He had investigated radio emissions from Venus and concluded that there was a surface temperature of . As a visiting scientist to NASA's Jet Propulsion Laboratory, he contributed to the first Mariner missions to Venus, working on the design and management of the project. Mariner 2 confirmed his conclusions on the surface conditions of Venus in 1962.
Sagan was among the first to hypothesize that Saturn's moon Titan might possess oceans of liquid compounds on its surface and that Jupiter's moon Europa might possess subsurface oceans of water. This would make Europa potentially habitable. Europa's subsurface ocean of water was later indirectly confirmed by the spacecraft "Galileo". The mystery of Titan's reddish haze was also solved with Sagan's help. The reddish haze was revealed to be due to complex organic molecules constantly raining down onto Titan's surface.
He further contributed insights regarding the atmospheres of Venus and Jupiter as well as seasonal changes on Mars. He also perceived global warming as a growing, man-made danger and likened it to the natural development of Venus into a hot, life-hostile planet through a kind of runaway greenhouse effect. Sagan and his Cornell colleague Edwin Ernest Salpeter speculated about life in Jupiter's clouds, given the planet's dense atmospheric composition rich in organic molecules. He studied the observed color variations on Mars' surface and concluded that they were not seasonal or vegetational changes as most believed but shifts in surface dust caused by windstorms.
Sagan is best known, however, for his research on the possibilities of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation.
He is also the 1994 recipient of the Public Welfare Medal, the highest award of the National Academy of Sciences for "distinguished contributions in the application of science to the public welfare". He was denied membership in the Academy, reportedly because his media activities made him unpopular with many other scientists.
Scientific and critical thinking advocacy.
Sagan's ability to convey his ideas allowed many people to understand the cosmos better—simultaneously emphasizing the value and worthiness of the human race, and the relative insignificance of the Earth in comparison to the Universe. He delivered the 1977 series of Royal Institution Christmas Lectures in London. He hosted and, with Ann Druyan, co-wrote and co-produced the highly popular thirteen-part Public Broadcasting Service (PBS) television series "".
"Cosmos" covered a wide range of scientific subjects including the origin of life and a perspective of humans' place in the Universe. The series was first broadcast by PBS in 1980, winning an Emmy and a Peabody Award. It has been broadcast in more than 60 countries and seen by over 500 million people, making it the most widely watched PBS program in history. In addition, "Time" magazine ran a cover story about Sagan soon after the show broadcast, referring to him as "creator, chief writer and host-narrator of the new public television series Cosmos, takes the controls of his fantasy spaceship". However, Sagan was criticized for putting too much attention into the series, with several of his classes at Cornell being cancelled and complaints from his colleagues.
Sagan was a proponent of the search for extraterrestrial life. He urged the scientific community to listen with radio telescopes for signals from potential intelligent extraterrestrial life-forms. Sagan was so persuasive that by 1982 he was able to get a petition advocating SETI published in the journal "Science" and signed by 70 scientists, including seven Nobel Prize winners. This signaled a tremendous increase in the respectability of a then-controversial field. Sagan also helped Frank Drake write the Arecibo message, a radio message beamed into space from the Arecibo radio telescope on November 16, 1974, aimed at informing potential extraterrestrials about Earth.
Sagan was chief technology officer of the professional planetary research journal "Icarus" for twelve years. He co-founded The Planetary Society, the largest space-interest group in the world, with over 100,000 members in more than 149 countries, and was a member of the SETI Institute Board of Trustees. Sagan served as Chairman of the Division for Planetary Science of the American Astronomical Society, as President of the Planetology Section of the American Geophysical Union, and as Chairman of the Astronomy Section of the American Association for the Advancement of Science (AAAS).
At the height of the Cold War, Sagan became involved in public awareness efforts for the effects of nuclear war when "Twilight at Noon", a 1982 mathematical climate model, suggested that a substantial nuclear exchange could trigger a nuclear twilight and upset the delicate balance of life on Earth by cooling the surface. In 1983 he was one of five authors—the "S"—in the follow-up "TTAPS" report (as the research paper came to be known), which contained the first usage of the term "nuclear winter" which his colleague Richard P. Turco coined. In 1984 he co-authored the book "" and in 1990 he co-authored the book "A Path Where No Man Thought: Nuclear Winter and the End of the Arms Race", which explains the nuclear winter hypothesis and with that advocates nuclear disarmament.
Sagan also wrote books to popularize science, such as "Cosmos", which reflected and expanded upon some of the themes of "A Personal Voyage" and became the best-selling science book ever published in English; "The Dragons of Eden: Speculations on the Evolution of Human Intelligence", which won a Pulitzer Prize; and "Broca's Brain: Reflections on the Romance of Science". Sagan also wrote the best-selling science fiction novel "Contact" in 1985, based on a film treatment he wrote with his wife in 1979, but he did not live to see the book's 1997 motion picture adaptation, which starred Jodie Foster and won the 1998 Hugo Award for Best Dramatic Presentation.
He wrote a sequel to "Cosmos", "Pale Blue Dot: A Vision of the Human Future in Space", which was selected as a notable book of 1995 by "The New York Times". He appeared on PBS' "Charlie Rose" program in January 1995. Sagan also wrote the introduction for Stephen Hawking's bestseller, "A Brief History of Time". Sagan was also known for his popularization of science, his efforts to increase scientific understanding among the general public, and his positions in favor of scientific skepticism and against pseudoscience, such as his debunking of the Betty and Barney Hill abduction. To mark the tenth anniversary of Sagan's death, David Morrison, a former student of Sagan's, recalled "Sagan's immense contributions to planetary research, the public understanding of science, and the skeptical movement" in "Skeptical Inquirer".
Following Saddam Hussein's threats to light Kuwait's oil wells on fire in response to any physical challenge to Iraqi control of the oil assets, Sagan and his "TTAPS" colleagues warned in January 1991 in the Baltimore Sun and Wilmington Morning Star newspapers that if the fires were left to burn over a period of several months, enough smoke from the 600 or so 1991 Kuwaiti oil fires "might get so high as to disrupt agriculture in much of South Asia ..." and that this possibility should "affect the war plans"; these claims were also the subject of a televised debate between Sagan and physicist Fred Singer on 22 January, aired on the ABC News program "Nightline". In the televised debate, Sagan argued that the effects of the smoke would be similar to the effects of a nuclear winter, with Singer arguing to the contrary. After the debate, the fires burnt for many months before extinguishing efforts were complete. The results of the smoke did not produce continental-sized cooling. Sagan later conceded in "The Demon-Haunted World" that the prediction did not turn out to be correct: "it "was" pitch black at noon and temperatures dropped 4°–6 °C over the Persian Gulf, but not much smoke reached stratospheric altitudes and Asia was spared".
In his later years Sagan advocated the creation of an organized search for asteroids/near-Earth objects (NEO) that might impact the Earth, and to postpone developing the technology to defend against them. He argued that the nuclear detonation, along with the other methods of deflection proposed as a means to alter the orbit of an asteroid, created a Deflection Dilemma: if the ability to deflect an asteroid away from the Earth exists, then one would also have the ability to divert a non-threatening object towards Earth, creating an immensely destructive weapon. In a 1994 paper, he co-authored, he ridiculed a 3-day long "Near-Earth Object Interception Workshop" held by LANL in 1993 that did not, "even in passing" state that such interception and deflection technologies could have these "ancillary dangers".
Sagan however was hopeful that the natural impact threat and the intrinsically double edged essence of the methods to prevent these threats, would serve as a "new and potent motivation to maturing international relations". Later acknowledging that, with sufficient international oversight, in the future a "work our way up" approach to fielding nuclear explosive deflection methods could be fielded, and when sufficient knowledge was gained, to use them to aid in mining asteroids. His interest in the use of nuclear detonations in space grew out of his work in 1958 for the Armour Research Foundation's Project A119, concerning the possibility of detonating a nuclear device on the Lunar surface.
Sagan was a critic of Plato, having said of the ancient Greek philosopher: "Science and mathematics were to be removed from the hands of the merchants and the artisans. This tendency found its most effective advocate in a follower of Pythagoras named Plato" and "He (Plato) believed that ideas were far more real than the natural world. He advised the astronomers not to waste their time observing the stars and planets. It was better, he believed, just to think about them. Plato expressed hostility to observation and experiment. He taught contempt for the real world and disdain for the practical application of scientific knowledge. Plato's followers succeeded in extinguishing the light of science and experiment that had been kindled by Democritus and the other Ionians."
Popularizing science.
Speaking about his activities in popularizing science, Sagan said that there were at least two reasons for scientists to share the purposes of science and its contemporary state. Simple self-interest was one: much of the funding for science came from the public, and the public therefore had the right to know how the money was being spent. If scientists increased public admiration for science, there was a good chance of having more public supporters. The other reason was the excitement of communicating one's own excitement about science to others.
"Billions and billions".
From "Cosmos" and his frequent appearances on "The Tonight Show Starring Johnny Carson", Sagan became associated with the catchphrase "billions and billions". Sagan said that he never actually used the phrase in the "Cosmos" series. The closest that he ever came was in the book "Cosmos", where he talked of "billions "upon" billions":(Richard Feynman, a precursor to Sagan, is observed to use the phrase "billions and billions" many times in his "red books".) However, Sagan's frequent use of the word "billions", and distinctive delivery emphasizing the "b" (which he did intentionally, in place of more cumbersome alternatives such as "billions with a 'b, in order to distinguish the word from "millions"), made him a favorite target of comic performers, including Johnny Carson, Gary Kroeger, Mike Myers, Bronson Pinchot, Penn Jillette, Harry Shearer, and others. Frank Zappa satirized the line in the song "Be in My Video", noting as well "atomic light". Sagan took this all in good humor, and his final book was entitled "Billions and Billions", which opened with a tongue-in-cheek discussion of this catchphrase, observing that Carson was an amateur astronomer and that Carson's comic caricature often included real science.
He is also known for expressing wonderment at the vastness of space and time, as in his phrase "The total number of stars in the Universe is larger than all the grains of sand on all the beaches of the planet Earth."
As a humorous tribute to Sagan and his association with the catchphrase "billions and billions", a "sagan" has been defined as a unit of measurement equivalent to a very large number of anything.
Social concerns.
Sagan believed that the Drake equation, on substitution of reasonable estimates, suggested that a large number of extraterrestrial civilizations would form, but that the lack of evidence of such civilizations highlighted by the Fermi paradox suggests technological civilizations tend to self-destruct. This stimulated his interest in identifying and publicizing ways that humanity could destroy itself, with the hope of avoiding such a cataclysm and eventually becoming a spacefaring species. Sagan's deep concern regarding the potential destruction of human civilization in a nuclear holocaust was conveyed in a memorable cinematic sequence in the final episode of "Cosmos", called "Who Speaks for Earth?" Sagan had already resigned from the Air Force Scientific Advisory Board's UFO investigating Condon Committee and voluntarily surrendered his top secret clearance in protest over the Vietnam War. Following his marriage to his third wife (novelist Ann Druyan) in June 1981, Sagan became more politically active—particularly in opposing escalation of the nuclear arms race under President Ronald Reagan.
In March 1983, Reagan announced the Strategic Defense Initiative—a multibillion-dollar project to develop a comprehensive defense against attack by nuclear missiles, which was quickly dubbed the "Star Wars" program. Sagan spoke out against the project, arguing that it was technically impossible to develop a system with the level of perfection required, and far more expensive to build such a system than it would be for an enemy to defeat it through decoys and other means—and that its construction would seriously destabilize the "nuclear balance" between the United States and the Soviet Union, making further progress toward nuclear disarmament impossible.
When Soviet leader Mikhail Gorbachev declared a unilateral moratorium on the testing of nuclear weapons, which would begin on August 6, 1985—the 40th anniversary of the atomic bombing of Hiroshima—the Reagan administration dismissed the dramatic move as nothing more than propaganda, and refused to follow suit. In response, US anti-nuclear and peace activists staged a series of protest actions at the Nevada Test Site, beginning on Easter Sunday in 1986 and continuing through 1987. Hundreds of people in the "Nevada Desert Experience" group were arrested, including Sagan, who was arrested on two separate occasions as he climbed over a chain-link fence at the test site during the underground Operation Charioteer and United States's Musketeer nuclear test series of detonations.
Sagan was also a vocal advocate of the controversial notion of testosterone poisoning, arguing in 1992 that human males could become gripped by an "usually severe of testosterone poisoning" and this could compel them to become genocidal. In his review of Moondance magazine writer Daniela Gioseffi's 1990 book "Women on War", he argues that females are the only half of humanity "untainted by testosterone poisoning". One chapter of his 1993 book, "Shadows of Forgotten Ancestors" is dedicated to testosterone and its alleged poisonous effects.
Personal life and beliefs.
Sagan was married three times. In 1957, he married biologist Lynn Margulis, mother of Dorion Sagan and Jeremy Sagan. After Sagan and Margulis divorced, he married artist Linda Salzman in 1968, mother of Nick Sagan. During these marriages, Sagan focused heavily on his career, a factor which may have contributed to Sagan's first divorce. In 1981, Sagan married author Ann Druyan, mother of Alexandra Rachel (Sasha) Sagan and Samuel Democritus Sagan. Sagan and Druyan remained married until his death in 1996.
Isaac Asimov described Sagan as one of only two people he ever met whose intellect surpassed his own. The other, he claimed, was the computer scientist and artificial intelligence expert Marvin Minsky.
Sagan wrote frequently about religion and the relationship between religion and science, expressing his skepticism about the conventional conceptualization of God as a sapient being. For example: Some people think God is an outsized, light-skinned male with a long white beard, sitting on a throne somewhere up there in the sky, busily tallying the fall of every sparrow. Others—for example Baruch Spinoza and Albert Einstein—considered God to be essentially the sum total of the physical laws which describe the universe. I do not know of any compelling evidence for anthropomorphic patriarchs controlling human destiny from some hidden celestial vantage point, but it would be madness to deny the existence of physical laws.
In another description of his view on the concept of God, Sagan emphatically wrote: The idea that God is an oversized white male with a flowing beard who sits in the sky and tallies the fall of every sparrow is ludicrous. But if by God one means the set of physical laws that govern the universe, then clearly there is such a God. This God is emotionally unsatisfying ... it does not make much sense to pray to the law of gravity.
On atheism, Sagan commented in 1981: An atheist is someone who is certain that God does not exist, someone who has compelling evidence against the existence of God. I know of no such compelling evidence. Because God can be relegated to remote times and places and to ultimate causes, we would have to know a great deal more about the universe than we do now to be sure that no such God exists. To be certain of the existence of God and to be certain of the nonexistence of God seem to me to be the confident extremes in a subject so riddled with doubt and uncertainty as to inspire very little confidence indeed.
Sagan also commented on Christianity, stating "My long-time view about Christianity is that it represents an amalgam of two seemingly immiscible parts, the religion of Jesus and the religion of Paul. Thomas Jefferson attempted to excise the Pauline parts of the New Testament. There wasn't much left when he was done, but it was an inspiring document."
Regarding the relationship between spirituality and science, Sagan stated: "Science is not only compatible with spirituality; it is a profound source of spirituality. When we recognize our place in an immensity of light-years and in the passage of ages, when we grasp the intricacy, beauty, and subtlety of life, then that soaring feeling, that sense of elation and humility combined, is surely spiritual."
An environmental appeal, "Preserving and Cherishing the Earth", signed by Sagan with other noted scientists in January 1990, stated that "The historical record makes clear that religious teaching, example, and leadership are powerfully able to influence personal conduct and commitment... Thus, there is a vital role for religion and science."
In reply to a question in 1996 about his religious beliefs, Sagan answered, "I'm agnostic." Sagan maintained that the idea of a creator God of the Universe was difficult to prove or disprove and that the only conceivable scientific discovery that could challenge it would be an infinitely old universe. Sagan's views on religion have been interpreted as a form of pantheism comparable to Einstein's belief in Spinoza's God. His son, Dorion Sagan said, "My father believed in the God of Spinoza and Einstein, God not behind nature but as nature, equivalent to it." His last wife, Ann Druyan, stated: When my husband died, because he was so famous and known for not being a believer, many people would come up to me—it still sometimes happens—and ask me if Carl changed at the end and converted to a belief in an afterlife. They also frequently ask me if I think I will see him again. Carl faced his death with unflagging courage and never sought refuge in illusions. The tragedy was that we knew we would never see each other again. I don't ever expect to be reunited with Carl.
In 2006, Ann Druyan edited Sagan's 1985 Glasgow "Gifford Lectures in Natural Theology" into a book, "", in which he elaborates on his views of divinity in the natural world.
Sagan is also widely regarded as a freethinker or skeptic; one of his most famous quotations, in "Cosmos", was, "Extraordinary claims require extraordinary evidence" (called the "Sagan Standard" by some). This was based on a nearly identical statement by fellow founder of the Committee for the Scientific Investigation of Claims of the Paranormal, Marcello Truzzi, "An extraordinary claim requires extraordinary proof." This idea had been earlier aphorized in Théodore Flournoy's work "From India to the Planet Mars" (1899) from a longer quote by Pierre-Simon Laplace (1749–1827), a French mathematician and astronomer, as the Principle of Laplace: "The weight of the evidence should be proportioned to the strangeness of the facts."
Late in his life, Sagan's books elaborated on his skeptical, naturalistic view of the world. In "The Demon-Haunted World", he presented tools for testing arguments and detecting fallacious or fraudulent ones, essentially advocating wide use of critical thinking and the scientific method. The compilation "Billions and Billions: Thoughts on Life and Death at the Brink of the Millennium", published in 1997 after Sagan's death, contains essays written by Sagan, such as his views on abortion, and his widow Ann Druyan's account of his death as a skeptic, agnostic, and freethinker.
Sagan warned against humans' tendency towards anthropocentrism. He was the faculty adviser for the Cornell Students for the Ethical Treatment of Animals. In the "Cosmos" chapter "Blues For a Red Planet", Sagan wrote, "If there is life on Mars, I believe we should do nothing with Mars. Mars then belongs to the Martians, even if the Martians are only microbes."
Sagan was a user and advocate of marijuana. Under the pseudonym "Mr. X", he contributed an essay about smoking cannabis to the 1971 book "Marihuana Reconsidered". The essay explained that marijuana use had helped to inspire some of Sagan's works and enhance sensual and intellectual experiences. After Sagan's death, his friend Lester Grinspoon disclosed this information to Sagan's biographer, Keay Davidson. The publishing of the biography, "Carl Sagan: A Life", in 1999 brought media attention to this aspect of Sagan's life. Not long after his death, widow Ann Druyan had gone on to preside over the board of directors of the National Organization for the Reform of Marijuana Laws (NORML), a non-profit organization dedicated to reforming cannabis laws.
In 1994, engineers at Apple Computer code-named the Power Macintosh 7100 "Carl Sagan" in the hope that Apple would make "billions and billions" with the sale of the PowerMac 7100. The name was only used internally, but Sagan was concerned that it would become a product endorsement and sent Apple a cease-and-desist letter. Apple complied, but engineers retaliated by changing the internal codename to "BHA" for “Butt-Head Astronomer”. Sagan then sued Apple for libel in federal court. The court granted Apple's motion to dismiss Sagan's claims and opined in dicta that a reader aware of the context would understand Apple was "clearly attempting to retaliate in a humorous and satirical way", and that “It strains reason to conclude that Defendant was attempting to criticize Plaintiff's reputation or competency as an astronomer. One does not seriously attack the expertise of a scientist using the undefined phrase ‘butt-head’.” Sagan then sued for Apple's original use of his name and likeness, but again lost. Sagan appealed the ruling. In November 1995, an out-of-court settlement was reached and Apple's office of trademarks and patents released a conciliatory statement that “Apple has always had great respect for Dr. Sagan. It was never Apple's intention to cause Dr. Sagan or his family any embarrassment or concern.” Apple's third and final code name for the project was "LAW", short for "Lawyers are Wimps".
Sagan briefly served as an adviser on Stanley Kubrick's film "". Sagan proposed that the film suggest, rather than depict, extraterrestrial superintelligence.
Sagan and UFOs.
In 1947, the year that inaugurated the "flying saucer" craze, the young Sagan suspected the "discs" might be alien spaceships.
Sagan's interest in UFO reports prompted him on August 3, 1952, to write a letter to U.S. Secretary of State Dean Acheson to ask how the United States would respond if flying saucers turned out to be extraterrestrial. He later had several conversations on the subject in 1964 with Jacques Vallée. Though quite skeptical of any extraordinary answer to the UFO question, Sagan thought scientists should study the phenomenon, at least because there was widespread public interest in UFO reports.
Stuart Appelle notes that Sagan "wrote frequently on what he perceived as the logical and empirical fallacies regarding UFOs and the abduction experience. Sagan rejected an extraterrestrial explanation for the phenomenon but felt there were both empirical and pedagogical benefits for examining UFO reports and that the subject was, therefore, a legitimate topic of study."
In 1966 Sagan was a member of the Ad Hoc Committee to Review Project Blue Book, the U.S. Air Force's UFO investigation project. The committee concluded Blue Book had been lacking as a scientific study, and recommended a university-based project to give the UFO phenomenon closer scientific scrutiny. The result was the Condon Committee (1966–68), led by physicist Edward Condon, and in their final report they formally concluded that UFOs, regardless of what any of them actually were, did not behave in a manner consistent with a threat to national security.
Sociologist Ron Westrum writes that "The high point of Sagan's treatment of the UFO question was the AAAS' symposium in 1969. A wide range of educated opinions on the subject were offered by participants, including not only proponents such as James McDonald and J. Allen Hynek but also skeptics like astronomers William Hartmann and Donald Menzel. The roster of speakers was balanced, and it is to Sagan's credit that this event was presented in spite of pressure from Edward Condon." With physicist Thornton Page, Sagan edited the lectures and discussions given at the symposium; these were published in 1972 as "UFO's: A Scientific Debate". Some of Sagan's many books examine UFOs (as did one episode of "Cosmos") and he claimed a religious undercurrent to the phenomenon.
Sagan again revealed his views on interstellar travel in his 1980 "Cosmos" series. In one of his last written works, Sagan argued that the chances of extraterrestrial spacecraft visiting Earth are vanishingly small. However, Sagan did think it plausible that Cold War concerns contributed to governments misleading their citizens about UFOs, and wrote that "some UFO reports and analyses, and perhaps voluminous files, have been made inaccessible to the public which pays the bills ... It's time for the files to be declassified and made generally available." He cautioned against jumping to conclusions about suppressed UFO data and stressed that there was no strong evidence that aliens were visiting the Earth either in the past or present.
Death.
After suffering from myelodysplasia for two years, and receiving three bone marrow transplants (the donor was his sister Cari), Sagan died of pneumonia at the age of 62 at the Fred Hutchinson Cancer Research Center in Seattle, Washington, in the early morning of December 20, 1996.
He was buried at Lakeview Cemetery in Ithaca, New York.
Posthumous recognition.
The 1997 movie "Contact", based on Sagan's novel of the same name and finished after his death, ends with the dedication "For Carl". His photo can also be seen at 59:23 in the film.
In 1997 the Sagan Planet Walk was opened in Ithaca, New York. It is a walking-scale model of the Solar System, extending 1.2 km from the center of The Commons in downtown Ithaca to the Sciencenter, a hands-on museum. The exhibition was created in memory of Carl Sagan, who was an Ithaca resident and Cornell Professor. Professor Sagan had been a founding member of the museum's advisory board.
The landing site of the unmanned Mars Pathfinder spacecraft was renamed the "Carl Sagan Memorial Station" on July 5, 1997. Asteroid 2709 Sagan is named in his honor, as is the Carl Sagan Institute for the search of habitable planets.
Sagan's son, Nick Sagan, wrote several episodes in the "Star Trek" franchise. In an episode of "" entitled "Terra Prime", a quick shot is shown of the relic rover "Sojourner", part of the Mars Pathfinder mission, placed by a historical marker at Carl Sagan Memorial Station on the Martian surface. The marker displays a quote from Sagan: "Whatever the reason you're on Mars, I'm glad you're there, and I wish I was with you." Sagan's student Steve Squyres led the team that landed the rovers "Spirit" and "Opportunity" successfully on Mars in 2004.
On November 9, 2001, on what would have been Sagan's 67th birthday, the Ames Research Center dedicated the site for the Carl Sagan Center for the Study of Life in the Cosmos. "Carl was an incredible visionary, and now his legacy can be preserved and advanced by a 21st century research and education laboratory committed to enhancing our understanding of life in the universe and furthering the cause of space exploration for all time", said NASA Administrator Daniel Goldin. Ann Druyan was at the Center as it opened its doors on October 22, 2006.
Sagan has at least three awards named in his honor:
In 2006, the Carl Sagan Medal was awarded to astrobiologist and author David Grinspoon, the son of Sagan's close friend Lester Grinspoon.
August 2007 the Independent Investigations Group (IIG) awarded Sagan posthumously a Lifetime Achievement Award. This honor has also been awarded to Harry Houdini and James Randi.
Beginning in 2009, a musical project known as Symphony of Science sampled several excerpts of Sagan from his series "Cosmos" and remixed them to electronic music. To date, the videos have received over 21 million views worldwide on YouTube.
The 2014 Swedish science fiction short film "Wanderers" uses excerpts of Sagan's narration of his book "Pale Blue Dot", played over digitally-created visuals of humanity's possible future expansion into outer space.
In February 2015, the Finnish-based symphonic metal band Nightwish released the song "Sagan" as a non-album bonus track for their single "Élan". The song, written by the band's songwriter/composer/keyboardist Tuomas Holopainen, is an homage to the life and work of the late Carl Sagan.
In August 2015, it was announced that a biopic of Sagan's life was being planned by Warner Bros.

</doc>
<doc id="6827" url="https://en.wikipedia.org/wiki?curid=6827" title="Cuban Missile Crisis">
Cuban Missile Crisis

The Cuban Missile Crisis, also known as the October Crisis (), the Caribbean Crisis (, tr. "Karibskiy krizis"), or the Missile Scare, was a 13-day (October 16–28, 1962) confrontation between the United States and the Soviet Union concerning Soviet ballistic missiles deployment in Cuba. Along with being televised worldwide, it was the closest the Cold War came to escalating into a full-scale nuclear war.
In response to the failed Bay of Pigs Invasion of 1961, and the presence of American Jupiter ballistic missiles in Italy and Turkey against the USSR with Moscow within range, Soviet leader Nikita Khrushchev decided to agree to Cuba's request to place nuclear missiles in Cuba to deter future harassment of Cuba. An agreement was reached during a secret meeting between Khrushchev and Fidel Castro in July and construction on a number of missile launch facilities started later that summer.
An election was underway in the U.S. The White House had denied charges that it was ignoring dangerous Soviet missiles 90 miles from Florida. These missile preparations were confirmed when an Air Force U-2 spy plane produced clear photographic evidence of medium-range (SS-4) and intermediate-range (R-14) ballistic missile facilities. The United States established a military blockade to prevent further missiles from entering Cuba. It announced that they would not permit offensive weapons to be delivered to Cuba and demanded that the weapons already in Cuba be dismantled and returned to the USSR.
After a long period of tense negotiations, an agreement was reached between Kennedy and Khrushchev. Publicly, the Soviets would dismantle their offensive weapons in Cuba and return them to the Soviet Union, subject to United Nations verification, in exchange for a U.S. public declaration and agreement never to invade Cuba without direct provocation. Secretly, the US also agreed that it would dismantle all U.S.-built Jupiter MRBMs, which were deployed in Turkey and Italy against the Soviet Union but were not known to the public.
When all offensive missiles and Ilyushin Il-28 light bombers had been withdrawn from Cuba, the blockade was formally ended on November 20, 1962. The negotiations between the United States and the Soviet Union pointed out the necessity of a quick, clear, and direct communication line between Washington and Moscow. As a result, the Moscow–Washington hotline was established. A series of agreements sharply reduced U.S.-Soviet tensions for the following years.
Earlier actions by the United States of America.
The United States was concerned about an expansion of Communism, and a Latin American country allying openly with the USSR was regarded as unacceptable, given the U.S.-Soviet enmity since the end of World War II. Such an involvement would also directly defy the Monroe Doctrine, a U.S. policy which, while limiting the United States' involvement with European colonies and European affairs, held that European powers ought not to have involvement with states in the Western Hemisphere.
The U.S. had been embarrassed publicly by the failed Bay of Pigs Invasion in April 1961, which had been launched under President John F. Kennedy by CIA-trained forces of Cuban exiles. Afterward, former President Eisenhower told Kennedy that "the failure of the Bay of Pigs will embolden the Soviets to do something that they would otherwise not do." The half-hearted invasion left Soviet premier Nikita Khrushchev and his advisers with the impression that Kennedy was indecisive and, as one Soviet adviser wrote, "too young, intellectual, not prepared well for decision making in crisis situations ... too intelligent and too weak." U.S. covert operations continued in 1961 with the unsuccessful Operation Mongoose.
In addition, Khrushchev's impression of Kennedy's weakness was confirmed by the President's soft response during the Berlin Crisis of 1961, particularly the building of the Berlin Wall. Speaking to Soviet officials in the aftermath of the crisis, Khrushchev asserted, "I know for certain that Kennedy doesn't have a strong background, nor, generally speaking, does he have the courage to stand up to a serious challenge." He also told his son Sergei that on Cuba, Kennedy "would make a fuss, make more of a fuss, and then agree."
In January 1962, General Edward Lansdale described plans to overthrow the Cuban Government in a top-secret report (partially declassified 1989), addressed to President Kennedy and officials involved with Operation Mongoose. CIA agents or "pathfinders" from the Special Activities Division were to be infiltrated into Cuba to carry out sabotage and organization, including radio broadcasts. In February 1962, the U.S. launched an embargo against Cuba, and Lansdale presented a 26-page, top-secret timetable for implementation of the overthrow of the Cuban Government, mandating that guerrilla operations begin in August and September, and in the first two weeks of October: "Open revolt and overthrow of the Communist regime."
Balance of power.
When Kennedy ran for president in 1960, one of his key election issues was an alleged "missile gap" with the Soviets leading. In fact, the U.S. led the Soviets by a wide margin that would only increase. In 1961, the Soviets had only four intercontinental ballistic missiles (R-7 Semyorka). By October 1962, they may have had a few dozen, although some intelligence estimates were as high as 75.
The U.S., on the other hand, had 170 ICBMs and was quickly building more. It also had eight "George Washington"– and "Ethan Allen"–class ballistic missile submarines with the capability to launch 16 Polaris missiles each, with a range of .
Khrushchev increased the perception of a missile gap when he loudly boasted to the world that the USSR was building missiles "like sausages" whose numbers and capabilities actually were nowhere close to his assertions. The Soviet Union did have medium-range ballistic missiles in quantity, about 700 of them; however, these were very unreliable and inaccurate. The U.S. had a considerable advantage in total number of nuclear warheads (27,000 against 3,600) at the time and in all the technologies needed to deliver them accurately.
The U.S. also led in missile defensive capabilities, naval and air power; but the USSR enjoyed a two-to-one advantage in conventional ground forces, more pronounced in field guns and tanks (particularly in the European theater).
Soviet deployment of missiles in Cuba (Operation Anadyr).
In May 1962, Soviet Premier Nikita Khrushchev was persuaded by the idea of countering the United States' growing lead in developing and deploying strategic missiles by placing Soviet intermediate-range nuclear missiles in Cuba, despite the misgivings of the Soviet Ambassador in Havana, Alexandr Ivanovich Alexeyev who argued that Castro would not accept the deployment of these missiles. Khrushchev faced a strategic situation where the U.S. was perceived to have a "splendid first strike" capability that put the Soviet Union at a huge disadvantage. In 1962, the Soviets had only 20 ICBMs capable of delivering nuclear warheads to the U.S. from inside the Soviet Union. The poor accuracy and reliability of these missiles raised serious doubts about their effectiveness. A newer, more reliable generation of ICBMs would only become operational after 1965. Therefore, Soviet nuclear capability in 1962 placed less emphasis on ICBMs than on medium and intermediate-range ballistic missiles (MRBMs and IRBMs). These missiles could hit American allies and most of Alaska from Soviet territory but not the contiguous 48 states of the U.S. Graham Allison, the director of Harvard University's Belfer Center for Science and International Affairs, points out, "The Soviet Union could right the nuclear imbalance by deploying new ICBMs on its own soil. In order to meet the threat it faced in 1962, 1963, and 1964, it had very few options. Moving existing nuclear weapons to locations from which they could reach American targets was one."
A second reason Soviet missiles were deployed to Cuba was because Khrushchev wanted to bring West Berlin—the American/British/French-controlled democratic enclave within Communist East Germany—into the Soviet orbit. The East Germans and Soviets considered western control over a portion of Berlin a grave threat to East Germany. For this reason, among others, Khrushchev made West Berlin the central battlefield of the Cold War. Khrushchev believed that if the U.S. did nothing over the missile deployments in Cuba, he could muscle the West out of Berlin using said missiles as a deterrent to western counter-measures in Berlin. If the U.S. tried to bargain with the Soviets after becoming aware of the missiles, Khrushchev could demand trading the missiles for West Berlin. Since Berlin was strategically more important than Cuba, the trade would be a win for Khrushchev. President Kennedy recognized this: "The advantage is, from Khrushchev's point of view, he takes a great chance but there are quite some rewards to it."
In early 1962, a group of Soviet military and missile construction specialists accompanied an agricultural delegation to Havana. They obtained a meeting with Cuban leader Fidel Castro. The Cuban leadership had a strong expectation that the U.S. would invade Cuba again and they enthusiastically approved the idea of installing nuclear missiles in Cuba. However, according to another source, Fidel Castro objected to the missiles deployment that would have made him look like a Soviet puppet, but was persuaded that missiles in Cuba would be an irritant to the U.S. and help the interests of the entire socialist camp. Further, the deployment would include short-range tactical weapons (with a range of 40 km, usable only against naval vessels) that would provide a "nuclear umbrella" for attacks upon the island.
By May, Khrushchev and Castro agreed to place strategic nuclear missiles secretly in Cuba. Like Castro, Khrushchev felt that a U.S. invasion of Cuba was imminent, and that to lose Cuba would do great harm to the communist cause, especially in Latin America. He said he wanted to confront the Americans "with more than words ... the logical answer was missiles." The Soviets maintained their tight secrecy, writing their plans longhand, which were approved by Rodion Malinovsky on July 4 and Khrushchev on July 7.
From the very beginning, the Soviets' operation entailed elaborate denial and deception, known in the USSR as "maskirovka". All of the planning and preparation for transporting and deploying the missiles were carried out in the utmost secrecy, with only a very few told the exact nature of the mission. Even the troops detailed for the mission were given misdirection, told they were headed for a cold region and outfitted with ski boots, fleece-lined parkas, and other winter equipment. The Soviet code name was Operation Anadyr. Anadyr was also the name of a river flowing into the Bering Sea, the name of the capital of Chukotsky District, and a bomber base in the far eastern region. All these were meant to conceal the program from both internal and external audiences.
Specialists in missile construction under the guise of "machine operators," "irrigation specialists" and "agricultural specialists" arrived in July. A total of 43,000 foreign troops would ultimately be brought in. Marshal Sergei Biryuzov, chief of the Soviet Rocket Forces, led a survey team that visited Cuba. He told Khrushchev that the missiles would be concealed and camouflaged by the palm trees.
The Cuban leadership was further upset when in September the U.S. Congress approved U.S. Joint Resolution 230, which expressed Congress's resolve to prevent the creation of an externally supported military establishment. On the same day, the U.S. announced a major military exercise in the Caribbean, PHIBRIGLEX-62, which Cuba denounced as a deliberate provocation and proof that the U.S. planned to invade Cuba.
The Soviet leadership believed, based on their perception of Kennedy's lack of confidence during the Bay of Pigs Invasion, that he would avoid confrontation and accept the missiles as a "fait accompli". On September 11, the Soviet Union publicly warned that a U.S. attack on Cuba or on Soviet ships carrying supplies to the island would mean war. The Soviets continued the "Maskirovka" program to conceal their actions in Cuba. They repeatedly denied that the weapons being brought into Cuba were offensive in nature. On September 7, Soviet Ambassador to the United States Anatoly Dobrynin assured United States Ambassador to the United Nations Adlai Stevenson that the USSR was supplying only defensive weapons to Cuba. On September 11, the Telegrafnoe Agentstvo Sovetskogo Soyuza (Soviet News Agency TASS) announced that the Soviet Union had no need or intention to introduce offensive nuclear missiles into Cuba. On October 13, Dobrynin was questioned by former Undersecretary of State Chester Bowles about whether the Soviets planned to put offensive weapons in Cuba. He denied any such plans. On October 17, Soviet embassy official Georgy Bolshakov brought President Kennedy a personal message from Khrushchev reassuring him that "under no circumstances would surface-to-surface missiles be sent to Cuba."
As early as August 1962, the U.S. suspected the Soviets of building missile facilities in Cuba. During that month, its intelligence services gathered information about sightings by ground observers of Russian-built MiG-21 fighters and Il-28 light bombers. U-2 spyplanes found S-75 Dvina (NATO designation "SA-2") surface-to-air missile sites at eight different locations. CIA director John A. McCone was suspicious. Sending antiaircraft missiles into Cuba, he reasoned, "made sense only if Moscow intended to use them to shield a base for ballistic missiles aimed at the United States." On August 10, he wrote a memo to President Kennedy in which he guessed that the Soviets were preparing to introduce ballistic missiles into Cuba.
With important Congressional elections scheduled for November, the Crisis became enmeshed in American politics. On August 31, Senator Kenneth Keating (R-New York), who received his information from Cuban exiles in Florida, warned on the Senate floor that the Soviet Union may be constructing a missile base in Cuba. He charged the Kennedy Administration was covering up a major threat to the U.S. Air Force General Curtis LeMay presented a pre-invasion bombing plan to Kennedy in September, while spy flights and minor military harassment from U.S. forces at Guantanamo Bay Naval Base were the subject of continual Cuban diplomatic complaints to the U.S. government.
The first consignment of R-12 missiles arrived on the night of September 8, followed by a second on September 16. The R-12 was an intermediate-range ballistic missile, capable of carrying a thermonuclear warhead. It was a single-stage, road-transportable, surface-launched, storable liquid propellant fueled missile that could deliver a megaton-class nuclear weapon. The Soviets were building nine sites—six for R-12 medium-range missiles (NATO designation "SS-4 Sandal") with an effective range of and three for R-14 intermediate-range ballistic missiles (NATO designation "SS-5 Skean") with a maximum range of .
Cuba positioning.
On October 7, Cuban President Osvaldo Dorticós spoke at the UN General Assembly: "If ... we are attacked, we will defend ourselves. I repeat, we have sufficient means with which to defend ourselves; we have indeed our inevitable weapons, the weapons, which we would have preferred not to acquire, and which we do not wish to employ."
Missiles reported.
The missiles in Cuba allowed the Soviets to effectively target the majority of the continental U.S. The planned arsenal was forty launchers. The Cuban populace readily noticed the arrival and deployment of the missiles and hundreds of reports reached Miami. U.S. intelligence received countless reports, many of dubious quality or even laughable, and most of which could be dismissed as describing defensive missiles. Only five reports bothered the analysts. They described large trucks passing through towns at night carrying very long canvas-covered cylindrical objects that could not make turns through towns without backing up and maneuvering. Defensive missiles could make these turns. These reports could not be satisfactorily dismissed.
Aerial images find Soviet missiles.
The United States had been sending U-2 surveillance over Cuba since the failed Bay of Pigs Invasion. The first issue that led to a pause in reconnaissance flights took place on August 30, when a U-2 operated by the US Air Force's Strategic Air Command flew over Sakhalin Island in the Soviet Far East by mistake. The Soviets lodged a protest and the U.S. apologized. Nine days later, a Taiwanese-operated U-2 was lost over western China to an SA-2 SAM. U.S. officials were worried that one of the Cuban or Soviet SAMs in Cuba might shoot down a CIA U-2, initiating another international incident. In a meeting with members of the Committee on Overhead Reconnaissance (COMOR) on 10 September, US Secretary of State Dean Rusk and National Security Advisor McGeorge Bundy heavily restricted further U-2 flights over Cuban airspace. The resulting lack of coverage over the island for the next five weeks became known to historians as the "Photo Gap." During this period, no significant U-2 coverage was achieved over the interior of the island. U.S. officials attempted to use a Corona photoreconnaissance satellite to obtain coverage over reported Soviet military deployments, but imagery acquired over western Cuba by a Corona KH-4 mission on 1 October was heavily covered by clouds and haze and failed to provide any usable intelligence. At the end of September, Navy reconnaissance aircraft photographed the Soviet ship "Kasimov" with large crates on its deck the size and shape of Il-28 light bomber fuselages.
In September 1962, analysts from the Defense Intelligence Agency (DIA) noticed that Cuban surface-to-air missile sites were arranged in a pattern similar to those used by the Soviet Union to protect its ICBM bases, leading DIA to lobby for the resumption of U-2 flights over the island. Although in the past the flights had been conducted by the CIA, due to pressure from the Defense Department, the authority was transferred to the Air Force. Following the loss of a CIA U-2 over the Soviet Union in May 1960, it was thought that if another U-2 were shot down, an Air Force aircraft arguably being used for a legitimate military purpose would be easier to explain than a CIA flight.
When the reconnaissance missions were re-authorized on October 9, poor weather kept the planes from flying. The U.S. first obtained U-2 photographic evidence of the missiles on October 14, when a U-2 flight piloted by Major Richard Heyser took 928 pictures on a path selected by DIA analysts, capturing images of what turned out to be an SS-4 construction site at San Cristóbal, Pinar del Río Province (now in Artemisa Province), in western Cuba.
President notified.
On October 15, the CIA's National Photographic Interpretation Center (NPIC) reviewed the U-2 photographs and identified objects that they interpreted as medium range ballistic missiles. This identification was made, in part, on the strength of reporting provided by Oleg Penkovsky, a double agent in the GRU working for CIA and MI6. Although he provided no direct reports of the Soviet missile deployments to Cuba, technical and doctrinal details of Soviet missile regiments provided by Penkovsky in the months and years prior to the Crisis helped NPIC analysts correctly identify the missiles on U-2 imagery.
That evening, the CIA notified the Department of State and at 8:30 pm EDT, National Security Adviser McGeorge Bundy chose to wait until the next morning to tell the President. Secretary of Defense Robert McNamara was briefed at midnight. The next morning, Bundy met with Kennedy and showed him the U-2 photographs and briefed him on the CIA's analysis of the images. At 6:30 pm EDT, Kennedy convened a meeting of the nine members of the National Security Council and five other key advisors, in a group he formally named the Executive Committee of the National Security Council (EXCOMM) after the fact on October 22 by the National Security Action Memorandum 196. Without informing the members of EXCOMM, President Kennedy tape recorded all of their proceedings, and Sheldon M. Stern, head of the Kennedy library has transcribed some of them.
Responses considered.
The U.S. had no plan in place because U.S. intelligence had been convinced that the Soviets would never install nuclear missiles in Cuba. The EXCOMM quickly discussed several possible courses of action, including:
The Joint Chiefs of Staff unanimously agreed that a full-scale attack and invasion was the only solution. They believed that the Soviets would not attempt to stop the US from conquering Cuba: Kennedy was skeptical.
Kennedy concluded that attacking Cuba by air would signal the Soviets to presume "a clear line" to conquer Berlin. Kennedy also believed that U.S. allies would think of the U.S. as "trigger-happy cowboys" who lost Berlin because they could not peacefully resolve the Cuban situation.
The EXCOMM then discussed the effect on the strategic balance of power, both political and military. The Joint Chiefs of Staff believed that the missiles would seriously alter the military balance, but McNamara disagreed. An extra 40, he reasoned, would make little difference to the overall strategic balance. The U.S. already had approximately 5,000 strategic warheads, while the Soviet Union had only 300. McNamara concluded that the Soviets having 340 would not therefore substantially alter the strategic balance. In 1990, he reiterated that "it made "no" difference ... The military balance wasn't changed. I didn't believe it then, and I don't believe it now."
The EXCOMM agreed that the missiles would affect the "political" balance. First, Kennedy had explicitly promised the American people less than a month before the crisis that "if Cuba should possess a capacity to carry out offensive actions against the United States ... the United States would act." Second, U.S. credibility among their allies, and among the American people, would be damaged if they allowed the Soviet Union to appear to redress the strategic balance by placing missiles in Cuba. Kennedy explained after the crisis that "it would have politically changed the balance of power. It would have appeared to, and appearances contribute to reality."
On October 18, President Kennedy met with Soviet Minister of Foreign Affairs, Andrei Gromyko, who claimed the weapons were for defensive purposes only. Not wanting to expose what he already knew, and wanting to avoid panicking the American public, the President did not reveal that he was already aware of the missile build-up.
By October 19, frequent U-2 spy flights showed four operational sites. As part of the blockade, the U.S. military was put on high alert to enforce the blockade and to be ready to invade Cuba at a moment's notice. The 1st Armored Division was sent to Georgia, and five army divisions were alerted for combat operations. The Strategic Air Command (SAC) distributed its shorter-ranged B-47 Stratojet medium bombers to civilian airports and sent aloft its B-52 Stratofortress heavy bombers.
Operational plans.
Two Operational Plans (OPLAN) were considered. OPLAN 316 envisioned a full invasion of Cuba by Army and Marine units supported by the Navy following Air Force and naval airstrikes. However, Army units in the U.S. would have had trouble fielding mechanized and logistical assets, while the U.S. Navy could not supply sufficient amphibious shipping to transport even a modest armored contingent from the Army. OPLAN 312, primarily an Air Force and Navy carrier operation, was designed with enough flexibility to do anything from engaging individual missile sites to providing air support for OPLAN 316's ground forces.
Quarantine.
Kennedy met with members of EXCOMM and other top advisers throughout October 21, considering two remaining options: an air strike primarily against the Cuban missile bases, or a naval blockade of Cuba. A full-scale invasion was not the administration's first option. McNamara supported the naval blockade as a strong but limited military action that left the U.S. in control. However, the term "blockade" was problematic. According to international law a blockade is an act of war, but the Kennedy administration did not think that the USSR would be provoked to attack by a mere blockade. Additionally, legal experts at the State Department and Justice Department concluded that a declaration of war could be avoided so long as another legal justification, based on the Rio Treaty for defense of the Western Hemisphere, was obtained via a resolution by a two-thirds vote from the members or the Organization of American States (OAS).
Admiral Anderson, Chief of Naval Operations wrote a position paper that helped Kennedy to differentiate between what they termed a "quarantine" of offensive weapons and a blockade of all materials, claiming that a classic blockade was not the original intention. Since it would take place in international waters, Kennedy obtained the approval of the OAS for military action under the hemispheric defense provisions of the Rio Treaty.
On October 19, the EXCOMM formed separate working groups to examine the air strike and blockade options, and by the afternoon most support in the EXCOMM shifted to the blockade option. Reservations about the plan continued to be voiced as late as the 21st; however, the paramount one being that once the blockade was put into effect, the Soviets would rush to complete some of the missiles. Consequently, the U.S. could find itself bombing operational missiles were the blockade to fail to force Khrushchev to remove the missiles already on the island.
Speech to the nation.
At 3:00 pm EDT on October 22, President Kennedy formally established the Executive Committee (EXCOMM) with National Security Action Memorandum (NSAM) 196. At 5:00 pm, he met with Congressional leaders who contentiously opposed a blockade and demanded a stronger response. In Moscow, Ambassador Kohler briefed Chairman Khrushchev on the pending blockade and Kennedy's speech to the nation. Ambassadors around the world gave notice to non-Eastern Bloc leaders. Before the speech, US delegations met with Canadian Prime Minister John Diefenbaker, British Prime Minister Harold Macmillan, West German Chancellor Konrad Adenauer, and French President Charles de Gaulle to brief them on the US intelligence and their proposed response. All were supportive of the US position.
On October 22 at 7:00 pm EDT, President Kennedy delivered a nationwide televised address on all of the major networks announcing the discovery of the missiles.
Kennedy described the administration's plan:
During the speech a directive went out to all U.S. forces worldwide placing them on DEFCON 3. The heavy cruiser USS "Newport News" was designated flagship for the blockade, with the USS "Leary" (DD-879) as "Newport News"s destroyer escort.
Crisis deepens.
On October 23, at 11:24 am EDT, a cable drafted by George Ball to the U.S. Ambassador in Turkey and NATO notified them that they were considering making an offer to withdraw what the U.S. knew to be nearly obsolete missiles from Italy and Turkey in exchange for the Soviet withdrawal from Cuba. Turkish officials replied that they would "deeply resent" any trade involving the U.S.'s missile presence in their country. Two days later, on the morning of October 25, journalist Walter Lippmann proposed the same thing in his syndicated column. Castro reaffirmed Cuba's right to self-defense and said that all of its weapons were defensive and Cuba would not allow an inspection.
International response.
Three days after Kennedy's speech, the Chinese "People's Daily" announced that "650,000,000 Chinese men and women were standing by the Cuban people." In West Germany, newspapers supported the U.S.'s response, contrasting it with the weak American actions in the region during the preceding months. They also expressed some fear that the Soviets might retaliate in Berlin. In France on October 23, the crisis made the front page of all the daily newspapers. The next day, an editorial in "Le Monde" expressed doubt about the authenticity of the CIA's photographic evidence. Two days later, after a visit by a high-ranking CIA agent, they accepted the validity of the photographs. Also in France, in the October 29 issue of "Le Figaro", Raymond Aron wrote in support of the American response. On October 24, Pope John XXIII sent a message to the Soviet embassy in Rome to be transmitted to the Kremlin, in which he voiced his concern for peace. In this message he stated "We beg all governments not to remain deaf to this cry of humanity. That they do all that is in their power to save peace."
Soviet broadcast.
At the time, the crisis continued unabated, and on the evening of October 24, the Soviet news agency TASS broadcast a telegram from Khrushchev to President Kennedy, in which Khrushchev warned that the United States' "outright piracy" would lead to war. However, this was followed at 9:24 pm by a telegram from Khrushchev to Kennedy which was received at 10:52 pm EDT, in which Khrushchev stated, "if you weigh the present situation with a cool head without giving way to passion, you will understand that the Soviet Union cannot afford not to decline the despotic demands of the USA" and that the Soviet Union views the blockade as "an act of aggression" and their ships will be instructed to ignore it.
US alert level raised.
The U.S. requested an emergency meeting of the United Nations Security Council on October 25. U.S. Ambassador to the United Nations Adlai Stevenson confronted Soviet Ambassador Valerian Zorin in an emergency meeting of the SC challenging him to admit the existence of the missiles. Ambassador Zorin refused to answer. The next day at 10:00 pm EDT, the U.S. raised the readiness level of SAC forces to DEFCON 2. For the only confirmed time in U.S. history, while the B-52 bombers went on continuous airborne alert, the B-47 medium bombers were dispersed to various military and civilian airfields, and made ready to take off, fully equipped, on 15 minutes' notice. One-eighth of SAC's 1,436 bombers were on airborne alert, some 145 intercontinental ballistic missiles stood on ready alert, while Air Defense Command (ADC) redeployed 161 nuclear-armed interceptors to 16 dispersal fields within nine hours with one-third maintaining 15-minute alert status. Twenty-three nuclear-armed B-52s were sent to orbit points within striking distance of the Soviet Union so that the latter might observe that the U.S. was serious. Jack J. Catton later estimated that about 80 percent of SAC's planes were ready for launch during the crisis; David A. Burchinal recalled that, by contrast,
"By October 22, Tactical Air Command (TAC) had 511 fighters plus supporting tankers and reconnaissance aircraft deployed to face Cuba on one-hour alert status. However, TAC and the Military Air Transport Service had problems. The concentration of aircraft in Florida strained command and support echelons; which faced critical undermanning in security, armaments, and communications; the absence of initial authorization for war-reserve stocks of conventional munitions forced TAC to scrounge; and the lack of airlift assets to support a major airborne drop necessitated the call-up of 24 Reserve squadrons."
On October 25 at 1:45 am EDT, Kennedy responded to Khrushchev's telegram, stating that the U.S. was forced into action after receiving repeated assurances that no offensive missiles were being placed in Cuba, and that when these assurances proved to be false, the deployment "required the responses I have announced ... I hope that your government will take necessary action to permit a restoration of the earlier situation."
Blockade challenged.
At 7:15 am EDT on October 25, the USS "Essex" and USS "Gearing" attempted to intercept the "Bucharest" but failed to do so. Fairly certain the tanker did not contain any military material, they allowed it through the blockade. Later that day, at 5:43 pm, the commander of the blockade effort ordered the USS "Joseph P. Kennedy, Jr" to intercept and board the Lebanese freighter "Marucla". This took place the next day, and the "Marucla" was cleared through the blockade after its cargo was checked.
At 5:00 pm EDT on October 25, William Clements announced that the missiles in Cuba were still actively being worked on. This report was later verified by a CIA report that suggested there had been no slow-down at all. In response, Kennedy issued Security Action Memorandum 199, authorizing the loading of nuclear weapons onto aircraft under the command of SACEUR (which had the duty of carrying out first air strikes on the Soviet Union). During the day, the Soviets responded to the blockade by turning back 14 ships presumably carrying offensive weapons.
Crisis stalemated.
The next morning, October 26, Kennedy informed the EXCOMM that he believed only an invasion would remove the missiles from Cuba. However, he was persuaded to give the matter time and continue with both military and diplomatic pressure. He agreed and ordered the low-level flights over the island to be increased from two per day to once every two hours. He also ordered a crash program to institute a new civil government in Cuba if an invasion went ahead.
At this point, the crisis was ostensibly at a stalemate. The USSR had shown no indication that they would back down and had made several comments to the contrary. The U.S. had no reason to believe otherwise and was in the early stages of preparing for an invasion, along with a nuclear strike on the Soviet Union in case it responded militarily, which was assumed.
Secret negotiations.
At 1:00 pm EDT on October 26, John A. Scali of ABC News had lunch with Aleksandr Fomin (alias of spy Alexander Feklisov) at Fomin's request. Following the instructions of the Politburo of the CPSU, Fomin noted, "War seems about to break out" and asked Scali to use his contacts to talk to his "high-level friends" at the State Department to see if the U.S. would be interested in a diplomatic solution. He suggested that the language of the deal would contain an assurance from the Soviet Union to remove the weapons under UN supervision and that Castro would publicly announce that he would not accept such weapons in the future, in exchange for a public statement by the U.S. that it would never invade Cuba. The U.S. responded by asking the Brazilian government to pass a message to Castro that the U.S. would be "unlikely to invade" if the missiles were removed.
On October 26 at 6:00 pm EDT, the State Department started receiving a message that appeared to be written personally by Khrushchev. It was Saturday at 2:00 am in Moscow. The long letter took several minutes to arrive, and it took translators additional time to translate and transcribe it.
Robert F. Kennedy described the letter as "very long and emotional." Khrushchev reiterated the basic outline that had been stated to John Scali earlier in the day, "I propose: we, for our part, will declare that our ships bound for Cuba are not carrying any armaments. You will declare that the United States will not invade Cuba with its troops and will not support any other forces which might intend to invade Cuba. Then the necessity of the presence of our military specialists in Cuba will disappear." At 6:45 pm EDT, news of Fomin's offer to Scali was finally heard and was interpreted as a "set up" for the arrival of Khrushchev's letter. The letter was then considered official and accurate, although it was later learned that Fomin was almost certainly operating of his own accord without official backing. Additional study of the letter was ordered and continued into the night.
Crisis continues.
Castro, on the other hand, was convinced that an invasion of Cuba was soon at hand, and on October 26, he sent a telegram to Khrushchev that appeared to call for a preemptive nuclear strike on the U.S. However, in a 2010 interview, Castro said of his recommendation for the Soviets to attack America "before" they made any move against Cuba: "After I've seen what I've seen, and knowing what I know now, it wasn't worth it at all." Castro also ordered all anti-aircraft weapons in Cuba to fire on any US aircraft, whereas in the past they had been ordered only to fire on groups of two or more. At 6:00 am EDT on October 27, the CIA delivered a memo reporting that three of the four missile sites at San Cristobal and the two sites at Sagua la Grande appeared to be fully operational. They also noted that the Cuban military continued to organize for action, although they were under order not to initiate action unless attacked.
At 9:00 am EDT on October 27, Radio Moscow began broadcasting a message from Khrushchev. Contrary to the letter of the night before, the message offered a new trade, that the missiles on Cuba would be removed in exchange for the removal of the Jupiter missiles from Italy and Turkey. At 10:00 am EDT, the executive committee met again to discuss the situation and came to the conclusion that the change in the message was due to internal debate between Khrushchev and other party officials in the Kremlin. Kennedy realized that he would be in an "insupportable position if this becomes Khrushchev's proposal", because: 1) The missiles in Turkey were not militarily useful and were being removed anyway; and 2) "It's gonna – to any man at the United Nations or any other rational man, it will look like a very fair trade." National Security Adviser McGeorge Bundy explained why Khrushchev's public acquiescence could not be considered: "The current threat to peace is not in Turkey, it is in Cuba."
McNamara noted that another tanker, the "Grozny", was about out and should be intercepted. He also noted that they had not made the USSR aware of the blockade line and suggested relaying this information to them via U Thant at the United Nations.
While the meeting progressed, at 11:03 am EDT a new message began to arrive from Khrushchev. The message stated, in part,
"You are disturbed over Cuba. You say that this disturbs you because it is ninety-nine miles by sea from the coast of the United States of America. But ... you have placed destructive missile weapons, which you call offensive, in Italy and Turkey, literally next to us ... I therefore make this proposal: We are willing to remove from Cuba the means which you regard as offensive ... Your representatives will make a declaration to the effect that the United States ... will remove its analogous means from Turkey ... and after that, persons entrusted by the United Nations Security Council could inspect on the spot the fulfillment of the pledges made."
The executive committee continued to meet through the day.
Throughout the crisis, Turkey had repeatedly stated that it would be upset if the Jupiter missiles were removed. Italy's Prime Minister Fanfani, who was also Foreign Minister "ad interim", offered to allow withdrawal of the missiles deployed in Apulia as a bargaining chip. He gave the message to one of his most trusted friends, Ettore Bernabei, the general manager of RAI-TV, to convey to Arthur M. Schlesinger, Jr.. Bernabei was in New York to attend an international conference on satellite TV broadcasting. Unknown to the Soviets, the U.S. regarded the Jupiter missiles as obsolescent and already supplanted by the Polaris nuclear ballistic submarine missiles.
On the morning of October 27, a U-2F (the third CIA U-2A, modified for air-to-air refueling) piloted by USAF Major Rudolf Anderson, departed its forward operating location at McCoy AFB, Florida. At approximately 12:00 pm EDT, the aircraft was struck by a S-75 Dvina (NATO designation "SA-2 Guideline") SAM missile launched from Cuba. The aircraft was shot down and Anderson was killed. The stress in negotiations between the USSR and the U.S. intensified, and only later was it learned that the decision to fire the missile was made locally by an undetermined Soviet commander acting on his own authority. Later that day, at about 3:41 pm EDT, several U.S. Navy RF-8A Crusader aircraft on low-level photoreconnaissance missions were fired upon.
At 4:00 pm EDT, Kennedy recalled members of EXCOMM to the White House and ordered that a message immediately be sent to U Thant asking the Soviets to suspend work on the missiles while negotiations were carried out. During this meeting, General Maxwell Taylor delivered the news that the U-2 had been shot down. Kennedy had earlier claimed he would order an attack on such sites if fired upon, but he decided to not act unless another attack was made. Forty years later, McNamara said:
Drafting the response.
Emissaries sent by both Kennedy and Nikita Khrushchev agreed to meet at the Yenching Palace Chinese restaurant in the Cleveland Park neighborhood of Washington D.C. on the evening of October 27. Kennedy suggested that they take Khrushchev's offer to trade away the missiles. Unknown to most members of the EXCOMM, Robert Kennedy had been meeting with the Soviet Ambassador in Washington to discover whether these intentions were genuine. The EXCOMM was generally against the proposal because it would undermine NATO's authority, and the Turkish government had repeatedly stated it was against any such trade.
As the meeting progressed, a new plan emerged and Kennedy was slowly persuaded. The new plan called for the President to ignore the latest message and instead to return to Khrushchev's earlier one. Kennedy was initially hesitant, feeling that Khrushchev would no longer accept the deal because a new one had been offered, but Llewellyn Thompson argued that he might accept it anyway. White House Special Counsel and Adviser Ted Sorensen and Robert Kennedy left the meeting and returned 45 minutes later with a draft letter to this effect. The President made several changes, had it typed, and sent it.
After the EXCOMM meeting, a smaller meeting continued in the Oval Office. The group argued that the letter should be underscored with an oral message to Ambassador Dobrynin stating that if the missiles were not withdrawn, military action would be used to remove them. Dean Rusk added one proviso, that no part of the language of the deal would mention Turkey, but there would be an understanding that the missiles would be removed "voluntarily" in the immediate aftermath. The President agreed, and the message was sent.
At Dean Rusk's request, Fomin and Scali met again. Scali asked why the two letters from Khrushchev were so different, and Fomin claimed it was because of "poor communications." Scali replied that the claim was not credible and shouted that he thought it was a "stinking double cross." He went on to claim that an invasion was only hours away, at which point Fomin stated that a response to the US message was expected from Khrushchev shortly, and he urged Scali to tell the State Department that no treachery was intended. Scali said that he did not think anyone would believe him, but he agreed to deliver the message. The two went their separate ways, and Scali immediately typed out a memo for the EXCOMM.
Within the U.S. establishment, it was well understood that ignoring the second offer and returning to the first put Khrushchev in a terrible position. Military preparations continued, and all active duty Air Force personnel were recalled to their bases for possible action. Robert Kennedy later recalled the mood, "We had not abandoned all hope, but what hope there was now rested with Khrushchev's revising his course within the next few hours. It was a hope, not an expectation. The expectation was military confrontation by Tuesday, and possibly tomorrow ..."
At 8:05 pm EDT, the letter drafted earlier in the day was delivered. The message read, "As I read your letter, the key elements of your proposals—which seem generally acceptable as I understand them—are as follows: 1) You would agree to remove these weapons systems from Cuba under appropriate United Nations observation and supervision; and undertake, with suitable safe-guards, to halt the further introduction of such weapon systems into Cuba. 2) We, on our part, would agree—upon the establishment of adequate arrangements through the United Nations, to ensure the carrying out and continuation of these commitments (a) to remove promptly the quarantine measures now in effect and (b) to give assurances against the invasion of Cuba." The letter was also released directly to the press to ensure it could not be "delayed."
With the letter delivered, a deal was on the table. However, as Robert Kennedy noted, there was little expectation it would be accepted. At 9:00 pm EDT, the EXCOMM met again to review the actions for the following day. Plans were drawn up for air strikes on the missile sites as well as other economic targets, notably petroleum storage. McNamara stated that they had to "have two things ready: a government for Cuba, because we're going to need one; and secondly, plans for how to respond to the Soviet Union in Europe, because sure as hell they're going to do something there."
At 12:12 am EDT, on October 27, the U.S. informed its NATO allies that "the situation is growing shorter ... the United States may find it necessary within a very short time in its interest and that of its fellow nations in the Western Hemisphere to take whatever military action may be necessary." To add to the concern, at 6 am the CIA reported that all missiles in Cuba were ready for action.
Later that same day, what the White House later called "Black Saturday," the U.S. Navy dropped a series of "signaling depth charges" (practice depth charges the size of hand grenades) on a Soviet submarine (B-59) at the blockade line, unaware that it was armed with a nuclear-tipped torpedo with orders that allowed it to be used if the submarine was "hulled" (a hole in the hull from depth charges or surface fire). The decision to launch these required agreement from all three officers on board, but one of them, Vasili Arkhipov, objected and so the launch was narrowly averted.
On the same day a U.S. U-2 spy plane made an accidental, unauthorized ninety-minute overflight of the Soviet Union's far eastern coast. The Soviets responded by scrambling MiG fighters from Wrangel Island; in turn the Americans launched F-102 fighters armed with nuclear air-to-air missiles over the Bering Sea.
On October 27, Khrushchev also received a letter from Castro – what is now known as the Armageddon Letter (dated Oct. 26) – interpreted as urging the use of nuclear force in the event of an attack on Cuba. "I believe the imperialists' aggressiveness is extremely dangerous and if they actually carry out the brutal act of invading Cuba in violation of international law and morality, that would be the moment to eliminate such danger forever through an act of clear legitimate defense, however harsh and terrible the solution would be," Castro wrote.
Crisis ends.
On October 27, after much deliberation between the Soviet Union and Kennedy's cabinet, Kennedy secretly agreed to remove all missiles set in southern Italy and in Turkey, the latter on the border of the Soviet Union, in exchange for Khrushchev removing all missiles in Cuba. There is some dispute as to whether removing the missiles from Italy was part of the secret agreement, although Khrushchev wrote in his memoirs that it was; nevertheless, when the crisis had ended McNamara gave the order to dismantle the missiles in both Italy and Turkey.
At 9:00 am EST, on October 28, a new message from Khrushchev was broadcast on Radio Moscow. Khrushchev stated that, "the Soviet government, in addition to previously issued instructions on the cessation of further work at the building sites for the weapons, has issued a new order on the dismantling of the weapons which you describe as 'offensive' and their crating and return to the Soviet Union."
At 10AM, October 28th, Kennedy first learned of Khrushchev's solution to the crisis: Remove the 15 Jupiters in Turkey and Russia will remove the rockets from Cuba. The Soviet premier had made the offer in a public statement for the world to hear. Despite almost solid opposition from his senior advisers, Kennedy quickly embraced the Soviet offer. "This is a pretty good play of his," Kennedy said, according to a tape recording he made secretly of the Cabinet Room meeting. Kennedy had deployed the Jupiters in March of the year, causing a stream of angry outbursts from the Soviet premier. "Most people will think this is a rather even trade and we ought to take advantage of it," Kennedy said. Vice President Lyndon Johnson was the first to endorse the missile swap but others continued to oppose the offer. Finally, Kennedy ended the debate. "We can't very well invade Cuba with all its toil and blood," Kennedy said,"when we could have gotten them out by making a deal on the same missiles on Turkey. If that's part of the record, then you don't have a very good war."
Kennedy immediately responded, issuing a statement calling the letter "an important and constructive contribution to peace." He continued this with a formal letter: 
The US continued the blockade, and in the following days, aerial reconnaissance proved that the Soviets were making progress in removing the missile systems. The 42 missiles and their support equipment were loaded onto eight Soviet ships. On November 2, 1962, President Kennedy addressed the U.S. via radio and television broadcasts regarding the dismantlement process of the Soviet R-12 missile bases located in the Caribbean region. The ships left Cuba from November 5–9. The U.S. made a final visual check as each of the ships passed the blockade line. Further diplomatic efforts were required to remove the Soviet IL-28 bombers, and they were loaded on three Soviet ships on December 5 and 6. Concurrent with the Soviet commitment on the IL-28s, the U.S. Government announced the end of the blockade effective at 6:45 pm EST on November 20, 1962. 
At the time when the Kennedy administration thought that the Cuban Missile Crisis was resolved, nuclear tactical rockets stayed in Cuba since they were not part of the Kennedy-Khrushchev understandings. However, the Soviets changed their minds, fearing possible future Cuban militant steps, and on November 22, 1962, the Soviet Deputy Prime Minister Anastas Mikoyan told Castro that those rockets with the nuclear warheads were being removed too.
In his negotiations with the Soviet Ambassador Anatoly Dobrynin, Robert Kennedy informally proposed that the Jupiter missiles in Turkey would be removed "within a short time after this crisis was over." The last US missiles were disassembled by April 24, 1963, and were flown out of Turkey soon after.
The practical effect of this Kennedy-Khrushchev Pact was that it effectively strengthened Castro's position in Cuba, guaranteeing that the U.S. would not invade Cuba. It is possible that Khrushchev only placed the missiles in Cuba to get Kennedy to remove the missiles from Italy and Turkey and that the Soviets had no intention of resorting to nuclear war if they were out-gunned by the U.S. Because the withdrawal of the Jupiter missiles from NATO bases in Southern Italy and Turkey was not made public at the time, Khrushchev appeared to have lost the conflict and become weakened. The perception was that Kennedy had won the contest between the superpowers and Khrushchev had been humiliated. This is not entirely the case as both Kennedy and Khrushchev took every step to avoid full conflict despite the pressures of their governments. Khrushchev held power for another two years.
Aftermath.
The compromise embarrassed Khrushchev and the Soviet Union because the withdrawal of U.S. missiles from Italy and Turkey was a secret deal between Kennedy and Khrushchev. Khrushchev went to Kennedy thinking that the crisis was getting out of hand. The Soviets were seen as retreating from circumstances that they had started. Khrushchev's fall from power two years later was in part because of the Politburo embarrassment at both Khrushchev's eventual concessions to the U.S. and his ineptitude in precipitating the crisis in the first place. According to Dobrynin, the top Soviet leadership took the Cuban outcome as "a blow to its prestige bordering on humiliation."
Cuba perceived the outcome as a betrayal by the Soviets, given that decisions on how to resolve the crisis had been made exclusively by Kennedy and Khrushchev. Castro was especially upset that certain issues of interest to Cuba, such as the status of the U.S. Naval Base in Guantánamo, were not addressed. This caused Cuban–Soviet relations to deteriorate for years to come. On the other hand, Cuba continued to be protected from invasion.
Although LeMay told the President that he considered the resolution of the Cuban Missile Crisis the "greatest defeat in our history", his was a minority position. He had pressed for an immediate invasion of Cuba as soon as the crisis began, and still favored invading Cuba even after the Soviets had withdrawn their missiles. 25 years later, LeMay still believed that "We could have gotten not only the missiles out of Cuba, we could have gotten the Communists out of Cuba at that time."
After the crisis, the U.S. and the Soviet Union created the Moscow–Washington hotline, a direct communications link between Moscow and Washington, D.C. The purpose was to have a way that the leaders of the two Cold War countries could communicate directly to solve such a crisis. The world-wide US Forces DEFCON 3 status was returned to DEFCON 4 on November 20, 1962. U-2 pilot Major Anderson's body was returned to the U.S. and he was buried with full military honors in South Carolina. He was the first recipient of the newly created Air Force Cross, which was awarded posthumously.
Although Anderson was the only combatant fatality during the crisis, 11 crew members of three reconnaissance Boeing RB-47 Stratojets of the 55th Strategic Reconnaissance Wing were also killed in crashes during the period between September 27 and November 11, 1962. Seven crew died when a MATS Boeing C-135B Stratolifter delivering ammunition to Guantanamo Bay Naval Base stalled and crashed on approach on October 23.
Critics including Seymour Melman and Seymour Hersh suggested that the Cuban Missile Crisis encouraged U.S. use of military means, such as in the Vietnam War. This Soviet–American confrontation was synchronous with the Sino-Indian War, dating from the U.S.'s military blockade of Cuba; historians speculate that the Chinese attack against India for disputed land was meant to coincide with the Cuban Missile Crisis.
Post-crisis revelations.
Arthur M. Schlesinger, Jr., a historian and adviser to President Kennedy, told National Public Radio in an interview on October 16, 2002, that Castro did not want the missiles, but that Khrushchev had pressured Castro to accept them. Castro was not completely happy with the idea but the Cuban National Directorate of the Revolution accepted them to protect Cuba against U.S. attack, and to aid its ally, the Soviet Union. Schlesinger believed that when the missiles were withdrawn, Castro was angrier with Khrushchev than he was with Kennedy because Khrushchev had not consulted Castro before deciding to remove them. Although Castro was infuriated by Khrushchev, he planned on striking the United States with remaining missiles should an invasion of the island occur.
In early 1992, it was confirmed that Soviet forces in Cuba had, by the time the crisis broke, received tactical nuclear warheads for their artillery rockets and Il-28 bombers. Castro stated that he would have recommended their use if the U.S. invaded despite knowing Cuba would be destroyed.
Arguably the most dangerous moment in the crisis was only recognized during the Cuban Missile Crisis Havana conference in October 2002. Attended by many of the veterans of the crisis, they all learned that on October 27, 1962, the USS "Beale" had tracked and dropped signaling depth charges (the size of hand grenades) on the B-59, a Soviet Project 641 (NATO designation "Foxtrot") submarine which, unknown to the US, was armed with a 15-kiloton nuclear torpedo. Running out of air, the Soviet submarine was surrounded by American warships and desperately needed to surface. An argument broke out among three officers on the "B-59", including submarine captain Valentin Savitsky, political officer Ivan Semonovich Maslennikov, and Deputy brigade commander Captain 2nd rank (U.S. Navy Commander rank equivalent) Vasili Arkhipov. An exhausted Savitsky became furious and ordered that the nuclear torpedo on board be made combat ready. Accounts differ about whether Commander Arkhipov convinced Savitsky not to make the attack, or whether Savitsky himself finally concluded that the only reasonable choice left open to him was to come to the surface. During the conference McNamara stated that nuclear war had come much closer than people had thought. Thomas Blanton, director of the National Security Archive, said, "A guy called Vasili Arkhipov saved the world."
Fifty years after the crisis, Graham Allison wrote:
BBC journalist Joe Matthews published on October 13, 2012, the story behind the 100 tactical nuclear warheads mentioned by Graham Allison in the excerpt above. Khrushchev feared that Castro's hurt pride and widespread Cuban indignation over the concessions he had made to Kennedy might lead to a breakdown of the agreement between the Soviet Union and the U.S. In order to prevent this, Khrushchev decided to offer to give Cuba more than 100 tactical nuclear weapons that had been shipped to Cuba along with the long-range missiles, but which crucially had escaped the notice of U.S. intelligence. Khrushchev determined that because the Americans had not listed the missiles on their list of demands, keeping them in Cuba would be in the Soviet Union's interests.
Anastas Mikoyan was tasked with the negotiations with Castro over the missile transfer deal designed to prevent a breakdown in the relations between Cuba and the Soviet Union. While in Havana, Mikoyan witnessed the mood swings and paranoia of Castro, who was convinced that Moscow had made the agreement with the U.S. at the expense of Cuba's defense. Mikoyan, on his own initiative, decided that Castro and his military not be given control of weapons with an explosive force equal to 100 Hiroshima-sized bombs under any circumstances. He defused the seemingly intractable situation, which risked re-escalating the crisis, on November 22, 1962. During a tense, four-hour meeting, Mikoyan convinced Castro that despite Moscow's desire to help, it would be in breach of an unpublished Soviet law (which didn't actually exist) to transfer the missiles permanently into Cuban hands and provide them with an independent nuclear deterrent. Castro was forced to give way and – much to the relief of Khrushchev and the rest of the Soviet government – the tactical nuclear weapons were crated and returned by sea to the Soviet Union during December 1962.
Memories in the media.
The American popular media, especially television, made heavy use of the events of the missile crisis and both fictional and documentary forms. Jim Willis includes the Crisis as one of the 100 "media moments that changed America." Sheldon Stern finds that a half century later there are still many "misconceptions, half-truths, and outright lies" that have shaped media versions of what happened in the White House during those harrowing two weeks. According to William Cohn, television programs are typically the main source used by the American public to know about and interpret the past. The Soviet media proved somewhat disorganized as it was unable to generate a coherent popular history. Khrushchev lost power and he was airbrushed out of the story. Cuba was no longer portrayed as a heroic David against the American Goliath. One contradiction that pervaded the Soviet media campaign was between the pacifistic rhetoric of the peace movement that emphasizes the horrors of nuclear war, versus the militancy of the need to prepare Soviets for war against American aggression.

</doc>
<doc id="6828" url="https://en.wikipedia.org/wiki?curid=6828" title="Aquilegia">
Aquilegia

Aquilegia (common names: granny's bonnet or columbine) is a genus of about 60-70 species of perennial plants that are found in meadows, woodlands, and at higher altitudes throughout the Northern Hemisphere, known for the spurred petals of their flowers.
Etymology.
The genus name "Aquilegia" is derived from the Latin word for eagle ("aquila"), because the shape of the flower petals, which are said to resemble an eagle's claw. The common name "columbine" comes from the Latin for "dove", due to the resemblance of the inverted flower to five doves clustered together.
Description.
The fruit is a follicle.
the five points that stick out further than the petals are the calix (chalis).
Relatives.
Columbines are closely related to plants in the genera "Actaea" (baneberries) and "Aconitum" (wolfsbanes/monkshoods), which like "Aquilegia" produce cardiogenic toxins.
Insects.
They are used as food plants by some Lepidoptera (butterfly and moth) caterpillars. These are mainly of noctuid moths – noted for feeding on many poisonous plants without harm – such as Cabbage Moth ("Mamestra brassicae"), Dot Moth ("Melanchra persicariae") and Mouse Moth ("Amphipyra tragopoginis"). The Engrailed ("Ectropis crepuscularia"), a geometer moth, also uses columbine as a larval foodplant.
Plants in the "Aquilegia" genus are a major food source for "Bombus hortorum", a species of bumblebee. Specifically, they have been found to forage on species of "Aquilegia vulgaris" in Belgium and "Aquilegia chrysantha" in North America and Belgium. The bees do not show any preference in color of the flowers. 
Cultivation.
Columbine is a hardy perennial, which propagates by seed. It will grow to a height of 15 to 20 inches. It will grow in full sun; however, it prefers growing in partial shade and well drained soil, and is able to tolerate average soils and dry soil conditions. Columbine is rated at hardiness zone 3 in the USA so does not require mulching or protection in the winter.
Large numbers of hybrids are available for the garden, since the European "A. vulgaris" was hybridized with other European and North American varieties.
The British National Collection of Aquilegia is held by Mrs Carrie Thomas at Killay near Swansea.
Uses.
The flowers of various species of columbine were consumed in moderation by Native Americans as a condiment with other fresh greens, and are reported to be very sweet, and safe if consumed in small quantities. The plant's seeds and roots are highly poisonous however, and contain cardiogenic toxins which cause both severe gastroenteritis and heart palpitations if consumed as food. Native Americans used very small amounts of "Aquilegia" root as a treatment for ulcers. However, the medical use of this plant is better avoided due to its high toxicity; columbine poisonings may be fatal.
An acute toxicity test in mice has demonstrated that ethanol extract mixed with isocytisoside, the main flavonoid compound from the leaves and stems of "Aquilegia vulgaris", can be classified as non-toxic, since a dose of 3000 mg/kg did not cause mortality.
Culture.
The Colorado Blue Columbine ("A. caerulea") is the official state flower of Colorado (see also Columbine, Colorado).
Evolution.
Columbines have been important in the study of evolution. It was found that Sierra Columbine ("A. pubescens") and Crimson Columbine ("A. formosa") each have adapted specifically to a pollinator. Bees and hummingbirds are the visitors to "A. formosa", while hawkmoths would only visit "A. pubescens" when given a choice. Such a "pollination syndrome", being due to flower color and orientation controlled by their genetics, ensures reproductive isolation and can be a cause of speciation.
"Aquilegia" petals show an enormous range of petal spur length diversity ranging from a centimeter to the 15 cm spurs of "Aquilegia longissima". Selection from pollinator shifts is suggested to have driven these changes in nectar spur length. 
Interestingly, it was shown that this amazing spur length diversity is achieved solely through changing cell shape, not cell number or cell size. This suggests that a simple microscopic change can result in a dramatic evolutionarily relevant morphological change.
Species.
Columbine species include:

</doc>
<doc id="6829" url="https://en.wikipedia.org/wiki?curid=6829" title="Cache (computing)">
Cache (computing)

In computing, a cache ( , or in AuE) is a component that stores data so future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation, or the duplicate of data stored elsewhere. A "cache hit" occurs when the requested data can be found in a cache, while a "cache miss" occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests can be served from the cache, the faster the system performs.
To be cost-effective and to enable efficient use of data, caches are relatively small. Nevertheless, caches have proven themselves in many areas of computing because access patterns in typical computer applications exhibit the locality of reference. Moreover, access patterns exhibit temporal locality if data is requested again that has been recently requested already, while spatial locality refers to requests for data physically stored close to data that has been already requested.
Operation.
Hardware implements cache as a block of memory for temporary storage of data likely to be used again. Central processing units (CPUs) and hard disk drives (HDDs) frequently use a cache, as do web browsers and web servers.
A cache is made up of a pool of entries. Each entry has associated data, which is a copy of the same data in some "backing store". Each entry also has a tag, which specifies the identity of the data in the backing store of which the entry is a copy.
When the cache client (a CPU, web browser, operating system) needs to access data presumed to exist in the backing store, it first checks the cache. If an entry can be found with a tag matching that of the desired data, the data in the entry is used instead. This situation is known as a cache hit. So, for example, a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL. In this example, the URL is the tag, and the contents of the web page is the data. The percentage of accesses that result in cache hits is known as the hit rate or hit ratio of the cache.
The alternative situation, when the cache is consulted and found not to contain data with the desired tag, has become known as a cache miss. The previously uncached data fetched from the backing store during miss handling is usually copied into the cache, ready for the next access.
During a cache miss, the CPU usually ejects some other entry in order to make room for the previously uncached data. The heuristic used to select the entry to eject is known as the replacement policy. One popular replacement policy, "least recently used" (LRU), replaces the least recently used entry (see cache algorithm). More efficient caches compute use frequency against the size of the stored contents, as well as the latencies and throughputs for both the cache and the backing store. This works well for larger amounts of data, longer latencies and slower throughputs, such as experienced with a hard drive and the Internet, but is not efficient for use with a CPU cache.
Writing policies.
When a system writes data to cache, it must at some point write that data to the backing store as well. The timing of this write is controlled by what is known as the "write policy".
There are two basic writing approaches:
A write-back cache is more complex to implement, since it needs to track which of its locations have been written over, and mark them as "dirty" for later writing to the backing store. The data in these locations are written back to the backing store only when they are evicted from the cache, an effect referred to as a "lazy write". For this reason, a read miss in a write-back cache (which requires a block to be replaced by another) will often require two memory accesses to service: one to write the replaced data from the cache back to the store, and then one to retrieve the needed data.
Other policies may also trigger data write-back. The client may make many changes to data in the cache, and then explicitly notify the cache to write back the data.
No data is returned on write operations, thus there are two approaches for situations of write-misses:
Both write-through and write-back policies can use either of these write-miss policies, but usually they are paired in this way:
Entities other than the cache may change the data in the backing store, in which case the copy in the cache may become out-of-date or "stale". Alternatively, when the client updates the data in the cache, copies of those data in other caches will become stale. Communication protocols between the cache managers which keep the data consistent are known as coherency protocols.
Applications.
CPU cache.
Small memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop and server microprocessors may have as many as six, each specialized for a specific function. Examples of caches with a specific function are the D-cache and I-cache (data cache and instruction cache).
GPU cache.
Earlier graphics processing units (GPUs) did not feature hardware-managed caches; however, as GPUs moved toward general-purpose computing (GPGPU), they have introduced and used progressively larger caches. For example, GT200 architecture GPUs did not feature an L2 cache, while the Fermi GPU has 768 KB of last-level cache, the Kepler GPU has 1536 KB of last-level cache, and the Maxwell GPU has 2048 KB of last-level cache.
Translation lookaside buffer.
A memory management unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations. This specialized cache is called a translation lookaside buffer (TLB).
Disk cache.
While CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel.
While the disk buffer, which is an integrated part of the hard disk drive, is sometimes misleadingly referred to as "disk cache", its main functions are write sequencing and read prefetching. Repeated cache hits are relatively rare, due to the small size of the buffer in comparison to the drive's capacity. However, high-end disk controllers often have their own on-board cache of the hard disk drive's data blocks.
Finally, a fast local hard disk drive can also cache information held on even slower data storage devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management. Also, fast flash-based solid-state drives (SSDs) can be used as caches for slower rotational-media hard disk drives, working together as hybrid drives or solid-state hybrid drives (SSHDs).
Web cache.
Web browsers and web proxy servers employ web caches to store previous responses from web servers, such as web pages and images. Web caches reduce the amount of information that needs to be transmitted across the network, as information previously stored in the cache can often be re-used. This reduces bandwidth and processing requirements of the web server, and helps to improve responsiveness for users of the web.
Web browsers employ a built-in web cache, but some internet service providers or organizations also use a caching proxy server, which is a web cache that is shared among all users of that network.
Another form of cache is P2P caching, where the files most sought for by peer-to-peer applications are stored in an ISP cache to accelerate P2P transfers. Similarly, decentralised equivalents exist, which allow communities to perform the same task for P2P traffic, for example, Corelli.
Memoization.
A cache can store data that is computed on demand rather than retrieved from a backing store. Memoization is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation.
Other caches.
The BIND DNS daemon caches a mapping of domain names to IP addresses, as does a resolver library.
Write-through operation is common when operating over unreliable networks (like an Ethernet LAN), because of the enormous complexity of the coherency protocol required between multiple write-back caches when communication is unreliable. For instance, web page caches and client-side network file system caches (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.
Search engines also frequently make web pages they have indexed available from their cache. For example, Google provides a "Cached" link next to each search result. This can prove useful when web pages from a web server are temporarily or permanently inaccessible.
Another type of caching is storing computed results that will likely be needed again, or memoization. For example, ccache is a program that caches the output of the compilation, in order to speed up later compilation runs.
Database caching can substantially improve the throughput of database applications, for example in the processing of indexes, data dictionaries, and frequently used subsets of data.
A distributed cache uses networked hosts to provide scalability, reliability and performance to the application. The hosts can be co-located or spread over different geographical regions.
Buffer vs. cache.
The semantics of a "buffer" and a "cache" are not necessarily mutually exclusive; even so, there are fundamental differences in intent between the process of caching and the process of buffering.
Fundamentally, caching realizes a performance increase for transfers of data that is being repeatedly transferred. While a caching system may realize a performance increase upon the initial (typically write) transfer of a data item, this performance increase is due to buffering occurring within the caching system.
With read caches, a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache's intermediate storage, deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand,
With typical caching implementations, a data item that is read or written for the first time is effectively being buffered; and in the case of a write, mostly realizing a performance increase for the application from where the write originated. Additionally, the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering. The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering, although this form may negatively impact the performance of at least the initial reads (even though it may positively impact the performance of the sum of the individual reads). In practice, caching almost always involves some form of buffering, while strict buffering does not involve caching.
A buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices. Thus, addressable memory is used as an intermediate stage. Additionally, such a buffer may be feasible when a large block of data is assembled or disassembled (as required by a storage device), or when data may be delivered in a different order than that in which it is produced. Also, a whole buffer of data is usually transferred sequentially (for example to hard disk), so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer's latency as opposed to caching where the intent is to reduce the latency. These benefits are present even if the buffered data are written to the buffer once and read from the buffer once.
A cache also increases transfer performance. A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block. But the main performance-gain occurs because there is a good chance that the same data will be read from cache multiple times, or that written data will soon be read. A cache's sole purpose is to reduce accesses to the underlying slower storage. Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers.

</doc>
<doc id="6830" url="https://en.wikipedia.org/wiki?curid=6830" title="Columbus, Indiana">
Columbus, Indiana

Columbus is a city in and the county seat of Bartholomew County, Indiana, United States. The population was 44,061 at the 2010 census. In its built environment, the relatively small city has provided a unique place for noted Modern architecture. Located about 40 miles (64 km) south of Indianapolis, on the east fork of the White River, it is the state's 20th largest city. It is also the principal city of the Columbus, Indiana metropolitan statistical area, which encompasses all of Bartholomew County.
"National Geographic Traveler" ranked Columbus 11th on its historic destinations list in late 2008, describing the city as "authentic, unique, and unspoiled." Columbus won the national contest "America in Bloom" in 2006, and in 2004 it was named one of "The Ten Most Playful Towns" by "Nick Jr. Family Magazine". The July 2005 edition of "GQ" magazine named Columbus one of the "62 Reasons to Love Your Country". Columbus is the headquarters of the engine company Cummins, Inc.
History.
The land which is now Columbus was bought by General John Tipton and Luke Bonesteel in 1820. Tipton built a log cabin on Mount Tipton, a small hill overlooking White River and the surrounding flat, heavily forested and swampy valley. The town was known as Tiptonia, named in honor of Tipton. The town's name was changed to Columbus on March 20, 1821. General Tipton was upset by the name change and decided to leave the newly founded town. He later became the highway commissioner for the State of Indiana and was assigned to building a highway from Indianapolis, Indiana to Louisville, Kentucky. When the road reached Columbus, Tipton constructed the first bypass road ever built; it detoured south around the west side of Columbus en route to Seymour.
Joseph McKinney was the first to plot the town of Columbus, but no date was recorded. 
It was recorded for years in the local history books that the land on which Columbus sits was donated by General Tipton; however, a deed purporting to show a sale of the land was acquired in 2003 by Historic Columbus Indiana. The deed indicated that General Tipton actually sold the land.
A ferry was established to avoid crossing both the Flatrock and Driftwood rivers, which join only a short distance above the site of the ferry. This became a village of three or four log cabins, and a store was added in 1821. Later that year, Bartholomew County was organized by an act of the State Legislature and named to honor the famous Hoosier militiaman, General Joseph Bartholomew. Columbus was incorporated on June 28, 1864.
The first railroad in Indiana reached Columbus from Madison, Indiana in 1844. This eventually became the Madison branch of the Pennsylvania Railroad. The railroad fostered the growth of the community into one of the largest communities in Indiana, and three more railroads reached the city by 1850.
Columbus is host to the oldest theater in Indiana, The Crump Theatre, which was built in 1889 by John Crump. Today the building is in the Columbus Historic District and an all-ages venue with occasional musical performances. Columbus was host to the oldest continually operated bookstore in Indiana, Cummins Bookstore, which began operations in 1892 and closed in late 2007.
The Irwin Union Bank building was built in 1954. It was designated a National Historic Landmark by the National Park Service in 2001 in recognition of its unique architecture. The building consists of a one-story bank structure adjacent to a three-story office annex. A portion of the office annex was built along with the banking hall in 1954. The remaining larger portion, designed by Kevin Roche John Dinkeloo and Associates, was built in 1973. Eero Saarinen designed the bank building with its glazed hall to be set off against the blank background of its three-story brick annex. Two steel and glass vestibule connectors lead from the north side of this structure to the annex. The building was designed to distance the Irwin Union Bank from traditional banking architecture, which mostly echoed imposing, neoclassical style buildings of brick or stone. Tellers were behind iron bars and removed from their customers. Saarinen worked to develop a building that would welcome customers rather than intimidate them.
Columbus has been home to many manufacturing companies, including Noblitt-Sparks Industries (which built radios under the Arvin brand in the 1930s) and Arvin Industries, now Meritor, Inc. After merging with Meritor Automotive on July 10, 2000, the headquarters of the newly created ArvinMeritor Industries was established in Troy, Michigan, the home of parent company, Rockwell International. It was announced in February 2011 that the company name would revert to Meritor, Inc. Cummins, Inc. is by far the region's largest employer, and the Infotech Park accounts for a sizable number of research jobs in Columbus proper. Just south of Columbus are the North American headquarters of Toyota Material Handling, U.S.A., Inc., the world's largest material handling (forklift) manufacturer. Other notable industries include architecture, a discipline for which Columbus is famous worldwide. The late J. Irwin Miller (then president and chairman of Cummins Engine Company) launched the Cummins Foundation, a charitable program that helps subsidize a large number of architectural projects throughout the city by up-and-coming engineers and architects.
Early in the 20th century, Columbus also was home to a number of pioneering car manufacturers, including Reeves, which produced the unusual four-axle Octoauto and the twin rear-axle Sextoauto, both around 1911.
Because Columbus is far enough from Indianapolis, it benefits tremendously from nearby commuters who recognize Columbus as a major city in its own right. Nearly 19,000 workers commute into the city from the surrounding townships and villages. In recent years city officials have explored ways to revitalize the city and return Columbus to the days when Miller's architectural innovation made it one of the most envied cities in the US. Economic development, widespread beautification innovations, various tax incentives, and increased law enforcement have helped Columbus overcome what some considered a slump during the 1980s and 1990s.
In addition to the Columbus Historic District and Irwin Union Bank, the Bartholomew County Courthouse, Columbus City Hall, First Baptist Church, First Christian Church, Haw Creek Leather Company, Mabel McDowell Elementary School, McEwen-Samuels-Marr House, McKinley School, Miller House, North Christian Church, and The Republic are listed on the National Register of Historic Places.
Geography.
Columbus is located at (39.213998, −85.911056). The Driftwood and Flatrock Rivers converge at Columbus to form the East Fork of the White River.
According to the 2010 census, Columbus has a total area of , of which (or 98.62%) is land and (or 1.38%) is water.
Airport.
Columbus is served by the Columbus Municipal Airport (KBAK). It is located approximately three miles north of Columbus. The airport handles approximately 40,500 operations per year, with roughly 87% general aviation, 4% air taxi, 8% military and <1% commercial service. The airport has two concrete runways; a 6,401 foot runway with approved ILS and GPS approaches (Runway 5-23) and a 5,001 foot crosswind runway, also with GPS approaches, (Runway 14-32).
Demographics.
2010 census.
As of the census of 2010, there were 44,061 people, 17,787 households, and 11,506 families residing in the city. The population density was . There were 19,700 housing units at an average density of . The racial makeup of the city was 86.9% White, 2.7% African American, 0.2% Native American, 5.6% Asian, 0.1% Pacific Islander, 2.5% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 5.8% of the population.
There were 17,787 households of which 33.5% had children under the age of 18 living with them, 48.5% were married couples living together, 11.7% had a female householder with no husband present, 4.5% had a male householder with no wife present, and 35.3% were non-families. 29.7% of all households were made up of individuals and 11.5% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.00.
The median age in the city was 37.1 years. 25.2% of residents were under the age of 18; 8.1% were between the ages of 18 and 24; 27.3% were from 25 to 44; 24.9% were from 45 to 64; and 14.4% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.
2000 census.
As of the census of 2000, there were 39,059 people, 15,985 households, and 10,566 families residing in the city. The population density was 1,505.3 people per square mile (581.1/km²). There were 17,162 housing units at an average density of 661.4 per square mile (255.3/km²). The racial makeup of the city was 91.32% White, 2.71% Black or African American, 0.13% Native American, 3.23% Asian, 0.05% Pacific Islander, 1.39% from other races, and 1.19% from two or more races. 2.81% of the population were Hispanic or Latino of any race.
There were 15,985 households out of which 31.8% had children under the age of 18 living with them, 51.9% were married couples living together, 11.0% had a female householder with no husband present, and 33.9% were non-families. 29.1% of all households were composed of individuals and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.39, and the average family size was 2.94.
In the city the population was spread out with 25.7% under the age of 18, 8.0% from 18 to 24 years, 29.5% from 25 to 44 years, 23.0% from 45 to 64 years, and 13.7% over the age of 65. The median age was 36 years. There were 92.8 males for every 100 females and 89.6 males for every 100 females over age 18.
The median income for a household in the city was $41,723, and the median income for a family was $52,296. Males had a median income of $40,367 versus $24,446 for females, and the per capita income was $22,055. About 6.5% of families and 8.1% of the population were below the poverty line, including 9.7% of those under age 18 and 8.8% of those age 65 or over.
Arts and culture.
Columbus is a city known for its architecture. J. Irwin Miller, 2nd CEO and a nephew of a Co-Founder of Cummins Inc., the Columbus-headquartered diesel engine manufacturer, instituted a program in which the Cummins company paid the architects' fee, provided the client selected a firm from a list compiled by Miller. The plan was initiated with public schools and was so successful that Miller decided to defray the design costs of fire stations, public housing, and other community structures. The high number of notable public buildings and sculptures in the Columbus area, designed by such individuals as Eero Saarinen, I.M. Pei, Robert Venturi, Cesar Pelli, and Richard Meier have led to Columbus earning the nickname "Athens of the Prairie." Six buildings, built between 1942 and 1965, are National Historic Landmarks, and approximately 60 other buildings sustain the Bartholomew County seat's reputation as a showcase of modern architecture. National Geographic Magazine once devoted an entire article to the town's architecture.
Sports.
The Indiana Diesels of the Premier Basketball League play their home games at the gymnasium at Ceraland Park, with plans to move to a proposed downtown sports complex in the near future.
Columbus also boasts a roller derby league, the Terrorz of Tiny Towns. Established in 2010, this league hosts weekly practices at Columbus Skateland. The town also has two cricket teams, both which play under the name of Columbus Indiana Cricket Club; their home ground is at Ceraland park.
Parks and recreation.
Columbus boasts over of parks and green space and over 20 miles of People Trails. These amenities, in addition to several athletic and community facilities, including Donner Aquatic Center, Lincoln Park Softball Complex, Hamilton Center Ice Arena, Clifty Park, Foundation for Youth/Columbus Gymnastics Center and The Commons, are managed and maintained by the Columbus Parks and Recreation Department.
Government.
Columbus uses the Mayor-Council form of government. The council consists of seven members. Five are elected from one of five wards the other two are elected at-large. The Mayor is elected in a citywide vote. The current mayor is Jim Lienhoop.
Notable residents.
This is a list of notable people who were born in, or who currently live, or have lived in Columbus.
Entertainment.
There is currently one mainstream movie theater, AMC 12, which shows new movies, and the Yes! Cinema shows independent, older, and foreign films from its location downtown. The landmark Crump Theatre featured occasional local performances, such as comedy and local rock or punk bands, and occasional theatrical performances.
There is a canoe livery, Blue's Canoes, that offers canoeing, rafting and kayaking trips on the nearby Driftwood River.
Education.
The Bartholomew Consolidated School Corporation operates public schools.

</doc>
<doc id="6834" url="https://en.wikipedia.org/wiki?curid=6834" title="List of computer scientists">
List of computer scientists

This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.
Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.

</doc>
<doc id="6836" url="https://en.wikipedia.org/wiki?curid=6836" title="Cultural production and nationalism">
Cultural production and nationalism

Literature, visual arts, music, and scholarship have complex relationships with ideological forces.
The 19th Century.
In the 19th century nationalism was an especially potent influence on all of these fields. To summarize, every established national group used cultural productions to assert and strengthen a sense of national unity and destiny; less politically consolidated groups, especially those pursuing the goal of nationhood, used them in the same ways, though often with a note of determination that makes them easier to see from our contemporary point of reference.
Natural admiration for excellence and justifiable pride in a predecessor's achievements is sometimes difficult to sort out from other intentions. Dante was a great poet, the Societa Dantesca Italiana did great work in editing and publishing a usable and affordable text, but the "Divine Comedy" was certainly used by the newly unified Italian government (see History of Italy) to encourage a more homogeneous, Tuscan-influenced dialect for the whole peninsula (see Italian language).
The Academy.
This relationship between ideology and serious work is particularly ambiguous in the academic fields of historical importance. Much as 19th century science is often treated as the inventor of conceptions of evolution and race which had serious negative political and social consequences, many 19th century historians pursued what they intended as reasonably objective research projects in the history of their own and other regions either to end by themselves using the results to support nationalistic goals or to see their work used that way by others. 
More politically consolidated nations sponsored historical research projects which produced results of permanent value - such as the "Monumenta Germaniae Historica" ("Monuments of German History") project. The "MGH" is a vast series (it runs to hundreds of volumes and is still publishing) of edited primary source material essential for scholarly work on late Antiquity and the Middle Ages. However, the term "German" in the title was interpreted in the broadest possible sense, and its initial royal patronage made the connection clear between a perceived unity of Germanness in history and 19th century Germanness.

</doc>
<doc id="6839" url="https://en.wikipedia.org/wiki?curid=6839" title="CRESU experiment">
CRESU experiment

The CRESU experiment () is an experiment investigating chemical reactions taking place at very low temperatures.
The technique involves the expansion of a gas or mixture of gases through a de Laval nozzle from a high pressure reservoir into a vacuum chamber. As it expands, the nozzle collimates the gas into a uniform supersonic beam that is essentially collision free and has a temperature that, in the centre of mass frame, can be significantly below that of the reservoir gas. Each nozzle produces a characteristic temperature. This way, any temperature between room temperature and about 10K can be achieved. There are relatively few CRESU apparatuses in existence for the simple reason that the gas throughput and pumping requirements are huge, which makes them expensive to run. Two of the leading centres have been the University of Rennes (France) and the University of Birmingham (UK). A more recent development has been a pulsed version of the CRESU, which requires far less gas and therefore smaller pumps. One might well ask why we should use such a complex method for producing low temperature gases when they could be produced much more easily using liquid helium. The answer is simple: most species have a negligible vapour pressure at such low temperatures and this means that they quickly condense on the sides of the apparatus. Essentially, the CRESU technique provides a "wall-less flow tube," which allows the kinetics of gas phase reactions to be investigated at much lower temperatures than otherwise possible.
Chemical kinetics experiments can then be carried out in a "pump-probe" fashion using a laser to initiate the reaction (for example by preparing one of the reagents by photolysis of a precursor), followed by observation of that same species (for example by laser-induced fluorescence) after a known time delay. The fluorescence signal is captured by a photomultiplier a known distance downstream of the de Laval nozzle. The time delay can be varied up to the maximum corresponding to the flow time over that known distance. By studying how quickly the reagent species disappears in the presence of differing concentrations of a (usually stable) co-reagent species the reaction rate constant at the low temperature of the CRESU flow can be determined.
Reactions studied by the CRESU technique typically have no significant activation energy barrier. In the case of neutral-neutral reactions (i.e., not involving any charged species, ions), these type of barrier-free reactions usually involve free radical species such as molecular oxygen (O2), the cyanide radical (CN) or the hydroxyl radical (OH). The energetic driving force for these reactions is typically an attractive long range intermolecular potential.
CRESU experiments have been used to show deviations from Arrhenius kinetics at low temperatures: as the temperature is reduced, the rate constant actually increases. They can explain why chemistry is so prevalent in the interstellar medium, where many different polyatomic species have been detected (by radio astronomy), but where temperatures are so low that conventional wisdom might suggest that chemical reactions do not occur.

</doc>
<doc id="6840" url="https://en.wikipedia.org/wiki?curid=6840" title="Cygwin">
Cygwin

Cygwin ( ) is a Unix-like environment and command-line interface for Microsoft Windows. Cygwin provides native integration of Windows-based applications, data, and other system resources with applications, software tools, and data of the Unix-like environment. Thus it is possible to launch Windows applications from the Cygwin environment, as well as to use Cygwin tools and applications within the Windows operating context.
Cygwin consists of two parts: a dynamic-link library (DLL) as an API compatibility layer providing a substantial part of the POSIX API functionality, and an extensive collection of software tools and applications that provide a Unix-like look and feel.
Cygwin was originally developed by Cygnus Solutions, which was later acquired by Red Hat. It is free and open source software, released under the GNU General Public License version 3. Today it is maintained by employees of Red Hat, NetApp and many other volunteers.
Description.
Cygwin consists of a library that implements the POSIX system call API in terms of Win32 system calls, a GNU development toolchain (including GCC and GDB) to allow software development, and running of a large number of application programs equivalent to those on Unix systems. Programmers have ported many Unix, GNU, BSD and Linux programs and packages to Cygwin, including the X Window System, K Desktop Environment 3, GNOME, Apache, and TeX. Cygwin permits installing inetd, syslogd, sshd, Apache, and other daemons as standard Windows services, allowing Microsoft Windows systems to emulate Unix and Linux servers.
Cygwin programs are installed by running Cygwin's "setup" program, which downloads the necessary program and feature package files from repositories on the Internet. Setup can install, update, and remove programs and their source code packages. A complete installation will take in excess of 36 GB of hard disk space, but usable configurations may require as little as 1 or 2 GB.
Efforts to reconcile concepts that differ between Unix and Windows systems include:
The version of gcc that comes with Cygwin has various extensions for creating Windows DLLs, specifying whether a program is a windowing or console-mode program, adding resources, etc. Support for compiling programs that do not require the POSIX compatibility layer provided by the Cygwin DLL used to be included in the default codice_13, but is provided by cross-compilers contributed by the MinGW-w64 project.
Cygwin is used heavily for porting many popular pieces of software to the Windows platform. It is used to compile Sun Java, OpenOffice.org, LibreOffice, and even web server software like Lighttpd and Hiawatha.
Red Hat normally licenses the Cygwin library under the GNU General Public License version 3 with an exception to allow linking to any free and open source software whose license conforms to the Open Source Definition. Red Hat also sells commercial licenses to those who wish to redistribute programs that use the Cygwin library under proprietary terms.
History.
Cygwin began in 1995 as a project of Steve Chamberlain, a Cygnus engineer who observed that Windows NT and 95 used COFF as their object file format, and that GNU already included support for x86 and COFF, and the C library newlib. He thought it would be possible to retarget GCC and produce a cross compiler generating executables that could run on Windows. This proved practical and a prototype was quickly developed.
The next step was to attempt to bootstrap the compiler on a Windows system, requiring sufficient emulation of Unix to let the GNU configure shell script run. A Bourne shell-compatible command interpreter, such as bash, was needed and in turn a fork system call emulation and standard input/output. Windows includes similar functionality, so the Cygwin library just needed to provide a POSIX-compatible application programming interface (API) and properly translate calls and manage private versions of data, such as file descriptors.
Initially, Cygwin was called gnuwin32 (not to be confused with the current GnuWin32 project). The name was changed to Cygwin32 to emphasize Cygnus' role in creating it. When Microsoft registered the trademark Win32, the 32 was dropped to simply become Cygwin.
By 1996, other engineers had joined in, because it was clear that Cygwin would be a useful way to provide Cygnus' embedded tools hosted on Windows systems (the previous strategy had been to use DJGPP). It was especially attractive because it was possible to do a three-way cross-compile, for instance to use a hefty Sun workstation to build, say, a Windows-x-MIPS cross-compiler, which was faster than using the PC at the time. In 1999, Cygnus offered Cygwin 1.0 as a commercial product of interest in its own right although subsequent versions have not been released, instead relying on continued open source releases.
Geoffrey Noer was the project lead from 1996 to 1998. Christopher Faylor was the project lead from 1998 to mid-2014. Corinna Vinschen became co-lead since early 2004 when Faylor left Red Hat and has been lead since mid-2014, when Faylor withdrew from active participation in the project.
Features.
Cygwin's default package selection is fairly small, containing little more than the bash shell and the core file manipulation utilities expected of a Unix command line. Additional packages are available as optional installs from within Cygwin's package manager ("setup-x86.exe" – 32bit & "setup-x86_64.exe" – 64bit). These include (among many others):
The Cygwin/X project contributes an implementation of the X Window System that allows graphical Unix programs to display their user interfaces on the Windows desktop. This can be used with both local and remote programs. Cygwin ships with a fairly small number of X applications, for example:
In addition to the low-level Xlib/XCB libraries for developing X applications, Cygwin also ships with various higher-level and cross-platform GUI frameworks, including GTK+ and Qt.
The Cygwin Ports project provides many additional packages that are not available in the Cygwin distribution itself. Examples include GNOME and K Desktop Environment 3 as well as the MySQL database and the PHP scripting language.
Alternative Windows/Unix integration tools.
Several open-source and proprietary alternatives provide simultaneous access to both Windows and UNIX environments on the same hardware.
Toolsets like Microsoft Windows Services for UNIX (SFU), UWIN, MKS Toolkit for Enterprise Developers and Hamilton C shell also aim to provide a Unix-like user- and development-environment. They implement at least a shell and a set of the most popular utilities. Most include the familiar GNU and/or Unix development tools, including make, yacc, lex and a cc command which acts a wrapper around a supported C compiler. SFU also includes the GCC compiler.
MinGW provides a native software port of the GCC to Microsoft Windows, along with a set of freely-distributable import libraries and header files for the Windows API. MinGW allows developers to create native Microsoft Windows applications. In addition, a component of MinGW known as MSYS ("Minimal SYStem"), which derives from Cygwin version 1.3.3, provides a minimal Unix-like shell environment including bash and a selection of POSIX tools sufficient to enable autoconf scripts to run.
Numerous virtualization solutions provide x86 platform virtualization to run Windows and Unix-like operating systems simultaneously on the same hardware, but without the integration of the environments that Cygwin provides. Some, like VirtualBox and VMware Player run on Windows and Linux hosts and can run many other operating systems. Cooperative Linux (abbreviated "coLinux") runs a full, but modified Linux kernel like a driver under Windows, effectively making Windows and Linux two coroutines, using cooperative multitasking to switch between them.
Winelib, a part of the Wine project, is the inverse of Cygwin – it is a free and open-source compatibility layer for Unix-like operating systems on the x86 or x86-64 architecture that can allow programs written for Microsoft Windows to run on Unix-like operating systems. Unlike Cygwin, which requires "You rebuild your application from source if you want it to run on Windows", the full Wine product supports executing unmodified Windows binaries.

</doc>
<doc id="6845" url="https://en.wikipedia.org/wiki?curid=6845" title="Corinth">
Corinth

Corinth (; , "Kórinthos", ) is a city and former municipality in Corinthia, Peloponnese, Greece. Since the 2011 local government reform it is part of the municipality of Corinth, of which it is the seat and a municipal unit. It is the capital of Corinthia.
It was founded as Nea Korinthos or New Corinth (Νέα Κόρινθος) in 1858 after an earthquake destroyed the existing settlement of Corinth, which had developed in and around the site of ancient Corinth.
Geography.
Located about west of Athens, Corinth is surrounded by the coastal townlets of (clockwise) Lechaio, Isthmia, Kechries, and the inland townlets of Examilia and the archaeological site and village of ancient Corinth. Natural features around the city include the narrow coastal plain of Vocha, the Corinthian Gulf, the Isthmus of Corinth cut by its canal, the Saronic Gulf, the Oneia Mountains, and the monolithic rock of Acrocorinth, where the medieval acropolis was built.
History.
Corinth derives its name from Ancient Corinth, a city-state of antiquity. In 1858, the old city, now known as Archaia Korinthos (Αρχαία Κόρινθος), located SW of the modern city, was totally destroyed by a magnitude 6.5 earthquake. Nea Korinthos or New Corinth was then built a few kilometers away on the coast of the Gulf of Corinth. A magnitude 6.3 earthquake in 1928 devastated the new city, which was then rebuilt on the same site. It was rebuilt again after a great fire in 1933.
Demographics.
The Municipality of Corinth (Δήμος Κορινθίων) had a population of 58,192 according to the 2011 census, the second most populous municipality in the Peloponnese Region after Kalamata. The municipal unit of Corinth had 38,132 inhabitants, of which Corinth itself had 30,176 inhabitants, placing it in third place behind Kalamata and Tripoli among the cities of the Peloponnese Region.
The municipal unit of Corinth (Δημοτική ενότητα Κορινθίων) includes apart from Corinth proper the town of Archaia Korinthos (2,198 inhabitants in 2011), the town of Examilia (2,905 inhabitants), and the smaller settlements of Xylokeriza (1,316 inhabitants) and Solomos (817 inhabitants).
Economy.
Industry.
Corinth is a major industrial hub at a national level. Corinth Refineries is the largest oil refining Industrial complex in Europe. Copper cables, petroleum products, leather, medical equipment, marble, gypsum, ceramic tiles, salt, mineral water and beverages, meat products, and gums are produced nearby. , a period of deindustrialization has commenced as a large pipework complex, a textile factory and a meat packing facility diminished their operations.
Transport.
Roads.
Corinth is a major road hub. The A7 toll motorway for Tripoli and Kalamata, (and Sparta via A71 toll), branches off the A8/European route E94 toll motorway from Athens at Corinth. Corinth is the main entry point to the Peloponnesian peninsula, the southernmost area of continental Greece.
Bus.
KTEL Korinthias provides intercity bus service in the peninsula and to Athens via the Isthmos station southeast of the city center. Local bus service is also available.
Railways.
The city has been connected to the Proastiakos, the Athens suburban rail network, since 2005, when the new Corinth railway station was completed.
Port.
The port of Corinth, located north of the city centre and close to the northwest entrance of the Corinth Canal, at 37 56.0’ N / 22 56.0’ E, serves the local needs of industry and agriculture. It is mainly a cargo exporting facility.
It is an artificial harbour (depth approximately , protected by a concrete mole (length approximately 930 metres, width 100 metres, mole surface 93,000 m2). A new pier finished in the late 1980s doubled the capacity of the port. The reinforced mole protects anchored vessels from strong northern winds.
Within the port operates a customs office facility and a Hellenic Coast Guard post. Sea traffic is limited to trade in the export of local produce, mainly citrus fruits, grapes, marble, aggregates and some domestic imports. The port operates as a contingency facility for general cargo ships, bulk carriers and ROROs, in case of strikes at Piraeus port.
Ferries.
There was formerly a ferry link to Catania, Sicily and Genoa in Italy.
Canal.
The Corinth Canal, carrying ship traffic between the western Mediterranean Sea and the Aegean Sea, is about east of the city, cutting through the Isthmus of Corinth that connects the Peloponnesian peninsula to the Greek mainland, thus effectively making the former an island. The builders dug the canal through the Isthmus at sea level; no locks are employed. It is in length and only wide at its base, making it impassable for most modern ships. It now has little economic importance.
The canal was mooted in classical times and an abortive effort was made to build it in the 1st century AD. Construction finally got underway in 1881 but was hampered by geological and financial problems that bankrupted the original builders. It was completed in 1893, but due to the canal's narrowness, navigational problems and periodic closures to repair landslips from its steep walls, it failed to attract the level of traffic anticipated by its operators. It is now used mainly for tourist traffic.
Sport.
The city's association football team is Korinthos F.C. ("Π.Α.E. Κόρινθος"), established in 1999 after the merger of Pankorinthian Football Club ("Παγκορινθιακός") and Corinth Football Club ("Κόρινθος"). During the 2006-2007 season, the team played in the Greek Fourth Division's Regional Group 7. The team went undefeated that season and it earned the top spot. This granted the team a promotion to the Gamma Ethnikí (Third Division) for the 2007-2008 season. For the 2008-2009 season, Korinthos F.C. competed in the Gamma Ethniki (Third Division) southern grouping.
The city's ROBLOX football team is the Corinth Spartans, established in 2013 after the creation of the Old Football League. The team is currently ranked in the Old Western Conference. The team currently has a 5-3 playoff record with two OWC titles. They won the Victory Bowl in season three.
Twin towns — sister cities.
Corinth, Mississippi
Corinth is twinned with:
Other locations named after Corinth.
Due to its ancient history and the presence of St. Paul the Apostle in Corinth some locations all over the world have been named Corinth.

</doc>
<doc id="6846" url="https://en.wikipedia.org/wiki?curid=6846" title="Colossae">
Colossae

Colossae (; Greek: Κολοσσαί) was an ancient city of Phrygia in Asia Minor, and was the location of a Christian community to which the Apostle Paul addressed a canonically accepted epistle (letter), which is known for its content's exaltation of the supremacy of Chistianity's namesake. Writing in the 4th century BC, Xenophon refers to Colossae as one of six large cities of Phrygia. It was populated by peoples of Greek and Hebrew origin (Antiochus the Great having relocated there, two thousand Jewish families from Babylonia and Mesopotamia), as well as other cultures and ethncities, as it was an early center of trade given its location on the Lycus (a tributary of the Maeander River) and its position near the great military and commercial road from Ephesus to the Euphrates. It was situated 10 miles southeast of Laodicea, 13 miles from the ancient city of Heiropolis, and 3 miles rom Mount Cadmus, at the head of a gorge. Commerce of the city included trade in wool—the dyed wool "collossinus" was named for the place—and in the products of weaving and other trades. It was also known for its fusion of religious influences (syncretism), which included Jewish, Gnostic, and pagan influences that in the first century AD were described as an angel-cult (a matter addressed by the Pauline letter). The city was decimated by an earthquake in the 60s AD, rebuilt independent of the support of Rome, overrun by the Saracens in the 7th and 8th centuries AD, and then destroyed, ultimately, by the Turks in the 12th century, with the remnant of its population relocating, among other places, to nearby Chonae. As of 2015, it had never been excavated, though plans are reported for an Australian led expedition to the site.
Despite a treacherously ambiguous cartography and history, Colossae has been clearly distinguished in modern research from nearby "Chonae" (Χῶναι), now called Honaz, with what remains of the buried ruins of the mound associated with Colossae lying a few to the north of Chonae/Honaz.
Location and geography.
Colossae was located in Phrygia, in Asia Minor. Writing in the 4th century BC, Xenophon refers to Colossae as one of six large cities of Phrygia. It was an early center of trade, given its location on the Lycus (a tributary of the Maeander River) and its position near the great military and commercial road from Ephesus to the Euphrates. It was situated 10 miles southeast of Laodicea, 13 miles from the ancient city of Heiropolis, and 3 miles rom Mount Cadmus, at the head of a gorge.
Despite a treacherously ambiguous cartography and history, Colossae has been clearly distinguished in modern research from nearby "Chonae" (Χῶναι), now called Honaz, with what remains of the buried ruins of Colossae ("the mound") lying 3 km to the north of Chonae/Honaz.
Pre-Pauline and Pauline characteristics.
Colossae was populated by peoples of Greek and Hebrew origin. Antiochus the Great is said to have relocated two thousand Jewish families from Babylonia and Mesopotamia to the city, and other cultures and ethncities were present as well.
Commerce of the city included trade in wool—the dyed wool "collossinus" was named for the place—and in the products of weaving and other trades. It was also known for its fusion of religious influences (syncretism), which included Jewish, Gnostic, and pagan influences that in the first century AD were described as an angel-cult.
History.
Before the Pauline period.
Some further highlights regarding the history of Colossae include:
The Pauline period.
Colossae was the location of a Christian community to which the Apostle Paul addressed a canonically accepted epistle (letter), which is known for its content's exaltation of the supremacy of Chistianity's namesake. One aim of the letter was to address the challenges that the community faced in its context of the syncretistic Gnostic religions that were developing in Asia minor.
Judging from the Letter to the Colossians, Epaphras was a person of some importance in the Christian community there (; ), and tradition presents him as its first bishop. It does not appear from his Epistle to the Colossians that St. Paul had visited the city, for the epistle only speaks of him having heard of their faith () and since he tells Philemon of his hope to visit it upon being freed from prison (see ). Tradition also gives Philemon as the second bishop of the see.
The first historically documented bishop is Epiphanius, who was not personally at the Council of Chalcedon, but whose metropolitan bishop Nunechius of Laodicea, the capital of the Roman province of Phrygia Pacatiana signed the acts on his behalf.
Decimation and destruction.
The city was decimated by an earthquake in the 60s AD, and was rebuilt independent of the support of Rome. The city was later overrun by the Saracens in the 7th and 8th centuries AD. Colossae was destroyed, ultimately, by the Turks in the 12th century, with the remnant of its population relocating, among other places, to nearby Chonae.
Modern legacy and study.
As of 2015, it had never been excavated, though plans are reported for an Australian led expedition to the site.

</doc>
<doc id="6848" url="https://en.wikipedia.org/wiki?curid=6848" title="Charge of the Goddess">
Charge of the Goddess

The Charge is the promise of the Goddess (who is embodied by the High Priestess) to all witches that she will teach and guide them. It has been called "perhaps the most important single theological document in the neo-Pagan movement". It is used not only in Wicca, but as part of the foundational documents of the Reclaiming (neopaganism) tradition of witchcraft co-founded by Starhawk.
Several versions of the Charge exist, though they all have the same basic premise, that of a set of instructions given by the Great Goddess to her worshippers. The best-known version is that compiled by Gerald Gardner. This version, titled "Leviter Veslis"or "Lift Up the Veil", includes material paraphrased from works by Aleister Crowley, primarily from Liber AL (The Book of the Law, particularly from Ch 1, spoken by Nuit, the Star Goddess), and from Liber LXV (The Book of the Heart Girt with a Serpent) and from Crowley's essay "The Law of Liberty", thus linking modern Wicca to the cosmology and revelations of Thelema. It has been shown that Gerald Gardner's book collection which was acquired by Ripley's Believe It or Not! included a copy of Crowley's The Blue Equinox" (1919) which includes all of the Crowley quotations transferred by Gardner to the Charge of the Goddess. [http://www.ceisiwrserith.com/wicca/charge.htm]
There are also two versions written by Doreen Valiente in the mid-1950s, after her 1953 Wiccan initiation. The first was a poetic paraphrase which eliminated almost all the material derived from Leland and Crowley. The second was a prose version which is contained within the traditional Gardnerian Book of Shadows and more closely resembles Gardner's "Leviter Veslis" version of 1949.
Several different versions of a Wiccan "Charge of the God" have since been created to mirror and accompany the "Charge of the Goddess".
Themes.
The opening paragraph names a collection of goddesses, some derived from Greek or Roman mythology, others from Celtic or Arthurian legends, affirming a belief that these various figures represent a single Great Mother:
This theme echoes the ancient Roman belief that the Goddess Isis was known by ten thousand names and also that the Goddess still worshipped today by Wiccans and other neopagans is known under many guises but is in fact one universal divinity.
The second paragraph is largely derived and paraphrased from the words that Aradia, the messianic daughter of Diana, speaks to her followers in Charles Godfrey Leland's 1899 book "Aradia, or the Gospel of the Witches" (London: David Nutt; various reprints). The third paragraph is largely written by Doreen Valiente,[http://www.ceisiwrserith.com/wicca/charge.htm] with a significant content of phrases loosely from "The Book of the Law" and "The Book of the Heart Girt with the Serpent" by Aleister Crowley.
The charge affirms that "all" acts of love and pleasure are sacred to the Goddess e.g.
"Let my worship be within the heart that rejoices,
for behold, all acts of love and pleasure are my rituals.
Therefore, let there be beauty and strength,
power and compassion, honor and humility,
mirth and reverence within you."
History.
Ancient Precedents.
In book eleven, chapter 47 of Apuleius's "The Golden Ass", Isis delivers what Ceisiwr Serith calls "essentially a charge of a goddess". This is rather different from the modern version known in Wicca, though they have the same premise, that of the rules given by a great Mother Goddess to her faithful.
The Charge of the Goddess is also known under the title "Leviter Veslis". This has been identified by the historian Ronald Hutton, cited in an article by Roger Dearnsley "The Influence of Aleister Crowley on "Ye Bok of Ye Art Magical", as a piece of medieval ecclesiastical Latin used to mean "lifting the veil." 
Origins of Wiccan Charge.
The earliest known Wiccan version is found in a document dating from the late 1940s, Gerald Gardner's ritual notebook titled "Ye Bok of Ye Art Magical" (formerly in the collection of Ripley's International, Toronto).The oldest identifiable source contained in this version is the final line, which is traceable to the 17th century "Centrum Naturae Concentratum" of Alipili (or Ali Puli). This version also draws extensively from Charles Godfrey Leland's "Aradia, or the Gospel of the Witches" (1899) and other modern sources, particularly from the works of Aleister Crowley.
It is believed to have been compiled by Gerald Gardner or possibly another member of the New Forest coven. Gardner intended his version to be a theological statement justifying the Gardnerian sequence of initiations. Like the Charge found in Freemasonry, where the charge is a set of instructions read to a candidate standing in a temple, the Charge of the Goddess was intended to be read immediately before an initiation.
Valiente felt that the influence of Crowley on the Charge was too obvious, and she did not want "the Craft" ( a common term for Wicca) associated with Crowley. Gardner invited her to rewrite the Charge. She proceeded to do so, her first version being into verse.
The initial verse version by Doreen Valiente consisted of eight verses, the second of which was :
Valiente was unhappy with this version, saying that "people seemed to have some difficulty with this, because of the various goddess-names which they found hard to pronounce", and so she rewrote it as a prose version, much of which differs from her initial version, and is more akin to Gardner's version. This prose version has since been modified and reproduced widely by other authors.

</doc>
<doc id="6849" url="https://en.wikipedia.org/wiki?curid=6849" title="Cy Young">
Cy Young

Denton True "Cy" Young (March 29, 1867 – November 4, 1955) was an American Major League Baseball pitcher. During his 22-season baseball career (1890–1911), he pitched for five different teams. Young established numerous pitching records, some of which have stood for a century. Young compiled 511 wins, which is most in Major League history and 94 ahead of Walter Johnson who is second on the list. Young was elected to the National Baseball Hall of Fame in 1937.
One year after Young's death, the Cy Young Award was created to honor the previous season's best pitcher.
In addition to wins, Young still holds the major league records for most career innings pitched (7,356), most career games started (815), and most complete games (749). He also retired with 316 losses, the most in MLB history. Young's 76 career shutouts are fourth all-time. He also won at least 30 games in a season five times, with ten other seasons of 20 or more wins. In addition, Young pitched three no-hitters, including the third perfect game in baseball history, first in baseball's "modern era". In 1999, 88 years after his final major league appearance and 44 years after his death, editors at "The Sporting News" ranked Young 14th on their list of "Baseball's 100 Greatest Players". That same year, baseball fans named him to the Major League Baseball All-Century Team.
Young's career started in 1890 with the Cleveland Spiders. After eight years with the Spiders, Young was moved to St. Louis in 1899. After two years there, Young jumped to the newly created American League, joining the Boston franchise. He was traded back to Cleveland in 1909, before spending the final two months of his career with the Boston Rustlers. After his retirement, Young went back to his farm in Ohio, where he stayed until his death at age 88 in 1955.
Early life.
Cy Young was the oldest child born to McKinzie Young, Jr. and German American Nancy Mottmiller. The couple had four more children: Jesse Carlton, Alonzo, Ella, and Anthony. When the couple married, McKinzie's father gave him the of farm land he owned. Young was born in Gilmore, a tiny farming community located in Washington Township, Tuscarawas County, Ohio. He was christened Denton True Young. Some sources later, and even today, list his middle name erroneously as "Tecumseh", apparently as a result of being nicknamed "The Chief" by teammates.
He was raised on one of the local farms and went by the name Dent Young in his early years. Young was also known as "Farmer Young" and "Farmboy Young". Young stopped his formal education after he completed the sixth grade so he could help out on the family's farm. In 1885, Young moved with his father to Nebraska, and in the summer of 1887, they returned to Gilmore.
Young played for many amateur baseball leagues during his youth, including a "semi-pro" Carrollton team in 1888. Young pitched and played second base. The first box score known containing the name Young came from that season. In that game, Young played first base and had three hits in three at-bats. After the season, Young received an offer to play for the minor league Canton team, which started Young's professional career.
Professional career.
Before Major League Baseball.
Young began his professional career in 1889 with the Canton, Ohio team of the Tri-State League, a professional minor league. During his tryout, Young impressed the scouts, recalling years later, "I almost tore the boards off the grandstand with my fast ball." Cy Young's nickname came from the fences that he had destroyed using his fastball. The fences looked like a cyclone had hit them. Reporters later shortened the name to "Cy", which became the nickname Young used for the rest of his life. During Young's one year with the Canton team, he won 15 games and lost 15.
Franchises in the National League, the major professional baseball league at the time, wanted the best players available to them. Therefore, in 1890, Young signed with the Cleveland Spiders, a team which had moved from the American Association to the National League the previous year.
Cleveland Spiders.
On August 6, 1890, Young's major league debut, he pitched a three-hit shutout. While Young was on the Spiders, Chief Zimmer was his catcher more often than any other player. Bill James, a baseball statistician, estimated that Zimmer caught Young in more games than any other battery in baseball history.
Early on, Young established himself as one of the harder-throwing pitchers in the game. Bill James wrote that Zimmer often put a piece of beefsteak inside his baseball glove to protect his catching hand from Young's fastball. In the absence of radar guns, however, it is impossible to say just how hard Young actually threw. Young continued to perform at a high level during the 1890 season. On the last day of the season, Young won both games of a doubleheader. In the first weeks of Young's career, Cap Anson, the player-manager of the Chicago Colts spotted Young's ability. Anson told Spiders' manager Gus Schmelz, "He's too green to do your club much good, but I believe if I taught him what I know, I might make a pitcher out of him in a couple of years. He's not worth it now, but I'm willing to give you $1,000 ($ today) for him." Schmelz replied, "Cap, you can keep your thousand and we'll keep the rube."
Two years after Young's debut, the National League moved the pitcher's position back by . Since 1881, pitchers had pitched within a "box" whose front line was from home base, and since 1887 they had been compelled to toe the back line of the box when delivering the ball. The back line was away from home. In 1893, was added to the back line, yielding the modern pitching distance of . In the book "The Neyer/James Guide to Pitchers", sports journalist Rob Neyer wrote that the speed with which pitchers like Cy Young, Amos Rusie, and Jouett Meekin threw was the impetus that caused the move.
The 1892 regular season was a success for Young, who led the National League in wins (36), ERA (1.93), and shutouts (9). Just as many contemporary Minor League Baseball leagues operate today, the National League was using a split season format during the 1892 season. The Boston Beaneaters won the first-half title, and the Spiders won the second-half title, with a best-of-nine series determining the league champion. Despite the Spiders' second half run, the Beaneaters swept the series, five games to none. Young pitched three complete games in the series, but lost two decisions. He also threw a complete game shutout, but the game ended in a 0–0 tie.
The Spiders faced the Baltimore Orioles in the Temple Cup, a precursor to the World Series, in 1895. Young won three games in the series and Cleveland won the Cup, four games to one. It was around this time that Young added what he called a "slow ball" to his pitching repertoire to reduce stress on his arm. The pitch today is called a changeup.
In 1896, Young lost a no-hitter with two outs in the ninth inning when Ed Delahanty of the Philadelphia Phillies hit a single. On September 18, 1897, Young pitched the first no-hitter of his career in a game against the Cincinnati Reds. Although Young did not walk a batter, the Spiders committed four errors while on defense. One of the errors had originally been ruled a hit, but the Cleveland third baseman sent a note to the press box after the eighth inning, saying he had made an error, and the ruling was changed. Young later said, that, despite his teammate's gesture, he considered the game to be a one-hitter.
Shift to St. Louis.
Prior to the 1899 season, Frank Robison, the Spiders owner, bought the St. Louis Browns, thus owning two clubs simultaneously. The Browns were renamed the "Perfectos", and restocked with Cleveland talent. Just weeks before the season opener, most of the better Spiders players were transferred to St. Louis, including fellow pitcher Pete McBride and three future Hall of Famers: Young, Jesse Burkett, and Bobby Wallace. The roster maneuvers failed to create a powerhouse Perfectos team, as St. Louis finished fifth in both 1899 and 1900. Meanwhile, the depleted Spiders lost 134 games, the most in MLB history, before folding. Young spent two years with St. Louis, which is where he found his favorite catcher, Lou Criger. The two men were teammates for a decade.
Move to the American League.
In 1901, the rival American League declared major league status and set about raiding National League rosters. Young left St. Louis and joined the American League's Boston Americans for a $3,500 contract ($ today). Young would remain with the Boston team until 1909. In his first year in the American League, Young was dominant. Pitching to Criger, who had also jumped to Boston, Young led the league in wins, strikeouts, and ERA, thus earning the colloquial AL Triple Crown for pitchers. Young won almost 42% of his team's games in 1901, accounting for 33 of his team's 79 wins. In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.
The Boston Americans played the Pittsburgh Pirates in the first modern World Series in 1903. Young, who started Game One against the visiting Pirates, thus threw the first pitch in modern World Series history. The Pirates scored four runs in that first inning, and Young lost the game. Young performed better in subsequent games, winning his next two starts. He also drove in three runs in Game Five. Young finished the series with a 2–1 record and a 1.85 ERA in four appearances, and Boston defeated Pittsburgh, five games to three games.
After one-hitting Boston on May 2, 1904, Philadelphia Athletics pitcher Rube Waddell taunted Young to face him so that he could repeat his performance against Boston's ace. Three days later, Young pitched a perfect game against Waddell and the Athletics. It was the first perfect game in American League history. Waddell was the 27th and last batter, and when he flied out, Young shouted, "How do you like that, you hayseed?" Waddell had picked an inauspicious time to issue his challenge. Young's perfect game was the centerpiece of a pitching streak. Young set major league records for the most consecutive scoreless innings pitched and the most consecutive innings without allowing a hit; the latter record still stands at 25.1 innings, or 76 hitless batters. Even after allowing a hit, Young's scoreless streak reached a then-record 45 shutout innings. Before Young, only two pitchers had thrown perfect games. This occurred in 1880, when Lee Richmond and John Montgomery Ward pitched perfect games within five days of each other, although under somewhat different rules: the front edge of the pitcher's box was only from home base (the modern release point is about farther away); walks required eight balls; and pitchers were obliged to throw side-armed. Young's perfect game was the first under the modern rules established in 1893. One year later, on July 4, 1905, Rube Waddell beat Young and the Americans, 4–2, in a 20-inning matchup. Young pitched 13 consecutive scoreless innings before he gave up a pair of unearned runs in the final inning. Young did not walk a batter and was later quoted: "For my part, I think it was the greatest game of ball I ever took part in." In 1907, Young and Waddell faced off in a scoreless 13-inning tie.
In 1908, Young pitched the third no-hitter of his career. Three months past his 41st birthday, Cy Young was the oldest pitcher to record a no-hitter, a record which would stand 82 years until 43-year-old Nolan Ryan surpassed the feat. Only a walk kept Young from his second perfect game. After that runner was caught stealing, no other batter reached base. At this time, Young was the second-oldest player in either league. In another game one month before his no-hitter, he allowed just one single while facing 28 batters. On August 13, 1908, the league celebrated "Cy Young Day." No American League games were played on that day, and a group of All-Stars from the league's other teams gathered in Boston to play against Young and the Red Sox. When the season ended, he posted a 1.26 ERA, which gave him not only the lowest in his career, but also gave him a Major League record of being the oldest pitcher with 150+ innings pitched to post a season ERA under 1.50.
Cleveland Naps and retirement.
Young was traded back to Cleveland, the place where he played over half his career, before the 1909 season, to the Cleveland Naps of the American League. The following season, 1910, he won his 500th career game on July 19 against Washington. He split 1911, his final year, between the Naps and the Boston Rustlers.
On September 22, 1911, Young shut out the Pittsburgh Pirates, 1–0, for his last career victory. In his final start two weeks later, the last eight batters of Young's career combined to hit a triple, four singles, and three doubles.
College coaching career.
In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University, in Macon, Georgia during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.
After baseball.
Beginning in 1912, Young lived and worked on his farm. In 1913, he served as manager of the Cleveland Green Sox of the Federal League, which was at the time an outlaw minor league. However, he never worked in baseball after that.
Young's wife, Robba, whom he had known since childhood, died in 1933. After she died, Young tried several jobs, and eventually moved in with friends John and Ruth Benedum and did odd jobs for them. Young took part in many baseball events after his retirement. In 1937, 26 years after he retired from baseball, Young was inducted into the Baseball Hall of Fame. He was among the first to donate mementos to the Hall.
By 1940, Young's only source of income was stock dividends of $300 per year ($ today). On November 4, 1955, Young died on the Benedum's farm at the age of 88. He was buried in Peoli, Ohio.
Baseball legacy.
Young retired with 511 career wins, which remains the record for most career wins by a pitcher. At the time, Pud Galvin had the second most career wins with 364. Walter Johnson, then in his fourth season, finished his career with 417 wins and, as of 2015, remains second on the list. In 1921, Johnson broke Young's career record for strikeouts.
Young's career is seen as a bridge from baseball's earliest days to its modern era; he pitched against stars such as Cap Anson, already an established player when the National League was first formed in 1876, as well as against Eddie Collins, who played until 1930. When Young's career began, pitchers delivered the baseball underhand and fouls were not counted as strikes. The pitcher's mound was not moved back to its present position of until Young's fourth season; he did not wear a glove until his sixth season.
Young led his league in wins five times (1892, 1895, and 1901–1903), finishing second twice. His career high was 36 in 1892. He had fifteen seasons with twenty or more wins, two more than the runners-up, Christy Mathewson and Warren Spahn. Young won two ERA titles during his career, in 1892 (1.93) and in 1901 (1.62), and was three times the runner-up. Young's earned run average was below 2.00 six times, but this was not uncommon during the dead-ball era. Although Young threw over 400 innings in each of his first four full seasons, he did not lead his league until 1902. He had 40 or more complete games nine times. Young also led his league in strikeouts twice (with 140 in 1896, and 158 in 1901), and in shutouts seven times. Young led his league in fewest walks per nine innings thirteen times and finished second one season. Only twice in his 22-year career did Young finish lower than 5th in the category. Although the WHIP ratio was not calculated until well after Young's death, Young was the retroactive league leader in this category seven times and was second or third another seven times. Young is tied with Roger Clemens for the most career wins by a Boston Red Sox pitcher. They each won 192 games while with the franchise.
Particularly after his fastball slowed, Young relied upon his control. Young was once quoted as saying, "Some may have thought it was essential to know how to curve a ball before anything else. Experience, to my mind, teaches to the contrary. Any young player who has good control will become a successful curve pitcher long before the pitcher who is endeavoring to master both curves and control at the same time. The curve is merely an accessory to control." In addition to his exceptional control, Young was also a workhorse who avoided injury. For nineteen consecutive years, from 1891 through 1909, Young was in his leagues' top ten for innings pitched; in fourteen of the seasons, he was in the top five. Not until 1900, a decade into his career, did Young pitch two consecutive incomplete games. By habit, Young restricted his practice throws in spring training. "I figured the old arm had just so many throws in it", said Young, "and there wasn't any use wasting them." Young once described his approach before a game:
I never warmed up ten, fifteen minutes before a game like most pitchers do. I'd loosen up, three, four minutes. Five at the outside. And I never went to the bullpen. Oh, I'd relieve all right, plenty of times, but I went right from the bench to the box, and I'd take a few warm-up pitches and be ready. Then I had good control. I aimed to make the batter hit the ball, and I threw as few pitches as possible. That's why I was able to work every other day.
By the time of his retirement, Young's control had faltered. He had also gained weight. In three of his last four years, he was the oldest player in the league.
In 1956, about one year after Young's death, the Cy Young Award was created. The first award was given to Brooklyn's Don Newcombe. Originally, it was a single award covering the whole of baseball. The honor was divided into two Cy Young Awards in 1967, one for each league.
On September 23, 1993, a statue dedicated to him was unveiled by Northeastern University on the site of the Red Sox's original stadium, the Huntington Avenue Grounds. It was there that Young had pitched the first game of the 1903 World Series, as well as the first perfect game in the modern era of baseball. A home plate-shaped plaque next to the statue reads:
On October 1, 1903 the first modern World Series between the American League champion Boston Pilgrims (later known as the Red Sox) and the National League champion Pittsburgh Pirates was played on this site. General admission tickets were fifty cents. The Pilgrims, led by twenty-eight game winner Cy Young, trailed the series three games to one but then swept four consecutive victories to win the championship five games to three.

</doc>
<doc id="6851" url="https://en.wikipedia.org/wiki?curid=6851" title="Coronation Street">
Coronation Street

Coronation Street (informally known as Corrie) is a British soap opera created by Granada Television and shown on ITV since 9 December 1960. The programme centres on Coronation Street in Weatherfield, a fictional town based on Salford, its terraced houses, café, corner shop, newsagents, textile factory and The Rovers Return pub. In the show's fictional history, the street was built in the early 1900s and named in honour of the coronation of King Edward VII.
The programme was conceived in 1960 by scriptwriter Tony Warren at Granada Television in Manchester. Warren's initial kitchen sink drama proposal was rejected by the station's founder Sidney Bernstein, but he was persuaded by producer Harry Elton to produce the programme for thirteen pilot episodes. Within six months of the show's first broadcast, it had become the most-watched programme on British television, and is now a significant part of British culture. The show has been one of the most financially lucrative programmes on British commercial television, underpinning the success of Granada Television and ITV."
"Coronation Street" is made by Granada Television at MediaCity Manchester and shown in all ITV regions, as well as internationally. On 17 September 2010, it became the world's longest-running TV soap opera in production. On 23 September 2015, "Coronation Street" was broadcast live to mark ITV's 60th anniversary.
"Coronation Street" is noted for its depiction of a down-to-earth working class community combined with light-hearted humour, and strong characters.
History.
1960s.
The was aired on 9 December 1960 at 7 pm, and was not initially a critical success; "Daily Mirror" columnist Ken Irwin claimed the series would only last three weeks. Granada Television had commissioned only 13 episodes, and some inside the company doubted the show would last beyond its planned production run. Despite the criticism, viewers were immediately drawn into the serial, won over by "Coronation Street"'s 'ordinary' characters. The programme also made use of Northern English language and dialect; affectionate local terms like "eh, chuck?", "nowt" (, meaning "nothing"), and "by 'eck!" became widely heard on British television for the first time.
Early episodes told the story of student Kenneth Barlow (William Roache), who had won a place at university, and thus found his working-class background something of an embarrassment. The character was one of the few to have experienced life 'outside' of "Coronation Street". In some ways this predicts the growth of globalisation, and the decline of similar communities. In an episode from 1961, Barlow declares: "You can't go on just thinking about your own street these days. We're living with people on the other side of the world. There's more to worry about than Elsie Tanner and her boyfriends." Roache is the only remaining member of the original cast, which makes him the longest-serving actor in "Coronation Street", and in British and global soap history.
At the centre of many early stories, there was Ena Sharples (Violet Carson), caretaker of the Glad Tidings Mission Hall, and her friends: timid Minnie Caldwell (Margot Bryant), and bespectacled Martha Longhurst (Lynne Carol). The trio were likened to the Greek chorus, and the three witches in William Shakespeare's "Macbeth", as they would sit in the snug bar of the Rovers Return, passing judgement over family, neighbours and frequently each other. Headstrong Ena often clashed with Elsie Tanner, whom she believed espoused a dauntlessly loose set of morals. Elsie resented Ena's interference and gossip, which most of the time had little basis in reality.
In April 1961, Jed Stone made his first appearance and returned the following year in 1962. He left in 1963, but returned three years later in 1966. He left again and then returned 42 years later in 2008.
In March 1961, "Coronation Street" reached No.1 in the television ratings and remained there for the rest of the year. Earlier in 1961, a Television Audience Measurement (TAM) showed that 75% of available viewers (15 million) tuned into "Corrie", and by 1964 the programme had over 20 million regular viewers, with ratings peaking on 2 December 1964, at 21.36 million viewers.
Storylines throughout the decade included: a mystery poison-pen letter received by Elsie Tanner, the 1962 marriage of Ken Barlow and Valerie Tatlock, the death of Martha Longhurst in 1964, the birth of the Barlow twins in 1965, Elsie Tanner's wedding to Steve Tanner and a train crashing from the viaduct (both in 1967), the murder of Steve Tanner in 1968, and a coach crash in 1969.
In spite of rising popularity with viewers, "Coronation Street" was criticised by some for its outdated portrayal of the urban working class, and its representation of a community that was a nostalgic fantasy. After the first episode in 1960, the "Daily Mirror" printed: "The programme is doomed from the outset ... For there is little reality in this new serial, which apparently, we have to suffer twice a week." By 1967, critics were suggesting that the programme no longer reflected life in 1960s Britain, but reflected how life was in the "1950s". Granada hurried to update the programme, with the hope of introducing more issue-driven stories, including Lucille Hewitt becoming addicted to drugs, Jerry Booth being in a storyline about homosexuality, Emily Nugent having an out of wedlock child, and introducing a black family, but all of these ideas were dropped for fear of upsetting viewers.
1970s.
The show's production team was tested when many core cast members left the programme in the early 1970s. When Arthur Leslie died suddenly in 1970, his character, Rovers' landlord Jack Walker, died with him. Anne Reid quit as Valerie Barlow, and was killed off in 1971, electrocuting herself with a faulty hairdryer. Ratings reached a low of eight million in February 1973, Pat Phoenix quit as Elsie Tanner, Violet Carson (Ena Sharples) was written out for most of the year due to illness, and Doris Speed (Annie Walker) took two months' leave due to bereavement. The audience of ITV's other flagship soap opera "Crossroads" increased markedly at this time, as its established cast, such as Meg Richardson (Noele Gordon), grew in popularity. These sudden departures forced the writing team to quickly develop characters who had previously stood in the background. The roles of Bet Lynch (Julie Goodyear), Ivy Tilsley (Lynne Perrie), Deirdre Hunt (Anne Kirkbride), Rita Littlewood (Barbara Knox), and Mavis Riley (Thelma Barlow) were built up between 1972 and 1973 with characters such as Gail Potter (Helen Worth), Blanche Hunt (Patricia Cutts, Maggie Jones), and Vera Duckworth (Elizabeth Dawn) first appearing in 1974. These characters would remain at the centre of the programme for many years.
Comic storylines had been popular in the series in the 1960s, but had become sparse during the early 1970s. These were re-introduced by new producer Bill Podmore who joined the series in 1976. He had worked on Granada comedy productions prior to his appointment. Stan and Hilda Ogden were often at the centre of overtly funny storylines, with other comic characters including Eddie Yeats (Geoffrey Hughes), Fred Gee (Fred Feast), and Jack Duckworth (William Tarmey) all making their first appearances during the decade.
In 1976, Pat Phoenix returned to her role as Elsie Tanner and, after a spate of ill health, Violet Carson returned on a more regular basis as Ena. "Coronation Street's" stalwart cast slotted back into the programme alongside the newcomers, examining new relationships between characters of different ages and backgrounds: Eddie Yeats became the Ogdens' lodger, Gail Potter and Suzie Birchall moved in with Elsie, Mike Baldwin (Johnny Briggs) arrived in 1976 as the tough factory boss, and Annie Walker reigned at the Rovers with her trio of staff Bet Lynch, Betty Turpin, and Fred Gee.
Storylines throughout the decade included: a warehouse fire in 1975, the birth of Tracy Langton in 1977, the murder of Ernest Bishop in 1978, a lorry crashing into the Rovers Return in 1979, and the marriage of Brian Tilsley and Gail Potter (also in 1979).
For eleven weeks, between August and October 1979, industrial action forced "Coronation Street" and the entire ITV network (apart from the Channel Islands) off the air. When ITV did return, its first evening schedule included a special "catch-up" edition of "Coronation Street". This included storylines which would have taken place during the strike, and they were explained in the form of a narrative chat between Len Fairclough and Bet Lynch. For several weeks the channel had very few fresh episodes to show, and episodes of the game show 3-2-1 were screened in its place. "Coronation Street" returned to ITV screens with a regular scheduled time closer to the end of 1979.
"Coronation Street" had little competition within its prime time slot, and certain critics suggested that the programme had grown complacent, moving away from socially viable storylines and again presenting a dated view of working class life.
1980s.
Between 1980 and 1989, "Coronation Street" underwent some of the biggest changes since its launch. By May 1984, Ken Barlow stood as the only original cast member, after the departures of Ena Sharples (in 1980), Annie Walker (in 1983), Elsie Tanner (in 1984) and Albert Tatlock (also 1984). In 1983, antihero Len Fairclough (Peter Adamson), one of the show's central male characters since 1961, was killed off, and in 1984, Stan Ogden (Bernard Youens) died. While the press predicted the end of "Corrie", H. V. Kershaw declared that "There are no stars in "Coronation Street"." Writers drew on the show's many archetypes, with established characters stepping into the roles left by the original cast. Phyllis Pearce (Jill Summers) was hailed as the new Ena Sharples in 1982, the Duckworths moved into No.9 in 1983 and slipped into the role once held by the Ogdens, while Percy Sugden (Bill Waddington) appeared in 1983 and took over the grumpy war veteran role from Albert Tatlock. The question of who would take over the Rovers Return after Annie Walker's 1983 exit was answered in 1985 when Bet Lynch (who also mirrored the vulnerability and strength of Elsie Tanner) was installed as landlady. In 1983, Shirley Armitage became the first major black character in her role as machinist at Baldwin's Casuals.
Ken Barlow married Deirdre Langton on 27 July 1981. The episode was watched by over 24 million viewers – more ITV viewers than the wedding of Prince Charles and Lady Diana two days later. In the 1980s relationships were cemented between established characters: Alf Roberts (Bryan Mosley) married Audrey Potter (Sue Nicholls) in 1985, and Kevin Webster (Michael Le Vell) married Sally Seddon (Sally Dynevor) in 1986. Bet Lynch married Alec Gilroy in 1987, and the marriages of Ivy Tilsley and Don Brennan, and of Derek Wilton and Mavis Riley took place in 1988.
In 1982, the arrival of Channel 4, and its edgy new soap opera "Brookside", was one of the biggest changes for "Coronation Street". Unlike "Coronation Street", which had a very nostalgic view of working-class life, "Brookside" brought together working and middle-class families in a more contemporary environment. The dialogue often included expletives and the stories were more hard-hitting, and of the current Zeitgeist. Whereas stories at this time in "Coronation Street" were largely about family affairs, "Brookside" concentrated on social affairs such as industrial action, unemployment, and the black market. The BBC also introduced a new prime time soap opera, "EastEnders" in 1985. Like "Brookside", "EastEnders" had a more gritty premise than "Coronation Street", although unlike "Brookside" it tended to steer clear of blue language and politicised stories.
While ratings for "Coronation Street" remained consistent throughout the decade, "EastEnders" regularly obtained higher viewing figures due to its omnibus episodes shown at weekends. With prime time competition, "Corrie" was again seen as being old fashioned, with the introduction of the 'normal' Clayton family in 1985 being a failure with viewers. Between 1988 and 1989, many aspects of the show were modernised by new producer David Liddiment. A new exterior set had been built in 1982, and in 1989 it was redeveloped to include new houses and shops. Production techniques were also changed with a new studio being built, and the inclusion of more location filming, which had moved the show from being shot on film to videotape in 1988. Due to new pressures, an introduction of the third weekly episode aired on 20 October 1989, to broadcast each Friday at 19:30.
The 1980s featured some of the most prominent storylines in the programme's history, such as Deirdre Barlow's affair with Mike Baldwin in 1983, the first soap storyline to receive widespread media attention. The feud between Ken Barlow and Mike Baldwin would continue for many years, with Mike even marrying Ken's daughter, Susan. In 1986, there was a fire at the Rovers Return, which attracted an audience of just under 27 million, and between 1986 and 1989, the story of Rita Fairclough's psychological abuse at the hands of Alan Bradley (Mark Eden), and then his subsequent death under the wheels of a Blackpool tram, was played out. The episode of Alan's death under the tram gave "Coronation Street" its highest ratings ever at 26.9 million, and it is the ninth-most watched UK broadcast of all time. Other stories included: the birth of Nicky Tilsley in 1980, Elsie Tanner's departure and Stan Ogden's funeral in 1984, the birth of Sarah-Louise Tilsley in 1987, and Brian Tilsley's murder in 1989.
New characters were introduced, such as Terry Duckworth (Nigel Pivaro), Curly Watts (Kevin Kennedy), Martin Platt (Sean Wilson), Reg Holdsworth (Ken Morley), and the McDonald family; one of whom, Simon Gregson, started on the show as Steve McDonald a week after his 15th birthday, and has been on the show ever since.
1990s.
In spite of updated sets and production changes, "Coronation Street" still received criticism. In 1992, chairman of the Broadcasting Standards Council, Lord Rees-Mogg, criticised the low-representation of ethnic minorities, and the programme's portrayal of the cosy familiarity of a bygone era. Some newspapers ran headlines such as ""Coronation Street" shuts out blacks" ("The Times"), and "'Put colour in t'Street" ("Daily Mirror"). Patrick Stoddart of "The Times" wrote: "The millions who watch "Coronation Street" – and who will continue to do so despite Lord Rees-Mogg – know real life when they see it ... in the most confident and accomplished soap opera television has ever seen". Black and Asian characters had appeared, but it wasn't until 1999 that the show featured its first regular non-white family, the Desai family.
New characters Des and Steph Barnes moved into one of the new houses in 1990, being dubbed by the media as Yuppies. Raquel Wolstenhulme (Sarah Lancashire) first appeared in 1991 and went on to become one of the most popular characters. The McDonald family were developed and the fiery relationships between Liz, Jim, Steve and Andy interested viewers. Other newcomers were Maud Grimes (Elizabeth Bradley), Roy Cropper (David Neilson), Judy and Gary Mallett, as well as Fred Elliot (John Savident) and Ashley Peacock (Steven Arnold). The amount of slapstick and physical humour in storylines increased during the 1990s, with comical characters such as Reg Holdsworth and his water bed.
In the early 1990s storylines included: the death of newborn Katie McDonald in 1992, Mike Baldwin's wedding to Alma Sedgewick (Amanda Barrie) in 1992, Tommy Duckworth being sold by his father Terry in 1993, Deirdre Barlow's marriage to Moroccan Samir Rachid, and the rise of Tanya Pooley (Eva Pope) between 1993 and 1994.
In 1995, Julie Goodyear (Bet Lynch) left the show. She made brief return appearances in 1999, 2002 and 2003.
In 1997, Brian Park took over as producer, with the idea of promoting young characters as opposed to the older cast. On his first day, he cut the characters of Derek Wilton, Don Brennan, Percy Sugden, Bill Webster, Billy Williams, and Maureen Holdsworth. Thelma Barlow, who played Derek's wife Mavis, was angered by the firing of her co-star and resigned. The production team lost some of its key writers when Barry Hill, Adele Rose and Julian Roach all resigned as well.
In line with Park's suggestion, younger characters were introduced: Nick Tilsley was recast, played by Adam Rickitt, single mother Zoe Tattersall (Joanne Froggatt) first appeared, and the Battersbys moved into No.5. Storylines focussed on tackling 'issues', such as drug dealers, eco-warriors, religious cults, and a transsexual woman. Park quit in 1998, after deciding that he had done what he intended to do; he maintained that his biggest achievement was the introduction of Hayley Patterson (Julie Hesmondhalgh), the first transsexual character in a British soap.
Some viewers were alienated by the new "Coronation Street", and sections of the media voiced their disapproval. Having received criticism of being too out of touch, "Corrie" now struggled to emulate the more modern "Brookside" and "EastEnders". In the "Daily Mirror", Victor Lewis-Smith wrote: "Apparently it doesn't matter that this is a first-class soap opera, superbly scripted and flawlessly performed by a seasoned repertory company."
One of "Coronation Street"'s best known storylines took place in March/April 1998, with Deirdre Rachid being wrongfully imprisoned after a relationship with con-man Jon Lindsay. This episode, when Deirdre was sent to prison, was watched by 19 million viewers, and 'Free the Weatherfield One' campaigns sprung up in a media frenzy. The then Prime Minister Tony Blair even passed comment on Deirdre's sentencing in Parliament. Deirdre was freed after three weeks, with Granada stating that they had always intended for her to be released, in spite of the media interest.
2000s.
On 8 December 2000, the show celebrated its fortieth year by broadcasting a live, hour-long . The Prince of Wales appeared as himself in an ITV News bulletin report. Earlier in the year, 13-year-old Sarah-Louise Platt (Tina O'Brien) had become pregnant and given birth to a baby girl, Bethany, on 4 June. The episode where Gail was told of her daughter's pregnancy was watched by 15 million viewers. In September 2000, Mike Baldwin married Linda Sykes but shortly afterwards, his drunken son Mark confessed he and Linda had been having an affair behind his dad's back. The episode attracted an audience of 16.8 million and in the 2001 British Soap Awards won Best Storyline.
From 1999 to 2001, issue-led storylines were introduced such as Toyah Battersby's rape, Roy and Hayley Cropper abducting their foster child, Sarah Platt's Internet chat room abduction and Alma Halliwell's death from cervical cancer. Such storylines were unpopular with viewers and ratings dropped and in October 2001, Macnaught was abruptly moved to another Granada department and Carolyn Reynolds took over. "Corrie" continued to struggle in the ratings, with "EastEnders" introducing some of its strongest stories. In 2002, Kieran Roberts was appointed as producer and aimed to re-introduce "gentle storylines and humour", after deciding that "the Street" should not try to compete with other soaps. In 2002, Gail Platt married Richard Hillman (Brian Capron), a financial advisor, who would go on to leave Duggie Ferguson to die, murder his ex-wife Patricia, attempt to murder his mother-in-law, Audrey Roberts, murder Maxine Peacock and attempt to murder Emily Bishop. After confessing to the murder of Maxine and his ex-wife, Hillman attempted to kill Gail, her children Sarah and David, and her granddaughter Bethany, by driving them into a canal. The storyline received wide press attention, and viewing figures peaked at 19.4 million, with Hillman dubbed a "serial killer" by the media. Todd Grimshaw became "Corrie's" first regular homosexual character. In 2003 another gay male character was introduced, Sean Tully played by Antony Cotton. The character of Karen McDonald (Suranne Jones) was developed, with her fiery marriage to Steve and warring with Tracy Barlow. In 2004, "Coronation Street" retconned the Baldwin family when Mike's nephew Danny Baldwin and his wife Frankie moved to the area from Essex, with their two sons Jamie and Warren. Until this time, Mike Baldwin had been portrayed as an only child, with his father appearing in the programme between 1980 and 1982 confirming the fact. The bigamy of Peter Barlow and his addiction to alcohol, later in the decade, Maya Sharma's revenge on former lover Devendra Alahan, Charlie Stubbs's psychological abuse of Shelley Unwin, and the deaths of Mike Baldwin, Vera Duckworth and Fred Elliott. In 2007, Tracy Barlow murdered Charlie Stubbs and claiming it was self-defence; the audience during this storyline peaked at 13.3 million. At the 2007 British Soap Awards, it won Best Storyline, and Kate Ford was voted Best Actress for her portrayal. Other storylines included Leanne Battersby becoming a prostitute and the show's first bi-sexual love triangle (between Michelle Connor, Sonny Dhillon, and Sean Tully). The Connor family were central to many storylines during 2007—the accidental death of a Polish worker at Underworld due to overworking, Michelle's discovery that her brothers Paul and Liam were the cause of her husband's death, Paul's use of an escort service, his kidnapping of Leanne and his subsequent death.
In July 2007, after 34 years in the role of Vera Duckworth, Elizabeth Dawn left the show due to ill health. After conversation between Dawn and producers Kieran Roberts and Steve Frost, the decision was made to kill Vera off. In January 2008, shortly before plans to retire to Blackpool, Vera's husband Jack found that she had died in her armchair.
Tina O'Brien revealed in the British press on 4 April 2007 that she would be leaving "Coronation Street". Sarah-Louise, who was involved in some of the decade's most controversial stories, left in December 2007 with her daughter (who'd been in an ectasy storyline earlier that year), Bethany Platt. In 2008, Michelle learning that Ryan was not her biological son, having been accidentally swapped at birth with Alex Neeson. Carla Connor turned to Liam for comfort and developed feelings for him. In spite of knowing about her feelings, Liam married Maria Sutherland. Maria and Liam's baby son was stillborn in April, and during an estrangement from Maria upon the death of their baby, Liam had a one-night stand with Carla, a story which helped pave the way for his departure. Gail's son David Platt (Jack P. Shepherd) pushed her down the stairs. Enraged that Gail refused to press charges, David vandalised the Street and was sent to a young offenders' facility for several months. In May 2008, Gail finally met Ted Page, the father she'd never known and in 2009, Gail's boyfriend, Joe McIntyre became addicted to pain killers, which came to a head when he broke into the medical centre. In August 2008, Jed Stone returned after 42 years. Liam Connor and his ex-sister-in-law Carla Connor gave into their feelings for each other and began an affair. Carla's fiancée Tony discovered the affair and had Liam killed in a hit-and-run in October. Carla struggled to come to terms with Liam's death, but decided she still loved Tony and married him on 3 December, in an episode attracting 10.3 million viewers. In April 2009 it was revealed that Eileen Grimshaw's father, Colin had slept with Eileen's old classmate, Paula Carp while she was still at school, and that Paula's daughter, Julie was in fact also Colin's daughter. In May, Norris Cole received a blast from the past with the reappearance of his estranged brother Ramsay Clegg (Andrew Sachs) who wanted a reconciliation. Peter Barlow's battle against alcoholism, Ken Barlow's affair with actress Martha Fraser after his dog Eccles fell in the canal, Maria giving birth to Liam's son and her subsequent relationship with Liam's killer Tony, Steve McDonald's marriage to Becky Granger and Kevin Webster's affair with Molly Dobbs. On Christmas Day 2009, Sally Webster told husband Kevin that she had breast cancer, just as he was about to leave her for lover Molly.
2010s.
Gail Platt married for the fourth time, this time to Joe McIntyre. Molly Dobbs became pregnant, with the father being either Tyrone Dobbs or Kevin Webster; Molly reconciled with Tyrone and nearly lost her life after a crash in a car which had not been properly repaired by Kevin. Tracy Barlow returned on 7 May 2010, plotting to reduce her sentence by convincing cellmate Gail to confess to Joe's murder. The show's first lesbian storyline developed as Sophie Webster realised her true feelings for her best friend Sian Powers, which proved incredibly popular with the British public. In 2010, Blanche Hunt was written out after the death of actress Maggie Jones on 2 December 2009. ITV honoured Blanche and Maggie with a half-hour special, "Goodbye Blanche", which was aired after the funeral episode. On 31 May 2010, the "Coronation Street" opening sequence was given a revamp. From that date all episodes of the series would be transmitted in High Definition. The change coincided with a week of dramatic storylines known as "Siege Week" being shown on five consecutive nights following "Britain's Got Talent." In the story Tony Gordon escaped from prison to seek vengeance on his enemies, culminating in the dramatic explosion of Underworld. Meanwhile, Gail Platt's murder trial took place, with two different verdicts being shown online. However, following the Cumbria shootings, that occurred on 2 June 2010, the soap was temporarily taken off-air, as it was felt that this plot was too similar to real life events. The remaining siege episodes were transmitted the following week instead, in a heavily truncated schedule. On 17 September 2010 "Coronation Street" entered "Guinness World Records" as the world's longest-running television soap opera after the American soap opera "As the World Turns" concluded. William Roache was listed as the world's longest-running soap actor.
"Coronation Street"'s 50th anniversary week was celebrated with seven episodes, plus a special one-hour live episode, broadcast from 6–10 December. The episodes averaged 14 million viewers, a 52.1% share of the audience. The anniversary was also publicised with ITV specials and news broadcasts. In the storyline, Nick Tilsley and Leanne Battersby's bar—The Joinery—exploded during Peter Barlow's stag party. As a result, the viaduct was destroyed, sending a Metrolink tram careering onto the street, destroying D&S Alahan's Corner Shop and The Kabin. Two characters, Ashley Peacock and Molly Dobbs, along with an unknown taxi driver, were killed as a result of the disaster. Rita Sullivan survived, despite being trapped under the rubble of her destroyed shop. Fiz Stape prematurely gave birth to a baby girl, Hope, after her husband, John, struck his stalker Charlotte Hoyle with a hammer to silence her. He later attempted to turn off Charlotte's life support machine, having gained access to her bedside by allowing her parents to believe he was her fiancé; they later made the decision themselves. Peter Barlow, believing himself on his deathbed, married Leanne Battersby in an impromptu ceremony, shortly before going into cardiac arrest, although he later rallied and made a slow recovery. The episode of "EastEnders" broadcast on the same day as "Coronation Street" 50th anniversary episode included a tribute, with the character Dot Branning saying that she never missed "Coronation Street".
In May 2011, Dennis Tanner returned after 43 years off screen. Beverley Callard and Vicky Entwistle who play Liz McDonald and Janice Battersby, respectively, quit the street. On 15 October 2011, Betty Driver, who had played Betty Williams since 1969, died of pneumonia, aged 91. In 2011, the major storyline of John Stape and his murder spree came to an end in May after he jumped off a hospital roof but left before he could be arrested. He returned in October before Fiz Stape was imprisoned for the murders. Following a car crash, John revealed the details about the murders and how Fiz was not involved. He later died from his injuries on 28 October. In December 2011, the popular Sophie & Sian lesbian relationship storyline came to an end when Sian left Weatherfield after jilting Sophie at their wedding, and later discovering that Sophie had shared a kiss with Dev Alahan's daughter Amber whilst she was away on holiday.
Frank Foster storyline centred on Maria flirting with Frank. Frank forces himself on a frightened Maria who then confides in Carla Connor. Carla is furious and tries to get Maria to go to the police; however, Maria wants to forget all about it and tells Carla to continue doing business with Frank to save the factory. This leads to a relationship between Frank and Carla. They become engaged but Carla calls it off due to her developing feelings for Peter Barlow. Frank finds out about the affair after hiring a private investigator and he attacks and rapes Carla. Frank was arrested under suspicion of rape but was found not guilty when the case went to court. Frank took over Underworld with the assistance of Sally Webster. One evening Frank gloated about getting away with rape to a terrified Carla whilst the factory was closed, unaware that somebody heard every word. Frank was found dead later that day by the Underworld machinists, he was hit over the head with a bottle of whiskey. A distraught Sally comforted Frank's mother, Anne Foster whilst she was grieving her son's death, but discovered it was Anne herself who murdered her own son. Anne attacked Sally and then attempted to flee the country only to be caught out by Carla and Kevin Webster.
Tyrone Dobbs was also the centre of a domestic abuse storyline when he was introduced to police officer Kirsty Soames whilst on a night out. Tyrone's friends paid Kirsty to flirt with him to cheer him up after his previous wife Molly's death. Tyrone and Kirsty immediately hit it off. Kirsty's temper began to show when she used to become jealous of Tyrone's friends, specifically his friendship with Fiz Stape. Kirsty fell pregnant and her temper soon turned violent as she lashed out at Tyrone on numerous occasions, attacking him with her fists and kitchen appliances. Tyrone hid his injuries from his concerned friends and refused to leave the house. Kirsty gave birth in the Rovers Return after a heated confrontation with Tina, yet Tyrone was absent due to him fixing Fiz's boiler. This made Kirsty secretly leave out Tyrone's name as her newborn daughter Ruby's father on her birth certificate. Kirsty's violent outbursts become more frequent and a terrified Tyrone finally confides in a horrified Fiz about Kirsty's domestic abuse, this leads to Tyrone and Fiz embarking on a secret affair after she confesses her love for him. Fiz and Tyrone conspire a plan together for Tyrone to wed Kirsty then report Kirsty to the police for her domestic abuse, resulting in Kirsty being sent to prison and Tyrone getting full custody of his daughter Ruby. Tyrone proposes to a delighted Kirsty, but Kirsty eventually discovers his affair with Fiz, and their plan. Kirsty eventually reveals Fiz and Tyrone's affair in front of everybody at her and Tyrone's wedding. After the wedding, Kirsty and Tyrone have a blazing row back at their house, resulting in Kirsty falling down the stairs after trying to attack Tyrone. The row is overheard by several neighbours and they think Tyrone has been assaulting Kirsty and call the police. Kirsty lies to the police and makes a false accusation that Tyrone has been beating her for months. This leads to Tyrone's arrest and he is bailed until his appending trial. Whilst on bail, Tyrone flees Weatherfield with Fiz, her daughter Hope and his daughter Ruby causing the police to issue a warrant for his arrest. Tyrone is caught and arrested and is kept in a prison cell until his trial. Kirsty lies again whilst giving evidence at Tyrone's trial by playing the victim to the jury. But whilst struggling being a single mum at home, Kirsty snaps at a screaming Ruby and starts screaming at her child to stop crying. Realising that she doesn't trust herself around her daughter, Kirsty interrupts Tyrone's trial right before the verdict is due and confesses to the judge that it was her who had been assaulting Tyrone and he was innocent. Tyrone gets released and Kirsty gets charged and is sentenced to three years in prison. Tyrone gets full custody of Ruby and starts his new life with his girlfriend Fiz.
Leanne split up with Peter and started a new relationship with Nick. They book their wedding for Christmas Day but Leanne visits Peter after Peter confesses he wants her back. Leanne's sister, Eva Price (Catherine Tyldesley) revealed all at the wedding, causing Nick to leave Leanne. That evening, Nick confides in his sister-in-law, Kylie Platt (Paula Lane), and she reveals to him that Eva has been quietly causing trouble between him and Leanne. After a heart to heart, they give into temptation and sleep together. However, the next morning, they agree that it was a mistake. Nick and Leanne reconcile and finally marry — just after Kylie announces she is pregnant. Nick encourages Kylie to have an abortion and she nearly goes through with it. David, who is desperate to start a family, discovers Kylie's pregnancy and stops her. Nick and Kylie agree to raise the baby up as David's as there is a chance it could be his. David overhears Kylie talking about her one-night stand with Nick, and begins a vendetta on Nick and Leanne, which includes a poison pen letter to Leanne, stolen money, bad reviews on the internet and watering down the spirits at the Bistro. Nick and David went in the Bistro van and started arguing about the one-night stand with Kylie. David was desperate not to let Nick reveal all to Kylie and Leanne, as David feared losing Kylie if she found out what David had done to take revenge on Nick. David, in a panic, tried to get Nick to stop the van. In the struggle, David removes Nick's seatbelt and stops the van in the middle of the road, which results in the van being hit by a lorry, knocking them both unconscious. David wakes up shortly after with minor injuries, but Nick does not. Nick is left comatose and is taken straight down to theatre to remove a blood clot from his brain. While Nick is in his coma, Kylie gives birth to a baby girl, Lily. When Nick awakes from his coma, he suffers a panic attack when he sees David in front of him. Later, when Nick can talk more fluently, he blackmails David to get Lily a paternity test to see who the father is or he will tell the police and the Platt family how the accident was really caused. David does so, and with his friend Tina, he opens the results, revealing that David is the father of baby Lily. Nick later finds life frustrating when he can't remember simple things. He is discharged from hospital with a walking stick, and goes to live with Leanne so that she can look after him. On the day of Lily's christening everything is revealed. Leanne finds out about Nick and Kylie's one-night stand and the entire Platt clan discover that David caused the car accident. They therefore disown David, but Nick is distraught when Leanne flees Weatherfield. She later comes back, but she splits up with Nick after he shouts at Simon and smacks her across the face at Christmas 2013. On the same day, Kylie goes on a drunken rampage and gets arrested. In January 2014, David and Kylie make up.
In March 2013, Karl Munro starts a fire in the cellar of the Rovers Return killing Sunita Alahan and a female firefighter. This episode attracted over 10 million viewers. Although he gets away with it at first, the truth eventually comes out and Karl is arrested and sentenced to prison later on in the year.
In 2014 Kal Nazir's family, the street's first Muslim family, was introduced. Burglar Michael Rodwell played by comedian Les Dennis, was introduced through a restorative justice storyline with Gail McIntyre. Three of the show's best-loved characters made their final appearances in 2014: Hayley Cropper, Tina McIntyre and Deirdre Barlow. Hayley was diagnosed with a terminal cancer of the pancreas after a check-up in 2013. Hayley decided to end her life before she succumbs to her illness much to the despair of her husband Roy Cropper. Roy reluctantly agreed to his wife's demands and Hayley drank a concoction of mashed up pills and died in bed of an overdose with her husband lying beside her.
Tina McIntyre embarked on an affair with Peter Barlow after they shared a passionate kiss during Peter and Carla Connor's wedding reception. Peter lies to Tina, telling her that he'd leave Carla and move away with her to Portsmouth when he was actually committing to Carla after they discovered she was pregnant. The affair causes Peter to relapse and starts drinking again. Carla's brother Rob Donovan discovers the affair and threatens them both that he'll tell Carla if they don't. Tina books a taxi for her and Peter but can't find Peter so she heads to the Rovers Return to find him and Carla celebrating Carla's pregnancy. Feeling crushed, Tina heads back to her flat and is pursued by Peter. They argue and Tina threatens Peter that she's going to reveal all about their affair to Carla, causing Peter to leave her flat and tell a heartbroken Carla about the affair before she does. Peter accidentally leaves Tina's flat door open whilst leaving and Rob notices and enters Tina's flat to confront her about her affair. Rob and Tina argue about her telling Carla about the affair, forcing them both onto her balcony outside. But when Tina threatens Rob about reporting his dodgy dealing at his pawn shop to the police, Rob pushes Tina from the balcony and she plummets to the ground. Rob frantically destroys Tina's flat in an attempt to make it look like a burglary and wipes his fingerprints off of everything he has touched. He heads outside to see a motionless Tina lying on the cobbles, he starts to call for an ambulance when Tina surprisingly is still alive and manages to get back to her feet. Rob tries backtracking his actions and tells Tina that she slipped but Tina goads Rob by saying she'll report him for attempted murder as well as him selling illegal goods. Rob takes desperate measures to shut Tina up so he grabs a lead pipe from a van outside the builder's yard and bludgeons Tina to her death. After Peter's confession, a furious Carla heads on the war path and goes in search of Tina. She notices Tina's door to her flat is open and heads inside to have it out with her. She enters the ransacked flat but doesn't find Tina anywhere, she heads out onto the balcony and looks over to discover Tina's body on the floor below. Tina dies the week later from her injuries.
Carla is in the frame for Tina's murder but after evidence and his outburst at Tina's funeral, Peter is arrested and charged with the murder. While in Highfield Prison, desperate for a drink as he is an alcoholic Peter comes across Jim McDonald who supplies with booze while making Peter get son Steve McDonald and Liz to visit. After Peter fails to Jim's command, Jim stops Peter's provision so Peter finds the booze supply and drinks it all down resulting to him being taken to hospital with internal bleeding. Peter later tells Carla he knew she killed Tina even though he didn't do it he'll take her secret to the grave. Carla considers going to the police but Rob is adamant she doesn't. Peter tells Tracy about Jim after he is beaten by Jim's lackeys so she tells Steve stop visiting and Jim turns nasty. A terrified Deirdre Barlow consults Jim's ex-wife Liz McDonald to have words so she does and Jim leaves Peter alone.
In October 2014, Steve thinks he may have melanoma and gets it checked out but is told he may have depression which he disregards. On the 13th Peter's trial begins and a guilty Steve who knew about the affair fears he has ruined Peter's chances of being released but Carla tells the jury on the 15th she believes Peter's innocent making his chances better. While Peter and newfound cell-mate/confidant Eugene Clleland celebrate, murderer Rob yells at Carla making her feel suspicious. At last all is revealed when new evidence emerges and Tracy lures Rob to a derelict factory claiming that they will flee the country together. It is revealed that Tracy lied and had led the police to their location. Rob refuses to admit defeat but finally gives in and is taken away, Tracy breaking down in the process. His name cleared, Peter decides to depart seeing as how there is nothing keeping him in Weatherfield other than son Simon. Peter leaves in November, leaving behind his family, his past, and Carla.
Because of the stress of the trial, Peter's stepmother Deirdre decided to go away to stay with her friend to get away. Deirdre's portrayer Anne Kirkbride died in January 2015, so the character was written out of the show, with her character dying off screen in July 2015.
It was also revealed this year that Max Turner, son of Kylie and stepson of David Platt, has attention deficit hyperactive disorder (ADHD) and he needs medication to control his illness. Kylie ends up getting hooked on to these drugs as the stress that Max is placing on her pushes her to the edge. She then rediscovers Max's biological father, Callum Logan, and he begins harassing the Platts, intent on gambling for custody of the boy. Eventually, Kylie begins reverting to her old ways, and Callum even begins supplying her with illegal highs to ensure he would get custody.
By Christmas 2014, many of the characters' lives are in tatters. David Platt has kicked Kylie out after he learned that she had taken her son's medication. While attempting to enjoy a Christmas dinner as one happy family, Owen ends up losing his cool and breaks Faye's earphones.
In January 2015, while driving the men and women of Underworld to an up class hotel, Steve loses control of their minibus when he is distracted by reckless drivers, and ends up crashing. The vehicle is hanging over the edge of a cliff, and leaving the passengers' lives hanging in the balance. Steve regains consciousness first and abandons the wreckage. While everyone emerges alive from the crash, young Sinead Tinker is left paralysed, potentially for life, and Steve's depression takes a turn for the worse. The episode where the bus crashes was broadcast on the same night actress Anne Kirkbride's death occurred and was announced.
In May 2015, Tracy seeks revenge on Carla and inadvertently starts a massive fire in the Victoria Court flats. Carla is rescued by Leanne. Leanne went back inside to rescue Amy, followed by Kal. The residents are horrified when they find a gas canister on fire. Amy and Leanne make it out safely but Kal is killed after the balcony of the flat explodes. A stray piece from the flaming debris then causes a second explosion at the nearby builders' yard, ignited by a batch of flammable chemicals. This blast hits Maddie as she walks past the yard, and she later dies from her injuries. Carla gets addicted to a gambling habit and is a mess. She takes Tracy to a cliff and tries to commit suicide but Tracy confesses. As the factory is in tatters, Johnny Connor and son Aidan Connor and sister Kate Connor come to bail her out.
In July 2015, Ken is dealt a blow when Bev arrives to tell him that Deirdre died just before she was due to return home. She explains to the Barlows how she died and says that she was sat on her patio chair in the garden quietly and peacefully, Bev states that the cause of death could've been a brain aneurysm. A party was held for her in the Rovers to celebrate her 60th birthday but turns out to be a wake after everyone is delivered the terrible news by Liz. Ken is also shocked to discover that Deirdre stayed away for so long because of Tracy's recent affair with Tony, Deirdre's funeral was held on 13 July 2015. Tracy met up with her ex-husband Robert Preston at Deirdre's funeral, and he soon moves in.
Callum, who was battling for custody over Max, attacked David and the Platt family for months until one day when, after the Platt's return from a week-long holiday, Sarah Platt, who had returned in March along with her daughter Bethany Platt, was babysitting Max. Whilst they were watching a scary movie, Gemma Winter, Callum's friend, came knocking at the door of Callum's flat. She had been beaten up by a thug. When Max was playing in his room, he found Callum's gun. He threatened to shoot everyone dead but eventually put the gun down. Callum threatened Sarah that if she told anyone about the gun then he would release a video of Bethany dealing and taking drugs to the police. Sarah flees with Max, refused to speak to anyone about it and told Bethany to stay away from him. However, he continued threatening the Platt's. Sarah confided in Jason Grimshaw to finish him off. He attacked Callum in the Rovers and told him to stay away. He was then arrested for possession of the gun but was released without charge due to lack of evidence. When Callum arrived at the Platts' house to collect Max, David and Jason prevented him. Furious, Callum and two of his thugs lead Jason into the ginnel and beat him within an inch of his life. Sarah found Jason and called an ambulance. Jason's parents Eileen and Tony were told that he might not walk or talk again. Max, who had witnessed the attack on Jason, told David and Kylie what he saw after he appeared terrified when Callum came to collect him. Max gave a statement to the police. Before Callum was arrested, he spiked Sarah's drink and knocked her unconscious. He threatened Bethany that he would kill Sarah if she didn't lie for him. The day after, on Lily Platt's 2nd birthday, Bethany revealed to the Platts that she made a statement against Max. The family was disgusted as they knew she was lying. Sarah informed Tony about Callum and the following day, he took matters into his own hands by beating up Callum in the ginnel, only to be stopped by Liz. Callum continued to attack the Platts because he needed money to pay off a dangerous drug dealer. Sarah lures him to the house and records what he is saying. Callum finds her phone and starts attacking Sarah, and Kylie comes in and beats him over the head with a wrench, killing him. They bury his body in a manhole in the garage and Sarah is traumatized.
Rob Donovan reads an article in the newspaper about Johnny at the factory. He then requests a visit from Tracy and tells his prison cellmate that he plans for revenge on Tracy and Carla of how they disowned him and how they called the police and had him jailed. Tracy turned up to visit Rob and wondered why he had chosen to contact her after such time, however Rob confessed to being angry at first but could never stop loving her and he didn't blame her for him being arrested. After Tracy's visit, Rob requested a visit from Johnny, and reveals that he is the father of Carla, as his mother shared the secret with him years ago. He demands £10,000 from Johnny to keep it secret. Johnny told him that Tracy was with Robert Preston. Johnny does everything in his power to ensure that nobody finds out, but steals some of Carla's DNA to perform a paternity test behind her back. Tracy goes to see Rob and misses Amy's violin recital. Rob then sent a visiting order to Robert. He told him about Tracy wanting to start a new relationship with him. Robert was disgusted and stormed out. He then split up with Tracy. Rob sends another visiting order out to Tracy. He reveals to her that everything was a lie and that he still hated her. She then attacked him and was dragged out of prison by security guards. Johnny, feeling the pressure from Rob and the result of the test being positive, reveals all to a disgusted Carla. Carla is furious and, when Johnny started bothering her, she slapped Johnny, however Johnny continues to plead with Carla not to tell Aidan and Kate. With Nick, Roy and Michelle constantly talking about Johnny, Carla heads for a casino where she bumps into Robert, and after gambling with him, Carla has sex with him. She instantly regrets it, and confides in Michelle, who attempts to persuade Carla not to tell Nick. Johnny confronts her and she slaps him, threatening to tell his children the truth unless he moves away, which he doesn't. Upon visiting Rob, where he threatens to tell Aidan and Kate himself about her paternity, Carla reveals that she is their half-sister. Kate and Aidan are distraught and Aidan sees red and punches Johnny. Carla is shocked to find that Robert plans to buy a share in the Bistro. Carla begs him to reconsider, but Robert threatens to reveal their affair to Nick if she does not let him buy into the Bistro. Nick then begins making a fuss about their wedding, and entrusts Robert to organise the meals for their reception. Carla finds it increasingly difficult to keep her deceit under wraps. She also begins bonding with Kate and they plan to get Aidan to talk to Johnny about their familial situation. Carla late tells Nick that she cannot marry him, however after he persuades her, she changes her mind. Upon hearing that Robert is intending on going into business with Nick, Carla goes to the bistro to confront him. As a powercut has forced the restaurant to close early. Carla and Robert engage in a bitter argument, with their one-night stand being mentioned. Unbeknown to the pair, but Tracy, who has recently reconciled with Robert, has overheard their conversation. Robert then leaves, and Carla drowns her sorrows. However, Jamie Bowman (James Atherton), and his friend, Lee, burst into the restaurant and violently attack Carla, pinning her down on the floor and grabbing her hair. Tracy, hearing the commotion, leaves Carla to the mercy of the thugs, feeling envious over Carla's liaison with Robert. After realising that there is no money in the restaurant, Jamie grabs Carla's handbag, causing her to fall over. She rises to her feet abruptly and gives chase. She jumps into the car, leaving the door wide open, wrestling with the thugs for her handbag. As the car drives out of the street, Jamie pushes Carla out of the vehicle, knocking her unconscious. She survived and Tracy pressures her to persuade Nick to sell the Bistro. He eventually sells to Robert, and Carla and Nick make plans to move to Devon.
On the morning of 1 March 2016, Coronation Street creator, Tony Warren, passed away, aged 79, after a "short illness" as confirmed by Coronation Street's Twitter account. Tributes were paid by cast members, past and present.
Characters.
Since 1960, "Coronation Street" has featured many characters whose popularity with viewers and critics has differed greatly. The original cast was created by Tony Warren, with the characters of Ena Sharples (Violet Carson), Elsie Tanner (Patricia Phoenix) and Annie Walker (Doris Speed) as central figures. These three women remained with the show for 20 years or more, and became archetypes of British soap opera, often being emulated by other serials. Ena was the street's busybody, battleaxe and self-proclaimed moral voice. Elsie was the tart with a heart, who was constantly hurt by men in the search for true love. Annie Walker, landlady of the Rovers Return Inn, had delusions of grandeur and saw herself as better than other residents of "Coronation Street".
"Coronation Street" became known for the portrayal of strong female characters, including original cast characters like Sharples, Walker and Tanner, and Hilda Ogden (who first appeared in 1964); who became household names during the 1960s. Warren's programme was largely matriarchal, which some commentators put down to the female-dominant environment in which he grew up. Consequently, the show has a long tradition of psychologically abused husbands, most famously Stan Ogden and Jack Duckworth, husbands of Hilda and Vera, respectively.
Ken Barlow (William Roache) entered the storyline as a young radical, reflecting the youth of 1960s Britain, where figures like The Beatles, The Rolling Stones and the model Twiggy were to reshape the concept of youthful rebellion. Though the rest of the original Barlow family were killed off before the end of the 1970s, Ken, who for 27 years was the only character from the first episode remaining, has remained the constant link throughout the entire series. In 2011, Dennis Tanner (Philip Lowrie), another character from the first episode, returned to "Coronation Street" after a 43-year absence. Since 1984, Ken Barlow had been the show's only remaining original character, although Emily Bishop (Eileen Derbyshire) has remained in the series since first appearing in early 1961, when the show was just weeks old, and is the show's longest-serving female character.
Stan Ogden and Hilda Ogden were introduced in 1964, with Hilda (Jean Alexander) becoming one of the most famous British soap characters of all time. In a 1982 poll, she was voted fourth most recognisable woman in Britain, after Queen Elizabeth The Queen Mother, Queen Elizabeth II and Diana, Princess of Wales. Hilda's best-known attributes were her pinny, hair curlers, and the "muriel" in her living room with three "flying" duck ornaments. Hilda Ogden's final episode on Christmas Day 1987, remains the highest-rated episode of "Coronation Street" ever, with nearly 27 million viewers. Stan Ogden had been killed off in 1984 following the death of actor Bernard Youens after a long illness which had restricted his appearances towards the end.
Bet Lynch (Julie Goodyear) first appeared in 1966, before becoming a regular in 1970, and went on to become one of the most famous "Corrie" characters. Bet stood as the central character of the show from 1985 until departing in 1995, often being dubbed as "Queen of the Street" by the media, and indeed herself. The character briefly returned in June 2002.
"Coronation Street" and its characters often rely heavily on archetypes, with the characterisation of some of its current and recent cast based loosely on past characters. Phyllis Pearce, Blanche Hunt (Maggie Jones) and Sylvia Goodwin embodied the role of the acid-tongued busybody originally held by Ena Sharples, Sally Webster (Sally Dynevor) has grown snobbish, like Annie Walker, and a number of the programme's female characters, such as Carla Connor, mirror the vulnerability of Elsie Tanner and Bet Lynch. Other recurring archetypes include the war veteran (Albert Tatlock, Percy Sugden and Gary Windass), the bumbling retail manager (Leonard Swindley, Reg Holdsworth, Norris Cole), quick-tempered toughs (Len Fairclough, Jim McDonald, Tommy Harris and Owen Armstrong), and the perennial losers (Stan and Hilda Ogden, Jack and Vera Duckworth, Les Battersby-Brown, Beth Tinker and Kirk Sutherland), Villains (Tracy Barlow, Rob Donovan, Anne Foster, Frank Foster, Tony Gordon, Richard Hillman, Greg Kelly, Callum Logan, Karl Munro, Pat Phelan, Maya Sharma, Kirsty Soames and John Stape) The show's former archivist and scriptwriter Daran Little disagreed with the characterisation of the show as a collection of stereotypes. "Rather, remember that Elsie, Ena and Co. were the first of their kind ever seen on British television. If later characters are stereotypes, it's because they are from the same original mould. It is the hundreds of programmes that have followed which have copied "Coronation Street"."
Storylines.
Over the show's history, "Coronation Street" have highlighted a wide range of different social issues, including rape, cancer, incest, hoarding, stillbirth, murder, arson, hit-and-run, miscarriage, adultery, euthanasia, homosexuality, depression, epilepsy, osteoporosis, childhood cancer, Autism, domestic violence, post traumatic stress disorder, teenage pregnancy, gambling addiction, widowers moving onto other relationships, brain aneurysm, drink driving, families terrorised by thugs, financial problems, parental abuse, identity theft, bereavement, terminal illness, post-natal depression, abortion, perjury, adoption, premature birth, fostering, surrogacy, prostitution and revenge porn.
Production.
Broadcast format.
Between 9 December 1960 and 3 March 1961, "Coronation Street" was broadcast twice weekly, on Wednesday and Friday. During this period, the Friday episode was broadcast live, with the Wednesday episode being pre-recorded 15 minutes later. When the programme went fully networked on 6 March 1961, broadcast days changed to Monday and Wednesday. The last regular episode to be shown live was broadcast on 3 February 1961.
The series was transmitted in black and white for the majority of the 1960s. Preparations were made to film episode 923, to be transmitted Wednesday 29 October 1969, in colour. This instalment featured the street's residents on a coach trip to the Lake District. In the end, suitable colour film stock for the cameras could not be found and the footage was shot in black and white. The following episode, transmitted Monday 3 November, was videotaped in colour but featured black and white film inserts and title sequence. Like BBC1, the ITV network was officially broadcast in black and white at this point (though programmes were actually broadcast in colour as early as July that year for colour transmission testing and adjustment) so the episode was seen by most in black and white.
Daran Little, for many years the official programme archivist, claims that the first episode to be transmitted in colour was episode 930 shown on 24 November 1969. The ITV network, like BBC1, began full colour transmissions on 15 November 1969 and it is therefore possible that the first transmitted colour episode is number 928 shown on 17 November.
In October 1970 a technicians' dispute turned into a work-to-rule when sound staff were denied a pay rise given to camera staff the year before for working with colour recording equipment. The terms of the work-to-rule were that staff refused to work with the new equipment (though the old black and white equipment had been disposed of by then) and therefore programmes were recorded and transmitted in black and white, including "Coronation Street" The dispute was resolved in early 1971 and the last black and white episode was broadcast on 8 February 1971.
Episode 5191, originally broadcast on 7 January 2002, was the first to be broadcast in widescreen format. "Coronation Street" was the last British soap to make the switch to 16:9 ("Take the High Road" remained in until it finished in 2003).
From 22 March 2010, "Coronation Street" was produced in 1080/50i for transmission on HDTV platforms on ITV HD. The first transmission in this format was episode 7351 on 31 May 2010 with a new set of titles and re-recorded theme tune. On 26 May 2010 ITV previewed the new HD titles on the "Coronation Street" website. Due to copyright reasons only viewers residing in the UK could see them on the ITV site.
Production staff.
"Coronation Street's" creator, Tony Warren, wrote the first 13 episodes of the programme in 1960, and continued to write for the programme intermittently until 1976. He had retained links with "Coronation Street" up to his death in 2016, often advising on storylines.
Harry Kershaw was the script editor for "Coronation Street" when the programme began in 1960, working alongside Tony Warren. Kershaw was also a script writer for the programme and the show's producer between 1962 and 1971. He remains the only person, along with John Finch, to have held the three posts of script editor, writer and producer. Kershaw continued to write for the programme until his retirement in January 1988.
Adele Rose was the longest-serving "Coronation Street" writer, completing 455 scripts between 1961 and 1998. She also created "Byker Grove".
Bill Podmore was the show's longest serving producer. By the time he stepped down in 1988 he had completed 13 years at the production helm. Nicknamed the "godfather" by the tabloid press, he was renowned for his tough, uncompromising style and was feared by both crew and cast alike. He is probably most famous for sacking Peter Adamson, the show's Len Fairclough, in 1983.
Kieran Roberts currently, , executively produces the show, with former "Emmerdale" producer Stuart Blackburn as the series producer. Blackburn was cut from his job in September 2015, with the then-series producer of "Emmerdale", Kate Oates, replacing him.
Michael Apted, best known for the "Up!" series of documentaries was a director on the programme in the early 1960s. This period of his career marked the first of his many collaborations with writer Jack Rosenthal. Rosenthal, noted for such television plays as "Bar Mitzvah Boy", began his career on the show, writing over 150 episodes between 1961 and 1969. Paul Abbott was a story editor on the programme in the 1980s and began writing episodes in 1989, but left in 1993 to produce "Cracker", for which he later wrote, before creating his own dramas such as "Touching Evil" and "Shameless". Russell T Davies was briefly a storyliner on the programme in the mid-1990s, also writing the script for the direct-to-video special "" He, too, has become a noted writer of his own high-profile television drama programmes, including "Queer as Folk" and the 2005 revival of "Doctor Who". Jimmy McGovern also wrote some episodes.
Theme music.
The show's theme music, a cornet piece, accompanied by a brass band plus clarinet and double bass, reminiscent of northern band music, was written by Eric Spear.
The identity of the trumpeter was not public knowledge until 1994, when jazz musician and journalist Ron Simmonds revealed that it was the Surrey musician Ronnie Hunt. He added, "an attempt was made in later years to re-record that solo, using Stan Roderick, but it sounded too good, and they reverted to the old one." In 2004, the "Manchester Evening News" published a contradictory story that a young musician from Wilmslow called David Browning played the trumpet on both the original recording of the theme in 1960 and a re-recording in 1964, for a one-off payment of £36. In June 2009, the "Mail on Sunday" resolved the matter. Browning conceded that Hunt recorded the original in 1960, but believed that his own re-recording in 1964 or 1972 had been used since that date. ITV then confirmed to the "Mail" that a second version had been recorded in the 1970s, but was only used for a very short while before reverting to Hunt's 1960 recording. In the 1980s the same version was converted to stereo.
Ronnie Hunt said he was paid £6, and found the experience frustrating as Eric Spear insisted on many takes before obtaining the sound that he wanted. After taking a break in a local pub, Hunt achieved the desired mournful sound by playing very close to the microphone.
A new, completely re-recorded version of the theme tune replaced the original when the series started broadcasting in HD on 31 May 2010. It accompanied a new montage-style credits sequence featuring images of Manchester and Weatherfield.
A reggae version of the theme tune was recorded by The I-Royals and released by Media Marvels and WEA in 1983.
Viewing figures.
Most episodes in the 1960s, 70s, and 80s rated with over 20 million viewers and during the 1990s and early 2000s 14–16 million per episode would be typical. Like most terrestrial television in the UK, a decline in viewership has taken place and the show posts an average audience of just under 9 million per episode remaining as one of the highest rated programmes in the UK.
The programme rates as one of the most watched programmes on UK television for every day it is aired. Viewership peaked on Christmas Day 1987 when an average of 28.5 million viewers tuned in to see Hilda Ogden leave the street to start a new life as a housekeeper for long term friend Dr Lowther (although there is some confusion as to whether or not this was actually the highest rating episode due to a special omnibus repeat of that week's episodes being combined with the original airing). Since EastEnders began in 1985, the two programmes have constantly battled it out for first place in the ratings.
Sets.
The regular exterior buildings shown in "Coronation Street" include a row of terrace houses, several townhouses, and communal areas including a newsagents ("The Kabin"), a cafe ("Roy's Rolls"), a general grocery shop ("D&S Alahan's"), a factory ("Underworld") and "Rovers Return Inn" public house. The Rovers Return Inn is the main meeting place for the show's characters.
Between 1960 and 1968 street scenes were filmed before a set constructed in a studio, with the house fronts reduced in scale to 3/4 and constructed from wood. In 1968 Granada built an outside set not all that different from the interior version previously used, with the wooden façades from the studio simply being erected on the new site. These were replaced with brick façades, and back yards were added in the 1970s.
In 1982 a permanent full-street set was built in the Granada backlot, an area between Quay Street and Liverpool Road in Manchester. The set was constructed from reclaimed Salford brick. The set was updated in 1989 with the construction of a new factory, two shop units and three modern town houses on the south side of the street.
Between 1989 and 1999 the Granada Studios Tour allowed members of the public to visit the set. The exterior set was extended and updated in 1999. This update added to the Rosamund Street and Victoria Street façades, and added a viaduct on Rosamund Street. Most interior scenes are shot in the adjoining purpose-built studio.
In 2008, "Victoria Court", an apartment building full of luxury flats, was started on Victoria Street.
In 2014, production moved to a new site at Trafford Wharf, a former dock area about two miles to the east, part of the MediaCityUK complex. The Trafford Wharf backlot is built upon a former truck stop site next to the Imperial War Museum North. It took two years from start to finish to recreate the iconic Street. The houses were built to almost full scale after previously being three-quarter size.
On 5 April 2014, the staff began to allow booked public visits to the old Quay Street set. An advert, with a voiceover from Victoria Wood, appeared on TV to advertise the tour.
Broadcast.
United Kingdom.
For 54 years, "Coronation Street" has remained at the centre of ITV's prime time schedule. The programme is shown in the UK in five episodes, over three evenings a week on ITV. From Friday 9 December 1960 until Friday 3 March 1961, the programme was shown in two episodes broadcast Wednesday and Friday at 19:00. Schedules were changed and from Monday 6 March 1961 until Wednesday 11 October 1989, the programme was shown in two episodes broadcast Monday and Wednesday at 19:30. The third weekly episode was introduced on Friday 20 October 1989, broadcast at 19:30. From 1996, an extra episode was broadcast at 19:30 on Sunday nights. Aside from Granada, the programme originally appeared on the following stations of the ITV network: Anglia Television, Associated-Rediffusion, Television Wales and the West, Scottish Television, Southern Television and Ulster Television. From episode 14 on Wednesday 25 January 1961, Tyne Tees Television broadcast the programme. That left ATV in the Midlands as the only ITV station not carrying the show. When they decided to broadcast the programme, national transmission was changed from Wednesday and Friday at 19:00 to Monday and Wednesday at 19:30 and the programme became fully networked under this new arrangement from episode 25 on Monday 6 March 1961.
As the ITV network grew over the next few years, the programme was transmitted by these new stations on these dates onward: Westward Television from episode 40 on 29 April 1961, Border Television from episode 76 on 1 September 1961, Grampian Television from episode 84 on 30 September 1961, Channel Television from episode 180 on 1 September 1962 and Teledu Cymru (north and west Wales) from episode 184 on 14 September 1962. At this point, the ITV network became complete and the programme was broadcast almost continuously across the country at 19:30 on Monday and Wednesday for the next twenty-seven years.
From episode 2981 on Friday 20 October 1989 at 19:30, a third weekly episode was introduced and this increased to four episodes a week from episode 4096 on Sunday 24 November 1996, again at 19:30. The second Monday episode was introduced in 2002 and was broadcast at 20:30 to usher in the return of Bet Lynch. The Monday 20:30 episode was used intermittently during the popular Richard Hillman story line but has become fully scheduled since episode 5568 on Monday 25 August 2003. Additional episodes have been broadcast during the weekly schedule of ITV at certain times, notably in 2004 when, between 22 and 26 November, eight episodes were shown.
Older episodes had been broadcast by satellite and cable channel Granada Plus from launch in 1996. The first episodes shown were from episode 1588 (originally transmitted on Monday 5 April 1976) onwards. Originally listed and promoted as "Classic Coronation Street", the "classic" was dropped in early 2002, at which stage the episodes were from late 1989. By the time of the channel's closure in 2004, the repeats had reached January 1994. In addition to this, "specials" were broadcast on Saturday afternoons in the early years of the channel with several episodes based on a particular theme or character(s) were shown. The latest episode shown in these specials was from 1991. In addition, on 27 and 28 December 2003, several Christmas Day editions of the show were broadcast.
From 23 July 2009 "Coronation Street" has been broadcast in five weekly instalments, at 19:30 and 20:30 on Mondays and Fridays, and at 20:30 on Thursday. The Thursday episode replaces the former Wednesday show. Occasional late night episodes of "Coronation Street" begin at 22:00, due to the watershed. Repeat episodes, omnibus broadcasts and specials have been shown on ITV and ITV2. In January 2008 the omnibus returned to the main ITV channel where it was aired on Saturday mornings/afternoons depending on the schedule and times. In May 2008 it moved to Sunday mornings until August 2008 when it returned to Saturdays. In January 2009 it moved back to Sunday mornings usually broadcasting at around 09:25 until December 2010. In January 2011 the omnibus moved to Saturday mornings on ITV at 09:25. During the Rugby World Cup, which took place in New Zealand, matches had to be broadcast on a Saturday morning, so the omnibus moved to Saturday lunchtimes/afternoons during September and October 2011. However, as of 22 October 2011 the omnibus moved back to Saturday mornings at 09:25 on ITV. From January 2012 the omnibus was no longer broadcast on ITV after four years, however it remains on ITV2.
On 30 June 2011 it was confirmed that Coronation Street would return to its traditional 19:30 timeslot on a Wednesday evening in September 2012.
International.
"Coronation Street" is shown in various countries worldwide.
The programme was first aired in Australia in 1963 on TCN-9 Sydney, GTV-9 Melbourne and NWS-9 Adelaide, and by 1966 "Coronation Street" was more popular in Australia than in the UK. The show eventually left free-to-air television in Australia in the 1970s. It briefly returned to the Nine Network in a daytime slot during 1994–95. In 2005 STW-9 Perth began to show episodes before the 18:00 news to improve the lead in to Nine News Perth, but this did not work and the show was cancelled a few months later. In 1996 Pay-TV began and Arena began screening the series in one-hour instalments on Saturdays and Sundays at 18:30 EST. The series was later moved to Pay-TV channel UKTV where it is still shown. In 2015 "Coronation Street" is shown on weeknights at 18:50 EST. Episodes on UKTV are around four weeks behind the UK. Seven broadcast old episodes daily on 7Two until September 2014.
In Canada, "Coronation Street" is broadcast on CBC Television. Until 2011, episodes were shown in Canada approximately 10 months after they aired in Britain; however, beginning in the fall of 2011, the CBC began showing two episodes every weekday, in order to catch up with the ITV showings, at 18:30 and 19:00 local time Monday-Friday, with an omnibus on Sundays at 07:30. By May 2014, the CBC was only two weeks behind Britain, so the show was reduced to a single showing weeknights at 18:30 local time. The show debuted on Toronto's CBLT in July 1966. The 2002 edition of the "Guinness Book of Records" recognises the 1,144 episodes sold to the now-defunct CBC-owned Saskatoon, Saskatchewan, TV station CBKST by Granada TV on 31 May 1971 to be the largest number of TV shows ever purchased in one transaction. The show traditionally aired on weekday afternoons in Canada, with a Sunday morning omnibus. In 2004, CBC moved the weekday airings from their daytime slot to prime time. In light of austerity measures imposed on the CBC in 2012, which includes further cutbacks on non-Canadian programming, one of the foreign shows to remain on the CBC schedule is "Coronation Street", according to the CBC's director of content planning Christine Wilson, who commented: "Unofficially I can tell you "Coronation Street" is coming back. If it didn't come back, something would happen on Parliament Hill." Kirstine Stewart, the head of the CBC's English-language division, once remarked: "Coronation Street fans are the most loyal, except maybe for curling viewers, of all CBC viewers," In late September 2014, CBC aired extra episodes to become only one week behind the UK in airing of new episodes.
In the Republic of Ireland, "Coronation Street" is simulcast on UTV Ireland. The show is UTV Ireland's most watched programme with an average of 365,000 people watching each night. The broadcaster relies on a live feed of the soap from the ITV network in London. If there was a breaking newsflash (e.g. about a Royal baby being born) on the UK channel, "Coronation Street" could not be broadcast. An omnibus is also shown on weekends. For a number of months in 2009 TV3 provided repeats of the night's episode on sister channel 3e at 21:00 Monday, Wednesday and Friday nights, this has since stopped. The show was first aired in 1978, beginning with episodes from 1976. Ireland eventually caught up with the current UK episodes in 1983. Until 1992 it was broadcast on RTÉ2 and from 1992 to 2001 it was broadcast on RTÉ One. In 2001 Granada TV bought 45 percent of TV3, which resulted in TV3 broadcasting series from 2001 to 2014. In 2006 ITV sold its share of the channel but TV3. TV3 continue to buy the soap until the end of 2014 when it moved to UTV Ireland. Coronation Street has broadcast on each of the main Irish networks, except for the Irish Language Network TG4.
In New Zealand, "Coronation Street" has been shown locally since 1964, first on NZBC television until 1975, and then on TV One, which broadcasts it in a 4-episode/2-hour block on Fridays from 19:30. Since September 2014, TV One has added a 2-episode/1-hour block on Saturday from 20:30. Because TV One has never upgraded to showing the equivalent of five episodes per week, New Zealand continues to fall further and further behind with episodes, and is 23 months behind Britain (as of 28 March 2014). During the weekday nights of the week ending 11 April 2014 and previous weeks, Coronation Street was the least watched programme on TV One in the 19:30 slot by a considerable margin in comparison to other weeknights, The serial aired on Tuesdays and Thursdays at 19:30 until October 2011, when the show moved to a 17:30 half-hour slot every weekday. The move proved unpopular with fans, and the series was quickly moved into its present prime-time slot within weeks. Episodes 7883, 7884, 7885 and 7886 were screened on 16 May 2014. These were originally aired in the UK between 4 and 11 June 2012.
In the United States, "Coronation Street" is available by broadcast or cable only in northern markets where CBC coverage from Canada overlaps the border or is available on local cable systems. It was broadcast on CBC's US cable channel, Trio until the CBC sold its stake in the channel to Universal, before it was shut down in 2006. Beginning in 2009, episodes were available in the United States through Amazon.com's on-demand service, one month behind their original UK airdates. The final series of shows available from Amazon appears to be from November 2012, as no new episodes have been uploaded. On 15 January 2013, online distributor Hulu began airing episodes of the show, posting a new episode daily, two weeks after their original airdates. Hulu's website how states: "New episodes of Coronation Street will be unavailable as of April 7th, 2016", with the same being said for British soap "Hollyoaks" 
"Coronation Street" was also shown on USA Network for an unknown period starting in 1982.
HM Forces and their families stationed overseas can watch "Coronation Street" on ITV, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands. It used to be shown on BFBS1.
Satellite channel ITV Choice shows the programme in Asia, Middle East, Cyprus, and Malta. In the United Arab Emirates, episodes of "Coronation Street" are broadcast one month after their UK showing.
Merchandise.
Several classic episodes were released on VHS video in the 1980s and 1990s in different sets, while a number of specially recorded feature-length episodes were released exclusively to video (see Coronation Street VHS and DVD releases).
"The Street", a magazine dedicated to the show, was launched in 1989. Edited by Bill Hill, the magazine contained a summary of recent storylines, interviews, articles about classic episodes, and stories that occurred from before 1960. The format was initially A5 size, expanding to A4 from the seventh issue. The magazine folded after issue 23 in 1993 when the publisher's contract with Granada Studios Tour expired and Granada wanted to produce their own magazine.
During the time when the studio tour was operating, a huge amount of branded merchandise was available from an on-site shop—everything from soap, to tea-towels, to model houses. These items gradually became scarce as the tours complex was wound down. Although there were large numbers produced, these items are becoming collectable by fans.
On 25 June 2010 a video game of the show was released on Nintendo DS. Players take the role of a detective as they work through a brand new storyline and befriend the various characters from the street, including Ken, Norris, Maria and Blanche.
Discography.
In 1995, to commemorate the programme's 35th anniversary, a CD called "The Coronation Street Album" was released, featuring cover versions of modern songs and standards by contemporary cast members.
An album featuring songs sung by some of the cast was released for the show's 50th anniversary. The singers include William Roache, Betty Driver, Kevin Kennedy and Katherine Kelly. The album is titled "Rogues, Angels, Heroes & Fools".
Spin-offs and specials.
Granada launched one spin-off in 1965, "Pardon the Expression", following the story of clothing store manager Leonard Swindley (Arthur Lowe) after he left Weatherfield. Swindley's management experience was tested when he was appointed assistant manager at a fictional department store, Dobson and Hawks. Granada produced two series of the spin-off, which ended in 1966.
In 1967, Arthur Lowe returned as Leonard Swindley in "Turn Out the Lights", a short-lived sequel to "Pardon the Expression". It ran for just one series of six episodes before it was cancelled.
The German TV series "Lindenstraße" took "Coronation Street" as the model. "Lindenstraße" started in 1985.
In 1985, a sister series, "Albion Market" was launched. It ran for one year, with 100 episodes produced.
On 8 December 2000 and 9 December 2010, live episodes were aired to mark the 40th and 50th anniversaries of the show. The first was mainly based around Vera Duckworth in hospital and the campaign to save the cobbles. The second was based around events following the tram crash.
"Coronation Street: Family Album" was several documentaries about various families living on the street.
"Farewell ..." was several documentaries featuring the best moments of a single character who had recently left the series—most notably, Farewell Blanche (Hunt), Farewell Jack (Duckworth), Farewell Mike (Baldwin), Farewell Vera (Duckworth), Farewell Janice (Battersby), Farewell Liz (McDonald) and Farewell Becky (McDonald). Most of these were broadcast on the same day as the character's final scenes in the series.
On 21 December 2008, a web-based miniseries ran on ITV.com; called "Corrie Confidential"; the first episode featured the characters Rosie and Sophie Webster in "Underworld".
"Stars on the Street" was aired around Christmas 2009. It featured actors from the soap talking about the famous guest stars who had appeared in the series including people who were in it before they were famous.
In 2010, several actors from the show appeared on "The Jeremy Kyle Show" as their soap characters: David Platt (Jack P. Shepherd), Nick Tilsley (Ben Price) and Tina McIntyre (Michelle Keegan). In the fictional, semi-improvised scenario, David accused Nick (his brother) and Tina (his ex-girlfriend) of sleeping together.
On 21 December 2012, Coronation Street produced a Text Santa special entitled "A Christmas Corrie" which featured Norris Cole in the style of Scrooge, being visited by the ghosts of dead characters. The ghosts were Mike Baldwin, Maxine Peacock, Derek Wilton and Vera Duckworth. Other special guests include Torvill and Dean, Lorraine Kelly and Sheila Reid. The episode concluded with Norris learning the error of his ways and dancing on the cobbles. The original plan for this feature was to have included Jack Duckworth, along with Vera, but actor Bill Tarmey died before filming commenced. In the end a recording of his voice was played.
On 3 February 2014, another web-based miniseries ran on ITV.com; called "Streetcar Stories". It showed what Steve and Lloyd get up to during the late nights in their Streetcar cab office. The first episode shows Steve and Lloyd making a cup of tea with The Stripper playing in the background, referencing Morecambe and Wise's Breakfast Sketch. The second episode involves the pair having a biscuit dunking competition.
"Corrie Extra!".
ITV.com launched a small spin-off drama series called 'Gary's Army Diaries' which revolves around Gary's experiences in Afghanistan and the loss of his best friend, Quinny. Due to their popularity, the three five-minute episodes were recut into a single 30-minute episode, which was broadcast on ITV2.
William Roache and Anne Kirkbride starred as Ken and Deirdre in a series of ten three-minute internet 'webisodes'. The first episode of the series titled, "Ken and Deirdre's Bedtime Stories" was activated on Valentine's Day 2011.
In 2011, an internet based spin-off starring Helen Flanagan as Rosie Webster followed her on her quest to be a supermodel.
Stage.
In August 2010, many "Coronation Street" characters were brought to the stage in Jonathan Harvey's comedy play "Corrie!". The play was commissioned to celebrate the 50th Anniversary of the TV series and was presented at The Lowry in Salford, England by ITV Studios and Phil McIntyre Entertainments. Featuring a cast of six actors who alternate roles of favourite characters including Ena Sharples, Hilda Ogden, Hayley and Roy, Richard Hillman, Jack Duckworth, Bet Lynch, Steve, Karen and Becky, the play weaves together some of the most memorable moments from the TV show. It toured UK theatres between February 2011 and July 2011 with guest star narrators including Roy Barraclough, Ken Morley and Gaynor Faye.
Films.
Over the years "Coronation Street" has released several straight-to-video films. Unlike other soaps which often used straight-to-video films to cover more contentious plot lines that may not be allowed by the broadcaster, "Coronation Street" has largely used these films to reset their characters in other locations.
In 1995, "Coronation Street: The Cruise" also known as "Coronation Street: The Feature Length Special" was released on VHS to celebrate the 35th anniversary of the show. ITV heavily promoted the programme as a direct-to-video exclusive but broadcast a brief version of it on 24 March 1996. The Independent Television Commission investigated the broadcast, as viewers complained that ITV misled them.
In 1997, following the controversial cruise spin-off, "" was released on VHS, featuring Jack Duckworth, Vera Duckworth, Fiona Middleton and Maxine Peacock on a trip to Las Vegas.
In 1999, six special episodes of "Coronation Street" were produced, following the story of Steve McDonald, Vicky McDonald, Vikram Desai, Bet Gilroy and Reg Holdsworth in Brighton. This video was titled "Coronation Street: Open All Hours" and released on VHS.
In 2008, ITV announced filming was to get underway for a new special DVD episode, "", following the Battersby-Brown family, which included the temporary return of Cilla Battersby-Brown.
In 2009, another DVD special, "", was released. The feature-length comedy drama followed Roy, Hayley and Becky as they travelled to Romania for the wedding of a face from their past.
On 1 November 2010, "Coronation Street: A Knight's Tale" was released. Reg Holdsworth and Curly Watts returned in the film. Mary tries to take Norris to an apparently haunted castle where she hoped to seduce him. Rosie gets a job there and she takes Jason with her. Brian Capron also guest starred as an assumed relative of Richard Hillman. He rises out of a lake as a comedic "wink to the audience" after Hillman drowned in 2003.
50th Anniversary (2010).
The BBC commissioned a one-off drama called "The Road to Coronation Street", about how the series first came into being. Jessie Wallace plays Pat Phoenix (Elsie Tanner) with Lynda Baron as Violet Carson (Ena Sharples), Celia Imrie as Doris Speed (Annie Walker) and James Roache as his own father William Roache (Ken Barlow). It was broadcast on 16 September 2010 on BBC Four.
In December 2010, ITV made a few special programmes to mark the 50th anniversary. "Coronation Street Uncovered: Live", hosted by Stephen Mulhern was shown after the episode with the tram crash was aired on ITV 2. On 7 and 9 December a countdown on the greatest Corrie moments, "Coronation Street: 50 Years, 50 Moments", the viewers voted "The Barlows at Alcoholics Anonymous" as the greatest moment. On 10 December Paul O'Grady hosted a quiz show, "Coronation Street: The Big 50" with three teams from the soap and a celebrity team answering questions about Coronation Street and other soaps. Also, "Come Dine with Me" and "Celebrity Juice" aired Coronation Street specials in the anniversary week.
Crossovers.
"Coronation Street" and rival soap opera "EastEnders" had a crossover for "Children in Need" in November 2010 called "East Street". "EastEnders" stars that visited Weatherfield include Laurie Brett as Jane Beale, Charlie G. Hawkins as Darren Miller, Kylie Babbington as Jodie Gold, Nina Wadia as Zainab Masood and John Partridge as Christian Clarke.
"Coronation Street: Viva Las Vegas!" also included some characters from "Emmerdale".
Corrie in popular culture.
The British rock band Queen produced a single "I Want to Break Free" in 1984 which reached number 3 position in UK charts and which is largely known for its music video for which all the band members dressed in women's clothes, which parodied the characters and is considered as a homage to the show. The video depicts Mercury as a housewife, loosely based on Bet Lynch, who wants to "break free" from his life. Although Lynch was a blonde in the soap opera, Mercury thought he would look too silly as a blonde and chose a dark wig. May plays another, more relaxed housewife based on Hilda Ogden.
Sponsorship.
Cadbury was the first sponsor of "Coronation Street" beginning in July 1996. The original sponsorship had a chocolate-like version of the street (which can be seen in place at the Cadbury World museum in Bournville, Birmingham) with chocolate characters resembling some of the actual "Coronation Street" characters. In the summer of 2006, Cadbury Trebor Bassetts had to recall over one million chocolate bars, due to suspected salmonella contamination, and "Coronation Street" stopped the sponsorship for several months. In late 2006, Cadbury did not renew their contract, but agreed to sponsor the show until "Coronation Street" found a new sponsor.
In July 2007, an ITV press release announced that Harveys was the new sponsor of "Coronation Street" on the ITV Network. Harveys' sponsorship began on 30 September 2007. In the "Coronation Street: Romanian Holiday" film, Roy and Hayley Cropper are filmed in front of a Harveys store. In "Coronation Street: A Knights Tale", a Harveys truck can be seen driving past Mary Taylor's motor-home to further promote the brand. On 11 April 2012, it was announced that Harveys had decided not to renew their contract and ceased sponsorship in December 2012. Compare The Market were named as the new sponsor.
In November 2011 a Nationwide Building Society ATM in Dev and Sunita Alahan's shop became the first use of paid-for product placement in a UK primetime show.
Awards and nominations.
"Coronation Street" is the second most award-winning British soap opera in the UK, behind rival soap "EastEnders".
Producers.
The first producer was Stuart Latham, from December 1960 to July 1961. In the 1960s and 1970s, most producers did stints of about one year. Longer-running producers included Eric Prytherch (May 1972 – April 1974); Bill Podmore (September 1977 – July 1982); Carolyn Reynolds (1991–1993); and Sue Pritchard (1993–1996). From 2008 until Summer 2010 the soap was produced by Kim Crowther, who was replaced by Phil Collinson (producer of Doctor Who from 2005 to 2009). Collinson left the job in 2013, with Stuart Blackburn replacing him. Kate Oates was announced as Blackburn's successor in September 2015, beginning the role in early 2016.

</doc>
