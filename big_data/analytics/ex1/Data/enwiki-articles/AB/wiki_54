<doc id="9974" url="https://en.wikipedia.org/wiki?curid=9974" title="European Commission">
European Commission

The European Commission (EC) is the executive body of the European Union responsible for proposing legislation, implementing decisions, upholding the EU treaties and managing the day-to-day business of the EU. Commissioners swear an oath at the European Court of Justice in Luxembourg, pledging to respect the treaties and to be completely independent in carrying out their duties during their mandate.
The Commission operates as a cabinet government, with 28 members of the Commission (informally known as "commissioners").</ref> There is one member per member state, though members are bound to represent the interests of the EU as a whole rather than their home state. One of the 28 is the Commission President (currently Jean-Claude Juncker) proposed by the European Council and elected by the European Parliament. The Council then appoints the other 27 members of the Commission in agreement with the nominated President, and the 28 members as a single body are then subject to a vote of approval by the European Parliament. The current Commission is the Juncker Commission, which took office in late 2014.
The term "Commission" is used either in the narrow sense of the 28-member "College of Commissioners" (or "College") or to also include the administrative body of about 23,000 European civil servants who are split into departments called directorates-general and services.</ref> The procedural languages of the Commission are English, French and German.
The Members of the Commission and their "cabinets" (immediate teams) are based in the Berlaymont building in Brussels.
History.
The European Commission derives from one of the five key institutions created in the supranational European Community system, following the proposal of Robert Schuman, French Foreign Minister, on 9 May 1950. Originating in 1951 as the High Authority in the European Coal and Steel Community, the Commission has undergone numerous changes in power and composition under various presidents, involving three Communities.
Establishment.
The first Commission originated in 1951 as the nine-member "High Authority" under President Jean Monnet (see Monnet Authority). The High Authority was the supranational administrative executive of the new European Coal and Steel Community (ECSC). It took office first on 10 August 1952 in Luxembourg. In 1958 the Treaties of Rome had established two new communities alongside the ECSC: the European Economic Community (EEC) and the European Atomic Energy Community (Euratom). However their executives were called "Commissions" rather than "High Authorities". The reason for the change in name was the new relationship between the executives and the Council. Some states such as France expressed reservations over the power of the High Authority and wished to limit it giving more power to the Council rather than the new executives.
Louis Armand led the first Commission of Euratom. Walter Hallstein led the first Commission of the EEC, holding the first formal meeting on 16 January 1958 at the Château of Val-Duchesse. It achieved agreement on a contentious cereal price accord as well as making a positive impression upon third countries when it made its international debut at the Kennedy Round of General Agreement on Tariffs and Trade (GATT) negotiations. Hallstein notably began the consolidation of European law and started to have a notable impact on national legislation. Little heed was taken of his administration at first but, with help from the European Court of Justice, his Commission stamped its authority solidly enough to allow future Commissions to be taken more seriously. However, in 1965 accumulating differences between the French government of Charles de Gaulle and the other member states (over British entry, direct elections to Parliament, the Fouchet Plan and the budget) triggered the "empty chair" crisis ostensibly over proposals for the common agricultural policy. Although the institutional crisis was solved the following year, it cost Etienne Hirsch his presidency of Euratom and later Walter Hallstein the EEC presidency despite otherwise being viewed as the most 'dynamic' leader until Jacques Delors.
Early development.
The three bodies, collectively named the European Executives, co-existed until 1 July 1967 when, under the Merger Treaty, they were combined into a single administration under President Jean Rey. Due to the merger the Rey Commission saw a temporary increase to 14 members, although subsequent Commissions were reduced back down to nine, following the formula of one member for small states and two for larger states. The Rey Commission completed the Community's customs union in 1968 and campaigned for a more powerful, elected, European Parliament. Despite Rey being the first President of the combined communities, Hallstein is seen as the first President of the modern Commission.
The Malfatti and Mansholt Commissions followed with work on monetary co-operation and the first enlargement to the north in 1973. With that enlargement the Commission's membership increased to thirteen under the Ortoli Commission (the United Kingdom as a large member was granted two Commissioners), which dealt with the enlarged community during economic and international instability at that time. The external representation of the Community took a step forward when President Roy Jenkins, recruited to the presidency in January 1977 from his role as Home Secretary of the United Kingdom's Labour government, became the first President to attend a G8 summit on behalf of the Community. Following the Jenkins Commission, Gaston Thorn's Commission oversaw the Community's enlargement to the south, in addition to beginning work on the Single European Act.
Jacques Delors.
The Commission headed by Jacques Delors was seen as giving the Community a sense of direction and dynamism. Delors and his team are also considered as the "founding fathers of the euro". The International Herald Tribune noted the work of Delors at the end of his second term in 1992: "Mr. Delors rescued the European Community from the doldrums. He arrived when Europessimism was at its worst. Although he was a little-known former French finance minister, he breathed life and hope into the EC and into the dispirited Brussels Commission. In his first term, from 1985 to 1988, he rallied Europe to the call of the single market, and when appointed to a second term he began urging Europeans toward the far more ambitious goals of economic, monetary and political union."
Jacques Santer.
The successor to Delors was Jacques Santer. The entire Santer Commission was forced to resign in 1999 by the Parliament as result of a fraud and corruption scandal, with a central role played by Édith Cresson. These frauds were revealed by an internal auditor Paul van Buitenen.
That was the first time a Commission had been forced to resign "en masse" and represented a shift of power towards the Parliament. However the Santer Commission did carry out work on the Amsterdam Treaty and the euro. In response to the scandal the European Anti-Fraud Office (OLAF) was created.
Romano Prodi.
Following Santer, Romano Prodi took office. The Amsterdam Treaty had increased the Commission's powers and Prodi was dubbed by the press as something akin to a Prime Minister. Powers were strengthened again with the Nice Treaty in 2001 giving the Presidents more power over the composition of their Commissions.
José Manuel Barroso.
In 2004 José Manuel Barroso became President: the Parliament once again asserted itself in objecting to the proposed membership of the Barroso Commission. Due to the opposition Barroso was forced to reshuffle his team before taking office. The Barroso Commission was also the first full Commission since the enlargement in 2004 to 25 members and hence the number of Commissioners at the end of the Prodi Commission had reached 30. As a result of the increase in the number of states, the Amsterdam Treaty triggered a reduction in the number of Commissioners to one per state, rather than two for the larger states.
Allegations of fraud and corruption were again raised in 2004 by former chief auditor Jules Muis. A Commission officer Guido Strack reported alleged fraud and abuses in his department in years 2002–2004 to OLAF and was fired as result. In 2008 Paul van Buitenen (the former auditor known from Santer Commission scandal) accused the European Anti-Fraud Office (OLAF) of a lack of independence and effectiveness.
Barroso's first Commission term expired on 31 October 2009. Under the Treaty of Nice, the first Commission to be appointed after the number of member states reached 27 would have to be reduced to "less than the number of Member States". The exact number of Commissioners was to be decided by a unanimous vote of the European Council and membership would rotate equally between member states. Following the accession of Romania and Bulgaria in January 2007, this clause took effect for the next Commission. The Treaty of Lisbon, which came into force on 1 December 2009, mandated a reduction of the number of commissioners to two-thirds of member-states from 2014 unless the Council decided otherwise. Membership would rotate equally and no member state would have more than one Commissioner. However, the treaty was rejected by voters in Ireland in 2008 with one main concern being the loss of their Commissioner. Hence a guarantee given for a rerun of the vote was that the Council would use its power to amend the number of Commissioners upwards. However, according to the treaties it still has to be fewer than the total number of members, thus it was proposed that the member state that does not get a Commissioner would get the post of High Representative – the so-called 26+1 formula. This guarantee (which may find its way into the next treaty amendment, probably in an accession treaty) contributed to the Irish approving the treaty in a second referendum in 2009.
Lisbon also combined the posts of European Commissioner for External Relations with the Council's High Representative for the Common Foreign and Security Policy. This post, also a Vice-President of the Commission, would chair the Council of the European Union's foreign affairs meetings as well as the Commission's external relations duties. The treaty further provides that the most recent European elections should be ""taken into account"" when appointing the Commission, although the President is still proposed by the European Council; the European Parliament ""elects"" the Commission rather than ""approves"" it as under the Treaty of Nice.
Jean-Claude Juncker.
In 2014, Jean-Claude Juncker became President of the European Commission.
Powers and functions.
The Commission was set up from the start to act as an independent supranational authority separate from governments; it has been described as "the only body paid to think European". The members are proposed by their member state governments, one from each. However, they are bound to act independently – neutral from other influences such as those governments which appointed them. This is in contrast to the Council, which represents governments, the Parliament, which represents citizens, the Economic and Social Committee, which represents organised civil society, and the Committee of the Regions, which represents local and regional authorities.
Through the Commission has several responsibilities: to develop medium-term strategies; to draft legislation and arbitrate in the legislative process; to represent the EU in trade negotiations; to make rules and regulations, for example in competition policy; to draw up the budget of the European Union; and to scrutinise the implementation of the treaties and legislation. The rules of procedure of the European Commission set out the Commission's operation and organisation.
Executive power.
Before the Treaty of Lisbon came into force, the executive power of the EU was held by the Council: it conferred on the Commission such powers for it to exercise. However, the Council was theoretically allowed to withdraw these powers, exercise them directly, or impose conditions on their use. This aspect has been changed by the Treaty of Lisbon, after which the Commission exercises its powers just by virtue of the treaties. Powers are more restricted than most national executives, in part due to the Commission's lack of power over areas like foreign policy – that power is held by the European Council, which some analysts have described as another executive.
Considering that under the Lisbon Treaty the European Council has become a formal institution with the power of appointing the Commission, it could be said that the two bodies hold the executive power of the EU (the European Council also holds individual national executive powers). However, it is the Commission that currently holds executive powers over the European Union. The governmental powers of the Commission have been such that some such as former Belgian Prime Minister Guy Verhofstadt have suggested changing its name to the "European Government", calling the present name of the Commission "ridiculous".
Legislative initiative.
The Commission differs from the other institutions in that it alone has legislative initiative in the EU. Only the Commission can make formal proposals for legislation: they cannot originate in the legislative branches. Under the Treaty of Lisbon, no legislative act is allowed in the field of the Common Foreign and Security Policy. In the other fields the Council and Parliament are able to request legislation; in most cases the Commission initiates the basis of these proposals. This monopoly is designed to ensure coordinated and coherent drafting of EU law. This monopoly has been challenged by some who claim the Parliament should also have the right, with most national parliaments holding the right in some respects. However, the Council and Parliament may request the Commission to draft legislation, though the Commission does have the power to refuse to do so as it did in 2008 over transnational collective conventions. Under the Lisbon Treaty, EU citizens are also able to request the Commission to legislate in an area via a petition carrying one million signatures, but this is not binding.
The Commission's powers in proposing law have usually centred on economic regulation. It has put forward a large number of regulations based on a "precautionary principle". This means that pre-emptive regulation takes place if there is a credible hazard to the environment or human health: for example on tackling climate change and restricting genetically modified organisms. This is opposed to weighting regulations for their effect on the economy. Thus, the Commission often proposes stricter legislation than other countries. Due to the size of the European market this has made EU legislation an important influence in the global market.
Recently the Commission has moved into creating European criminal law. In 2006, a toxic waste spill off the coast of Côte d'Ivoire, from a European ship, prompted the Commission to look into legislation against toxic waste. Some EU states at that time did not even have a crime against shipping toxic waste leading to the Commissioners Franco Frattini and Stavros Dimas to put forward the idea of "ecological crimes". Their right to propose criminal law was challenged in the European Court of Justice but upheld. As of 2007, the only other criminal law proposals which have been brought forward are on the intellectual property rights directive, and on an amendment to the 2002 counter-terrorism framework decision, outlawing terrorism‑related incitement, recruitment (especially via the internet) and training.
Enforcement.
Once legislation is passed by the Council and Parliament, it is the Commission's responsibility to ensure it is implemented. It does this through the member states or through its agencies. In adopting the necessary technical measures, the Commission is assisted by committees made up of representatives of member states and of the public and private lobbies (a process known in jargon as "comitology"). Furthermore, the Commission is responsible for the implementation of the EU budget; ensuring, along with the Court of Auditors, that EU funds are correctly spent.
In particular the Commission has a duty to ensure the treaties and law are upheld, potentially by taking member states or other institutions to the Court of Justice in a dispute. In this role it is known informally as the "guardian of the treaties". Finally, the Commission provides some external representation for the Union, alongside the member states and the Common Foreign and Security Policy, representing the Union in bodies such as the World Trade Organisation. It is also usual for the President to attend meetings of the G8.
College.
The Commission is composed of a college of "Commissioners" of 28 members, including the President and vice-presidents. Even though each member is appointed by a national government, one per state, they do not represent their state in the Commission. In practice, however, they do occasionally press for their national interest. Once proposed, the President delegates portfolios among each of the members. The power of a Commissioner largely depends upon their portfolio, and can vary over time. For example, the Education Commissioner has been growing in importance, in line with the rise in the importance of education and culture in European policy-making. Another example is the Competition Commissioner, who holds a highly visible position with global reach. Before the Commission can assume office, the college as a whole must be approved by the Parliament. Commissioners are supported by their personal cabinet who give them political guidance, while the Civil Service (the DGs, see below) deal with technical preparation.
Appointment.
The President of the Commission is first proposed by the European Council taking into account the latest Parliamentary elections; that candidate can then be elected by the European Parliament or not. If not, the European Council shall propose another candidate within one month.
The candidate has often been a leading national politician, but this is not a requirement. In 2009, the Lisbon Treaty was not in force and Barroso was not "elected" by the Parliament, but rather nominated by the European Council; in any case, the centre-right parties of the EU pressured for a candidate from their own ranks. In the end, a centre-right candidate was chosen: José Manuel Barroso of the European People's Party.
There are further criteria influencing the choice of the candidate, including: which area of Europe the candidate comes from, favoured as Southern Europe in 2004; the candidate's political influence, credible yet not overpowering members; language, proficiency in French considered necessary by France; and degree of integration, their state being a member of both the eurozone and the Schengen Agreement. In 2004, this system produced a number of candidates and was thus criticised by some MEPs: following the drawn-out selection, the ALDE group leader Graham Watson described the procedure as a "Justus Lipsius carpet market" producing only the "lowest common denominator"; while Green-EFA co-leader Daniel Cohn-Bendit asked Barroso after his first speech "If you are the best candidate, why were you not the first?"
Following the election of the President, and the appointment of the High Representative by the European Council, each Commissioner is nominated by their member state (except for those states who provided the President and High Representative) in consultation with the Commission President, although he holds no hard power to force a change in candidate. However the more capable the candidate is, the more likely the Commission President will assign them a powerful portfolio, the distribution of which is entirely at his discretion. The President's team is then subject to hearings at the European Parliament which will question them and then vote on their suitability as a whole. If members of the team are found to be too inappropriate, the President must then reshuffle the team or request a new candidate from the member state or risk the whole Commission being voted down. As Parliament cannot vote against individual Commissioners there is usually a compromise whereby the worst candidates are removed but minor objections are put aside so the Commission can take office. Once the team is approved by parliament, it is formally put into office by the European Council ().
Following their appointment, the President appoints a number of Vice-Presidents (the High Representative is mandated to be one of them) from among the commissioners. For the most part, the position grants little extra power to Vice-Presidents, except the first Vice-President who stands in for the President when he is away. Since 2009 the First Vice-President has gained further power by also being the High Representative.
Dismissal.
The European Parliament can dissolve the Commission as a whole following a vote of no-confidence but only the President can request the resignation of an individual Commissioner. However, individual Commissioners, by request of the Council or Commission, can be compelled to retire on account of a breach of obligation(s) and if so ruled by the European Court of Justice (Art. 245 and 247, Treaty on the Functioning of the European Union).
Political styles.
The Barroso Commission took office in late 2004 after being delayed by objections from the Parliament, which forced a reshuffle. In 2007 the Commission increased from 25 to 27 members with the accession of Romania and Bulgaria who each appointed their own Commissioners. With the increasing size of the Commission, Barroso adopted a more Presidential style of control over the college, which earned him some criticism.
However, under Barroso, the Commission began to lose ground to the larger member states as countries such as France, the UK and Germany sought to sideline its role. This has increased with the creation of the President of the European Council under the Treaty of Lisbon. There has also been a greater degree of politicisation within the Commission.
Administration.
The Commission is divided into departments known as Directorates-General (DGs) that can be likened to departments or ministries. Each covers a specific policy area such as Agriculture or Justice and citizens' rights or internal services such as Human Resources and Translation and is headed by Director-General who is responsible to a Commissioner. A Commissioner's portfolio can be supported by numerous DGs, they prepare proposals for them and if approved by a majority of Commissioners it goes forward to Parliament and Council for consideration. The Commission's civil service is headed by a Secretary General, currently Alexander Italianer. The rules of procedure of the European Commission set out the Commission's operation and organisation.
There has been criticism from a number of people that the highly fragmented DG structure wastes a considerable amount of time in turf wars as the different departments and Commissioners compete with each other. Furthermore, the DGs can exercise considerable control over a Commissioner with the Commissioner having little time to learn to assert control over their staff.
According to figures published by the Commission, 23,803 persons were employed by the Commission as officials and temporary agents in September 2012. In addition to these, 9230 "external staff" (e.g. Contractual agents, detached national experts, young experts, trainees etc.) were employed. The single largest DG is the Directorate-General for Translation, with a 2309-strong staff, while the largest group by nationality is Belgian (18.7%), probably due to a majority (17,664) of staff being based in the country.
Press.
Communication with the press is handled by the Directorate-General Communication. The Commission's chief spokesperson is Pia Ahrenkilde Hansen who takes the midday press briefings, commonly known as the "Midday Presser". It takes place every weekday in the Commission's press room at the Berlaymont where journalists may ask questions of Commission officials on any topic and legitimately expect to get an "on the record" answer for live TV. Such a situation is unique in the world.
It has been noted by one researcher that the press releases issued by the Commission are uniquely political. A release often goes through several stages of drafting which emphasises the role of the Commission and is used "for justifying the EU and the commission" increasing their length and complexity. Where there are multiple departments involved a press release can also be a source of competition between areas of the Commission and Commissioners themselves. This also leads to an unusually high number of press releases, 1907 for 2006, and is seen as a unique product of the EU's political set-up. The number of Commission press releases shows a decreasing trend. 1768 press releases were published in 2010 and 1589 in 2011.
There is a larger press corps in Brussels than Washington D.C.; in 2007 media outlets in every Union member-state had a Brussels correspondent. However, since the global downturn by 2010 the press corps in Brussels shrunk by a third. There is one journalist covering EU news for Latvia and none for Lithuania. Although there has been a worldwide cut in journalists, the considerable press releases and operations such as Europe by Satellite and EuroparlTV leads many news organisations to believe they can cover the EU from these source and news agencies. In the face of high-level criticism, the Commission is also due to shut down Presseurop on 20 December 2013.
Legitimacy.
While the Commission is the executive branch, the candidates are chosen individually by the 28 national governments, which means it is not possible for a Commission Member or its President to be removed by a direct election. Rather, the legitimacy of the Commission is mainly drawn from the vote of approval that is required from the European Parliament, along with Parliament's power to dismiss the body, which, in turn, raises the concern of the relatively low turnout (less than 50%) in elections for the European Parliament since 1999. While that figure may be higher than that of some national elections, including the off-year elections of the United States Congress, the fact that there are no elections for the position of Commission President calls the position's legitimacy into question in the eyes of some. The fact that the Commission can directly decide (albeit with oversight from specially formed 'comitology committees') on the shape and character of implementing legislation further raises concerns about democratic legitimacy.
Even though democratic structures and methods are developing there is not such a mirror in creating a European civil society. The Treaty of Lisbon may go some way to resolving the deficit in creating greater democratic controls on the Commission, including enshrining the procedure of linking elections to the selection of the Commission president. An alternative viewpoint is that electoral pressures undermine the Commission's role as an independent regulator, considering it akin with institutions such as independent central banks which deal with technical areas of policy. In addition some defenders of the Commission point out that legislation must be approved by the Council in all areas (the ministers of member states) and the European Parliament in some areas before it can be adopted, thus the amount of legislation which is adopted in any one country without the approval of its government is limited.
In 2009 the European ombudsman published statistics of citizens' complaints against EU institutions, with most of them filed against the Commission (66%) and concerning lack of transparency (36%). In 2010 the Commission was sued for blocking access to documents on EU biofuel policy. This happened after media accused the Commission of blocking scientific evidence against biofuel subsidies. Lack of transparency, unclear lobbyist relations, conflicts of interests and excessive spending of the Commission was highlighted in a number of reports by internal and independent auditing organisations. It has also been criticised on IT-related issues, particularly with regard to Microsoft.
Location.
The Commission is primarily based in Brussels, with the President's office and the Commission's meeting room on the 13th floor of the Berlaymont building. The Commission also operates out of numerous other buildings in Brussels and Luxembourg. When the Parliament is meeting in Strasbourg, the Commissioners also meet there in the Winston Churchill building to attend the Parliament's debates.

</doc>
<doc id="9975" url="https://en.wikipedia.org/wiki?curid=9975" title="Linear filter">
Linear filter

Linear filters process time-varying input signals to produce output signals, subject to the constraint of linearity. This results from systems composed solely of components (or digital algorithms) classified as having a linear response. Most filters implemented in analog electronics, in digital signal processing, or in mechanical systems are classified as causal, time invariant, and linear signal processing filters.
The general concept of linear filtering is also used in statistics, data analysis, and mechanical engineering among other fields and technologies. This includes non-causal filters and filters in more than one dimension such as those used in image processing; those filters are subject to different constraints leading to different design methods.
Impulse response and transfer function.
A linear time-invariant (LTI) filter can be uniquely specified by its impulse response "h", and the output of any filter is mathematically expressed as the convolution of the input with that impulse response. The frequency response, given by the filter's transfer function formula_1, is an alternative characterization of the filter. Typical filter design goals are to realize a particular frequency response, that is, the magnitude of the transfer function formula_2; the importance of the phase of the transfer function varies according to the application, inasmuch as the shape of a waveform can be distorted to a greater or lesser extent in the process of achieving a desired (amplitude) response in the frequency domain. The frequency response may be tailored to, for instance, eliminate unwanted frequency components from an input signal, or to limit an amplifier to signals within a particular band of frequencies.
The impulse response "h" of a linear time-invariant causal filter specifies the output that the filter would produce if it were to receive an input consisting of a single impulse at time 0. An "impulse" in a continuous time filter means a Dirac delta function; in a discrete time filter the Kronecker delta function would apply. The impulse response completely characterizes the response of any such filter, inasmuch as any possible input signal can be expressed as a (possibly infinite) combination of weighted delta functions. Multiplying the impulse response shifted in time according to the arrival of each of these delta functions by the amplitude of each delta function, and summing these responses together (according to the superposition principle, applicable to all linear systems) yields the output waveform.
Mathematically this is described as the convolution of a time-varying input signal "x(t)" with the filter's impulse response "h", defined as:
The first form is the continuous-time form, which describes mechanical and analog electronic systems, for instance. The second equation is a discrete-time version used, for example, by digital filters implemented in software, so-called "digital signal processing". The impulse response "h" completely characterizes any linear time-invariant (or shift-invariant in the discrete-time case) filter. The input "x" is said to be "convolved" with the impulse response "h" having a (possibly infinite) duration of time "T" (or of "N" sampling periods).
Filter design consists of finding a possible transfer function that can be implemented within certain practical constraints dictated by the technology or desired complexity of the system, followed by a practical design that realizes that transfer function using the chosen technology. The complexity of a filter may be specified according to the order of the filter.
Among the time-domain filters we here consider, there are two general classes of filter transfer functions that can approximate a desired frequency response. Very different mathematical treatments apply to the design of filters termed infinite impulse response (IIR) filters, characteristic of mechanical and analog electronics systems, and finite impulse response (FIR) filters, which can be implemented by discrete time systems such as computers (then termed "digital signal processing").
Infinite impulse response filters.
Consider a physical system that acts as a linear filter, such as a system of springs and masses, or an analog electronic circuit that includes capacitors and/or inductors (along with other linear components such as resistors and amplifiers). When such a system is subject to an impulse (or any signal of finite duration) it responds with an output waveform that lasts past the duration of the input, eventually decaying exponentially in one or another manner, but never completely settling to zero (mathematically speaking). Such a system is said to have an infinite impulse response (IIR). The convolution integral (or summation) above extends over all time: T (or N) must be set to infinity.
For instance, consider a damped harmonic oscillator such as a pendulum, or a resonant L-C tank circuit. If the pendulum has been at rest and we were to strike it with a hammer (the "impulse"), setting it in motion, it would swing back and forth ("resonate"), say, with an amplitude of 10 cm. After 10 minutes, say, the pendulum would still be swinging but the amplitude would have decreased to 5 cm, half of its original amplitude. After another 10 minutes its amplitude would be only 2.5 cm, then 1.25 cm, etc. However it would never come to a complete rest, and we therefore call that response to the impulse (striking it with a hammer) "infinite" in duration.
The complexity of such a system is specified by its order "N". N is often a constraint on the design of a transfer function since it specifies the number of reactive components in an analog circuit; in a digital IIR filter the number of computations required is proportional to N.
Finite impulse response filters.
A filter implemented in a computer program (or a so-called digital signal processor) is a discrete-time system; a different (but parallel) set of mathematical concepts defines the behavior of such systems. Although a digital filter can be an IIR filter if the algorithm implementing it includes feedback, it is also possible to easily implement a filter whose impulse truly goes to zero after N time steps; this is called a finite impulse response (FIR) filter.
For instance, suppose one has a filter that, when presented with an impulse in a time series:
outputs a series that responds to that impulse at time 0 until time 4, and has no further response, such as:
Although the impulse response has lasted 4 time steps after the input, starting at time 5 it has truly gone to zero. The extent of the impulse response is "finite", and this would be classified as a fourth-order FIR filter.
The convolution integral (or summation) above need only extend to the full duration of the impulse response T, or the order N in a discrete time filter.
Implementation issues.
Classical analog filters are IIR filters, and classical filter theory centers on the determination of transfer functions given by low order rational functions, which can be synthesized using the same small number of reactive components. Using digital computers, on the other hand, both FIR and IIR filters are straightforward to implement in software.
A digital IIR filter can generally approximate a desired filter response using less computing power than a FIR filter, however this advantage is more often unneeded given the increasing power of digital processors. The ease of designing and characterizing FIR filters makes them preferable to the filter designer (programmer) when ample computing power is available. Another advantage of FIR filters is that their impulse response can be made symmetric, which implies a response in the frequency domain that has zero phase at all frequencies (not considering a finite delay), which is absolutely impossible with any IIR filter.
Frequency response.
The frequency response or transfer function formula_2 of a filter can be obtained if the impulse response is known, or directly through analysis using Laplace transforms, or in discrete-time systems the Z-transform. The frequency response also includes the phase as a function of frequency, however in many cases the phase response is of little or no interest. FIR filters can be made to have zero phase, but with IIR filters that is generally impossible. With most IIR transfer functions there are related transfer functions having a frequency response with the same magnitude but a different phase; in most cases the so-called minimum phase transfer function is preferred.
Filters in the time domain are most often requested to follow a specified frequency response. Then, a mathematical procedure finds a filter transfer function that can be realized (within some constraints), and approximates the desired response to within some criterion. Common filter response specifications are described as follows:
FIR transfer functions.
Meeting a frequency response requirement with an FIR filter uses relatively straightforward procedures. In the most basic form, the desired frequency response itself can be sampled with a resolution of formula_6 and Fourier transformed to the time domain. This obtains the filter coefficients "hi", which implements a zero phase FIR filter that matches the frequency response at the sampled frequencies used. To better match a desired response, formula_6 must be reduced. However the duration of the filter's impulse response, and the number of terms that must be summed for each output value (according to the above discrete time convolution) is given by formula_8 where "T" is the sampling period of the discrete time system (N-1 is also termed the "order" of an FIR filter). Thus the complexity of a digital filter and the computing time involved, grows inversely with formula_6, placing a higher cost on filter functions that better approximate the desired behavior. For the same reason, filter functions whose critical response is at lower frequencies (compared to the sampling frequency "1/T") require a higher order, more computationally intensive FIR filter. An IIR filter can thus be much more efficient in such cases.
Elsewhere the reader may find further discussion of design methods for practical FIR filter design.
IIR transfer functions.
Since classical analog filters are IIR filters, there has been a long history of studying the range of possible transfer functions implementing various of the above desired filter responses in continuous time systems. Using transforms it is possible to convert these continuous time frequency responses to ones that are implemented in discrete time, for use in digital IIR filters. The complexity of any such filter is given by the "order" N, which describes the order of the rational function describing the frequency response. The order N is of particular importance in analog filters, because an Nth order electronic filter requires N reactive elements (capacitors and/or inductors) to implement. If a filter is implemented using, for instance, biquad stages using op-amps, N/2 stages are needed. In a digital implementation, the number of computations performed per sample is proportional to N. Thus the mathematical problem is to obtain the best approximation (in some sense) to the desired response using a smaller N, as we shall now illustrate.
Below are the frequency responses of several standard filter functions that approximate a desired response, optimized according to some criterion. These are all fifth-order low-pass filters, designed for a cutoff frequency of .5 in normalized units. Frequency responses are shown for the Butterworth, Chebyshev, inverse Chebyshev, and elliptic filters.
As is clear from the image, the elliptic filter is sharper than the others, but at the expense of ripples in both its passband and stopband. The Butterworth filter has the poorest transition but has a more even response, avoiding ripples in either the passband or stopband. A Bessel filter (not shown) has an even poorer transition in the frequency domain, but maintains the best phase fidelity of a waveform. Different applications emphasize different design requirements, leading to different choices among these (and other) optimizations, or requiring a filter of a higher order.
Example implementations.
A popular circuit implementing a second order active R-C filter is the Sallen-Key design, whose schematic diagram is shown here. This topology can be adapted to produce low-pass, band-pass, and high pass filters.
An Nth order FIR filter can be implemented in a discrete time system using a computer program or specialized hardware in which the input signal is subject to N delay stages. The output of the filter is formed as the weighted sum of those delayed signals, as is depicted in the accompanying signal flow diagram. The response of the filter depends on the weighting coefficients denoted "b0", "b1", ... "bN". For instance, if all of the coefficients were equal to unity, a so-called boxcar function, then it would implement a low-pass filter with a low frequency gain of N+1 and a frequency response given by the sinc function. Superior shapes for the frequency response can be obtained using coefficients derived from a more sophisticated design procedure.
Mathematics of filter design.
LTI system theory describes linear "time-invariant" (LTI) filters of all types. LTI filters can be completely described by their frequency response and phase response, the specification of which uniquely defines their impulse response, and "vice versa". From a mathematical viewpoint, continuous-time IIR LTI filters may be described in terms of linear differential equations, and their impulse responses considered as Green's functions of the equation. Continuous-time LTI filters may also be described in terms of the Laplace transform of their impulse response, which allows all of the characteristics of the filter to be analyzed by considering the pattern of poles and zeros of their Laplace transform in the complex plane. Similarly, discrete-time LTI filters may be analyzed via the Z-transform of their impulse response.
Before the advent of computer filter synthesis tools, graphical tools such as Bode plots and Nyquist plots were extensively used as design tools. Even today, they are invaluable tools to understanding filter behavior. Reference books had extensive plots of frequency response, phase response, group delay, and impulse response for various types of filters, of various orders. They also contained tables of values showing how to implement such filters as RLC ladders - very useful when amplifying elements were expensive compared to passive components. Such a ladder can also be designed to have minimal sensitivity to component variation a property hard to evaluate without computer tools.
Many different analog filter designs have been developed, each trying to optimise some feature of the system response. For practical filters, a custom design is sometimes desirable, that can offer the best tradeoff between different design criteria, which may include component count and cost, as well as filter response characteristics.
These descriptions refer to the "mathematical" properties of the filter (that is, the frequency and phase response). These can be "implemented" as analog circuits (for instance, using a Sallen Key filter topology, a type of active filter), or as algorithms in digital signal processing systems.
Digital filters are much more flexible to synthesize and use than analog filters, where the constraints of the design permits their use. Notably, there is no need to consider component tolerances, and very high Q levels may be obtained.
FIR digital filters may be implemented by the direct convolution of the desired impulse response with the input signal.
They can easily be designed to give a matched filter for any arbitrary pulse shape.
IIR digital filters are often more difficult to design, due to problems including dynamic range issues, quantization noise and instability.
Typically digital IIR filters are designed as a series of digital biquad filters.
All low-pass second-order continuous-time filters have a transfer function given by
All band-pass second-order continuous-time have a transfer function given by
where

</doc>
<doc id="9976" url="https://en.wikipedia.org/wiki?curid=9976" title="Ergative case">
Ergative case

The ergative case (abbreviated ) is the grammatical case that identifies the noun as a subject of a transitive verb in ergative–absolutive languages.
Characteristics.
In such languages, the ergative case is typically marked (most salient), while the absolutive case is unmarked. New work in case theory has vigorously supported the idea that the ergative case identifies the agent (the intentful performer of an action) of a verb (Woolford 2004).
In Kalaallisut (Greenlandic) for example, the ergative case is used to mark subjects of transitive verbs and possessors of nouns.
Nez Perce has a three-way nominal case system with both ergative ("-nim") and accusative ("-ne") plus an absolute (unmarked) case for intransitive subjects: "hipáayna qíiwn" ‘the old man arrived’; "hipáayna wewúkiye" ‘the elk arrived’; "wewúkiyene péexne qíiwnim" ‘the old man saw an elk’.
Sahaptin has an ergative noun case (with suffix "-nɨm") that is limited to transitive constructions only when the direct object is 1st or 2nd person: "iwapáatayaaš łmámanɨm" ‘the old woman helped me’; "paanáy iwapáataya łmáma" ‘the old woman helped him/her’ (direct); "páwapaataya łmámayin" ‘the old woman helped him/her’ (inverse).
Other languages that use the ergative case are Georgian, Chechen, and other Caucasian languages, Mayan languages, Mixe–Zoque languages, Wagiman and other Australian Aboriginal languages as well as Basque, Burushaski, Yaghnobi and Tibetan. Among all Indo-European languages only Zazaki, Kurdish language varieties (including Kurmanji and Sorani), and Hindi/Urdu along with most Indo-Aryan languages, are ergative.
The ergative case is also a feature of some constructed languages such as Na'vi, Ithkuil and Black Speech.

</doc>
<doc id="9977" url="https://en.wikipedia.org/wiki?curid=9977" title="Ewe">
Ewe


</doc>
<doc id="9978" url="https://en.wikipedia.org/wiki?curid=9978" title="Essenes">
Essenes

The Essenes (in Modern Hebrew: , "Isiyim"; Greek: Ἐσσηνοί, Ἐσσαῖοι, or Ὀσσαῖοι, "Essenoi, Essaioi, Ossaioi") were a sect of Second Temple Judaism that flourished from the 2nd century BC to the 1st century AD which some scholars claim seceded from the Zadokite priests. Being much fewer in number than the Pharisees and the Sadducees (the other two major sects at the time), the Essenes lived in various cities but congregated in communal life dedicated to asceticism (some groups practiced celibacy), voluntary poverty, and daily immersion. Many separate but related religious groups of that era shared similar mystic, eschatological, messianic, and ascetic beliefs. These groups are collectively referred to by various scholars as the "Essenes." Josephus records that Essenes existed in large numbers, and thousands lived throughout Roman Judaea.
The Essenes have gained fame in modern times as a result of the discovery of an extensive group of religious documents known as the Dead Sea Scrolls, which are commonly believed to be the Essenes' library—although not conclusive. These documents preserve multiple copies of parts of the Hebrew Bible untouched from possibly as early as 300 BCE until their discovery in 1946. Some scholars dispute the notion that the Essenes wrote the Dead Sea Scrolls. Rachel Elior questions even the existence of the Essenes.
The first reference is by the Roman writer Pliny the Elder (died ) in his "Natural History". Pliny relates in a few lines that the Essenes do not marry, possess no money, and had existed for thousands of generations. Unlike Philo, who did not mention any particular geographical location of the Essenes other than the whole land of Israel, Pliny places them in Ein Gedi, next to the Dead Sea.
A little later, Josephus gave a detailed account of the Essenes in "The Jewish War" (), with a shorter description in "Antiquities of the Jews" () and "The Life of Flavius Josephus" (). Claiming first hand knowledge, he lists the "Essenoi" as one of the three sects of Jewish philosophy alongside the Pharisees and the Sadducees. He relates the same information concerning piety, celibacy, the absence of personal property and of money, the belief in communality, and commitment to a strict observance of Sabbath. He further adds that the Essenes ritually immersed in water every morning, ate together after prayer, devoted themselves to charity and benevolence, forbade the expression of anger, studied the books of the elders, preserved secrets, and were very mindful of the names of the angels kept in their sacred writings.
Pliny, also a geographer, located them in the desert near the northwestern shore of the Dead Sea, where the Dead Sea Scrolls were discovered.
Name.
Josephus uses the name "Essenes" in his two main accounts but some manuscripts read here "Essaion"; "holding the Essenes in honour"; "a certain Essene named Manaemus"; "to hold all Essenes in honor"; "the Essenes").
In several places, however, Josephus has "Essaios", which is usually assumed to mean "Essene" ("Judas of the "Essaios" race"; "Simon of the "Essaios" race"; "John the "Essaios""; "those who are called by us "Essaioi""; "Simon a man of the "Essaios" race"). Josephus identified the Essenes as one of the three major Jewish sects of that period.
Philo's usage is "Essaioi", although he admits this Greek form of the original name that according to his etymology signifies "holiness" to be inexact. Pliny's Latin text has "Esseni".
Gabriele Boccaccini implies that a convincing etymology for the name Essene has not been found, but that the term applies to a larger group within Palestine that also included the Qumran community.
It was proposed before the Dead Sea Scrolls were discovered that the name came into several Greek spellings from a Hebrew self-designation later found in some Dead Sea Scrolls, '"osey hatorah", "observers of torah". Although dozens of etymology suggestions have been published, this is the only etymology published before 1947 that was confirmed by Qumran text self-designation references, and it is gaining acceptance among scholars. It is recognized as the etymology of the form "Ossaioi" (and note that Philo also offered an O spelling) and "Essaioi" and "Esseni" spelling variations have been discussed by VanderKam, Goranson, and others. In medieval Hebrew (e.g. Sefer Yosippon) "Hassidim" ("the pious ones") replaces "Essenes". While this Hebrew name is not the etymology of "Essaioi"/"Esseni", the Aramaic equivalent "Hesi'im" known from Eastern Aramaic texts has been suggested. Others suggest that Essene is a transliteration of the Hebrew word "chitzonim" (chitzon=outside), which the Mishna (e.g. Megila 4:8) uses to describe various sectarian groups. Another theory is that the name was borrowed from a cult of devotees to Artemis in Asia Minor, whose demeanor and dress somewhat resembled those of the group in Judaea.
However, Flavius Josephus – born Yosef ben Mattathias – was the son of a priestly family on both sides and a self-described Pharisee. "From ages sixteen to nineteen, according to his autobiography, Josephus experimented with the various Jewish sects in order to choose the best, finally deciding on the Pharisees as the most attuned to the people. In an apparent chronological conflict, however, Josephus also states that he spent these three years with a desert ascetic named Bannus, a period that ended when he was nineteen." We come to understand his true feelings about these so-called "Essenes" in Chapter 8 of "The Jewish War" as follows:
Location.
According to Josephus, the Essenes had settled "not in one city" but "in large numbers in every town". Philo speaks of "more than four thousand" "Essaioi" living in "Palestine and Syria", more precisely, "in many cities of Judaea and in many villages and grouped in great societies of many members".
Pliny locates them "on the west side of the Dead Sea, away from the coast... the town of Engeda".
Some modern scholars and archaeologists have argued that Essenes inhabited the settlement at Qumran, a plateau in the Judean Desert along the Dead Sea, citing Pliny the Elder in support, and giving credence that the Dead Sea Scrolls are the product of the Essenes. This theory, though not yet conclusively proven, has come to dominate the scholarly discussion and public perception of the Essenes.
Josephus' reference to a "gate of the Essenes" in his description of the course of "the most ancient" of the three walls of Jerusalem, in the Mount Zion area, perhaps suggests an Essene community living in this quarter of the city or regularly gathering at this part of the Temple precincts.
Rules, customs, theology, and beliefs.
The accounts by Josephus and Philo show that the Essenes led a strictly communal lifeoften compared by scholars to later Christian monastic living. Many of the Essene groups appear to have been celibate, but Josephus speaks also of another ""order" of Essenes" that observed the practice of being engaged for three years and then becoming married. According to Josephus, they had customs and observances such as collective ownership, electing a leader to attend to the interests of the group, and obedience to the orders from their leader. Also, they were forbidden from swearing oaths and from sacrificing animals. They controlled their tempers and served as channels of peace, carrying weapons only for protection against robbers. The Essenes chose not to possess slaves but served each other and, as a result of communal ownership, did not engage in trading. Josephus and Philo provide lengthy accounts of their communal meetings, meals and religious celebrations.
After a total of three years' probation, newly joining members would take an oath that included the commitment to practice piety towards "the Deity" (το θειον) and righteousness towards humanity, to maintain a pure lifestyle, to abstain from criminal and immoral activities, to transmit their rules uncorrupted and to preserve the books of the Essenes and the names of the Angels. Their theology included belief in the immortality of the soul and that they would receive their souls back after death. Part of their activities included purification by water rituals, which was supported by rainwater catchment and storage.
Ritual purification was a common practice among the peoples of Palestine during this period and was thus not specific to the Essenes. Ritual baths are found near many Synagogues of the period. Purity and cleanliness was considered so important to the Essenes that they would refrain from defecation on the Sabbath.
The Church Father Epiphanius (writing in the 4th century CE) seems to make a distinction between two main groups within the Essenes: ""Of those that came before his an Ossaean prophet time and during it, the Ossaeans and the Nazarean."" Epiphanius describes each group as following:
If it is correct to identify the community at Qumran with the Essenes (and claim that the community at Qumran are the authors of the Dead Sea Scrolls), then according to the Dead Sea Scrolls the Essenes' community school was called "Yahad" (meaning "community") in order to differentiate themselves from the rest of the Jews who are repeatedly labeled "The Breakers of the Covenant".
Scholarly discussion.
Josephus and Philo discuss the Essenes in detail. Most scholars believe that the community at Qumran that allegedly produced the Dead Sea Scrolls was an offshoot of the Essenes; however, this theory has been disputed by some, for example, Norman Golb argues that the primary research on the Qumran documents and ruins (by Father Roland de Vaux, from the "École Biblique et Archéologique de Jérusalem") lacked scientific method, and drew wrong conclusions that comfortably entered the academic canon. For Golb, the amount of documents is too extensive and includes many different writing styles and calligraphies; the ruins seem to have been a fortress, used as a military base for a very long period of timeincluding the 1st centuryso they could not have been inhabited by the Essenes; and the large graveyard excavated in 1870, just 50 metres east of the Qumran ruins was made of over 1200 tombs that included many women and childrenPliny clearly wrote that the Essenes who lived near the Dead Sea "had not one woman, had renounced all pleasure ... and no one was born in their race". Golb's book presents observations about de Vaux's premature conclusions and their uncontroverted acceptance by the general academic community. He states that the documents probably stemmed from various libraries in Jerusalem, kept safe in the desert from the Roman invasions. Other scholars refute these arguments—particularly since Josephus describes some Essenes as allowing marriage.
Another issue is the relationship between the "Essaioi" and Philo's "Therapeutae" and "Therapeutrides". He regarded the "Therapeutae" as a contemplative branch of the "Essaioi" who, he said, pursued an active life.
One theory on the formation of the Essenes suggests that the movement was founded by a Jewish high priest, dubbed by the Essenes the Teacher of Righteousness, whose office had been usurped by Jonathan (of priestly but not of Zadokite lineage), labeled the "man of lies" or "false priest". Others follow this line and a few argue that the Teacher of Righteousness was not only the leader of the Essenes at Qumran, but was also identical to the original Jesus about 150 years before the time of the Gospels. Fred Gladstone Bratton notes that
The Saint Thomas Christians ("Nasrani") of southwestern India may have connections with the Essenes, according to the Manimekalai, one of the great Tamil epic poems, which refers to a people called "Issani".
Connections with Kabbalah.
According to a Jewish legend, one of the Essenes, named Menachem, had passed at least some of his mystical knowledge to the Talmudic mystic Nehunya ben HaKanah, to whom the Kabbalistic tradition attributes Sefer HaBahir and, by some opinions, Sefer HaKanah, Sefer HaPeliah, and Sefer HaTemunah. Some Essene rituals, such as daily immersion in the mikveh, coincide with contemporary Hasidic practices; some historians have also suggested that the name "Essene" is a Hellenized form of the word "Hasidim" or "Hasid" ("pious ones"). However, the legendary connections between Essene and Kabbalistic tradition are not verified by modern historians.

</doc>
<doc id="9979" url="https://en.wikipedia.org/wiki?curid=9979" title="Eyes Wide Shut">
Eyes Wide Shut

Eyes Wide Shut is a 1999 erotic drama film based on Arthur Schnitzler's 1926 novella "Traumnovelle" ("Dream Story"), with the story transferred from early 20th century Vienna to 1990s New York. The film was directed, produced, and co-written by Stanley Kubrick. It was his last film, as he died four days after showing his final cut to Warner Bros. studios. The story, set in and around New York City, follows the sexually charged adventures of Dr. Bill Harford, who is shocked when his wife, Alice, reveals that she had contemplated an affair a year earlier. He embarks on a night-long adventure, during which he infiltrates a massive masked orgy of an unnamed secret society.
Kubrick obtained the filming rights for "Dream Story" in the 1960s, considering it a perfect novel for a film adaptation about sexual relations. The project was only revived in the 1990s, when the director hired writer Frederic Raphael to help him with the adaptation. The film was mostly shot in the United Kingdom (aside from some exterior establishing shots), and included a detailed recreation of some exterior Greenwich Village street scenes at Pinewood Studios. The film spent a long time in production, and holds the Guinness World Record for the longest continuous film shoot period, at 400 days.
"Eyes Wide Shut" was released on July 16, 1999, a few months following Kubrick's death, to positive critical reaction and intakes of $162 million at the worldwide box office. Its strong sexual content also made it controversial; to ensure a theatrical R rating in the United States, Warner Bros. digitally altered several scenes during post-production. The uncut version has since been released in DVD, HD DVD, and Blu-ray Disc formats.
Plot.
Dr. Bill Harford (Tom Cruise) and his wife, Alice (Nicole Kidman), are a young couple living in New York. They go to a Christmas party thrown by a wealthy patient, Victor Ziegler (Sydney Pollack). Bill meets an old friend from medical school, Nick Nightingale (Todd Field), who now plays piano professionally. While a Hungarian man named Sandor Szavost (Sky du Mont) tries to pick up Alice, two young models try to take Bill off for a tryst. He is interrupted by a call from his host upstairs, who had been having sex with Mandy (Julienne Davis), a young woman who has overdosed on a speedball. Mandy recovers with Bill's aid.
The next evening at home, while smoking cannabis, Alice asks him if he had sex with the two girls. After Bill reassures her, she asks if he is ever jealous of men who are attracted to her. As the discussion gets heated, he states that he thinks women are more faithful than men. She rebuts him, telling him of a recent fantasy she had about a naval officer they had encountered on a vacation. Disturbed by Alice's revelation, Bill is then called by the daughter of a patient who has just died; he then heads over to her place. In her pain, Marion Nathanson (Marie Richardson) impulsively kisses him and says she loves him. Putting her off before her fiance Carl (Thomas Gibson) arrives, Bill takes a walk. He meets a prostitute named Domino (Vinessa Shaw) and goes to her apartment.
Alice phones just as Domino begins to kiss Bill, after which he calls off the awkward encounter. Meeting Nick at the jazz club where he's just finishing his last set, Bill learns that Nick has an engagement where he must play piano blindfolded. Bill presses for details. He learns that to gain admittance, one needs a costume, a mask, and the password (which Nick writes down for him). Bill goes to a costume shop. He offers the owner, Mr. Milich (Rade Serbedzija), a generous amount of money to rent a costume. In the shop, Milich catches his teenage daughter (Leelee Sobieski) with two Japanese men and expresses outrage at their lack of a sense of decency.
Bill takes a taxi to the country mansion mentioned by Nick. He gives the password and discovers a quasi-religious sexual ritual is taking place. Although he is masked, a woman takes Bill aside and warns him he does not belong there, insisting he is in terrible danger. She is then whisked away by someone else, so Bill spends some time wandering from room to room in the mansion, where groups of masked people are engaged in various types of sexual acts, while others watch. He is then interrupted by a porter who tells Bill that the taxi driver wants to speak urgently with him at the front door. However, the porter takes him to the ritual room, where a disguised red-cloaked Master of Ceremonies confronts Bill with a question about a second password. Bill says he has forgotten. The Master of Ceremonies insists that Bill "kindly remove his mask", then his clothes. The masked woman who had tried to warn Bill now intervenes and insists that she will redeem him. Bill is ushered from the mansion and warned not to tell anyone about what happened there.
Just before dawn, Bill arrives home guilty and confused. He finds Alice laughing loudly in her sleep and awakens her. While crying, she tells him of a troubling dream in which she was having sex with the naval officer and many other men, and laughing at the idea of Bill seeing her with them. 
The next morning, Bill goes to Nick's hotel, where the desk clerk (Alan Cumming) tells Bill that a bruised and frightened Nick checked out a few hours earlier after returning with two large, dangerous-looking men. Nick tried to pass an envelope to the clerk when they were leaving, but it was intercepted, and Nick was driven away by the two men.
Bill goes to return the costume — but not the mask, which he has misplaced — and Milich, with his daughter by his side, states he can do other favors for Bill "and it needn't be a costume." The same two Japanese men leave; Milich implies to Bill that he has sold his daughter for prostitution. 
Bill returns to the country mansion in his own car and is met at the gate by a man with a note warning him to cease and desist his inquiries. At home, Bill thinks about Alice's dream while watching her tutor their daughter.
Bill reconsiders his sexual offers the night before. He first phones Marion, but hangs up after Carl answers the phone. Bill then goes to Domino's apartment with a gift. Her roommate Sally (Fay Masterson) is home, but not Domino. After Bill attempts to seduce Sally, she reveals to him that Domino has just tested positive for HIV. Bill leaves and notices a man is following him. After reading a newspaper story about a beauty queen who died of a drug overdose, Bill views the body at the morgue and identifies it as Mandy. 
Bill is summoned to Ziegler's house, where he is confronted with the events of the past night and day. Ziegler was one of those involved with the ritual orgy, and identified Bill and his connection with Nick. His own position with the secret society has been jeopardized by Bill's intrusion since Ziegler recommended Nick for the job. Ziegler claims that he had Bill followed for his own protection, and that the warnings made against Bill by the society are only intended to scare him from speaking about the orgy. But he implies the society is capable of acting on their threats, telling Bill: "If I told you their names, I don't think you'd sleep so well". Bill asks about the death of Mandy, whom Ziegler has identified as the masked woman at the party who'd "sacrificed" herself to prevent Bill's punishment, and about the disappearance of Nick, the piano player. Ziegler insists that Nick is safely back at his home in Seattle. Ziegler also says the "punishment" was a charade by the secret society to further frighten Bill, and it had nothing to do with Mandy's death; she was a hooker and addict and had indeed died from another accidental drug overdose. Bill clearly does not know if Ziegler is telling him the truth about Nick's disappearance or Mandy's death, but he says nothing further and lets the matter drop.
When he returns home, Bill finds the rented mask on his pillow next to his sleeping wife. He breaks down in tears and decides to tell Alice the whole truth of the past two days. The next morning, they go Christmas shopping with their daughter. Alice muses that they should be grateful they have survived, that she loves him, and there is something they must do as soon as possible. When Bill asks what it is, she simply says: "Fuck".
Production.
Development.
While Stanley Kubrick was interested in making a film about sexual relations as early as 1962, during production of "Dr. Strangelove", the project only took off after he read Arthur Schnitzler's "Dream Story" in 1968, when he was seeking a work to follow on "". Kubrick got interested in adapting the story, and with the help of then-journalist Jay Cocks, bought the filming rights to the novel. In the 1970s, Kubrick had thought of Woody Allen as the Jewish protagonist. For the following decade, Kubrick even considered making his "Dream Story" adaptation a sex comedy "with a wild and somber streak running through it", starring Steve Martin in the main role. The project was only revived in 1994, when Kubrick hired Frederic Raphael to work on the script, updating the setting from early 20th century Vienna to late 20th century New York City. Kubrick invited Michael Herr, a personal friend who helped write "Full Metal Jacket", for revisions, but Herr declined for fear that he would both be underpaid and would commit to an overlong production.
Adaptation.
Arthur Schnitzler's 1926 novella "Dream Story" is set around Vienna shortly after the turn of the century. The main characters are a couple named Fridolin and Albertina; their home is a typical suburban middle-class home, not the film's posh urban apartment. Schnitzler himself, like the protagonist of this novel, lived in Vienna, was Jewish, and a medical doctor, though Schnitzler eventually abandoned medicine for writing.
While Fridolin and Albertina, the protagonist couple of "Dream Story", are sometimes implied to be Jewish, there is nothing in the novella which justifies this assumption, and neither Fridolin nor Albertina are typical Jewish names; whereas Nachtigall (Nightingale) is overtly identified as Jewish. Kubrick (himself of Jewish descent) frequently removed references to the Jewishness of characters in the novels he adapted. In the case of "Eyes Wide Shut", Frederic Raphael (who is also Jewish) wanted to keep the Jewish background of the protagonists, but Kubrick insisted that they should be "vanilla" Americans, without any details that would arouse any presumptions. The director added that Bill should be a "Harrison Ford-ish goy" (though Ford's mother was Jewish), and created the surname of Harford as an allusion to the actor. This is reflected in the way the film's Bill Harford is taunted by college students when going home in the morning. In the film, Bill is taunted with homophobic slurs. In the novella, these boys are recognized to be members of an anti-Semitic college fraternity. Kubrick's co-screenwriter, Frederic Raphael, in an introduction to a Penguin Classics edition of "Dream Story", writes "Fridolin is not declared to be a Jew, but his feelings of cowardice, for failing to challenge his aggressor, echo the uneasiness of Austrian Jews in the face of Gentile provocation."
The novella is set during the Carnival, when people often wear masks to parties. The party that both husband and wife attend at the opening of the story is a Carnival Masquerade ball, whereas the film's story begins at Christmas time.
Critic Randy Rasmussen suggests that the character of Bill is fundamentally more naïve, strait-laced, less disclosing and more unconscious of his vindictive motives than his counterpart, Fridolin. For Rasmussen and others, the film's Bill Harford is essentially sleep-walking through life with no deeper awareness of his surroundings. In the novella, when his wife discloses a private sexual fantasy, he in turn admits one of his own (of a girl in her mid to late teens), while in the film he is simply shocked. The film's argument over whether he has fantasies over female patients and whether women have sexual fantasies is simply absent from the novella, where both husband and wife assume the other has fantasies. In the film, Bill's estrangement from Alice revolves around her confessing a recent fantasy to him; in the novella, both exchange fantasies, after which she declares that in her youth she could have easily married someone else, which is what precipitates their sense of estrangement.
In the novella, the husband long suspected that his patient (Marion) was infatuated with him, while in the film it is a complete surprise and he seems shocked. He is also more overwhelmed by the orgy in the film than in the novella. Fridolin is socially bolder but less sexual with the prostitute (Mizzi in the novella, Domino in the film). Fridolin is also conscious of looking old in the novella, though he hardly does in the film.
In the novella, the party (which is sparsely attended) uses "Denmark" as the password for entrance; that is significant in that Albertina had her infatuation with her soldier in Denmark. The film's password is "Fidelio", from the Latin word for "faithful", and which is the title of Beethoven's only opera ("Fidelio, or Married Love"). In early drafts of the screenplay, the password was "Fidelio Rainbow". Jonathan Rosenbaum noted that both passwords echo elements of one member of the couple's behaviour, though in opposite ways. The party in the novella consists mostly of nude ballroom dancing.
In the novella, the woman who "redeems" Fridolin at the party, saving him from punishment, is costumed as a nun, and most of the characters at the party are dressed as nuns or monks; Fridolin himself used a monk costume. This aspect was retained in the film's original screenplay, but was deleted in the filmed version.
In the novella, when the husband returns home, the wife's dream is an elaborate drama that concludes with him getting crucified in a village square after Fridolin refuses to separate from Albertina and become the paramour of the village princess, even though Albertina is now occupied with copulating with other men, and watches him "without pity". By being faithful, Fridolin thus fails to save himself from execution in Albertina's dream, although he was apparently spared by the woman's "sacrifice" at the masked sex party. In both the novella and film, the wife states that the laugh in her sleep just before she woke was a laugh of scornful contempt for her husband; although awake, she states this matter-of-factly. The novella makes it clear that Fridolin at this point hates Albertina more than ever, thinking they are now lying together "like mortal enemies". It has been argued that the dramatic climax of the novella is actually Albertina's dream, and the film has shifted the focus to Bill's visit to the secret society's orgy, whose content is more shocking in the film.
The adaptation created a character with no counterpart in the novella: Ziegler, who represents both the high wealth and prestige to which Bill Harford aspires, and a connection between Bill's two worlds (his regular life, and the secret society organizing the ball). Critic Randy Rasmussen interprets Ziegler as representing Bill's worst self, much as in other Kubrick films; the title character in "Dr. Strangelove" represents the worst of the American national security establishment, Charles Grady represents the worst of Jack Torrance in "The Shining", and Clare Quilty represents the worst of Humbert Humbert in "Lolita".
Ziegler's presence allows Kubrick to change the mechanics of the story in a few ways. In the film, Bill first meets his piano-playing friend at Ziegler's party, and then while wandering around town, seeks him out at the Sonata Café. In the novella, the café encounter with Nightingale is a delightful coincidence. Similarly, the dead woman whom Bill suspects of being the woman at the party who saved him is a baroness that he was acquainted with earlier, not a hooker at Ziegler's party.
More significantly, in the film, Ziegler gives a commentary on the whole story to Bill, including an explanation that the party incident, where Bill is apprehended, threatened, and ultimately redeemed by the woman's sacrifice, was staged. Whether this is to be believed or not, it is an exposition of Ziegler's view of the ways of the world as a member of the power elite.
The novella explains why the husband's mask is on the pillow next to his sleeping wife, she having discovered it when it slipped out of his suitcase, and placing it there as a statement of understanding. This is left unexplained in the film and left to the viewer's interpretation.
Casting.
When Warner Bros. president Terry Semel approved production, he asked Kubrick to cast a movie star, as "you haven't done that since Jack Nicholson "The Shining"." Cruise was in England because his wife Nicole Kidman was there shooting "The Portrait of a Lady", and eventually Cruise decided to visit Kubrick's estate with Kidman. After that meeting, the director awarded them the roles. Jennifer Jason Leigh and Harvey Keitel each were cast and filmed by Kubrick. Due to scheduling conflicts, both had to drop out – first Keitel with "Finding Graceland", then Leigh with "eXistenZ" – and they were replaced by Marie Richardson and Sydney Pollack in the final cut.
Filming.
Principal photography began in November 1996. Kubrick's perfectionism led to script pages being rewritten on the set, and most scenes requiring numerous takes. The shoot went much longer than expected, with Vinessa Shaw—playing the HIV-positive prostitute—being initially contracted for two weeks, but ending up working for two months. The crew got exhausted. Filming finally wrapped in June 1998. The "Guinness World Records" recognized "Eyes Wide Shut" as the longest constant movie shoot, "for over 15 months, a period that included an unbroken shoot of 46 weeks".
Given Kubrick's fear of flying, the entire film was shot in England. Sound-stage works were done at London's Pinewood Studios, which included a detailed recreation of Greenwich Village. Kubrick's perfectionism went as far as sending workmen to Manhattan to measure street widths and note newspaper vending machine locations. Real New York footage was also shot to be rear projected behind Cruise. Production was followed by a strong campaign of secrecy, helped by Kubrick always working with a short team on set. Outdoor locations included Hatton Garden for a Greenwich Village street, Hamleys for the toy store from the film's ending, and Mentmore Towers and Elveden Hall in Elveden, Suffolk, England for the mansion. Larry Smith, who had first served as a gaffer on both "Barry Lyndon" and "The Shining", was chosen by Kubrick to be the film's cinematographer. Kubrick refused to use studio lighting, forcing Smith to use the available light sources visible in the shot, such as lamps and Christmas tree lights. When this was not adequate, Smith used Chinese paper ball lamps to softly brighten the scene. The color was enhanced by push processing the film reels, which helped bring out the intensity of color.
Kubrick's perfectionism led him to oversee every visual element that would appear in a given frame, from props and furniture to the color of walls and other objects. One such element were the masks used in the orgy, which were inspired by the masked Carnival balls visited by the protagonists of the novel. Costume designer Marit Allen explained that Kubrick felt they fit in that scene for being part of the imaginary world, and ended up "creatthe impression of menace, but without exaggeration". Many masks as used in the Venetian carnival were sent to London, and Kubrick separated who would wear each piece. The paintings of Kubrick's wife Christiane are featured on decoration.
After shooting completed, Kubrick entered a prolonged post-production process. On March 1, 1999, Kubrick showed a cut to Cruise, Kidman, and the Warner Bros. executives. The director died six days later.
Music.
Jocelyn Pook wrote the original music for "Eyes Wide Shut", but like other Kubrick movies, the film was noted for its usage of classical music. The opening title music is Shostakovich's "Suite for Variety Stage Orchestra", misidentified as "Waltz 2 from Jazz Suite". One recurring piece is the second movement of György Ligeti's piano cycle "Musica ricercata". Kubrick originally intended to feature "Im Treibhaus" from Wagner's "Wesendonck Lieder", but the director eventually replaced it with Ligeti's tune feeling Wagner's song was "too beautiful". In the morgue scene, Franz Liszt's late solo piano piece, "Nuages Gris" ("Grey Clouds") (1881), is heard. "Rex tremendae" from Mozart's "Requiem" plays as Bill walks into the cafe and reads of Mandy's death.
Pook was hired after choreographer Yolande Snaith rehearsed the masked ball orgy scene using Pook's composition "Backwards Priests" – which features a Romanian Orthodox Divine Liturgy recorded in a church in Baia Mare, played backwards – as a reference track. Kubrick then called the composer and asked if she had anything else "weird" like that song, which was reworked for the final cut of the scene, with the title "Masked Ball". Pook ended up composing and recording four pieces of music, many times based on her previous work, totaling 24 minutes. The composer's work ended up having mostly string instruments – including a viola played by Pook herself – with no brass or woodwinds as Pook "just couldn't justify these other textures", particularly as she wanted the tracks played on dialogue-heavy scenes to be "subliminal" and felt such instruments would be intrusive.
Another track in the orgy, "Migrations", features a Tamil song sung by Manickam Yogeswaran, a Carnatic singer. The original cut had a scriptural recitation of the Bhagavad Gita, which Pook took from a previous Yogeswaran recording. As a result of Hindus protesting against their most sacred scripture being used in such a context, Warner Bros. issued a public apology, and hired the singer to record a similar track to replace the chant.
The party at Ziegler's house features rearrangements of love songs such as "When I Fall in Love" and "It Had to Be You", used in increasingly ironic ways considering how Alice and Bill flirt with other people in the scene. As Kidman was nervous about doing nude scenes, Kubrick stated she could bring music to liven up. When Kidman brought a Chris Isaak CD, Kubrick approved it, and incorporated Isaak's song "Baby Did a Bad, Bad Thing" to both an early romantic embrace of Bill and Alice and the film's trailer.
Themes and interpretation.
Genre.
The film was described by some reviewers, and partially marketed, as an erotic thriller, a categorization disputed by others. It is classified as such in the book "The Erotic Thriller in Contemporary Cinema", by Linda Ruth Williams, and was described as such in news articles about Cruise and Kidman's lawsuit over assertions they saw a sex therapist during filming. The positive review in "Combustible Celluloid" describes it as an erotic thriller upon first viewing, but actually a "complex story about marriage and sexuality". High-Def Digest also called it an erotic thriller.
However, reviewing the film at AboutFilm.com, Carlo Cavagna regards this as a misleading classification, as does Leo Goldsmith, writing at notcoming.com, and the review on Blu-ray.com. Writing in "TV Guide", Maitland McDonagh writes "No one familiar with the cold precision of Kubrick's work will be surprised that this isn't the steamy erotic thriller a synopsis (or the ads) might suggest." Writing in general about the genre of 'erotic thriller' for CineAction in 2001, Douglas Keesey states that "whatever Wide Shut's actual type, was at least marketed as an erotic thriller". Michael Koresky, writing in the 2006 issue of film journal "Reverse Shot", writes "this director, who defies expectations at every turn and brings genre to his feet, was ... setting out to make neither the 'erotic thriller' that the press maintained nor an easily identifiable 'Kubrick film'". "DVD Talk" similarly dissociates the film from this genre.
Christmas setting.
In addition to relocating the story from Vienna in the 1900s to New York City in the 1990s, Kubrick changed the time-frame of Schnitzler's story from Mardi Gras to Christmas. One critic believed Kubrick did this because of the rejuvenating symbolism of Christmas. Others have noted that Christmas lights allow Kubrick to employ some of his distinct methods of shooting including using source location lighting, as he did in "Barry Lyndon". and critic Randy Rasmussen noted that "colorful Christmas lights ... illuminate almost every location in the film." "Harper"'s film critic, Lee Siegel, believes the film's recurring motif is the Christmas tree, because it symbolizes the way that "Compared with the everyday reality of sex and emotion, our fantasies of gratification are, yes, pompous and solemn in the extreme ... For desire is like Christmas: it always promises more than it delivers." Author Tim Kreider noted that the "Satanic" mansion-party at Somerton is the only set in the film without a Christmas tree, stating "Almost every set is suffused with the dreamlike, hazy glow of colored lights and tinsel ... "Eyes Wide Shut", though it was released in summer, was "the" Christmas movie of 1999." Noting that Kubrick has shown viewers the dark side of Christmas consumerism, Louise Kaplan stated that the film illustrates ways that the "material reality of money" is shown replacing the spiritual values of Christmas, charity and compassion. While virtually every scene has a Christmas tree, there is "no Christmas music or cheery Christmas spirit." Critic Alonso Duralde, in his book "Have Yourself a Movie Little Christmas", categorized the film as a "Christmas movie for grownups" (as he also did with Bergman's "Fanny and Alexander" and "The Lion in Winter"), arguing that "Christmas weaves its way through the film from start to finish".
Use of Venetian masks.
Historians, travel guide authors, novelists, and merchants of Venetian masks have noted that these have a long history of being worn during promiscuous activities. Authors Tim Kreider and Thomas Nelson have linked the film's usage of these to Venice's reputation as a center of both eroticism and mercantilism. Nelson notes that the sex ritual combines elements of Venetian Carnival and Catholic rites. (In particular, the character of "Red Cloak" simultaneously serves as Grand Inquisitor and King of Carnival.) As such, Nelson argues the sex ritual is a symbolic mirror of the darker truth behind the façade of Victor Ziegler's earlier Christmas party. Writing in her 2007 book "Symbols in Stanley Kubrick's Movie 'Eyes Wide Shut, Carolin Ruwe argues that the mask is the prime symbol of the film, the masks at Somerton mansion reflecting the masks that all wear in society, a point reinforced by Tim Kreider, who noted the many masks in the prostitute's apartment and her having been renamed in the film "Domino", which is a style of Venetian mask.
Release.
Marketing.
Warner Bros. heavily promoted "Eyes Wide Shut", while following Kubrick's secrecy campaign – to the point the film's press kits contained no production notes – and also the director's suggestions to Semel regarding the marketing campaign, given one week prior to Kubrick's death. The first footage was shown to theater owners attending the 1999 edition of the ShoWest convention in Las Vegas. TV spots featured both Isaak and Ligeti's songs from the soundtrack, while revealing little about the movie's plot. The film also appeared on the cover of "Time" magazine, and on show business programs such as "Entertainment Tonight" and "Access Hollywood".
Box office.
"Eyes Wide Shut" opened on July 16, 1999, in the United States. The film topped the weekend box office, with $21.7 million from 2,411 screens. These numbers surpassed the studio's expectations of $20 million, and became both Cruise's sixth consecutive chart topper and Kubrick's highest opening weekend. Audiences attendance dropped from Friday to Saturday, which analysts attributed to the news coverage of John F. Kennedy, Jr.'s disappearance. "Eyes Wide Shut" ended up grossing a total of $55,691,208 in the US. The numbers put it as Kubrick's second most successful film in the country, behind "2001: A Space Odyssey", but were considered a box office disappointment.
Shortly after its screening at the Venice Film Festival, "Eyes Wide Shut" had a British premiere on September 3, 1999, at the Warner Village cinema in Leicester Square. The film's wide opening occurred the following weekend, and topped the UK charts with £1,189,672. It remained atop the charts the following weekend, and finished its box office run with £5,065,520.
The international performances for "Eyes Wide Shut" were more positive, with Kubrick's long-time assistant and brother-in-law Jan Harlan stating that "It was badly received in the Anglo-Saxon world, but it was very well received in the Latin world and Japan. In Italy, it was a huge hit." Overseas earnings of over $105 million led to a $162,091,208 box office run worldwide, turning it into the highest-grossing Kubrick film.
Critical reception.
"Eyes Wide Shut" received positive reviews from critics, and currently has a "Certified Fresh" score of 74% on Rotten Tomatoes, based on 146 reviews with an average rating of 7.5 out of 10. The critical consensus states "Kubrick's intense study of the human psyche yields an impressive cinematic work." The film also has a score of 68 out of 100 on Metacritic, based on 33 critics, indicating "Generally favorable reviews". Over 50 critics listed the film among the best of 1999.
Critics objected to two elements. The first complaint was that the movie's pacing was too slow; while this may have been intended to convey a dream state, critics objected that it made actions and decisions seem labored. Second, reviewers commented that Kubrick had shot his NYC scenes in a studio and that New York "didn't look like New York". Writing about erotic mystery thrillers, writer Leigh Lundin comments that watching the dissolving marriage was painful and the backdrop of Christmas against the dark topic was disturbing, but "the oblique, well-told plot rewards an attentive viewer".
Lee Siegel from "Harper's" felt that most critics responded mainly to the marketing campaign and did not address the film on its own terms. Others felt that American censorship took an esoteric film and made it even harder to understand. Reviewer James Berardinelli stated that it was arguably one of Kubrick's best films. Writing for "The New York Times", reviewer Janet Maslin commented "This is a dead-serious film about sexual yearnings, one that flirts with ridicule yet sustains its fundamental eeriness and gravity throughout. The dreamlike intensity of previous Kubrick visions is in full force here."
On the television show "Roger Ebert & the Movies", director Martin Scorsese named "Eyes Wide Shut" his fourth-favorite film of the 1990s. For the introduction to Michel Ciment's "Kubrick: The Definitive Edition", Scorsese wrote: "When "Eyes Wide Shut" came out a few months after Stanley Kubrick's death in 1999, it was severely misunderstood, which came as no surprise. If you go back and look at the contemporary reactions to any Kubrick picture (except the earliest ones), you'll see that all his films were initially misunderstood. Then, after five or ten years came the realization that "" or "Barry Lyndon" or "The Shining" was like nothing else before or since." Mystery writer and commentator Jon Breen agreed. In 2012, Slant Magazine ranked the film #2 on its list of the 100 Best Films of the 1990s. The BBC listed it number 61 on their 100 greatest American films of all time.
Home media.
"Eyes Wide Shut" was first released in VHS and DVD on March 7, 2000. The original DVD release corrects technical gaffes, including a reflected crew member, and altering a piece of Alice Harford's dialogue. Most home videos remove the verse that was claimed to be cited from the sacred Hindu scripture Bhagavad Gita (although it was Pook's reworking of "Backwards Priests" as stated above.)
On October 23, 2007, Warner Home Video released "Eyes Wide Shut" in a special edition DVD, plus the HD DVD and Blu-ray Disc formats. This is the first home video release that presents the film in anamorphic 1.78:1 (16:9) format (note that the film was shown theatrically as soft matted 1.66:1 in Europe and 1.85:1 in the USA and Japan). The previous DVD release used a 1.33:1 (4:3) aspect ratio. It is also the first American home video release to feature the uncut version. Although the earliest American DVD of the uncut version states on the cover that it includes both the R-rated and unrated editions, in actuality only the unrated edition is on the DVD.
Controversies.
Kubrick's opinion.
Jan Harlan, Kubrick's brother-in-law and executive producer, reported that Kubrick was "very happy" with the film and considered it to be his "greatest contribution to the art of cinema".
R. Lee Ermey, an actor in Kubrick's film "Full Metal Jacket", claimed that Kubrick phoned him two weeks before his death to express his despondency over "Eyes Wide Shut". "He told me it was a piece of shit", Ermey said in "Radar" magazine, "and that he was disgusted with it and that the critics were going to 'have him for lunch'. He said Cruise and Kidman had their way with him – exactly the words he used."
According to Todd Field, Kubrick's friend and an actor in "Eyes Wide Shut", Ermey's claims do not accurately reflect Kubrick's essential attitude. Field's response appeared in a 26 October 2006 interview with Slashfilm.com:
The polite thing would be to say 'No comment'. But the truth is that ... let's put it this way, you've never seen two actors more completely subservient and prostrate themselves at the feet of a director. Stanley was absolutely thrilled with the film. He was still working on the film when he died. And he probably died because he finally relaxed. It was one of the happiest weekends of his life, right before he died, after he had shown the first cut to Terry, Tom and Nicole. He would have kept working on it, like he did on all of his films. But I know that from people around him personally, my partner who was his assistant for thirty years. And I thought about R. Lee Ermey for "In the Bedroom". And I talked to Stanley a lot about that film, and all I can say is Stanley was adamant that I shouldn't work with him for all kinds of reasons that I won't get into because there is no reason to do that to anyone, even if they are saying slanderous things that I know are completely untrue.
In a reddit "Ask Me Anything" session, Stanley Kubrick's daughter, Katharina Kubrick, claimed that her father was very proud of the film. She also discredited Ermey's claims, saying to a user who asked about Kubrick's alleged comments, "to believe that for a second."
Studio censorship and classification.
Citing contractual obligations to deliver an R rating, Warner Bros. digitally altered the orgy for the American release, blocking out graphic sexuality by inserting additional figures to obscure the view, avoiding an adults-only NC-17 rating that limited distribution, as some large American theaters and video store operators disallow films with that rating. This alteration antagonized film critics and cinephiles, as they argued that Kubrick had never been shy about ratings ("A Clockwork Orange" was originally given an X-rating). The unrated version of "Eyes Wide Shut" was released in the United States on October 23, 2007, in DVD, HD DVD, and Blu-ray Disc formats.
The version in South America, Europe and Australia featured the orgy scene intact (theatrical and DVD release) with ratings mostly for people of 18+. In New Zealand and in Europe, the uncensored version has been shown on television with some controversy. In Australia, it was broadcast on Network Ten with the alterations in the American version for an MA rating, blurring and cutting explicit sexuality.
Roger Ebert objected to the technique of using digital images to mask the action. He said it "should not have been done at all" and it is "symbolic of the moral hypocrisy of the rating system that it would force a great director to compromise his vision, while by the same process making his adult film more accessible to young viewers." Although Ebert has been frequently cited as calling the standard North American R-rated version the "Austin Powers" version of "Eyes Wide Shut" – referencing two scenes in "" in which, through camera angles and coincidences, sexual body parts are blocked from view in a comical way – his review stated that this joke referred to an early rough draft of the altered scene, never publicly released.
References.
Notes
Bibliography

</doc>
<doc id="9986" url="https://en.wikipedia.org/wiki?curid=9986" title="Outline of education">
Outline of education

The following outline is provided as an overview of and topical guide to education:
Education – in the general sense is any act or experience that has a formative effect on the mind, character, or physical ability of an individual. In its technical sense, education is the process by which society deliberately transmits its accumulated knowledge, skills, and values from one generation to another. Education can also be defined as the process of becoming an educated person.

</doc>
<doc id="9987" url="https://en.wikipedia.org/wiki?curid=9987" title="Outline of engineering">
Outline of engineering

The following outline is provided as an overview of and topical guide to engineering:
Engineering – discipline, art, skill and profession of acquiring and applying scientific, mathematical, economic, social, and practical knowledge, in order to design and build structures, machines, devices, systems, materials and processes that safely realize improvements to the lives of people.

</doc>
<doc id="9988" url="https://en.wikipedia.org/wiki?curid=9988" title="Outline of entertainment">
Outline of entertainment

The following outline provides an overview of and topical guide to entertainment and the entertainment industry:
Entertainment is any activity which provides a diversion or permits people to amuse themselves in their leisure time, and may also provide fun, enjoyment and laughter. People may create their own entertainment, such as when they spontaneously invent a game; participate actively in an activity they find entertaining, such as when they play sport as a hobby; or consume an entertainment product passively, such as when they attend a performance. 
The entertainment industry (informally known as show business or show biz) is part of the tertiary sector of the economy and includes a large number of sub-industries devoted to entertainment. However, the term is often used in the mass media to describe the mass media companies that control the distribution and manufacture of mass media entertainment. In the popular parlance, the term "show biz" in particular connotes the commercially popular performing arts, especially musical theatre, vaudeville, comedy, film, and music. It applies to every aspect of entertainment including cinema, television, radio, theatre and music.

</doc>
<doc id="9992" url="https://en.wikipedia.org/wiki?curid=9992" title="List of contemporary ethnic groups">
List of contemporary ethnic groups

The following is a list of contemporary ethnic groups.
There has been constant debate over the classification of ethnic groups. Membership of an ethnic group tends to be associated with shared cultural heritage, ancestry, history, homeland, language or dialect, the term culture specifically including aspects such as religion, mythology and ritual, cuisine, dressing style, etc. 
By the nature of the concept, ethnic groups tend to be divided into ethnic subgroups, which may themselves be or not be identified as independent ethnic groups depending on the source consulted.
Largest ethnic groups.
The largest groups commonly identified as "ethnic groups" (as opposed to ethno-linguistic phyla, racial groups or similar).
Lists of ethnic groups.
by status
regional lists

</doc>
<doc id="9993" url="https://en.wikipedia.org/wiki?curid=9993" title="Edda">
Edda

The term "Edda" (; Old Norse "Edda", plural "Eddur") applies to the Old Norse "Edda" and has been adapted to fit the collection of poems known as the "Poetic Edda" which lacks an original title. Both works were written down in Iceland during the 13th century in Icelandic, although they contain material from earlier traditional sources, reaching into the Viking Age. The books are the main sources of medieval skaldic tradition in Iceland and Norse mythology.
Etymology.
There are several theories concerning the origins of the word "edda". One theory holds that it is identical to a word that means "great-grandmother" appearing in the Eddic poem "Rígsþula." Another theory holds that "edda" derives from Old Norse "óðr", "poetry". A third, proposed in 1895 by Eiríkr Magnússon, is that it derives from the Icelandic place name "Oddi", site of the church and school where students, including Snorri Sturluson, were educated. A fourth hypothesis—the derivation of the word "Edda" as the name of Snorri Sturluson’s treatise on poetry from the Latin "edo", "I compose (poetry)", by analogy with "kredda", "superstition", from Latin "credo", "creed"—is now widely accepted, though this acceptance may stem from its agreement with modern usage rather than historical accuracy. There is also suggestion from local Icelandic scholars that the Edda is the same Norse root as Sanskrit 'Vedas' meaning 'knowledge'.
The "Poetic Edda".
The "Poetic Edda", also known as "Sæmundar Edda" or the "Elder Edda", is a collection of Old Norse poems from the Icelandic medieval manuscript Codex Regius ("Royal Book"). Along with the "Prose Edda", the "Poetic Edda" is the most expansive source on Norse mythology. The first part of the Codex Regius preserves poems that narrate the creation and foretold destruction and rebirth of the Old Norse mythological world as well as individual myths about gods concerning Norse deities. The poems in the second part narrate legends about Norse heroes and heroines, such as Sigurd, Brynhildr and Gunnar.
The Codex Regius was written in the 13th century, but nothing is known of its whereabouts until 1643, when it came into the possession of Brynjólfur Sveinsson, then the Church of Iceland's Bishop of Skálholt. At that time, versions of the "Prose Edda" were well known in Iceland, but scholars speculated that there once was another "Edda"—an "Elder Edda"—which contained the pagan poems Snorri quotes in his book. When the Codex Regius was discovered, it seemed that this speculation had proven correct. Brynjólfur attributed the manuscript to Sæmundr the Learned, a larger-than-life 12th century Icelandic priest. While this attribution is rejected by modern scholars, the name "Sæmundar Edda" is still sometimes encountered.
Bishop Brynjólfur sent the Codex Regius as a present to King Christian IV of Denmark, hence the name "Codex Regius". For centuries it was stored in the Royal Library in Copenhagen but in 1971 it was returned to Iceland.
The "Prose Edda".
The "Prose Edda", sometimes referred to as the "Younger Edda" or "Snorri's Edda", is an Icelandic manual of poetics which also contains many mythological stories. Its purpose was to enable Icelandic poets and readers to understand the subtleties of alliterative verse, and to grasp the mythological allusions behind the many "kennings" that were used in skaldic poetry.
It was written by the Icelandic scholar and historian Snorri Sturluson around 1220. It survives in seven main manuscripts, written down from about 1300 to about 1600.
The "Prose Edda" consists of a Prologue and three separate books: "Gylfaginning", concerning the creation and foretold destruction and rebirth of the Norse mythical world; "Skáldskaparmál", a dialogue between Ægir, a Norse god connected with the sea, and Bragi, the skaldic god of poetry; and "Háttatal", a demonstration of verse forms used in Norse mythology.

</doc>
<doc id="9994" url="https://en.wikipedia.org/wiki?curid=9994" title="Ephemeris time">
Ephemeris time

The term ephemeris time (often abbreviated ET) can in principle refer to time in connection with any astronomical ephemeris. In practice it has been used more specifically to refer to:
Most of the following sections relate to the ephemeris time of the 1952 standard.
An impression has sometimes arisen that ephemeris time was in use from 1900: this probably arose because ET, though proposed and adopted in the period 1948–1952, was defined in detail using formulae that made retrospective use of the epoch date of 1900 January 0 and of Newcomb's Tables of the Sun.
The ephemeris time of the 1952 standard leaves a continuing legacy, through its ephemeris second which became closely duplicated in the length of the current standard SI second (see below: Redefinition of the second).
History of ephemeris time (1952 standard).
Ephemeris time (ET), adopted as standard in 1952, was originally designed as an approach to a uniform time scale, to be freed from the effects of irregularity in the rotation of the earth, "for the convenience of astronomers and other scientists", for example for use in ephemerides of the Sun (as observed from the Earth), the Moon, and the planets. It was proposed in 1948 by G M Clemence.
From the time of John Flamsteed (1646–1719) it had been believed that the Earth's daily rotation was uniform. But in the later nineteenth and early twentieth centuries, with increasing precision of astronomical measurements, it began to be suspected, and was eventually established, that the rotation of the Earth ("i.e." the length of the day) showed irregularities on short time scales, and was slowing down on longer time scales. The evidence was compiled by W de Sitter (1927) who wrote "If we accept this hypothesis, then the 'astronomical time', given by the earth's rotation, and used in all practical astronomical computations, differs from the 'uniform' or 'Newtonian' time, which is defined as the independent variable of the equations of celestial mechanics". De Sitter offered a correction to be applied to the mean solar time given by the Earth's rotation to get uniform time.
Other astronomers of the period also made suggestions for obtaining uniform time, including A Danjon (1929), who suggested in effect that observed positions of the Moon, Sun and planets, when compared with their well-established gravitational ephemerides, could better and more uniformly define and determine time.
Thus the aim developed, to provide a new time scale for astronomical and scientific purposes, to avoid the unpredictable irregularities of the mean solar time scale, and to replace for these purposes Universal Time (UT) and any other time scale based on the rotation of the Earth around its axis, such as sidereal time.
G M Clemence (1948) made a detailed proposal of this type based on the results of H Spencer Jones (1939). Clemence (1948) made it clear that his proposal was intended "for the convenience of astronomers and other scientists only" and that it was "logical to continue the use of mean solar time for civil purposes".
De Sitter and Clemence both referred to the proposal as 'Newtonian' or 'uniform' time. D Brouwer suggested the name 'ephemeris time'.
Following this, an astronomical conference held in Paris in 1950 recommended "that in all cases where the mean solar second is unsatisfactory as a unit of time by reason of its variability, the unit adopted should be the sidereal year at 1900.0, that the time reckoned in this unit be designated "ephemeris time"", and gave Clemence's formula (see Definition of ephemeris time (1952)) for translating mean solar time to ephemeris time.
The International Astronomical Union approved this recommendation at its 1952 general assembly. Practical introduction took some time (see Use of ephemeris time in official almanacs and ephemerides); ephemeris time (ET) remained a standard until superseded in the 1970s by further time scales (see Revision).
During the currency of ephemeris time as a standard, the details were revised a little. The unit was redefined in terms of the tropical year at 1900.0 instead of the sidereal year; and the standard second was defined first as 1/31556925.975 of the tropical year at 1900.0, and then as the slightly modified fraction 1/31556925.9747 instead, finally being redefined in 1967/8 in terms of the cesium atomic clock standard (see below).
Although ET is no longer directly in use, it leaves a continuing legacy. Its successor time scales, such as TDT, as well as the atomic time scale IAT (TAI), were designed with a relationship that "provides continuity with ephemeris time". ET was used for the calibration of atomic clocks in the 1950s. Close equality between the ET second with the later SI second (as defined with reference to the cesium atomic clock) has been verified to within 1 part in 1010.
In this way, decisions made by the original designers of ephemeris time influenced the length of today's standard SI second, and in turn, this has a continuing influence on the number of leap seconds which have been needed for insertion into current broadcast time scales, to keep them approximately in step with mean solar time.
Definition of ephemeris time (1952).
Ephemeris time was defined in principle by the orbital motion of the Earth around the Sun, (but its practical implementation was usually achieved in another way, see below).
Its detailed definition depended on Simon Newcomb's Tables of the Sun (1895), interpreted in a new way to accommodate certain observed discrepancies:
In the introduction to Newcomb's Tables of the Sun (1895) the basis of the tables (p. 9) includes a formula for the Sun's mean longitude, at a time indicated by interval T (in Julian centuries of 36525 mean solar days) reckoned from Greenwich Mean Noon on 0 January 1900:
Spencer Jones' work of 1939 showed that the positions of the Sun actually observed, when compared with those obtained from Newcomb's formula, show the need for the following correction to the formula to represent the observations:
(where "the times of observation are in Universal time, not corrected to Newtonian time", and 0.0748B represents an irregular fluctuation calculated from lunar observations).
Thus a conventionally corrected form of Newcomb's formula, to incorporate the corrections on the basis of mean solar time, would be the sum of the two preceding expressions:
Clemence's 1948 proposal did not adopt a correction of this kind in terms of mean solar time: instead, the same numbers were used as in Newcomb's original uncorrected formula (1), but now in a reverse sense, to define the time and time scale implicitly, based on the real position of the Sun:
where the time variable, here represented as E, now represents time in ephemeris centuries of 36525 ephemeris days of 86400 ephemeris seconds. The 1961 official reference put it this way: "The origin and rate of ephemeris time are defined to make the Sun's mean longitude agree with Newcomb's expression"
From the comparison of formulae (2) and (3), both of which express the same real solar motion in the same real time but on different time scales, Clemence arrived at an explicit expression, estimating the difference in seconds of time between ephemeris time and mean solar time, in the sense (ET-UT):
Clemence's formula, now superseded by more modern estimations, was included in the original conference decision on ephemeris time. In view of the fluctuation term, practical determination of the difference between ephemeris time and UT depended on observation. Inspection of the formulae above shows that the (ideally constant) unit of ephemeris time such as the ephemeris second has been for the whole of the twentieth century very slightly shorter than the corresponding (but not precisely constant) unit of mean solar time (which besides its irregular fluctuations tends gradually to increase), consistently also with the modern results of Morrison and Stephenson (see article ΔT).
Implementations.
Secondary realizations by lunar observations.
Although ephemeris time was defined in principle by the orbital motion of the Earth around the Sun, it was usually measured in practice by the orbital motion of the Moon around the Earth. These measurements can be considered as secondary realizations (in a metrological sense) of the primary definition of ET in terms of the solar motion, after a calibration of the mean motion of the Moon with respect to the mean motion of the Sun.
Reasons for the use of lunar measurements were practically based: the Moon moves against the background of stars about 13 times as fast as the Sun's corresponding rate of motion, and the accuracy of time determinations from lunar measurements is correspondingly greater.
When ephemeris time was first adopted, time scales were still based on astronomical observation, as they always had been. The accuracy was limited by the accuracy of optical observation, and corrections of clocks and time signals were published in arrear.
Secondary realizations by atomic clocks.
A few years later, with the invention of the cesium atomic clock, an alternative offered itself. Increasingly, after the calibration in 1958 of the cesium atomic clock by reference to ephemeris time, cesium atomic clocks running on the basis of ephemeris seconds began to be used and kept in step with ephemeris time. The atomic clocks offered a further secondary realization of ET, on a quasi-real time basis that soon proved to be more useful than the primary ET standard: not only more convenient, but also more precisely uniform than the primary standard itself. Such secondary realizations were used and described as 'ET', with an awareness that the time scales based on the atomic clocks were not identical to that defined by the primary ephemeris time standard, but rather, an improvement over it on account of their closer approximation to uniformity. The atomic clocks gave rise to the atomic time scale, and to what was first called Terrestrial Dynamical Time and is now Terrestrial Time, defined to provide continuity with ET.
The availability of atomic clocks, together with the increasing accuracy of astronomical observations (which meant that relativistic corrections were at least in the foreseeable future no longer going to be small enough to be neglected), led to the eventual replacement of the ephemeris time standard by more refined time scales including terrestrial time and barycentric dynamical time, to which ET can be seen as an approximation.
Revision of time scales.
In 1976 the IAU resolved that the theoretical basis for its current (1952) standard of Ephemeris Time was non-relativistic, and that therefore, beginning in 1984, Ephemeris Time would be replaced by two relativistic timescales intended to constitute dynamical timescales: Terrestrial Dynamical Time (TDT) and Barycentric Dynamical Time (TDB). Difficulties were recognized, which led to these being in turn superseded in the 1990s by time scales Terrestrial Time (TT), Geocentric Coordinate Time GCT(TCG) and Barycentric Coordinate Time BCT(TCB).
JPL ephemeris time argument Teph.
High-precision ephemerides of sun, moon and planets were developed and calculated at the Jet Propulsion Laboratory (JPL) over a long period, and the latest available were adopted for the ephemerides in the Astronomical Almanac starting in 1984. Although not an IAU standard, the ephemeris time argument Teph has been in use at that institution since the 1960s. The time scale represented by Teph has been characterized as a relativistic coordinate time that differs from Terrestrial Time only by small periodic terms with an amplitude not exceeding 2 milliseconds of time: it is linearly related to, but distinct (by an offset and constant rate which is of the order of 0.5 s/a) from the TCB time scale adopted in 1991 as a standard by the IAU. Thus for clocks on or near the geoid, Teph (within 2 milliseconds), but not so closely TCB, can be used as approximations to Terrestrial Time, and via the standard ephemerides Teph is in widespread use.
Partly in acknowledgement of the widespread use of Teph via the JPL ephemerides, IAU resolution 3 of 2006 (re-)defined Barycentric Dynamical Time (TDB) as a current standard. As re-defined in 2006, TDB is a linear transformation of TCB. The same IAU resolution also stated (in note 4) that the "independent time argument of the JPL ephemeris DE405, which is called Teph" (here the IAU source cites ), "is for practical purposes the same as TDB defined in this Resolution". Thus the new TDB, like Teph, is essentially a more refined continuation of the older ephemeris time ET and (apart from the periodic fluctuations) has the same mean rate as that established for ET in the 1950s.
Use of ephemeris time in official almanacs and ephemerides.
Ephemeris time based on the standard adopted in 1952 was introduced into the Astronomical Ephemeris (UK) and the American Ephemeris and Nautical Almanac, replacing UT in the main ephemerides in the issues for 1960 and after. (But the ephemerides in the Nautical Almanac, by then a separate publication for the use of navigators, continued to be expressed in terms of UT.) The ephemerides continued on this basis through 1983 (with some changes due to adoption of improved values of astronomical constants), after which, for 1984 onwards, they adopted the JPL ephemerides.
Previous to the 1960 change, the 'Improved Lunar Ephemeris' had already been made available in terms of ephemeris time for the years 1952-1959 (computed by W J Eckert from Brown's theory with modifications recommended by Clemence (1948)).
Redefinition of the second.
Successive definitions of the unit of ephemeris time are mentioned above (History). The value adopted for the 1956/1960 standard second:
was obtained from the linear time-coefficient in Newcomb's expression for the solar mean longitude (above), taken and applied with the same meaning for the time as in formula (3) above. The relation with Newcomb's coefficient can be seen from:
Caesium atomic clocks became operational in 1955, and quickly confirmed the evidence that the rotation of the earth fluctuated randomly. This confirmed the unsuitability of the mean solar second of Universal Time as a measure of time interval for the most precise purposes. After three years of comparisons with lunar observations, Markowitz et al. (1958) determined that the ephemeris second corresponded to 9 192 631 770 ± 20 cycles of the chosen cesium resonance.
Following this, in 1967/68, the General Conference on Weights and Measures (CGPM) replaced the definition of the SI second by the following:
The second is the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom.
Although this is an independent definition that does not refer to the older basis of ephemeris time, it uses the same quantity as the value of the ephemeris second measured by the cesium clock in 1958. This SI second referred to atomic time was later verified by Markowitz (1988) to be in agreement, within 1 part in 1010, with the second of ephemeris time as determined from lunar observations.
For practical purposes the length of the ephemeris second can be taken as equal to the length of the second of Barycentric Dynamical Time (TDB) or Terrestrial Time (TT) or its predecessor TDT.
The difference between ET and UT is called ΔT; it changes irregularly, but the long-term trend is parabolic, decreasing from ancient times until the nineteenth century, and increasing since then at a rate corresponding to an increase in the solar day length of 1.7 ms per century (see leap seconds).
International Atomic Time (TAI) was set equal to UT2 at 1 January 1958 0:00:00 . At that time, ΔT was already about 32.18 seconds. The difference between Terrestrial Time (TT) (the successor to ephemeris time) and atomic time was later defined as follows:
This difference may be assumed constant—the rates of TT and TAI are designed to be identical.

</doc>
<doc id="9995" url="https://en.wikipedia.org/wiki?curid=9995" title="EastEnders">
EastEnders

EastEnders is a British soap opera; the first episode was broadcast in the United Kingdom on BBC One on 19 February 1985. "EastEnders" storylines examine the domestic and professional lives of the people who live and work in the fictional London Borough of Walford in the East End of London. The series primarily centres on the residents of Albert Square, a Victorian square of terraced houses, and its neighbouring streets, namely Bridge Street, Turpin Road and George Street. The area encompasses a pub, street market, night club, community centre, funeral parlour, café, wine bar, various other small businesses, a park and allotments.
The series was initially screened as two half-hour instalments per week. Since 2001, episodes are broadcast each weekday, apart from Wednesday, on BBC One. A same-day repeat was shown on BBC Three, from 2004 until 15 February 2016, after which W took over the showing. An omnibus edition was broadcast on a Sunday, from 1985 until its discontinuation, in 2015, but, as with the repeat screening, this was picked up by W, in 2016.
It is one of the UK's highest-rated programmes, often appearing near or at the top of the week's BARB ratings. Within eight months of its launch, it reached the number-one spot in the ratings, and has consistently remained among the top-rated TV programmes in Britain. In 2013, the average audience share for an episode was around 30 per cent. Created by producer Julia Smith and script editor Tony Holland, "EastEnders" has remained a significant programme in terms of the BBC's success and audience share, and also in the history of British television drama, tackling many controversial and taboo issues in British culture and social life previously unseen on United Kingdom mainstream television.
"EastEnders" has won six BAFTA Awards and the Inside Soap Award for Best Soap for ten years running, as well as twelve National Television Awards for Most Popular Serial Drama and ten awards for Best Soap at the British Soap Awards. It has also won eight TV Quick and TV Choice Awards for Best Soap, six TRIC Awards for Soap of The Year, four Royal Television Society Awards for Best Continuing Drama and has been inducted into the Rose d'Or Hall of Fame.
Setting.
The central focus of "EastEnders" is the fictional Victorian square Albert Square in the fictional London Borough of Walford. In the show's narrative, Albert Square is a 19th century street, named after Prince Albert (1819–61), the husband of Queen Victoria (1819–1901, reigned 1837–1901). Thus, central to Albert Square is The Queen Victoria Public House.
Fans have tried to establish the location Walford would have within London if it were real. Walford East is a fictional tube station for Walford, and a tube map that was first seen on air in 1996 showed Walford East between Bow Road and West Ham, in the actual location of Bromley-by-Bow on the District and Hammersmith & City lines.
Walford has the postal district of E20. The postcode district was selected as if it were part of the actual E postcode area which covers much of east London although the next unused postcode district in the area was, and still is (), E19. The "E" stands for "Eastern". In 1917 the current postal districts in London were assigned alphabetically according to the name of the main sorting office for each district. If Walford had been assigned in this scheme it would have been given E17, which is the current postcode district for Walthamstow. In March 2011, Royal Mail allocated the E20 postal district to the 2012 Olympic Park. The postal district in "EastEnders" was entirely fictional up to that point, as London East postal districts stopped at E18 at that time. The show's creators opted for E20 instead of E19 as it was thought to sound better. In September 2011 the postal code for Albert Square was revealed in an episode as E20 6PQ.
An Albert Square exists in the East End of London in Ratcliff, and a further such square exists just beyond the East End in Stratford, but the show's producers based the square's design on Fassett Square in Dalston. There is also a market close to Fassett Square at Ridley Road. The postcode for the area, E8, was one of the working titles for the series. The name "Walford" is both a street in Dalston where Tony Holland lived and a blend of Walthamstow and Stratford—the areas of Greater London where the creators were born. Other parts of the Square and set interiors are based on other locations. The bridge is based upon one near the BBC Television Centre, the Queen Vic on the old pub at the end of Scrubs Lane/Harrow Road NW10, and the interior to the Fowlers' is based on a house in Manor Road, Colchester, close to where the supervising art director lived. The fictional local newspaper, the "Walford Gazette", in which local news events such as the arrests or murders of characters appear, mirrors the real "Hackney Gazette".
Characters.
"EastEnders" is built around the idea of relationships and strong families, with each character having a place in the community. This theme encompasses the whole Square, making the entire community a family of sorts, prey to upsets and conflict, but pulling together in times of trouble. Co-creator Tony Holland was from a large East End family, and such families have typified "EastEnders". The first central family was the combination of the Fowler family, consisting of Pauline Fowler, her husband Arthur Fowler, and teenage children Mark Fowler and Michelle Fowler and the Beale family, consisting of Pete Beale (Pauline's twin brother), his wife Kathy Beale and their teenage son Ian Beale. Pauline and Pete's mother was the domineering Lou Beale, who lived with Pauline and her family. Holland drew on the names of his own family for the characters.
The Watts and Mitchell families have been central to many notable "EastEnders" storylines, the show having been dominated by the Watts in the 1980s, with the 1990s focusing on the Mitchells. The early 2000s saw a shift in attention towards the newly introduced female Slater clan, before a renewal of emphasis upon the restored Watts family beginning in 2003. Since 2006, "EastEnders" has largely been dominated by the Mitchell and Branning families, though the early 2010s also saw a renewed focus on the Moon family, and from 2013 onwards, on the Carters. The Beales are the show's longest running family, having been in "EastEnders" since it began in 1985. Key people involved in the production of "EastEnders" have stressed how important the idea of strong families is to the programme. Peggy Mitchell, in particular, is notorious for her ceaseless repetition of such statements as "You're a Mitchell!" and "It's all about family!". Pauline Fowler is also known for her insistence on family and mentioning her brother and husband to instil loyalty from family members. Her mother Lou Beale is renowned for her family meetings and traditional approach to family. More recently, Derek Branning regularly expresses the importance of a strong family unit. As the eldest sibling, he is constantly asserting his position as head of his family and reminding everyone to pull together in times of trouble. Additionally, Derek commonly refers to himself, Max Branning and Jack Branning as "the Branning brothers."
Some families feature a stereotypical East End matriarch. Indeed, the matriarchal role is one that has been seen in various reincarnations since the programme's inception, often depicted as the centre of the family unit. The original matriarch was Lou Beale, though later examples include Pauline Fowler, Mo Harris, Pat Butcher, Peggy Mitchell, Zainab Masood, Cora Cross, and to some extent Dot Cotton. These characters are seen as being loud and interfering but most importantly, responsible for the well-being of the family and usually stressing the importance of family, reflecting on the past.
As is traditional in British soaps, female characters in general are central to the programme. These characters include strong, brassy, long-suffering women who exhibit diva-like behaviour and stoically battle through an array of tragedy and misfortune. Such characters include Angie Watts, Kathy Beale, Sharon Watts, Pat Butcher, Denise Fox and Tanya Branning. Conversely there are female characters who handle tragedy less well, depicted as eternal victims and endless sufferers, who include Sue Osman, Little Mo Mitchell, Laura Beale, Lisa Fowler, Ronnie Mitchell and Linda Carter. The 'tart with a heart' is another recurring character, often popular with viewers. Often their promiscuity masks a hidden vulnerability and a desire to be loved. Such characters have included Pat Butcher (though in her latter years, this changed), Tiffany Mitchell, Kat Slater, Stacey Slater, Dawn Swann and Roxy Mitchell.
A gender balance in the show is maintained via the inclusion of various "macho" male personalities such as Mick Carter, Phil Mitchell, Grant Mitchell, Jack Branning and Max Branning, "bad boys" such as Den Watts, Michael Moon and Vincent Hubbard, and "heartthrobs" such as Simon Wicks, Jamie Mitchell, Dennis Rickman and Joey Branning. Another recurring male character type is the smartly dressed businessman, often involved in gang culture and crime and seen as a local authority figure. Examples include Derek Branning, Steve Owen, Jack Dalton, Andy Hunter and Johnny Allen. Following criticism aimed at the show's over-emphasis on 'gangsters' in 2005, such characters have been significantly reduced. Another recurring male character seen in "EastEnders" is the 'loser' or 'soft touch', males often comically under the thumb of their female counterparts, which have included Arthur Fowler, Ricky Butcher, Lofty Holloway and Billy Mitchell. Other recurring character types that have appeared throughout the serial are "cheeky-chappies" Pete Beale, Alfie Moon, Garry Hobbs and Kush Kazemi, "lost girls" such as Mary Smith, Donna Ludlow and Mandy Salter, delinquents such as Stacey Slater, Jay Brown and Lola Pearce, "villains" such as Nick Cotton, Trevor Morgan, May Wright, Yusef Khan, Archie Mitchell and Dean Wicks, "bitches" such as Cindy Beale, Janine Butcher, Lucy Beale, Abi Branning and Babe Smith and cockney "wide boys" or "wheeler dealers" such as Frank Butcher, Alfie Moon, Kevin Wicks, Darren Miller and Fatboy.
Over the years "EastEnders" has typically featured a number of elderly residents, who are used to show vulnerability, nostalgia, stalwart-like attributes and are sometimes used for comedic purposes. The original elderly residents included Lou Beale, Ethel Skinner and Dot Cotton. Over the years they have been joined by the likes of Mo Butcher, Jules Tavernier, Marge Green, Nellie Ellis, Jim Branning, Charlie Slater, Mo Harris, Patrick Trueman, Cora Cross, Les Coker, Rose Cotton, Pam Coker, Stan Carter, Babe Smith and Claudette Hubbard. Focus on elderly characters has decreased since the show's inception. The programme has more recently included a higher number of teenagers and successful young adults in a bid to capture the younger television audience. This has spurred criticism, most notably from the actress Anna Wing, who played Lou Beale in the show. She commented "I don't want to be disloyal, but I think you need a few mature people in a soap because they give it backbone and body... if all the main people are young it gets a bit thin and inexperienced. It gets too lightweight."
"EastEnders" has been known to feature a 'comedy double-act', originally demonstrated with the characters of Dot and Ethel, whose friendship was one of the serial's most enduring. Other examples include Paul Priestly and Trevor Short, Huw Edwards and Lenny Wallace, Shirley Carter and Heather Trott, Garry Hobbs and Minty Peterson, Denise Fox and Zainab Masood, Poppy Meadow and Jodie Gold, and Kat Slater and Bianca Jackson. The majority of "EastEnders"' characters are working-class. Middle-class characters do occasionally become regulars, but have been less successful and rarely become long-term characters. In the main, middle-class characters exist as villains, such as James Wilmott-Brown, May Wright, Stella Crawford and Yusef Khan, or are used to promote positive liberal influences, such as Colin Russell or Rachel Kominski.
"EastEnders" has always featured a culturally diverse cast which has included black, Asian, Turkish, Polish and Latvian characters. "The expansion of minority representation signals a move away from the traditional soap opera format, providing more opportunities for audience identification with the characters and hence a wider appeal". Despite this, the programme has been criticised by the Commission for Racial Equality, who argued in 2002 that "EastEnders" was not giving a realistic representation of the East End's "ethnic make-up". They suggested that the average proportion of visible minority faces on "EastEnders" was substantially lower than the actual ethnic minority population in East London boroughs, and it therefore reflected the East End in the 1960s, not the East End of the 2000s. Furthermore, it was suggested that an element of "tokenism" and stereotyping surrounded many of these minority characters. The programme has since attempted to address these issues. A sari shop was opened and various characters of differing ethnicities were introduced throughout 2006 and 2007, including the Fox family, the Masoods, and various background artists. This was part of producer Diederick Santer's plan to "diversify", to make EastEnders "feel more 21st century". On 24 February 2009 for the first time in the soaps history, an entire episode was screened consisting entirely of Black actors. "EastEnders" has had varying success with ethnic minority characters. Possibly the least successful were the Indian Ferreira family, who were not well received by critics or viewers and were dismissed as unrealistic by the Asian community in the UK.
"EastEnders" has been praised for its portrayal of characters with disabilities, including Adam Best (spina bifida), Noah Chambers (deaf), Jean Slater and her daughter Stacey (bipolar disorder), Janet Mitchell (Down's syndrome), Jim Branning (stroke).
"EastEnders" has a high cast turnover and characters are regularly changed to facilitate storylines or refresh the format. The show has also become known for the return of characters after they have left the show. Sharon Rickman returned in August 2012 for her seventh stint on the show. Den Watts returned 14 years after he was believed to have died, a feat repeated by Kathy Beale in 2015. Speaking extras, including Tracey the barmaid (who has been in the show since the first episode in 1985), have made appearances throughout the show's duration, without being the focus of any major storylines. The character of Nick Cotton gained a reputation for making constant exits and returns since the programme's first year, until the character's death in 2015.
Over "EastEnders" 30-year history, Sharon, Kathy Beale and Ian Beale are the only three original characters remaining in the soap, following Kathy's return from the dead during "EastEnders" 30th anniversary episode. Ian Beale is the only character to have appeared continuously from the first episode without officially leaving, and is the longest serving character in "EastEnders". Dot Cotton is the longest serving female character in the show having served since 1985, while Pat Butcher is the longest-serving former character, appearing from 1986 until early 2012.
Production.
The majority of "EastEnders" episodes are filmed at the BBC Elstree Centre in Borehamwood, Hertfordshire. When the number of episodes was increased to four per week, more studio space was needed, so "Top of the Pops" was moved from its studio at Elstree to BBC Television Centre in April 2001. Episodes are produced in "quartets" of four episodes, each of which starts filming on a Tuesday and takes nine days to record. Each day, between 25 and 30 scenes are recorded. During the filming week, actors can film for as many as eight to twelve episodes. Exterior scenes are filmed on a specially constructed film lot, and interior scenes take place in four studios. The episodes are usually filmed about six to eight weeks in advance of broadcast. During the winter period, filming can take place up to twelve weeks in advance, due to less daylight for outdoor filming sessions. This time difference has been known to cause problems when filming outdoor scenes. On 8 February 2007, heavy snow fell on the set and filming had to be cancelled as the scenes due to be filmed on the day were to be transmitted in April. "EastEnders" is normally recorded using four cameras. When a quartet is completed, it is edited by the director, videotape editor and script supervisor. The producer then reviews the edits and decides if anything needs to be re-edited, which the director will do. A week later, sound is added to the episodes and they are technically reviewed, and are ready for transmission if they are deemed of acceptable quality.
Although episodes are predominantly recorded weeks before they are broadcast, occasionally, "EastEnders" includes current events in their episodes. In 1987, "EastEnders" covered the general election. Using a plan devised by co-creators Smith and Holland, five minutes of material was cut from four of the pre-recorded episodes preceding the election. These were replaced by specially recorded election material, including representatives from each major party, and a scene recorded on the day after the election reflecting the result, which was broadcast the following Tuesday. The result of the 2010 general election was referenced in the 7 May 2010 episode. During the 2006 FIFA World Cup, actors filmed short scenes following the tournament's events that were edited into the programme in the following episode. Last-minute scenes have also been recorded to reference the fiftieth anniversary of the end of the Second World War in 1995, the two-minute silence on Remembrance Day 2005 (2005 also being the year for the sixtieth anniversary of the end of the Second World War and the 200th anniversary of the Battle of Trafalgar), Barack Obama's election victory in 2008, the death of Michael Jackson in 2009, the 2010 Comprehensive Spending Review, Andy Murray winning the Men's Singles at the 2013 Wimbledon Championships, the wedding of Prince William and Kate Middleton, the birth of Prince George of Cambridge., Scotland voting no against independence in 2014, and the 100th anniversary of the beginning of the Great War.
"EastEnders" is often filmed on location, away from the studios in Borehamwood. Sometimes an entire quartet is filmed on location, which has a practical function and are the result of "EastEnders" making a "double bank", when an extra week's worth of episodes are recorded at the same time as the regular schedule, enabling the production of the programme to stop for a two-week break at Christmas. These episodes often air in late June or early July and again in late October or early November. The first time this happened was in December 1985 when Pauline (Wendy Richard) and Arthur Fowler (Bill Treacher) travelled to the Southend-on-Sea to find their son Mark, who had run away from home. In 1986, "EastEnders" filmed overseas for the first time, in Venice, and this was also the first time it was not filmed on videotape, as a union rule at the time prevented producers taking a video crew abroad and a film crew had to be used instead.
If scenes during a normal week are to be filmed on location, this is done during the normal recording week. Off-set locations that have been used for filming include Clacton (1989), Devon (September 1990), Hertfordshire (used for scenes set in Gretna Green in July 1991), Portsmouth (November 1991), Milan (1997), Ireland (1997), Amsterdam (December 1999), Brighton (2001) and Portugal (2003). In 2003, filming took place at Loch Fyne Hotel and Leisure Club in Inveraray, The Arkinglass Estate in Cairndow and Grims Dyke Hotel, Harrow Weald, north London, for a week of episodes set in Scotland. 9 April 2007 episode featured scenes filmed at St Giles Church and The Blacksmiths Arms public house in Wormshill, the Ringlestone Inn, two miles away and Court Lodge Farm in Stansted, Kent. Other locations have included the court house, a disused office block, Evershed House, and St Peter's Church, all in St Albans, an abandoned mental facility in Worthing, Carnaby Street in London, and a wedding dress shop in Muswell Hill, north London. A week of episodes in 2011 saw filming take place on a beach in Thorpe Bay and a pier in Southend-on-Sea—during which a stuntman was injured when a gust of wind threw him off balance and he fell onto rocks— with other scenes filmed on the Essex coast. In 2012, filming took place in Keynsham, Somerset. In January 2013, on-location filming at Grahame Park in Colindale, north London, was interrupted by at least seven youths who threw a firework at the set and threatened to cut members of the crew. In October 2013, scenes were filmed on a road near London Southend Airport in Essex.
The two-handers (when only two actors appear in an episode) were originally done for speed; while a two-hander is being filmed, the rest of the cast can be making another episode.
"EastEnders" has featured seven live broadcasts. For its 25th anniversary in February 2010, a live episode was broadcast in which Stacey Slater (Lacey Turner) was revealed as Archie Mitchell's (Larry Lamb) killer. Turner was told only 30 minutes before the live episode and to maintain suspense, she whispers this revelation to former lover and current father-in-law, Max Branning, in the very final moments of the live show. Many other cast members only found out at the same time as the public, when the episode was broadcast. On 23 July 2012, a segment of that evening's episode was screened live as Billy Mitchell (Perry Fenwick) carried the Olympic Flame around Walford in preparation for the 2012 Summer Olympics. In February 2015, for the soap's 30th anniversary, five episodes in a week featured live inserts throughout them. Episodes airing on Tuesday 17, Wednesday 18 and Thursday 19 (which featured an hour long episode and a second episode) all featured at least one live insert. The show revealed that the killer of Lucy Beale (Hetti Bywater) was her younger brother, Bobby (Eliot Carrington), during the second episode on Thursday, after a ten month mystery regarding who killed her. Bobby killed his sister in a flashback episode which revisited the night of the murder. The aftermath episode, which aired on Friday 20, was completely live and explained in detail Lucy's death. Carrington was told he was Lucy's killer on Monday 16, while Laurie Brett (who plays Bobby's adoptive mother, Jane) was informed in November, due to the character playing a huge role in the cover-up of Lucy's murder. Bywater only discovered Bobby was responsible for Lucy's death on the morning of Thursday 19, several hours before they filmed the scenes revealing Bobby as Lucy's killer.
Rebuilding the "EastEnders" set.
Executive producer Dominic Treadwell-Collins has said that he wants Albert Square to look like a real-life east London neighbourhood in 2014 so that soon the soap will "better reflect the more fashionable areas of east London beloved of young professionals" giving a flavour of the "creeping gentrification" of east London. He added, "It should feel more like London. It's been frozen in aspic for too long." In 2014, the BBC announced that they will rebuild the "EastEnders" set, to secure the long-term future of the show, with completion expected to be in 2018. The set will provide a modern, upgraded exterior filming resource for "EastEnders", and will copy the appearance of the existing buildings. However, it will be 20% bigger than the current set, in order to enable greater editorial ambition and improve working conditions for staff. A temporary set will be created on site to enable filming to continue while the permanent structure is rebuilt.
Realism.
"EastEnders" programme makers took the decision that the show was to be about "everyday life" in the inner city "today" and regarded it as a "slice of life". Creator/producer Julia Smith declared that "We don't make life, we reflect it". She also said, "We decided to go for a realistic, fairly outspoken type of drama which could encompass stories about homosexuality, rape, unemployment, racial prejudice, etc., in a believable context. Above all, we wanted realism".
In the 1980s, "EastEnders" featured "gritty" storylines involving drugs and crime, representing the issues faced by working-class Britain. Storylines included the cot death of 14-month-old Hassan Osman, Nick Cotton's homophobia, racism and murder of Reg Cox, Arthur Fowler's unemployment reflecting the recession of the 1980s, the rape of Kathy Beale in 1988 by James Willmott-Brown and Michelle Fowler's teenage pregnancy. The show also dealt with prostitution, mixed-race relationships, shoplifting, sexism, divorce, domestic violence and mugging.
As the show progressed into the 1990s, "EastEnders" still featured hard-hitting issues such as Mark Fowler discovering he was HIV positive in 1991, the death of his wife Gill from an AIDS-related illness in 1992, murder, adoption, abortion, Peggy Mitchell's battle with breast cancer, and Phil Mitchell's alcoholism and violence towards wife Kathy. Mental health issues were confronted in 1996 when 16-year-old Joe Wicks developed schizophrenia following the off-screen death of his sister in a car crash.
In the early 2000s, "EastEnders" covered the issue of euthanasia (Ethel Skinner's death in a pact with her friend Dot Cotton), the unveiling of Kat Slater's abuse by her uncle Harry as a child (which led to the birth of her daughter Zoe, who had been brought up to believe that Kat was her sister), the domestic abuse of Little Mo Morgan by husband Trevor (which involved rape and culminated in Trevor's death after he tried to kill Little Mo in a fire), Sonia Jackson giving birth at the age of 15 and then putting her baby up for adoption, and Janine Butcher's prostitution, agoraphobia and drug addiction. The soap also tackled the issue of mental illness and carers of people who have mental conditions, illustrated with mother and daughter Jean and Stacey Slater; Jean suffers from bipolar disorder, and teenage daughter Stacey was her carer (this storyline won a Mental Health Media Award in September 2006). Stacey went on to struggle with the disorder herself. The issue of illiteracy was highlighted by the characters of middle-aged Keith and his young son Darren. "EastEnders" has also covered the issue of Down's syndrome, as Billy and Honey Mitchell's baby, Janet, was born with the condition in 2006. "EastEnders" covered child abuse with its storyline involving Phil Mitchell's 11-year-old son Ben and lawyer girlfriend Stella Crawford, and child grooming involving the characters Tony King and Whitney Dean. David Proud, who plays the character of Adam Best, is the first wheelchair-using actor in the soap's history.
Aside from this, soap opera staples of youthful romance, jealousy, domestic rivalry, gossip and extramarital affairs are regularly featured, with high-profile storylines occurring several times a year. Whodunits also feature regularly, including the "Who Shot Phil?" storyline in 2001 that attracted over 19 million viewers and was one of the biggest successes in British soap television. Another whodunit is the murder of Archie Mitchell (Larry Lamb) who was killed on Christmas Day 2009 after making several enemies. The killer was revealed to be Stacey Branning in a special live episode of the show to mark its 25th anniversary; an episode which drew a peak of 17 million viewers.
History.
The idea for a new soap opera on BBC1 was conceived in 1983, by BBC executives, principally David Reid, the then Head of Series & serials, who was keen for the BBC to produce a new evening soap opera. They gave the job of creating this new soap to script writer Tony Holland and producer Julia Smith, famous for their work together on "Z-Cars". They created twenty-four original characters for the show, based upon Holland's own family, and people they remembered from their own experiences in the East End. Granada Television gave Smith unrestricted access to the "Coronation Street" production for a month so that she could get a sense how a continuing drama was produced.
They cast actors for their characters, and began to film the show at BBC Elstree Centre in Borehamwood, Hertfordshire. Alan Jeapes and Simon May created the title sequence and theme tune, and the show with a working title of "East 8" was renamed "EastEnders", when Smith and Holland realised they had been phoning casting agencies for months asking whether they had "any real East Enders" on their books. Julia Smith thought ""Eastenders"" "looked ugly written down", and capitalised the second 'e', and thus the name "EastEnders" was born. Filming commenced in late-1984 and the show was first broadcast on 19 February 1985, and became wildly popular, often displacing "Coronation Street" from the top of the ratings for the rest of the 1980s.
Scheduling.
Broadcast.
Since 1985, "EastEnders" has remained at the centre of BBC One's primetime schedule. It is currently broadcast at 7.30pm on Tuesday and Thursday, and 8pm on Monday and Friday. "EastEnders" was originally broadcast twice weekly at 7 pm on Tuesdays and Thursdays from 19 February 1985, with an omnibus at 3:30 pm on Sundays; however, in August 1985 the two episodes were moved to 7:30 pm as Michael Grade did not want the soap running in direct competition with "Emmerdale Farm", and this remained the same until 7 April 1994. The BBC had originally planned to take advantage of the 'summer break' that "Emmerdale Farm" usually took to capitalise on ratings, but ITV added extra episodes and repeats so that "Emmerdale Farm" was not taken off the air over the summer. Realising the futility of the situation, Grade decided to move the show to the later 7:30 pm slot, but to avoid tabloid speculation that it was a 'panic move' on the BBC's behalf, they had to "dress up the presentation of that move in such a way as to protect the show" giving "all kinds of reasons" for the move.
"EastEnders" output then increased to three times a week on Mondays, Tuesday and Thursdays from 11 April 1994 until 2 August 2001.
From 10 August 2001, "EastEnders" then added its fourth episode (shown on Fridays). This caused some controversy as it clashed with "Coronation Street", which at the time was moved to 8pm to make way for an hour-long episode of rural soap "Emmerdale" at 7pm. The move immediately provoked an angry response from ITV insiders, who argued that the BBC's last-minute move—only revealed at 3.30pm on the day—broke an unwritten scheduling rule that the two flagship soaps would not be put directly against each other. In this first head-to-head battle, "EastEnders" claimed victory over its rival.
In early 2003, viewers could watch episodes of "EastEnders" on digital channel BBC Three before they were broadcast on BBC One. This was to coincide with the relaunch of the channel and helped BBC Three break the one million viewers mark for the first time with 1.03 million who watched to see Mark Fowler's departure. EastEnders was repeated each evening at 22.30 on BBC Three – having previously been shown on that channel at 22.00. When BBC Three moved online in February 2016, the W channel took over the repeat broadcastings. As of 2006 according to the EastEnders website there are on average around 208 episodes outputted each year.
Repeats.
, episodes of "EastEnders" are repeated on BBC Three and are available on-demand through BBC iPlayer for thirty days after their original screening. "EastEnders" was regularly repeated at 10pm on BBC Choice since the channel's launch in 1998, a practice continued by BBC Three for many years until mid-2012 with the repeat moving to 10.30pm. From 25 December 2010 to 29 April 2011 the show was repeated on BBC HD in a Simulcast with BBC Three. In 2015, the BBC Three repeat moved back to 10pm. In February 2016, the repeat will move to W, the rebranded Watch, after BBC Three becomes an online-only channel.
The omnibus edition, a compilation of the week's episodes in a continuous sequence, originally aired on BBC One on Sunday afternoons, until 1 April 2012 when it was changed to a late Friday night or early Saturday morning slot, commencing 6 April 2012, though the exact time differed. It reverted to a weekend daytime slot as from January 2013 on BBC Two. In 2014, the omnibus moved back to around midnight on Friday nights, and in April 2015, the omnibus was axed, following detailed audience research and the introduction of 30-day catch up on BBC iPlayer and the planning of BBC One +1. When W takes over the same-day repeat of "EastEnders", the will also show a weekend omnibus.
From 20 February to 26 May 1995, as part of the programme's 10th Anniversary celebrations, episodes from 1985 were repeated each morning at 10am, starting from episode one. Four specially selected episodes from 1985 and 1986 were also repeated on BBC1 on Friday evenings at 8.00pm under the banner "The Unforgettable EastEnders". These included The wedding of Michelle Fowler and Lofty Holloway, The revelation of the father of Michelle's baby, a two-hander between Dot Cotton and Ethel Skinner and the 1986 Christmas episode featuring Den Watts presenting Angie Watts with divorce papers.
"EastEnders" reruns began on UKTV Gold when the channel launched in 1992. The series ran from the first episode and followed its original broadcast order until August 1996 when the channel looped back to the first episode. In October 2008 UKTV Gold ceased showing "EastEnders". The last episode shown was from January 2006. Watch launched in October 2008, and they briefly revived the "EastEnders" reruns from 5 January 2009 to 24 April 2009, finishing with episodes originally broadcast in June 2006.
On 1 December 2012, the BBC uploaded the first 54 episodes of "EastEnders" to YouTube, and on 23 July 2013 they uploaded a further 14 episodes bringing the total to 68.
International.
"EastEnders" is broadcast around the world in many English-speaking countries. It is shown on BBC Entertainment (formerly BBC Prime) in Europe and in Africa, where it is approximately six episodes behind the UK. It was also shown on BBC Prime in Asia, but when the channel was replaced by BBC Entertainment, it ceased showing the series. In Canada, "EastEnders" was shown on BBC Canada until 2010, at which point it was picked up by VisionTV.
In Ireland, "EastEnders" was shown on TV3 from September 1998 until March 2001, when it moved over to RTÉ One, after RTÉ lost the rights to air rival soap "Coronation Street" to TV3. The series is simulcast with BBC One, which is widely available in the Republic, but carries advertising since its 1998 debut on Irish TV.
HM Forces and their families stationed overseas can watch "EastEnders" on BBC One, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands and Tristan da Cunha. It was previously shown on BFBS1.
EastEnders is currently shown on BBC Entertainment on weekdays at 17:30 CET, having previously been shown on BBC Prime, BBC World Service Television and BBC TV Europe since the 1980s. In Sweden, Denmark, Finland and Norway, it is shown with local subtitles.
The series was broadcast in the United States until BBC America ceased broadcasts of the serial in 2003, amidst fan protests. In June 2004, the Dish Network satellite television provider picked up "EastEnders", broadcasting episodes starting at the point where BBC America had ceased broadcasting them, offering the serial as a pay-per-view item. Episodes air two months behind the UK schedule. Episodes from prior years are still shown on various PBS stations in the US.
The series was screened in Australia by ABC TV from 1987 until 1991. Currently the series is aired in Australia on Foxtel Pay TV channel BBC UKTV, from Mondays to Thursdays at 6.15 pm EST. It is around 2 weeks behind the UK airings. In New Zealand, it was shown by TVNZ on TV One for several years, and then on Prime each weekday afternoon. It is currently shown by BBC UKTV Mondays to Thursdays at 8.00pm. Episodes are about 2 weeks behind the UK.
Spin-offs and merchandise.
The 1993 2-part story, entitled Dimensions in Time, was made in collaboration with the cast of the BBC Doctor Who and was filmed partly on the EastEnders set.
In 1998, "EastEnders Revealed" was launched on BBC Choice (now BBC Three). The show takes a look behind the scenes of the "EastEnders" and investigates particular places, characters or families within "EastEnders". An episode of "EastEnders Revealed" that was commissioned for BBC Three attracted 611,000 viewers.
As part of the BBC's digital push, "EastEnders Xtra" was introduced in 2005. The show was presented by Angellica Bell and was available to digital viewers at 8.30pm on Monday nights. It was also shown after the Sunday omnibus. The series went behind the scenes of the show and spoke to some of the cast members. A new breed of behind-the-scenes programmes have been broadcast on BBC Three since 1 December 2006. These are all documentaries related to current storylines in "EastEnders", in a similar format to "EastEnders Revealed", though not using the "EastEnders Revealed" name.
In October 2009, a 12-part Internet spin-off series entitled "" was announced. The series was conceived by executive producer Diederick Santer "as a way of nurturing new, young talent, both on- and off-screen, and exploring the stories of the soaps' anonymous bystanders." "E20" features a group of sixth-form characters and targets the ""Hollyoaks" demographic". It was written by a team of young writers and was shown three times a week on the "EastEnders" website from 8 January 2010. A second ten-part series started in September 2010, with twice-weekly episodes available online and an omnibus on BBC Three. A third series of 15 episodes started in September 2011.
"EastEnders" and rival soap opera "Coronation Street" took part in a crossover episode for Children in Need on 19 November 2010 called "East Street".
On 4 April 2015, "EastEnders" confirmed plans for a BBC One series featuring Kat and Alfie Moon. The six-part drama, "Redwater", was created by executive producer Dominic Treadwell-Collins and his team. In the spin-off, the Moons will leave Walford and visit Ireland where they "search for answers to some very big questions."
Popularity and viewership.
An example of "EastEnders" popularity is that after episodes, electricity use in the United Kingdom rises significantly as viewers who have waited for the show to end begin boiling water for tea, a phenomenon known as TV pickup. Over five minutes, power demand rises by three GW, the equivalent of 1.5 to 1.75 million teakettles. National Grid personnel watch the show to know when closing credits begin so they can prepare for the surge, asking for additional power from France if necessary.
Ratings.
"EastEnders" proved highly popular and Appreciation Indexes reflected this, rising from 55–60 at the launch to 85–95 later on, a figure which was nearly ten points higher than the average for a British soap opera. Research suggested that people found the characters true to life, the plots believable and, importantly in the face of criticism of the content, people watched as a family and regarded it as viewing for all the family. Based on market research by BBC commissioning in 2003, "EastEnders" is most watched by 60- to 74-year-olds, closely followed by 45- to 59-year-olds. An average "EastEnders" episode attracts a total audience share between 35 and 40%. Aside from that, the 10pm repeat showing on BBC Three attracts an average of 500,000 viewers, whilst the Sunday omnibus generally attracts 3 million. "EastEnders" is one of the more popular programmes on British television and regularly attracts between 8 and 12 million viewers in official ratings. and while the show's ratings have fallen since its initial surge in popularity and the advent of multichannel digital television, the programme continues to be successful for the BBC. "EastEnders" two main rivals are ITV soaps "Coronation Street" and "Emmerdale". In 2001, "EastEnders" clashed with "Coronation Street" for the first time. "EastEnders" won the battle with 8.4 million viewers (41% share) whilst "Coronation Street" lagged behind with 7.3 million viewers (34% share). The live 25th anniversary show on 19 February 2010, which revealed Stacey Branning as Archie Mitchell's killer, received 16.41 million viewers, the show's highest rating since 14 November 2003.
The launch show in 1985 attracted 17 million viewers The Christmas Day 1986 episode attracted a combined 30.15 million viewers who tuned into either the original or omnibus repeat transmission to see Den Watts hand over divorce papers to wife Angie. This remains the highest rated episode of a soap in British television history.
On 21 September 2004, Louise Berridge, the then executive producer, quit following criticism of the show. The following day the show received its lowest ever ratings at that time (6.2 million) when ITV scheduled an hour-long episode of "Emmerdale" against it. "Emmerdale" was watched by 8.1 million people. The poor ratings motivated the press into reporting viewers were bored with implausible and ill thought out storylines. Kathleen Hutchison, who had been the executive producer of hospital drama "Holby City", was announced as the new executive producer. Within a few weeks, she announced a major shake-up of the cast with the highly criticised Ferreira family, first seen in June 2003, written out at the beginning of 2005. Hutchison went on to axe other characters including Andy Hunter, Kate Mitchell, Juley Smith and Derek Harkinson.
In January 2005, after just four months, Kathleen Hutchison left "EastEnders". John Yorke, who led "EastEnders" through what Mal Young (the then head of BBC drama) said was one of its most successful periods in 2001, returned to the BBC as the head of drama, meaning his responsibilities included the running of "EastEnders". He also brought back long serving script writer Tony Jordan. It is reported that the cast and crew did not get on well with Hutchison as she had them working up to midnight and beyond. She is also said to have rejected several planned storylines and demanded re-writes. This was one of the reasons storylines such as the Real Walford football team were suddenly ignored. But through her short reign she led "EastEnders" to some of its most healthy viewing figures in months. Yorke immediately stepped into her position until a few weeks later when Kate Harwood was announced as the new executive producer.
2005 saw "EastEnders" ratings again decline. On 1 March 2005 "EastEnders" received its second lowest ratings at that time, when both "EastEnders" and "Emmerdale" broadcast one-hour episodes starting at 7pm. The episode of "Emmerdale" attracted 9.06 million viewers, leaving "EastEnders" with just 6.2 million viewers. Two weeks later, on 17 March 2005, "EastEnders" received its lowest ever ratings at that time, when ITV screened another hour-long special of "Emmerdale" to mark the show's 4000th episode. "Emmerdale" was watched by 8.8 million viewers, whilst "EastEnders" was watched by 6.2 million viewers. Ratings reached a new all-time low in July 2006 with 5.2 million viewers, followed two days later by only 3.9 million, when it was scheduled against an hour-long episode of "Emmerdale" which attained 9.03 million viewers.
"EastEnders" received its second lowest ratings on 17 May 2007, when 4.0 million viewers tuned in to see Ian Beale and Phil Mitchell's car crash, part of the show's most expensive stunt. This was also the lowest ever audience share, with just 19.6%. This was attributed to a conflicting one hour special episode of "Emmerdale" on ITV1 which revealed the perpetrator in the long-running Tom King murder mystery storyline. "Emmerdale"'s audience peaked at 9.1 million. Ratings for the 10pm "EastEnders" repeat on BBC Three reached an all-time high of 1.4 million. However, on Christmas Day 2007, "EastEnders" gained one of its highest ratings for years and the highest ratings for any TV programme in 2007, when 13.9 million viewers saw Bradley Branning find out his wife Stacey had been cheating with his father, Max. The earlier first half had achieved 11.8 million viewers. The second half of the double bill was the most watched programme on Christmas Day 2007 in the UK, while the first half was third most watched, surpassed only by the "Doctor Who" Christmas special. When official figures came out a few weeks later, it was confirmed 14.38 million viewers had watched the Christmas Day episode of "EastEnders", and that it had the highest UK TV audience for a TV show during 2007.
The live-episode of "EastEnders" on 19 February 2010 averaged 15.6 million viewers, peaking at 16.6 million in the final five minutes of broadcast. In April 2010 following a head to head with an hour-long edition of "Emmerdale", EastEnders recorded its lowest viewing figures of 2010 with just 5.88 million viewers tuning in, bringing an end to a 10-week reign at the top of the Thursday night viewing figures. In July 2013 the programme became the third highest rated soap behind rivals Emmerdale and Coronation Street. On 12 November 2013 the programme had its highest figure since April with 7.8 million viewers (according to overnight figures) as viewers saw the kidnap of Ian Beale beating "Emmerdale" by 1 million viewers. On Christmas Day 2013, EastEnders was watched by 9.36 million viewers as Janine Butcher (Charlie Brooks) was arrested for murder. The following Christmas, EastEnders received 7.5 million viewers, in overnight figures, beating competing soaps Coronation Street and Emmerdale which had 6.6 million viewers and 5.7 million respectively.
The following week on New Year's Day, Eastenders was watched by an average of 8.53 million viewers according to overnight figures peaking with 10.4 million viewers as Ronnie Mitchell married Charlie Cotton. It was almost the most watched programme of that day and the highest rated episode since 21 January 2013.
During the 30th anniversary week in which there were live elements and the climax of the Who Killed Lucy Beale? storyline, 10.84 million viewers tuned in for the 30th anniversary episode itself in an hour long special on 19 February 2015 (peaking with 11.9 million) which featured Jane Beale seemingly being revealed as Lucy Beale's killer and the shock return of Kathy Beale. Later on in the same evening, at 9:25pm the BBC screened a special flashback episode showing the events during the night Lucy died and in a further twist her killer was revealed to be in fact Bobby Beale. The episode averaged 10.3 million viewers, and peaked with 11.2 million. The following day, the anniversary week was rounded off with another fully live episode (the second after 2010) with 9.97 million viewers watching the aftermath of the reveal, the Beale family finding out the truth of Lucy's killer and deciding to keep it a secret.
Internet.
Between 2001 and 2002, "EastEnders" was the 10th most searched-for TV show on the Internet. It was the 2nd most popular UK search term in 2003, and the 7th in 2004.
Criticism.
"EastEnders" is the most complained about programme on the BBC. It has received both praise and criticism for most of its storylines, which have dealt with difficult themes, such as violence, rape, murder and child abuse.
Mary Whitehouse, social critic, argued at the time that "EastEnders" represented a violation of "family viewing time" and that it undermined the watershed policy. She regarded "EastEnders" as a fundamental assault on the family and morality itself. She made reference to representation of family life and emphasis on psychological and emotional violence within the show. She was also critical of language such as "bleeding", "bloody hell", "bastard" and "for Christ's sake". However, Whitehouse also praised the programme, describing Michelle Fowler's decision not to have an abortion as a "very positive storyline". She also felt that "EastEnders" had been cleaned up as a result of her protests, though she later commented that "EastEnders" had returned to its old ways. Her criticisms were widely reported in the tabloid press as ammunition in its existing hostility towards the BBC. The stars of "Coronation Street" in particular aligned themselves with Mary Whitehouse, gaining headlines such as "STREETS AHEAD! RIVALS LASH SEEDY EASTENDERS" and "CLEAN UP SOAP! Street Star Bill Lashes 'Steamy' EastEnders".
In 1997 several episodes were shot and set in Ireland, resulting in criticisms for portraying the Irish in a negatively stereotypical way. Ted Barrington, the Irish Ambassador to the UK at the time, described the portrayal of Ireland as an "unrepresentative caricature", stating he was worried by the negative stereotypes and the images of drunkenness, backwardness and isolation. Jana Bennett, the BBC's then director of production, later apologised for the episodes, stating on BBC1's news bulletin: "It is clear that a significant number of viewers have been upset by the recent episodes of "EastEnders", and we are very sorry, because the production team and programme makers did not mean to cause any offence." A year later BBC chairman Christopher Bland admitted that as result of the Irish-set EastEnders episodes, the station failed in its pledge to represent all groups accurately and avoid reinforcing prejudice.
The long-running storyline of Mark Fowler's HIV was so successful in raising awareness that in 1999, a survey by the National Aids Trust found teenagers got most of their information about HIV from the soap, though one campaigner noted that in some ways the storyline was not reflective of what was happening at the time as the condition was more common among the gay community. Still, heterosexual Mark struggled with various issues connected to his HIV status, including public fears of contamination, a marriage breakdown connected to his inability to have children and the side effects of combination therapies. In 2002, when the makers of the series decided to write Mark out of the series as his disease became untreatable, he left Walford in 2003 to travel the world, and his death was announced a year later.
The child abuse storyline with Kat Slater and her uncle Harry saw calls to the National Society for the Prevention of Cruelty to Children (NSPCC) go up by 60%. The chief executive of the NSPCC praised the storyline for covering the subject in a direct and sensitive way, coming to the conclusion that people were more likely to report any issues relating to child protection because of it. In 2002, "EastEnders" also won an award from the Mental Health Media Awards held at BAFTA for this storyline.
"EastEnders" is often criticised for being too violent, most notably during a domestic violence storyline between Little Mo Morgan and her husband Trevor. As "EastEnders" is shown pre-watershed, there were worries that some scenes in this storyline were too graphic for its audience. Complaints against a scene in which Little Mo's face was pushed in gravy on Christmas Day were upheld by the Broadcasting Standards Council. However, a helpline after this episode attracted over 2000 calls. Erin Pizzey, who became internationally famous for having started one of the first Women's Refuges, said that "EastEnders" had done more to raise the issue of violence against women in one story than she had done in 25 years. The character of Phil Mitchell (played by Steve McFadden since early 1990) has been criticised on several occasions for glorifying violence and proving a bad role model to children. On one occasion following a scene in an episode broadcast in October 2002, where Phil brutally beat his godson, Jamie Mitchell (Jack Ryder), 31 complaints came from viewers who watched the scenes.
Originally there was a storyline written that the whole Ferreira family killed their pushy father Dan, but after actor Dalip Tahil could not get a visa for working in the UK the storyline was scrapped and instead Ronny Ferreira got stabbed and survived. This storyline was criticised by many as it seemed rushed and no reason was given for Dan's disappearance.
The BBC was accused of anti-religious bias by a House of Lords committee, who cited "EastEnders" as an example. Dr. Indarjit Singh, editor of the Sikh Messenger and patron of the World Congress of Faiths, said: ""EastEnders"' Dot Cotton is an example. She quotes endlessly from the Bible and it ridicules religion to some extent."
Several cast members have criticised the show. In 2003, Shaun Williamson, who was in the final months of his role of Barry Evans, said that the programme had become much grittier over the past ten to fifteen years, and found it "frightening" that parents let their young children watch. In July 2006, former cast member Tracy-Ann Oberman suggested that the scriptwriters had been "on crack" when they penned the storyline about Den's murder and described her 18 months on the show as being "four years of acting experience". Wendy Richard, who played Pauline Fowler for 21 years, has also claimed that she quit the show because of the producers' decision to remarry her character to Joe Macer (played by Ray Brooks), as she felt this was out of character for Pauline.
The birth of Billy and Honey Mitchell's baby, Janet, diagnosed with Down's syndrome, was criticised by the Royal College of Midwives for being inaccurate and unrealistic. They claim that Honey should not have been refused an epidural and should not have been told about her daughter's condition without her husband being present. They also claim that the baby appeared rigid when in fact she should have been floppy, and that nobody opened the baby's blanket to check. The BBC say a great deal of research was undertaken such as talking to families with children who have Down's syndrome, and liaising with a senior midwife as well as the Down's Syndrome Association. The BBC say Honey was not refused an epidural but had actually locked herself away in the bathroom. They were also unable to cast a baby with Down's syndrome for the first few episodes, which is why the baby appeared rigid. The Down's Syndrome Association say that the way in which Billy and Honey found out about their baby's condition and their subsequent support is not a best practice model, but is still a realistic situation. Conversely, learning disability charity Mencap praised the soap, saying the storyline will help to raise awareness.
In May 2007, it was decided that the ending of a current storyline featuring characters of Dawn Swann, Dr. May Wright and Rob Minter would be substantially rewritten due to the disappearance of toddler Madeleine McCann. The storyline would have seen May ran off with Dawn and Rob's baby shortly after it had been born. The move has attracted some criticism as to how it relates directly to the disappearance of the toddler, but the BBC has defended its actions by stating that "In the current circumstances it was felt any storyline that included a child abduction would be inappropriate and could cause distress to our viewers."
In 2008, the show was criticised for stereotyping their Asian and Black characters, by having a black single mum, Denise Wicks, and an Asian shopkeeper, Zainab Masood.
In 2010, "EastEnders" came under criticism from the police for the way that they were portrayed during the "Who Killed Archie?" storyline. During the storyline, DCI Jill Marsden and DC Wayne Hughes talk to locals about the case and Hughes accepts a bribe. The police claimed that such scenes were "damaging" to their reputation and added that the character DC Deanne Cunningham was "irritatingly inaccurate". In response to the criticism, "EastEnders" apologised for offending real-life detectives and confirmed that they use a police consultant for such storylines. In July 2010, complaints were received following the storyline of Christian minister Lucas Johnson committing a number of murders that he believed was his duty to God, claiming that the storyline was offensive to Christians.
Some storylines have provoked high levels of viewer complaints. In August 2006, a scene involving Carly Wicks (Kellie Shirley) and Jake Moon (Joel Beckett) having sex on the floor of Scarlet nightclub, and another scene involving Owen Turner violently attacking Denise Fox, prompted 129 and 128 complaints, respectively. Carly and Jake's sex scenes were later removed from the Sunday omnibus edition. The showdown of Rob, Dawn and May's storyline where May stated to Dawn she could give her an elective caesarean (Dawn being handcuffed to the bed) prompted 200 complaints. The 2007 domestic abuse storyline involving Ben Mitchell and Stella Crawford attracted sixty complaints from viewers, who found scenes where Ben was attacked by bullies as Stella looked on "upsetting". In March 2008, scenes showing Tanya Branning (Jo Joyner) and boyfriend, Sean Slater (Rob Kazinsky), burying Tanya's husband Max (Jake Wood) alive, attracted many complaints. The UK communications regulator Ofcom later found that the episodes depicting the storyline were in breach of the 2005 Broadcasting Code. They contravened the rules regarding protection of children by appropriate scheduling, appropriate depiction of violence before the 9 p.m. watershed and appropriate depiction of potentially offensive content. In September 2008, "EastEnders" began a grooming and paedophilia storyline involving characters Tony King (Chris Coghill), Whitney Dean (Shona McGarty), Bianca Jackson (Patsy Palmer), Lauren Branning (Madeline Duggan) and Peter Beale (Thomas Law). The storyline attracted over 200 complaints . In April 2009, many viewers complained about the death of Danielle Jones (Lauren Crace), moments after revealing herself to be Ronnie Mitchell's (Samantha Womack) long lost daughter.
In December 2010, Ronnie swapped her newborn baby, who died in cot, with Kat Moon's living baby. Around 3,400 complaints were received, with viewers branding the storyline "insensitive", "irresponsible" and "desperate". Roz Laws from the "Sunday Mercury" called the plot "shocking and ridiculous" and asked "are we really supposed to believe that Kat won't recognise that the baby looks different?" The Foundation for the Study of Infant Deaths (FSID) praised the storyline, and its director Joyce Epstein explained, "We are very grateful to "EastEnders" for their accurate depiction of the devastating effect that the sudden death of an infant can have on a family. We hope that this story will help raise the public's awareness of cot death, which claims 300 babies' lives each year." By 7 January, that storyline had generated the most complaints in show history: the BBC received about 8,500 complaints, and media regulator Ofcom received 374. Despite the controversy however, "EastEnders" pulled in rating highs of 9–10 million throughout the duration of the storyline. A two-minute fight scene in the pub shown in August 2012 received just one complaint to Ofcom from a viewer who felt it was too violent; Ofcom said they would investigate.
In October 2012, a storyline involving Lola Pearce, forced to hand over her baby Lexi, was criticised by the charity The Who Cares? Trust, who called the storyline an "unhelpful portrayal" and said it had already received calls from members of the public who were "distressed about the "EastEnders" scene where a social worker snatches a baby from its mother's arms". The scenes were also condemned by the British Association of Social Workers (BASW), calling the BBC "too lazy and arrogant" to correctly portray the child protection process, and saying that the baby was taken "without sufficient grounds to do so". Bridget Robb, acting chief of the BASW, said the storyline provoked "real anger among a profession well used to a less than accurate public and media perception of their jobs .. EastEnders' shabby portrayal of an entire profession has made a tough job even tougher." 
In October 2014, the BBC defended a storyline, after receiving 278 complaints about the 6 October 2014 episode where pub landlady Linda Carter was raped. On 17 November 2014 it was announced that Ofcom will investigate over the storyline. On 5 January 2015, the investigation was cleared by Ofcom. A spokesman of Ofcom said: "After carefully investigating complaints about this scene, Ofcom found the BBC took appropriate steps to limit offence to viewers. This included a warning before the episode and implying the assault, rather than depicting it. Ofcom also took into account the programme's role in presenting sometimes challenging or distressing social issues." 
There has been criticism that the programme does not authentically portray the ethnic diversity of the population of East London, with the programme being 'Twice as white' as the real east end.
In popular culture.
Since its premiere in 1985, "EastEnders" has had a large impact on British popular culture. It has frequently been referred to in many different media, including songs and television programmes.
Further reading.
Many books have been written about "EastEnders". Notably, from 1985 to 1988, author and television writer Hugh Miller wrote seventeen novels, detailing the lives of many of the show's original characters before 1985, when events on screen took place.
Kate Lock also wrote four novels centred on more recent characters; Steve Owen, Grant Mitchell, Bianca Jackson and Tiffany Mitchell. Lock also wrote a character guide entitled "Who's Who in EastEnders" (ISBN 978-0-563-55178-2) in 2000, examining main characters from the first fifteen years of the show.
Show creators Julia Smith and Tony Holland also wrote a book about the show in 1987, entitled "EastEnders: The Inside Story" (ISBN 978-0-563-20601-9), telling the story of how the show made it to screen. Two special anniversary books have been written about the show; "EastEnders: The First 10 Years: A Celebration" (ISBN 978-0-563-37057-4) by Colin Brake in 1995 and "EastEnders: 20 Years in Albert Square" (ISBN 978-0-563-52165-5) by Rupert Smith in 2005.

</doc>
<doc id="9996" url="https://en.wikipedia.org/wiki?curid=9996" title="Embroidery">
Embroidery

Embroidery is the handicraft of decorating fabric or other materials with needle and thread or yarn. Embroidery may also incorporate other materials such as metal strips, pearls, beads, quills, and sequins. Embroidery is most often used on caps, hats, coats, blankets, dress shirts, denim, stockings, and golf shirts. Embroidery is available with a wide variety of thread or yarn color.
The basic techniques or stitches on surviving examples of the earliest embroidery—chain stitch, buttonhole or blanket stitch, running stitch, satin stitch, cross stitch—remain the fundamental techniques of hand embroidery today.
History.
Origins.
The process used to tailor, patch, mend and reinforce cloth fostered the development of sewing techniques, and the decorative possibilities of sewing led to the art of embroidery. 
Indeed, the remarkable stability of basic embroidery stitches has been noted:
The art of embroidery has been found worldwide and several early examples have been found. Works in China have been dated to the Warring States period (5th–3rd century BC). In a garment from Migration period Sweden, roughly 300–700 AD, the edges of bands of trimming are reinforced with running stitch, back stitch, stem stitch, tailor's buttonhole stitch, and whipstitching, but it is uncertain whether this work simply reinforced the seams or should be interpreted as decorative embroidery.
Applications and Techniques.
Depending on time, location and materials available, embroidery could be the domain of a few experts or a widespread, popular technique. This flexibility lead to a variety of works, from the royal to the mundane.
Elaborately embroidered clothing, religious objects, and household items often were seen as a mark of wealth and status, as in the case of Opus Anglicanum, a technique used by professional workshops and guilds in medieval England. In 18th century England and its colonies, samplers employing fine silks were produced by the daughters of wealthy families. Embroidery was a skill marking a girl's path into womanhood as well as conveying rank and social standing.
Conversely, embroidery is also a folk art, using materials that were accessible to non-professionals. Examples include Hardanger from Norway, Merezhka from Ukraine, Mountmellick embroidery from Ireland, Nakshi kantha from Bangladesh and West Bengal, and Brazilian embroidery. Many techniques had a practical use such as Sashiko from Japan, which was used as a way to reinforce clothing.
The Islamic World.
Embroidery was an important art in the Medieval Islamic world. The 17th century Turkish traveler Evliya Çelebi called it the "craft of the two hands". Because embroidery was a sign of high social status in Muslim societies, it became widely popular. In cities such as Damascus, Cairo and Istanbul, embroidery was visible on handkerchiefs, uniforms, flags, calligraphy, shoes, robes, tunics, horse trappings, slippers, sheaths, pouches, covers, and even on leather belts. Craftsmen embroidered items with gold and silver thread. Embroidery cottage industries, some employing over 800 people, grew to supply these items.
In the 16th century, in the reign of the Mughal Emperor Akbar, his chronicler Abu al-Fazl ibn Mubarak wrote in the famous Ain-i-Akbari:
"His majesty (Akbar) pays much attention to various stuffs; hence Irani, Ottoman, and Mongolian articles of wear are in much abundance especially textiles embroidered in the patterns of "Nakshi", "Saadi", "Chikhan", "Ari", "Zardozi", "Wastli", "Gota" and "Kohra". The imperial workshops in the towns of Lahore, Agra, Fatehpur and Ahmedabad turn out many masterpieces of workmanship in fabrics, and the figures and patterns, knots and variety of fashions which now prevail astonish even the most experienced travelers. Taste for fine material has since become general, and the drapery of embroidered fabrics used at feasts surpasses every description."
Automation.
The development of machine embroidery and its mass production came about in stages in the Industrial Revolution. The earliest machine embroidery used a combination of machine looms and teams of women embroidering the textiles by hand. This was done in France by the mid-1800s. The manufacture of machine-made embroideries in St. Gallen in eastern Switzerland flourished in the latter half of the 19th century.
Classification.
Embroidery can be classified according to whether the design is stitched "on top of" or "through" the foundation fabric, and by the relationship of stitch placement to the fabric.
In free embroidery, designs are applied without regard to the weave of the underlying fabric. Examples include crewel and traditional Chinese and Japanese embroidery.
Counted-thread embroidery patterns are created by making stitches over a predetermined number of threads in the foundation fabric. Counted-thread embroidery is more easily worked on an even-weave foundation fabric such as embroidery canvas, aida cloth, or specially woven cotton and linen fabrics although non-evenweave linen is used as well. Examples include needlepoint and some forms of blackwork embroidery.
In canvas work threads are stitched through a fabric mesh to create a dense pattern that completely covers the foundation fabric. Traditional canvas work such as bargello is a counted-thread technique. Since the 19th century, printed and hand painted canvases, on which the printed or painted image serves as a guide to the placement of the various thread or yarn colors, have eliminated the need for counting threads. These are particularly suited to pictorial rather than geometric designs such as those deriving from the Berlin wool work craze of the early 19th century.
In drawn thread work and cutwork, the foundation fabric is deformed or cut away to create holes that are then embellished with embroidery, often with thread in the same color as the foundation fabric. These techniques are the forerunners of needlelace. When created with white thread on white linen or cotton, this work is collectively referred to as whitework.
Materials.
The fabrics and yarns used in traditional embroidery vary from place to place. Wool, linen, and silk have been in use for thousands of years for both fabric and yarn. Today, embroidery thread is manufactured in cotton, rayon, and novelty yarns as well as in traditional wool, linen, and silk. Ribbon embroidery uses narrow ribbon in silk or silk/organza blend ribbon, most commonly to create floral motifs.
Surface embroidery techniques such as chain stitch and couching or laid-work are the most economical of expensive yarns; couching is generally used for goldwork. Canvas work techniques, in which large amounts of yarn are buried on the back of the work, use more materials but provide a sturdier and more substantial finished textile.
In both canvas work and surface embroidery an embroidery hoop or frame can be used to stretch the material and ensure even stitching tension that prevents pattern distortion. Modern canvas work tends to follow symmetrical counted stitching patterns with designs emerging from the repetition of one or just a few similar stitches in a variety of hues. In contrast, many forms of surface embroidery make use of a wide range of stitching patterns in a single piece of work.
Machine.
Contemporary embroidery is stitched with a computerized embroidery machine using patterns digitized with embroidery software. In machine embroidery, different types of "fills" add texture and design to the finished work. Machine embroidery is used to add logos and monograms to business shirts or jackets, gifts, and team apparel as well as to decorate household linens, draperies, and decorator fabrics that mimic the elaborate hand embroidery of the past.
There has also been a development in free hand machine embroidery, new machines have been designed that allow for the user to create free-motion embroidery which has its place in textile arts, quilting, dressmaking, home furnishings and more.
Qualifications.
City and Guilds qualification in Embroidery allows embroiderers to become recognized for their skill. This qualification also gives them the credibility to teach. For example, the notable textiles artist, Kathleen Laurel Sage- Textiles Artist, began her teaching career by getting the City and Guilds Embroidery 1 and 2 qualifications. She has now gone on to write a book on the subject.

</doc>
<doc id="9997" url="https://en.wikipedia.org/wiki?curid=9997" title="Edward Mitchell Bannister">
Edward Mitchell Bannister

Edward Mitchell Bannister (ca. 1828 – January 9, 1901) was a Black Canadian-American Tonalist painter. Like other Tonalists, his style and predominantly pastoral subject matter were drawn from his admiration for Millet and the French Barbizon School.
Biography.
Bannister was born in St. Andrews, New Brunswick and moved to New England in the late 1840s, where he remained for the rest of his life. While Bannister was well known in the artistic community of his adopted home of Providence, Rhode Island and admired within the wider East Coast art world (he won a bronze medal for his large oil "Under the Oaks" at the 1876 Philadelphia Centennial), he was largely forgotten for almost a century for a complexity of reasons, principally connected with racial prejudice.
With the ascendency of the Civil Rights Movement in the 1970s, his work was again celebrated and collected. In 1978, Rhode Island College dedicated its Art Gallery in Bannister's name with the exhibition: "Four From Providence ~ Alston, Bannister, Jennings & Prophet". This event was attended and commented on by numerous notable political figures of the time, and supported by the Rhode Island Committee for Humanities and the Rhode Island Historical Society. Events like this, across the entire cultural landscape, have ensured that his artwork and life will not be again forgotten.
Although primarily known for his idealised landscapes and seascapes, Bannister also executed portraits, biblical and mythological scenes, and genre scenes. An intellectual autodidact, his tastes in literature were typical of an educated Victorian painter, including Spenser, Virgil, Ruskin and Tennyson, from whose works much of his iconography can be traced.
Bannister died of a heart attack in 1901 while attending a prayer meeting at his church, Elmwood Avenue Free Baptist Church. He is buried in the North Burial Ground in Providence.
Legacy.
The historian Anne Louise Avery is currently compiling the first Catalogue Raisonné and major biography of Bannister's work. See the International Foundation for Art Research for further details.
E. M. Bannister Gallery at Rhode Island College is named after Bannister.
House.
The house at 93 Benevolent Street in Providence, where Bannister lived between 1884 to 1899, is owned by Brown University. Brown plans to renovate the property and restore it to its original appearance.

</doc>
<doc id="10000" url="https://en.wikipedia.org/wiki?curid=10000" title="Eiffel">
Eiffel

Eiffel may refer to:

</doc>
<doc id="10002" url="https://en.wikipedia.org/wiki?curid=10002" title="Emil Kraepelin">
Emil Kraepelin

Emil Kraepelin (; 15 February 1856 – 7 October 1926) was a German psychiatrist. H. J. Eysenck's "Encyclopedia of Psychology" identifies him as the founder of modern scientific psychiatry, as well as of psychopharmacology and psychiatric genetics. Kraepelin believed the chief origin of psychiatric disease to be biological and genetic malfunction. His theories dominated psychiatry at the start of the 20th century and, despite the later psychodynamic influence of Sigmund Freud and his disciples, enjoyed a revival at century's end.
Family and early life.
Kraepelin, whose father, Karl Wilhelm, was a former opera singer, music teacher, and later successful story teller, was born in 1856 in Neustrelitz, in the Duchy of Mecklenburg-Strelitz in Germany. He was first introduced to biology by his brother Karl, 10 years older and, later, the director of the Zoological Museum of Hamburg.
Education and career.
Kraepelin began his medical studies in 1874 at the University of Leipzig and completed them at the University of Würzburg (1877–78). At Leipzig, he studied neuropathology under Paul Flechsig and experimental psychology with Wilhelm Wundt. Kraepelin would be a disciple of Wundt and had a lifelong interest in experimental psychology based on his theories. While there, Kraepelin wrote a prize-winning essay, "The Influence of Acute Illness in the Causation of Mental Disorders". At Würzburg he completed his "Rigorosum" (roughly equivalent to an MBBS viva-voce examination) in March 1878, his "Staatsexamen" (licensing examination) in July 1878, and his "Approbation" (his license to practice medicine; roughly equivalent to an MBBS) on 9 August 1878. From August 1878 to 1882, he worked with Bernhard von Gudden at the University of Munich. Returning to the University of Leipzig in February 1882, he worked in Wilhelm Heinrich Erb's neurology clinic and in Wundt's psychopharmacology laboratory. He completed his "Habilitation" thesis at Leipzig; it was entitled "The Place of Psychology in Psychiatry". On 3 December 1883 he completed his "" (habilitation recognition procedure) at Munich.
Kraepelin's major work, "Compendium der Psychiatrie: Zum Gebrauche für Studirende und Aertze" ("Compendium of Psychiatry: For the Use of Students and Physicians"), was first published in 1883 and was expanded in subsequent multivolume editions to "Ein Lehrbuch der Psychiatrie" ("A Textbook: Foundations of Psychiatry and Neuroscience"). In it, he argued that psychiatry was a branch of medical science and should be investigated by observation and experimentation like the other natural sciences. He called for research into the physical causes of mental illness, and started to establish the foundations of the modern classification system for mental disorders. Kraepelin proposed that by studying case histories and identifying specific disorders, the progression of mental illness could be predicted, after taking into account individual differences in personality and patient age at the onset of disease.
In 1884 he became senior physician in the Prussian provincial town of Leubus, Silesia Province, and the following year he was appointed director of the Treatment and Nursing Institute in Dresden. On 1 July 1886, at the age of 30, Kraepelin was named Professor of Psychiatry at the University of Dorpat (today the University of Tartu) in what is today Estonia (see Burgmair et al., vol. IV). Four years later, on 5 December 1890, he became department head at the University of Heidelberg, where he remained until 1904. While at Dorpat he became the director of the 80-bed University Clinic. There he began to study and record many clinical histories in detail and "was led to consider the importance of the course of the illness with regard to the classification of mental disorders".
In 1903 Kraepelin moved to Munich to become Professor of Clinical Psychiatry at the University of Munich.
He was elected a member of the Royal Swedish Academy of Sciences in 1908.
In 1912 at the request of the German Society of Psychiatry, he began plans to establish a centre for research. Following a large donation from the Jewish German-American banker James Loeb, who had at one time been a patient, and promises of support from "patrons of science", the German Institute for Psychiatric Research was founded in 1917 in Munich. Initially housed in existing hospital buildings, it was maintained by further donations from Loeb and his relatives. In 1924 it came under the auspices of the Kaiser Wilhelm Society for the Advancement of Science. The German American Rockefeller family's Rockefeller Foundation made a large donation enabling the development of a new dedicated building for the institute, along Kraepelin's guidelines, which was officially opened in 1928.
Kraepelin spoke out against the barbarous treatment that was prevalent in the psychiatric asylums of the time, and crusaded against alcohol, capital punishment and the imprisonment rather than treatment of the insane. He rejected psychoanalytical theories that posited innate or early sexuality as the cause of mental illness, and he rejected philosophical speculation as unscientific. He focused on collecting clinical data and was particularly interested in neuropathology (e.g., diseased tissue).
In the later period of his career, as a convinced champion of social Darwinism, he actively promoted a policy and research agenda in racial hygiene and eugenics.
Kraepelin retired from teaching at the age of 66, spending his remaining years establishing the Institute. The ninth and final edition of his "Textbook" was published in 1927, shortly after his death. It comprised four volumes and was ten times larger than the first edition of 1883.
Theories and classification schemes.
Kraepelin announced that he had found a new way of looking at mental illness, referring to the traditional view as "symptomatic" and to his view as "clinical". This turned out to be his paradigm-setting synthesis of the hundreds of mental disorders classified by the 19th century, grouping diseases together based on classification of syndrome—common "patterns" of symptoms over time—rather than by simple similarity of major symptoms in the manner of his predecessors. Kraepelin described his work in the 5th edition of his textbook as a "decisive step from a symptomatic to a clinical view of insanity. . . . The importance of external clinical signs has . . . been subordinated to consideration of the conditions of origin, the course, and the terminus which result from individual disorders. Thus, all purely symptomatic categories have disappeared from the nosology".
Psychosis and mood.
Kraepelin is specifically credited with the classification of what was previously considered to be a unitary concept of psychosis, into two distinct forms (known as the Kraepelinian dichotomy):
Drawing on his long-term research, and using the criteria of course, outcome and prognosis, he developed the concept of dementia praecox, which he defined as the "sub-acute development of a peculiar simple condition of mental weakness occurring at a youthful age". When he first introduced this concept as a diagnostic entity in the fourth German edition of his "Lehrbuch der Psychiatrie" in 1893, it was placed among the degenerative disorders alongside, but separate from, catatonia and dementia paranoides. At that time, the concept corresponded by and large with Ewald Hecker's hebephrenia. In the sixth edition of the "Lehrbuch" in 1899 all three of these clinical types are treated as different expressions of one disease, dementia praecox.
One of the cardinal principles of his method was the recognition that any given symptom may appear in virtually any one of these disorders; e.g., there is almost no single symptom occurring in dementia praecox which cannot sometimes be found in manic depression. What distinguishes each disease symptomatically (as opposed to the underlying pathology) is not any particular (pathognomonic) symptom or symptoms, but a specific pattern of symptoms. In the absence of a direct physiological or genetic test or marker for each disease, it is only possible to distinguish them by their specific pattern of symptoms. Thus, Kraepelin's system is a method for pattern recognition, not grouping by common symptoms.
Kraepelin also demonstrated specific patterns in the genetics of these disorders and specific and characteristic patterns in their course and outcome. Generally speaking, there tend to be more schizophrenics among the relatives of schizophrenic patients than in the general population, while manic depression is more frequent in the relatives of manic depressives. Though, of course, this does not demonstrate genetic linkage, as this might be a socio-environmental factor as well.
He also reported a pattern to the course and outcome of these conditions. Kraepelin believed that schizophrenia had a deteriorating course in which mental function continuously (although perhaps erratically) declines, while manic-depressive patients experienced a course of illness which was intermittent, where patients were relatively symptom-free during the intervals which separate acute episodes. This led Kraepelin to name what we now know as schizophrenia, dementia praecox (the dementia part signifying the irreversible mental decline). It later became clear that dementia praecox did not necessarily lead to mental decline and was thus renamed schizophrenia by Eugen Bleuler to correct Kraepelin's misnomer.
In addition, as Kraepelin accepted in 1920, "It is becoming increasingly obvious that we cannot satisfactorily distinguish these two diseases"; however, he maintained that "On the one hand we find those patients with irreversible dementia and severe cortical lesions. On the other are those patients whose personality remains intact". Nevertheless, overlap between the diagnoses and neurological abnormalities (when found) have continued, and in fact a diagnostic category of schizoaffective disorder would be brought in to cover the intermediate cases.
Kraepelin devoted very few pages to his speculations about the etiology of his two major insanities, dementia praecox and manic-depressive insanity. However, from 1896 to his death in 1926 he held to the speculation that these insanities (particularly dementia praecox) would one day probably be found to be caused by a gradual systemic or "whole body" disease process, probably metabolic, which affected many of the organs and nerves in the body but affected the brain in a final, decisive cascade.
Psychopathic personalities.
In the first through sixth edition of Kraepelin's influential psychiatry textbook, there was a section on moral insanity, which meant then a disorder of the emotions or moral sense without apparent delusions or hallucinations, and which Kraepelin defined as "lack or weakness of those sentiments which counter the ruthless satisfaction of egotism". He attributed this mainly to degeneration. This has been described as a psychiatric redefinition of Cesare Lombroso's theories of the "born criminal", conceptualised as a "moral defect", though Kraepelin stressed it was not yet possible to recognise them by physical characteristics.
In fact from 1904 Kraepelin changed the section heading to "The born criminal", moving it from under "Congenital feeble-mindedness" to a new chapter on "Psychopathic personalities". They were treated under a theory of degeneration. Four types were distinguished: born criminals (inborn delinquents), pathological liars, querulous persons, and Triebmenschen (persons driven by a basic compulsion, including vagabonds, spendthrifts, and dipsomaniacs). The concept of "psychopathic inferiorities" had been recently popularised in Germany by Julius Ludwig August Koch, who proposed congenital and acquired types. Kraepelin had no evidence or explanation suggesting a congenital cause, and his assumption therefore appears to have been simple "biologism". Others, such as Gustav Aschaffenburg, argued for a varying combination of causes. Kraepelin's assumption of a moral defect rather than a positive drive towards crime has also been questioned, as it implies that the moral sense is somehow inborn and unvarying, yet it was known to vary by time and place, and Kraepelin never considered that the moral sense might just be different. Kurt Schneider criticized Kraepelin's nosology for appearing to be a list of behaviors that he considered undesirable, rather than medical conditions, though Schneider's alternative version has also been criticised on the same basis. Nevertheless, many essentials of these diagnostic systems were introduced into the diagnostic systems, and remarkable similarities remain in the DSM-IV and ICD-10. The issues would today mainly be considered under the category of personality disorders, or in terms of Kraepelin's focus on psychopathy.
Kraepelin had referred to psychopathic conditions (or "states") in his 1896 edition, including compulsive insanity, impulsive insanity, homosexuality, and mood disturbances. From 1904, however, he instead termed those "original disease conditions, and introduced the new alternative category of psychopathic personalities. In the eighth edition from 1909 that category would include, in addition to a separate "dissocial" type, the excitable, the unstable, the Triebmenschen driven persons, eccentrics, the liars and swindlers, and the quarrelsome. It has been described as remarkable that Kraepelin now considered mood disturbances to be not part of the same category, but only attenuated (more mild) phases of manic depressive illness; this corresponds to current classification schemes.
Alzheimer's disease.
Kraepelin postulated that there is a specific brain or other biological pathology underlying each of the major psychiatric disorders. As a colleague of Alois Alzheimer, he was a co-discoverer of Alzheimer's disease, and his laboratory discovered its pathological basis. Kraepelin was confident that it would someday be possible to identify the pathological basis of each of the major psychiatric disorders.
Degeneration of the race.
Upon moving to become Professor of Clinical Psychiatry at the University of Munich in 1903, Kraepelin increasingly wrote on social policy issues. He was a strong and influential proponent of eugenics and racial hygiene. His publications included a focus on alcoholism, crime, degeneration and hysteria. Kraepelin was convinced that such institutions as the education system and the welfare state, because of their trend to break the processes of natural selection, undermined the Germans’ biological "struggle for survival". He was concerned to preserve and enhance the German people, the Volk, in the sense of nation or race. He appears to have held Lamarckian concepts of evolution, such that cultural deterioration could be inherited. He was a strong ally and promoter of the work of fellow psychiatrist (and pupil and later successor as director of the clinic) Ernst Rudin to clarify the mechanisms of genetic inheritance as to make a so-called "empirical genetic prognosis".
Martin Brune has pointed out that Kraepelin and Rudin also appear to have been ardent advocates of a self-domestication theory, a version of social Darwinism which held that modern culture was not allowing people to be weeded out, resulting in more mental disorder and deterioration of the gene pool. Kraepelin saw a number of "symptoms" of this, such as "weakening of viability and resistance, decreasing fertility, proletarianisation, and moral damage due to "penning up people" ["Zusammenpferchung"]. He also wrote that "the number of idiots, epileptics, psychopaths, criminals, prostitutes, and tramps who descend from alcoholic and syphilitic parents, and who transfer their inferiority to their offspring, is incalculable". He felt that "the well-known example of the Jews, with their strong disposition towards nervous and mental disorders, teaches us that their extraordinarily advanced domestication may eventually imprint clear marks on the race". Brune states that Kraepelin's nosological system was "to a great deal, built on the degeneration paradigm".
Influence.
Kraepelin's great contribution in classifying schizophrenia and manic depression remains relatively unknown to the general public, and his work, which had neither the literary quality nor paradigmatic power of Freud's, is little read outside scholarly circles. Kraepelin's contributions were also to a large extent marginalized throughout a good part of the 20th century during the success of Freudian etiological theories. However, his views now dominate many quarters of psychiatric research and academic psychiatry. His fundamental theories on the diagnosis of psychiatric disorders form the basis of the major diagnostic systems in use today, especially the American Psychiatric Association's DSM-IV and the World Health Organization's ICD system, based on the Research Diagnostic Criteria and earlier Feighner Criteria developed by espoused "neo-Kraepelinians", though Robert Spitzer and others in the DSM committees were keen not to include assumptions about causation as Kraepelin had.
Kraepelin has been described as a "scientific manager" and political operator, who developed a large-scale, clinically oriented, epidemiological research programme. In this role he took in clinical information from a wide range of sources and networks. Despite proclaiming high clinical standards for himself to gather information "by means of expert analysis of individual cases", he would also draw on the reported observations of officials not trained in psychiatry. The various editions of his textbooks do not contain detailed case histories of individuals, however, but mosaiclike compilations of typical statements and behaviors from patients with a specific diagnosis. In broader terms, he has been described as a bourgeois or reactionary citizen.
Kraepelin wrote in a "knapp und klar" (concise and clear) style that made his books useful tools for physicians. Abridged and clumsy English translations of the sixth and seventh editions of his textbook in 1902 and 1907 (respectively) by Allan Ross Diefendorf (1871–1943), an assistant physician at the Connecticut Hospital for the Insane at Middletown, inadequately conveyed the literary quality of his writings that made them so valuable to practitioners.
Dreams.
In the Heidelberg and early Munich years he edited "Psychologische Arbeiten", a journal on experimental psychology. One of his own famous contributions to this journal also appeared in the form of a monograph (105 pp.) entitled "Über Sprachstörungen im Traume" ("On Language Disturbances in Dreams"). Kraepelin, on the basis on the dream-psychosis analogy, studied for more than 20 years language disorder in dreams in order to study indirectly schizophasia. The dreams Kraepelin collected are mainly his own. They lack extensive comment by the dreamer. In order to study them the full range of biographical knowledge available today on Kraepelin is necessary (see, e.g., Burgmair et al., I-VII).
External links.
For biographies of Kraepelin see:
For English translations of Kraepelin's work see:

</doc>
<doc id="10003" url="https://en.wikipedia.org/wiki?curid=10003" title="Evoluon">
Evoluon

The Evoluon is a conference centre and former science museum erected by the electronics and electrical company Philips at Eindhoven in the Netherlands in 1966. Since its construction, it has become a landmark and a symbol for the city.
The building is unique due to its very futuristic design, resembling a landed flying saucer. It was designed by architects Leo de Bever and Louis Christiaan Kalff, while the exhibition it housed was conceived by James Gardner. De Bever and Kalff only got two demands for the design of the building, it had to be "spectacular" and it had to be possible to hold exhibitions in the building.
Its concrete dome is in diameter and is held in place by of reinforcing steel bars.
In the 1960s and 1970s the Evoluon attracted large visitor numbers, since its interactive exhibitions were a new and unique concept in the Netherlands at that time. But when competing science museums opened in other cities, the number of visitors began to decline. After several years of losing money, the original museum closed down in 1989 and the Evoluon was converted into a conference centre, opening in 1998.
In the UK the Evoluon is chiefly remembered from Bert Haanstra's wordless short film entitled simply "Evoluon", commissioned by Philips to publicise the museum, and shown as a trade test colour film on BBC television from 1968 to 1972.
In October 2013 the Evoluon was used to stage four 3D-concerts by the German electronic band Kraftwerk, each before an audience of 1,200 spectators. Key band member Ralf Hütter handpicked the venue for its retro-futuristic look. Bespoke 3D-visuals of the saucer section of the building descending from space were used in the live rendition of their track "Spacelab".

</doc>
<doc id="10004" url="https://en.wikipedia.org/wiki?curid=10004" title="Educational essentialism">
Educational essentialism

Educational essentialism is an educational philosophy whose adherents believe that children should learn the traditional basic subjects thoroughly. In this philosophical school of thought, the aim is to instill students with the "essentials" of academic knowledge, enacting a back-to-basics approach. Essentialism ensures that the accumulated wisdom of our civilization as taught in the traditional academic disciplines is passed on from teacher to student. Such disciplines might include Reading, Writing, Literature, Foreign Languages, History, Mathematics, Science, Art, and Music. Moreover, this traditional approach is meant to train the mind, promote reasoning, and ensure a common culture.
Principles of essentialism.
Essentialism is a relatively conservative stance to education that strives to teach students the knowledge of our society and civilization through a core curriculum. This core curriculum involves such areas that include the study of the surrounding environment, basic natural laws, and the disciplines that promote a happier, more educated living. Other non-traditional areas are also integrated as well in moderation to balance the education. Essentialists' goals are to instill students with the "essentials" of academic knowledge, patriotism, and character development through traditional (or back-to-basic) approaches. This is to promote reasoning, train the mind, and ensure a common culture for all Americans.
Essentialism is the most typically enacted philosophy in American classrooms today. Traces of this can be found in the organized learning centered on teacher and textbooks, in addition to the regular assignments and evaluations typical in essentialist education.
Essentialism as a teacher-centered philosophy.
The role of the teacher as the leader of the classroom is a very important tenet of Educational essentialism. The teacher is the center of the classroom, so they should be rigid and disciplinary. Establishing order in the classroom is crucial for student learning; effective teaching cannot take place in a loud and disorganized environment. It is the teacher's responsibility to keep order in the classroom. The teacher must interpret essentials of the learning process, take the leadership position and set the tone of the classroom. These needs require an educator who is academically well-qualified with an appreciation for learning and development. The teacher must control the students with distributions of rewards and penalties.
History of essentialism.
The Essentialist movement first began in the United States in the year 1938. In Atlantic City, New Jersey, a group met for the first time called "The Essentialist's Committee for the Advancement of Education." Their emphasis was to reform the educational system to a rational-based system.
The term essentialist first appeared in the book "An Introduction to the Philosophy of Education" which was written by Michael John Demiashkevich. In his book, Demiashkevich labels some specific educators (including William C. Bagley) as “essentialists." Demiashkevich compared the essentialists to the different viewpoints of the Progressive Education Association. He described how the Progressives preached a “hedonistic doctrine of change” whereas the essentialists stressed the moral responsibility of man for his actions and looked toward permanent principles of behavior (Demiashkevich likened the arguments to those between the Socratics and the Sophists in Greek philosophy). In 1938 Bagley and other educators met together where Bagley gave a speech detailing the main points of the essentialism movement and attacking the public education in the United States. One point that Bagley noted was that students in the U.S. were not getting an education on the same levels as students in Europe who were the same age.
A recent branch has emerged within the essentialist school of thought called "neoessentialism." Emerging in the eighties as a response to the essentialist ideals of the thirties as well as to the criticism of the fifties and the advocates for education in the seventies, neoessentialism was created to try to appease the problems facing the United States at the time. The most notable change within this school of thought is that it called for the creation of a new discipline, computer science.
Renowned essentialists.
William Bagley (1874–1946) was an important historical essentialist. William C. Bagley completed his undergraduate degree at Michigan Agricultural College in 1895. It wasn’t until after finishing his undergrad studies that he truly wanted to be a teacher. Bagley did his Graduate studies at the University of Chicago and at Cornell University. He acquired his Ph.D. in 1900 after which he took his first school job a Principal in a St. Louis, Missouri Elementary School. Bagley’s devotion increased during his work at Montana State Normal School in Dillon, Montana. It was here where he decided to dedicate his time to the education of teachers and where he published "The Educative Process", launching his name across the nation. Throughout his career Bagley argued against the conservative position that teachers were not in need of special training for their work. He believed that liberal arts material was important in teacher education. Bagley also believed the dominant theories of education of the time were weak and lacking.
In April 1938, he published the "Essentialist's Platform", in which he outlined three major points of essentialism. He described the right of students to a well-educated and culturally knowledgeable teacher. Secondly, he discussed the importance of teaching the ideals of community to each group of students. Lastly, Bagley wrote of the importance of accuracy, thoroughness and effort on part of the student in the classroom.
Another important essentialist is E. D. Hirsch (1928-). Hirsch was Founder and Chairman of the Core of Knowledge Foundation and author to several books concerning fact-based approaches to education. Now retired, he spent many years teaching at the University of Virginia while also being an advocate for the "back to basics" movement. In his most popular book, "Cultural Literacy — What Every American Needs To Know", he offers lists, quotations, and information regarding what he believes is essential knowledge.
See also Arthur Bestor.
Schools enacting an essentialist curriculum.
The Core Knowledge Schools were founded on the philosophy of essentialist E.D. Hirsch. Although it is difficult to maintain a pure and strict essentialist-only curriculum, these schools have the central aim of establishing a common knowledge base for all citizens. To do so, they follow a nationwide, content-specific, and teacher-centered curriculum. The Core Knowledge curriculum also allows for local variance above and beyond the core curriculum. Central curricular aims are academic excellence and the learning of knowledge, and teachers who are masters of their knowledge areas serve this aim.
Criticism of essentialism.
However, because Essentialism is largely teacher-centered, the role of the student is often called into question. Presumably, in an essentialist classroom, the teacher is the one designing the curriculum for the students based upon the core disciplines. Moreover, he or she is enacting the curriculum and setting the standards to which the students must meet. The teacher's evaluation role undermines students' interest in study. As a result, the students begin to take on more of a passive role in their education as they are forced to meet and learn such standards and information.
Furthermore, there is also speculation that an essentialist education helps in promoting the cultural lag. This philosophy of education is very traditional in the mindset of passing on the knowledge of the culture via the academic disciplines. Thus, students are forced to think in the mindset of the larger culture, and individual creativity is often squelched.

</doc>
<doc id="10005" url="https://en.wikipedia.org/wiki?curid=10005" title="Progressive education">
Progressive education

Progressive education is a pedagogical movement that began in the late nineteenth century; it has persisted in various forms to the present. The term "progressive" was engaged to distinguish this education from the traditional Euro-American curricula of the 19th century, which was rooted in classical preparation for the university and strongly differentiated by social class. By contrast, progressive education finds its roots in present experience. Most progressive education programs have these qualities in common: 
Educational theory.
Progressive education can be traced back to the works of John Locke and Jean-Jacques Rousseau, both of whom are known as forerunners of ideas that would be developed by theorists such as Dewey. Locke believed that "truth and knowledge… arise out of observation and experience rather than manipulation of accepted or given ideas". He further discussed the need for children to have concrete experiences in order to learn. Rousseau deepened this line of thinking in Emile, or On Education, where he argued that subordination of students to teachers and memorization of facts would not lead to an education.
Johann Bernhard Basedow.
In Germany, Johann Bernhard Basedow (1724-1790) established the Philanthropinum at Dessau in 1774. He developed new teaching methods based on conversation and play with the child, and a program of physical development. Such was his success that he wrote a treatise on his methods, "On the best and hitherto unknown method of teaching children of noblemen".
Christian Gotthilf Salzmann.
Christian Gotthilf Salzmann (1744–1811) was the founder of the Schnepfenthal institution, a school dedicated to new modes of education (derived heavily from the ideas of Jean-Jacques Rousseau). He wrote "Elements of Morality, for the Use of Children", one of the first books translated into English by Mary Wollstonecraft.
Johann Heinrich Pestalozzi.
Johann Heinrich Pestalozzi (1746–1827) was a Swiss pedagogue and educational reformer who exemplified Romanticism in his approach. He founded several educational institutions both in German- and French-speaking regions of Switzerland and wrote many works explaining his revolutionary modern principles of education. His motto was "Learning by head, hand and heart". His research and theories closely resemble those outlined by Rousseau in Emile. He is further considered by many to be the “father of modern educational science” His psychological theories pertain to education as they focus on the development of object teaching, that is, he felt that individuals best learned through experiences and through a direct manipulation and experience of objects. He further speculated that children learn through their own internal motivation rather than through compulsion. (See Intrinsic vs. Extrinsic motivation). A teacher's task will be to help guide their students as individuals through their learning and allow it to unfold naturally.
Friedrich Fröbel.
Friedrich Wilhelm August Fröbel (1782 – 1852) was a student of Pestalozzi who laid the foundation for modern education based on the recognition that children have unique needs and capabilities. He believed in “self-activity” and play as essential factors in child education. The teacher’s role was not to indoctrinate but to encourage self-expression through play, both individually and in group activities. He created the concept of kindergarten.
Johann Friedrich Herbart.
Johann Friedrich Herbart (1776–1841) emphasized the connection between individual development and the resulting societal contribution. The five key ideas which composed his concept of individual maturation were Inner Freedom, Perfection, Benevolence, Justice, and Equity or Recompense. According to Herbart, abilities were not innate but could be instilled, so a thorough education could provide the framework for moral and intellectual development. In order to develop a child to lead to a consciousness of social responsibility, Herbart advocated that teachers utilize a methodology with five formal steps: “Using this structure a teacher prepared a topic of interest to the children, presented that topic, and questioned them inductively, so that they reached new knowledge based on what they had already known, looked back, and deductively summed up the lesson’s achievements, then related them to moral precepts for daily living”.
John Melchior Bosco.
John Melchior Bosco (1815 – 1888) was concerned about the education of street children who had left their villages to find work in the rapidly industrialized city of Turin, Italy. Exploited as cheap labor or imprisoned for unruly behavior, Bosco saw the need of creating a space where they would feel at home. He called it an 'Oratory' where they could play, learn, share friendships, express themselves, develop their creative talents and pick up skills for gainful self-employment. With those who had found work, he set up a mutual-fund society (an early version of the Grameen Bank) to teach them the benefits of saving and self-reliance. The principles underlying his educational method that won over the hearts and minds of thousands of youth who flocked to his oratory were: 'be reasonable', 'be kind', 'believe' and 'be generous in service'. Today his method of education is practiced in nearly 3000 institutions set up around the world by the members of the Salesian Society he founded in 1873.
Cecil Reddie.
While studying for his doctorate in Göttingen in 1882-1883, Cecil Reddie was greatly impressed by the progressive educational theories being applied there. Reddie founded Abbotsholme School in Derbyshire, England in 1889. Its curriculum enacted the ideas of progressive education. Reddie rejected rote learning, classical languages and corporal punishment. He combined studies in modern languages and the sciences and arts with a program of physical exercise, manual labour, recreation, crafts and arts. Abbotsholme was imitated throughout Europe and was particularly influential in Germany. He often engaged foreign teachers, who learned its practices, before returning home to start their own schools. Hermann Lietz an Abbotsholme teacher founded five schools (Landerziehungsheime für Jungen) on Abbotsholme's principles. Other people he influenced included Kurt Hahn, Adolphe Ferrière and Edmond Demolins. His ideas also reached Japan, where it turned into “Taisho-era Free Education Movement” (Taisho Jiyu Kyoiku Undo)
John Dewey.
In the United States the "Progressive Education Movement", starting in the 1880s and lasting for sixty years, helped boost American public schools from a budding idea to the regular norm. John Dewey, a principal figure in this movement from the 1880s to 1904, set the tone for educational philosophy as well as concrete school reforms. His thinking had been influenced by the ideas of Fröbel and Herbart. His reactions to the prevailing theories and practices in education, corrections made to these philosophies, and recommendations to teachers and administrators to embrace “the new education,” provide a vital account of the history of the development of educational thinking in the late nineteenth and early twentieth centuries. Dewey placed so called pragmatism above moral absolutes and helped give rise to situational ethics. Beginning in 1897 John Dewey published a summary of his theory on progressive education in School Journal. His theoretical standpoints are divided into five sections outlined below.
What Education Is.
Education according to Dewey is the “participation of the individual in the social consciousness of the race” (Dewey, 1897, para. 1). As such, education should take into account that the student is a social being. The process begins at birth with the child unconsciously gaining knowledge and gradually developing their knowledge to share and partake in society.
The educational process has two sides, the psychological and the sociological, with the psychological forming the basis. (Dewey, 1897). A child’s own instincts will help develop the material that is presented to them. These instincts also form the basis of their knowledge with everything building upon it. This forms the basis of Dewey’s assumption that one cannot learn without motivation.
Knowledge is a social condition and it is important to help students construct their own learning, as stated: “Hence it is impossible to prepare the child for any precise set of conditions. To prepare him for the future life means to give him command of himself; it means so to train him that he will have the full and ready use of all his capacities; that his eye and ear and hand may be tools ready to command, that his judgment may be capable of grasping the conditions under which it has to work, and the executive forces be trained to act economically and efficiently” (Dewey, 1897, Para. 7)
Instruction must focus on the child as a whole for you can never be sure as to where society may end or where that student will be needed or will take themselves.
What the school is.
“Education fails because it neglects this fundamental principle of the school as a form of community life. It conceives the school as a place where certain information is to be given, where certain lessons are to be learned, or where certain habits are to be formed” (Dewey, 1897, para. 17) Dewey felt that as education is a social construct, it is therefore a part of society and should reflect the community.
"Education is the process of living and is not meant to be the preparation of future living," (Dewey, 1897), so school must represent the present life. As such, parts of the student’s home life (such as moral and ethical education) should take part in the schooling process. The teacher is a part of this, not as an authoritative figure, but as a member of the community who is there to assist the student.
The subject matter of education.
According to Dewey, the curriculum in the schools should reflect that of society. The center of the school curriculum should reflect the development of humans in society. The study of the core subjects (language, science, history) should be coupled with the study of cooking, sewing and manual training. Furthermore, he feels that “progress is not in the succession of studies but in the development of new attitudes towards, and new interests in, experience” (Dewey, 1897, para. 38)
The nature of method.
Method is focused on the child’s powers and interests. If the child is thrown into a passive role as a student, absorbing information, the result is a waste of the child’s education. (Dewey, 1897). Information presented to the student will be transformed into new forms, images and symbols by the student so that they fit with their development and interests. The development of this is natural. To repress this process and attempt to “substitute the adult for the child” (Dewey, 1897, para. 52) would weaken the intellectual curiosity of the child.
The school and social progress.
Education is the most fundamental method of social reconstruction for progress and reform. Dewey believes that “education is a regulation of the process of coming to share in the social consciousness; and that the adjustment of individual activity on the basis of this social consciousness is the only sure method of social reconstruction” (Dewey, 1897, para. 60). As such, Dewey gives way to Social Reconstruction and schools as means to reconstruct society (See Social Reconstruction in Education). Finally, as schools become a means for social reconstruction, our educations must be given the proper equipment to help perform this task and guide their students.
Laborschule Bielefeld was founded in 1974 in Germany. The Laborschule explicitly uses democratic concepts as suggested by Dewey. Studies in the last years have proven the successful implementation of these concepts into a living community.
Rudolf Steiner.
Rudolf Steiner (1869-1925) first described the principles of what was to become Waldorf education in 1907. He established a series of schools based on these principles beginning in 1919. The focus of the education is on creating a developmentally-appropriate curriculum that holistically integrates practical, artistic, social, and academic experiences. There are more than a thousand schools and many more early childhood centers worldwide; it has also become a popular form of homeschooling.
Maria Montessori.
Maria Montessori (1870-1952) began to develop her philosophy and methods in 1897. She based her work on her observations of children and experimentation with the environment, materials, and lessons available to them. She frequently referred to her work as "scientific pedagogy". Although Montessori education spread to the United States in 1911 there were conflicts with the American educational establishment and was opposed by William Heard Kilpatrick. However Montessori education returned to the United States in 1960 and has since spread to thousands of schools there.
Robert Baden-Powell.
In July 1906, Ernest Thompson Seton sent Robert Baden-Powell a copy of his book "The Birchbark Roll of the Woodcraft Indians". Seton was a British-born Canadian-American living in the United States. They shared ideas about youth training programs. In 1907 Baden-Powell wrote a draft called "Boy Patrols". In the same year, to test his ideas, he gathered 21 boys of mixed social backgrounds and held a week-long camp in August on Brownsea Island in England. His organizational method, now known as the Patrol System and a key part of Scouting training, allowed the boys to organize themselves into small groups with an elected patrol leader. Baden Powell then wrote "Scouting for Boys" (London, 1908). The Brownsea camp and the publication of "Scouting for Boys" are generally regarded as the start of the Scout movement which spread throughout the world. Baden-Powell and his sister Agnes Baden-Powell introduced the Girl Guides in 1910.
Comparison with traditional education.
Traditional education uses extrinsic motivation, such as grades and prizes. Progressive education is more likely to use intrinsic motivation, basing activities on the interests of the child. Praise may be discouraged as a motivator.
21st century skills.
21st century skills are a series of higher-order skills, abilities, and learning dispositions that have been identified as being required for success in the rapidly changing, digital society and workplaces. Many of these skills are also defining qualities of "progressive education" as well as being associated with deeper learning, which is based on mastering skills such as analytic reasoning, complex problem solving, and teamwork. These skills differ from traditional academic skills in that they are not primarily content knowledge-based.
Progressive education in the West.
France.
Edmond Demolins was inspired by Abbotsolme and Bedales to found the Ecole des Roches in Verneuil-sur-Avre in 1899. Paul Robin implemented progressive principles at the orphanage at Cempuis from 1880 to 1894. This was the first French mixed school, and a scandal at that time. Sebastien Faure in 1904 created a libertarian school 'La Ruche' (the Hive).
Germany.
Hermann Lietz founded three Landerziehungsheime (country boarding schools) in 1904 based on Reddie’s model for boys of different ages. Lietz eventually succeeded in establishing five more Landerziehungsheime. Edith and Paul Geheeb founded Odenwaldschule in Heppenheim in the Odenwald in 1910 using their concept of progressive education, which integrated the work of the head and hand.
Poland.
Janusz Korczak was one notable follower and developer of Pestalozzi's ideas. He wrote
"The names of Pestalozzi, Froebel and Spencer shine with no less brilliance than the names of the greatest inventors of the twentieth century. For they discovered more than the unknown forces of nature; they discovered the unknown half of humanity: children." His Orphan’s Home in Warsaw became a model institution and exerted influence on the educational process in other orphanages of the same type.
Spain.
In Spain, the Escuela Moderna was founded in 1901 by Francisco Ferrer, a Catalan educator and anarchist. He had been influenced by Cecil Reddie. The Modern Schools, also called 'Ferrer Schools', that were founded in the United States, were based on Escuela Moderna. As in Spain the schools were intended to educate the working-classes from a secular, class-conscious perspective. The Modern Schools imparted day-time academic classes for children, and night-time continuing-education lectures for adults.
United Kingdom.
The ideas from Reddie's Abbotsholme spread to schools such as Bedales School (1893), King Alfred School, London (1898) and St Christopher School, Letchworth (1915), as well as all the Friends’ schools, Steiner Waldorf schools and those belonging to the Round Square Conference. The King Alfred School was radical for its time in that it provided a secular education and that boys and girls were educated together. Alexander Sutherland Neill believed children should achieve self-determination and should be encouraged to think critically rather than blindly obeying. He implemented his ideas with the founding of Summerhill School in 1921. Neill believed that children learn better when they are not compelled to attend lessons. The school was also managed democratically, with regular meetings to determine school rules. Pupils had equal voting rights with school staff.
United States.
Early practitioners.
Fröbel’s student Margarethe Schurz founded the first kindergarten in the United States at Watertown, Wisconsin in 1856, and she also inspired Elizabeth Peabody, who went on to found the first English-speaking kindergarten in the United States – the language at Schurz’s kindergarten had been German, to serve an immigrant community – in Boston in 1860. This paved the way for the concept’s spread in the USA. The German émigré Adolph Douai had also founded a kindergarten in Boston in 1859, but was obliged to close it after only a year. By 1866, however, he was founding others in New York City.
William Heard Kilpatrick (1871–1965) was a pupil of Dewey and one of the most effective practitioners of the concept as well as the more adept at proliferating the progressive education movement and spreading word of the works of Dewey. He is especially well known for his “project method of teaching”. This developed the progressive education notion that students were to be engaged and taught so that their knowledge may be directed to society for a socially useful need. Like Dewey he also felt that students should be actively engaged in their learning rather than actively disengaged with the simple reading and regurgitation of material.
The most famous early practitioner of progressive education was Francis Parker; its best-known spokesperson was the philosopher John Dewey. In 1875 Francis Parker became superintendent of schools in Quincy, Massachusetts after spending two years in Germany studying emerging educational trends on the continent. Parker was opposed to rote learning, believing that there was no value in knowledge without understanding. He argued instead schools should encourage and respect the child’s creativity. Parker’s Quincy System called for child-centered and experience-based learning. He replaced the traditional curriculum with integrated learning units based on core themes related to the knowledge of different disciplines. He replaced traditional readers, spellers and grammar books with children’s own writing, literature, and teacher prepared materials. In 1883 Parker left Massachusetts to become Principal of the Cook County Normal School in Chicago, a school that also served to train teachers in Parker’s methods. In 1894 Parker’s Talks on Pedagogics, which drew heavily on the thinking of Fröbel, Pestalozzi and Herbart, became one of the first American writings on education to gain international fame.
That same year, philosopher John Dewey moved from the University of Michigan to the newly established University of Chicago where he became chair of the department of philosophy, psychology and education. He and his wife enrolled their children in Parker’s school before founding their own school two years later.
Whereas Parker started with practice and then moved to theory, Dewey began with hypotheses and then devised methods and curricula to test them. By the time Dewey moved to Chicago at the age of thirty-five, he had already published two books on psychology and applied psychology. He had become dissatisfied with philosophy as pure speculation and was seeking ways to make philosophy directly relevant to practical issues. Moving away from an early interest in Hegel, Dewey proceeded to reject all forms of dualism and dichotomy in favor of a philosophy of experience as a series of unified wholes in which everything can be ultimately related.
In 1896, John Dewey opened what he called the laboratory school to test his theories and their sociological implications. With Dewey as the director and his wife as principal, the University of Chicago Laboratory school, was dedicated “to discover in administration, selection of subject-matter, methods of learning, teaching, and discipline, how a school could become a cooperative community while developing in individuals their own capacities and satisfy their own needs.” (Cremin, 136) For Dewey the two key goals of developing a cooperative community and developing individuals’ own capacities were not at odds; they were necessary to each other. This unity of purpose lies at the heart of the progressive education philosophy. In 1912, Dewey sent out students of his philosophy to found The Park School of Buffalo and The Park School of Baltimore to put it into practice. These schools operate to this day within a similar progressive approach.
At Columbia, Dewey worked with other educators such as Charles Eliot and Abraham Flexner to help bring progressivism into the mainstream of American education. In 1917 Columbia established the Lincoln School of Teachers College “as a laboratory for the working out of an elementary and secondary curriculum which shall eliminate obsolete material and endeavor to work up in usable form material adapted to the needs of modern living.” (Cremin, 282) Based on Flexner’s demand that the modern curriculum “include nothing for which an affirmative case can not be made out” (Cremin, 281) the new school organized its activities around four fundamental fields: science, industry, aesthetics and civics. The Lincoln School built its curriculum around “units of work” that reorganized traditional subject matter into forms embracing the development of children and the changing needs of adult life. The first and second grades carried on a study of community life in which they actually built a city. A third grade project growing out of the day-to-day life of the nearby Hudson River became one of the most celebrated units of the school, a unit on boats, which under the guidance of its legendary teacher Miss Curtis, became an entrée into history, geography, reading, writing, arithmetic, science, art and literature. Each of the units was broadly enough conceived so that different children could concentrate on different aspects depending on their own interests and needs. Each of the units called for widely diverse student activities, and each sought to deal in depth with some critical aspect of contemporary civilization. Finally each unit engaged children working together cooperatively and also provided opportunities for individual research and exploration.
In 1924, Agnes de Lima, the lead writer on education for The New Republic and Nation, published a collection of her articles on progressive education as a book, titled "Our Enemy the Child".
In 1918 The National Education Association, representing superintendents and administrators in smaller districts across the country, issued its report "Cardinal Principles of Secondary Education." It emphasized the education of students in terms of health, a command of fundamental processes, worthy home membership, vocation, citizenship, worthy use of leisure, and ethical character. They Emphasized life adjustment and reflected the social efficiency model of progressive education.
From 1919 to 1955 the Progressive Education Association founded by Stanwood Cobb and others worked to promote a more student-centered approach to education. During the Great Depression the organization conducted an Eight Year study evaluating the effects of progressive programs. More than 1500 students over four years were compared to an equal number of carefully matched students at conventional schools. When they reached college, the experimental students were found to equal or surpass traditionally educated students on all outcomes: grades, extracurricular participation, dropout rates, intellectual curiosity, and resourcefulness. Moreover, the study found that the more the school departed from the traditional college preparatory program, the better was the record of the graduates. (Kohn, Schools, 232)
By mid-century many public school programs had also adopted elements of progressive curriculum. At mid-century Dewey believed that progressive education had “not really penetrated and permeated the foundations of the educational institution.”(Kohn, Schools, 6,7) As the influence of progressive pedagogy grew broader and more diffuse, practitioners began to vary their application of progressive principles. As varying interpretations and practices made evaluation of progressive reforms more difficult to assess, critics began to propose alternative approaches.
The seeds of the debate over progressive education can be seen in the differences of Parker and Dewey. These have to do with how much and by whom curriculum should be worked out from grade to grade, how much the child’s emerging interests should determine classroom activities, the importance of child-centered vs. societal–centered learning, the relationship of community building to individual growth, and especially the relationship between emotion, thought and experience.
In 1955 the publication of Rudolf Flesch's "Why Johnny Can’t Read" leveled criticism of reading programs at the progressive emphasis on reading in context. The conservative McCarthy era raised questions about the liberal ideas at the roots of the progressive reforms. The launching of Sputnik in 1957 at the height of the cold war gave rise to a number of intellectually competitive approaches to disciplinary knowledge, such as BSCS biology PSSC physics, led by university professors such as Jerome Bruner and Jerrold Zacharias.
Interestingly, some of the cold war reforms incorporated elements of progressivism. For example, the work of Zacharias and Bruner was based in the developmental psychology of Jean Piaget and incorporated many of Dewey’s ideas of experiential education. Bruner’s analysis of developmental psychology became the core of a pedagogical movement known as constructivism, which argues that the child is an active participant in making meaning and must be engaged in the progress of education for learning to be effective. This psychological approach has deep connections to the work of both Parker and Dewey and led to a resurgence of their ideas in second half of the century.
In 1965, President Johnson inaugurated the Great Society and the Elementary and Secondary Education Act suffused public school programs with funds for sweeping education reforms. At the same time the influx of federal funding also gave rise to demands for accountability and the behavioral objectives approach of Robert F. Mager and others foreshadowed the No Child Left Behind Act passed in 2002. Against these critics eloquent spokespersons stepped forward in defense of the progressive tradition. The Open Classroom movement, led by Herb Kohl and George Dennison, recalled many of Parker's child centered reforms.
The late 1960s and early 1970s saw a rise and decline in the number of progressive schools. There were several reasons for the decline:
Progressive education has been viewed as an alternative to the test-oriented instruction legislated by the No Child Left Behind educational funding act. Alfie Kohn has been an outspoken critic of the No Child Left Behind Act and a passionate defender of the progressive tradition.
Taxpayer revolts, leading to cuts in funding for public education in many states, have led to the founding of an unprecedented number of independent schools, many of which have progressive philosophies. The charter school movement has also spawned an increase in progressive programs. Most recently, public outcry against No Child Left Behind testing and teaching to the test has brought progressive education again into the limelight. Despite the variations that still exist among the progressive programs throughout the country, most progressive schools today are vitalized by these common practices:
Education outside of schools.
Organizations like the Boy Scouts of America and Camp Fire arose, even amidst concerns by opponents of the progressive movement in the United States, because some people felt that social welfare of young men should be maintained through education alone. After decades of growing interest in and development of experiential education and scouting (not Scouting) in the United States, and the emergence of the Scout Movement in 1907, in 1910 Boy Scouts of America was founded in the merger of three older Scouting organizations: Boy Scouts of the United States, the National Scouts of America and the Peace Scouts of California. Its founder, Chicago publisher W. D. Boyce was visiting London, in 1909, when he met the Unknown Scout and learned of the Scouting movement. Soon after his return to the U.S., Boyce incorporated the Boy Scouts of America on February 8, 1910. Edgar M. Robinson and Lee F. Hanmer became interested in the nascent BSA program and convinced Boyce to turn the program over to the YMCA for development. Robinson enlisted Ernest Thompson Seton, Daniel Carter Beard and other prominent leaders in the early youth movements. After initial development, Robinson turned the movement over to James E. West who became the first Chief Scout Executive and the Scouting movement began to expand in the U.S. As BSA grew, it absorbed other Scouting organizations.
Developments since the 1950s.
Changes in educational establishments came about as US Americans and Europeans began to feel they had fallen behind the Soviet Union technologically after the successful launch of "Sputnik 1" in October 1957. A rethinking of education theory following that, along with the subsequent and prevailing conservative political climate, helped to cause progressivism to fall from favor.

</doc>
<doc id="10006" url="https://en.wikipedia.org/wiki?curid=10006" title="Electronic musical instrument">
Electronic musical instrument

An electronic musical instrument is a musical instrument that produces sound using electronics. Such an instrument sounds by outputting an electrical audio signal that ultimately drives a loudspeaker.
An electronic instrument might include a user interface for controlling its sound, often by adjusting the pitch, frequency, or duration of each note. However, it is increasingly common to separate user interface and sound-generating functions into a music controller (input device) and a music synthesizer, respectively, with the two devices communicating through a musical performance description language such as MIDI or Open Sound Control.
All electronic musical instruments can be viewed as a subset of audio signal processing applications. Simple electronic musical instruments are sometimes called sound effects; the border between sound effects and actual musical instruments is often hazy.
Electronic musical instruments are now widely used in most styles of music. Development of new electronic musical instruments, controllers, and synthesizers continues to be a highly active and interdisciplinary field of research. Specialized conferences, notably the International Conference on New Interfaces for Musical Expression, have organized to report cutting-edge work, as well as to provide a showcase for artists who perform or create music with new electronic music instruments, controllers, and synthesizers.
Early electronic musical instruments.
In the 18th-century, musicians and composers adapted a number of acoustic instruments to exploit the novelty of electricity. Thus, in the broadest sense, the first electrified musical instrument was the Denis d'or, dating from 1753, followed shortly by the clavecin électrique by the Frenchman Jean-Baptiste de Laborde in 1761. The former instrument consisted of a keyboard instrument of over 700 strings, electrified temporarily to enhance sonic qualities. The latter was a keyboard instrument with plectra (picks) activated electrically. However, neither instrument used electricity as a sound-source.
The first electric synthesizer was invented in 1876 by Elisha Gray. The "Musical Telegraph" was a chance by-product of his telephone technology when Gray accidentally discovered that he could control sound from a self-vibrating electromagnetic circuit and so invented a basic oscillator. The Musical Telegraph used steel reeds oscillated by electromagnets and transmitted over a telephone line. Gray also built a simple loudspeaker device into later models, which consisted of a diaphragm vibrating in a magnetic field.
A significant invention, which later had a profound effect on electronic music, was the audion in 1906. This was the first thermionic valve, or vacuum tube and which led to the generation and amplification of electrical signals, radio broadcasting, and electronic computation, among other things.
Other early synthesizers included the Telharmonium (1897), the Theremin (1919), Jörg Mager's Spharophon (1924) and Partiturophone, Taubmann's similar Electronde (1933), Maurice Martenot's ondes Martenot ("Martenot waves", 1928), Trautwein's Trautonium (1930). The Mellertion (1933) used a non-standard scale, Bertrand's Dynaphone could produce octaves and perfect fifths, while the Emicon was an American, keyboard-controlled instrument constructed in 1930 and the German Hellertion combined four instruments to produce chords. Three Russian instruments also appeared, Oubouhof's Croix Sonore (1934), Ivor Darreg's microtonal 'Electronic Keyboard Oboe' (1937) and the ANS synthesizer, constructed by the Russian scientist Evgeny Murzin from 1937 to 1958. Only two models of this latter were built and the only surviving example is currently stored at the Lomonosov University in Moscow. It has been used in many Russian movies—like "Solaris"—to produce unusual, "cosmic" sounds.
Hugh Le Caine, John Hanert, Raymond Scott, composer Percy Grainger (with Burnett Cross), and others built a variety of automated electronic-music controllers during the late 1940s and 1950s. In 1959 Daphne Oram produced a novel method of synthesis, her "Oramics" technique, driven by drawings on a 35 mm film strip; it was used for a number of years at the BBC Radiophonic Workshop. This workshop was also responsible for the theme to the TV series Doctor Who, a piece, largely created by Delia Derbyshire, that more than any other ensured the popularity of electronic music in the UK.
Telharmonium.
In 1897 Thaddeus Cahill patented an instrument called the Telharmonium (or Teleharmonium, also known as the Dynamaphone). Using tonewheels to generate musical sounds as electrical signals by additive synthesis, it was capable of producing any combination of notes and overtones, at any dynamic level. This technology was later used to design the Hammond organ. Between 1901 and 1910 Cahill had three progressively larger and more complex versions made, the first weighing seven tons, the last in excess of 200 tons. Portability was managed only by rail and with the use of thirty boxcars. By 1912, public interest had waned, and Cahill's enterprise was bankrupt.
Theremin.
Another development, which aroused the interest of many composers, occurred in 1919-1920. In Leningrad, Leon Theremin (actually Lev Termen) built and demonstrated his Etherophone, which was later renamed the Theremin. This led to the first compositions for electronic instruments, as opposed to noisemakers and re-purposed machines. The Theremin was notable for being the first musical instrument played without touching it.
Composers who ultimately utilized the Theremin included Varèse—in his piece "Ecuatorial" (1934)—while conductor Leopold Stokowski experimented with its use in arrangements from the classical repertory.
In 1929, Joseph Schillinger composed "First Airphonic Suite for Theremin and Orchestra", premièred with the Cleveland Orchestra with Leon Theremin as soloist. The next year Henry Cowell commissioned Theremin to create the first electronic rhythm machine, called the Rhythmicon. Cowell wrote some compositions for it, and he and Schillinger premiered it in 1932.
Ondes Martenot.
The 1920s have been called the apex of the Mechanical Age and the dawning of the Electrical Age. In 1922, in Paris, Darius Milhaud began experiments with "vocal transformation by phonograph speed change." These continued until 1927.
This decade brought a wealth of early electronic instruments—along with the Theremin, there is the presentation of the Ondes Martenot, which was designed to reproduce the microtonal sounds found in Hindu music, and the Trautonium.
Maurice Martenot invented the Ondes Martenot in 1928, and soon demonstrated it in Paris. Composers using the instrument ultimately include Boulez, Honegger, Jolivet, Koechlin, Messiaen, Milhaud, Tremblay, and Varèse. Radiohead guitarist and multi-instrumentalist Jonny Greenwood also uses it in his compositions and a plethora of Radiohead songs. In 1937, Messiaen wrote "Fête des belles eaux" for 6 ondes Martenot, and wrote solo parts for it in "Trois petites Liturgies de la Présence Divine" (1943–44) and the "Turangalîla-Symphonie" (1946–48/90).
Trautonium.
The Trautonium was invented in 1928. It was based on the subharmonic scale, and the resulting sounds were often used to emulate bell or gong sounds, as in the 1950s Bayreuth productions of "Parsifal". In 1942, Richard Strauss used it for the bell- and gong-part in the Dresden première of his "Japanese Festival Music". This new class of instruments, microtonal by nature, was only adopted slowly by composers at first, but by the early 1930s there was a burst of new works incorporating these and other electronic instruments.
Hammond organ and Novachord.
In 1929 Laurens Hammond established his company for the manufacture of electronic instruments. He went on to produce the Hammond organ, which was based on the principles of the Telharmonium, along with other developments including early reverberation units.
The first commercially manufactured synthesizer was the Novachord, built by the Hammond Organ Company from 1938 to 1942, which offered 72-note polyphony using 12 oscillators driving monostable-based divide-down circuits, basic envelope control and resonant low-pass filters. The instrument featured 163 vacuum tubes and weighed 500 pounds. The instrument's use of envelope control is significant, since this is perhaps the most significant distinction between the modern synthesizer and other electronic instruments.
Analogue synthesis 1950–80.
The most commonly used electronic instruments are synthesizers, so-called because they artificially generate sound using a variety of techniques. All early circuit-based synthesis involved the use of analogue circuitry, particularly voltage controlled amplifiers, oscillators and filters. An important technological development was the invention of the Clavivox synthesizer in 1956 by Raymond Scott with subassembly by Robert Moog. French composer and engineer Edgard Varèse created a variety of compositions using electronic horns, whistles, and tape. Most notably, he wrote "Poème électronique" for the Phillips pavilion at the Brussels World Fair in 1958.
Modular synthesizers.
RCA produced experimental devices to synthesize voice and music in the 1950s. The Mark II Music Synthesizer, housed at the Columbia-Princeton Electronic Music Center in New York City. Designed by Herbert Belar and Harry Olson at RCA, with contributions from Vladimir Ussachevsky and Peter Mauzey, it was installed at Columbia University in 1957. Consisting of a room-sized array of interconnected sound synthesis components, it was only capable of producing music by programming, using a paper tape sequencer punched with holes to control pitch sources and filters, similar to a mechanical player piano but capable of generating a wide variety of sounds. The vacuum tube system had to be patched to create timbres.
In the 1960s synthesizers were still usually confined to studios due to their size. They were usually modular in design, their stand-alone signal sources and processors connected with patch cords or by other means and controlled by a common controlling device. Harald Bode, Don Buchla, Hugh Le Caine, Raymond Scott and Paul Ketoff were among the first to build such instruments, in the late 1950s and early 1960s. Buchla later produced a commercial modular synthesizer, the Buchla Music Easel. Robert Moog, who had been a student of Peter Mauzey and one of the RCA Mark II engineers, created a synthesizer that could reasonably be used by musicians, designing the circuits while he was at Columbia-Princeton. The Moog synthesizer was first displayed at the Audio Engineering Society convention in 1964. It required experience to set up sounds but was smaller and more intuitive than what had come before, less like a machine and more like a musical instrument. Moog established standards for control interfacing, using a logarithmic 1-volt-per-octave for pitch control and a separate triggering signal. This standardization allowed synthesizers from different manufacturers to operate simultaneously. Pitch control was usually performed either with an organ-style keyboard or a music sequencer producing a timed series of control voltages. During the late 1960s hundreds of popular recordings used Moog synthesizers. Other early commercial synthesizer manufacturers included ARP, who also started with modular synthesizers before producing all-in-one instruments, and British firm EMS.
Integrated synthesizers.
In 1970, Moog designed the Minimoog, a non-modular synthesizer with a built-in keyboard. The analogue circuits were interconnected with switches in a simplified arrangement called "normalization." Though less flexible than a modular design, normalization made the instrument more portable and easier to use. The Minimoog sold 12,000 units. further standardized the design of subsequent synthesizers with its integrated keyboard, pitch and modulation wheels and VCO->VCF->VCA signal flow. It has become celebrated for its "fat" sound—and its tuning problems. Miniaturized solid-state components allowed synthesizers to become self-contained, portable instruments that soon appeared in live performance and quickly became widely used in popular music and electronic art music.
Polyphony.
Many early analog synthesizers were monophonic, producing only one tone at a time. Popular monophonic synthesizers include the Moog Minimoog. A few, such as the Moog Sonic Six, ARP Odyssey and EML 101, could produce two different pitches at a time when two keys were pressed. Polyphony (multiple simultaneous tones, which enables chords) was only obtainable with electronic organ designs at first. Popular electronic keyboards combining organ circuits with synthesizer processing included the ARP Omni and Moog's Polymoog and Opus 3.
By 1976 affordable polyphonic synthesizers began to appear, notably the Yamaha CS-50, CS-60 and CS-80, the Sequential Circuits Prophet-5 and the Oberheim Four-Voice. These remained complex, heavy and relatively costly. The recording of settings in digital memory allowed storage and recall of sounds. The first practical polyphonic synth, and the first to use a microprocessor as a controller, was the Sequential Circuits Prophet-5 introduced in late 1977. For the first time, musicians had a practical polyphonic synthesizer that could save all knob settings in computer memory and recall them at the touch of a button. The Prophet-5's design paradigm became a new standard, slowly pushing out more complex and recondite modular designs.
Tape recording.
In 1935, another significant development was made in Germany. Allgemeine Elektrizitäts Gesellschaft (AEG) demonstrated the first commercially produced magnetic tape recorder, called the "Magnetophon". Audio tape, which had the advantage of being fairly light as well as having good audio fidelity, ultimately replaced the bulkier wire recorders.
The term "electronic music" (which first came into use during the 1930s) came to include the tape recorder as an essential element: "electronically produced sounds recorded on tape and arranged by the composer to form a musical composition" It was also indispensable to Musique concrète.
Tape also gave rise to the first, analogue, sample-playback keyboards, the Chamberlin and its more famous successor the Mellotron, an electro-mechanical, polyphonic keyboard originally developed and built in Birmingham, England in the early 1960s.
Sound sequencer.
In 1951 former jazz composer Raymond Scott invented the first sequencer, which consisted of hundreds of switches controlling stepping relays, timing solenoids, tone circuits and 16 individual oscillators.
Hardware hacking.
It was within this period (1966–67) that Reed Ghazala discovered and began to teach "circuit bending"—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage’s aleatoric music concept.
Much of this manipulation of circuits directly, especially to the point of destruction, was pioneered by Louis and Bebe Barron in the early 1950s, such as their work with John Cage on the "Williams Mix" and especially in the soundtrack to Forbidden Planet.
The digital era 1980–2000.
Digital synthesis.
The first digital synthesizers were academic experiments in sound synthesis using digital computers. FM synthesis was developed for this purpose; as a way of generating complex sounds digitally with the smallest number of computational operations per sound sample. In 1983 Yamaha introduced the first stand-alone digital synthesizer, the DX-7. It used frequency modulation synthesis (FM synthesis), first developed by John Chowning at Stanford University during the late sixties. Chowning exclusively licensed his FM synthesis patent to Yamaha in 1975. Yamaha subsequently released their first FM synthesizers, the GS-1 and GS-2, which were costly and heavy. There followed a pair of smaller, preset versions, the CE20 and CE25 Combo Ensembles, targeted primarily at the home organ market and featuring four-octave keyboards. Yamaha's third generation of digital synthesizers was a commercial success; it consisted of the DX7 and DX9 (1983). Both models were compact, reasonably priced, and dependent on custom digital integrated circuits to produce FM tonalities. The DX7 was the first mass market all-digital synthesizer. It became indispensable to many music artists of the 1980s, and demand soon exceeded supply. The DX7 sold over 200,000 units within three years.
The DX series was not easy to program but offered a detailed, percussive sound that led to the demise of the electro-mechanical Rhodes piano. Following the success of FM synthesis Yamaha signed a contract with Stanford University in 1989 to develop digital waveguide synthesis, leading to the first commercial physical modeling synthesizer, Yamaha's VL-1, in 1994.
Sampling.
The Fairlight CMI (Computer Musical Instrument), the first polyphonic digital sampler, was the harbinger of sample-based synthesizers. Designed in 1978 by Peter Vogel and Kim Ryrie and based on a dual microprocessor computer designed by Tony Furse in Sydney, Australia, the Fairlight CMI gave musicians the ability to modify volume, attack, decay, and use special effects like vibrato. Sample waveforms could be displayed on-screen and modified using a light pen. The Synclavier from New England Digital was a similar system. Jon Appleton (with Jones and Alonso) invented the Dartmouth Digital Synthesizer, later to become the New England Digital Corp's Synclavier. The Kurzweil K250, first produced in 1983, was also a successful polyphonic digital music synthesizer, noted for its ability to reproduce several instruments synchronously and having a velocity-sensitive keyboard.
Computer music.
An important new development was the advent of computers for the purpose of composing music, as opposed to manipulating or creating sounds. Iannis Xenakis began what is called "musique stochastique," or "stochastic music", which is a method of composing that employs mathematical probability systems. Different probability algorithms were used to create a piece under a set of parameters. Xenakis used graph paper and a ruler to aid in calculating the velocity trajectories of glissandi for his orchestral composition "Metastasis" (1953–54), but later turned to the use of computers to compose pieces like "ST/4" for string quartet and "ST/48" for orchestra (both 1962).
The impact of computers continued in 1956. Lejaren Hiller and Leonard Isaacson composed "Illiac Suite" for string quartet, the first complete work of computer-assisted composition using algorithmic composition.
In 1957, Max Mathews at Bell Lab wrote MUSIC-N series, a first computer program family for generating digital audio waveforms through direct synthesis. Then Barry Vercoe wrote MUSIC 11 based on MUSIC IV-BF, a next-generation music synthesis program (later evolving into csound, which is still widely used).
In mid 80s, Miller Puckette at IRCAM developed graphic signal-processing software for 4X called Max (after Max Mathews), and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode ) for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.
MIDI.
In 1980, a group of musicians and music merchants met to standardize an interface by which new instruments could communicate control instructions with other instruments and the prevalent microcomputer. This standard was dubbed MIDI (Musical Instrument Digital Interface). A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.
The advent of MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and in synchrony, with each device responding according to conditions predetermined by the composer.
MIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.
Modern electronic musical instruments.
The increasing power and decreasing cost of sound-generating electronics (and especially of the personal computer), combined with the standardization of the MIDI and Open Sound Control musical performance description languages, has facilitated the separation of musical instruments into music controllers and music synthesizers.
By far the most common musical controller is the musical keyboard. Other controllers include the radiodrum, Akai's EWI, the guitar-like SynthAxe, the BodySynth, the Buchla Thunder, the Continuum Fingerboard, the Roland Octapad, various isomorphic keyboards including the Thummer, and Kaossilator Pro, and kits like I-CubeX.
The Reactable.
The Reactable is a round translucent table with a backlit interactive display. By placing and manipulating blocks called "tangibles" on the table surface, while interacting with the visual display via finger gestures, a virtual modular synthesizer is operated, creating music or sound effects.
Percussa AudioCubes.
AudioCubes are autonomous wireless cubes powered by an internal computer system and rechargeable battery. They have internal RGB lighting, and are capable of detecting each other's location, orientation and distance. The cubes can also detect distances to the user's hands and fingers. Through interaction with the cubes, a variety of music and sound software can be operated. AudioCubes have applications in sound design, music production, DJing and live performance.
Kaossilator.
The Kaossilator and Kaossilator Pro are compact instruments where the position of a finger on the touch pad controls two note-characteristics; usually the pitch is changed with a left-right motion and the tonal property, filter or other parameter changes with an up-down motion. The touch pad can be set to different musical scales and keys. The instrument can record a repeating loop of adjustable length, set to any tempo, and new loops of sound can be layered on top of existing ones. This lends itself to electronic dance-music but is more limited for controlled sequences of notes, as the pad on a regular Kaossilator is featureless.
Eigenharp.
The Eigenharp is a large instrument resembling a bassoon, which can be interacted with through touch-sensitive buttons, a drum sequencer and a mouthpiece. The sound processing is done on a separate computer.
The Xth Sense.
The Xth Sense is a wearable instrument that uses muscle sounds from the human body (known as mechanomyogram) to make music and sound effects. As a performer moves, the body produces muscle sounds that are captured by a chip microphone worn on arm or legs. The muscle sounds are then live sampled using a dedicated software program and a library of modular audio effects. The performer controls the live sampling parameters by weighing force, speed and articulation of the movement.
AlphaSphere.
The AlphaSphere is a spherical instrument that consists of 48 tactile pads that respond to pressure as well as touch. Custom software allows the pads to be indefinitely programmed individually or by groups in terms of function, note, and pressure parameter among many other settings. The primary concept of the AlphaSphere is to increase the level of expression available to electronic musicians, by allowing for the playing style of a musical instrument.
Chip music.
Chiptune, chipmusic, or chip music is music written in sound formats where many of the sound textures are synthesized or sequenced in real time by a computer or video game console sound chip, sometimes including sample-based synthesis and low bit sample playback. Many chip music devices featured synthesizers in tandem with low rate sample playback.
DIY culture.
During the late 1970s and early 1980s, DIY (Do it yourself) designs were published in hobby electronics magazines (notably the Formant modular synth, a DIY clone of the Moog system, published by Elektor) and kits were supplied by companies such as Paia in the US, and Maplin Electronics in the UK.
Circuit bending.
Circuit bending is the creative customization of the circuits within electronic devices such as low voltage, battery-powered guitar effects, children's toys and small digital synthesizers to create new musical or visual instruments and sound generators. Emphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with "bent" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit. With the revived interest for analogue synthesizer circuit bending became a cheap solution for many experimental musicians to create their own individual analogue sound generators. Nowadays many schematics can be found to build noise generators such as the Atari Punk Console or the Dub Siren as well as simple modifications for children toys such as the famous Speak & Spells that are often modified by circuit benders.
Modular synthesizers.
The modular synthesizer is a type of synthesizer consisting of separate interchangeable modules. These are also available as kits for hobbyist DIY constructors. Many hobbyist designers also make available bare PCB boards and front panels for sale to other hobbyists.
Future electronic musical instruments.
According to a forum post in December 2010, Sixense Entertainment is working on musical control with the Sixense TrueMotion motion controller.
Immersive virtual musical instruments, or immersive virtual instruments for music and sound aim to represent musical events and sound parameters in a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, allowing the development of novel interaction metaphors beyond manipulation such as prehension.

</doc>
<doc id="10008" url="https://en.wikipedia.org/wiki?curid=10008" title="Electrode">
Electrode

An electrode is an electrical conductor used to make contact with a nonmetallic part of a circuit (e.g. a semiconductor, an electrolyte, a vacuum or air). The word was coined by William Whewell at the request of the scientist Michael Faraday from the Greek words "elektron", meaning amber (from which the word electricity is derived), and "hodos", a way.
Anode and cathode in electrochemical cells.
An electrode in an electrochemical cell is referred to as either an "anode" or a "cathode" (words that were coined by William Whewell at Faraday's request). The anode is now defined as the electrode at which electrons leave the cell and oxidation occurs, and the cathode as the electrode at which electrons enter the cell and reduction occurs. Each electrode may become either the anode or the cathode depending on the direction of current through the cell. A bipolar electrode is an electrode that functions as the anode of one cell and the cathode of another cell.
Primary cell.
A primary cell is a special type of electrochemical cell in which the reaction cannot be reversed, and the identities of the anode and cathode are therefore fixed. The anode is always the negative electrode. The cell can be discharged but not recharged.
Secondary cell.
A secondary cell, for example a rechargeable battery, is a cell in which the chemical reactions are reversible. When the cell is being charged, the anode becomes the positive (+) and the cathode the negative (−) electrode. This is also the case in an electrolytic cell. When the cell is being discharged, it behaves like a primary cell, with the anode as the negative and the cathode as the positive electrode.
Other anodes and cathodes.
In a vacuum tube or a semiconductor having polarity (diodes, electrolytic capacitors) the anode is the positive (+) electrode and the cathode the negative (−). The electrons enter the device through the cathode and exit the device through the anode. Many devices have other electrodes to control operation, e.g., base, gate, control grid.
In a three-electrode cell, a counter electrode, also called an auxiliary electrode, is used only to make a connection to the electrolyte so that a current can be applied to the working electrode. The counter electrode is usually made of an inert material, such as a noble metal or graphite, to keep it from dissolving.
Welding electrodes.
In arc welding an electrode is used to conduct current through a workpiece to fuse two pieces together. Depending upon the process, the electrode is either consumable, in the case of gas metal arc welding or shielded metal arc welding, or non-consumable, such as in gas tungsten arc welding. For a direct current system the weld rod or stick may be a cathode for a filling type weld or an anode for other welding processes. For an alternating current arc welder the welding electrode would not be considered an anode or cathode.
Alternating current electrodes.
For electrical systems which use alternating current the electrodes are the connections from the circuitry to the object to be acted upon by the electric current but are not designated anode or cathode because the direction of flow of the electrons changes periodically, usually many times per second.
Uses.
Electrodes are used to provide current through nonmetal objects to alter them in numerous ways and to measure conductivity for numerous purposes. Examples include:
Chemically modified electrodes.
Chemically modified electrodes are electrodes that have their surfaces chemically modified to change the electrode's physical, chemical, electrochemical, optical, electrical, and transport properties. These electrodes are used for advanced purposes in research and investigation.

</doc>
<doc id="10011" url="https://en.wikipedia.org/wiki?curid=10011" title="Epistolary novel">
Epistolary novel

An epistolary novel is a novel written as a series of documents. The usual form is letters, although diary entries, newspaper clippings and other documents are sometimes used. Recently, electronic "documents" such as recordings and radio, blogs, and e-mails have also come into use. The word "epistolary" is derived from Latin from the Greek word ἐπιστολή "epistolē", meaning a letter (see epistle).
The epistolary form can add greater realism to a story, because it mimics the workings of real life. It is thus able to demonstrate differing points of view without recourse to the device of an omniscient narrator.
Early works.
There are two theories on the genesis of the epistolary novel. The first claims that the genre originated from novels with inserted letters, in which the portion containing the third person narrative in between the letters was gradually reduced. The other theory claims that the epistolary novel arose from miscellanies of letters and poetry: some of the letters were tied together into a (mostly amorous) plot. Both claims have some validity. The first truly epistolary novel, the Spanish "Prison of Love" ("Cárcel de amor") (c.1485) by Diego de San Pedro, belongs to a tradition of novels in which a large number of inserted letters already dominated the narrative. Other well-known examples of early epistolary novels are closely related to the tradition of letter-books and miscellanies of letters. Within the successive editions of Edmé Boursault's "Letters of Respect, Gratitude and Love" ("Lettres de respect, d'obligation et d'amour") (1669), a group of letters written to a girl named Babet were expanded and became more and more distinct from the other letters, until it formed a small epistolary novel entitled "Letters to Babet" ("Lettres à Babet"). The immensely famous "Letters of a Portuguese Nun" ("Lettres portugaises") (1669) generally attributed to Gabriel-Joseph de La Vergne, comte de Guilleragues, though a small minority still regard Marianna Alcoforado as the author, is claimed to be intended to be part of a miscellany of Guilleragues prose and poetry.
The founder of the epistolary novel in English is said by many to be James Howell (1594–1666) with "Familiar Letters" (1645–50), who writes of prison, foreign adventure, and the love of women.
The first novel to expose the complex play that the genre allows was Aphra Behn's "Love-Letters Between a Nobleman and His Sister", which appeared in three volumes in 1684, 1685, and 1687. The novel shows the genre's results of changing perspectives: individual points were presented by the individual characters, and the central voice of the author and moral evaluation disappeared (at least in the first volume; her further volumes introduced a narrator). Behn furthermore explored a realm of intrigue with letters that fall into the wrong hands, faked letters, letters withheld by protagonists, and even more complex interaction.
The epistolary novel as a genre became popular in the 18th century in the works of such authors as Samuel Richardson, with his immensely successful novels "Pamela" (1740) and "Clarissa" (1749). In France, there was "Lettres persanes" (1721) by Montesquieu, followed by "Julie, ou la nouvelle Héloïse" (1761) by Jean-Jacques Rousseau, and Laclos' "Les Liaisons dangereuses" (1782), which used the epistolary form to great dramatic effect, because the sequence of events was not always related directly or explicitly. In Germany, there was Johann Wolfgang von Goethe's "Die Leiden des jungen Werthers" (1774) ("The Sorrows of Young Werther") and Friedrich Hölderlin's "Hyperion". The first North American novel, "The History of Emily Montague" (1769) by Frances Brooke was written in epistolary form.
Starting in the 18th century, the epistolary form was subject to much ridicule, resulting in a number of savage burlesques. The most notable example of these was Henry Fielding's "Shamela" (1741), written as a parody of "Pamela". In it, the female narrator can be found wielding a pen and scribbling her diary entries under the most dramatic and unlikely of circumstances. Oliver Goldsmith used the form to satirical effect in The Citizen of the World, subtitled "Letters from a Chinese Philosopher Residing in London to his Friends in the East" (1760–61). So did the diarist Fanny Burney in a successful comic first novel, "Evelina" (1788).
The epistolary novel slowly fell out of use in the late 18th century. Although Jane Austen tried her hand at the epistolary in juvenile writings and her novella "Lady Susan" (1794), she abandoned this structure for her later work. It is thought that her lost novel "First Impressions", which was redrafted to become "Pride and Prejudice", may have been epistolary: "Pride and Prejudice" contains an unusual number of letters quoted in full and some play a critical role in the plot.
The epistolary form nonetheless saw continued use, surviving in exceptions or in fragments in nineteenth-century novels. In Honoré de Balzac's novel "Letters of Two Brides", two women who became friends during their education at a convent correspond over a 17-year period, exchanging letters describing their lives. Mary Shelley employs the epistolary form in her novel "Frankenstein" (1818). Shelley uses the letters as one of a variety of framing devices, as the story is presented through the letters of a sea captain and scientific explorer attempting to reach the north pole who encounters Victor Frankenstein and records the dying man's narrative and confessions. Published in 1848, Anne Brontë's novel "The Tenant of Wildfell Hall" is framed as a retrospective letter from one of the main heroes to his friend and brother-in-law with the diary of the eponymous tenant inside it. In the late 19th century, Bram Stoker released one of the most widely recognized and successful novels in the epistolary form to date, "Dracula". Printed in 1897, the novel is compiled entirely of letters, diary entries, newspaper clippings, telegrams, doctor's notes, ship's logs, and the like, which Stoker adroitly employs to balance believability and dramatic tension.
Types of epistolary novels.
There are three types of epistolary novels: monologic (giving the letters of only one character, like "Letters of a Portuguese Nun" and "The Sorrows Of Young Werther"), dialogic (giving the letters of two characters, like Mme Marie Jeanne Riccoboni's "Letters of Fanni Butlerd" (1757), and polylogic (with three or more letter-writing characters, such as in Bram Stoker's "Dracula"). In addition, a crucial element in polylogic epistolary novels like "Clarissa", and "Dangerous Liaisons" is the dramatic device of 'discrepant awareness': the simultaneous but separate correspondences of the heroines and the villains creating dramatic tension.
Later works.
Epistolary novels have made several memorable appearances in more recent literature:

</doc>
<doc id="10013" url="https://en.wikipedia.org/wiki?curid=10013" title="Evidence-based medicine">
Evidence-based medicine

Evidence-based medicine (EBM) is an approach to medical practice intended to optimize decision-making by emphasizing the use of evidence from well designed and conducted research. Although all medicine based on science has some degree of empirical support, EBM goes further, classifying evidence by its epistemologic strength and requiring that only the strongest types (coming from meta-analyses, systematic reviews, and randomized controlled trials) can yield strong recommendations; weaker types (such as from case-control studies) can yield only weak recommendations. The term was originally used to describe an approach to teaching the practice of medicine and improving decisions by individual physicians about individual patients. Use of the term rapidly expanded to include a previously described approach that emphasized the use of evidence in the design of guidelines and policies that apply to groups of patients and populations ("evidence-based practice policies"). It has subsequently spread to describe an approach to decision making that is used at virtually every level of health care as well as other fields (evidence-based practice).
Whether applied to medical education, decisions about individuals, guidelines and policies applied to populations, or administration of health services in general, evidence-based medicine advocates that to the greatest extent possible, decisions and policies should be based on evidence, not just the beliefs of practitioners, experts, or administrators. It thus tries to assure that a clinician's opinion, which may be limited by knowledge gaps or biases, is supplemented with all available knowledge from the scientific literature so that best practice can be determined and applied. It promotes the use of formal, explicit methods to analyze evidence and makes it available to decision makers. It promotes programs to teach the methods to medical students, practitioners, and policy makers.
Background, history and definition.
In its broadest form, evidence-based medicine is the application of the scientific method into healthcare decision-making. Medicine has a long tradition of both basic and clinical research that dates back at least to Avicenna. However, until recently, the process by which research results were incorporated in medical decisions was highly subjective. Called "clinical judgment" and "the art of medicine", the traditional approach to making decisions about individual patients depended on having each individual physician determine what research evidence, if any, to consider, and how to merge that evidence with personal beliefs and other factors. In the case of decisions that applied to groups of patients or populations, the guidelines and policies would usually be developed by committees of experts, but there was no formal process for determining the extent to which research evidence should be considered or how it should be merged with the beliefs of the committee members. There was an implicit assumption that decision makers and policy makers would incorporate evidence in their thinking appropriately, based on their education, experience, and ongoing study of the applicable literature.
Clinical decision making.
Beginning in the late 1960s, several flaws became apparent in the traditional approach to medical decision-making. Alvan Feinstein's publication of "Clinical Judgment" in 1967 focused attention on the role of clinical reasoning and identified biases that can affect it. In 1972, Archie Cochrane published "Effectiveness and Efficiency", which described the lack of controlled trials supporting many practices that had previously been assumed to be effective. In 1973, John Wennberg began to document wide variations in how physicians practiced. Through the 1980s, David M. Eddy described errors in clinical reasoning and gaps in evidence. In the mid 1980s, Alvin Feinstein, David Sackett and others published textbooks on clinical epidemiology, which translated epidemiological methods to physician decision making. Toward the end of the 1980s, a group at RAND showed that large proportions of procedures performed by physicians were considered inappropriate even by the standards of their own experts. These areas of research increased awareness of the weaknesses in medical decision making at the level of both individual patients and populations, and paved the way for the introduction of evidence-based methods.
Evidence-based.
The term "evidence-based medicine", as it is currently used, has two main tributaries. Chronologically, the first is the insistence on explicit evaluation of evidence of effectiveness when issuing clinical practice guidelines and other population-level policies. The second is the introduction of epidemiological methods into medical education and individual patient-level decision-making.
Evidence-based guidelines and policies.
The term "evidence-based" was first used by David M. Eddy in the course of his work on population-level policies such as clinical practice guidelines and insurance coverage of new technologies. He first began to use the term "evidence-based" in 1987 in workshops and a manual commissioned by the Council of Medical Specialty Societies to teach formal methods for designing clinical practice guidelines. The manual was widely available in unpublished form in the late 1980s and eventually published by the American College of Medicine. Eddy first published the term "evidence-based" in March, 1990 in an article in the "Journal of the American Medical Association" that laid out the principles of evidence-based guidelines and population-level policies, which Eddy described as "explicitly describing the available evidence that pertains to a policy and tying the policy to evidence. Consciously anchoring a policy, not to current practices or the beliefs of experts, but to experimental evidence. The policy must be consistent with and supported by evidence. The pertinent evidence must be identified, described, and analyzed. The policymakers must determine whether the policy is justified by the evidence. A rationale must be written." He discussed "evidence-based" policies in several other papers published in "JAMA" in the spring of 1990. Those papers were part of a series of 28 published in "JAMA" between 1990 and 1997 on formal methods for designing population-level guidelines and policies.
Medical education.
The term "evidence-based medicine" was introduced slightly later, in the context of medical education. This branch of evidence-based medicine has its roots in clinical epidemiology. In the autumn of 1990, Gordon Guyatt used it in an unpublished description of a program at McMaster University for prospective or new medical students. Guyatt and others first published the term two years later (1992) to describe a new approach to teaching the practice of medicine. In 1996, David Sackett and colleagues clarified the definition of this tributary of evidence-based medicine as "the conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients. ... means integrating individual clinical expertise with the best available external clinical evidence from systematic research." This branch of evidence-based medicine aims to make individual decision making more structured and objective by better reflecting the evidence from research. It requires the application of population-based data to the care of an individual patient, while respecting the fact that practitioners have clinical expertise reflected in effective and efficient diagnosis and thoughtful identification and compassionate use of individual patients' predicaments, rights, and preferences. This tributary of evidence-based medicine had its foundations in clinical epidemiology, a discipline that teaches medical students and physicians how to apply clinical and epidemiological research studies to their practices. The methods were published to a broad physician audience in a series of 25 "Users’ Guides to the Medical Literature" published in "JAMA" between 1993 and 2000 by the Evidence-based Medicine Working Group at McMaster University. Other definitions for individual level evidence-based medicine have been put forth. For example, in 1995 Rosenberg and Donald defined it as "the process of finding, appraising, and using contemporaneous research findings as the basis for medical decisions." In 2010, Greenhalgh used a definition that emphasized quantitative methods: "the use of mathematical estimates of the risk of benefit and harm, derived from high-quality research on population samples, to inform clinical decision-making in the diagnosis, investigation or management of individual patients." Many other definitions have been offered for individual level evidence-based medicine, but the one by Sackett and colleagues is the most commonly cited.
The two original definitions highlight important differences in how evidence-based medicine is applied to populations versus individuals. When designing policies such as guidelines that will be applied to large groups of people in settings where there is relatively little opportunity for modification by individual physicians, evidence-based policymaking stresses that there be good evidence documenting that the effectiveness of the test or treatment under consideration. In the setting of individual decision-making there is additional information about the individual patients. Practitioners can be given greater latitude in how they interpret research and combine it with their clinical judgment. Recognizing the two branches of EBM, in 2005 Eddy offered an umbrella definition: "Evidence-based medicine is a set of principles and methods intended to ensure that to the greatest extent possible, medical decisions, guidelines, and other types of policies are based on and consistent with good evidence of effectiveness and benefit."
Progress.
Both branches of evidence-based medicine spread rapidly. On the evidence-based guidelines and policies side, explicit insistence on evidence of effectiveness was introduced by the American Cancer Society in 1980. The U.S. Preventive Services Task Force (USPSTF) began issuing guidelines for preventive interventions based on evidence-based principles in 1984. In 1985, the Blue Cross Blue Shield Association applied strict evidence-based criteria for covering new technologies. Beginning in 1987, specialty societies such as the American College of Physicians, and voluntary health organizations such as the American Heart Association, wrote many evidence-based guidelines. In 1991, Kaiser Permanente, a managed care organization in the US, began an evidence-based guidelines program. In 1991, Richard Smith wrote an editorial in the "British Medical Journal" and introduced the ideas of evidence-based policies in the UK. In 1993, the Cochrane Collaboration created a network of 13 countries to produce of systematic reviews and guidelines. In 1997, the US Agency for Healthcare Research and Quality (then known as the Agency for Health Care Policy and Research, or AHCPR) established Evidence-based Practice Centers (EPCs) to produce evidence reports and technology assessments to support the development of guidelines. In the same year, a National Guideline Clearinghouse that followed the principles of evidence-based policies was created by AHRQ, the AMA, and the American Association of Health Plans (now America's Health Insurance Plans). In 1999, the National Institute for Clinical Excellence (NICE) was created in the UK. A central idea of this branch of evidence-based medicine is that evidence should be classified according to the rigor of its experimental design, and the strength of a recommendation should depend on the strength of the evidence.
On the medical education side, programs to teach evidence-based medicine have been created in medical schools in Canada, the US, the UK, Australia, and other countries. A 2009 study of UK programs found the more than half of UK medical schools offered some training in evidence-based medicine, although there was considerable variation in the methods and content, and EBM teaching was restricted by lack of curriculum time, trained tutors and teaching materials. Many programs have been developed to help individual physicians gain better access to evidence. For example, Up-to-date was created in the early 1990s. The Cochrane Center began publishing evidence reviews in 1993. BMJ Publishing Group launched a 6-monthly periodical in 1995 called Clinical Evidence that provided brief summaries of the current state of evidence about important clinical questions for clinicians. Since then many other programs have been developed to make evidence more accessible to practitioners.
Current practice.
The term evidence-based medicine is now applied to both the programs that are designing evidence-based guidelines and the programs that teach evidence-based medicine to practitioners. By 2000, "evidence-based medicine" had become an umbrella term for the emphasis on evidence in both population-level and individual-level decisions. In subsequent years, use of the term "evidence-based" had extended to other levels of the health care system. An example is "evidence-based health services", which seek to increase the competence of health service decision makers and the practice of evidence-based medicine at the organizational or institutional level. The concept has also spread outside of healthcare; for example, in his 1996 inaugural speech as President of the Royal Statistical Society, Adrian Smith proposed that "evidence-based policy" should be established for education, prisons and policing policy and all areas of government work.
The multiple tributaries of evidence-based medicine share an emphasis on the importance of incorporating evidence from formal research in medical policies and decisions. However they differ on the extent to which they require good evidence of effectiveness before promulgating a guideline or payment policy, and they differ on the extent to which it is feasible to incorporate individual-level information in decisions. Thus, evidence-based guidelines and policies may not readily 'hybridise' with experience-based practices orientated towards ethical clinical judgement, and can lead to contradictions, contest, and unintended crises. The most effective 'knowledge leaders' (managers and clinical leaders) use a broad range of management knowledge in their decision making, rather than just formal evidence. Evidence-based guidelines may provide the basis for governmentality in health care and consequently play a central role in the distant governance of contemporary health care systems.
Methods.
Steps.
The steps for designing explicit, evidence-based guidelines were described in the late 1980s: Formulate the question (population, intervention, comparison intervention, outcomes, time horizon, setting); search the literature to identify studies that inform the question; interpret each study to determine precisely what it says about the question; if several studies address the question, synthesize their results (meta-analysis); summarize the evidence in "evidence tables"; compare the benefits, harms and costs in a "balance sheet"; draw a conclusion about the preferred practice; write the guideline; write the rationale for the guideline; have others review each of the previous steps; implement the guideline.
For the purposes of medical education and individual-level decision making, five steps of EBM in practice were described in 1992 and the experience of delegates attending the 2003 Conference of Evidence-Based Health Care Teachers and Developers was summarized into five steps and published in 2005. This five step process can broadly be categorized as:
Evidence reviews.
Systematic reviews of published research studies is a major part of the evaluation of particular treatments. The Cochrane Collaboration is one of the best-known programs that conducts systematic reviews. Like other collections of systematic reviews, it requires authors to provide a detailed and repeatable plan of their literature search and evaluations of the evidence. Once all the best evidence is assessed, treatment is categorized as (1) likely to be beneficial, (2) likely to be harmful, or (3) evidence did not support either benefit or harm.
A 2007 analysis of 1,016 systematic reviews from all 50 Cochrane Collaboration Review Groups found that 44% of the reviews concluded that the intervention was likely to be beneficial, 7% concluded that the intervention was likely to be harmful, and 49% concluded that evidence did not support either benefit or harm. 96% recommended further research. A 2001 review of 160 Cochrane systematic reviews (excluding complementary treatments) in the 1998 database revealed that, according to two readers, 41.3% concluded positive or possibly positive effect, 20% concluded evidence of no effect, 8.1% concluded net harmful effects, and 21.3% of the reviews concluded insufficient evidence. A review of 145 alternative medicine Cochrane reviews using the 2004 database revealed that 38.4% concluded positive effect or possibly positive (12.4%) effect, 4.8% concluded no effect, 0.69% concluded harmful effect, and 56.6% concluded insufficient evidence.
Assessing the quality of evidence.
Evidence quality can be assessed based on the source type (from meta-analyses and systematic reviews of triple-blind randomized clinical trials with concealment of allocation and no attrition at the top end, down to conventional wisdom at the bottom), as well as other factors including statistical validity, clinical relevance, currency, and peer-review acceptance. Evidence-based medicine categorizes different types of clinical evidence and rates or grades them according to the strength of their freedom from the various biases that beset medical research. For example, the strongest evidence for therapeutic interventions is provided by systematic review of randomized, triple-blind, placebo-controlled trials with allocation concealment and complete follow-up involving a homogeneous patient population and medical condition. In contrast, patient testimonials, case reports, and even expert opinion (however some critics have argued that expert opinion "does not belong in the rankings of the quality of empirical evidence because it does not represent a form of empirical evidence" and continue that "expert opinion would seem to be a separate, complex type of knowledge that would not fit into hierarchies otherwise limited to empirical evidence alone.") have little value as proof because of the placebo effect, the biases inherent in observation and reporting of cases, difficulties in ascertaining who is an expert and more.
Several organizations have developed grading systems for assessing the quality of evidence. An example that put forth by the U.S. Preventive Services Task Force (USPSTF).
Another example of a system for grading evidence is the Oxford (UK) CEBM Levels of Evidence, Most of the evidence ranking schemes grade evidence for therapy and prevention, but not for diagnostic tests, prognostic markers, or harm. The Oxford CEBM Levels of Evidence addresses this issue and provides 'Levels' of evidence for claims about prognosis, diagnosis, treatment benefits, treatment harms, and screening. The original CEBM Levels was first released in September 2000 for Evidence-Based On Call to make the process of finding evidence feasible and its results explicit. In 2011, the Oxford CEBM Levels were redesigned by an international team to make it more understandable and to take into account recent developments in evidence ranking schemes. The Oxford CEBM Levels of Evidence have been used by patients, clinicians and also to develop clinical guidelines including recommendations for the optimal use of phototherapy and topical therapy in psoriasis and guidelines for the use of the BCLC staging system for diagnosing and monitoring hepatocellular carcinoma in Canada.
Categories of recommendations.
In guidelines and other publications, recommendation for a clinical service is classified by the balance of risk versus benefit of the service and the level of evidence on which this information is based. The U.S. Preventive Services Task Force uses:
A system was developed by the GRADE working group and takes into account more dimensions than just the quality of medical research. It requires users of GRADE (short for Grading of Recommendations Assessment, Development and Evaluation) who are performing an assessment of the quality of evidence, usually as part of a systematic review, to consider the impact of different factors on their confidence in the results. Authors of GRADE tables, grade the quality of evidence into four levels, on the basis of their confidence in the observed effect (a numerical value) being close to what the true effect is. The confidence value is based on judgements assigned in five different domains in a structured manner. The GRADE working group defines 'quality of evidence' and 'strength of recommendations' based on the quality as two different concepts which are commonly confused with each other.
Systematic reviews may include Randomized Controlled trials that have low risk of bias, or, observational studies that have high risk of bias. In the case of Randomized controlled trials, the quality of evidence is high, but can be downgraded in five different domains.
In the case of observational studies, the quality of evidence starts of lower and may be upgraded in three domains in addition to being subject to downgrading.
Meaning of the levels of quality of evidence as per GRADE:
Guideline panelists may make Strong or Weak recommendations on the basis of further criteria. Some of the important criteria are:
Despite the differences between systems, the purposes are the same: to guide users of clinical research information on which studies are likely to be most valid. However, the individual studies still require careful critical appraisal.
Statistical measures.
Evidence-based medicine attempts to express clinical benefits of tests and treatments using mathematical methods. Tools used by practitioners of evidence-based medicine include:
Quality of clinical trials.
Evidence-based medicine attempts to objectively evaluate the quality of clinical research by critically assessing techniques reported by researchers in their publications.
Limitations and criticism.
Although evidence-based medicine is regarded as the gold standard of clinical practice, there are a number of limitations and criticisms of its use many of which remain unresolved despite nearly two centuries of debate.
Assessing the teaching of evidence-based medicine.
Two instruments, the Berlin questionnaire and the Fresno TestFresno test</ref> are the most validated. These questionnaires have been used in diverse settings.

</doc>
<doc id="10016" url="https://en.wikipedia.org/wiki?curid=10016" title="End zone">
End zone

The end zone refers to the scoring area on the field, according to gridiron-based codes of football. It is the area between the end line and goal line bounded by the sidelines. There are two end zones, each being on an opposite side of the field. It is bordered on all sides by a white line indicating its beginning and end points, with orange, square pylons placed at each of the four corners as a visual aid. Canadian rule books use the terms "goal area" and "dead line" instead of "end zone" and "end line" respectively, but the latter terms are the more common in colloquial Canadian English. Unlike sports like association football and ice hockey which require the puck or ball to pass completely over the goal line to count as a score, both Canadian and American football merely need the nose of the ball to break the vertical plane of the outer edge of the goal line.
A similar concept exists in both rugby football codes, where it is known as the "in-goal area". The difference between rugby and gridiron-based codes is that in rugby, the ball must be touched to the ground in the in-goal area to count as a try (the rugby equivalent of a touchdown), whereas in the gridiron-based games, simply possessing the ball while it is in the end zone is sufficient to count it as a touchdown.
Ultimate frisbee also uses an end zone scoring area. Scores in this sport are counted when a pass is received in the end zone.
History.
The end zones were invented as a result of the creation of the forward pass. Prior to this, the goal line and end line were the same, and players scored a touchdown by leaving the field of play through that line. Goal posts were placed on the goal line, and any kicks that did not result in field goals but left the field through the end lines were simply recorded as touchbacks (or, in the Canadian game, singles; it was during the pre-end zone era that Hugh Gall set the record for most singles in a game, with eight).
In the earliest days of the forward pass, the pass had to be caught in-bounds and could not be thrown across the goal line (as the receiver would be out of bounds). This also made it difficult to pass the ball when very close to one's own goal line, since merely dropping back to pass or kick would result in a safety (rules of the forward pass at the time required the passer to be five yards behind the line of scrimmage, which would make throwing the forward pass when the ball was snapped from behind one's own five-yard line illegal in itself).
Thus, in 1912, the end zone was introduced in American football. In an era when professional football was still in its early years and college football dominated the game, the resulting enlargement of the field was constrained by fact that many college teams were already playing in well-developed stadiums, complete with stands and other structures at the ends of the fields, thereby making any substantial enlargement of the field unfeasible at many schools. Eventually, a compromise was reached: 12 yards of end zone were added to each end of the field, but in return, the playing field was shortened from 110 yards to 100, resulting in the physical size of the field being only slightly longer than before. Goal posts were originally kept on the goal lines, but after they began to interfere with play, they moved back to the end lines in 1927, where they have remained in college football ever since. The National Football League moved the goal posts up to the goal line again in 1933, then back again to the end line in 1974.
As with many other aspects of gridiron football, Canadian football adopted the forward pass and end zones much later than American football. The forward pass and end zones were adopted in 1929. In Canada, college football never reached a level of prominence comparable to U.S. college football, and professional football was still in its infancy in the 1920s. As a result, Canadian football was still being played in rudimentary facilities in the late 1920s. A further consideration was that the Canadian Rugby Union (the governing body of Canadian football at the time) wanted to reduce the prominence of single points (then called "rouges") in the game. Therefore, the CRU simply appended 25-yard end zones to the ends of the existing 110-yard field, creating a much larger field of play. Since moving the goal posts back 25 yards would have made the scoring of field goals excessively difficult, and since the CRU did not want to reduce the prominence of field goals, the goal posts were left on the goal line where they remain today. However, the rules governing the scoring of singles were changed: teams were required to either kick the ball out of bounds through the end zone or force the opposition to down a kicked ball in their own end zone in order to be awarded a point. By 1986, at which point CFL stadiums were becoming bigger and comparable in development to their American counterparts in an effort to stay financially competitive, the CFL reduced the depth of the end zone to 20 yards.
Scoring.
A team scores a touchdown by entering its opponent's end zone while carrying the ball or catching the ball while being within the end zone. If the ball is carried by a player, it is considered a score when any part of the ball is directly above or beyond any part of the goal line between the pylons. In addition, a two-point conversion may be scored after a touchdown by similar means.
In Ultimate Frisbee, a goal is scored by completing a pass into the end zone.
Size.
The end zone in American football is 10 yards long by yards (160 feet) wide. Each corner is marked with a pylon.
A full-sized end zone in Canadian football is 20 yards long by 65 yards wide. Prior to the 1980s, the Canadian end zone was 25 yards long. The first stadium to use the 20 yard long end zone was B.C. Place in Vancouver, which was completed in 1983. The floor of B.C. Place was (and is) too short to accommodate a field 160 yards in length. The shorter end zone proved popular enough that the CFL adopted it league-wide in 1986.
In Canadian football stadiums that also feature a running track, it is usually necessary to truncate the back corners of the end zones, since a rectangular field 150 yards long and 65 yards wide will not fit completely inside an oval-shaped running track. Such truncations are marked as straight diagonal lines, resulting in an end zone with six corners and six pylons. As of 2016, Montreal's Percival Molson Stadium is the only CFL stadium that has the rounded-off style end zones.
During the CFL's American expansion in the mid-1990s, several stadiums, by necessity, used 15-yard end zones (some even shorter than 15).
Ultimate Frisbee uses an end zone 40 yards wide and 20 yards deep (37 m × 18 m).
The goal post.
The location and dimensions of a goal post differ from league to league, but it is usually within the boundaries of the end zone. In earlier football games (both professional and collegiate), the goal post began at the goal line, and was usually an H-shaped bar. Nowadays, for player safety reasons, almost all goal posts in the professional and collegiate levels of American football are T-shaped, and reside just outside the rear of both end zones.
The goal posts in Canadian football still reside on the goal line instead of the back of the end zones, partly because the number of field goal attempts would dramatically decrease if the posts were moved 20 yards back in that sport.
At the high school level, it is not uncommon to see multi-purpose goal posts that include football goal posts at the top and a soccer net at the bottom; these are usually seen at smaller schools and in multi-purpose stadiums where facilities are used for multiple sports. When these or H-shaped goal posts are used in football, the lower portions of the posts are covered with several inches of heavy foam padding to protect the safety of the players.
Decoration.
Most professional and collegiate teams have their logo, team name, or both painted on the surface of the end zone, with team colors filling the background. Many championship and bowl games at college and professional level are commemorated by the names of the opposing teams each being painted in one of the opposite end zones. In the CFL and Arena leagues, along with bowl games, local, national, or bowl game sponsors may also have their logos placed in the end zone.
In many places, particularly in smaller high schools and colleges, end zones are undecorated, or have plain white diagonal stripes spaced several yards apart, in lieu of colors and decorations. One notable use of this design in higher levels is with the Pittsburgh Steelers, who kept their diagonal-line end zone decoration at Heinz Field after positive fan reaction.
One of the quirks of the American Football League was its use of unusual patterns such as argyle in its end zones, a tradition revived in 2009 by the Denver Broncos, itself a former AFL team. The XFL standardized its playing fields so that all eight of its teams had uniform fields with the XFL logo in each end zone and no team identification.

</doc>
<doc id="10017" url="https://en.wikipedia.org/wiki?curid=10017" title="Ettore Ximenes">
Ettore Ximenes

Ettore Ximenes (April 11, 1855, Palermo – December 20, 1926, Rome) was an Italian sculptor.
Biography.
Ximenes initially embarked on literary studies but then took up sculpture and attended the courses at the Palermo Academy of Fine Arts. After 1872, he continued training at the Naples Academy under Domenico Morelli and Stanislao Lista. He also established a close relationship with Vincenzo Gemito.
He returned to Palermo in 1874 and won a competition for a four-year grant, which enabled him to study and open a studio for sculpture in Florence. In 1873 at Vienna, he exhibited "Work without Genius". In 1877 at Naples, he exhibited a life-size statue titled "The Equilibrium" about a gymnast walking on a sphere. He would make copies of this work in small marble and bronze statuettes.
He exhibited a stucco "Christ and the Adultress" and "Il cuore del re (Heart of the King)", the latter depicting an oft-repeated story of King Vittorio Emanuele during one of his frequent hunts, encountering and offering charity to a peasant child. At the 1878 Paris World Exposition he displayed: "The Brawl" and "il Marmiton". In Paris, he met with Auguste Rodin and Jean-Baptiste Carpeaux.
In 1878, he also completed a life-size stucco of "il Ciceruacchio", a statue of the Italian patriot Angelo Brunetti and his thirteen-year-old son, depicting them at the moment of their execution in 1849 by Austrian troops. The Cicervacchio statue, with its tinge of revolutionary zeal, did not find commissions for completing the work in marble.
He then completed a nude statue of "Nanà" based on the novel by Émile Zola; the statue was exhibited at the 1879 Salon di Paris. The next year at the Paris Salon, he displayed "La Pesca meravigliosa", where a fisherman rescues a bathing maiden. Returning to Italy, he displayed the bust del minister Giuseppe Zanardelli. At the Mostra of Rome, he displayed "The assassination of Julius Caeser"; and at the Exposition of Venice, "Ragazzi messi in fila". Ximenes' realism gave way to Symbolist and Neo-Renaissance elements. In addition to sculpture, he also produced illustrations for the works of Edmondo De Amicis published by the Treves publishing house.
Ximenes was involved in many of the major official monumental projects in Italy from the 1880s on and devoted his energies as from 1911 primarily to commissions for important public works in São Paulo, Kiev, New York and Buenos Aires.

</doc>
<doc id="10018" url="https://en.wikipedia.org/wiki?curid=10018" title="Edsger W. Dijkstra">
Edsger W. Dijkstra

Edsger Wybe Dijkstra (; 11 May 1930 – 6 August 2002) was a Dutch computer scientist. A theoretical physicist by training, he worked as a programmer at the Mathematisch Centrum (Amsterdam) from 1952 to 1962. He was a professor of mathematics at the Eindhoven University of Technology (1962–1984) and a research fellow at the Burroughs Corporation (1973–1984). He held the Schlumberger Centennial Chair in Computer Sciences at the University of Texas at Austin from 1984 until 1999, and retired as Professor Emeritus in 1999.
One of the most influential members of computing science's founding generation, Dijkstra helped shape the new discipline from both an engineering and a theoretical perspective. His fundamental contributions cover diverse areas of computing science, including compiler construction, operating systems, distributed systems, sequential and concurrent programming, programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, graph algorithms, and philosophical foundations of computer science and computer programming. Many of his papers are the source of new research areas. Several concepts and problems that are now standard in computer science were first identified by Dijkstra and/or bear names coined by him.
Computer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike physics there were no theoretical concepts or coding systems. Dijkstra was one of the moving forces behind the acceptance of computer programming as a scientific discipline. A training background in mathematics and physics led to his applying similar disciplines of mathematical logic and methodology to computer programming. In 1968, computer programming was in a state of crisis. Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra coined the phrase "structured programming" and during the 1970s this became the new programming orthodoxy. Dijkstra's ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects.
Dijkstra was an early theoretical pioneer in many research areas of computing science, including algorithm design, programming methodology, and software architecture. The academic study of concurrent computing and concurrent programming started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem. He was also one of the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion (mutex), deadlock (deadly embrace), finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.
Biography.
Early years.
Edsger W. Dijkstra was born in Rotterdam. His father was a chemist who was president of the Dutch Chemical Society; he taught chemistry at a secondary school and was later its superintendent. His mother was a mathematician, but never had a formal job.
Dijkstra had considered a career in law during his last years in high school. He had hoped to represent the Netherlands in the United Nations. He graduated in 1948 with the highest possible marks in mathematics, physics, chemistry, and biology. So his teachers and parents counseled him to seek a career in the sciences. He decided to join the University of Leiden, to study mathematics and physics during the first years, and theoretical physics later on.
In the early 1950s, electronic computers were a novelty. Dijkstra stumbled on his career quite by accident. His father had seen an advertisement for a three-week course in programming for the electronic computer EDSAC, to be given at Cambridge University, and he advised his son to attend the course. Dijkstra felt that computers might become an important tool for theoretical physicists, and he decided to attend this course in September 1951. When he asked his supervisor, Professor dr. A. Haantjes, for a letter of recommendation, it turned out that Professor Haantjes knew Adriaan van Wijngaarden, the director of the Computation Department at the Mathematical Center in Amsterdam, and he knew that van Wijngaarden had taken the same course in 1950. Professor Haantjes phoned van Wijngaarden who invited Dijkstra to visit the Mathematical Center and wrote the letter of recommendation. Van Wijngaarden also offered Dijkstra a job, which he accepted, becoming officially the Netherlands' first "programmer" in March 1952.
In 1956 he graduated from the Leiden University in mathematics and theoretical physics. In 1959 he received his PhD from the University of Amsterdam for his thesis entitled 'Communication with an Automatic Computer', devoted to a description of the assembly language designed for the first commercial computer developed in the Netherlands, the X1. It also dealt with the concept of an interrupt, a novelty at that time. His PhD thesis supervisor was van Wijngaarden.
For some time Dijkstra remained committed to physics, working on it in Leiden three days out of each week. With increasing exposure to computing, however, his focus began to shift. As he recalled:
When Dijkstra married Maria (Ria) C. Debets in 1957, he was required as a part of the marriage rites to state his profession. He stated that he was a programmer, which was unacceptable to the authorities, there being no such profession at that time in The Netherlands.
Mathematisch Centrum, Amsterdam.
From 1952 until 1962 Dijkstra worked at the Mathematisch Centrum in Amsterdam. The early computers were built out of vacuum tubes. They occupied large amounts of space, consumed power voraciously, and failed regularly. As Dijkstra recalls, "In retrospect one can only wonder that those first machines worked at all, at least sometimes. The overwhelming problem was to get and keep the machine in working order."
He worked closely with Bram Jan Loopstra and Carel S. Scholten, who had been hired to build a computer. Their mode of interaction remains a model of disciplined engineering: They would first decide upon the interface between the hardware and the software, by writing a programming manual. Then the hardware designers would have to be faithful to their part of the contract, while Dijkstra, the programmer, would write software for the nonexistent machine. Two of the lessons he learned from this experience were the importance of clear documentation, and that program debugging can be largely avoided through careful design.
Dijkstra formulated and solved the shortest path problem for a demonstration at the official inauguration of the ARMAC computer in 1956, but —because of the absence of journals dedicated to automatic computing— did not publish the result until 1959.
At the Mathematical Center, Dijkstra and his colleague J. A. Zonneveld developed a compiler for the programming language ALGOL 60; it had a profound influence on his later thinking on programming as a scientific activity. He and Zonneveld had completed the implementation of the first ALGOL 60 compiler by August 1960, more than a year before a compiler was produced by another group.
Eindhoven University of Technology.
In 1962 Dijkstra moved to Eindhoven in the south of the Netherlands, where he became a professor in the Mathematics Department at the Eindhoven University of Technology. Then in 1964 Dijkstra and his family moved to a newly built house in Nuenen, a small town/village on the outskirts of Eindhoven.
The university did not have a separate computer science department and the culture of the mathematics department did not particularly suit him. Dijkstra tried to build a group of computer scientists who could collaborate on solving problems. This was an unusual model of research for the Mathematics Department.
In the late 1960s he built the THE operating system (named for the university, then known as Technische Hogeschool Eindhoven), which has influenced the designs of subsequent operating systems.
Burroughs Corporation.
Dijkstra joined Burroughs Corporation, a company known at that time for the production of computers based on an innovative hardware architecture, as its Research Fellow in August 1973. His duties consisted of visiting some of the company's research centers a few times a year and carrying on his own research, which he did in the smallest Burroughs research facility, namely, his study on the second floor of his house in Nuenen. In fact, Dijkstra was the only research fellow of Burroughs Corporation and worked for it from home, occasionally travelling to its branches in the United States. As a result he reduced his appointment at the university to one day a week. That day, Tuesday, soon became known as the day of the famous 'Tuesday Afternoon Club', a seminar during which he discussed with his colleagues scientific articles, looking at all aspects – notation, organisation, presentation, language, content, etc. Shortly after he moved in 1984 to the University of Texas at Austin (USA), a new 'branch' of the Tuesday Afternoon Club emerged in Austin.
He was already famous by that time, and he received a large number of invitations to lecture throughout the world. He used these visits to interact with other computer scientists, mentor younger scientists, and sharpen his skills as an English speaker.
The Burroughs years saw him at his most prolific in output of research articles. He wrote nearly 500 documents in the EWD series (described below), most of them technical reports, for private circulation within a select group.
The University of Texas at Austin.
Dijkstra accepted the Schlumberger Centennial Chair in the Computer Science Department at the University of Texas at Austin in 1984. He had been visiting the Burroughs Research Center in Austin since the late 1970s and had become quite familiar with the Department of Computer Science and its faculty. As he says in an autobiographical note, "...when The University of Texas at Austin offered me the Schlumberger Centennial Chair in Computer Sciences, the offer was actually welcome. The offer was not so surprising, nor my acceptance, for I knew Austin, I knew UT and they knew me. All through my years at Burroughs I had made it a rule to visit, wherever I went, the local university and since the late 70s Burroughs had had an Austin Research Center, a visit to which was included in most of my trips to the USA." Dijkstra remained prolific in research during his years at Austin. He had embarked on a long-term project on "Streamlining Mathematical Arguments".
The years in Austin saw Dijkstra at his best as a teacher and mentor for a generation of students, both undergraduate and graduate. From his days in the Eindhoven University of Technology, he had thought deeply about how computer science should be taught. He had sharpened his thinking on education over a number of years, and Austin provided him the opportunity for trying out his ideas.
Last years.
Dijkstra worked in Austin until his retirement in November 1999. To mark the occasion and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, which took place on his 70th birthday in May 2000.
Dijkstra and his wife returned from Austin to his original house in Nuenen (Netherlands) where he found that he had only months to live. He said that he wanted to retire in Austin, Texas, but to die in the Netherlands. Dijkstra died on 6 August 2002 after a long struggle with cancer. He and his wife Maria (Ria) Debets were survived by their three children: Marcus, Femke and the computer scientist Rutger M. Dijkstra.
Scientific contributions and impacts.
As an early theoretical pioneer in many research areas of computing science, Dijkstra helped shape the new discipline from both an engineering and an academic perspective. Many of his papers are the source of new research areas. Many concepts that are now standard in computer science were first identified by Dijkstra and/or bear names coined by him. Several important problems were also first formulated and solved by him. A 1994 survey of over a thousand professors of computer science was conducted to obtain a list of 38 most influential scholarly papers in the field, and Dijkstra is the author of five papers.
During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra made numerous seminal contributions to many areas of computing science, including compiler construction, operating systems, concurrent programming/concurrent computing, distributed computing/distributed programming, programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, algorithm design, and philosophical foundations of computer science and computer programming. In addition, Dijkstra was intensely interested in teaching computer science, and in the relationships between academic computing science and the software industry.
Algorithmic work.
Dijkstra's algorithmic work (especially graph algorithms, concurrent algorithms, and distributed algorithms) plays an important role in many areas of computing science. According to Leslie Lamport (2002), Dijkstra "started the field of concurrent and distributed algorithms with his 1965 CACM paper "Solution of a Problem in Concurrent Programming Control", in which he first stated and solved the mutual exclusion problem." As Lamport explains, "that paper is probably why PODC exists (...). It remains to this day the most influential paper in the field. That it did not win a PODC Influential Paper Award reflects an artificial separation between concurrent and distributed algorithms–a separation that has never existed in Dijkstra's work."
In 1959 Dijkstra published in a 3-page article 'A note on two problems in connexion with graphs' the algorithm to find the shortest path in a graph, now called Dijkstra's algorithm. Its impact over the next 40 years is summarised from the article of Mikkel Thorup, 'Undirected Single Source Shortest Paths with Positive Integer Weights in Linear Time' (1999): "Since 1959, all theoretical developments in SSSP Shortest Paths for general directed and undirected graphs have been based on Dijkstra's algorithm." Dijkstra's algorithm is used in SPF, Shortest Path First, which is used in the routing protocols OSPF and IS-IS. Various modifications to Dijkstra's algorithm have been proposed by many authors using heuristics to reduce the run time of shortest path search. One of the most used heuristic algorithms is the A* search algorithm, the main goal is to reduce the run time by reducing the search space. A* search algorithm (first described by Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute in 1968) is an extension of Dijkstra's 1959 algorithm. Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm in about 20 minutes without aid of paper and pen and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in Netherland (64 cities, so that 6 bits would suffice to represent the city in the algorithm).
A year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm. The Prim's algorithm was originally developed in 1930 by Czech mathematician Vojtěch Jarník and later independently rediscovered and republished by Robert C. Prim in 1957 and Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm.
In 1961 Dijkstra first described the shunting-yard algorithm, a method for parsing mathematical expressions specified in infix notation, in the Mathematisch Centrum report. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was named the "shunting yard" algorithm because its operation resembles that of a railroad shunting yard. The shunting-yard algorithm is commonly used to implement operator-precedence parsers.
In 1962 or 1963 Dijkstra proposed the semaphore mechanism for mutual exclusion algorithm for n processes (a generalization of Dekker's algorithm), which was probably the first published concurrent algorithm and which introduced a new area of algorithmic research. He also identified the deadlock problem and proposed the banker's algorithm that prevents deadlock.
In 1974 Dijkstra presented three self-stabilizing algorithms for mutual exclusion on a ring. Dijkstra's work is considered to be the first to introduce and demonstrate the self-stabilization concept.
In the mid-1970s Dijkstra (together with other authors) introduced two useful abstractions (mutator and collector) to the study of garbage collection. The mutator abstracts the process that performs the computation, including allocation of a new storage cell. The collector is the process that automatically reclaims garbage. Furthermore this paper gives a formalization of "tri-color marking" that is basic to incremental garbage collection.
In the early 1980s Dijkstra and Carel S. Scholten proposed the Dijkstra–Scholten algorithm for detecting termination in distributed systems.
In 1981 Dijkstra developed smoothsort, a comparison-based sorting algorithm and a variation of heapsort.
Compiler construction and programming language research.
Dijkstra was known to be a fan of ALGOL 60, and worked on the team that implemented the first compiler for that language. He was closely involved in the ALGOL 60 development, realisation and popularisation. As discussed by Peter Naur in the article 'The European side of the last phase of the development of ALGOL 60', in the "Proceedings of the First ACM SIGPLAN Conference on History of Programming Languages", January 1978, Dijkstra took part in the period 1958–1959 in a number of meetings that culminated in the publication of the report defining the ALGOL 60 language. Dijkstra's name does not appear in the list of 13 authors of the final report. Apparently, he eventually left the committee because he could not agree with the majority opinions. Still, while at the Mathematisch Centrum (Amsterdam), he wrote jointly with Jaap Zonneveld the first ALGOL 60 compiler. Dijkstra and Zonneveld, who collaborated on the compiler, agreed not to shave until the project was completed; while Zonneveld shaved shortly thereafter, Dijkstra kept his beard for the rest of his life.
ALGOL was the result of a collaboration of American and European committees. ALGOL 60 (short for ALGOrithmic Language 1960) is a member of the ALGOL family of computer programming languages. It followed on from ALGOL 58 and inspired many languages that followed it. It gave rise to many other programming languages, including BCPL, B, Pascal, Simula and C. Algol 60 was a sophisticatedly designed computer language and it provided a large number of hitherto unknown implementation challenges. As Bjarne Stroustrup notes, "one problem with Algol60 was that no one knew how to implement it." A major new challenge in Algol 60 implementation was the run-time allocation and management of data. In 1960 Dijkstra and Zonneveld showed how recursive procedures could be executed using a run-time stack of activation records, and how to efficiently access identifiers from statically enclosing scopes using the so-called 'display'. The ALGOL 60 compiler was one of the first to support recursion employing a novel method to do so. Dijkstra's short book "Primer of Algol 60 Programming", originally published in 1962, was the standard reference for the language for several years.
Programming paradigm and methodology.
Computer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike mature sciences there were no theoretical concepts or coding systems. Programming as a professional activity was poorly understood in those years. In Dijkstra's own words:
In the late 1960s computer programming was in a state of crisis. Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were neither sufficient nor up to the mark. The term "software crisis" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany. Edsger Dijkstra's 1972 ACM Turing Award Lecture makes reference to this same problem: "The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem."
While Dijkstra had programmed extensively in machine code in the 1950s, he came to the conclusion that in high-level languages frequent use of the GOTO statement was usually symptomatic of poor structure. In 1968 he wrote a private paper "A Case against the GO TO Statement", which was then published as a letter in CACM. Editor Niklaus Wirth gave this letter the heading "Go To Statement Considered Harmful", which introduced the phrase "considered harmful" into computing.
Dijkstra argued that the programming statement GOTO, found in many high-level programming languages, is a major source of errors, and should therefore be eliminated. This letter caused a huge debate in the programming community. Some went to the length of equating good programming with the elimination of GO TO. Dijkstra refused to mention the debate, or even the GO TO statement, in his seminal article, "Notes on Structured Programming". The debate has long since died down; programming languages provide alternatives to the GO TO, few programmers today use it liberally, and most never use it at all. Fortran introduced structured programming constructs in 1978 and in successive revisions the relatively loose semantic rules governing the allowable use of goto were tightened; the "extended range" in which a programmer could use a GOTO to enter and leave a still-executing DO loop was removed from the language in 1978, and by 1995 several forms of Fortran GOTO, including the Computed GOTO and the Assigned GOTO, had been deleted from the language. Some widely used modern programming languages, such as Java and Python lack the GOTO statement, though most provide some means of breaking out of a selection, or either breaking out of or moving on to the next step of an iteration.
Dijkstra's thesis was that departures from linear control flow were clearer if allowed only in disciplined higher-level structures such as the if-then-else statement and the while loop. This methodology was developed into structured programming movement, the title of his 1972 book, coauthored with C.A.R. Hoare and Ole-Johan Dahl. Considered by many as the first significant movement in history of computer programming, structured programming became the new programming orthodoxy during the 1970s. Before this decade of intense focus, programming was regarded as a private, puzzle-solving activity of writing computer instructions to work as a program. After this decade, programming could be regarded as a public, mathematics-based activity of restructuring specifications into programs. Before, the challenge was in getting programs to run at all, and then in getting them further debugged to do the right things. After, programs could be expected to both run and do the right things with little or no debugging. Before, it was common wisdom that no sizable program could be error-free. After, many sizable programs have run a year or more with no errors detected. These expectations and achievements are not universal because of the inertia of industrial practices. But they are well-enough established to herald fundamental change in software development."</ref> Bertrand Meyer remarked: "The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming."
Structured programming is often regarded as "goto-less programming". But as Bertrand Meyer notes: "As the first book on the topic ["Structured Programming" by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto. Its principal message is that programming should be considered a scientific discipline based on mathematical rigor." As a programming paradigm, structured programming – especially in the 1970s and 1980s – significantly influenced the birth of many modern programming languages such as Pascal, C, Modula-2, and Ada. The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978. The C++ language was a considerably extended and enhanced version of the popular structured programming language C (see also: list of C-based programming languages). Since C++ was developed from a more traditional structured language, it is a 'hybrid language', rather than a pure object-oriented programming language.
Dijkstra was particularly impressed by the dimension of the software problem when he attended the famous 1968 NATO Conference on Software Engineering, which was the first conference devoted to the problem. He claimed that programming as a discipline in contrast to a craft, that programming must no longer be an ad hoc activity, and that programming methodology should become a scientific discipline. Dijkstra formulated some of his early ideas about programming as a mathematical discipline starting around the time of publication of his letter ("Go To Statement Considered Harmful"). He continued to point out that software productivity is closely related to rigor in design, which eliminates software bugs at an early stage.
Program design and development (software engineering research).
Dijkstra's ideas about programming methodology (especially the structured programming movement) helped lay the foundations for the birth and development of the professional discipline of software engineering (in particular the software design and development), enabling programmers to organize and manage increasingly complex software projects. In the late 1960s Dijkstra discussed the concept of program families. And in the mid 1970s David Parnas and others clarified the idea and showed how to apply it in software engineering principles.
Dijkstra sent "Notes on Structured Programming", later to be one of his seminal works, to a few friends soliciting their comments. This note became a sensation, and major corporations initiated programs based on his ideas to integrate rigorous practices into their programming projects. Dijkstra had advocated certain design principles, which have now become completely accepted in the computer science community: Large systems should be constructed out of many smaller components; each component should be defined only by its interface and not by its implementation; smaller components may be designed following a similar process of decomposition, thus leading to a top-down style of design; the design should start by enumerating the "separable concerns" and by considering each group of concerns independently of the others; and mathematical logic is and must be the basis for software design. This work has had far-reaching impact on all areas of computer science, from teaching of the very first course in programming to the designs of complex software. Mathematical analysis of designs and specifications have become central activities in computer science research.
The rise of the structured programming movement led to many other "structured" approaches applied to software design. The techniques of structured analysis and structured design are outgrowths of structured programming concepts and techniques, and of the early ideas about modular design. Principles of modularity were strengthened by Larry Constantine's concepts of coupling (to be minimized between modules) and cohesion (to be maximized within modules), by David Parnas's techniques of information hiding, and by abstract data types. A number of tools and methods employing structured concepts were developed, such as Structured Design, Jackson's Structured Programming, Ross' Structured Analysis and Design Technique (SADT), Yourdon's Structured Method, Structured Systems Analysis and Design Method (SSADM), and James Martin's Information Engineering. The field of software metrics is often considered as a direct influence of the structured programming movement on software engineering in the 1970s.
Separation of concerns (SoC), one of the basic principles in software engineering, is a design principle for separating a computer program into distinct sections, such that each section addresses a separate concern. The term "separation of concerns" was coined by Dijkstra in his 1974 paper "On the role of scientific thought".
Operating system research.
In the 1960s Dijkstra and his colleagues in Eindhoven designed and implemented THE (standing for 'Technische Hogeschool Eindhoven') operating system, which was organised into clearly identified layers. Layering has proved itself in the operating system domain; however, the same benefits are available when applied to e-commerce or to thin client–oriented applications. Layered architectures have become essential in supporting the iterative development process by promoting reusability, scalability, and maintainability."</ref> His 1968 article on this subject provided the foundation for subsequent designs of the operating systems.
Dijkstra organized the design of the system in layers in order to reduce the overall complexity of the software. Though the term 'architecture' had not yet been used to describe software design, this was certainly considered the first glimpse of software architecture. It introduced a number of design principles which have become part of the working vocabulary of every professional programmer: levels of abstraction, programming in layers, the semaphore, and cooperating sequential processes. His original paper on the THE operating system was reprinted in the 25th Anniversary issue of Communications of the ACM, in January 1983. By way of introduction, the Editor-in-Chief says, "This project initiated a long line of research in multilevel systems architecture — a line that continues to the present day because hierarchical modularity is a powerful approach to organizing large systems."
Concurrent computing and programming.
In a one-page paper from 1965 Dijkstra introduced the 'mutual exclusion problem' for n processes and discussed a solution to it. It was probably the first published concurrent algorithm. The notion, standard by now, of a 'critical section' was also introduced in this paper. Per Brinch Hansen, a pioneer in the field of concurrent computing, considers Dijkstra's "Cooperating Sequential Processes" (1965) to be the first classic paper in concurrent programming. As Brinch Hansen notes, 'Dijkstra lays the conceptual foundation for abstract concurrent programming' with that paper.
In 1968 Dijkstra published his seminal paper 'Cooperating sequential processes', a 70-page essay that originated the field of concurrent programming. He discussed in it the notion of mutual exclusion (mutex) and the criteria a satisfactory solution should satisfy. He also redressed the historical perspective left out of his 1965 paper by including the first known correct solution to the mutual exclusion problem, for two processes, due to Theodorus Dekker. Dijkstra subsequently generalized Dekker's solution to n processes. Further, he proposed the first synchronisation mechanism for concurrent processes, the semaphore with its two operations, P and V. He also identified the 'deadlock problem' (called there 'the problem of the deadly embrace') and proposed an elegant 'Banker's algorithm' that prevents deadlock. The deadlock detection and prevention became perennial research problems in the field of concurrent programming.
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently.
The dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them. It was originally formulated in 1965 by Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals. Soon after, Tony Hoare gave the problem its present formulation. Several years after Dijkstra posed the problem, he was surprised to find that the Massachusetts Institute of Technology designers of the sophisticated MULTICS operating system had not thought about deadlock at all, and the system would, on occasion, abruptly stop – like so many philosophers each holding a single chopstick. With gentle irony, he recalled: "You can hardly blame M.I.T. for not taking notice of an obscure computer scientist in a small town (Nuenen) in the Netherlands." The sleeping barber problem, a classic inter-process communication and synchronization problem between multiple operating system processes, is often attributed to Dijkstra.
Distributed computing.
Dijkstra was one of the very early pioneers of the research on principles of distributed computing. As the citation for the Dijkstra Prize recognizes, "no other individual has had a larger influence on research in principles of distributed computing." Some of his papers are even considered to be those that established the field. Dijkstra's 1965 paper, "Solution of a Problem in Concurrent Programming Control" was the first to present the correct solution to the mutual exclusion problem. Leslie Lamport writes that this work "is probably why PODC exists" and it "started the field of concurrent and distributed algorithms".
In particular, his paper "Self-stabilizing Systems in Spite of Distributed Control" (1974) started the sub-field of self-stabilization. It is also considered as the first scientific examination of fault-tolerant systems. Dijkstra's paper was not widely noticed until Leslie Lamport's invited talk at the ACM Symposium on Principles of Distributed Computing (PODC) in 1983. In his report on Dijkstra's work on self-stabilizing distributed systems, Lamport regard it to be 'a milestone in work on fault tolerance' and 'a very fertile field for research', as can be seen by studying the book "Self-stabilization" (2000) of Shlomi Dolev and by browsing through the proceedings of the annual workshops on self-stabilizing systems. Self-stabilization is related to autonomic computing, which entails several "self-*" attributes: self-management, self-configuration, self-healing, self-optimization, and self-protection.
In the early 1980s he published two small papers on the problem of detecting termination in distributed systems. The first one, four pages long, was written with Carel Scholten; the second, three pages long, with Wim Feijen and Netty van Gasteren.
Formal specification and verification.
From the 1970s, Dijkstra's chief interest was formal verification. The prevailing opinion at the time was that one should first write a program and then provide a mathematical proof of correctness. Dijkstra objected, noting that the resulting proofs are long and cumbersome and give no insight on how the program was developed. An alternative method is "program derivation", to "develop proof and program hand in hand". One starts with a mathematical "specification" of what a program is supposed to do and applies mathematical transformations to the specification until it is turned into a program that can be executed. The resulting program is then known to be "correct by construction". Much of Dijkstra's later work concerns ways to streamline mathematical argument.
In 1976 Dijkstra published a seminal book, "A Discipline of Programming", which put forward his method of systematic development of programs together with their correctness proofs. In his exposition he used his 'Guarded Command Language'. The language, with its reliance on non-determinism, the adopted weakest precondition semantics and the proposed development method has had a considerable impact on the field to this day. The refinement calculus, originally proposed by Ralph-Johan Back and developed by Carroll Morgan, is an extension of Dijkstra's weakest precondition calculus, where program statements are modeled as predicate transformers.
In 1984, to add further support to this approach to programming, he published jointly with Wim Feijen an introductory textbook for first-year students of computer science. The book, first published in Dutch, was entitled "Een methode van programmeren". The English edition appeared in 1988 as "A Method of Programming".
Then in 1990 Dijkstra published his book "Predicate Calculus and Program Semantics" with Carel S. Scholten. The book was devoted to logical and mathematical analysis of his weakest precondition semantics with a long prelude concerning predicate calculus.
On the nature of computer science and computer programming.
Dijkstra wrote a large number of essays on the nature of computer science research and education, on its relationship to the industry and society, and even on the manner in which scientific articles should be written and presented.
In his 1988 paper "On the Cruelty of Really Teaching Computer Science", Dijkstra argues that computer programming should be understood as a branch of mathematics, and that the formal provability of a program is a major criterion for correctness.
In a 2001 interview, he stated a desire for "elegance", whereby the correct approach would be to process thoughts mentally, rather than attempt to render them until they are complete. The analogy he made was to contrast the compositional approaches of Mozart and Beethoven.
Many of his opinions on computer science and programming have become widespread. For example, the programming phrase "two or more, use a for" (a rule of thumb when to use a loop) is sometimes attributed to him.
He was the first to make the claim that programming is so inherently complex that, in order to manage it successfully, programmers need to harness every trick and abstraction possible.
Dijkstra was one of the most famous opponents of the engineering view of computing science. Like Peter Naur and Kristen Nygaard, Dijkstra dislikes the very term 'computer science'. Computer science, as Dijkstra pointed out, deserves a better name. He suggests it can be called 'computing science'. Instead of the computer, or computing technology, Dijkstra wanted to emphasize the abstract mechanisms that computing science uses to master complexity. When expressing the abstract nature of computing science, he wrote,
In "The Humble Programmer" (1972), Dijkstra wrote: "We must not forget that it is not our scientists' business to make programs, it is our business to design classes of computations that will display a desired behaviour."
Dijkstra also opposed the inclusion of software engineering under the umbrella of academic computer science. He wrote that, "As economics is known as 'The Miserable Science', software engineering should be known as 'The Doomed Discipline', doomed because it cannot even approach its goal since its goal is self-contradictory." And "software engineering has accepted as its charter 'How to program if you cannot.
Personality and working style.
In the world of computing science, Dijkstra is well-known as a "character". In the preface of his book "A Discipline of Programming" (1976) he stated the following: "For the absence of a bibliography I offer neither explanation nor apology." In fact, most of his articles and books have no references at all. This approach to references was deplored by some researchers. But Dijkstra chose this way of working to preserve his self-reliance.
As a university professor for much of his life, Dijkstra saw teaching not just as a required activity but as a serious research endeavor. His approach to teaching was unconventional. His lecturing style has been described as idiosyncratic. When lecturing, the long pauses between sentences have often been attributed to the fact that English is not Dijkstra's first language. However the pauses also served as a way for him to think on his feet and he was regarded as a quick and deep thinker while engaged in the act of lecturing. His courses for students in Austin had little to do with computer science but they dealt with the presentation of mathematical proofs. At the beginning of each semester he would take a photo of each of the students, in order to memorize their names. He never followed a textbook, with the possible exception of his own while it was under preparation. When lecturing, he would write proofs in chalk on a blackboard rather than using overhead foils. He invited the students to suggest ideas, which he then explored, or refused to explore because they violated some of his tenets. He assigned challenging homework problems, and would study his students' solutions thoroughly. He conducted his final examinations orally, over a whole week. Each student was examined in Dijkstra's office or home, and an exam lasted several hours.
He was also highly original in his way of assessing people's capacity for a job. When Vladimir Lifschitz came to Austin in 1990 for a job interview, Dijkstra gave him a puzzle. Vladimir solved it and has been working in Austin since then.
Despite having invented much of the technology of software, Dijkstra eschewed the use of computers in his own work for many decades. Even after he succumbed to his UT colleagues' encouragement and acquired a Macintosh computer, he used it only for e-mail and for browsing the World Wide Web. Dijkstra never wrote his articles using a computer. He preferred to rely on his typewriter and later on his Montblanc pen. Dijkstra's favorite writing instrument was the Montblanc Meisterstück fountain pen. He repeatedly tried other pens, but none ever displaced the Montblanc.
He had no use for word processors, believing that one should be able to write a letter or article without rough drafts, rewriting, or any significant editing. He would work it all out in his head before putting pen to paper, and once mentioned that when he was a physics student he would solve his homework problems in his head while walking the streets of Leiden.
Most of Dijkstra's publications were written by him alone. He never had a secretary and took care of all his correspondence alone. When colleagues prepared a Festschrift for his sixtieth birthday, published by Springer-Verlag, he took the trouble to thank each of the 61 contributors separately, in a hand-written letter.
Throughout Dijkstra's career, his work was characterized by elegance and economy. A prolific writer (especially as an essayist), Dijkstra authored more than 1,300 papers, many written by hand in his precise script. They were essays and parables; fairy tales and warnings; comprehensive explanation and pedagogical pretext. Most were about mathematics and computer science; others were trip reports that are more revealing about their author than about the people and places visited. It was his habit to copy each paper and circulate it to a small group of colleagues who would copy and forward the papers to another limited group of scientists. His love affair with simplicity came at an early age and under his mother's guidance. He once said he had asked his mother whether mathematics was a difficult topic. She replied that he must learn all the formulas and that furthermore if he required more than five lines to prove something, he was on the wrong track.
Dijkstra was famous for his wit, eloquence, and way with words, such as in his remark, "The question of whether Machines Can Think (…) is about as relevant as the question of whether Submarines Can Swim."; his advice to a promising researcher, who asked how to select a topic for research, "Do only what only you can do". Dijkstra was also known for his vocal criticism. As an outspoken and critical visionary, he strongly opposed the teaching of BASIC.
In many of his more humorous essays, Dijkstra described a fictional company of which he served as chairman. The company was called Mathematics, Inc., a company that he imagined having commercialized the production of mathematical theorems in the same way that software companies had commercialized the production of computer programs. He invented a number of activities and challenges of Mathematics Inc. and documented them in several papers in the EWD series. The imaginary company had produced a proof of the Riemann Hypothesis but then had great difficulties collecting royalties from mathematicians who had proved results assuming the Riemann Hypothesis. The proof itself was a trade secret. Many of the company's proofs were rushed out the door and then much of the company's effort had to be spent on maintenance. A more successful effort was the Standard Proof for Pythagoras' Theorem, that replaced the more than 100 incompatible existing proofs. Dijkstra described Mathematics Inc. as "the most exciting and most miserable business ever conceived". EWD 443 (1974) describes his fictional company as having over 75 percent of the world's market share.
EWDs.
Dijkstra was well-known for his habit of carefully composing manuscripts with his fountain pen. The manuscripts are called EWDs, since Dijkstra numbered them with "EWD", his initials, as a prefix. According to Dijkstra himself, the EWDs started when he moved from the Mathematical Centre in Amsterdam to the Eindhoven University of Technology (then Technische Hogeschool Eindhoven). After going to Eindhoven, Dijkstra experienced a writer's block for more than a year. Dijkstra distributed photocopies of a new EWD among his colleagues. Many recipients photocopied and forwarded their copies, so the EWDs spread throughout the international computer science community. The topics were computer science and mathematics, and included trip reports, letters, and speeches. These short articles span a period of 40 years. Almost all EWDs appearing after 1972 were hand-written. They are rarely longer than 15 pages and are consecutively numbered. The last one, No. 1318, is from 14 April 2002. Within computer science they are known as the EWD reports, or, simply the EWDs. More than 1300 EWDs have been scanned, with a growing number transcribed to facilitate search, and are available online at the Dijkstra archive of the University of Texas.
Personal life.
Dijkstra's self-confidence went together with a remarkably modest lifestyle, to the point of being spartan. His and his wife's house in Nuenen is simple, small and unassuming. He did not own a TV, a VCR or a mobile telephone, and did not go to the movies. In contrast, he played the piano well and, while in Austin, liked to go to concerts. An enthusiastic listener of classical music, Dijkstra's favorite composer was Mozart.
Influence and recognition.
In 1972 the Association for Computing Machinery (ACM) acknowledged Dijkstra's seminal contributions to the field by awarding him the distinguished Turing Award. The citation for the award reads:
The introduction given at the awards ceremony is a tribute to Dijkstra:
In the words of Sir Tony Hoare, FRS, delivered by him at Dijkstra's funeral:
In March 2003, the following email was sent to the distributed computing community:
Former ACM President Peter J. Denning wrote about Dijkstra:
Awards and honors.
Among Dijkstra's awards and honors are:
The Distinguished Fellowship of the British Computer Society (BCS) is awarded under bylaw 7 of the BCS's Royal Charter. The award was first approved in 1969 and the first election was made in 1971 to Dijkstra.
On the occasion of Dijkstra's 60th birthday in 1990, The Department of Computer Science (UTCS) at the University of Texas at Austin organized a two-day seminar in his honor. Speakers came from all over the United States and Europe, and a group of computer scientists contributed research articles which were edited into a book.
In 2002, the C&C Foundation of Japan recognized Dijkstra "for his pioneering contributions to the establishment of the scientific basis for computer software through creative research in basic software theory, algorithm theory, structured programming, and semaphores." Dijkstra was alive to receive notice of the award, but it was accepted by his family in an award ceremony after his death.
Shortly before his death in 2002, Dijkstra received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.
The Dijkstra Award for Outstanding Academic Achievement in Computer Science (Loyola University Chicago, Department of Computer Science) is named for Edger W. Dijkstra. Beginning in 2005, this award recognizes the top academic performance by a graduating computer science major. Selection is based on GPA in all major courses and election by department faculty.
The Department of Computer Science (UTCS) at the University of Texas at Austin hosted the inaugural Edsger W. Dijkstra Memorial Lecture on October 12, 2010. Tony Hoare, Emeritus Professor at Oxford and Principal Researcher at Microsoft Research, was the speaker for the event. This lecture series was made possible by a generous grant from Schlumberger to honor the memory of Dijkstra.
Bibliography.
Primary sources.
Books:
Articles, a selection:
Secondary sources.
Books:
Articles:

</doc>
<doc id="10021" url="https://en.wikipedia.org/wiki?curid=10021" title="Educational perennialism">
Educational perennialism

Educational perennialism is a normative educational philosophy. Perennialists believe that one should teach the things that are of everlasting pertinence to all people everywhere, and that the emphasis should be on principles, not facts. Since people are human, one should teach first about humans, rather than machines or techniques and liberal rather than vocational topics.
Although perennialism may appear similar to essentialism, perennialism focuses first on personal development, while essentialism focuses first on essential skills. Essentialist curricula thus tend to be much more vocational and fact-based, and far less liberal and principle-based. Both philosophies are typically considered to be teacher-centered, as opposed to student-centered philosophies of education such as progressivism. However, since the teachers associated with perennialism are in a sense the authors of the Western masterpieces themselves, these teachers may be open to student criticism through the associated Socratic method, which, if carried out as true dialogue, is a balance between students, including the teacher promoting the discussion.
Secular perennialism.
The word perennial in secular perennialism suggests something that lasts an indefinitely long time, recurs again and again, or is self-renewing. As promoted primarily by Robert Hutchins and Mortimer Adler, a universal curriculum based upon the common and essential nature of all human beings is recommended. This form of perennialism comprises the humanist and scientific traditions. Hutchins and Adler implemented these ideas with great success at the University of Chicago, where they still strongly influence the curriculum in the form of the undergraduate Common Core. Other notable figures in the movement include Stringfellow Barr and Scott Buchanan (who together initiated the Great Books program at St. John's College in Annapolis, Maryland), Mark Van Doren, Alexander Meiklejohn, and Sir Richard Livingstone, an English classicist with an American following.
Secular perennialists espouse the idea that education should focus on the historical development of a continually developing common western base of human knowledge and art, the timeless value of classic thought on central human issues by landmark thinkers, and revolutionary ideas critical to historical western paradigm shifts or changes in world view. A program of studies which is highly general, nonspecialized, and nonvocational is advocated. They firmly believe that exposure of all citizens to the development of thought by those most responsible for the evolution of the Western tradition is integral to the survival of the freedoms, human rights and responsibilities inherent to a true Democracy.
Adler states: 
... our political democracy depends upon the reconstitution of our schools. Our schools are not turning out young people prepared for the high office and the duties of citizenship in a democratic republic. Our political institutions cannot thrive, they may not even survive, if we do not produce a greater number of thinking citizens, from whom some statesmen of the type we had in the 18th century might eventually emerge. We are, indeed, a nation at risk, and nothing but radical reform of our schools can save us from impending disaster... Whatever the price... the price we will pay for not doing it will be much greater.
Hutchins writes in the same vein: 
The business of saying ... that people are not capable of achieving a good education is too strongly reminiscent of the opposition of every extension of democracy. This opposition has always rested on the allegation that the people were incapable of exercising the power they demanded. Always the historic statement has been verified: you cannot expect the slave to show the virtues of the free man unless you first set him free. When the slave has been set free, he has, in the passage of time, become indistinguishable from those who have always been free ... There appears to be an innate human tendency to underestimate the capacity of those who do not belong to "our" group. Those who do not share our background cannot have our ability. Foreigners, people who are in a different economic status, and the young seem invariably to be regarded as intellectually backward ...
As with the essentialists, perennialists are educationally conservative in the requirement of a curriculum focused upon fundamental subject areas, but stress that the overall aim should be exposure to history's finest thinkers as models for discovery. The student should be taught such basic subjects as English, languages, history, mathematics, natural science, philosophy, and fine arts. Adler states: "The three R's, which always signified the formal disciplines, are the essence of liberal or general education."
Secular perennialists agree with progressivists that memorization of vast amounts of factual information and a focus on second-hand information in textbooks and lectures does not develop rational thought. They advocate learning through the development of meaningful conceptual thinking and judgement by means of a directed reading list of the profound, aesthetic, and meaningful great books of the Western canon. These books, secular perennialists argue, are written by the world's finest thinkers, and cumulatively comprise the "Great Conversation" of mankind with regard to the central human questions. Their basic argument for the use of original works (abridged translations being acceptable as well) is that these are the products of "genius". Hutchins remarks:
Great books are great teachers; they are showing us every day what ordinary people are capable of. These books come out of ignorant, inquiring humanity. They are usually the first announcements for success in learning. Most of them were written for, and addressed to, ordinary people.
It is important to note that the Great Conversation is not static, which is the impression that one might obtain from some descriptions of perennialism, a confusion with religious perennialism, or even the term perennialism itself. The Great Conversation and the set of related great books changes as the representative thought of man changes or progresses, and is therefore representative of an evolution of thought, but is not based upon the whim or fancy of the latest cultural fads. Hutchins makes this point very clear:
In the course of history... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation. ...the West needs to recapture and reemphasize and bring to bear upon its present problems the wisdom that lies in the works of its greatest thinkers and in the discussion that they have carried on.
Perennialism was a solution proposed in response to what was considered by many to be a failing educational system. Again Hutchins writes:
The products of American high schools are illiterate; and a degree from a famous college or university is no guarantee that the graduate is in any better case. One of the most remarkable features of American society is that the difference between the "uneducated" and the "educated" is so slight.
In this regard John Dewey and Hutchins were in agreement. Hutchins's book "The Higher Learning in America" deplored the "plight of higher learning" that had turned away from cultivation of the intellect and toward anti-intellectual practicality due in part, to a lust for money. In a highly negative review of the book, Dewey wrote a series of articles in "The Social Frontier" which began by applauding Hutchins' attack on "the aimlessness of our present educational scheme.
Perennialists believe that reading is to be supplemented with mutual investigations (between the teacher and the student) and minimally-directed discussions through the Socratic method in order to develop a historically oriented understanding of concepts. They argue that accurate, independent reasoning distinguishes the developed or educated mind and they thus stress the development of this faculty. A skilled teacher would keep discussions on topic and correct errors in reasoning, but it would be the class, not the teacher, who would reach the conclusions. While not directing or leading the class to a conclusion, the teacher may work to accurately formulate problems within the scope of the texts being studied.
While the standard argument for utilizing a modern text supports distillation of information into a form relevant to modern society, perennialists argue that many of the historical debates and the development of ideas presented by the great books are relevant to any society, at any time, and thus that the suitability of the great books for instructional use is unaffected by their age.
Perennialists freely acknowledge that any particular selection of great books will disagree on many topics; however, they see this as an advantage, rather than a detriment. They believe that the student must learn to recognize such disagreements, which often reflect current debates. The student becomes responsible for thinking about the disagreements and reaching a reasoned, defensible conclusion. This is a major goal of the Socratic discussions. They do not advocate teaching a settled scholarly interpretation of the books, which would cheat the student of the opportunity to learn rational criticism and to know his own mind.
Religious perennialism.
Perennialism was originally religious in nature, developed first by Thomas Aquinas in the thirteenth century in his work "De Magistro" ("The Teacher").
In the nineteenth century, John Henry Newman presented a detailed defense of educational perennialism in "The Idea of a University". Discourse 5 of that work, "Knowledge Its Own End", is still relevant as a clear statement of a Christian educational perennialism.
There are several epistemological options, which affect the pedagogical options. The possibilities may be surveyed by considering four extreme positions, as indicated in the following table:

</doc>
<doc id="10024" url="https://en.wikipedia.org/wiki?curid=10024" title="MDMA">
MDMA

3,4-Methylenedioxymethamphetamine (MDMA), commonly known as ecstasy (E), is a psychoactive drug used primarily as a recreational drug. Desired effects of MDMA include increased empathy, pleasure, happiness; heightened sensations; altered sense of time, and sexuality. When taken by mouth, effects begin after 30–45 minutes and resolve after 3–6 hours. It can also be snorted or smoked. As of 2016, MDMA has no accepted medical uses.
Adverse effects of MDMA use include addiction, memory problems, paranoia, difficulty sleeping, teeth grinding, blurred vision, sweating, and a rapid heartbeat. --> Use may also lead to depression and fatigue. --> Deaths have been reported due to increased body temperature and dehydration. MDMA increases the release and slows the reuptake of the neurotransmitters serotonin, dopamine, and norepinephrine in parts of the brain — and has stimulant and psychedelic effects. The initial increase is followed by a short-term decrease in the neurotransmitters. --> It is structurally similar to methamphetamine, but it has more in common with the pharmacological effects of amphetamine and hallucinogens.
MDMA was first made in 1912. It was used to improve psychotherapy in the 1970s and become popular as a street drug in the 1980s. MDMA is commonly associated with dance parties, raves, and electronic dance music. It is often sold mixed with other substances such as ephedrine or ketamine. In 2013, between 9 and 28 million people between the ages of 15 and 65 used ecstasy (0.2% to 0.6% of the world population). --> This was broadly similar to the percentage of people who use cocaine, amphetamines, and opioids, but fewer than for cannabis. In the United States, it was used by about 0.9 million people in 2010.
Having or using MDMA is generally illegal in most countries. --> Limited exceptions are sometimes made for research. --> Research is investigating whether a few low doses of MDMA may assist in treating severe, treatment-resistant mental disorders. More research is needed to determine if its usefulness outweighs the risk of harm.
Uses.
Medical.
, MDMA has no accepted medical indications. Previously, it saw limited use in psychiatric counseling.
Alternative medicine.
A small number of therapists continue to use MDMA in counseling despite its illegal status.
Recreational.
MDMA is often considered the drug of choice within the rave culture and is also used at clubs, festivals and house parties. In the rave environment, the sensory effects from the music and lighting are often highly synergistic with the drug. The psychedelic amphetamine quality of MDMA offers multiple reasons for its appeals to users in the rave setting. Some users enjoy the feeling of mass communion from the inhibition-reducing effects of the drug, while others use it as party fuel because of the drug's stimulatory effects.
MDMA is sometimes taken in conjunction with other psychoactive drugs, such as LSD, psilocybin mushrooms, and ketamine. Users sometimes use mentholated products while taking MDMA for its cooling sensation.
MDMA has been used as an adjunct to New Age spiritual practices.
Forms.
MDMA has become widely known as ecstasy (shortened "E", "X", or "XTC"), usually referring to its tablet form, although this term may also include the presence of possible adulterants. The UK term Mandy and the US term Molly colloquially refer to MDMA in a crystalline powder form that is thought to be free of adulterants. However, due to the global supply shortage of sassafras oil, substances that are sold as "Molly" frequently contain no MDMA and instead contain methylone, ethylone, MDPV, mephedrone, or any other of the group of compounds commonly known as bath salts.
Powdered MDMA is typically 30–40% pure, due to bulking agents (e.g., lactose) and binding agents. Tablets sold as ecstasy sometimes only contain 3,4-methylenedioxyamphetamine (MDA) instead of MDMA; the proportion of seized ecstasy tablets with MDMA-like impurities has varied annually and by country.
MDMA is also sold in the form of the hydrochloride salt, either as loose crystals or in gelcaps.
Corporate logos on pills.
A number of ecstasy manufacturers brand their pills with a logo, often being a corporate logo, to help distinguish between suppliers; one of the most notable logos which appeared on pills is the Mitsubishi logo which was very popular during one time.
There have also been sightings of pills with logos of products or shows popular with children, such as Shaun the Sheep.
Effects.
In general, MDMA users begin reporting subjective effects within 30 to 60 minutes of consumption, hitting a peak at about 75 to 120 minutes which plateaus for about 3.5 hours.
The desired short-term psychoactive effects of MDMA have been reported to include:
Adverse effects.
Short-term.
The most serious short-term physical health risks of MDMA are hyperthermia and dehydration. Cases of life-threatening or fatal hyponatremia (excessively low sodium concentration in the blood) have developed in MDMA users attempting to prevent dehydration by consuming excessive amounts of water without replenishing electrolytes.
The immediate adverse effects of MDMA use can include:
Intermediate-term.
The adverse effects that last up to a week following cessation of moderate MDMA use include:
Long-term.
, the long-term effects of MDMA on human brain structure and function have not been fully determined. However, there is evidence of structural and functional deficits in MDMA users with a high lifetime exposure. In contrast, there is no evidence of structural or functional changes in MDMA users with only a moderate (<50 doses used and <100 tablets consumed) lifetime exposure. MDMA use at high doses has been shown to produce brain lesions, a form of brain damage, in the serotonergic neural pathways of humans and animals. It is unclear if typical MDMA users may develop neurotoxic brain lesions. In addition, long-term exposure to MDMA in humans has been shown to produce marked neurotoxicity in striatal, hippocampal, prefrontal, and occipital serotonergic axon terminals. Neurotoxic damage to axon terminals has been shown to persist for more than two years. Brain temperature during MDMA use is positively correlated with MDMA-induced neurotoxicity. Adverse neuroplastic changes to brain microvasculature and white matter also occur in humans using low doses of MDMA. Reduced gray matter density in certain brain structures has also been noted in human MDMA users.
MDMA also produces persistent cognitive impairments in humans. Impairments in multiple aspects of cognition, including memory, visual processing, and sleep have been noted in humans; the magnitude of these impairments is correlated with lifetime ecstasy or MDMA usage. Memory is impacted by ecstasy use, which is associated with impairments in several forms of memory. Some studies indicate repeated recreational users of ecstasy have increased rates of depression and anxiety, even after quitting the drug.
At high doses, MDMA induces a neuroimmune response which, through several mechanisms, increases the permeability of the blood-brain barrier, thereby making the brain more susceptible to environmental toxins and pathogens. In addition, MDMA has immunosuppressive effects in the peripheral nervous system, but pro-inflammatory effects in the central nervous system.
Pregnancy.
MDMA is a toxic to the fetus and considered to have a moderate toxic effect. In utero exposure to MDMA is associated with a neuro- and cardiotoxicity and impaired motor functioning. Motor delays may be temporary during infancy or long-term. The severity of these developmental delays increases with heavier MDMA use.
Reinforcement disorders.
Approximately 60% of MDMA users experience withdrawal symptoms, including, but not limited to: fatigue, loss of appetite, depression, and trouble concentrating. Tolerance is expected to occur with consistent MDMA use.
MDMA has been shown to induce ΔFosB in the nucleus accumbens. Since MDMA releases dopamine in the striatum, the mechanisms by which it induces ΔFosB in the nucleus accumbens are analogous to other dopaminergic psychostimulants. Therefore, chronic use of MDMA at high doses can result in altered brain structure and drug addiction, which occur as a consequence of ΔFosB overexpression in the nucleus accumbens.
Overdose.
MDMA overdose symptoms vary widely due to the involvement of multiple organ systems. Some of the more overt overdose symptoms are listed in the table below.
Interactions.
A number of drug interactions can occur between MDMA and other drugs, including serotonergic drugs. MDMA also interacts with drugs which inhibit CYP450 enzymes, like ritonavir (Norvir), particularly CYP2D6 inhibitors. Concurrent use of MDMA with another serotonergic drug can result in a life-threatening condition called serotonin syndrome. Severe overdose resulting in death has also been reported in people who took MDMA in combination with certain monoamine oxidase inhibitors, such as phenelzine (Nardil), tranylcypromine (Parnate), or moclobemide (Aurorix, Manerix).
Pharmacology.
Pharmacodynamics.
MDMA acts primarily as a presynaptic releasing agent of serotonin, norepinephrine, and dopamine, which arises from its activity at trace amine-associated receptor 1 (TAAR1) and vesicular monoamine transporter 2 (VMAT2). MDMA is a monoamine transporter substrate (i.e., a substrate for DAT, NET, and SERT), so it enters monoamine neurons via these neuronal membrane transport proteins; by acting as a monoamine transporter substrate, MDMA produces competitive reuptake inhibition at the neuronal membrane transporters (i.e., it competes with endogenous monoamines for reuptake). MDMA inhibits both vesicular monoamine transporters (VMATs), the second of which (VMAT2) is highly expressed within monoamine neurons at vesicular membranes. Once inside a monoamine neuron, MDMA acts as a VMAT2 inhibitor and a TAAR1 agonist.
Inhibition of VMAT2 by MDMA results in increased concentrations of the associated neurotransmitter (serotonin, norepinephrine, or dopamine) in the cytosol of a monoamine neuron. Activation of TAAR1 by MDMA triggers protein kinase A and protein kinase C signaling events which then phosphorylates the associated monoamine transporters – DAT, NET, or SERT – of the neuron. In turn, these phosphorylated monoamine transporters either reverse transport direction – i.e., move neurotransmitters from the cytosol to the synaptic cleft – or withdraw into the neuron, respectively producing neurotransmitter efflux and noncompetitive reuptake inhibition at the neuronal membrane transporters. MDMA has ten times more affinity for uptake at serotonin transporters compared to dopamine and norepinephrine transporters and consequently has mainly serotonergic effects.
In summary, MDMA enters monoamine neurons by acting as a monoamine transporter substrate. MDMA activity at VMAT2 moves neurotransmitters out from synaptic vesicles and into the cytosol; MDMA activity at TAAR1 moves neurotransmitters out of the cytosol and into the synaptic cleft.
MDMA also has weak agonist activity at postsynaptic serotonin receptors 5-HT1 and 5-HT2 receptors, and its more efficacious metabolite MDA likely augments this action. A placebo-controlled study in 15 human volunteers found 100 mg MDMA increased blood levels of oxytocin, and the amount of oxytocin increase was correlated with the subjective prosocial effects of MDMA.("S")-MDMA is more effective in eliciting 5-HT, NE, and DA release, while ("D")-MDMA is overall less effective, and more selective for 5-HT and NE release (having only a very faint efficacy on DA release).
MDMA is a ligand at both sigma receptor subtypes, though its efficacies at the receptors have not yet been elucidated.
Pharmacokinetics.
MDMA reaches maximal concentrations in the blood stream between 1.5 and 3 hr after ingestion. It is then slowly metabolized and excreted, with levels of MDMA and its metabolites decreasing to half their peak concentration over the next several hours.
Metabolites of MDMA that have been identified in humans include 3,4-methylenedioxyamphetamine (MDA), 4-hydroxy-3-methoxymethamphetamine (HMMA), 4-hydroxy-3-methoxyamphetamine (HMA), 3,4-dihydroxyamphetamine (DHA) (also called alpha-methyldopamine (α-Me-DA)), 3,4-methylenedioxyphenylacetone (MDP2P), and 3,4-Methylenedioxy-N-hydroxyamphetamine (MDOH). The contributions of these metabolites to the psychoactive and toxic effects of MDMA are an area of active research. 80% of MDMA is metabolised in the liver, and about 20% is excreted unchanged in the urine.
MDMA is known to be metabolized by two main metabolic pathways: (1) "O"-demethylenation followed by catechol-"O"-methyltransferase (COMT)-catalyzed methylation and/or glucuronide/sulfate conjugation; and (2) "N"-dealkylation, deamination, and oxidation to the corresponding benzoic acid derivatives conjugated with glycine. The metabolism may be primarily by cytochrome P450 (CYP450) enzymes CYP2D6 and CYP3A4 and COMT. Complex, nonlinear pharmacokinetics arise via autoinhibition of CYP2D6 and CYP2D8, resulting in zeroth order kinetics at higher doses. It is thought that this can result in sustained and higher concentrations of MDMA if the user takes consecutive doses of the drug.
MDMA and metabolites are primarily excreted as conjugates, such as sulfates and glucuronides. MDMA is a chiral compound and has been almost exclusively administered as a racemate. However, the two enantiomers have been shown to exhibit different kinetics. The disposition of MDMA may also be stereoselective, with the S-enantiomer having a shorter elimination half-life and greater excretion than the R-enantiomer. Evidence suggests that the area under the blood plasma concentration versus time curve (AUC) was two to four times higher for the ("R")-enantiomer than the ("S")-enantiomer after a 40 mg oral dose in human volunteers. Likewise, the plasma half-life of ("R")-MDMA was significantly longer than that of the ("S")-enantiomer (5.8 ± 2.2 hours vs 3.6 ± 0.9 hours). However, because MDMA excretion and metabolism have nonlinear kinetics, the half-lives would be higher at more typical doses (100 mg is sometimes considered a typical dose).
Physical and chemical properties.
MDMA is in the substituted methylenedioxyphenethylamine and substituted amphetamine classes of chemicals. As a free base, MDMA is a colorless oil insoluble in water. The most common salt of MDMA is the hydrochloride salt; pure MDMA hydrochloride is water-soluble and appears as a white or off-white powder or crystal.
Synthesis.
There are numerous methods available in the literature to synthesize MDMA via different intermediates. The original MDMA synthesis described in Merck's patent involves brominating safrole to 1-(3,4-methylenedioxyphenyl)-2-bromopropane and then reacting this adduct with methylamine. Most illicit MDMA is synthesized using MDP2P (3,4-methylenedioxyphenyl-2-propanone) as a precursor. MDP2P in turn is generally synthesized from piperonal, safrole or isosafrole. One method is to isomerize safrole to isosafrole in the presence of a strong base, and then oxidize isosafrole to MDP2P. Another method uses the Wacker process to oxidize safrole directly to the MDP2P intermediate with a palladium catalyst. Once the MDP2P intermediate has been prepared, a reductive amination leads to racemic MDMA (an equal parts mixture of ("R")-MDMA and ("S")-MDMA). Relatively small quantities of essential oil are required to make large amounts of MDMA. The essential oil of "Ocotea cymbarum", for example, typically contains between 80 and 94% safrole. This allows 500 ml of the oil to produce between 150 and 340 grams of MDMA.
Detection in body fluids.
MDMA and MDA may be quantitated in blood, plasma or urine to monitor for use, confirm a diagnosis of poisoning or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Some drug abuse screening programs rely on hair, saliva, or sweat as specimens. Most commercial amphetamine immunoassay screening tests cross-react significantly with MDMA or its major metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. The concentrations of MDA in the blood or urine of a person who has taken only MDMA are, in general, less than 10% those of the parent drug.
History.
Early research and use.
MDMA was first synthesized in 1912 by Merck chemist Anton Köllisch. At the time, Merck was interested in developing substances that stopped abnormal bleeding. Merck wanted to avoid an existing patent held by Bayer for one such compound: hydrastinine. Köllisch developed a preparation of a hydrastinine analogue, methylhydrastinine, at the request of fellow lab members, Walther Beckh and Otto Wolfes. MDMA (called methylsafrylamin, safrylmethylamin or N-Methyl-a-Methylhomopiperonylamin in Merck laboratory reports) was an intermediate compound in the synthesis of methylhydrastinine. Merck was not interested in MDMA itself at the time. On 24 December 1912, Merck filed two patent applications that described the synthesis and some chemical properties of MDMA and its subsequent conversion to methylhydrastinine.
Merck records indicate its researchers returned to the compound sporadically. A 1920 Merck patent describes a chemical modification to MDMA. In 1927, Max Oberlin studied the pharmacology of MDMA while searching for substances with effects similar to adrenaline or ephedrine, the latter being structurally similar to MDMA. Compared to ephedrine, Oberlin observed that it had similar effects on vascular smooth muscle tissue, stronger effects at the uterus, and no "local effect at the eye". MDMA was also found to have effects on blood sugar levels comparable to high doses of ephedrine. Oberlin concluded that the effects of MDMA were not limited to the sympathetic nervous system. Research was stopped "particularly due to a strong price increase of safrylmethylamine", which was still used as an intermediate in methylhydrastinine synthesis. Albert van Schoor performed simple toxicological tests with the drug in 1952, most likely while researching new stimulants or circulatory medications. After pharmacological studies, research on MDMA was not continued. In 1959, Wolfgang Fruhstorfer synthesized MDMA for pharmacological testing while researching stimulants. It is unclear if Fruhstorfer investigated the effects of MDMA in humans.
Outside of Merck, other researchers began to investigate MDMA. In 1953 and 1954, the United States Army commissioned a study of toxicity and behavioral effects in animals injected with mescaline and several analogues, including MDMA. Conducted at the University of Michigan in Ann Arbor, these investigations were declassified in October 1969 and published in 1973. A 1960 Polish paper by Biniecki and Krajewski describing the synthesis of MDMA as an intermediate was the first published scientific paper on the substance.
MDMA may have been in non-medical use in the western United States in 1968. An August 1970 report at a meeting of crime laboratory chemists indicates MDMA was being used recreationally in the Chicago area by 1970. MDMA likely emerged as a substitute for its analog methylenedioxyamphetamine (MDA), a drug at the time popular among users of psychedelics which was made a Schedule 1 substance in the United States in 1970.
Shulgin's research and therapeutic use.
American chemist and psychopharmacologist Alexander Shulgin reported he synthesized MDMA in 1965 while researching methylenedioxy compounds at Dow Chemical Company, but did not test the psychoactivity of the compound at this time. Around 1970, Shulgin sent instructions for N-methylated MDA (MDMA) synthesis to the founder of a Los Angeles chemical company who had requested them. This individual later provided these instructions to a client in the Midwest.
Shulgin first heard of the psychoactive effects of N-methylated MDA around 1975 from a young student who reported "amphetamine-like content". Around 30 May 1976, Shulgin again heard about the effects of N-methylated MDA, this time from a graduate student in a medicinal chemistry group he advised at San Francisco State University who directed him to the University of Michigan study. She and two close friends had consumed 100 mg of MDMA and reported positive emotional experiences. Following the self-trials of a colleague at the University of San Francisco, Shulgin synthesized MDMA and tried it himself in September and October 1976. Shulgin first reported on MDMA in a presentation at a conference in Bethesda, Maryland in December 1976. In 1978, he and David E. Nichols published a report on the drug's psychoactive effect in humans. They described MDMA as inducing "an easily controlled altered state of consciousness with emotional and sensual overtones" comparable "to marijuana, to psilocybin devoid of the hallucinatory component, or to low levels of MDA". Believing MDMA allowed users to strip away habits and perceive the world clearly, Shulgin called the drug "window".
While not finding his own experiences with MDMA particularly powerful, Shulgin was impressed with the drug's disinhibiting effects and believed it could be useful in psychotherapy. Shulgin took to occasionally using MDMA for relaxation, referring to it as "my low-calorie martini", and giving the drug to his friends, researchers, and others whom he thought could benefit from it. One such person was Leo Zeff, a psychotherapist who had been known to use psychedelic substances in his practice. When he tried the drug in 1977, Zeff was so impressed with the effects of MDMA that he came out of his semi-retirement to promote its use in psychotherapy. Over the following years, Zeff traveled around the US and occasionally to Europe, eventually training an estimated four thousand psychotherapists in the therapeutic use of MDMA. Zeff named the drug "Adam", believing it put users in a state of primordial innocence.
Psychotherapists who used MDMA believed the drug eliminated the typical fear response and increased communication. Sessions were usually held in the home of the patient or the therapist. The role therapist was minimized in favor of patient self-discovery accompanied by MDMA induced feelings of empathy. Depression, substance abuse, relationship problems, premenstrual syndrome, and autism were among several psychiatric disorders MDMA assisted therapy was reported to treat. According to psychiatrist George Greer, therapists who used MDMA in their practice were impressed by the results. Anecdotally, MDMA was said to greatly accelerate therapy.
Rising recreational use.
In the late seventies and early eighties, "Adam" spread through personal networks of psychotherapists, psychiatrists, users of psychedelics, and yuppies. Hoping MDMA could avoid criminalization like LSD and mescaline, psychotherapists and experimenters attempted to limit the spread of MDMA and information about it while conducting informal research. Early MDMA distributors were deterred from large scale operations by the threat of possible legislation. Between the 1970s and the mid-1980s, this network of MDMA users consumed an estimated 500,000 doses.
A small recreational market for MDMA developed by the late 1970s, consuming perhaps 10,000 doses in 1976. By the early 1980s MDMA was being used in Boston and New York City nightclubs such as Studio 54 and Paradise Garage. Into the early 1980s, as the recreational market slowly expanded, production of MDMA was dominated by a small group of therapeutically minded Boston chemists. Having commenced production in 1976, this "Boston Group" did not keep up with growing demand and shortages frequently occurred.
Perceiving a business opportunity, Michael Clegg, the Southwest distributor for the Boston Group, started his own "Texas Group" backed financially by Texas friends. In 1981, Clegg had coined "Ecstasy" as a slang term for MDMA to increase its marketability. Starting in 1983, the Texas Group mass-produced MDMA in a Texas lab or imported it from California and marketed tablets using pyramid sales structures and toll-free numbers. MDMA could be purchased via credit card and taxes were paid on sales. Under the brand name "Sassyfras", MDMA tablets were sold in brown bottles. The Texas Group advertised "Ecstasy parties" at bars and discos, describing MDMA as a "fun drug" and "good to dance to". MDMA was openly distributed in Austin and Dallas-Fort Worth area bars and nightclubs, becoming popular with yuppies, college students, and gays.
Recreational use also increased after several cocaine dealers switched to distributing MDMA following experiences with the drug. A California laboratory that analyzed confidentially submitted drug samples first detected MDMA in 1975. Over the following years the number of MDMA samples increased, eventually exceeding the number of MDA samples in the early 1980s. By the mid-1980s, MDMA use had spread to colleges around the United States.
Media attention and scheduling.
United Kingdom.
In the United Kingdom, MDMA was made illegal in 1977 by a modification order to the existing Misuse of Drugs Act 1971. Although MDMA was not named explicitly in this legislation, the order extended the definition of Class A drugs to include various ring-substituted phenethylamines.
United States.
In an early media report on MDMA published in 1982, a Drug Enforcement Administration (DEA) spokesman stated the agency would ban the drug if enough evidence for abuse could be found. By mid-1984, MDMA use was becoming more noticed. Bill Mandel reported on "Adam" in a 10 June San Francisco Chronicle article, but misidentified the drug as methyloxymethylenedioxyamphetamine (MMDA). In the next month, the World Health Organization identified MDMA as the only substance out of twenty phenethylamines to be seized a significant number of times.
After a year of planning and data collection, MDMA was proposed for scheduling by the DEA on 27 July 1984 with a request for comments and objections. The DEA was surprised when a number of psychiatrists, psychotherapists, and researchers objected to the proposed scheduling and requested a hearing. In a Newsweek article published the next year, a DEA pharmacologist stated that the agency had been unaware of its use among psychiatrists. An initial hearing was held on 1 February 1985 at the DEA offices in Washington, D.C. with administrative law judge Francis L. Young presiding. It was decided there to hold three more hearings that year: Los Angeles on 10 June, Kansas City, Missouri on 10–11 July, and Washington, D.C. on 8–11 October.
Sensational media attention was given to the proposed criminalization and the reaction of MDMA proponents, effectively advertising the drug. In response to the proposed scheduling, the Texas Group increased production from 1985 estimates of 30,000 tablets a month to as many as 8,000 per day, potentially making two million ecstasy tablets in the months before MDMA was made illegal. By some estimates the Texas Group distributed 500,000 tablets per month in Dallas alone. According to one participant in an ethnographic study, the Texas Group produced more MDMA in eighteen months than all other distribution networks combined across their entire histories. By May 1985, MDMA use was widespread in California, Texas, southern Florida, and the northeastern United States. According to the DEA there was evidence of use in twenty-eight states and Canada. Urged by Senator Lloyd Bentsen, the DEA announced an emergency Schedule I classification of MDMA on 31 May 1985. The agency cited increased distribution in Texas, escalating street use, and new evidence of MDA (an analog of MDMA) neurotoxicity as reasons for the emergency measure. The ban took effect one month later on 1 July 1985 in the midst of Nancy Reagan's "Just Say No" campaign.
As a result of several expert witnesses testifying that MDMA had an accepted medical usage, the administrative law judge presiding over the hearings recommended that MDMA be classified as a Schedule III substance. Despite this, DEA administrator John C. Lawn overruled and classified the drug as Schedule I. Later Harvard psychiatrist Lester Grinspoon sued the DEA, claiming that the DEA had ignored the medical uses of MDMA, and the federal court sided with Grinspoon, calling Lawn's argument "strained" and "unpersuasive", and vacated MDMA's Schedule I status. Despite this, less than a month later Lawn reviewed the evidence and reclassified MDMA as Schedule I again, claiming that the expert testimony of several psychiatrists claiming over 200 cases where MDMA had been used in a therapeutic context with positive results could be dismissed because they weren't published in medical journals. No double blind studies had yet been conducted as to the efficacy of MDMA for psychotherapy.
United Nations.
While engaged in scheduling debates in the United States, the DEA also pushed for international scheduling. In 1985 the World Health Organization's Expert Committee on Drug Dependence recommended that MDMA be placed in Schedule I of the 1971 United Nations Convention on Psychotropic Substances. The committee made this recommendation on the basis of the pharmacological similarity of MDMA to previously scheduled drugs, reports of illicit trafficking in Canada, drug seizures in the United States, and lack of well-defined therapeutic use. While intrigued by reports of psychotherapeutic uses for the drug, the committee viewed the studies as lacking appropriate methodological design and encouraged further research. Committee chairman Paul Grof dissented, believing international control was not warranted at the time and a recommendation should await further therapeutic data. The Commission on Narcotic Drugs added MDMA to Schedule I of the convention on 11 February 1986.
Post-scheduling.
The use of MDMA in Texas clubs declined rapidly after criminalization, although by 1991 the drug remained popular among young middle-class whites and in nightclubs. In 1985, MDMA use became associated with Acid House on the Spanish island of Ibiza. Thereafter in the late 1980s, the drug spread alongside rave culture to the UK and then to other European and American cities. Illicit MDMA use became increasingly widespread among young adults in universities and later, in high schools. Since the mid-1990s, MDMA has become the most widely used amphetamine-type drug by college students and teenagers. MDMA became one of the four most widely used illicit drugs in the US, along with cocaine, heroin, and cannabis.
According to some estimates as of 2004, only marijuana attracts more first time users in the US.
After MDMA was criminalized, most medical use stopped, although some therapists continued to prescribe the drug illegally. Later, Charles Grob initiated an ascending-dose safety study in healthy volunteers. Subsequent legally-approved MDMA studies in humans have taken place in the US. in Detroit (Wayne State University), Chicago (University of Chicago), San Francisco (UCSF and California Pacific Medical Center), Baltimore (NIDA–NIH Intramural Program), and South Carolina, as well as in Switzerland (University Hospital of Psychiatry, Zürich), the Netherlands (Maastricht University), and Spain (Universitat Autònoma de Barcelona).
"Molly", short for 'molecule', was recognized as a slang term for crystalline or powder MDMA in the 2000s.
In 2010, the BBC reported that use of MDMA had decreased in the UK in previous years. This may be due to increased seizures during use and decreased production of the precursor chemicals used to manufacture MDMA. Unwitting substitution with other drugs, such as mephedrone and methamphetamine, as well as legal alternatives to MDMA, such as BZP, MDPV, and methylone, are also thought to have contributed to its decrease in popularity.
Society and culture.
Legal status.
MDMA is legally controlled in most of the world under the UN Convention on Psychotropic Substances and other international agreements, although exceptions exist for research and limited medical use. In general, the unlicensed use, sale or manufacture of MDMA are all criminal offences.
Australia.
In Australia, MDMA was declared an illegal substance in 1986 because of its harmful effects and potential for abuse. It is classed as a Schedule 9 Prohibited Substance in the country, meaning it is available for scientific research purposes only. Any other type of sale, use or manufacture is strictly prohibited by law. Permits for research uses on humans must be approved by a recognized ethics committee on human research. In Western Australia under the Misuse of Drugs Act 1981 4.0g of MDMA is the amount required determining a court of trial, 2.0g is considered a presumption with intent to sell or supply and 28.0g is considered trafficking under Australian law.
United Kingdom.
In the United Kingdom, MDMA is illegal under a 1977 modification to the Misuse of Drugs Act 1971 thereby making it illegal to sell, buy, or possess the drug without a licence. Penalties include a maximum of seven years and/or unlimited fine for possession; life and/or unlimited fine for production or trafficking.
United States.
In the United States, MDMA is currently placed in Schedule I of the Controlled Substances Act. In a 2011 federal court hearing the American Civil Liberties Union successfully argued that the sentencing guideline for MDMA/ecstasy is based on outdated science, leading to excessive prison sentences. Other courts have upheld the sentencing guidelines. The United States District Court for the Eastern District of Tennessee explained its ruling by noting that "an individual federal district court judge simply cannot marshal resources akin to those available to the Commission for tackling the manifold issues involved with determining a proper drug equivalency."
Netherlands.
In the Netherlands, the Expert Committee on the List (Expertcommissie Lijstensystematiek Opiumwet) issued a report in June 2011 which discussed the evidence for harm and the legal status of MDMA, arguing in favor of maintaining it on List I.
Canada.
In Canada, MDMA is listed as a Schedule 1 as it is an analogue of amphetamine. The CDSA was updated as a result of the Safe Streets Act changing amphetamines from Schedule III to Schedule I in March 2012.
Economics.
Europe.
In 2008 the European Monitoring Centre for Drugs and Drug Addiction noted that although there were some reports of tablets being sold for as little as €1, most countries in Europe then reported typical retail prices in the range of €3 to €9 per tablet, typically containing 25–65 mg of MDMA. By 2014 the EMCDDA reported that the range was more usually between €5 and €10 per tablet, typically containing 57–102 mg of MDMA, although MDMA in powder form was becoming more common.
North America.
The United Nations Office on Drugs and Crime stated in its 2014 World Drug Report that US ecstasy retail prices range from US$1 to $70 per pill, or from $15,000 to $32,000 per kilogram. A new research area named Drug Intelligence aims to automatically monitor distribution networks based on image processing and machine learning techniques, in which an Ecstasy pill picture is analyzed to detect correlations among different production batches. These novel techniques allow police scientists to facilitate the monitoring of illicit distribution networks. Among adolescent users in the United States between 1999 and 2008, girls were more likely to use MDMA than boys.
, most of the MDMA in the United States is produced in British Columbia, Canada and imported by Canada-based Asian transnational criminal organizations. The market for MDMA in the United States is relatively small compared to methamphetamine, cocaine, and heroin.
Australia.
MDMA is particularly expensive in Australia, costing A$15–A$30 per tablet. In terms of purity data for Australian MDMA, the average is around 34%, ranging from less than 1% to about 85%. The majority of tablets contain 70–85 mg of MDMA. Most MDMA enters Australia from the Netherlands, the UK, Asia, and the US.
Harm assessment.
Researchers such as David Nutt disagree with the categorization of MDMA with other more harmful drugs, especially when compared to alcohol which ranked as the most harmful drug using a multicriteria approach. A 2010 UK study which took into account impairment of cognitive functioning placed MDMA at number 17 out of 20 recreational drugs. In a 2011 survey of 292 clinical experts in Scotland, MDMA ranked 13th in personal harm and 16th in social harm out of 19 common recreational drugs.
Research.
The Multidisciplinary Association for Psychedelic Studies (MAPS) is currently funding pilot studies or clinical trials investigating the use of MDMA in psychotherapy to treat posttraumatic stress disorder (PTSD), social anxiety in autistic adults, and anxiety in terminal illness. MDMA has also been proposed as an adjunct to substance abuse treatment. Contrary to ongoing treatment with approved psychiatric medications, MDMA is taken only a few times.
A review of the safety and efficacy of MDMA as a treatment for various disorders, particularly PTSD, indicated that MDMA has therapeutic efficacy in some patients; however, it emphasized that MDMA is not a safe medical treatment due to lasting neurotoxic and cognition impairing effects in humans. The author noted that oxytocin and -cycloserine are potentially safer co-drugs in PTSD treatment, albeit with limited evidence of efficacy. This review and a second corroborating review by a different author both concluded that, because of MDMA's demonstrated potential to cause lasting harm in humans (e.g., serotonergic neurotoxicity and persistent memory impairment), "considerably more research must be performed" on its efficacy in PTSD treatment to determine if the potential treatment benefits outweigh its potential to harm to a patient.

</doc>
<doc id="10025" url="https://en.wikipedia.org/wiki?curid=10025" title="Flag of Europe">
Flag of Europe

The Flag of Europe, or European Flag, consists of a circle of 12 golden (yellow) stars on an azure background. It is an official emblem of both the Council of Europe (CoE) and the European Union (EU) and is termed the "Flag of Europe" or "European Flag" by both organisations. It was first adopted in 1955 by the Council of Europe, intended to represent the continent as a whole. However, it is sometimes known as the "flag of the European Union". It can also be used to indicate eurozone countries.
In its broadest sense, the flag can represent the continent of Europe or the countries of Europe independent of any institution. Consequently, the number of stars does not vary according to the member states as they are intended to represent all the peoples of Europe, even those outside the EU.
The flag was designed in 1955 for the Council of Europe as its symbol, and the CoE urged it to be adopted by other organisations. In 1985 the European Economic Community (EEC), adopted it as its own flag (having had no flag of its own before) at the initiative of the European Parliament.
The flag is not mentioned in the EU's treaties, its incorporation being dropped along with the European Constitution, but it is formally adopted in law. In order to avoid confusion with the European Union, the Council of Europe uses a modified version with a lower-case "e" in the centre of the stars which is referred to as the "Council of Europe Logo" while it is again flying the flag of Europe in front of and/or in its headquarters, annexes and field office premises.
Since its adoption by the European Union it has become more associated with the EU due to the EU's higher profile and heavy usage of the emblem. The flag has also been used to represent Europe in sporting events and as a pro-democracy banner outside the Union. It has partly inspired other flags, such as those of other European organisations and those of sovereign states where the EU has been heavily involved (such as Kosovo and Bosnia and Herzegovina).
History.
Creation.
The search for a symbol began in 1950 when a committee was set up in order to look into the question of a European flag. There were numerous proposals but a clear theme for stars and circles emerged. Count Richard Nikolaus von Coudenhove-Kalergi proposed that they adopt the flag of his International Paneuropean Union, which was a blue field, with a red cross inside an orange circle at the centre, which he had himself recently adopted for the European Parliamentary Union. Due to the cross symbolism, this was rejected by Turkey (a member of the Council of Europe since 1949). Kalergi then suggested adding a crescent to the cross design, to overcome the Muslim objections. Another organisation's flag was the European Movement, which had a large green E on a white background. A further design was one based on the Olympic rings: eight silver rings on a blue background. It was rejected due to the rings' similarity with "dial", "chain" and "zeros". One proposal had a large yellow star on a blue background, but it was rejected due to its similarity with the so-called Burnet flag and the flag of the Belgian Congo.
The Consultative Assembly narrowed their choice to two designs. One was by Salvador de Madariaga, the founder of the College of Europe, who suggested a constellation of stars on a blue background (positioned according to capital cities, with a large star for Strasbourg, the seat of the Council). He had circulated his flag round many European capitals and the concept had found favour. The second was a variant by Arsène Heitz, who worked for the Council's postal service and had submitted dozens of designs; the design of his that was accepted by the Assembly was similar to Salvador de Madariaga's, but rather than a constellation, the stars were arranged in a circle. In 1987, Heitz revealed that his inspiration was the crown of twelve stars of the Woman of the Apocalypse, often found in Marian iconography. The Book of Revelation describes the Woman of the Apocalypse: "And there appeared a great wonder in heaven; a woman clothed with the sun, and the moon under her feet, and upon her head a crown of twelve stars..." However, Heinz has not suggested that his design held a religious meaning.
The Consultative Assembly favoured Heitz's design. However, the flag the Assembly chose had fifteen stars, reflecting the number of states of the Council of Europe. The Consultative Assembly chose this flag and recommended the Council of Europe to adopt it. The Committee of Ministers (the Council's main decision making body) agreed with the Assembly that the flag should be a circle of stars, but the number was a source of contention. The number twelve was chosen, and Paul M. G. Lévy drew up the exact design of the new flag as it is today. The Parliamentary Assembly of the Council of Europe approved it on 25 October 1955. Adopted on 8 December 1955, the flag was unveiled at the Château de la Muette in Paris on 13 December 1955.
European Communities.
Following Expo 58 in Brussels, the flag caught on and the Council of Europe lobbied for other European organisations to adopt the flag as a sign of European unity. The European Parliament took the initiative in seeking a flag to be adopted by the European Communities. Shortly after the first direct elections in 1979 a draft resolution was put forward on the issue. The resolution proposed that the Communities' flag should be that of the Council of Europe and it was adopted by the Parliament on 11 April 1983.
The June 1984 European Council (the Communities' leaders) summit in Fontainebleau stressed the importance of promoting a European image and identity to citizens and the world. The following year, meeting in Milan, the 28–29 June European Council approved a proposal from the Committee on a People’s Europe (Adonnino Committee) in favour of the flag and adopted it. Following the permission of the Council of Europe, the Communities began to use it from 1986, with it being raised outside the Berlaymont building (the seat of the European Commission) for the first time on 29 May 1986. The European Union, which was established by the Maastricht Treaty in 1992 to replace the EC and encompass its
functions, also adopted the flag. Since then the use of the flag has been controlled jointly by the Council of Europe and the European Union.
Previous flags.
Prior to development of political institutions, flags representing Europe were limited to unification movements. The most popular were the European Movement's large green 'E' on a white background, and the "Pan European flag" (see "Creation" below). With the development of institutions, aside from the Council of Europe, came other emblems and flags. None were intended to represent wider Europe and have since been replaced by the current flag of Europe.
The first major organisation to adopt one was the European Coal and Steel Community (ECSC), which merged into the European Communities. The ECSC was created in 1952 and the flag of the ECSC was unveiled in 1958 Expo in Brussels.
The flag had two stripes, blue at the top, black at the bottom with six gold (silver after 1973) stars, three on each stripe. Blue was for steel, black for coal and the stars were the six member-states. The stars increased with the members until 1986 when they were fixed at twelve. When the ECSC treaty expired in 2002, the flag was lowered outside the European Commission in Brussels and replaced with the European flag.
The European Parliament also used its own flag from 1973, but never formally adopted it. It fell out of use with the adoption of the twelve star flag by the Parliament in 1983. The flag followed the yellow and blue colour scheme however instead of twelve stars there were the letters EP and PE (initials of the European Parliament in the then six community languages) surrounded by a wreath.
Barcode flag.
In 2002, Dutch architect Rem Koolhaas and his architecture firm Office for Metropolitan Architecture (OMA) designed a new flag in response to Commission President Romano Prodi's request to find ways of rebranding the Union in a way that represents Europe's "diversity and unity". The proposed new design was dubbed the "barcode", as it displays the colors of every European flag (of the then 15 members) as vertical stripes. As well as the barcode comparison, it had been compared unfavourably to wallpaper, a TV test card, and deckchair fabric. Unlike the current flag, it would change to reflect the member states.
It was never officially adopted by the EU or any organisation; however, it was used as the logo of the Austrian EU Presidency in 2006. It had been updated with the colours of the 10 members who had joined since the proposal, and was designed by Koolhaas's firm. Its described aim is "to portray Europe as the common effort of different nations, with each retaining its own unique cultural identity". There were initially some complaints, as the stripes of the flag of Estonia were displayed incorrectly.
Recent events.
In April 2004, the flag was flown in space for the first time by European Space Agency astronaut André Kuipers while on board the International Space Station.
The flag was to have been given a formal status in the proposed European Constitution. However, since the ratification of that failed, the leaders removed the state-like elements such as the flag from the replacement Treaty of Lisbon. The European Parliament however had supported the inclusion of symbols, and in response backed a proposal to use the symbols, such as the flag more often in the Parliament. Jo Leinen MEP suggesting that the Parliament should again take the "avant-garde" in their use. Later, in September 2008, Parliament's Committee on Constitutional Affairs proposed a formal change in the institution's rules of procedure to make better use of the symbols. Specifically, the flag would be present in all meeting rooms (not just the hemicycle) and at all official events. The proposal was passed on 8 October 2008 by 503 votes to 96 (15 abstentions).
Additionally, a declaration by sixteen Member States on the symbols, including the flag, was included in the final act of the Treaty of Lisbon stating that the flag, the anthem, the motto and the currency and Europe Day "will for them continue as symbols to express the sense of community of the people in the European Union and their allegiance to it."
Usage.
Council of Europe.
The flag was originally designed by the Council of Europe, and as such the CoE holds the copyright for the flag. However, the Council of Europe agreed that the European Communities could use the flag and it had promoted its use by other regional organisations since it was created. The Council of Europe now shares responsibility with the European Commission for ensuring that use of the symbol respects the dignity of the flag—taking measures to prevent misuse of it.
Besides using the flag, the Council also uses a defaced version of the flag as its emblem: it is the existing design with a stylised, green "e" over the stars.
European Union.
The flag symbolises the EU as a whole. All EU institutions, bodies and agencies have their own logo or emblem, albeit often inspired by the flag's design and colours. As part of the EU's usage, the flag appears on the euro banknotes. Euro coins also display the twelve stars of the flag on both the national and common sides and the flag is sometimes used as an indication of the currency or the eurozone (a collective name for those countries that use the Euro). The flag appears also on many driving licences and vehicle registration plates issued in the Union.
Protocol.
It is mandatory for the flag to be used in every official speech made by the President of the European Council and it is often used at official meetings between the leaders of an EU state and a non-EU state (the national flag and European flag appearing together). While normally the national flag takes precedence over the European flag in the national context, meetings between EU leaders sometimes differ. For example, the Italian flag code expressly replaces the Italian flag for the European flag in precedence when dignitaries from other EU countries visit – for example the EU flag would be in the middle of a group of three flags rather than the Italian flag.
The flag is usually flown by the government of the country holding the rotating presidency Council of Ministers, though in 2009 the Czech President Václav Klaus, a eurosceptic, refused to fly the flag from his castle. In response, Greenpeace projected an image of the flag onto the castle and attempted to fly the flag from the building themselves.
Some members also have their own rules regarding the use of the flag alongside their national flag on domestic occasions, for example the obligatory use alongside national flags outside police stations or local government buildings. As an example according to the Italian laws it is mandatory for most public offices and buildings to hoist the European Flag alongside the Italian national Flag (Law 22/2000 and Presidential Decree 121/2000). Outside official use, the flag may not be used for aims incompatible with European values.
In national usage, national protocol usually demands the national flag takes precedence over the European flag (which is usually displayed to the right of the national flag from the observer's perspective). On occasions where the European flag is flown alongside all national flags (for example, at a European Council meeting), the national flags are placed in alphabetical order (according to their name in the main language of that state) with the European flag either at the head, or the far right, of the order of flags.
Extraordinary flying of the flag is common on the EU's flag day, known as Europe Day, which is celebrated annually on 9 May. On Europe Day 2008, the flag was flown for the first time above the German Reichstag.
Military and naval use.
In addition to the flags use by the government and people, the flag is also used in EU military operations; however, it is not used as a civil ensign. In 2003, a member of the European Parliament tabled a proposal in a temporary committee of the European Parliament that national civil ensigns be defaced with the European flag. This proposal was rejected by Parliament in 2004, and hence the European flag is not used as a European civil ensign.
Despite not having a civil ensign, the EU's Fishery Inspection teams display a blue and yellow pennant. The pennant is flown by inspection vessels in EU waters. The flag is triangular and quartered blue and yellow and was adopted according to "EEC Regulation #1382/87" on 20 May 1978. There are no other variants or alternative flags used by the EU (in contrast to countries which have presidential, naval and military variants).
Ukraine.
The flag became a symbol of European integration of Ukraine in 2010s, particularly after Euromaidan. Ukraine is not a part of the EU but the flag is used by the Cabinet of Ukraine, Prime Minister of Ukraine, and MFA UA during official meetings.
Wider use.
The flag has been used to represent Europe in its wider sense. The Council of Europe covers all but three European countries, thereby representing much of Europe.
In particular, the flag has become a banner for pro-Europeanism outside the Union, for example in Georgia, where the flag is on most government buildings since the coming to power of Mikhail Saakashvili, who used it during his inauguration, stating: "European flag is Georgia’s flag as well, as far as it embodies our civilisation, our culture, the essence of our history and perspective, and our vision for the future of Georgia." It was later used in 2008 by pro-western Serbian voters ahead of an election.
It is also used as a pro-democracy emblem in countries such as Belarus, where it has been used on protest marches alongside the banned former national flag and flags of opposition movements. The flag was used widely in a 2007 European March in Minsk as protesters rallied in support of democracy and accession to the EU. Similarly, the flag was flown during the 2013 Euromaidan pro-democracy and pro-EU protests in Ukraine.
The flag, or features of it, are often used in the logos of organisations of companies which stress a pan-European element, for example European lobbyist groups or transnational shipping companies.
The flag is also used in certain sports arrangements where a unified Team Europe is represented as in the Ryder Cup and the Mosconi Cup.
Following the 2004 Summer Olympics, President Romano Prodi pointed out that the combined medal total of the European Union was far greater than that of any other country and called for EU athletes to fly the European flag at the following games alongside their own as a sign of solidarity (which did not happen).
The design of the European flag was displayed on the Eiffel Tower in Paris to celebrate the French presidency of the EU Council in the second half of 2008.
Design.
The flag is rectangular with 2:3 proportions: its fly (width) is one and a half times the length of its hoist (height). Twelve gold (or yellow) stars are centered in a circle (the radius of which is a third of the length of the hoist) upon a blue background. All the stars are upright (one point straight up), have five points and are spaced equally according to the hour positions on the face of a clock. Each star radius is equal to one-eighteenth of the height of the hoist.
The heraldic description given by the EU is: "On an azure field a circle of twelve golden mullets, their points not touching." The Council of Europe gives the flag a symbolic description in the following terms;
Colours.
The base colour of the flag is a dark blue (reflex blue, a mix of cyan and magenta), while the golden stars are portrayed in Yellow. The colours are regulated according to the Pantone colouring system (see table for specifications).
A large number of designs were proposed for the flag before the current flag was agreed. The rejected proposals are preserved in the Council of Europe Archives. One of these consists of a design of white stars on a light blue field, as a gesture to the peace and internationalism of the United Nations. An official website makes a reference to blue and gold being the original colours of Count Richard Nikolaus von Coudenhove-Kalergi, who proposed a Pan European Union in 1923, and was an active proponent of the early Community.
Number of stars.
The number of stars on the flag is fixed at 12, and is not related to the number of member states of the EU (although the EU did have 12 member states from 1986 to 1994). This is because it originally was the flag of the Council of Europe. In 1953, the Council of Europe had 15 members; it was proposed that the future flag should have one star for each member, and would not change based on future members. West Germany objected to this as one of the members was the disputed area of Saarland, and to have its own star would imply sovereignty for the region. Twelve was eventually adopted as a number with no political connotations and as a symbol of unity.
While 12 is the correct number of stars, sometimes flags or emblems can be found that incorrectly show 15 (as of the rejected proposal) or 25 (as suggested by some after the expansion of the EU to 25 member states in 2004). However, the flag also remains that of the Council of Europe, which now has 47 member states.
Derivative designs.
The design of the European flag has been used in a variation, such as that of the Council of Europe mentioned above, and also to a greater extent such as the flag of the Western European Union (WEU; now defunct), which uses the same colours and the stars but has a number of stars based on membership and in a semicircle rather than a circle. It is also defaced with the initials of the former Western European Union in two languages.
The flag of Bosnia and Herzegovina does not have such a strong connection as the WEU flag, but was partly inspired by the European involvement in, and aspirations of, Bosnia and Herzegovina. It uses the same blue and yellow colours and the stars, although of a different number and colour, are a direct reference to those of the European flag.
Likewise, the Republic of Kosovo uses blue, yellow and stars in its flag in reference to the European flag, symbolising its European ambitions (membership of the European Union). Kosovo has, like Bosnia and Herzegovina, seen heavy European involvement in its affairs, with the European Union assuming a supervisory role after its declared independence in 2008.
The flag of the Brussels-Capital Region consists of a yellow Iris with a white outline upon a blue background. Its colours are based on the colours of the Flag of Europe, because Brussels is considered the unofficial capital of the EU.
The national flag of Cape Verde also shows similarity to the flag of the European Union. The flag is made of a circular formation of ten yellow stars on a dark blue background and a band of white and red. The stars represent the main islands of the nation (a chain of islands off the coast of West Africa). The blue represents the ocean and the sky. The band of white and red represents the road toward the construction of the nation, and the colours stand for peace (white) and effort (red). The flag was adopted on 22 September 1992.
Other labels take reference to the European flag such as the EU organic food label that uses the twelve stars but reorders them into the shape of a leaf on a green background. The original logo of the European Broadcasting Union used the twelve stars on a blue background adding ray beams to connect the countries.
Marian interpretation.
In 1987, following the adoption of the flag by the EEC, Arsène Heitz (1908–1989), one of the designers who had submitted proposals for the flag's design, suggested a religious inspiration for it. He claimed that the circle of stars was based on the iconographic tradition of showing the Blessed Virgin Mary as the Woman of the Apocalypse, wearing a "crown of twelve stars".
The French satirical magazine "Le Canard enchaîné" reacted to Heitz's statement with an article entitled "L’Europe violée par la Sainte Vierge" ("Europe Raped by the Blessed Virgin") in the 20 December 1989 edition.
Heitz also made a connection to the date of the flag's adoption, 8 December 1955, coinciding with the Catholic Feast of the Immaculate Conception of the Blessed Virgin Mary.
Paul M. G. Lévy, then Director of Information at the Council of Europe responsible for designing the flag, in a 1989 statement maintained that he had not been aware of any religious connotations.
In an interview given 26 February 1998, Lévy denied not only awareness of the "Marian" connection, but also denied that the final design of a circle of twelve stars was Heitz's. To the question "Who really designed the flag?" Lévy replied:
Carlo Curti Gialdino (2005) has reconstructed the design process to the effect that Heitz's proposal contained varying numbers of stars, from which the version with twelve stars was chosen by the Committee of Ministers meeting at Deputy level in January 1955 as one out of two remaining candidate designs.
Lévy's 1998 interview apparently gave rise to a new variant of the "Marian" anecdote.
An article published in "Die Welt" in August 1998 alleged that it was Lévy himself who was inspired to introduce a "Marian" element as he walked past a statue of the Blessed Virgin Mary.
An article posted in "La Raison" in February 2000 further connected the donation of a stained glass window for Strasbourg Cathedral by the Council of Europe on 21 October 1956. 
This window, a work by Parisian master Max Ingrand, shows a blessing Madonna underneath a circle of 12 stars on dark blue ground. The overall design of the Madonna is inspired by the banner of the cathedral's "Congrégation Mariale des Hommes", and the twelve stars are found on the statue venerated by this congregation inside the cathedral (twelve is also the number of members of the congregation's council).

</doc>
<doc id="10026" url="https://en.wikipedia.org/wiki?curid=10026" title="Anthem of Europe">
Anthem of Europe

"Ode to Joy" (German original title: "") is the anthem of the European Union and the Council of Europe; both of which refer to it as the European Anthem due to the Council's intention that, as a semi-modern composition with a mythological flair, it does represent Europe as a whole, rather than any organisation. It is based on the final movement of Beethoven's 9th Symphony composed in 1823, and is played on official occasions by both organisations.
History.
Origin.
Friedrich Schiller wrote the poem "An die Freude" ("To Joy") in 1785 as a "celebration of the brotherhood of man". In later life, the poet was contemptuous of this popularity and dismissed the poem as typical of "the bad taste of the age" in which it had been written. After Schiller's death, the poem provided the words for the choral movement of Ludwig van Beethoven's 9th Symphony.
Adoption.
In 1971 the Parliamentary Assembly of the Council of Europe decided to propose adopting the prelude to the "Ode to Joy" from Beethoven's 9th Symphony as the European anthem, taking up a suggestion made by Richard Nikolaus von Coudenhove-Kalergi in 1955. Beethoven was generally seen as the natural choice for a European anthem. The Committee of Ministers of the Council of Europe officially announced the European Anthem on 19 January 1972 at Strasbourg: the prelude to "Ode to Joy", 4th movement of Ludwig van Beethoven's 9th symphony. In 1974 the same piece of music was adopted as the National Anthem of Rhodesia.
Conductor Herbert von Karajan was asked to write three instrumental arrangements – for solo piano, for wind instruments and for symphony orchestra and he conducted the performance used to make the official recording. He wrote his decisions on the score, notably those concerning the tempo. Karajan decided on minim (half note) = 80 whereas Beethoven had written crotchet (quarter note) = 120.
The anthem was launched via a major information campaign on Europe Day in 1972. In 1985, it was adopted by EU heads of State and government as the official anthem of the then European Community – since 1993 the European Union. It is not intended to replace the national anthems of the Member States but rather to celebrate the values they all share and their unity in diversity. It expresses the ideals of a united Europe: freedom, peace, and solidarity.
Recent events.
It was to have been included in the European Constitution along with the other European symbols; however, the treaty failed ratification and was replaced by the Treaty of Lisbon, which does not include any symbols. A declaration was attached to the treaty, in which sixteen member states formally recognised the proposed symbols. In response, the European Parliament decided that it would make greater use of the anthem, for example at official occasions. In October 2008, the Parliament changed its rules of procedure to have the anthem played at the opening of Parliament after elections and at formal sittings.
Usage.
"Ode to Joy" is the anthem of the Council of Europe and the European Union, promoted as a symbol for the whole of Europe as are the other European symbols. It is used on occasions such as Europe Day and formal events such as the signing of treaties. The European Parliament seeks to make greater use of the music, then-Parliament President Hans-Gert Pöttering stated he was moved when the anthem was played for him on his visit to Israel and ought to be used in Europe more often.
In 2008 it was used by Kosovo as its national anthem until it adopted its own, and it was played at its declaration of independence, as a nod to the EU's role in its independence from Serbia.
At the 2007 signing ceremony for the Treaty of Lisbon, the plenipotentiaries of the European Union's twenty-seven Member States stood in attendance while the "Ode to Joy" was played and a choir of 26 Portuguese children sang the original German lyrics.
In 1992 the anthem was used by CIS national football team at the 1992 UEFA European Football Championship.
On 4 October 2010 the anthem was used when a European team beat a team representing the United States of America to win the Ryder Cup golf tournament. The European Ryder Cup captain Colin Montgomerie decided to break with tradition and play the European anthem by itself instead of the individual anthems from participating European nations. It was similarly employed at the 2014 Ryder Cup prizegiving ceremony on 28 September, after Europe had beaten America under its captain, Paul McGinley.
"Ode to Joy" is used as the theme song to the 2016 UEFA Euro qualifying football competition at the introduction of every match.
"Ode to Joy" , automatically orchestrated in seven different styles, has been used on 18 June 2015 during the ceremony celebrating the 5000th ERC grantee as anthem of the European Research Council to represent achievements of European research.
Unofficial lyrics.
Due to the large number of languages used in the European Union, the anthem is purely instrumental, and the German lyrics that Friedrich Schiller wrote and on which Beethoven based the melody upon have no official status. Despite this, the German lyrics are often sung by choirs or ordinary people when the anthem is played: for example, at the 2004 enlargement on the German-Polish border, the crowd watching the ceremony playing the music sang along with the German lyrics.
Aside from this, several translations of the poem used by Beethoven as well as original works have attempted to provide lyrics to the anthem in various languages. Versions of the anthem including lyrics have been sung outside official EU occasions.
In France, several adaptations of Beethoven's composition were known long before the onset of European Union. A version by the librettist Maurice Bouchor (1855–1929) entitled "Hymn to Universal Humanity" ("Hymne à l'universelle humanité") adding several verses to a preceding version of Jean Ruault, was published. This version and another by Maurice Bouchor, published with Julien Thiersot under the title "Hymn for future times" ("Hymne des temps futurs") in a music book which was widespread among basic schools, is performed unofficially by school choirs during European events. Another version by the Catholic writer Joseph Folliet (1903–1972) is also known.

</doc>
<doc id="10029" url="https://en.wikipedia.org/wiki?curid=10029" title="Timeline of the evolutionary history of life">
Timeline of the evolutionary history of life

This timeline of evolution of life represents the current scientific theory outlining the major events during the development of life on planet Earth. In biology, evolution is any change across successive generations in the heritable characteristics of biological populations. Evolutionary processes give rise to diversity at every level of biological organization, from kingdoms to species, and individual organisms and molecules, such as DNA and proteins. The similarities between all present day organisms indicate the presence of a common ancestor from which all known species, living and extinct, have diverged through the process of evolution. More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described.
While the dates given in this article are estimates based on scientific evidence, there has been controversy between more traditional views of increased biodiversity through a cone of diversity with the passing of time and the view that the basic pattern on Earth has been one of decimation and diversification and that in certain past times, such as the Cambrian explosion, there was great diversity.
Extinctions.
Periodic extinctions have temporarily reduced diversity, eliminating:
Dates are approximate.
Detailed timeline.
In this timeline, Ma (for "megaannum") means "million years ago," ka (for "kiloannum") means "thousand years ago," and ya means "years ago."
Hadean Eon.
4000 Ma and earlier.
Archean Eon.
4000 Ma – 2500 Ma
Proterozoic Eon.
2500 Ma – 542 Ma. Contains the Palaeoproterozoic, Mesoproterozoic and Neoproterozoic eras.
Phanerozoic Eon.
542 Ma – present
The Phanerozoic Eon, literally the "period of well-displayed life," marks the appearance in the fossil record of abundant, shell-forming and/or trace-making organisms. It is subdivided into three eras, the Paleozoic, Mesozoic and Cenozoic, which are divided by major mass extinctions.
Palaeozoic Era.
542 Ma – 251.0 Ma and contains the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian periods.
Mesozoic Era.
From 251.4 Ma to 66 Ma and containing the Triassic, Jurassic and Cretaceous periods.
Cenozoic Era.
66 Ma – present

</doc>
