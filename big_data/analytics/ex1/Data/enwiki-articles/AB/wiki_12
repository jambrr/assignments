<doc id="7424" url="https://en.wikipedia.org/wiki?curid=7424" title="Crochet">
Crochet

Crochet (; ) is a process of creating fabric by interlocking loops of yarn, thread, or strands of other materials using a crochet hook. The name is derived from the French term "crochet", meaning "small hook." These are made of materials such as metal, wood, or plastic and are manufactured commercially and produced in artisan workshops. The salient difference between crochet and knitting, beyond the implements used for their production, is that each stitch in crochet is completed before proceeding with the next one, while knitting keeps a large number of stitches open at a time. (Variant forms such as Tunisian crochet and broomstick lace keep multiple crochet stitches open at a time.)
Etymology.
The word crochet is derived from the Old French "crochet", a diminutive of "croche", in turn from the Germanic "croc", both meaning "hook". It was used in 17th-century French lace making, "crochetage" designating a stitch used to join separate pieces of lace, and "crochet" subsequently designating both a specific type of fabric and the hooked needle used to produce it. Although that fabric is not known to be crochet in the present sense, a genealogical relationship between the techniques sharing that name appears likely.
Origins.
Knitted textiles survive from early periods but the first substantive evidence of crocheted fabric relates to its appearance in Europe during the 19th century. Earlier work identified as crochet was commonly made by nålebinding, a separate looped yarn technique.
The first known published instructions for crochet appeared in the Dutch magazine, "Penélopé", in 1823. This includes a color plate showing five different style purses of which three were intended to be crocheted with silk thread. The first is "simple open crochet" ("crochet simple ajour"), a mesh of chain-stitch arches. The second (illustrated here) starts in a semi-open form ("demi jour"), where chain-stitch arches alternate with equally long segments of slip-stitch crochet, and closes with a star made with "double-crochet stitches" ("dubbelde hekelsteek"— double-crochet in British terminology; single-crochet in US). The third purse is made entirely in double-crochet. The instructions prescribe the use of a tambour needle (as illustrated below) and introduce a number of decorative techniques.
The earliest dated English reference to garments made of cloth produced by looping yarn with a hook — "shepherd's knitting" — is in, "The Memoirs of a Highland Lady", by Elizabeth Grant (1797–1830). The journal entry, itself, is dated 1812 but was not recorded in its subsequently published form until some time between 1845 and 1867, and the actual date of publication was first in 1898. Nonetheless, the 1833 volume of "Penélopé" describes and illustrates a shepherd's hook, and recommends its use for crochet with coarser yarn.
In 1842, one of the numerous books discussing crochet that began to appear in the 1840s states:
Two years later, the same author, writes:
An instruction book from 1846 describes "Shepherd or Single Crochet" as what in current British usage is either called single crochet or slip-stitch crochet, with U.S. American terminology always using the latter (reserving single crochet for use as noted above). It similarly equates "Double" and "French crochet".
Notwithstanding the categorical assertion of a purely British origin, there is solid evidence of a connection between French tambour embroidery and crochet. The former method of production was illustrated in detail in 1763 in Diderot's Encyclopedia. The tip of the needle shown there is indistinguishable from that of a present-day inline crochet hook and the chain stitch separated from a cloth support is a fundamental element of the latter technique. The 1824 "Penélopé" instructions unequivocally state that the tambour tool was used for crochet and the first of the 1840s instruction books uses the terms "tambour" and "crochet" as synonyms. This equivalence is retained in the 4th edition of that work, 1847.
The strong taper of the shepherd's hook eases the production of slip-stitch crochet but is less amenable to stitches that require multiple loops on the hook at the same time. Early yarn hooks were also continuously tapered but gradually enough to accommodate multiple loops. The design with a cylindrical shaft that is commonplace today was largely reserved for tambour-style steel needles. Both types gradually merged into the modern form that appeared toward the end of the 19th century, including both tapered and cylindrical segments, and the continuously tapered bone hook remained in industrial production until World War II.
The early instruction books make frequent reference to the alternate use of 'ivory, bone, or wooden hooks' and 'steel needles in a handle', as appropriate to the stitch being made. Taken with the synonymous labeling of shepherd's- and single crochet, and the similar equivalence of French- and double crochet, there is a strong suggestion that crochet is rooted both in tambour embroidery and shepherd's knitting, leading to thread and yarn crochet respectively; a distinction that is still made. The locus of the fusion of all these elements — the "invention" noted above — has yet to be determined, as does the origin of shepherd's knitting.
Shepherd's hooks are still being made for local slip-stitch crochet traditions. The form in the accompanying photograph is typical for contemporary production. A longer continuously tapering design intermediate between it and the 19th-century tapered hook was also in earlier production, commonly being made from the handles of forks and spoons.
Irish crochet.
In the 19th century, as Ireland was facing the Great Irish Famine (1845-1849), crochet lace work was introduced as a form of famine relief (the production of crocheted lace being an alternative way of making money for impoverished Irish workers). Mademoiselle Riego de la Blanchardiere is generally credited with the invention of Irish Crochet, publishing the first book of patterns in 1846. Irish lace became popular in Europe and America, and was made in quantity until the first World War.
Modern practice and culture.
Fashions in crochet changed with the end of the Victorian era in the 1890s. Crocheted laces in the new Edwardian era, peaking between 1910 and 1920, became even more elaborate in texture and complicated stitching.
The strong Victorian colours disappeared, though, and new publications called for white or pale threads, except for fancy purses, which were often crocheted of brightly colored silk and elaborately beaded. After World War I, far fewer crochet patterns were published, and most of them were simplified versions of the early 20th-century patterns. After World War II, from the late 1940s until the early 1960s, there was a resurgence in interest in home crafts, particularly in the United States, with many new and imaginative crochet designs published for colorful doilies, potholders, and other home items, along with updates of earlier publications. These patterns called for thicker threads and yarns than in earlier patterns and included wonderful variegated colors. The craft remained primarily a homemaker's art until the late 1960s and early 1970s, when the new generation picked up on crochet and popularized granny squares, a motif worked in the round and incorporating bright colors.
Although crochet underwent a subsequent decline in popularity, the early 21st century has seen a revival of interest in handcrafts and DIY, as well as great strides in improvement of the quality and varieties of yarn. There are many more new pattern books with modern patterns being printed, and most yarn stores now offer crochet lessons in addition to the traditional knitting lessons. There are many books you can purchase from local book stores to teach yourself how to crochet whether it be as a beginner or intermediate. There are also many books for children and teenagers who are hoping to take up the hobby. 
Filet crochet, Tunisian crochet, tapestry crochet, broomstick lace, hairpin lace, cro-hooking, and Irish crochet are all variants of the basic crochet method.
Crochet has experienced a revival on the catwalk as well. Christopher Kane's Fall 2011 Ready-to-Wear collection makes intensive use of the granny square, one of the most basic of crochet motifs. In addition, crochet has been utilized many times by designers on the popular reality show "Project Runway". Even websites such as Etsy and Ravelry have made it easier for individual hobbyists to sell and distribute their patterns or projects across the internet.
Laneya Wiles released a music video titled "Straight Hookin'" which makes a play on the word "hookers," which has a double meaning for both "one who crochets" and "a prostitute."
Materials.
Basic materials required for crochet are a hook and some type of material that will be crocheted, most commonly yarn or thread. Additional tools are convenient for keeping stitches counted, measuring crocheted fabric, or making related accessories. Examples include cardboard cutouts, which can be used to make tassels, fringe, and many other items; a pom-pom circle, used to make pom-poms; a tape measure and a gauge measure, both used for measuring crocheted work and counting stitches; a row counter; and occasionally plastic rings, which are used for special projects.
In recent years, yarn selections have moved beyond synthetic and plant and animal-based fibers to include bamboo, qiviut, hemp, and banana stalks, to name a few.
Crochet hook.
The crochet hook comes in many sizes and materials, such as bone, bamboo, aluminium, plastic, and steel. Because sizing is categorized by the diameter of the hook's shaft, a crafter aims to create stitches of a certain size in order to reach a particular gauge specified in a given pattern. If gauge is not reached with one hook, another is used until the stitches made are the needed size. Crafters may have a preference for one type of hook material over another due to aesthetic appeal, yarn glide, or hand disorders such as arthritis, where bamboo or wood hooks are favored over metal for the perceived warmth and flexibility during use. Hook grips and ergonomic hook handles are also available to assist crafters.
Steel crochet hooks range in size from 0.4 to 3.5 millimeters, or from 00 to 16 in American sizing. These hooks are used for fine crochet work such as doilies and lace.
Aluminium, bamboo, and plastic crochet hooks are available from 2.5 to 19 millimeters in size, or from B to S in American sizing.
Artisan-made hooks are often made of hand-turned woods, sometimes decorated with semi-precious stones or beads.
Crochet hooks used for Tunisian crochet are elongated and have a stopper at the end of the handle, while double-ended crochet hooks have a hook on both ends of the handle. There is also a double hooked apparatus called a Cro-hook that has become popular.
A hairpin loom is often used to create lacy and long stitches, known as hairpin lace. While this is not in itself a hook, it is a device used in conjunction with a crochet hook to produce stitches.
See : List of United States standard crochet hook and knitting needle sizes
Yarn.
Yarn for crochet is usually sold as balls or skeins (hanks), although it may also be wound on spools or cones. Skeins and balls are generally sold with a "yarn band", a label that describes the yarn's weight, length, dye lot, fiber content, washing instructions, suggested needle size, likely gauge, etc. It is a common practice to save the yarn band for future reference, especially if additional skeins must be purchased. Crocheters generally ensure that the yarn for a project comes from a single dye lot. The dye lot specifies a group of skeins that were dyed together and thus have precisely the same color; skeins from different dye lots, even if very similar in color, are usually slightly different and may produce a visible stripe when added onto existing work. If insufficient yarn of a single dye lot is bought to complete a project, additional skeins of the same dye lot can sometimes be obtained from other yarn stores or online.
The thickness or weight of the yarn is a significant factor in determining the gauge, i.e., how many stitches and rows are required to cover a given area for a given stitch pattern. Thicker yarns generally require large-diameter crochet hooks, whereas thinner yarns may be crocheted with thick or thin hooks. Hence, thicker yarns generally require fewer stitches, and therefore less time, to work up a given project. Patterns and motifs are coarser with thicker yarns and produce bold visual effects, whereas thinner yarns are best for refined or delicate patternwork. Yarns are standardly grouped by thickness into six categories: superfine, fine, light, medium, bulky and superbulky. Quantitatively, thickness is measured by the number of wraps per inch (WPI). The related "weight per unit length" is usually measured in tex or denier.
Before use, hanks are wound into balls in which the yarn emerges from the center, making crocheting easier by preventing the yarn from becoming easily tangled. The winding process may be performed by hand or done with a ballwinder and swift.
A yarn's usefulness is judged by several factors, such as its "loft" (its ability to trap air), its "resilience" (elasticity under tension), its washability and colorfastness, its "hand" (its feel, particularly softness vs. scratchiness), its durability against abrasion, its resistance to pilling, its "hairiness" (fuzziness), its tendency to twist or untwist, its overall weight and drape, its blocking and felting qualities, its comfort (breathability, moisture absorption, wicking properties) and its appearance, which includes its color, sheen, smoothness and ornamental features. Other factors include allergenicity, speed of drying, resistance to chemicals, moths, and mildew, melting point and flammability, retention of static electricity, and the propensity to accept dyes. Desirable properties may vary for different projects, so there is no one "best" yarn.
Although crochet may be done with ribbons, metal wire or more exotic filaments, most yarns are made by spinning fibers. In spinning, the fibers are twisted so that the yarn resists breaking under tension; the twisting may be done in either direction, resulting in a Z-twist or S-twist yarn. If the fibers are first aligned by combing them and the spinner uses a worsted type drafting method such as the short forward draw, the yarn is smoother and called a "worsted"; by contrast, if the fibers are carded but not combed and the spinner uses a woolen drafting method such as the long backward draw, the yarn is fuzzier and called "woolen-spun". The fibers making up a yarn may be continuous "filament" fibers such as silk and many synthetics, or they may be "staples" (fibers of an average length, typically a few inches); naturally filament fibers are sometimes cut up into staples before spinning. The strength of the spun yarn against breaking is determined by the amount of twist, the length of the fibers and the thickness of the yarn. In general, yarns become stronger with more twist (also called "worst"), longer fibers and thicker yarns (more fibers); for example, thinner yarns require more twist than do thicker yarns to resist breaking under tension. The thickness of the yarn may vary along its length; a "slub" is a much thicker section in which a mass of fibers is incorporated into the yarn.
The spun fibers are generally divided into animal fibers, plant and synthetic fibers. These fiber types are chemically different, corresponding to proteins, carbohydrates and synthetic polymers, respectively. Animal fibers include silk, but generally are long hairs of animals such as sheep (wool), goat (angora, or cashmere goat), rabbit (angora), llama, alpaca, dog, cat, camel, yak, and muskox (qiviut). Plants used for fibers include cotton, flax (for linen), bamboo, ramie, hemp, jute, nettle, raffia, yucca, coconut husk, banana trees, soy and corn. Rayon and acetate fibers are also produced from cellulose mainly derived from trees. Common synthetic fibers include acrylics, polyesters such as dacron and ingeo, nylon and other polyamides, and olefins such as polypropylene. Of these types, wool is generally favored for crochet, chiefly owing to its superior elasticity, warmth and (sometimes) felting; however, wool is generally less convenient to clean and some people are allergic to it. It is also common to blend different fibers in the yarn, e.g., 85% alpaca and 15% silk. Even within a type of fiber, there can be great variety in the length and thickness of the fibers; for example, Merino wool and Egyptian cotton are favored because they produce exceptionally long, thin (fine) fibers for their type.
A single spun yarn may be crochet as is, or braided or plied with another. In plying, two or more yarns are spun together, almost always in the opposite sense from which they were spun individually; for example, two Z-twist yarns are usually plied with an S-twist. The opposing twist relieves some of the yarns' tendency to curl up and produces a thicker, "balanced" yarn. Plied yarns may themselves be plied together, producing "cabled yarns" or "multi-stranded yarns". Sometimes, the yarns being plied are fed at different rates, so that one yarn loops around the other, as in bouclé. The single yarns may be dyed separately before plying, or afterwords to give the yarn a uniform look.
The dyeing of yarns is a complex art. Yarns need not be dyed; or they may be dyed one color, or a great variety of colors. Dyeing may be done industrially, by hand or even hand-painted onto the yarn. A great variety of synthetic dyes have been developed since the synthesis of indigo dye in the mid-19th century; however, natural dyes are also possible, although they are generally less brilliant. The color-scheme of a yarn is sometimes called its colorway. Variegated yarns can produce interesting visual effects, such as diagonal stripes.
Process.
Crocheted fabric is begun by placing a slip-knot loop on the hook (though other methods, such as a magic ring or simple folding over of the yarn may be used), pulling another loop through the first loop, and repeating this process to create a chain of a suitable length. The chain is either turned and worked in rows, or joined to the beginning of the row with a slip stitch and worked in rounds. Rounds can also be created by working many stitches into a single loop. Stitches are made by pulling one or more loops through each loop of the chain. At any one time at the end of a stitch, there is only one loop left on the hook. Tunisian crochet, however, draws all of the loops for an entire row onto a long hook before working them off one at a time. Like knitting, crochet can be worked either flat or in the round.
Types of stitches.
There are five main types of basic stitches. 1. Chain Stitch - the most basic of all stitches and used to begin most projects. 2. Slip Stitch - used to join chain stitch to form a ring. 3. Single Crochet Stitch - easiest stitch to master Single Crochet Stitch Tutorial 4. Half Double Crochet Stitch - the 'in-between' stitch Half-Double Crochet Tutorial 5. Double Crochet Stitch - many uses for this unlimited use stitch Double Crochet Stitch Tutorial
The more advanced stitches include the Shell Stitch, V Stitch, Spike Stitch, Afghan Stitch, Butterfly Stitch, Popcorn Stitch, and Crocodile Stitch.
International crochet terms and notations.
In the English-speaking crochet world, basic stitches have different names that vary by country. The differences are usually referred to as UK/US or British/American. To help counter confusion when reading patterns, a diagramming system using a standard international notation has come into use (illustration, left).
Another terminological difference is known as "tension" (UK) and "gauge" (US). Individual crocheters work yarn with a loose or a tight hold and, if unmeasured, these differences can lead to significant size changes in finished garments that have the same number of stitches. In order to control for this inconsistency, printed crochet instructions include a standard for the number of stitches across a standard swatch of fabric. An individual crocheter begins work by producing a test swatch and compensating for any discrepancy by changing to a smaller or larger hook. North Americans call this "gauge", referring to the end result of these adjustments; British crocheters speak of "tension", which refers to the crafter's grip on the yarn while producing stitches.
Differences from and similarities to knitting.
One of the more obvious differences is that crochet uses one hook while much knitting uses two needles. In most crochet, the artisan usually has only one live stitch on the hook (with the exception being Tunisian crochet), while a knitter keeps an entire row of stitches active simultaneously. Dropped stitches, which can unravel a fabric, rarely interfere with crochet work, due to a second structural difference between knitting and crochet. In knitting, each stitch is supported by the corresponding stitch in the row above and it supports the corresponding stitch in the row below, whereas crochet stitches are only supported by and support the stitches on either side of it. If a stitch in a finished crocheted item breaks, the stitches above and below remain intact, and because of the complex looping of each stitch, the stitches on either side are unlikely to come loose unless heavily stressed.
Round or cylindrical patterns are simple to produce with a regular crochet hook, but cylindrical knitting requires either a set of circular needles or three to five special double-ended needles. Many crocheted items are composed of individual motifs which are then joined together, either by sewing or crocheting, whereas knitting is usually composed of one fabric, such as entrelac.
Freeform crochet is a technique that can create interesting shapes in three dimensions because new stitches can be made independently of previous stitches almost anywhere in the crocheted piece. It is generally accomplished by building shapes or structural elements onto existing crocheted fabric at any place the crafter desires.
Knitting can be accomplished by machine, while many crochet stitches can only be crafted by hand. The height of knitted and crocheted stitches is also different: a single crochet stitch is twice the height of a knit stitch in the same yarn size and comparable diameter tools, and a double crochet stitch is about four times the height of a knit stitch.
While most crochet is made with a hook, there is also a method of crocheting with a knitting loom. This is called loomchet. Slip stitch crochet is very similar to knitting. Each stitch in slip stitch crochet is formed the same way as a knit or purl stitch which is then bound off. A person working in slip stitch crochet can follow a knitted pattern with knits, purls, and cables, and get a similar result.
It is a common perception that crochet produces a thicker fabric than knitting, tends to have less "give" than knitted fabric, and uses approximately a third more yarn for a comparable project than knitted items. Though this is true when comparing a single crochet swatch with a stockinette swatch, both made with the same size yarn and needle/hook, it is not necessarily true for crochet in general. Most crochet uses far less than 1/3 more yarn than knitting for comparable pieces, and a crocheter can get similar feel and drape to knitting by using a larger hook or thinner yarn. Tunisian crochet and slip stitch crochet can in some cases use less yarn than knitting for comparable pieces. According to sources claiming to have tested the 1/3 more yarn assertion, a single crochet stitch (sc) uses approximately the same amount of yarn as knit garter stitch, but more yarn than stockinette stitch. Any stitch using yarnovers uses less yarn than single crochet to produce the same amount of fabric. Cluster stitches, which are in fact multiple stitches worked together, will use the most length.
Standard crochet stitches like sc and dc also produce a thicker fabric, more like knit garter stitch. This is part of why they use more yarn. Slip stitch can produce a fabric much like stockinette that is thinner and therefore uses less yarn.
It is possible to use the same yarn or wool for both crochet and knitting, providing you have the correct size knitting needles or crochet hooks for the yarn you are using. There are some yarn that are only made for crochet, for example DMC make Cebelia No.10 which is a very thin yarn and works well with Amigurumi crochet.
Charity.
It has been very common for people and groups to crochet clothing and other garments and then donate them to soldiers during war. People have also crocheted clothing and then donated it to hospitals, for sick patients and also for newborn babies. Sometimes groups will crochet for a specific charity purpose, such as crocheting for homeless shelters, nursing homes, etc.
It is also becoming increasingly popular to crochet hats (commonly referred to as "chemo caps") and donate them to cancer treatment centers, for those undergoing chemotherapy. During the month of October pink hats and scarves are made and proceeds are donated to breast cancer funds.
Mathematics and hyperbolic crochet.
Crochet has been used to illustrate shapes in hyperbolic space that are difficult to reproduce using other media or are difficult to understand when viewed two-dimensionally. A hyperbolic model of a coral reef has also been constructed for environmental purposes.
A paper model based on the pseudosphere was created by William Thurston, however, it was quite delicate. Crochet has been used by the mathematician Daina Taimina in order to create a version of the hyperbolic plane. Daina Taimina used the art of crochet to create a strong, durable model (see related image), which received an exhibition by the Institute For Figuring.
As hyperbolic and mathematics-based crochet has continued to become more popular, there have been several events highlighting work from various fiber artists. Two such shows include Sant Ocean Hall at the Smithsonian in Washington D.C. and Sticks, Hooks, and the Mobius: Knit and Crochet Go Cerebral at Lafayette College in Pennsylvania.
Architecture.
In "Style in the technical arts", Gottfried Semper looks at the textile with great promise and historical precedent. In Section 53, he writes of the "loop stitch, or Noeud Coulant: a knot that, if untied, causes the whole system to unravel." In the same section, Semper confesses his ignorance of the subject of crochet but believes strongly that it is a technique of great value as a textile technique and possibly something more.
There are a small number of architects currently interested in the subject of crochet as it relates to architecture. The following publications, explorations and thesis projects can be used as a resource to see how crochet is being used within the capacity of architecture.
Yarn bombing.
In the past few years, a practice called yarn bombing, or the use of knitted or crocheted cloth to modify and beautify one's (usually outdoor) surroundings, emerged in the US and spread worldwide. Yarn bombers sometimes target existing pieces of graffiti for beautification. In 2010, an entity dubbed "the Midnight Knitter" hit West Cape May. Residents awoke to find knit cozies hugging tree branches and sign poles. In September 2015, Grace Brett was named "The World's Oldest Yarn Bomber". She is part of a group of yarn graffiti-artists called the Souter Stormers, who beautify their local town in Scotland. When she is not yarn bombing, she is utilizing her craft by making items for her children and grandchildren.
Entomology.
Prolegs of lepidopteran larvae have a small circle of gripping hooks, called "crochets". The arrangement of the crochets can be helpful in identification to family level. Although the point has been debated, prolegs are not widely regarded as true legs, derived from the primitive uniramous limbs. Certainly in their morphology they are not jointed, and so lack the five segments (coxa, trochanter, femur, tibia, tarsus) of thoracic insect legs. Prolegs do have limited musculature, but much of their movement is hydraulically powered.

</doc>
<doc id="7425" url="https://en.wikipedia.org/wiki?curid=7425" title="Electromagnetic coil">
Electromagnetic coil

An electromagnetic coil is an electrical conductor such as a wire in the shape of a coil, spiral or helix. Electromagnetic coils are used in electrical engineering, in applications where electric currents interact with magnetic fields, in devices such as inductors, electromagnets, transformers, and sensor coils. Either an electric current is passed through the wire of the coil to generate a magnetic field, or conversely an external "time-varying" magnetic field through the interior of the coil generates an EMF (voltage) in the conductor.
A current through any conductor creates a circular magnetic field around the conductor due to Ampere's law. The advantage of using the coil shape is that it increases the strength of magnetic field produced by a given current. The magnetic fields generated by the separate turns of wire all pass through the center of the coil and add (superpose) to produce a strong field there. The more turns of wire, the stronger the field produced. Conversely, a "changing" external magnetic flux induces a voltage in a conductor such as a wire, due to Faraday's law of induction. The induced voltage can be increased by winding the wire into a coil, because the field lines intersect the circuit multiple times.
The direction of the magnetic field produced by a coil can be determined by the right hand grip rule. If the fingers of the right hand are wrapped around the magnetic core of a coil in the direction of conventional current through the wire, the thumb will point in the direction the magnetic field lines pass through the coil. The end of a magnetic core from which the field lines emerge is defined to be the North pole.
There are many different types of coils used in electric and electronic equipment.
Windings and taps.
The wire or conductor which constitutes the coil is called the winding. The hole in the center of the coil is called the core area or "magnetic axis". Each loop of wire is called a turn. In windings in which the turns touch, the wire must be insulated with a coating of nonconductive insulation such as plastic or enamel to prevent the current from passing between the wire turns. The winding is often wrapped around a "coil form" made of plastic or other material to hold it in place. The ends of the wire are brought out and attached to an external circuit. Windings may have additional electrical connections along their length; these are called taps. A winding which has a single tap in the center of its length is called center-tapped.
Coils can have more than one winding, insulated electrically from each other. When there are two or more windings around a common magnetic axis, the windings are said to be inductively coupled or magnetically coupled. A time-varying current through one winding will create a time-varying magnetic field which passes through the other winding, which will induce a time-varying voltage in the other windings. This is called a transformer.
Magnetic core.
Many electromagnetic coils have a magnetic core, a piece of ferromagnetic material like iron in the center to increase the magnetic field. The current through the coil magnetizes the iron, and the field of the magnetized material adds to the field produced by the wire. This is called a ferromagnetic-core or iron-core coil. A ferromagnetic core can increase the magnetic field of a coil by hundreds or thousands of times over what it would be without the core. A ferrite core coil is a variety of coil with a core made of ferrite, a ferrimagnetic ceramic compound. Ferrite coils have lower losses at high frequencies.
A coil without a ferromagnetic core is called an air-core coil. This includes coils wound on plastic or other nonmagnetic forms, as well as coils which actually have empty air space inside their windings.
Types of coils.
Coils can be classified by the frequency of the current they are designed to operate with:
Coils can be classified by their function:
Electromagnets.
Electromagnets are coils that generate a magnetic field for some external use, often to exert a mechanical force on something. A few specific types: 
Inductors.
Inductors or reactors are coils which generate a magnetic field which interacts with the coil itself, to induce a back EMF which opposes changes in current through the coil. Inductors are used as circuit elements in electrical circuits, to temporarily store energy or resist changes in current. A few types:
Transformers.
A transformer is a device with two or more magnetically coupled windings (or sections of a single winding). A time varying current in one coil (called the primary winding) generates a magnetic field which induces a voltage in the other coil (called the secondary winding). A few types: 
Transducer coils.
These are coils used to translate time-varying magnetic fields to electric signals, and vice versa. A few types:
There are also types of coil which don't fit into these categories.

</doc>
<doc id="7426" url="https://en.wikipedia.org/wiki?curid=7426" title="Charles I of England">
Charles I of England

Charles I (19 November 1600 – 30 January 1649) was monarch of the three kingdoms of England, Scotland, and Ireland from 27 March 1625 until his execution in 1649.
Charles was the second son of King James VI of Scotland, but after his father inherited the English throne in 1603, he moved to England, where he spent much of the rest of his life. He became heir apparent to the English, Irish and Scottish thrones on the death of his elder brother, Henry Frederick, Prince of Wales, in 1612. An unsuccessful and unpopular attempt to marry him to the Spanish Habsburg princess Maria Anna culminated in an eight-month visit to Spain in 1623 that demonstrated the futility of the marriage negotiations. Two years later, he married the Bourbon princess Henrietta Maria of France instead.
After his succession, Charles quarrelled with the Parliament of England, which sought to curb his royal prerogative. Charles believed in the divine right of kings and thought he could govern according to his own conscience. Many of his subjects opposed his policies, in particular the levying of taxes without parliamentary consent, and perceived his actions as those of a tyrannical absolute monarch. His religious policies, coupled with his marriage to a Roman Catholic, generated the antipathy and mistrust of reformed groups such as the Puritans and Calvinists, who thought his views too Catholic. He supported high church ecclesiastics, such as Richard Montagu and William Laud, and failed to aid Protestant forces successfully during the Thirty Years' War. His attempts to force the Church of Scotland to adopt high Anglican practices led to the Bishops' Wars, strengthened the position of the English and Scottish parliaments and helped precipitate his own downfall.
From 1642, Charles fought the armies of the English and Scottish parliaments in the English Civil War. After his defeat in 1645, he surrendered to a Scottish force that eventually handed him over to the English Parliament. Charles refused to accept his captors' demands for a constitutional monarchy, and temporarily escaped captivity in November 1647. Re-imprisoned on the Isle of Wight, Charles forged an alliance with Scotland, but by the end of 1648 Oliver Cromwell's New Model Army had consolidated its control over England. Charles was tried, convicted, and executed for high treason in January 1649. The monarchy was abolished and a republic called the Commonwealth of England was declared. The monarchy was restored to Charles's son, Charles II, in 1660.
Early life.
The second son of King James VI of Scotland and Anne of Denmark, Charles was born in Dunfermline Palace, Fife, on 19 November 1600. At a Protestant ceremony in the Chapel Royal at Holyrood Palace in Edinburgh on 23 December 1600, he was baptised by David Lindsay, Bishop of Ross, and created Duke of Albany, the traditional title of the second son of the King of Scotland, with the subsidiary titles of Marquess of Ormond, Earl of Ross and Lord Ardmannoch.
James VI was the first cousin twice removed of Queen Elizabeth I of England, and when she died childless in March 1603, he became King of England as James I. Charles was a weak and sickly infant, and while his parents and older siblings left for England in April and early June that year, due to his fragile health, he remained in Scotland with his father's friend Lord Fyvie, appointed as his guardian.
By 1604, when Charles was three and a half, he was able to walk the length of the great hall at Dunfermline Palace without assistance, and it was decided that he was strong enough to make the journey to England to be reunited with his family. In mid-July 1604, Charles left Dunfermline for England where he was to spend most of the rest of his life. In England, Charles was placed under the charge of Elizabeth, Lady Carey, the wife of courtier Sir Robert Carey, who put him in boots made of Spanish leather and brass to help strengthen his weak ankles. His speech development was also slow, and he retained a stammer, or hesitant speech, for the rest of his life.
In January 1605, Charles was created Duke of York, as is customary in the case of the English sovereign's second son, and made a Knight of the Bath. Thomas Murray, a Presbyterian Scot, was appointed as a tutor. Charles learnt the usual subjects of classics, languages, mathematics and religion. In 1611, he was made a Knight of the Garter.
Eventually, Charles apparently conquered his physical infirmity, which might have been caused by rickets. He became an adept horseman and marksman, and took up fencing. Even so, his public profile remained low in contrast to that of his physically stronger and taller elder brother, Henry Frederick, Prince of Wales, whom Charles adored and attempted to emulate. However, in early November 1612, Henry died at the age of 18 of what is suspected to have been typhoid (or possibly porphyria). Charles, who turned 12 two weeks later, became heir apparent. As the eldest surviving son of the sovereign, Charles automatically gained several titles (including Duke of Cornwall and Duke of Rothesay). Four years later, in November 1616, he was created Prince of Wales and Earl of Chester.
Heir apparent.
In 1613, his sister Elizabeth married Frederick V, Elector Palatine, and moved to Heidelberg. In 1617, the Habsburg Archduke Ferdinand of Austria, a Catholic, was elected king of Bohemia. The following year, the Bohemians rebelled, defenestrating the Catholic governors. In August 1619, the Bohemian diet chose as their monarch Frederick V, who was leader of the Protestant Union, while Ferdinand was elected Holy Roman Emperor in the imperial election. Frederick's acceptance of the Bohemian crown in defiance of the emperor marked the beginning of the turmoil that would develop into the Thirty Years' War. The conflict, originally confined to Bohemia, spiralled into a wider European war, which the English Parliament and public quickly grew to see as a polarised continental struggle between Catholics and Protestants. In 1620, Charles's brother-in-law, Frederick V, was defeated at the Battle of White Mountain near Prague and his hereditary lands in the Electoral Palatinate were invaded by a Habsburg force from the Spanish Netherlands. James, however, had been seeking marriage between the new Prince of Wales and Ferdinand's niece, Habsburg princess Maria Anna of Spain, and began to see the Spanish match as a possible diplomatic means of achieving peace in Europe.
Unfortunately for James, negotiation with Spain proved generally unpopular, both with the public and with James's court. The English Parliament was actively hostile towards Spain and Catholicism, and thus, when called by James in 1621, the members hoped for an enforcement of recusancy laws, a naval campaign against Spain, and a Protestant marriage for the Prince of Wales. James's Lord Chancellor, Francis Bacon, was impeached before the House of Lords for corruption. The impeachment was the first since 1459 without the king's official sanction in the form of a bill of attainder. The incident set an important precedent as the process of impeachment would later be used against Charles and his supporters: the Duke of Buckingham, Archbishop Laud, and the Earl of Strafford. James insisted that the House of Commons be concerned exclusively with domestic affairs, while the members protested that they had the privilege of free speech within the Commons' walls, demanding war with Spain and a Protestant Princess of Wales. Charles, like his father, considered the discussion of his marriage in the Commons impertinent and an infringement of his father's royal prerogative. In January 1622, James dissolved Parliament, angry at what he perceived as the members' impudence and intransigence.
Charles and the Duke of Buckingham, James's favourite and a man who had great influence over the prince, travelled incognito to Spain in February 1623 to try to reach agreement on the long-pending Spanish match. In the end, however, the trip was an embarrassing failure. The Infanta thought Charles was little more than an infidel, and the Spanish at first demanded that he convert to Roman Catholicism as a condition of the match. The Spanish insisted on toleration of Catholics in England and the repeal of the penal laws, which Charles knew would never be agreed by Parliament, and that the Infanta remain in Spain for a year after any wedding to ensure that England complied with all the terms of the treaty. A personal quarrel erupted between Buckingham and the Count of Olivares, the Spanish chief minister, and so Charles conducted the ultimately futile negotiations personally. When Charles returned to London in October, without a bride and to a rapturous and relieved public welcome, he and Buckingham pushed a reluctant King James to declare war on Spain.
With the encouragement of his Protestant advisers, James summoned the English Parliament in 1624 so that he could request subsidies for a war. Charles and Buckingham supported the impeachment of the Lord Treasurer, Lionel Cranfield, 1st Earl of Middlesex, who opposed war on grounds of cost and who quickly fell in much the same manner as Bacon had. James told Buckingham he was a fool, and presciently warned his son that he would live to regret the revival of impeachment as a parliamentary tool. An under-funded makeshift army under Ernst von Mansfeld set off to recover the Palatinate, but it was so poorly provisioned that it never advanced beyond the Dutch coast.
By 1624, James was growing ill, and as a result was finding it difficult to control Parliament. By the time of his death in March 1625, Charles and the Duke of Buckingham had already assumed "de facto" control of the kingdom.
Early reign.
With the failure of the Spanish match, Charles and Buckingham turned their attention to France. On 1 May 1625 Charles was married by proxy to the fifteen-year-old French princess Henrietta Maria in front of the doors of the Notre Dame de Paris. Charles had seen Henrietta Maria in Paris while en route to Spain. The couple married in person on 13 June 1625 in Canterbury. Charles delayed the opening of his first Parliament until after the second ceremony, to forestall any opposition. Many members of the Commons were opposed to the king's marriage to a Roman Catholic, fearing that Charles would lift restrictions on Catholic recusants and undermine the official establishment of the reformed Church of England. Although he told Parliament that he would not relax religious restrictions, he promised to do exactly that in a secret marriage treaty with Louis XIII of France. Moreover, the treaty placed under French command an English naval force that would be used to suppress the Protestant Huguenots at La Rochelle. Charles was crowned on 2 February 1626 at Westminster Abbey, but without his wife at his side because she refused to participate in a Protestant religious ceremony.
Distrust of Charles's religious policies increased with his support of a controversial anti-Calvinist ecclesiastic, Richard Montagu, who was in disrepute among the Puritans. In his pamphlet "A New Gag for an Old Goose" (1624), a reply to the Catholic pamphlet "A New Gag for the New Gospel", Montagu argued against Calvinist predestination, the doctrine that salvation and damnation were preordained by God. Anti-Calvinists – known as Arminians – believed that human beings could influence their own fate through the exercise of free will. Arminian divines had been one of the few sources of support for Charles's proposed Spanish marriage. With the support of King James, Montagu produced another pamphlet, entitled "Appello Caesarem", in 1625 shortly after the old king's death and Charles's accession. To protect Montagu from the stricture of Puritan members of Parliament, Charles made the cleric one of his royal chaplains, increasing many Puritans' suspicions that Charles favoured Arminianism as a clandestine attempt to aid the resurgence of Catholicism.
Rather than direct involvement in the European land war, the English Parliament preferred a relatively inexpensive naval attack on Spanish colonies in the New World, hoping for the capture of the Spanish treasure fleets. Parliament voted to grant a subsidy of £140,000, which was an insufficient sum for Charles's war plans. Moreover, the House of Commons limited its authorisation for royal collection of tonnage and poundage (two varieties of customs duties) to a period of one year, although previous sovereigns since Henry VI of England had been granted the right for life. In this manner, Parliament could delay approval of the rates until after a full-scale review of customs revenue. The bill made no progress in the House of Lords past its first reading. Although no Parliamentary Act for the levy of tonnage and poundage was obtained, Charles continued to collect the duties.
A poorly conceived and executed naval expedition against Spain under the leadership of Buckingham went badly, and the House of Commons began proceedings for the impeachment of the duke. In May 1626, Charles nominated Buckingham as Chancellor of Cambridge University in a show of support, and had two members who had spoken against Buckingham – Dudley Digges and Sir John Eliot – arrested at the door of the House. The Commons was outraged by the imprisonment of two of their members, and after about a week in custody, both were released. On 12 June 1626, the Commons launched a direct protestation attacking Buckingham, stating,
"We protest before your Majesty and the whole world that until this great person be removed from intermeddling with the great affairs of state, we are out of hope of any good success; and do fear that any money we shall or can give will, through his misemployment, be turned rather to the hurt and prejudice of this your kingdom than otherwise, as by lamentable experience we have found those large supplies formerly and lately given." Despite Parliament's protests, however, Charles refused to dismiss his friend, dismissing Parliament instead.
Meanwhile, domestic quarrels between Charles and Henrietta Maria were souring the early years of their marriage. Disputes over her jointure, appointments to her household, and the practice of her religion culminated in the king expelling the vast majority of her French attendants in August 1626. Despite Charles's agreement to provide the French with English ships as a condition of marrying Henrietta Maria, in 1627 he launched an attack on the French coast to defend the Huguenots at La Rochelle. The action, led by Buckingham, was ultimately unsuccessful. Buckingham's failure to protect the Huguenots – and his retreat from Saint-Martin-de-Ré – spurred Louis XIII's siege of La Rochelle and furthered the English Parliament's and people's detestation of the duke.
Charles provoked further unrest by trying to raise money for the war through a "forced loan": a tax levied without parliamentary consent. In November 1627, the test case in the King's Bench, the "Five Knights' Case", found that the king had a prerogative right to imprison without trial those who refused to pay the forced loan. Summoned again in March 1628, on 26 May Parliament adopted a Petition of Right, calling upon the king to acknowledge that he could not levy taxes without Parliament's consent, not impose martial law on civilians, not imprison them without due process, and not quarter troops in their homes. Charles assented to the petition on 7 June, but by the end of the month he had prorogued Parliament and re-asserted his right to collect customs duties without authorisation from Parliament.
On 23 August 1628, Buckingham was assassinated. Charles was deeply distressed. According to Edward Hyde, 1st Earl of Clarendon, he "threw himself upon his bed, lamenting with much passion and with abundance of tears". He remained grieving in his room for two days. In contrast, the public rejoiced at Buckingham's death, which accentuated the gulf between the court and the nation, and between the Crown and the Commons. Although the death of Buckingham effectively ended the war with Spain and eliminated his leadership as an issue, it did not end the conflicts between Charles and Parliament. It did, however, coincide with an improvement in Charles's relationship with his wife, and by November 1628 their old quarrels were at an end. Perhaps Charles's emotional ties were transferred from Buckingham to Henrietta Maria. She became pregnant for the first time, and the bond between them grew ever stronger. Together, they embodied an image of virtue and family life, and their court became a model of formality and morality.
Personal rule.
Parliament prorogued.
In January 1629, Charles opened the second session of the English Parliament, which had been prorogued in June 1628, with a moderate speech on the tonnage and poundage issue. Members of the House of Commons began to voice opposition to Charles's policies in light of the case of John Rolle, a Member of Parliament whose goods had been confiscated for failing to pay tonnage and poundage. Many MPs viewed the imposition of the tax as a breach of the Petition of Right. When Charles ordered a parliamentary adjournment on 2 March, members held the Speaker, Sir John Finch, down in his chair so that the ending of the session could be delayed long enough for resolutions against Catholicism, Arminianism and tonnage and poundage to be read out and acclaimed by the chamber. The provocation was too much for Charles, who dissolved Parliament and had nine parliamentary leaders, including Sir John Eliot, imprisoned over the matter, thereby turning the men into martyrs, and giving popular cause to their protest.
Shortly after the prorogation, without the means in the foreseeable future to raise funds from Parliament for a European war, or the influence of Buckingham, Charles made peace with France and Spain. The following eleven years, during which Charles ruled England without a Parliament, are referred to as the personal rule or the "eleven years' tyranny". Ruling without Parliament was not exceptional, and was supported by precedent. Only Parliament, however, could legally raise taxes, and without it Charles's capacity to acquire funds for his treasury was limited to his customary rights and prerogatives.
Finances.
A large fiscal deficit had arisen in the reigns of Elizabeth I and James I. Notwithstanding Buckingham's short lived campaigns against both Spain and France, there was little financial capacity for Charles to wage wars overseas. Throughout his reign Charles was obliged to rely primarily on volunteer forces for defence and on diplomatic efforts to support his sister, Elizabeth, and his foreign policy objective for the restoration of the Palatinate. England was still the least taxed country in Europe, with no official excise and no regular direct taxation. To raise revenue without reconvening Parliament, Charles resurrected an all-but-forgotten law called the "Distraint of Knighthood", in abeyance for over a century, which required any man who earned £40 or more from land each year to present himself at the king's coronation to be knighted. Relying on this old statute, Charles fined individuals who had failed to attend his coronation in 1626.
The chief tax imposed by Charles was a feudal levy known as ship money, which proved even more unpopular, and lucrative, than poundage and tonnage before it. Previously, collection of ship money had been authorised only during wars, and only on coastal regions. Charles, however, argued that there was no legal bar to collecting the tax for defence during peacetime and throughout the whole of the kingdom. Ship money, paid directly to the Treasury of the Navy, provided between £150,000 to £200,000 annually between 1634 and 1638, after which yields declined. Opposition to ship money steadily grew, but the 12 common law judges of England declared that the tax was within the king's prerogative, though some of them had reservations. The prosecution of John Hampden for non-payment in 1637–38 provided a platform for popular protest, and the judges only found against Hampden by the narrow margin of 7–5.
The king also derived money through the granting of monopolies, despite a statute forbidding such action, which, though inefficient, raised an estimated £100,000 a year in the late 1630s. One such monopoly was for soap, referred to as popish soap. Charles also raised funds from the Scottish nobility, at the price of considerable acrimony, by the Act of Revocation (1625), whereby all gifts of royal or church land made to the nobility since 1540 were revoked, with continued ownership being subject to an annual rent. In addition, the boundaries of the royal forests in England were restored to their ancient limits as part of a scheme to maximise income by exploiting the land and fining land users within the reasserted boundaries for encroachment. The focus of the programme was disafforestation and sale of forest lands for development as pasture and arable, or in the case of the Forest of Dean, development for the iron industry. Disafforestation frequently caused riots and disturbances including those known as the Western Rising.
The practice of granting extensive monopolies agitated the public, who were forced to pay higher prices by the monopoly holders. Against the background of this unrest, Charles faced bankruptcy in the summer of 1640 as parliament continued to refuse new taxes. The City of London, preoccupied with its own grievances further refused to make any loans to the king, and likewise he was unable to subscribe any foreign loans. In this extremity, Charles seized the money held in trust at the mint of the Exchequer in the tower of London. The royal mint held a monopoly on the exchange of foreign coin and from this the mint operated as a bank containing much capital of the merchants and goldsmiths of the city. In July, Charles seized all £130,000 of this money, and in August he followed it up by seizing all the stocks of pepper held by the East India Company, and selling it at distress prices.
Religious conflicts.
Throughout Charles's reign, the issue of how far the English Reformation should progress was constantly brought to the forefront of political debate. Arminian theology emphasised clerical authority and the individual's ability to reject or accept salvation, and was consequently viewed as heretical and a potential vehicle for the reintroduction of Roman Catholicism by its Calvinist opponents. Charles's sympathy to the teachings of Arminianism, and specifically his wish to move the Church of England away from Calvinism in a more traditional and sacramental direction, were perceived by Puritans as irreligious tendencies. In addition, Charles's Protestant subjects followed news of the European war closely and grew increasingly dismayed by Charles's diplomacy with Spain and his failure to support the Protestant cause abroad effectively.
In 1633, Charles appointed William Laud as Archbishop of Canterbury. Together, they began a series of anti-Calvinist reforms that attempted to ensure religious uniformity by restricting non-conformist preachers, insisting that the liturgy be celebrated as prescribed in the Book of Common Prayer, organising the internal architecture of English churches so as to emphasise the sacrament of the altar, and re-issuing King James's Declaration of Sports, which permitted secular activities on the sabbath. The Feoffees for Impropriations, an organisation that bought benefices and advowsons so that Puritans could be appointed to them, was dissolved. To prosecute those who opposed his reforms, Laud used the two most powerful courts in the land, the Court of High Commission and the Court of Star Chamber. The courts became feared for their censorship of opposing religious views, and became unpopular among the propertied classes for inflicting degrading punishments on gentlemen. For example, in 1637 William Prynne, Henry Burton and John Bastwick were pilloried, whipped and mutilated by cropping and imprisoned indefinitely for publishing anti-episcopal pamphlets.
When Charles attempted to impose his religious policies in Scotland he faced numerous difficulties. Although born in Scotland, Charles had become estranged from his northern kingdom; his first visit since early childhood was for his Scottish coronation in 1633. To the dismay of the Scots, who had removed many traditional rituals from their liturgical practice, Charles insisted that the coronation be conducted in the Anglican rite. In 1637, the king ordered the use of a new prayer book in Scotland that was almost identical to the English Book of Common Prayer, without consulting either the Scottish Parliament or the Kirk. Although written, under Charles's direction, by Scottish bishops, many Scots resisted it, seeing the new prayer book as a vehicle for introducing Anglicanism to Scotland. On 23 July, riots erupted in Edinburgh upon the first Sunday of the prayer book's usage, and unrest spread throughout the Kirk. The public began to mobilise around a reaffirmation of the National Covenant, whose signatories pledged to uphold the reformed religion of Scotland and reject any innovations that were not authorised by Kirk and Parliament. When the General Assembly of the Church of Scotland met in November 1638, it condemned the new prayer book, abolished episcopal church government by bishops, and adopted Presbyterian government by elders and deacons.
Bishops' Wars.
Charles perceived the unrest in Scotland as a rebellion against his authority, precipitating the First Bishops' War in 1639. Charles did not seek subsidies from the English Parliament to wage war, but instead raised an army without parliamentary aid and marched to Berwick-upon-Tweed, on the border of Scotland. Charles's army did not engage the Covenanters as the king feared the defeat of his forces, whom he believed to be significantly outnumbered by the Scots. In the Treaty of Berwick, Charles regained custody of his Scottish fortresses and secured the dissolution of the Covenanters' interim government, albeit at the decisive concession that both the Scottish Parliament and General Assembly of the Scottish Church were called.
Charles's military failure in the First Bishops' War caused a financial and diplomatic crisis for Charles that deepened when his efforts to raise finance from Spain, while simultaneously continuing his support for his Palatine relatives, led to the public humiliation of the Battle of the Downs, where the Dutch destroyed a Spanish bullion fleet off the coast of Kent in sight of the impotent English navy.
Charles continued peace negotiations with the Scots in a bid to gain time before launching a new military campaign. Because of his financial weakness, he was forced to call Parliament into session in an attempt to raise funds for such a venture. Both English and Irish parliaments were summoned in the early months of 1640. In March 1640, the Irish Parliament duly voted in a subsidy of £180,000 with the promise to raise an army 9,000 strong by the end of May. In the English general election in March, however, court candidates fared badly, and Charles's dealings with the English Parliament in April quickly reached stalemate. The earls of Northumberland and Strafford attempted to broker a compromise whereby the king would agree to forfeit ship money in exchange for £650,000 (although the cost of the coming war was estimated at around £1 million). Nevertheless, this alone was insufficient to produce consensus in the Commons. The Parliamentarians' calls for further reforms were ignored by Charles, who still retained the support of the House of Lords. Despite the protests of Northumberland, the Short Parliament (as it came to be known) was dissolved in May 1640, less than a month after it assembled.
By this stage Strafford, Lord Deputy of Ireland since 1632, had emerged as Charles's right-hand man and together with Laud, pursued a policy of "Thorough" that aimed to make central royal authority more efficient and effective at the expense of local or anti-government interests. Although originally a critic of the king, Strafford defected to royal service in 1628 (due in part to Buckingham's persuasion), and had since emerged, alongside Laud, as the most influential of Charles's ministers.
Bolstered by the failure of the English Short Parliament, the Scottish Parliament declared itself capable of governing without the king's consent and, in August 1640, the Covenanter army moved into the English county of Northumberland. Following the illness of the earl of Northumberland, who was the king's commander-in-chief, Charles and Strafford went north to command the English forces, despite Strafford being ill himself with a combination of gout and dysentery. The Scottish soldiery, many of whom were veterans of the Thirty Years' War, had far greater morale and training compared to their English counterparts, and met virtually no resistance until reaching Newcastle upon Tyne where, at the Battle of Newburn, they defeated the English forces and occupied the city, as well as the neighbouring county of Durham.
As demands for a parliament grew, Charles took the unusual step of summoning a great council of peers. By the time it met, on 24 September at York, Charles had resolved to follow the almost universal advice to call a parliament. After informing the peers that a parliament would convene in November, he asked them to consider how he could acquire funds to maintain his army against the Scots in the meantime. They recommended making peace. A cessation of arms, although not a final settlement, was negotiated in the humiliating Treaty of Ripon, signed in October 1640. The treaty stated that the Scots would continue to occupy Northumberland and Durham and be paid £850 per day until peace was restored and the English Parliament recalled, which would be required to raise sufficient funds to pay the Scottish forces. Consequently, Charles summoned what later became known as the Long Parliament. Once again, Charles's supporters fared badly at the polls. Of the 493 members of the Commons returned in November, over 350 were opposed to the king.
Long Parliament.
Tensions escalate.
The Long Parliament proved just as difficult for Charles as had the Short Parliament. It assembled on 3 November 1640 and quickly began proceedings to impeach the king's leading counsellors of high treason. Strafford was taken into custody on 10 November; Laud was impeached on 18 December; Lord Keeper Finch was impeached the following day, and he consequently fled to the Hague with Charles's permission on 21 December. To prevent the king from dissolving it at will, Parliament passed the Triennial Act, which required Parliament to be summoned at least once every three years, and permitted the Lord Keeper of the Great Seal and 12 peers to summon Parliament if the king failed to do so. The Act was coupled with a subsidy bill, and so to secure the latter, Charles grudgingly granted royal assent in February 1641.
Strafford had become the principal target of the Parliamentarians, particularly John Pym, and he went on trial for high treason on 22 March 1641. However, the key allegation by Sir Henry Vane that Strafford had threatened to use the Irish army to subdue England was not corroborated and on 10 April Pym's case collapsed. Pym and his allies immediately launched a bill of attainder, which simply declared Strafford guilty and pronounced the sentence of death.
Charles assured Strafford that "upon the word of a king you shall not suffer in life, honour or fortune", and the attainder could not succeed if Charles withheld assent. Furthermore, many members and most peers were opposed to the attainder, not wishing, in the words of one, to "commit murder with the sword of justice". However, increased tensions and an attempted coup by royalist army officers in support of Strafford and in which Charles was involved began to sway the issue. The Commons passed the bill on 20 April by a large margin (204 in favour, 59 opposed, and 230 abstained), and the Lords acquiesced (by 26 votes to 19, with 79 absent) in May. Charles, fearing for the safety of his family in the face of unrest, assented reluctantly on 9 May after consulting his judges and bishops. Strafford was beheaded three days later.
On 3 May, Parliament's Protestation had attacked the "wicked counsels" of Charles's "arbitrary and tyrannical government"; while those who signed the petition undertook to defend the king's "person, honour and estate", they also swore to preserve "the true reformed religion", parliament, and the "rights and liberties of the subjects". Within a week, Charles had assented to an unprecedented Act, which forbade the dissolution of the English Parliament without Parliament's consent. In the following months, ship money, fines in distraint of knighthood and excise without parliamentary consent were declared unlawful, and the Courts of Star Chamber and High Commission were abolished. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. The House of Commons also launched bills attacking bishops and episcopacy, but these failed in the Lords.
Charles had made important concessions in England, and temporarily improved his position in Scotland by securing the favour of the Scots on a visit from August to November 1641 during which he conceded to the official establishment of Presbyterianism. However, following an attempted royalist coup in Scotland, known as "The Incident", Charles's credibility was significantly undermined.
Irish rebellion.
In Ireland, the population was split into three main socio-political groups: the Gaelic Irish, who were Catholic; the Old English, who were descended from medieval Normans and were also predominantly Catholic; and the New English, who were Protestant settlers from England and Scotland aligned with the English Parliament and the Covenanters. Strafford's administration had improved the Irish economy and boosted tax revenue, but had done so by heavy-handedly imposing order. He had trained up a large Catholic army in support of the king and had weakened the authority of the Irish Parliament, while continuing to confiscate land from Catholics for Protestant settlement at the same time as promoting a Laudian Anglicanism that was anathema to Presbyterians. As a result, all three groups had become disaffected. Strafford's impeachment provided a new departure for Irish politics whereby all sides joined together to present evidence against him. In a similar manner to the English Parliament, the Old English members of the Irish Parliament argued that while opposed to Strafford they remained loyal to Charles. They argued that the king had been led astray by malign counsellors, and that, moreover, a viceroy such as Strafford could emerge as a despotic figure instead of ensuring that the king was directly involved in governance. Strafford's fall from power weakened Charles's influence in Ireland. The dissolution of the Irish army was unsuccessfully demanded three times by the English Commons during Strafford's imprisonment, until Charles was eventually forced through lack of money to disband the army at the end of Strafford's trial. Disputes concerning the transfer of land ownership from native Catholic to settler Protestant, particularly in relation to the plantation of Ulster, coupled with resentment at moves to ensure the Irish Parliament was subordinate to the Parliament of England, sowed the seeds of rebellion. When armed conflict arose between the Gaelic Irish and New English, in late October 1641, the Old English sided with the Gaelic Irish while simultaneously professing their loyalty to the king.
In November 1641, the House of Commons passed the Grand Remonstrance, a long list of grievances against actions by Charles's ministers committed since the beginning of his reign (that were asserted to be part of a grand Catholic conspiracy of which the king was an unwitting member), but it was in many ways a step too far by Pym and passed by only 11 votes – 159 to 148. Furthermore, the Remonstrance had very little support in the House of Lords, which the Remonstrance attacked. The tension was heightened by news of the Irish rebellion, coupled with inaccurate rumours of Charles's complicity. Throughout November, a series of alarmist pamphlets published stories of atrocities in Ireland, which included massacres of New English settlers by the native Irish who could not be controlled by the Old English lords. Rumours of "papist" conspiracies in England circulated the kingdom, and English anti-Catholic opinion was strengthened, damaging Charles's reputation and authority.
The English Parliament distrusted Charles's motivations when he called for funds to put down the Irish rebellion; many members of the Commons suspected that forces raised by Charles might later be used against Parliament itself. Pym's Militia Bill was intended to wrest control of the army from the king, but it did not have the support of the Lords, let alone Charles. Instead, the Commons passed the bill as an ordinance, which they claimed did not require royal assent. The Militia Ordinance appears to have prompted more members of the Lords to support the king. In an attempt to strengthen his position, Charles generated great antipathy in London, which was already fast falling into anarchy, when he placed the Tower of London under the command of Colonel Thomas Lunsford, an infamous, albeit efficient, career officer. When rumours reached Charles that Parliament intended to impeach his wife for supposedly conspiring with the Irish rebels, the king decided to take drastic action.
Five members.
Charles suspected, probably correctly, that some members of the English Parliament had colluded with the invading Scots. On 3 January, Charles directed Parliament to give up five members of the Commons – Pym, John Hampden, Denzil Holles, William Strode and Sir Arthur Haselrig – and one peer – Lord Mandeville – on the grounds of high treason. When Parliament refused, it was possibly Henrietta Maria who persuaded Charles to arrest the five members by force, which Charles intended to carry out personally. However, news of the warrant reached Parliament ahead of him, and the wanted men slipped away by boat shortly before Charles entered the House of Commons with an armed guard on 4 January 1642. Having displaced the Speaker, William Lenthall, from his chair, the king asked him where the MPs had fled. Lenthall, on his knees, famously replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." Charles abjectly declared "all my birds have flown", and was forced to retire, empty-handed.
The botched arrest attempt was politically disastrous for Charles. No English sovereign had ever entered the House of Commons, and his unprecedented invasion of the chamber to arrest its members was considered a grave breach of parliamentary privilege. In one stroke Charles destroyed his supporters' efforts to portray him as a defence against innovation and disorder.
Parliament quickly seized London, and Charles fled the capital for Hampton Court Palace on 10 January 1642, moving two days later to Windsor Castle. After sending his wife and eldest daughter to safety abroad in February, he travelled northwards, hoping to seize the military arsenal at Hull. To his dismay, he was rebuffed by the town's Parliamentary governor, Sir John Hotham, who refused him entry in April, and Charles was forced to withdraw.
English Civil War.
In mid-1642, both sides began to arm. Charles raised an army using the medieval method of commission of array, and Parliament called for volunteers for its militia. Following futile negotiations, Charles raised the royal standard in Nottingham on 22 August 1642. At the start of the First English Civil War, Charles's forces controlled roughly the Midlands, Wales, the West Country and northern England. He set up his court at Oxford. Parliament controlled London, the south-east and East Anglia, as well as the English navy.
After a few skirmishes, the opposing forces met in earnest at Edgehill, on 23 October 1642. Charles's nephew Prince Rupert of the Rhine disagreed with the battle strategy of the royalist commander Lord Lindsey, and Charles sided with Rupert. Lindsey resigned, leaving Charles to assume overall command assisted by Lord Forth. Rupert's cavalry successfully charged through the parliamentary ranks, but instead of swiftly returning to the field, rode off to plunder the parliamentary baggage train. Lindsey, acting as a colonel, was wounded and bled to death without medical attention. The battle ended inconclusively as the daylight faded.
In his own words, the experience of battle had left Charles "exceedingly and deeply grieved". He regrouped at Oxford, turning down Rupert's suggestion of an immediate attack on London. After a week, he set out for the capital on 3 November, capturing Brentford on the way while simultaneously continuing to negotiate with civic and parliamentary delegations. At Turnham Green on the outskirts of London, the royalist army met resistance from the city militia, and faced with a numerically superior force, Charles ordered a retreat. He over-wintered in Oxford, strengthening the city's defences and preparing for the next season's campaign. Peace talks between the two sides collapsed in April.
The war continued indecisively over the next couple of years, and Henrietta Maria returned to Britain for 17 months from February 1643. After Rupert captured Bristol in July 1643, Charles visited the port city and lay siege to Gloucester, further up the river Severn. His plan to undermine the city walls failed due to heavy rain, and on the approach of a parliamentary relief force, Charles lifted the siege and withdrew to Sudeley Castle. The parliamentary army turned back towards London, and Charles set off in pursuit. The two armies met at Newbury, Berkshire, on 20 September. Just as at Edgehill, the battle stalemated at nightfall, and the armies disengaged. In January 1644, Charles summoned a Parliament at Oxford, which was attended by about 40 peers and 118 members of the Commons; all told, the Oxford Parliament, which sat until March 1645, was supported by the majority of peers and about a third of the Commons. Charles became disillusioned by the assembly's ineffectiveness, calling it a "mongrel" in private letters to his wife.
In 1644, Charles remained in the southern half of England while Rupert rode north to relieve Newark and York, which were under threat from parliamentary and Scottish Covenanter armies. Charles was victorious at the battle of Cropredy Bridge in late June, but the royalists in the north were defeated at the battle of Marston Moor just a few days later. The king continued his campaign in the south, encircling and disarming the parliamentary army of the Earl of Essex. Returning northwards to his base at Oxford, he fought at Newbury for a second time before the winter closed in; the battle ended indecisively. Attempts to negotiate a settlement over the winter, while both sides re-armed and re-organised, were again unsuccessful.
At the battle of Naseby on 14 June 1645, Rupert's horsemen again mounted a successful charge, against the flank of Parliament's New Model Army, but Charles's troops elsewhere on the field were pushed back by the opposing forces. Charles, attempting to rally his men, rode forward but as he did so, Lord Carnwath seized his bridle and pulled him back, fearing for the king's safety. Carnwath's action was misinterpreted by the royalist soldiers as a signal to move back, leading to a collapse of their position. The military balance tipped decisively in favour of Parliament. There followed a series of defeats for the royalists, and then the Siege of Oxford, from which Charles escaped (disguised as a servant) in April 1646. He put himself into the hands of the Scottish Presbyterian army besieging Newark, and was taken northwards to Newcastle upon Tyne. After nine months of negotiations, the Scots finally arrived at an agreement with the English Parliament: in exchange for £100,000, and the promise of more money in the future, the Scots withdrew from Newcastle and delivered Charles to the parliamentary commissioners in January 1647.
Captivity.
Parliament held Charles under house arrest at Holdenby House in Northamptonshire until Cornet George Joyce took him by threat of force from Holdenby on 3 June in the name of the New Model Army. By this time, mutual suspicion had developed between Parliament, which favoured army disbandment and Presbyterianism, and the New Model Army, which was primarily officered by Independent non-conformists who sought a greater political role. Charles was eager to exploit the widening divisions, and apparently viewed Joyce's actions as an opportunity rather than a threat. He was taken first to Newmarket, at his own suggestion, and then transferred to Oatlands and subsequently Hampton Court, while more ultimately fruitless negotiations took place. By November, he determined that it would be in his best interests to escape – perhaps to France, Southern England or to Berwick-upon-Tweed, near the Scottish border. He fled Hampton Court on 11 November, and from the shores of Southampton Water made contact with Colonel Robert Hammond, Parliamentary Governor of the Isle of Wight, whom he apparently believed to be sympathetic. Hammond, however, confined Charles in Carisbrooke Castle and informed Parliament that Charles was in his custody.
From Carisbrooke, Charles continued to try to bargain with the various parties. In direct contrast to his previous conflict with the Scottish Kirk, on 26 December 1647 he signed a secret treaty with the Scots. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles's behalf and restore him to the throne on condition that Presbyterianism be established in England for three years.
The royalists rose in May 1648, igniting the Second Civil War, and as agreed with Charles, the Scots invaded England. Uprisings in Kent, Essex, and Cumberland, and a rebellion in South Wales, were put down by the New Model Army, and with the defeat of the Scots at the Battle of Preston in August 1648, the royalists lost any chance of winning the war.
Charles's only recourse was to return to negotiations, which were held at Newport on the Isle of Wight. On 5 December 1648, Parliament voted by 129 to 83 to continue negotiating with the king, but Oliver Cromwell and the army opposed any further talks with someone they viewed as a bloody tyrant and were already taking action to consolidate their power. Hammond was replaced as Governor of the Isle of Wight on 27 November, and placed in the custody of the army the following day. In Pride's Purge on 6 and 7 December, the members of Parliament out of sympathy with the military were arrested or excluded by Colonel Thomas Pride, while others stayed away voluntarily. The remaining members formed the Rump Parliament. It was effectively a military coup.
Trial.
Charles was moved to Hurst Castle at the end of 1648, and thereafter to Windsor Castle. In January 1649, the Rump House of Commons indicted him on a charge of treason, which was rejected by the House of Lords. The idea of trying a king was a novel one. The Chief Justices of the three common law courts of England – Henry Rolle, Oliver St John and John Wilde – all opposed the indictment as unlawful. The Rump Commons declared itself capable of legislating alone, passed a bill creating a separate court for Charles's trial, and declared the bill an act without the need for royal assent. The High Court of Justice established by the Act consisted of 135 commissioners, but many either refused to serve or chose to stay away. Only 68 (all firm Parliamentarians) attended Charles's trial on charges of high treason and "other high crimes" that began on 20 January 1649 in Westminster Hall. John Bradshaw acted as President of the Court, and the prosecution was led by the Solicitor General, John Cook.
Charles was accused of treason against England by using his power to pursue his personal interest rather than the good of the country. The charge stated that he, "for accomplishment of such his designs, and for the protecting of himself and his adherents in his and their wicked practices, to the same ends hath traitorously and maliciously levied war against the present Parliament, and the people therein represented", and that the "wicked designs, wars, and evil practices of him, the said Charles Stuart, have been, and are carried on for the advancement and upholding of a personal interest of will, power, and pretended prerogative to himself and his family, against the public interest, common right, liberty, justice, and peace of the people of this nation." Reflecting the modern concept of command responsibility, the indictment held him "guilty of all the treasons, murders, rapines, burnings, spoils, desolations, damages and mischiefs to this nation, acted and committed in the said wars, or occasioned thereby." An estimated 300,000 people, or 6% of the population, died during the war.
Over the first three days of the trial, whenever Charles was asked to plead, he refused, stating his objection with the words: "I would know by what power I am called hither, by what lawful authority...?" He claimed that no court had jurisdiction over a monarch, that his own authority to rule had been given to him by God and by the traditional laws of England, and that the power wielded by those trying him was only that of force of arms. Charles insisted that the trial was illegal, explaining that, The court, by contrast, challenged the doctrine of sovereign immunity, and proposed that "the King of England was not a person, but an office whose every occupant was entrusted with a limited power to govern 'by and according to the laws of the land and not otherwise'."
At the end of the third day, Charles was removed from the court, which then heard over 30 witnesses against the king in his absence over the next two days, and on 26 January condemned him to death. The following day, the king was brought before a public session of the commission, declared guilty and sentenced. Fifty-nine of the commissioners signed Charles's death warrant.
Execution.
Charles's beheading was scheduled for Tuesday, 30 January 1649. Two of his children remained in England under the control of the Parliamentarians: Elizabeth and Henry. They were permitted to visit him on 29 January, and he bade them a tearful farewell. The following morning, he called for two shirts to prevent the cold weather causing any noticeable shivers that the crowd could have mistaken for fear: "the season is so sharp as probably may make me shake, which some observers may imagine proceeds from fear. I would have no such imputation."
He walked under guard from St James's Palace, where he had been confined, to the Palace of Whitehall, where an execution scaffold was erected in front of the Banqueting House. Charles was separated from spectators by large ranks of soldiers, and his last speech reached only those with him on the scaffold. He blamed his fate on his failure to prevent the execution of his loyal servant Strafford: "An unjust sentence that I suffered to take effect, is punished now by an unjust sentence on me." He declared that he had desired the liberty and freedom of the people as much as any, "but I must tell you that their liberty and freedom consists in having government ... It is not their having a share in the government; that is nothing appertaining unto them. A subject and a sovereign are clean different things." He continued, "I shall go from a corruptible to an incorruptible Crown, where no disturbance can be."
At about 2:00 p.m., Charles put his head on the block after saying a prayer and signalled the executioner when he was ready by stretching out his hands; he was then beheaded with one clean stroke. According to observer Philip Henry, a moan "as I never heard before and desire I may never hear again" rose from the assembled crowd, some of whom then dipped their handkerchiefs in the king's blood as a memento.
The executioner was masked and disguised, and there is debate over his identity. The commissioners approached Richard Brandon, the common hangman of London, but he refused, at least at first, despite being offered £200. It is possible he relented and undertook the commission after being threatened with death, but there are others who have been named as potential candidates, including George Joyce, William Hulet and Hugh Peters. The clean strike, confirmed by an examination of the king's body at Windsor in 1813, suggests that the execution was carried out by an experienced headsman.
It was common practice for the severed head of a traitor to be held up and exhibited to the crowd with the words "Behold the head of a traitor!" Although Charles's head was exhibited, the words were not used, possibly because the executioner did not want his voice recognised. On the day after the execution, the king's head was sewn back onto his body, which was then embalmed and placed in a lead coffin.
The commission refused to allow Charles's burial at Westminster Abbey, so his body was conveyed to Windsor on the night of 7 February. He was buried in private in the Henry VIII vault alongside the coffins of Henry VIII and Henry's third wife, Jane Seymour, in St George's Chapel, Windsor Castle, on 9 February 1649. The king's son, Charles II, later planned for an elaborate royal mausoleum to be erected in Hyde Park, London, but it was never built.
Legacy.
Ten days after Charles's execution, on the day of his interment, a memoir purporting to be written by the king appeared for sale. This book, the "Eikon Basilike" (Greek: the "Royal Portrait"), contained an "apologia" for royal policies, and it proved an effective piece of royalist propaganda. John Milton wrote a Parliamentary rejoinder, the "Eikonoklastes" ("The Iconoclast"), but the response made little headway against the pathos of the royalist book. Anglicans and royalists fashioned an image of martyrdom, and the Church of England canonised him as a saint, in the Convocations of Canterbury and York of 1660. High Anglicans commemorated his martyrdom on the anniversary of his death and churches, such as those at Falmouth and Tunbridge Wells, were founded in his honour.
Partly inspired by his visit to the Spanish court in 1623, Charles became a passionate and knowledgeable art collector, amassing one of the finest art collections ever assembled. His intimate courtiers including the Duke of Buckingham and the Earl of Arundel shared his interest and have been dubbed the Whitehall group. In Spain, he sat for a sketch by Velázquez, and acquired works by Titian and Correggio, among others. In England, his commissions included the ceiling of the Banqueting House, Whitehall, by Rubens and paintings by other artists from the Low Countries such as van Honthorst, Mytens, and van Dyck. In 1627 and 1628, he purchased the entire collection of the Duke of Mantua, which included work by Titian, Correggio, Raphael, Caravaggio, del Sarto and Mantegna. Charles's collection grew further to encompass Bernini, Bruegel, da Vinci, Holbein, Hollar, Tintoretto and Veronese, and self-portraits by both Dürer and Rembrandt. By Charles's death, there were an estimated 1760 paintings, most of which were sold and dispersed by Parliament.
With the monarchy overthrown, England became a republic or "Commonwealth". The House of Lords was abolished by the Rump Commons, and executive power was assumed by a Council of State. All significant military opposition in Britain and Ireland was extinguished by the forces of Oliver Cromwell in the Third English Civil War and the Cromwellian conquest of Ireland. Cromwell forcibly disbanded the Rump Parliament in 1653, thereby establishing The Protectorate with himself as Lord Protector. Upon his death in 1658, he was briefly succeeded by his ineffective son, Richard. Parliament was reinstated, and the monarchy was restored to Charles I's eldest son, Charles II, in 1660.
Assessments.
In the words of John Philipps Kenyon, "Charles Stuart is a man of contradictions and controversy". Revered by high Tories who considered him a saintly martyr, he was condemned by Whig historians, such as Samuel Rawson Gardiner, who thought him duplicitous and delusional. In recent decades, most historians have criticised him, the main exception being Kevin Sharpe who offered a more sympathetic view of Charles that has not been widely adopted. While Sharpe argued that the king was a dynamic man of conscience, Professor Barry Coward thought Charles "was the most incompetent monarch of England since Henry VI", a view shared by Ronald Hutton, who called him "the worst king we have had since the Middle Ages".
Archbishop William Laud, who was beheaded by Parliament during the war, described Charles as "A mild and gracious prince who knew not how to be, or how to be made, great." Charles was more sober and refined than his father, but he was intransigent and deliberately pursued unpopular policies that ultimately brought ruin on himself. Both Charles and James were advocates of the divine right of kings, but while James's ambitions concerning absolute prerogative were tempered by compromise and consensus with his subjects, Charles believed that he had no need to compromise or even to explain his actions. He thought that he was answerable only to God. "Princes are not bound to give account of their actions," he wrote, "but to God alone".
Titles, styles, honours and arms.
Titles and styles.
The official style of Charles I as king was "Charles, by the Grace of God, King of England, Scotland, France and Ireland, Defender of the Faith, etc." The style "of France" was only nominal, and was used by every English monarch from Edward III to George III, regardless of the amount of French territory actually controlled. The authors of his death warrant referred to him as "Charles Stuart, King of England".
Arms.
As Duke of York, Charles bore the royal arms of the kingdom differenced by a label Argent of three points, each bearing three torteaux Gules. The Prince of Wales bore the royal arms differenced by a plain label Argent of three points. As king, Charles bore the royal arms undifferenced: Quarterly, I and IV Grandquarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland). In Scotland, the Scottish arms were placed in the first and fourth quarters with the English and French arms in the second quarter.
Issue.
Charles had nine children, two of whom eventually succeeded as king, and two of whom died at or shortly after birth.

</doc>
<doc id="7431" url="https://en.wikipedia.org/wiki?curid=7431" title="Counter-Strike">
Counter-Strike

Counter-Strike (also known as Half-Life: Counter-Strike) is a first-person shooter video game developed by Valve Corporation. It was initially developed and released as a "Half-Life" modification by Minh "Gooseman" Le and Jess "Cliffe" Cliffe in 1999, before Le and Cliffe were hired and the game's intellectual property acquired. "Counter-Strike" was first released by Valve on the Microsoft Windows platform in 2000. The game later spawned a franchise, and is the first installment in the "Counter-Strike" series. Several remakes and Ports of "Counter-Strike" have been released on the Xbox console, as well as OS X and Linux. It is sometimes referred to as Counter-Strike 1.6 to distinguish it from other titles of the series, 1.6 being the final major software update the game received.
Set in various locations around the globe, players assume the roles of members of combating teams of the governmental counter-terrorist forces and various terrorist militants opposing them. During each round of gameplay, the two teams are tasked with defeating the other by the means of either achieving the map's objectives, or else eliminating all of the enemy combatants. Each player may customize their arsenal of weapons and accessories at the beginning of every match, with currency being earned after the end of each round.
Gameplay.
"Counter-Strike" is a first-person shooter game in which players join either the terrorist team, the counter-terrorist team, or become spectators. Each team attempts to complete their mission objective and/or eliminate the opposing team. Each round starts with the two teams spawning simultaneously.
The objectives vary depending on the type of map, and these are the most usual ones:
A player can choose to play as one of eight different default character models (four for each side, although "" added two extra models, bringing the total to ten). Players are generally given a few seconds before the round begins (known as "freeze time") to prepare and buy equipment, during which they cannot attack or move. They can return to the buy area within a set amount of time to buy more equipment (some custom maps included neutral "buy zones" that could be used by both teams). Once the round has ended, surviving players retain their equipment for use in the next round; players who were killed begin the next round with the basic default starting equipment.
Standard monetary bonuses are awarded for winning a round, losing a round, killing an enemy, being the first to instruct a hostage to follow, rescuing a hostage, planting the bomb (Terrorist) or defusing the bomb (Counter-Terrorist).
The scoreboard displays team scores in addition to statistics for each player: name, kills, deaths, and ping (in milliseconds). The scoreboard also indicates whether a player is dead, carrying the bomb (on bomb maps), or is the VIP (on assassination maps), although information on players on the opposing team is hidden from a player until his/her death, as this information can be important.
Killed players become "spectators" for the duration of the round; they cannot change their names before their next spawn, text chat cannot be sent to or received from live players, and voice chat can only be received from live players and not sent to them. Spectators are generally able to watch the rest of the round from multiple selectable views, although some servers disable some of these views to prevent dead players from relaying information about living players to their teammates through alternative media (most notably voice in the case of Internet cafes and Voice over IP programs such as TeamSpeak or Ventrilo). This form of cheating is known as "ghosting."
Development.
"Counter-Strike" itself is a mod, and it has developed its own community of script writers and mod creators. Some mods add bots, while others remove features of the game, and others create different modes of play. Some mods, often called "admin plugins", give server administrators more flexible and efficient control over their servers. There are some mods which affect gameplay heavily, such as Gun Game, where players start with a basic pistol and must score kills to receive better weapons, and Zombie Mod, where one team consists of zombies and must "spread the infection" by killing the other team (using only the knife). There are also the Superhero and mods which mix the first-person gameplay of "Counter-Strike" with an experience system, allowing a player to become more powerful as they continue to play. The game is also highly customizable on the player's end, allowing the user to install or even create their own custom skins, HUDs, spray graphics, sprites, and sound effects, given the proper tools.
Valve Anti-Cheat.
"Counter-Strike" has been a target for cheating in online games since its release. In-game, cheating is often referred to as "hacking" in reference to programs or "hacks" executed by the client. Valve has implemented an anti-cheat system called Valve Anti-Cheat (VAC). Players cheating on a VAC-enabled server risk having their account permanently banned from all VAC-secured servers.
With the first version of VAC, a ban took hold almost instantly after being detected and the cheater had to wait two years to have the account unbanned. Since VAC's second version, cheaters are not banned automatically. With the second version, Valve instituted a policy of 'delayed bans,' the theory being that if a new hack is developed which circumvents the VAC system, it will spread amongst the 'cheating' community. By delaying the initial ban, Valve hopes to identify and ban as many cheaters as possible. Like any software detection system, some cheats are not detected by VAC. To remedy this, some servers implement a voting system, in which case players can call for a vote to kick or ban the accused cheater. VAC's success at identifying cheats and banning those who use them has also provided a boost in the purchasing of private cheats. These cheats are updated frequently to minimize the risk of detection, and are generally only available to a trusted list of recipients who collectively promise not to reveal the underlying design. Even with private cheats however, some servers have alternative anticheats to coincide with VAC itself. This can help with detecting some cheaters, but most paid for cheats are designed to bypass these alternative server-based anticheats.
Release.
When "Counter-Strike" was published by Sierra Entertainment/Vivendi Universal Games, it was bundled with "Team Fortress Classic", "" multiplayer, and the "Wanted", "Half-Life: Absolute Redemption" and "Firearms" mods.
On March 24, 1999, Planet Half-Life opened its "Counter-Strike" section. Within two weeks, the site had received 10,000 hits. On June 19, 1999, the first public beta of "Counter-Strike" was released, followed by numerous further "beta" releases. On April 12, 2000, Valve announced that the "Counter-Strike" developers and Valve had teamed up. In January 2013, Valve began testing a version of "Counter-Strike" for OS X and Linux, eventually releasing the update to all users in April 2013.
Reception.
Upon its retail release, "Counter-Strike" received highly favorable reviews. The New York Times reported that E-Sports Entertainment ESEA League started the first professional fantasy e-sports league in 2004 with the game "Counter-Strike". Some credit the move into professional competitive team play with prizes as a major factor in "Counter-Strike" longevity and success.
Brazilian sale ban.
On January 17, 2008, a Brazilian federal court order prohibiting all sales of "Counter-Strike" and "EverQuest" began to be enforced. The federal Brazilian judge Carlos Alberto Simões de Tomaz ordered the ban in October 2007 because, as argued by the judge, the games "bring imminent stimulus to the subversion of the social order, attempting against the democratic state and the law and against public security." As of June 18, 2009, a regional federal court order lifting the prohibition on the sale of "Counter-Strike" was published. The game is now being sold again in Brazil.
Legacy.
Following the success of the first "Counter-Strike", Valve went on to make multiple sequels to the game. ', a game using "Counter-Strike's" GoldSrc engine, was released in 2004. ', a remake of the original "Counter-Strike" game, was the first in the series to use Valve's Source engine and was also released in 2004, only eight months after the release of "Counter-Strike: Condition Zero". The next game in the "Counter-Strike" series to be developed primarily by Valve Corporation was "", released for Windows, OS X, Linux, PlayStation 3, and Xbox 360 in 2012.
The game also spawned multiple spin-offs in the form of arcade games developed by Nexon Corporation and targeted primarily at Asian gaming markets. Four "Counter-Strike" games have been developed and released by Nexon Corporation thus far, "Counter-Strike Neo", "Counter-Strike Online", "", and "Counter-Strike Online 2".

</doc>
<doc id="7434" url="https://en.wikipedia.org/wiki?curid=7434" title="Camille Pissarro">
Camille Pissarro

Camille Pissarro (; 10 July 1830 – 13 November 1903) was a Danish-French Impressionist and Neo-Impressionist painter born on the island of St Thomas (now in the US Virgin Islands, but then in the Danish West Indies). His importance resides in his contributions to both Impressionism and Post-Impressionism. Pissarro studied from great forerunners, including Gustave Courbet and Jean-Baptiste-Camille Corot. He later studied and worked alongside Georges Seurat and Paul Signac when he took on the Neo-Impressionist style at the age of 54.
In 1873 he helped establish a collective society of fifteen aspiring artists, becoming the "pivotal" figure in holding the group together and encouraging the other members. Art historian John Rewald called Pissarro the "dean of the Impressionist painters", not only because he was the oldest of the group, but also "by virtue of his wisdom and his balanced, kind, and warmhearted personality". Cézanne said "he was a father for me. A man to consult and a little like the good Lord," and he was also one of Gauguin's masters. Renoir referred to his work as "revolutionary", through his artistic portrayals of the "common man", as Pissarro insisted on painting individuals in natural settings without "artifice or grandeur".
Pissarro is the only artist to have shown his work at all eight Paris Impressionist exhibitions, from 1874 to 1886. He "acted as a father figure not only to the Impressionists" but to all four of the major Post-Impressionists, including Georges Seurat, Paul Cézanne, Vincent van Gogh and Paul Gauguin.
Early years.
Jacob Abraham Camille Pissarro was born on 10 July 1830 on the island of St. Thomas to Frederick and Rachel Manzano de Pissarro. His father was of Portuguese Jewish descent and held French nationality. His mother was from a French-Jewish family from the island of St. Thomas. His father was a merchant who came to the island from France to deal with the hardware store of a deceased uncle and married his widow. The marriage caused a stir within St. Thomas' small Jewish community because she was previously married to Frederick's uncle and according to Jewish law a man is forbidden from marrying his aunt. In subsequent years his four children were forced to attend the all-black primary school. Upon his death, his will specified that his estate be split equally between the synagogue and St. Thomas' Protestant church.
When Camille was twelve his father sent him to boarding school in France. He studied at the Savary Academy in Passy near Paris. While a young student, he developed an early appreciation of the French art masters. Monsieur Savary himself gave him a strong grounding in drawing and painting and suggested he draw from nature when he returned to St. Thomas, which he did when he was seventeen. However, his father preferred he work in his business, giving him a job working as a cargo clerk. He took every opportunity during those next five years at the job to practice drawing during breaks and after work.
When he turned twenty-one, Danish artist Fritz Melbye, then living on St. Thomas, inspired Pissarro to take on painting as a full-time profession, becoming his teacher and friend. Pissarro then chose to leave his family and job and live in Venezuela, where he and Melbye spent the next two years working as artists in Caracas and La Guaira. He drew everything he could, including landscapes, village scenes, and numerous sketches, enough to fill up multiple sketchbooks. In 1855 he moved back to Paris where he began working as assistant to Anton Melbye, Fritz Melbye's brother.
Life in France.
In Paris he worked as assistant to Danish painter Anton Melbye. He also studied paintings by other artists whose style impressed him: Courbet, Charles-François Daubigny, Jean-François Millet, and Corot. He also enrolled in various classes taught by masters, at schools such as École des Beaux-Arts and Académie Suisse. But Pissarro eventually found their teaching methods "stifling," states art historian John Rewald. This prompted him to search for alternative instruction, which he requested and received from Corot.
Paris Salon and Corot's influence.
His initial paintings were in accord with the standards at the time to be displayed at the Paris Salon, the official body whose academic traditions dictated the kind of art that was acceptable. The Salon's annual exhibition was essentially the only marketplace for young artists to gain exposure. As a result, Pissarro worked in the traditional and prescribed manner to satisfy the tastes of its official committee.
In 1859 his first painting was accepted and exhibited. His other paintings during that period were influenced by Camille Corot, who tutored him. He and Corot both shared a love of rural scenes painted from nature. It was by Corot that Pissarro was inspired to paint outdoors, also called "plein air" painting. Pissarro found Corot, along with the work of Gustave Courbet, to be "statements of pictorial truth," writes Rewald. He discussed their work often. Jean-François Millet was another whose work he admired, especially his "sentimental renditions of rural life".
Use of natural outdoor settings.
During this period Pissarro began to understand and appreciate the importance of expressing on canvas the beauties of nature without adulteration. After a year in Paris, he therefore began to leave the city and paint scenes in the countryside to capture the daily reality of village life. He found the French countryside to be "picturesque," and worthy of being painted. It was still mostly agricultural and sometimes called the "golden age of the peasantry". Pissarro later explained the technique of painting outdoors to a student:
Corot, however, would complete his own scenic paintings back in his studio where they would often be revised to his preconceptions. Pissarro, on the other hand, preferred to finish his paintings outdoors, often at one sitting, which gave his work a more realistic feel. As a result, his art was sometimes criticised as being "vulgar," because he painted what he saw: "rutted and edged hodgepodge of bushes, mounds of earth, and trees in various stages of development." According to one source, details such as those were equivalent to today's art showing garbage cans or beer bottles on the side of a street scene. This difference in style created disagreements between Pissarro and Corot.
With Monet, Cézanne, and Guillaumin.
In 1859, while attending the free school, the Académie Suisse, Pissarro became friends with a number of younger artists who likewise chose to paint in the more realistic style. Among them were Claude Monet, Armand Guillaumin and Paul Cézanne. What they shared in common was their dissatisfaction with the dictates of the Salon. Cézanne's work had been mocked at the time by the others in the school, and, writes Rewald, in his later years Cézanne "never forgot the sympathy and understanding with which Pissarro encouraged him." As a part of the group, Pissarro was comforted from knowing he was not alone, and that others similarly struggled with their art.
Pissarro agreed with the group about the importance of portraying individuals in natural settings, and expressed his dislike of any artifice or grandeur in his works, despite what the Salon demanded for its exhibits. In 1863 almost all of the group's paintings were rejected by the Salon, and French Emperor Napoleon III instead decided to place their paintings in a separate exhibit hall, the Salon des Refusés. However, only works of Pissarro and Cézanne were included, and the separate exhibit brought a hostile response from both the officials of the Salon and the public.
In subsequent Salon exhibits of 1865 and 1866, Pissarro acknowledged his influences from Melbye and Corot, whom he listed as his masters in the catalogue. But in the exhibition of 1868 he no longer credited other artists as an influence, in effect declaring his independence as a painter. This was noted at the time by art critic and author Émile Zola, who offered his opinion:
Another writer tries to describe elements of Pissarro's style:
And though, on orders from the hanging Committee and the Marquis de Chennevières, Pissarro's paintings of Pontoise for example had been skyed, hung near the ceiling, this did not prevent Jules-Antoine Castagnary from noting that the qualities of his paintings had been observed by art lovers. At the age of thirty-eight, Pissarro had begun to win himself a reputation as a landscapist to rival Corot and Daubigny.
In the late 1860s or early 1870s, Pissarro became fascinated with Japanese prints, which influenced his desire to experiment in new compositions. He described the art to his son Lucien:
Marriage and children.
In 1871 he married his mother's maid, Julie Vellay, a vineyard grower's daughter, with whom he would later have seven children. They lived outside of Paris in Pontoise and later in Louveciennes, both of which places inspired many of his paintings including scenes of village life, along with rivers, woods, and people at work. He also kept in touch with the other artists of his earlier group, especially Monet, Renoir, Cézanne, and Frédéric Bazille.
The London years.
After the outbreak of the Franco-Prussian War of 1870–71, having only Danish nationality and being unable to join the army, he moved his family to Norwood, then a village on the edge of London. However, his style of painting, which was a forerunner of what was later called "Impressionism", did not do well. He wrote to his friend, Theodore Duret, that "my painting doesn't catch on, not at all ..."
Pissarro met the Paris art dealer Paul Durand-Ruel, in London, who became the dealer who helped sell his art for most of his life. Durand-Ruel put him in touch with Monet who was likewise in London during this period. They both viewed the work of British landscape artists John Constable and J. M. W. Turner, which confirmed their belief that their style of open air painting gave the truest depiction of light and atmosphere, an effect that they felt could not be achieved in the studio alone. Pissarro's paintings also began to take on a more spontaneous look, with loosely blended brushstrokes and areas of impasto, giving more depth to the work.
Paintings.
Through the paintings Pissarro completed at this time, he records Sydenham and the Norwoods at a time when they were just recently connected by railways, but prior to the expansion of suburbia. One of the largest of these paintings is a view of "St. Bartholomew's Church" at Lawrie Park Avenue, commonly known as "The Avenue, Sydenham", in the collection of the London National Gallery. Twelve oil paintings date from his stay in Upper Norwood and are listed and illustrated in the catalogue raisonné prepared jointly by his fifth child Ludovic-Rodolphe Pissarro and Lionello Venturi and published in 1939. These paintings include "Norwood Under the Snow", and "Lordship Lane Station", views of The Crystal Palace relocated from Hyde Park, "Dulwich College", "Sydenham Hill", "All Saints Church Upper Norwood", and a lost painting of St. Stephen's Church.
Returning to France, in 1890 Pissarro again visited England and painted some ten scenes of central London. He came back again in 1892, painting in Kew Gardens and Kew Green, and also in 1897, when he produced several oils described as being of Bedford Park, Chiswick, but in fact all being of the nearby Stamford Brook area except for one of Bath Road, which runs from Stamford Brook along the south edge of Bedford Park.
French Impressionism.
When Pissarro returned to his home in France after the war, he discovered that of the 1,500 paintings he had done over 20 years, which he was forced to leave behind when he moved to London, only 40 remained. The rest had been damaged or destroyed by the soldiers, who often used them as floor mats outside in the mud to keep their boots clean. It is assumed that many of those lost were done in the Impressionist style he was then developing, thereby "documenting the birth of Impressionism." Armand Silvestre, a critic, went so far as to call Pissarro "basically the inventor of this painting"; however, Pissarro's role in the Impressionist movement was "less that of the great man of ideas than that of the good counselor and appeaser ..." "Monet ... could be seen as the guiding force."
He soon reestablished his friendships with the other Impressionist artists of his earlier group, including Cézanne, Monet, Manet, Renoir, and Degas. Pissarro now expressed his opinion to the group that he wanted an alternative to the Salon so their group could display their own unique styles.
To assist in that endeavour, in 1873 he helped establish a separate collective, called the "Société Anonyme des Artistes, Peintres, Sculpteurs et Graveurs," which included fifteen artists. Pissarro created the group's first charter and became the "pivotal" figure in establishing and holding the group together. One writer noted that with his prematurely grey beard, the forty-three-year-old Pissarro was regarded as a "wise elder and father figure" by the group. Yet he was able to work alongside the other artists on equal terms due to his youthful temperament and creativity. Another writer said of him that "he has unchanging spiritual youth and the look of an ancestor who remained a young man".
Impressionist exhibitions that shocked the critics.
The following year, in 1874, the group held their first 'Impressionist' Exhibition, which shocked and "horrified" the critics, who primarily appreciated only scenes portraying religious, historical, or mythological settings. They found fault with the Impressionist paintings on many grounds:
A "revolutionary" style.
Pissarro showed five of his paintings, all landscapes, at the exhibit, and again Émile Zola praised his art and that of the others. In the Impressionist exhibit of 1876; however, art critic Albert Wolff complained in his review, "Try to make M. Pissarro understand that trees are not violet, that sky is not the color of fresh butter ..." Journalist and art critic Octave Mirbeau on the other hand, writes, "Camille Pissarro has been a revolutionary through the revitalized working methods with which he has endowed painting".
According to Rewald, Pissarro had taken on an attitude more simple and natural than the other artists. He writes:
In later years, Cézanne also recalled this period and referred to Pissarro as "the first Impressionist". In 1906, a few years after Pissarro's death, Cézanne, then 67 and a role model for the new generation of artists, paid Pissarro a debt of gratitude by having himself listed in an exhibition catalogue as "Paul Cézanne, pupil of Pissarro".
Pissarro, Degas, and American impressionist Mary Cassatt planned a journal of their original prints in the late 1870s, a project that nevertheless came to nothing when Degas withdrew. Art historian and the artist's great-grandson Joachim Pissarro notes that they "professed a passionate disdain for the Salons and refused to exhibit at them." Together they shared an "almost militant resolution" against the Salon, and through their later correspondences it is clear that their mutual admiration "was based on a kinship of ethical as well as aesthetic concerns".
Cassatt had befriended Degas and Pissarro years earlier when she joined Pissarro's newly formed French Impressionist group and gave up opportunities to exhibit in the United States. She and Pissarro were often treated as "two outsiders" by the Salon since neither were French or had become French citizens. However, she was "fired up with the cause" of promoting Impressionism and looked forward to exhibiting "out of solidarity with her new friends". Towards the end of the 1890s she began to distance herself from the Impressionists, avoiding Degas at times she did not have the strength to defend herself against his "wicked tongue". Instead, she came to prefer the company of "the gentle Camille Pissarro", with whom she could speak frankly about the changing attitudes toward art. She once described him as a teacher "that could have taught the stones to draw correctly."
Neo-Impressionist period.
By the 1880s, Pissarro began to explore new themes and methods of painting to break out of what he felt was an artistic "mire". As a result, Pissarro went back to his earlier themes by painting the life of country people, which he had done in Venezuela in his youth. Degas described Pissarro's subjects as "peasants working to make a living".
However, this period also marked the end of the Impressionist period due to Pissarro's leaving the movement. As Joachim Pissarro points out, "Once such a die-hard Impressionist as Pissarro had turned his back on Impressionism, it was apparent that Impressionism had no chance of surviving ..."
It was Pissarro's intention during this period to help "educate the public" by painting people at work or at home in realistic settings, without idealising their lives. Pierre-Auguste Renoir, in 1882, referred to Pissarro's work during this period as "revolutionary," in his attempt to portray the "common man." Pissarro himself did not use his art to overtly preach any kind of political message, however, although his preference for painting humble subjects was intended to be seen and purchased by his upper class clientele. He also began painting with a more unified brushwork along with pure strokes of color.
Studying with Seurat and Signac.
In 1885 he met Georges Seurat and Paul Signac, both of whom relied on a more "scientific" theory of painting by using very small patches of pure colours to create the illusion of blended colours and shading when viewed from a distance. Pissarro then spent the years from 1885 to 1888 practising this more time-consuming and laborious technique, referred to as pointillism. The paintings that resulted were distinctly different from his Impressionist works, and were on display in the 1886 Impressionist Exhibition, but under a separate section, along with works by Seurat, Signac, and his son Lucien.
All four works were considered an "exception" to the eighth exhibition. Joachim Pissarro notes that virtually every reviewer who commented on Pissarro's work noted "his extraordinary capacity to change his art, revise his position and take on new challenges." One critic writes:
Pissarro explained the new art form as a "phase in the logical march of Impressionism", but he was alone among the other Impressionists with this attitude, however. Joachim Pissarro states that Pissarro thereby became the "only artist who went from Impressionism to Neo-Impressionism".
In 1884, art dealer Theo van Gogh asked Pissarro if he would take in his older brother, Vincent, as a boarder in his home. Lucien Pissarro wrote that his father was impressed by Van Gogh's work and had "foreseen the power of this artist", who was 23 years younger. Although Van Gogh never boarded with him, Pissarro did explain to him the various ways of finding and expressing light and color, ideas which he later used in his paintings, notes Lucien.
Abandoning Neo-Impressionism.
Pissarro eventually turned away from Neo-Impressionism, claiming its system was too artificial. He explains in a letter to a friend:
However, after reverting to his earlier style, his work became, according to Rewald, "more subtle, his color scheme more refined, his drawing firmer ... So it was that Pissarro approached old age with an increased mastery."
But the change also added to Pissarro's continual financial hardship which he felt until his 60s. His "headstrong courage and a tenacity to undertake and sustain the career of an artist", writes Joachim Pissarro, was due to his "lack of fear of the immediate repercussions" of his stylistic decisions. In addition, his work was strong enough to "bolster his morale and keep him going", he writes. His Impressionist contemporaries, however, continued to view his independence as a "mark of integrity", and they turned to him for advice, referring to him as "Père Pissarro" (father Pissarro).
Later years.
In his older age Pissarro suffered from a recurring eye infection that prevented him from working outdoors except in warm weather. As a result of this disability, he began painting outdoor scenes while sitting by the window of hotel rooms. He often chose hotel rooms on upper levels to get a broader view. He moved around northern France and painted from hotels in Rouen, Paris, Le Havre and Dieppe. On his visits to London, he would do the same.
Pissarro died in Paris on 13 November 1903 and was buried in Père Lachaise Cemetery.
Legacy and influence.
During the period Pissarro exhibited his works, art critic Armand Silvestre had called Pissarro the "most real and most naive member" of the Impressionist group. His work has also been described by art historian Diane Kelder as expressing "the same quiet dignity, sincerity, and durability that distinguished his person." She adds that "no member of the group did more to mediate the internecine disputes that threatened at times to break it apart, and no one was a more diligent proselytizer of the new painting."
According to Pissarro's son, Lucien, his father painted regularly with Cézanne beginning in 1872. He recalls that Cézanne walked a few miles to join Pissarro at various settings in Pontoise. While they shared ideas during their work, the younger Cézanne wanted to study the countryside through Pissarro's eyes, as he admired Pissarro's landscapes from the 1860s. Cézanne, although only nine years younger than Pissarro, said that "he was a father for me. A man to consult and a little like the good Lord."
Lucien Pissarro was taught painting by his father, and described him as a "splendid teacher, never imposing his personality on his pupil." Gauguin, who also studied under him, referred to Pissarro "as a force with which future artists would have to reckon". Art historian Diane Kelder notes that it was Pissarro who introduced Gauguin, who was then a young stockbroker studying to become an artist, to Degas and Cézanne. Gauguin, near the end of his career, wrote a letter to a friend in 1902, shortly before Pissarro's death:
The American impressionist Mary Cassatt, who at one point lived in Paris to study art, and joined his Impressionist group, noted that he was "such a teacher that he could have taught the stones to draw correctly."
Caribbean author and scholar Derek Walcott based his book-length poem, "Tiepolo's Hound" (2000), on Pissarro's life.
Lost and found paintings.
During the early 1930s throughout Europe, Jewish owners of numerous fine art masterpieces found themselves forced to give up or sell off their collections for minimal prices due to anti-Jewish laws created by the new Nazi regime. Many Jews were forced to flee Germany. When those forced into exile owned valuables, including artwork, they were often seized by officials for personal gain. In the decades after World War II, many art masterpieces were found on display in various galleries and museums in Europe and the United States. Some, as a result of legal action, were later returned to the families of the original owners. Many of the recovered paintings were then donated to the same or other museums as a gift.
One such lost piece, Pissarro's 1897 oil painting, "Rue St. Honoré, Apres Midi, Effet de Pluie", was discovered hanging at Madrid's government-owned museum, the Museo Thyssen-Bornemisza. In January 2011 the Spanish government denied a request by the US ambassador to return the painting. At the subsequent trial in Los Angeles, the court ruled that the Thyssen-Bornemisza Collection Foundation was the rightful owner. Pissarro's "Le Quai Malaquais, Printemps" is said to have been similarly stolen, while in 1999, Pissarro's 1897 "Le Boulevard de Montmartre, Matinée de Printemps" appeared in the Israel Museum in Jerusalem, its donor having been unaware of its pre-war provenance. In January 2012, "Le Marché aux Poissons" (The Fish Market), a color monotype, was returned after 30 years.
During his lifetime, Camille Pissarro sold few of his paintings. By the 21st century, however, his paintings were selling for millions. An auction record for the artist was set on 6 November 2007 at Christie's in New York, where a group of four paintings, "Les Quatre Saisons" (the Four Seasons), sold for $14,601,000 (estimate $12,000,000 – $18,000,000). In November 2009 "Le Pont Boieldieu et la Gare d'Orléans, Rouen, Soleil" sold for $7,026,500 at Sotheby's in New York. In February 2014 the 1897 "Le Boulevard de Montmartre, Matinée de Printemps", originally owned by the German industrialist and Holocaust victim Max Silberberg (de), sold at Sotheby's in London for £19.9M, nearly five times the previous record.
A family of painters.
Camille's son Lucien was an Impressionist and Neo-impressionist painter as were his second and third sons Georges Henri Manzana Pissarro and Félix Pissarro. Lucien's daughter Orovida Pissarro was also a painter. Camille's great-grandson, Joachim Pissarro, became Head Curator of Drawing and Painting at the Museum of Modern Art in New York City and a professor in Hunter College's Art Department. From the only daughter of Camille, Jeanne Pissarro, other painters include Henri Bonin-Pissarro (1918–2003) and Claude Bonin-Pissarro (born 1921), who is the father of the Abstract artist Frédéric Bonin-Pissarro (born 1964).
References.
"Critical Catalogue of Paintings".
In June 2006 publishers Skira/Wildenstein released "Pissarro: Critical Catalogue of Paintings", compiled by Joachim Pissarro (descendant of the painter) and Claire Durand-Ruel Snollaerts (descendant of the French art dealer Paul Durand-Ruel). The 1,500-page, three-volume work is the most comprehensive collection of Pissarro paintings to date, and contains accompanying images of drawings and studies, as well as photographs of Pissarro and his family that had not previously been published. ISBN 88-7624-525-1

</doc>
<doc id="7435" url="https://en.wikipedia.org/wiki?curid=7435" title="Cardiology diagnostic tests and procedures">
Cardiology diagnostic tests and procedures

The diagnostic tests in cardiology are methods of identifying heart conditions associated with healthy vs. unhealthy, pathologic, heart function.
Bedside.
History.
Obtaining a medical history is always the first "test", part of understanding the likelihood of significant disease, as detectable within the current limitations of clinical medicine. Yet heart problems often produce no symptoms until very advanced, and many symptoms, such as palpitations and sensations of extra or missing heart beats correlate poorly with relative heart health "vs" disease. Hence, a history alone is rarely sufficient to diagnose a heart condition.
Auscultation.
"Auscultation" employs a stethoscope to more easily hear various normal and abnormal sounds, such as normal heart beat sounds and the usual heart beat sound changes associated with breathing versus heart murmurs.
Laboratory.
Blood tests.
A variety of "blood tests" are available for analyzing cholesterol transport behavior, HDL, LDL, triglycerides, lipoprotein little a, homocysteine, C-reactive protein, blood sugar control: fasting, after eating or averages using glycosylated albumen or hemoglobin, myoglobin, creatine kinase, troponin, brain-type natriuretic peptide, etc. to assess the evolution of coronary artery disease and evidence of existing damage. A great many more physiologic markers related to atherosclerosis and heart function are used and being developed and evaluated in research.
(*) due to the high cost, LDL is usually calculated instead of being measured directly<br>
source: Beyond Cholesterol, Julius Torelli MD, 2005 ISBN 0-312-34863-0
Electrophysiology.
Electrocardiogram.
"Electrocardiography" (ECG/EKG in German vernacular. Elektrokardiogram) monitors electrical activity of the heart, primarily as recorded from the skin surface. A 12 lead recording, recording the electrical activity in three planes, anterior, posterior, and lateral is the most commonly used form. The ECG allows observation of the heart electrical activity by visualizing waveform beat origin (typically from the sinoatrial or SA node) following down the bundle of HIS and ultimately stimulating the ventricles to contract forcing blood through the body. Much can be learned by observing the QRS morphology (named for the respective portions of the polarization/repolarization waveform of the wave, P,Q,R,S,T wave). Rhythm abnormalities can also be visualized as in slow heart rate bradycardia, or fast heart rate tachycardia.
Holter monitor.
A "Holter monitor" records a continuous EKG rhythm pattern (rarely a full EKG) for 24 hours or more. These monitors are used for suspected frequent rhythm abnormalities, especially ones the wearer may not recognize by symptoms. They are more expensive than event monitors.
Event monitor.
An "Event monitor" records short term EKG rhythm patterns, generally storing the last 2 to 5 minutes, adding in new and discarding old data, for 1 to 2 weeks or more. There are several different types with different capabilities. When the wearer presses a button on the monitor, it quits discarding old and continues recording for a short additional period. The wearer then plays the recording, via a standard phone connection, to a center with compatible receiving and rhythm printing equipment, after which the monitor is ready to record again. These monitors are used for suspected infrequent rhythm abnormalities, especially ones the wearer does recognize by symptoms. They are less expensive than Holter monitors.
Cardiac stress testing.
"Cardiac stress testing" is used to determine to assess cardiac function and to disclose evidence of exertion-related cardiac hypoxia. Radionuclide testing using thallium or technetium can be used tos of perfusion abnormalities. With a maximal stress test the level of exercise is increased until the patient heart rate will not increase any higher, despite increased exercise. A fairly accurate estimate of the target heart rate, based on extensive clinical research, can be estimated by the formula 220 beats per minute minus patient's age. This linear relation is accurate up to about age 30, after which it mildly underestimates typical maximum attainable heart rates achievable by healthy individuals. Other formulas exist, such as that by Miller (217 - (0.85 × Age)) and others [http://www.brianmac.co.uk/maxhr.htm]. Achieving a high enough heart rate at the end of exercise is critical to improving the sensitivity of the test to detect high grade heart artery stenosis.
Electrophysiology study.
The electrophysiology study or EP study is the end all of electrophysiological tests of the heart. It involves a catheter with electrodes probing the endocardium, the inside of the heart, and testing the conduction pathways and electrical activity of individual areas of the heart.
Medical imaging.
Cardiac imaging techniques include Coronary catheterization, echocardiogram, and Intravascular ultrasound.

</doc>
<doc id="7437" url="https://en.wikipedia.org/wiki?curid=7437" title="Carlo Collodi">
Carlo Collodi

Carlo Lorenzini, better known by the pen name Carlo Collodi (; 24 November 1826 – 26 October 1890), was a children's writer born in the Grand Duchy of Tuscany and writer of the world-renowned fairy tale novel "The Adventures of Pinocchio".
Early life.
Collodi was born in Florence on 24 November 1826. He spent most of his childhood in the town of Collodi where his mother was born.
His mother was a farmer's daughter and his father was a cook.
He had 10 other siblings but seven died at a young age.
Career.
During the Italian wars of Independence in 1848 and 1860 Collodi served as a volunteer with the Tuscan army. His active interest in political matters may be seen in his earliest literary works as well as in the founding of the satirical newspaper "Il Lampione" in 1853. This newspaper was censored by order of the Grand Duke of Tuscany. In 1854 he published his second newspaper, "Lo scaramuccia" ("The Controversy").
Lorenzini's first publications were in his periodicals. A debut came in 1856 with the play "Gli amici di casa" and parodic guidebook "Un romanzo in vapore", both in 1856; he had also begun intense activity on other political newspapers such as "Il Fanfulla"; at the same time he was employed by the Censorship Commission for the Theatre. During this period he composed various satirical sketches and stories (sometimes simply by collating earlier articles), including "Macchiette" (1880), "Occhi e nasi" (1881), "Storie allegre" (1887).
In 1875 he entered the domain of children's literature with "Racconti delle fate", a translation of French fairy tales by Perrault. In 1876 Lorenzini wrote "Giannettino" (inspired by Alessandro Luigi Parravicini's "Giannetto"), the "Minuzzolo", and "Il viaggio per l'Italia di Giannettino", a pedagogic series which explored the re-unification of Italy through the ironic thoughts and actions of the character Giannettino.
Lorenzini became fascinated by the idea of using an amiable, rascally character as a means of expressing his own convictions through allegory. In 1880 he began writing "Storia di un burattino" ("The story of a marionette"), also called "Le avventure di Pinocchio", which was published weekly in "Il Giornale per i Bambini", the first Italian newspaper for children.
Death.
Lorenzini died in Florence in 1890, unaware of the fame and popularity that awaited his work. He is buried at San Miniato al Monte Basilica.

</doc>
<doc id="7439" url="https://en.wikipedia.org/wiki?curid=7439" title="Constructible number">
Constructible number

A point in the Euclidean plane is a constructible point if, given a fixed coordinate system (or a fixed line segment of unit length), the point can be constructed with unruled straightedge and compass. A complex number is a constructible number if its corresponding point in the Euclidean plane is constructible from the usual "x"- and "y"-coordinate axes.
It can then be shown that a real number "r" is constructible if and only if, given a line segment of unit length, a line segment of length |"r" | can be constructed with compass and straightedge. It can also be shown that a complex number is constructible if and only if its real and imaginary parts are constructible.
In terms of algebra, a number is constructible if and only if it can be written using the four basic arithmetic operations and the extraction of square roots but of no higher-order roots. The set of constructible numbers can be completely characterized in the language of field theory: the constructible numbers form the quadratic closure of the rational numbers: the smallest field extension that is closed under square root and complex conjugation. This has the effect of transforming geometric questions about compass and straightedge constructions into algebra. This transformation leads to the solutions of many famous mathematical problems, which defied centuries of attack.
Geometric definitions.
The geometric definition of a constructible point is as follows. First, for any two distinct points "P" and "Q" in the plane, let "L"("P", "Q" ) denote the unique line through "P" and "Q", and let "C" ("P", "Q" ) denote the unique circle with center "P", passing through "Q". (Note that the order of "P" and "Q" matters for the circle.) By convention, "L"("P", "P" ) = "C" ("P", "P" ) = {"P" }. Then a point "Z" is "constructible from E, F, G and H" if either
Since the order of "E", "F", "G", and "H" in the above definition is irrelevant, the four letters may be permuted in any way. Put simply, "Z" is constructible from "E", "F", "G" and "H" if it lies in the intersection of any two distinct lines, or of any two distinct circles, or of a line and a circle, where these lines and/or circles can be determined by "E", "F", "G", and "H", in the above sense.
Now, let "A" and "A"′ be any two distinct fixed points in the plane. A point "Z" is "constructible" if either
Put simply, "Z" is constructible if it is either "A" or "A"′, or if it is obtainable from a finite sequence of points starting with "A" and "A"′, where each new point is constructible from previous points in the sequence.
For example, the center point of "A" and "A"′ is defined as follows. The circles "C" ("A", "A"′) and "C" ("A"′, "A") intersect in two distinct points; these points determine a unique line, and the center is defined to be the intersection of this line with "L"("A", "A"′).
Transformation into algebra.
All rational numbers are constructible, and all constructible numbers are algebraic numbers.
Also, if "a" and "b" are constructible numbers with "b" ≠ 0, then formula_1 and are constructible. 
Thus, the set "K" of all constructible complex numbers forms a field, a subfield of the field of algebraic numbers. A complex number is constructible if and only if the real and imaginary parts are both constructible.
Furthermore, "K" is closed under square roots and complex conjugation. These facts can be used to characterize the field of constructible numbers, because, in essence, the equations defining lines and circles are no worse than quadratic. The characterization is the following: a complex number is constructible if and only if it lies in a field at the top of a finite tower of quadratic extensions, starting with the rational field Q. More precisely, "z" is constructible if and only if there exists a tower of fields
formula_2
where "z" is in "K""n" and for all 0 ≤ "j" < "n", the dimension ["K""j" + 1 : "K""j" ] = 2.
Trigonometric numbers.
Trigonometric numbers are irrational cosines or sines of angles that are rational multiples of formula_3 Such a number is constructible if and only if the denominator of the fully reduced multiple is a power of 2 or the product of a power of 2 with the product of one or more distinct Fermat primes. Thus, for example, formula_4 Is constructible because 15 is the product of the two Fermat primes 3 and 5.
See Trigonometric constants expressed in real radicals for a list of trigonometric numbers expressed in terms of square roots.
Impossible constructions.
The algebraic characterization of constructible numbers provides an important "necessary" condition for constructibility: if "z" is constructible, then it is algebraic, and its minimal irreducible polynomial has degree a power of 2, or equivalently, the field extension Q("z")/Q has dimension a power of 2. One should note that it is true, (but not obvious to show) that the converse is false — this is not a "sufficient" condition for constructibility. However, this defect can be remedied by considering the normal closure of Q("z")/Q.
The non-constructibility of certain numbers proves the impossibility of certain problems attempted by the philosophers of ancient Greece. In the following chart, each row represents a specific ancient construction problem. The left column gives the name of the problem. The second column gives an equivalent algebraic formulation of the problem. In other words, the solution to the problem is affirmative if and only if each number in the given set of numbers is constructible. Finally, the last column provides the simplest known counterexample. In other words, the number in the last column is an element of the set in the same row, but is not constructible.

</doc>
<doc id="7441" url="https://en.wikipedia.org/wiki?curid=7441" title="Carson City, Nevada">
Carson City, Nevada

Carson City, officially the Consolidated Municipality of Carson City, is an independent city and the capital of the US state of Nevada, named after the mountain man Kit Carson. As of the 2010 census, the population was 55,274. The majority of the population of the town lives in Eagle Valley, on the eastern edge of the Carson Range, a branch of the Sierra Nevada. Carson City is about south of Reno and originated as a stopover for California bound emigrants, but developed into a city with the Comstock Lode, a silver strike in the mountains to the northeast. The city has served as the capital of Nevada since statehood in 1864 and for much of its history was a hub for the Virginia and Truckee Railroad, although the tracks were removed in the 1950s. Prior to 1969, Carson City was also the county seat of Ormsby County. In 1969, the county was abolished, and its territory was merged with Carson City to form the Consolidated Municipality of Carson City. With the consolidation, the city limits today extend west across the Sierra Nevada to the California state line in the middle of Lake Tahoe. Like other independent cities in the United States, it is treated as a county-equivalent for census purposes.
History.
The first European Americans to arrive in what is known as Eagle Valley were John C. Fremont and his exploration party in January 1843. Fremont named the river flowing through the valley Carson River in honor of Christopher "Kit" Carson, the mountain man and scout he had hired for his expedition. Prior to the Fremont expedition, the Washoe people inhabited the valley and surrounding areas. Settlers named the area Washoe in reference to the tribe.
By 1851 the Eagle Station ranch located along the Carson River served as a trading post and stopover for travelers on the California Trail's Carson Branch which ran through Eagle Valley. The valley and trading post received their name from a bald eagle that was hunted and killed by one of the early settlers and was featured on a wall inside the post.
As the area was part of the Utah Territory, it was governed from Salt Lake City, where the territorial government was headquartered. Early settlers bristled at the control exerted by Mormon-influenced officials and desired the creation of the Nevada territory. A vigilante group of influential settlers, headed by Abraham Curry, sought a site for a capital city for the envisioned territory. In 1858, Abraham Curry bought Eagle Station and thereafter renamed the settlement Carson City. As Curry and several other partners had Eagle Valley surveyed for development. Curry had decided for himself that Carson City would someday serve as the capital city and left a plot open in the center of town for a future capitol building.
Following the discovery of gold and silver in 1859 on the nearby Comstock Lode, Carson City's population began to rise. Curry built the Warm Springs Hotel a mile to the east of the center of town. When the territorial governor James W. Nye traveled to Nevada, he chose Carson City as the territorial capital, influenced by Carson City lawyer William Stewart, who escorted him from San Francisco to Nevada. As such, Carson City bested Virginia City and American Flat. Curry loaned the Warm Springs Hotel to the territorial Legislature as a meeting hall. The Legislature named Carson City to be the seat of Ormsby County and selected the hotel as the territorial prison with Curry serving as its first warden. Today the property still serves as part of the state prison.
When Nevada became a state in 1864 during the Civil War, Carson City was confirmed as Nevada's permanent capital. Carson City's development was no longer dependent on the mining industry and instead became a thriving commercial center. The Virginia & Truckee Railroad was built between Virginia City and Carson City. A wooden flume was also built from the Sierra Nevadas into Carson City. The current capitol building was constructed from 1870 to 1871. The United States Mint operated a branch mint in Carson City between the years 1870 and 1893, which struck gold and silver coins. People came from China during that time, many of them to work on the railroad. Some of them owned businesses and taught school. By 1880, almost a thousand Chinese people, "one for every five Caucasians," lived in Carson City.
Carson City's population and transportation traffic decreased when the Central Pacific Railroad built a line through Donner Pass, too far to the north to benefit Carson City. The city was slightly revitalized with the mining booms in Tonopah and Goldfield. The US federal building (now renamed the Paul Laxalt Building) was completed in 1890 as was the Stewart Indian School. Even these developments were not enough to prevent the city's population from dropping to just over 1,500 people by 1930. Carson City resigned itself to small city status, advertising itself as "America's smallest capital." The city slowly grew after World War II; by 1960 it had reached its 1880, boom-time population.
20th-century revitalization and growth.
As early as the late 1940s, discussions began about merging Ormsby County and Carson City. By this time, the county was little more than Carson City and a few hamlets to the west. However, the effort did not pay off until 1966, when a statewide referendum formally approved the merger. The required constitutional amendment was passed in 1968. On April 1, 1969; Ormsby County and Carson City officially merged as the Consolidated Municipality of Carson City. With this consolidation, Carson City absorbed former town sites such as Empire City, which had grown up in the 1860s as a milling center along the Carson River and current US 50. Carson City could now advertise itself as one of America's largest state capitals with its of city limits.
In 1991, the city adopted a downtown master plan, specifying that no building within of the capitol would surpass it in height. This plan effectively prohibited future high-rise development in the center of downtown. The Ormsby House is currently the tallest building in downtown Carson City, at a height of 117 feet. The structure was completed in 1972.
Demographics.
Carson City is the smallest of the United States' 366 Metropolitan Statistical Areas.
As of the 2010 census there are 55,274 people, 20,171 households, and 13,252 families residing in the city. The population density is 366 people per square mile (141/km2). There are 21,283 housing units at an average density of 148/sq mi (57/km2). The racial makeup of the city is 81.1% White, 1.9% Black or African American, 2.4% Native American, 2.1% Asian, 0.2% Pacific Islander, 9.4% from other races, and 2.9% from two or more races. 21% of the population are Hispanic or Latino of any race.
As of the 2000 census, there are 20,171 households, out of which 29.8% have children under the age of 18 living with them, 50.0% are married couples living together, 11.0% have a female householder with no husband present, and 34.3% are non-families. 27.8% of all households are made up of individuals and 11.00% have someone living alone who is 65 years of age or older. The average household size is 2.44 and the average family size is 2.97. The city's age distribution is: 23.4% under the age of 18, 7.9% from 18 to 24, 28.9% from 25 to 44, 24.9% from 45 to 64, and 14.9% who are 65 years of age or older. The median age is 39 years. For every 100 females there are 106.90 males. For every 100 females age 18 and over, there are 108.20 males.
Data from the 2000 census indicates that the median income for a household in the city is $41,809, and the median income for a family is $49,570. Males have a median income of $35,296 versus $27,418 for females. The per capita income for the city is $20,943. 10.0% of the population and 6.9% of families are below the poverty line. Out of the total population, 13.7% of those under the age of 18 and 5.8% of those 65 and older are living below the poverty line.
Languages.
As of 2010, 82.31% (42,697) of Carson City residents age 5 and older spoke English at home as a primary language, while 14.12% (7,325) spoke Spanish, 0.61% (318) French, and numerous Indic languages were spoken as a main language by 0.50% (261) of the population over the age of five. In total, 17.69% (9,174) of Carson City's population age 5 and older spoke a mother language other than English.
Economy.
The following is a list of the top employers in Carson City from the fourth quarter of 2012:
1,500 - 1,999 Employees
1,000 - 1,499 Employees
500 - 999 Employees
200 - 499 Employees
100-199 Employees
Government and politics.
Ormsby County consolidated with Carson City in 1969, and the county simultaneously dissolved. The city is now governed by a five-member board of supervisors, consisting of a mayor and four supervisors. All members are elected at-large, but each of the four supervisors must reside in respective wards, numbered 1 through 4. The mayor and supervisors serve four year terms. Elections are staggered so that the mayor and the supervisors from Wards 2 and Ward 4 are elected in presidential election years, and the supervisors from Ward 1 and 3 are elected in the even-numbered years in between (i.e., the same year as gubernatorial elections).
Nevada's capital is generally considered a Republican stronghold, often voting for Republicans by wide margins. In 2004, George Bush defeated John Kerry 57-40%. In 2008 however Barack Obama became the first Democrat since 1964 to win Ormsby County/Carson City, defeating John McCain 49% to 48%, by 204 votes, a margin of under 1%.
Carson City, being the state capital, is home to many political protests and demonstrations at any given time.
In an attempt to either make proposed spent nuclear fuel storage facility at Yucca Mountain prohibitively expensive (by raising property tax rates to the maximum allowed) or to allow the state to collect the potential federal payments of property taxes on the facility, the state government in 1987 carved Yucca Mountain out of Nye County and created a new county with no residents out of the area surrounding Yucca called Bullfrog County. Carson City became the county seat of Bullfrog County, even though it is not located in Bullfrog County and is more than from Yucca Mountain. A state judge found the process unconstitutional in 1989, and Bullfrog County's territory was retroceded to Nye County.
Climate.
Carson City features a semi-arid climate with cool but not inordinately cold winters and hot summers. The city is situated in a high desert river valley approximately above sea level. There are four fairly distinct seasons, all of which are relatively mild compared to many parts of the country and to what one may expect given its elevation. Winters see typically light to moderate snowfall, with a median of . Most precipitation occurs in winter and spring, with summer and fall being fairly dry, drier than neighboring California. There are 37 days of + highs annually, with + temperatures occurring in some years.
The Carson River flows from Douglas County through the southwestern edge of Carson City.
Also notably, Carson City has warmed more than any other city in the nation during the last 30 years.
Education.
The Carson City School District operates ten schools in Carson City. The six elementary schools are Bordewich-Bray Elementary School, Empire Elementary School, Fremont Elementary School, Fritsch Elementary School, Mark Twain Elementary School, and Al Seeliger Elementary School. The two middle schools are Carson Middle School and Eagle Valley Middle School. Carson High School and the alternative Pioneer High School serve high school students. Carson High is on Saliman Road.
Western Nevada College (WNC) is a regionally accredited, two-year and four-year institution which is part of the Nevada System of Higher Education. It has an education program. The school also offers associate of arts, associate of science.
Sports and recreation.
Carson City has never hosted any professional team sports. However, a variety of sports are offered at parks and recreation. Many neighborhood parks offers a wide variety of features, including picnic tables, beaches, restrooms, fishing, softball, basketball, pond, tennis, and volleyball. The largest park is Mills Park, which has a total land area of and includes the narrow gauge Carson & Mills Park Railroad.
While there are no ski slopes within Carson City, the city is located close to Heavenly Mountain Resort, Diamond Peak and Mount Rose skiing areas.
Points of interest.
Museums.
Children's Museum of Northern Nevada - Carson City www.cmnn.org
In popular culture.
Films.
The following is a list of movies with scenes filmed in Carson City:
The following is a list of films with scenes set in Carson City but filmed elsewhere:
Television episode:
Notable people.
Carson City has served as one of the state’s centers for politics and business. Every state governor since Denver S. Dickerson has resided in the Governor's Mansion located in Carson City. "See also: List of Governors of Nevada." The following personalities took up a residence in Carson City at some point in their lives.
Transportation.
There are two highways in the city US Route 395 and US Route 50. Carson City is home to one under-construction freeway Interstate 580. Phase 1 of the Carson City Freeway Project from US 395, just north of the city, to US 50 was completed in February 2006 and Phase 2A, extending from Rt. 50 to Fairview Drive, was officially opened on September 24, 2009. Phase 2B, Fairview Drive to Rt. 50, awaits funding and, according to Director Martinovich at NDOT, completion is anticipated for the fall of 2017 Prior to 2012, Carson City was one of only five state capitals not directly served by an Interstate highway; the city lost this distinction when I-580 was extended into the city limits.
Carson City's first modern bus system, Jump Around Carson, or JAC, opened to the public. JAC uses a smaller urban bus that is ideal for Carson City. However, there is virtually no ground public transportation to other destinations. Passenger trains haven't served Carson City since 1950, when the Virginia and Truckee Railroad was shut down. Greyhound Lines stopped their bus services to the town in 2006 and Amtrak discontinued their connecting thruway bus to Sacramento in 2008. There is now only a limited Monday – Friday RTC bus service to Reno which is still served by both Greyhound and Amtrak.
Carson City is also served by the Carson Airport, which is a regional airport in the northern part of the city. Reno-Tahoe International Airport, which is away, handles domestic commercial flights.

</doc>
<doc id="7442" url="https://en.wikipedia.org/wiki?curid=7442" title="Clark Kent">
Clark Kent

Clark Joseph/Jerome Kent is a fictional character appearing in American comic books published by DC Comics. Created by Jerry Siegel and Joe Shuster, he debuted in "Action Comics" #1 (June 1938) and serves as the civilian and secret identity of the superhero Superman.
Over the decades there has been considerable debate as to which personality the character identifies with most. From his first introduction in 1938 to the mid-1980s, "Clark Kent" was seen mostly as a disguise for Superman, enabling him to mix with ordinary people. This was the view in most comics and other media such as movie serials and TV (e.g., in "Atom Man vs. Superman" starring Kirk Alyn and "The Adventures of Superman" starring George Reeves) and radio. In 1986, during John Byrne's revamping of the character, Clark Kent became more emphasized. Different takes persist in the present.
Overview.
As Superman's alter ego, the personality, concept, and name of Clark Kent have become ingrained in popular culture as well, becoming synonymous with secret identities and innocuous fronts for ulterior motives and activities. In 1992, Superman co-creator Joe Shuster told the "Toronto Star" that the name derived from 1930s cinematic leading men Clark Gable and Kent Taylor, but the persona from bespectacled silent film comic Harold Lloyd and himself. Another, perhaps more likely possibility, is that Jerry Siegel pulled from his own love of pulp heroes Doc Clark Savage and The Shadow alias Kent Allard. This idea was notably stated in the book "Men of Tomorrow: Geeks, Gangsters, and the Rise of the American Comic Book".
Clark's middle name is given variously as either Joseph, Jerome or Jonathan, all being allusions to creators Jerry Siegel and Joe Shuster.
Beginnings.
In the earliest "Superman" comics, Clark Kent's primary purpose was to fulfill the perceived dramatic requirement that a costumed superhero cannot remain on full duty all the time. Clark thus acted as little more than a front for Superman's activities. Although his name and history were taken from his early life with his adoptive Earth parents, everything about Clark was staged for the benefit of his alternate identity: as a reporter for the "Daily Planet", he receives late-breaking news before the general public, has a plausible reason to be present at crime scenes, and need not strictly account for his whereabouts as long as he makes his story deadlines. He sees his job as a journalist as an extension of his Superman responsibilities—bringing truth to the forefront and fighting for the little guy. He believes that everybody has the right to know what is going on in the world, regardless of who is involved.
To deflect suspicion that he is Superman, Clark Kent adopted a largely passive and introverted personality with conservative mannerisms, a higher-pitched voice, and a slight slouch. This personality is typically described as "mild-mannered", perhaps most famously by the opening narration of Max Fleischer's "Superman" animated theatrical shorts. These traits extended into Clark's wardrobe, which typically consists of a bland-colored business suit, a red necktie, black-rimmed glasses (which in Pre-Crisis stories had lenses of Kryptonian material that would not be damaged when he fired his heat vision through them), combed-back hair, and occasionally a fedora.
Fellow reporter Lois Lane became the object of Clark's/Superman's romantic affection. Lois' affection for Superman and her rejection of Clark's clumsy advances have been a recurring theme in Superman comics and movies and on television.
Transitions.
Clark wears his Superman costume underneath his street clothes, allowing easy changes between the two personae and the dramatic gesture of ripping open his shirt to reveal the familiar "S" emblem when called into action. Superman usually stores his Clark Kent clothing compressed in a secret pouch within his cape, though some stories have shown him leaving his clothes in some covert location (such as the "Daily Planet" storeroom) for later retrieval.
In the Pre-Crisis comic book title "Superman Family", Clark is featured in a series of stories called "The Private Life of Clark Kent" wherein he solves problems subtly without changing into Superman.
Adoption.
Adopted by Jonathan and Martha Kent from the Kansas town of Smallville, Clark (and thus Superman) was raised with the values of a typical rural American town, including attending the local Methodist Church (though it is debated by comic fans if Superman is a Methodist).
Most continuities state that the Kents had been unable to have biological children. In the Golden and Silver Age versions of his origin, after the Kents retrieved Clark from his rocket, they brought him to the Smallville Orphanage and returned a few days later to formally adopt the orphan, giving him as a first name Martha's maiden name, "Clark". In John Byrne's 1986 origin version "The Man of Steel," instead of adopting him through an orphanage, the Kents passed Clark off as their own child after their farm was isolated for months by a series of snowstorms that took place shortly after they found his rocket, using their past medical history of various miscarriages to account for their reasons for keeping Martha's pregnancy secret.
Silver Age.
In the Silver Age comics continuity, Clark's superpowers manifested upon his landing on Earth and he gradually learned to master them, adopting the superhero identity of Superboy at the age of eight. He subsequently developed Clark's timid demeanor as a means of ensuring that no one would suspect any connection between the two alter-egos.
Modern Age retroactive continuity.
In the wake of John Byrne's reboot of Superman continuity in "The Man of Steel", many traditional aspects of Clark Kent were dropped in favor of giving him a more aggressive and extroverted personality (although not as strong as Lois's), including such aspects as making Clark a top football player in high school along with being a successful author and Pulitzer Prize-winning writer, which includes at least two original novels, "The Janus Contract", and "Under a Yellow Sun". Furthermore, Clark's motivations for his professional writing were deepened as both a love for the art that "contributes at least as much social good as his Superman activities" and as a matter of personal fulfillment in an intellectual field in which his abilities give no unfair competition to his colleagues beyond typing extraordinarily fast. Following "One Year Later", Clark adopts some tricks to account for his absences, such as feigning illness or offering to call the police. These, as well as his slouching posture, are references to his earlier mild-mannered Pre-Crisis versions, but he still maintains a sense of authority and his assertive self. Feeling that Clark is the real person and that Clark is not afraid to be himself in his civilian identity, John Byrne has stated in interviews that he took inspiration for this portrayal from the George Reeves version of Superman.
Clark's favorite movie is "To Kill a Mockingbird" (in which Gregory Peck wears glasses not unlike Kent's). According to the "DC Comics Official Guide to Superman," Clark enjoys peanut butter and jelly sandwiches, football games, and the smell of Kansas in the springtime. His favorite baseball team is the Metropolis Monarchs and his favorite football team is the Metropolis Sharks. As of "One Year Later", Clark is in his mid-thirties, stands at , and weighs about . Unlike in the Silver Age, his powers developed over several years, only coming to their peak when he was an adult.
Secret identity.
Superman's secret identity as Clark Kent is one of the DC Universe's greatest secrets. Only a few trusted people are aware of it, such as Batman and other members of the Justice League, Superman's cousin Supergirl, and Clark's childhood friend Lana Lang (In pre-Crisis stories, Lana did not know, but their friend Pete Ross did, unbeknownst to anyone, including Clark). Lex Luthor, other supervillains, and various civilians have learned the secret identity several times, though their knowledge is usually removed through various means (the boxer Muhammad Ali is one of the very few to deduce the identity and retain the knowledge).
Traditionally, Lois Lane (and sometimes others) would often suspect Superman of truly being Clark Kent; this was particularly prominent in Silver Age stories, including those in the series "Superman's Girl Friend Lois Lane". More recent stories (post-Crisis) often feature the general public assuming that Superman has no secret identity owing to the fact that he, unlike most heroes, does not wear a mask. In "The Secret Revealed", a supercomputer constructed by Lex Luthor calculated Superman's true identity from information that had been assembled by his staff, but Lex dismissed the idea because he could not believe that someone so powerful would want another, weaker identity. In post-"Crisis" continuity, Lois Lane, feeling that someone like Clark could not be Superman, never suspected the dual identity beyond one isolated incident before Clark finally revealed it to her. In "Visitor", Lois finds Superman at the Kent farm with Lana Lang and asks him point-blank if he is Clark Kent. Before he can answer, the Kents tell her that they raised Superman alongside Clark like a brother. In the 2009 retcon of the mythos, Lois Lane is fully aware from the beginning, along with Perry White, that the meek, pudgy, and bumbling Clark Kent deliberately holds himself back: however, still far from associating him with Superman, they simply believe he's hiding his qualities as a good reporter. In the current continuity established by DC's New 52 relaunch in 2011, Lois Lane remains unaware that Clark is Superman until she discovers his identity, when, as part of an elaborate plan by a new villain to blackmail Superman. Superman also revealed his identity to Jimmy Olsen.
In the future of the Legion of Super-Heroes, his secret identity is historical fact, with exhibits at a Superman Museum depicting the hero and his friends' and family's adventures.
Security of identity.
Various explanations over the decades have been offered for why people have never suspected Superman and Clark Kent of being one and the same:
Identity change.
When crises arise, Clark quickly changes into Superman. Originally during his appearances in "Action Comics" and later in his own magazine, the Man of Steel would strip to his costume and stand revealed as Superman, often with the transformation having already been completed. But within a short time, Joe Shuster and his ghost artists began depicting Clark Kent ripping open his shirt to reveal the "S" insignia on his chest—an image that became so iconic that other superheroes, during the Golden Age and later periods, would copy the same type of change during transformations.
In the Fleischer theatrical cartoons released by Paramount, the mild-mannered reporter often ducked into a telephone booth or stockroom to make the transformation. Since the shorts were produced during the rise of film noir in cinema, the change was usually represented as a stylized sequence: Clark Kent's silhouette is clearly seen behind a closed door's pebble glass window (or a shadow thrown across a wall) as he strips to his Superman costume. Then, the superhero emerges having transformed from his meek disguise to his true self. In the comic books and in the George Reeves television series, he favors the "Daily Planet"s storeroom for his changes of identities (the heroic change between identities within the storeroom is almost always seen in the comics, but never viewed in the Reeves series).
The CBS Saturday morning series "The New Adventures of Superman" produced by Filmation Studios—as well as "The Adventures of Superboy" from the same animation house—featured the iconic "shirt rip" to reveal the "S" or Clark Kent removing his unbuttoned white dress shirt in a secluded spot, usually thanks to stock animation which was reused over dozens of episodes, to reveal his costume underneath while uttering his famed line "This is a job for Superman!"
As a dramatic plot device, Clark often has to quickly improvise in order to find a way to change unnoticed. For example, in "Superman" (1978), Clark, unable to use a newer, open-kiosk pay phone (and getting a nice laugh from the theater audience), runs down the street and rips open his shirt to reveal his costume underneath. He quickly enters a revolving door, spinning through it at incredible speed while changing clothes. Thus made invisible, he appears to have entered the building as Clark Kent and exited seconds later as Superman. Later in the film, when the need to change is more urgent (as he believes the city is about to be poisoned by Lex Luthor), he simply jumps out a window of the Daily Planet offices, changing at super-speed as he falls (the film merely shows the falling Kent blurring into a falling Superman) and flies off. Further films in the series continued this tradition, with Clark blurring into Superman, changing at super-speed while he runs.
In "Lois & Clark," Clark's usual method of changing was to either "suddenly" remember something urgent that required his immediate attention or leave the room/area under the pretense of contacting a source, summoning the police, heading to a breaking story's location, etc. The change would then frequently occur off-screen, although the shirt-rip reveal was a prominently used move well-associated with the show. Clark also developed a method of rapidly spinning into his costume at super speed which became a trademark change, especially during the third and fourth seasons of the series, and extremely popular with the show's fans.
In one scene of "", Clark becomes aware of an emergency while talking with Bruce Wayne and, in the next panel, he has flown out of his Kent clothing and glasses so quickly that they have had no time to fall.
In Season 8 of "Smallville," Clark begins to show a bit more of his double identity. He starts slowing down his superspeed enough for surveillance cameras to see his iconic red and blue streak. This reveals to the citizens of Metropolis that a superhero is among them and the name "The Red-Blue Blur" is coined. When Jimmy Olsen becomes suspicious, Clark decides to reserve his usual red-and-blue for saving people. He carries a backpack with him to work every day, containing his change of clothes. He begins to practice his speed change at home and at the "Daily Planet." He changes in a superspeed spin in the "Daily Planet"s phone booth and once even in his office chair. The last minute of the last episode of "Smallville" had Clark responding to an emergency, rushing to the top of the Daily Planet, and then using the familiar shirt-rip while the camera zoomed in on the familiar S-logo to the original John Williams fanfare.
Debate over true identity.
A relatively recent debate is which of the two identities (Superman or Clark Kent) is the real person and which is the façade. Fans and Superman scholars follow one of three interpretations:
Clark Kent has also been depicted without the Superman alter ego. In the Elseworlds stories starting with "", he is the son of Jonathan Kent, who saves his son from the destruction of the Earth. Clark ends up on Krypton, where he is adopted by Jor-El and becomes the planet's Green Lantern.
In other media.
"The Adventures of Superman" radio series (1940-1951).
In the early "Adventures of Superman" radio episodes, Kal-El landed on Earth as an adult. He saved a man and his son and they gave him the idea of living as a normal person. They gave him the name of Clark Kent, and he later got a job as a newspaper reporter under that name. In that role he adopted a higher voice and a more introverted personality – clearly establishing that Kent is the secret identity and Superman is the true person.
Later episodes shifted to the usual origin story, in which Kal-El landed on Earth as a baby and was raised by the Kent family.
Clayton "Bud" Collyer voiced both Clark Kent and Superman, until Michael Fitzmaurice replaced him in the final episodes.
Kirk Alyn film serials (1948-1950).
In the film serials "Superman" (1948) and "Atom Man vs. Superman" (1950), Kirk Alyn portrays Clark as a mild-mannered reporter who comes to Metropolis and secures a job at the "Daily Planet", following the death of his foster parents. While he quickly gains the respect of "Planet" editor Perry White, he is forced to contend with rival reporter Lois Lane, who often uses trickery to prevent Clark from pursuing a lead (giving her the chance to scoop him). Nevertheless, his journalistic skills are useful as he pursues stories on the crime boss known as the Spider Lady, and the criminal scientist Luthor (who had yet to receive his first name, Lex).
"Adventures of Superman" TV series (1952-1958).
In the 1950s George Reeves series, Clark Kent is portrayed as a cerebral character who is the crime reporter for the "Daily Planet" and who as Kent uses his intelligence and powers of deduction to solve crimes (often before Inspector Henderson does) before catching the villain as Superman. Examples include the episodes "Mystery of the Broken Statues", "A Ghost for Scotland Yard", "The Man in the Lead Mask", and "The Golden Vulture". George Reeves' Kent/Superman is also established as a champion of justice for the oppressed in episodes like "The Unknown People" and "The Birthday Letter". Although Kent is described in the show introduction as "mild-mannered", he can be very assertive, often giving orders to people and taking authoritative command of situations, though, as in the Pre-Crisis Superman stories at that time, Clark is still considered the secret identity. He gets people to trust his judgment very easily and has a good, often wisecracking, sense of humor. Reeves, who first appeared as the character in the 1951 film "Superman and the Mole Men", was older than subsequent Superman actors.
Christopher Reeve films (1978-1987) and "Superman Returns" (2006).
In 1978, the first of four Superman films was made in which Clark Kent and Superman were portrayed by Christopher Reeve (with teenage Kent played by Jeff East in the first film). This was followed nearly two decades later by a fifth film called "Superman Returns" with Brandon Routh giving a performance very similar to Reeve's. In contrast to George Reeves' intellectual Clark Kent, Reeve's version is much more of an awkward fumbler and bungler, although Reeve is also an especially athletic, dashing and debonair Superman. Clark Kent's hair is always absolutely flat, while Superman's hair has a slight wave and is parted on the opposite side as Kent's. These films leave the impression that Clark Kent is really a "secret" identity that is used to enable Superman to serve humanity better, rather than just a role to help him assimilate into the human community.
A great deal of emphasis is placed on his origins on the planet Krypton with exotic crystalline sets designed by John Barry, effectively giving Superman a third persona as Kal-El. The first film is in three sections: Kal-El's infancy on Krypton (shot in London on the 007 stage), Clark Kent's teen years in Smallville, and Kent/Superman's adult life in Metropolis (shot in New York City). In earlier sections of the film, Reeve's Kent interacts with both his earthly parents and the spirit of his Kryptonian father through a special crystal, in a way George Reeves never did. The film has a fair amount of quasi-Biblical imagery suggestive of Superman as a sort of Christ-figure sent by Jor-El "to show humans the way". (See also Superman (1978 film)#Themes). In "Superman II" Reeve's Superman has to sacrifice his powers (effectively becoming just Clark Kent) in order to have a love relationship with Lois Lane, a choice he eventually abrogates to protect the world.
The relationship between Superman and Kent came to actual physical blows in "Superman III". Superman is given a piece of manufactured Kryptonite, but instead of weakening or killing him it drives him crazy, depressed, angry, and casually destructive, committing crimes which range from petty acts of vandalism to environmental disasters, like causing an oil spillage in order to bed a lusty woman by the name of Lorelei in league with the villains. Driven alcoholic, Superman, his outfit dirty and neglected, eventually goes to a car wrecking yard where Kent, in a proper business suit and glasses, suddenly emerges from within him. A fight ensues in which the "evil" Superman tries to dispose of the "good" Kent, but the latter fights back, "kills" the evil side to his nature and, reclaiming the Superman mantle, sets off to repair the damage and capture the villains. Les Daniels comments in his book, "DC Comics: A Celebration of the World's Favourite Comic Book Heroes", "The 'good' Superman, ultimately triumphant, (is) dressed as Clark, thus implying that he is the more valid personality (as well as the one Lana loves)" and expresses annoyance that "Something could have been made of this, but sadly nothing was".
The indirect "Christianization" of Superman in the Reeve films (admitted by film producer Pierre Spengler on the DVD commentaries) has provoked comment on the Jewish origins of Superman. Rabbi Simcha Weinstein's book "Up, Up and Oy Vey: How Jewish History, Culture and Values Shaped the Comic Book Superhero" says that Superman is both a pillar of society and one whose cape conceals a "nebbish", saying, "He's a bumbling, nebbish Jewish stereotype. He's Woody Allen."
"Lois & Clark: The New Adventures of Superman" (1993-1997).
Clark Kent's character is given one of its heaviest emphases in the 1990s series "". It is made very clear during the series, even discussed directly by the characters, that Clark Kent is who he really is, rather than his superheroic alter-ego.
In "Lois and Clark", Kent (Dean Cain) is a stereotypical wide-eyed farm kid from Kansas with the charm, grace and humor of George Reeves, but without the awkward geekiness of Christopher Reeve. Emphasis is laid on the comic elements of his dual relationship with Lois Lane (Teri Hatcher). The ban on Christopher Reeve's Superman having a relationship with a human while retaining his superpowers is entirely absent in the world of "Lois and Clark." In the final season, Clark Kent marries Lois Lane (a few years after her almost-marriage to his arch-enemy Lex Luthor, whom she refused at the altar), finding love, happiness, and completeness in this relationship which does not jeopardize his Superman persona.
Superman's secret identity was discovered by a number of villains during the series. In some cases, like that of Lex Luthor, the villain died before he could share the discovery. In two cases, the claim was discredited by having Superman and Clark appear together in public, using a hologram in the first case and a Clark Kent from a parallel universe in the second (in the first case, there was also footage filmed of Superman uniforms in Kent's closet, but that was explained by stating Superman simply needs a place to store them). In one case, Superman destroyed the evidence (a time traveler's journal), and stated that the villain's unsupported words will be ignored.
"Smallville" TV series (2001-2011).
"Smallville" was adapted to television in 2001, by Alfred Gough and Miles Millar. Clark Kent is played by Tom Welling, with others portraying Clark as an infant. Throughout the series, Clark never officially adopted a costume until around the eighth season, but prior to this was seen wearing Superman's traditional colors of red and blue, more often as the series progresses (more commonly a blue shirt underneath a red jacket, reflecting Superman's uniform and cape colors). He is going through a process of character formation, making many mistakes in his youth, over time forming better and better judgment, while always self-consciously aware of his status as an alien from another planet who is different from other people. In season eight, he begins a fight against evil, hoping to be a source of inspiration and hope to others. A modest amount of religious imagery is seen occasionally in the series, but to a lesser degree than in the Christopher Reeve series.
"Smallville"'s Kent is particularly inwardly conflicted as he attempts to live the life of a normal human being, while keeping the secret of his alien heritage from his friends. Throughout the first seven seasons of the series he has a complicated relationship with Lana Lang, as well as his self-perceived guilt over the fact that the meteor shower that killed Lana's parents and created most of the superhumans he fought in the show's first few years was caused by his rocket coming to Earth and dragging pieces of Krypton with it. Clark's powers appear over time. He is not aware of all of his powers at the start of the show; for instance, his heat vision and super breath do not develop until seasons two and six, respectively, and his power of flight did not emerge until the series finale, up until that point the power appeared only in a few rare cases, such as when he was temporarily 're-programmed' to assume a Kryptonian persona or when he was trapped in a virtual reality.
Clark Kent starts out best friends with Lex Luthor, whom he meets after saving the latter's life. (Boyhood friendship with Lex Luthor had been the basis of a "Superboy" adventure published in 1960).
Clark and Lex remain entangled for most of the series. Lex Luthor's father, Lionel Luthor, is an unscrupulous industrialist with whom Lex has a troubled relationship. Lex would like to transcend his family background and be a better person than his father, but after multiple setbacks he slowly slips into evil, becoming convinced that only way he can "protect" the world from the perceived alien threats is by taking control of it, regardless of the cost to others. In turn, Clark Kent has a slightly dark side with which he comes to grips over time, made even worse by his experiences with Red Kryptonite, which causes him to lose his morals and act solely on impulse while under its influence. In different ways to Luthor, at times Clark also does not have a fully ideal relationship either with his adoptive father, Jonathan, nor with an A.I. based on Jor-El that was sent by the original to guide him, Jonathan occasionally having trouble relating to Clark while Jor-El's lack of his template's emotions causes him to treat Clark too harshly at times. The younger Luthor slightly envies Clark's "clean-cut" and wholesome parents (who disapprove of Clark's friendship with Luthor), while Clark is impressed with Luthor's wealth while failing to understand some of the manipulations he carries out in his interactions with others. Even in his better days, Luthor is highly ambitious for power and wealth, at one time noting that he shares his name with Alexander the Great. Clark Kent, on the other hand, has no idea what he is going to do with his life while bewildered by his powers, and his uncertainty as to why he was sent to Earth.
In season eight of "Smallville", Clark Kent begins to work as a reporter at the "Daily Planet". Shortly after he begins to save lives as an anonymous superhero crimefighter, which becomes known as the "Red-Blue Blur" after a photograph is taken of one of his rescues.
In season nine, Clark unintentionally begins to formalize his dual identity to protect his secret and also privately introduces the well-known glasses to Lois Lane. Additionally, during the opening scene of the season nine finale, Clark finds a gift from his mother containing his Superman suit (although the suit is subsequently taken by Jor-El until Clark is ready for it).
In season ten, for the first time in public Clark begins to formulate a bumbling/stuttering Kent with glasses akin to the Christoper Reeve/Brandon Routh portrayal of the character. In the season ten finale of the series he fully adopts the Superman identity, when he takes action to save Earth from Darkseid, who was drawn to Earth by Clark's actions and sought to take the hero as a host.
"Smallville"s Kent has also appeared in various literature (including comics and over a dozen young adult novels) based on the television series.
Animated series.
In the 1940s Superman shorts, Clark is shown to have a wisecracking sense of humor and he and Lois are good friends. At the near end of each short, Clark gives out a smile and a wink to the audience (that was carried over to the 1966 Superman animated series).
In the "" of the mid to late 1990s, Clark Kent is shown as a mild-mannered but competent reporter and is shown exposing various criminals through his reporter identity. In this identity, Clark and Lois are good friends (with Lois frequently calling him "Smallville" in a teasing but good-natured way) but do not share romantic feelings; instead, it is Superman and Lois who have a romantic relationship. Lana Lang on the other hand knows of Clark's identity as Superman but seems more interested in Clark because she knew him as that first.
"Supergirl".
Clark was mentioned by Cat Grant in "Supergirl" episode "Stronger Together".
DC Extended Universe.
In the Superman reboot film "Man of Steel", Clark Kent is portrayed by Henry Cavill, with Dylan Sprayberry and Cooper Timberline portraying younger versions of the character.
In this film, Kal-El is Krypton's first natural birth in centuries, a birth without using Krypton's genesis chamber. In order to save Krypton's future and stop Zod's coup, his biological father Jor-El steals Krypton's DNA template (Codex), bonds it to Kal-El's cells, and sends him to Earth before Krypton explodes. Kal-El's ship lands in a small Kansas town. He is raised as the adoptive son of Jonathan and Martha Kent, who name him Clark Kent. As a boy, Clark is a conflicted and lonely person who questions his place and purpose in the world. At a very young age, he learns of his superhuman abilities including superhearing, heat vision, X-ray vision, superhuman strength, and invulnerability. Despite being ridiculed throughout his childhood and adolescence, he uses his abilities to help others. However, he was depicted as being an angry individual, who is forced to show restraint on his temptations to bring harm to those who attempt to do so to him; a trait that follows him into adulthood. When he learns about his alien background as a boy, he is frightened and confused.
After Jonathan's death, an adult Clark spends several years living a nomadic lifestyle, working different jobs under false identities while saving people in secret, as well as struggling to cope with the loss of his adoptive father. He eventually infiltrates a U.S. military investigation of a Kryptonian scout spaceship in the Arctic. Clark enters the alien ship and communicates with the preserved consciousness of Jor-El in the form of a hologram. Jor-El reveals Clark's origins and the extinction of his race, and tells Clark that he was sent to Earth to bring hope to mankind. Lois Lane, a journalist from the Daily Planet sent to write a story on the discovery, sneaks inside the ship while following Clark and is rescued by him when she is injured. Lois's editor, Perry White, rejects her story of a "superhuman" rescuer, so she traces Clark back to Kansas with the intention of writing an exposé. After hearing his story, she decides not to reveal his secret. After the discovery of his background and purpose, he is shown to be less confused and a little more joyful, as evidenced by his discussion with his adoptive mother Martha.
When Zod arrives to transform Earth into a new Krypton, Lois helps Clark/Superman stop Zod. By film's end, to create an alias that gives him access to dangerous situations without arousing suspicion, Clark takes a job as a reporter at the Daily Planet and adopts a modernized version of his "mild-mannered" look from the comics.
It is worth noting that, as a nod to many comics, Clark is implied to have an interest in football, as evidenced when he is seen watching a game while drinking beer just before Zod's arrival and ultimatum.
Henry Cavill reprises his role as Clark Kent in the film '. Here, his full name is given as Clark Joseph Kent. Nearly two years after the events of Man of Steel, Clark and Lois are close in their relationship, but Clark finds himself continually questioning his role as Superman. At the beginning, he rescues Lois from African terrorists when a riot ensues that he is blamed for. When he hears of Batman's actions in Gotham, he decides to investigate against Perry White's orders, believing Batman's methods to be unjust. He meets Bruce Wayne at a party hosted by Lex Luthor and grows suspicious when he hears Alfred communicating to Bruce in an earpiece. He later confronts Batman when the vigilante is chasing down Luthor's men who have kryptonite, and orders him to cease his activities. Superman is later summoned by Senator June Finch to the U.S. Capitol to discuss his actions, but the room is bombed by Luthor, framing Superman once more. He goes into a self-exile, feeling guilty for not stopping the bombing. 
Later, he dons the costume once more when Lois is endangered by Lex, and confronts the scientist on the roof. However, Lex reveals he knows all about Clark's true identity and blackmails Superman into fighting Batman by holding Martha hostage. Superman tries to reason with Batman, revealing that he knows his secrets, but this leads to a fight in which Batman nearly kills Superman with a kryptonite spear. Superman pleads for Batman to "save Martha", which was also the latter's late mother's name, causing him to come to his senses and realize Superman is not a threat. Upon learning of Luthor's plan, Batman leaves to rescue Martha while Superman confronts Luthor, who unleashes a monstrous artificially-bred creature he dubs as Clark's "Doomsday" made with Kryptonian technology on the crashed ship. Superman is aided by Batman and the mysterious Wonder Woman in confronting the monster, but none of them are able to put the creature down. Knowing it's kryptonian, Clark retrieves the kryptonite spear and impales Doomsday with the object, who in response mortally wounds Clark by stabbing him with one of its claws. Two separate funerals are held. Metropolis holds a funeral for Superman, but the Kent farm holds a private one for Clark Kent (which contains his actual body). Martha gives Lois an engagement ring that Clark planned to give to her. The dirt around Clark's casket briefly levitates indicating he may still be alive.
Batman also had a vision at one point in the movie where he leads a rebellion against the forces of Superman in a dystopian future where Clark kills two hostages with his heat vision before killing Batman while saying "You took her from me." A mysterious time traveler also briefly appears warning him that Lois is the key. Whether or not Batman's vision comes true remains to be seen.

</doc>
<doc id="7445" url="https://en.wikipedia.org/wiki?curid=7445" title="Classification of finite simple groups">
Classification of finite simple groups

In mathematics, the classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four classes described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference with respect to the case of integer factorization is that such "building blocks" do not necessarily determine uniquely a group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.
The proof of the classification theorem consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.
Statement of the classification theorem.
The classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.
Daniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.
Overview of the proof of the classification theorem.
wrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:
Groups of small 2-rank.
The simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.
The simple groups of small 2-rank include:
The classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.
All groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)
Groups of component type.
A group is said to be of component type if for some centralizer "C" of an involution, "C"/"O"("C") has a component (where "O"("C") is the core of "C", the maximal normal subgroup of odd order).
These are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.
A major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of "C"/"O"("C") is the image of a component of "C".
The idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.
Groups of characteristic 2 type.
A group is of characteristic 2 type if the generalized Fitting subgroup "F"*("Y") of every 2-local subgroup "Y" is a 2-group.
As the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.
The rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.
Groups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.
The three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of "standard type" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.
The general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.
Existence and uniqueness of the simple groups.
The main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster group totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.
History of the proof.
Gorenstein's program.
In 1972 announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:
Timeline of the proof.
Many of the items in the list below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the "wrong" order.
Second-generation classification.
The proof of the theorem, as it stood around 1985 or so, can be called "first generation". Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called "revisionism", was originally led by Daniel Gorenstein.
, six volumes of the second generation proof have been published . In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.
Gorenstein and his collaborators have given several reasons why a simpler proof is possible.
Why is the proof so long?
Gorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups.
Consequences of the classification.
This section lists some results that have been proved using the classification of finite simple groups.

</doc>
<doc id="7446" url="https://en.wikipedia.org/wiki?curid=7446" title="Chalcolithic">
Chalcolithic

The Chalcolithic (; "khalkós", "copper" and "líthos", "stone") period or Copper Age, also known as the Eneolithic or Æneolithic (from Latin "aeneus" "of copper"), is a phase of the Bronze Age before it was discovered that adding tin to copper formed the harder bronze. The Copper Age was originally defined as a transition between the Neolithic and the Bronze Age. However, because it is characterized by the use of metals, the Copper Age is considered a part of the Bronze Age rather than the Stone Age.
The archaeological site of Belovode on the Rudnik mountain in Serbia contains the world's oldest securely dated evidence of copper smelting at high temperature, from 5,000 BCE.
Names and definitions.
Origin of names.
The multiple names result from multiple recognitions of the period. Originally the term "Bronze Age" meant that either copper or bronze was being used as the chief hard substance for the manufacture of tools and weapons. In 1881, John Evans, recognizing that the use of copper often preceded the use of bronze, distinguished between a transitional Copper Age and the Bronze Age proper. He did not include this transitional period in the tripartite system of Early, Middle and Late Bronze Age but placed it at the beginning outside of it. He did not, however, present it as a fourth age, but chose to retain the traditional three-age system.
In 1884, Gaetano Chierici, perhaps following the lead of Evans, renamed it in Italian as the "Eneo-litica", or "Bronze-stone" transition. This phrase was never intended to mean that the period was the only one in which both bronze and stone were used. The Copper Age features the use of copper, excluding bronze; moreover, stone continued to be used throughout both the Bronze Age and the Iron Age. "Litica" simply names the Stone Age as the point from which the transition began and is not another -lithic age. The Eneolithic was never part of the Stone Age, which ended conclusively the moment the first smelter succeeded in obtaining copper from copper ore for the first time.
Subsequently British scholars used either Evans's "Copper Age" or the term "Eneolithic" (or Aeneolithic), a translation of Chierici's "eneo-litica". After several years, a number of complaints appeared in the literature that "Eneolithic" seemed to the untrained eye to be produced from e-neolithic, "outside the Neolithic," clearly not a definitive characterization of the Copper Age. About the year 1900, many writers began to substitute "Chalcolithic" for Eneolithic, to avoid the false segmentation. It was at this time that the misunderstanding began among those who had not understood the Italian. The -lithic was seen as a new -lithic age, a part of the Stone Age in which copper was used, which may appear paradoxical. Today Copper Age, Eneolithic and Chalcolithic are used synonymously to mean Evans's original definition of Copper Age.
Classification and characteristics.
The period is a transitional one, but does not stand outside the traditional three-age system.
The analysing stone tool assemblages from sites on the Tehran Plain in Iran has illustrated the effects of the introduction of copper working technologies on the in-place systems of lithic craft specialists and raw materials. Networks of exchange and specialized processing and production that had evolved during the Neolithic seem to have collapsed by the Middle Chalcolithic ("ca." 4500-3500 BCE) and been replaced by the use of local materials by a primarily household base production of stone tools.
It appears that copper was not widely exploited at first, and that efforts in alloying it with tin and other metals began quite soon, making it difficult to distinguish the distinct Chalcolithic cultures from later periods. The boundary between the Copper and Bronze Ages is indistinct, since alloys faded in and out of use due to the erratic supply of tin.
Regions.
The emergence of metallurgy may have occurred first in the Fertile Crescent, where it gave rise to the Bronze Age in the 4th millennium BCE (the traditional view), though finds from the Vinča culture in Europe have now been securely dated to slightly earlier than those of the Fertile Crescent.
There was an independent invention of copper and bronze smelting first by Andean civilizations in South America extended later by sea commerce to the Mesoamerican civilization in West Mexico (see Metallurgy in pre-Columbian America and Metallurgy in pre-Columbian Mesoamerica).
According to Parpola, ceramic similarities between the Indus Civilization, southern Turkmenistan, and northern Iran during 4300–3300 BCE of the Chalcolithic period (Copper Age) suggest considerable mobility and trade.
Dates.
The literature of European archaeology, in general, avoids the use of 'chalcolithic' (the term 'Copper Age' is preferred), whereas Middle Eastern archaeologists regularly use it. The Copper Age in the Middle East and the Caucasus began in the late 5th millennium BCE and lasted for about a millennium before it gave rise to the Early Bronze Age. The transition from the European Copper Age to Bronze Age Europe occurs about the same time, between the late 5th and the late 3rd millennia BCE.
Europe.
An archaeological site in Serbia contains the oldest securely dated evidence of copper making at high temperature, from 7,500 years ago. The find in June 2010 extends the known record of copper smelting by about 800 years, and suggests that copper smelting may have been invented in separate parts of Asia and Europe at that time rather than spreading from a single source.
In Serbia, a copper axe was found at Prokuplje, which indicates that humans were using metals in Europe by 7,500 years ago (~5,500 BCE), many years earlier than previously believed. Knowledge of the use of copper was far more widespread than the metal itself. The European Battle Axe culture used stone axes modeled on copper axes, even with imitation "mold marks" carved in the stone. Ötzi the Iceman, who was found in the Ötztal Alps in 1991 and whose remains were dated to about 3300 BCE, was found with a Mondsee copper axe.
Examples of Chalcolithic cultures in Europe include Vila Nova de São Pedro and Los Millares on the Iberian Peninsula. Pottery of the Beaker people has been found at both sites, dating to several centuries after copper-working began there. The Beaker culture appears to have spread copper and bronze technologies in Europe, along with Indo-European languages. The term "Chalcolithic" is not generally used by British prehistorians, who disagree whether it applies in the British context.
South Asia.
In Bhirrana, the earliest Indus site, copper bangles and copper arrowheads were found. The inhabitants of Mehrgarh in present-day Pakistan fashioned tools with local copper ore (ore used as pigment) between 7000–3300 BCE. 
Nausharo site dated to 4500 years ago, a pottery workshop in province of Baluchistan, Pakistan, unearthed 12 blades or blade fragments. The dimensions of these blades are: length 12–18 cm, width 12–20 mm and relatively thin in thickness. The archaeological experiments show that these blades were made with copper indentor, and functioned as potter's tool to trim and shape unfired pottery. Petrographic analysis indicates local pottery manufacturing, but also reveals that existence of few exotic black-slipped pottery from Indus Valley.
East Asia.
5th millennium BCE copper artifacts start to appear in East Asia, such as Jiangzhai and Hongshan culture, but those metal artifacts were not widely used.
Middle East.
Timna Valley contains evidence of copper mining 9,000 to 7,000 years ago. The process of transition from Neolithic to Chalcolithic in the Middle East is characterized in archaeological stone tool assemblages by a decline in high quality raw material procurement and use. This dramatic shift is seen throughout the region, including the Tehran Valley, Iran. Here, analysis of six archaeological sites determined a marked downward trend in not only material quality, but also in aesthetic variation in the lithic artefacts. Fazeli et al. use these results as evidence of the loss of craft specialisation caused by increased use of copper tools.
Africa.
North Africa and the Nile Valley imported its iron technology from the Near East and followed the Near Eastern course of Bronze Age and Iron Age development. However the Iron Age and Bronze Age occurred simultaneously in much of Africa. The earliest dating of iron in Sub-Saharan Africa is 2500 BCE at Egaro, west of Termit, making it contemporary to the Middle East. The Egaro date is debatable with archaeologists, due to the method used to attain it. The Termit date of 1500 BCE is widely accepted.
In the region of the Aïr Mountains in Niger, we have the development of independent copper smelting between 3000–2500 BCE. The process was not in a developed state, indicating smelting was not foreign. It became mature about 1500 BCE. d
Americas.
The term is also applied to American civilizations that already used copper and copper alloys thousands of years before the European migration. Besides cultures in the Andes and Mesoamerica, the Old Copper Complex, centered in the Upper Great Lakes region – present-day Michigan and Wisconsin in the United States – mined and fabricated copper as tools, weapons, and personal ornaments. The evidence of smelting or alloying that has been found is subject to some dispute and a common assumption by archaeologists in that objects were cold-worked into shape. Artifacts from some of these sites have been dated from 4000 to 1000 BCE, making them some of the oldest Chalcolithic sites in the entire world. Furthermore, some archaeologists find artifactual and structural evidence of casting by Hopewellian and Mississippian peoples to be demonstrated in the archaeological record.

</doc>
<doc id="7447" url="https://en.wikipedia.org/wiki?curid=7447" title="Circumcision and law">
Circumcision and law

There exist laws restricting or regulating circumcision, some dating back to ancient times. In a number of modern states, circumcision is presumed to be legal, but under certain circumstances, more general laws, such as laws about assault or child custody, may sometimes be interpreted as applying to situations involving circumcision. Some countries have placed restrictions on circumcision.
History.
There are ancient religious requirements for circumcision. The Hebrew Bible commands Jews to circumcise their male children on the eighth day of life, and to circumcise their male slaves ().
Laws banning circumcision are also ancient. The ancient Greeks prized the foreskin and disapproved of the Jewish custom of circumcision. 1 Maccabees, 1:60–61 states that King Antiochus IV of Syria, the occupying power of Judea in 170 BCE, outlawed circumcision on penalty of death. one of the grievances leading to the Maccabean Revolt.
According to the "Historia Augusta", the Roman emperor Hadrian issued a decree banning circumcision in the empire, and some modern scholars argue that this was a main cause of the Jewish Bar Kokhba revolt of 132 CE. The Roman historian Cassius Dio, however, made no mention of such a law, and blamed the Jewish uprising instead on Hadrian's decision to rebuild Jerusalem as Aelia Capitolina, a city dedicated to Jupiter.
Antoninus Pius permitted Jews to circumcise their own sons. However, he forbade the circumcision of non-Jews that were either foreign-slaves or non-Jewish members of the household, contrary to He also made it illegal for a man to convert to Judaism. Antoninus Pius exempted the Egyptian priesthood from the otherwise universal ban on circumcision.
Modern law.
Australia.
In 1993, a non-binding research paper of the Queensland Law Reform Commission ("Circumcision of Male Infants") concluded that "On a strict interpretation of the assault provisions of the Queensland Criminal Code, routine circumcision of a male infant could be regarded as a criminal act", and that doctors who perform circumcision on male infants may be liable to civil claims by that child at a later date. No prosecutions have occurred in Queensland, and circumcisions continue to be performed.
In a case of sexual assault in Queensland, Australia (1997), a district court awarded a man damages for nervous shock after a botched attempt to circumcise him with a broken beer bottle in a drunken attack. Making Australian legal history, the award was made against the assailant for unlawful wounding.
In 1999, a Perth man won A$360,000 in damages after a doctor admitted he botched a circumcision operation at birth which left the man with a badly deformed penis.
In 2002, Queensland police charged a father with grievous bodily harm for having his two sons, then aged nine and five, circumcised without the knowledge and against the wishes of the mother. The mother and father were in a family court dispute. The charges were dropped when the police prosecutor revealed that he did not have all family court paperwork in court and the magistrate refused to grant an adjournment.
Cosmetic circumcision for newborn males is currently banned in all Australian public hospitals, South Australia being the last state to adopt the ban in 2007; the procedure was not forbidden from being performed in private hospitals. In the same year, the Tasmanian President of the Australian Medical Association, Haydn Walters, stated that they would support a call to ban circumcision for non-medical, non-religious reasons. In 2009, the Tasmanian Law Reform Institute released its Issues Paper investigating the law relating to male circumcision in Tasmania, it "highlights the uncertainty in relation to whether doctors can legally perform circumcision on infant males".
The Tasmania Law Reform Institute released its recommendations for reform of Tasmanian law relative to male circumcision on 21 August 2012. The report makes fourteen recommendations for reform of Tasmanian law relative to male circumcision.
Bulgaria.
Male circumcision was very strongly discouraged in Bulgaria in the 1980s as part of attempts to pressure the country's Muslim minority but there was no actual legislation against the practice.
Canada.
According to the College of Physicians and Surgeons of British Columbia:
England and Wales.
Male circumcision has traditionally been presumed to be legal under British law, however some authors have argued that there is no solid foundation for this view in English law.
The passage of the Human Rights Act 1998 has led to some speculation that the lawfulness of the circumcision of male children is unclear.
One 1999 case, "Re "J" (child's religious upbringing and circumcision)" said that circumcision in Britain required the consent of all those with parental responsibility, or the permission of the court, acting for the best interests of the child, and issued an order prohibiting the circumcision of a male child of a non-practicing Muslim father and non-practicing Christian mother with custody. The reasoning included evidence that circumcision carried some medical risk; that the operation would be likely to weaken the relationship of the child with his mother, who strongly objected to circumcision without medical necessity; that the child may be subject to ridicule by his peers as the odd one out and that the operation might irreversibly reduce sexual pleasure, by permanently removing some sensory nerves, even though cosmetic foreskin restoration might be possible. The court did not rule out circumcision against the consent of one parent. It cited a hypothetical case of a Jewish mother and an agnostic father with a number of sons, all of whom, by agreement, had been circumcised as infants in accordance with Jewish laws; the parents then have another son who is born after they have separated; the mother wishes him to be circumcised like his brothers; the father refuses his agreement. In such a case, a decision in favor of circumcision was said to be likely.
In 2001 the General Medical Council had found a doctor who had botched circumcision operations guilty of abusing his professional position and that he had acted "inappropriately and irresponsibly", and struck him off the register. A doctor who had referred patients to him, and who had pressured a mother into agreeing to the surgery, was also condemned. He was put on an 18-month period of review and retraining, and was allowed to resume unrestricted practice as a doctor in March 2003, after a committee found that he had complied with conditions it placed on him. According to the "Northern Echo", he "told the committee he has now changed his approach to circumcision referrals, accepting that most cases can be treated without the need for surgery.".
Fox and Thomson (2005) argue that consent cannot be given for non-therapeutic circumcision. They say there is "no compelling legal authority for the common view that circumcision is lawful."
In 2005 a Muslim man had his son circumcised against the wishes of the child's mother who was the custodial parent. He was found not guilty of assault occasioning actual bodily harm by a majority verdict of the jury.
In 2009 it was reported that a 20-year-old man whose father had him ritually circumcised as a baby is preparing to sue the doctor who circumcised him. This is believed to be the first time a person who was circumcised as an infant has made a claim in the UK. The case is expected to be heard in 2010.
Europe.
On 1 October 2013, the Parliamentary Assembly of the Council of Europe adopted a resolution in which they state they are "particularly worried about a category of violation of the physical integrity of children," and include in this category "circumcision of young boys for religious reasons."
Finland.
In August 2006, a Finnish court ruled that the circumcision of a four-year-old boy arranged by his mother, who is Muslim, to be an illegal assault. The boy's father, who had not been consulted, reported the incident to the police. A local prosecutor stated that the prohibition of circumcision is not gender-specific in Finnish law. A lawyer for the Ministry of Social Affairs and Health stated that there is neither legislation nor prohibition on male circumcision, and that "the operations have been performed on the basis of common law." The case was appealed and in October 2008 the Finnish Supreme Court ruled that the circumcision, " carried out for religious and social reasons and in a medical manner, did not have the earmarks of a criminal offence. It pointed out in its ruling that the circumcision of Muslim boys is an established tradition and an integral part of the identity of Muslim men". In 2008, the Finnish government was reported to be considering a new law to legalise circumcision if the practitioner is a doctor and if the child consents. In December 2011, Helsinki District Court said that the Supreme Court's decision does not mean that circumcision is legal for any non-medical reasons. The court referred to the Convention on Human rights and Biomedicine of the Council of Europe, which was ratified in Finland in 2010.
In February 2010, a Jewish couple were fined for causing bodily harm to their then infant son who was circumcised in 2008 by a mohel brought in from the UK. Normal procedure for persons of Jewish faith in Finland is to have a locally certified mohel who works in Finnish healthcare perform the operation. In the 2008 case, the infant was not anesthetized and developed complications that required immediate hospital care. The parents were ordered to pay 1500 euros in damages to their child.
Germany.
In October 2006, a Turkish national who performed ritual circumcisions on seven boys was convicted of causing dangerous bodily harm by the state court in Düsseldorf.
In September 2007, a Frankfurt am Main appeals court found that the circumcision of an 11-year-old boy without his approval was an unlawful personal injury. The boy, whose parents were divorced, was visiting his Muslim father during a vacation when his father forced him to be ritually circumcised. The boy had planned to sue his father for .
In May 2012, the Cologne regional appellate court ruled that religious circumcision of male children amounts to bodily injury, and is a criminal offense in the area under its jurisdiction. The decision based on the article "Criminal Relevance of Circumcising Boys. A Contribution to the Limitation of Consent in Cases of Care for the Person of the Child" published by Holm Putzke, a German law professor at the University of Passau. The court arrived at its judgment by application of the human rights provisions of the Basic Law, a section of the Civil Code, and some sections of the Criminal Code to non-therapeutic circumcision of male children. Some observers said it could set a legal precedent that criminalizes the practice. Jewish and Muslim groups were outraged by the ruling, viewing it as trampling on freedom of religion.
The German ambassador to Israel, Andreas Michaelis, told Israeli lawmakers that Germany was working to resolve the issue and that it doesn't apply at a national level, but instead only to the local jurisdiction of the court in Cologne. The Council of the Coordination of Muslims in Germany condemned the ruling, stating that it is "a serious attack on religious freedom." Ali Kizilkaya, a spokesman of the council, stated that, "The ruling does not take everything into account, religious practice concerning circumcision of young Muslims and Jews has been carried out over the millenia on a global level." The Roman Catholic archbishop of Aachen, Heinrich Mussinghoff, said that the ruling was "very surprising", and the contradiction between "basic rights on freedom of religion and the well-being of the child brought up by the judges is not convincing in this very case." Hans Ulrich Anke, the head of the Protestant Church in Germany, said the ruling should be appealed since it didn't "sufficiently" consider the religious significance of the rite. A spokesman, Steffen Seibert, for German Chancellor Angela Merkel stated that Jewish and Muslim communities will be free to practice circumcision responsibly, and the government would find a way around the local ban in Cologne. The spokesman stated "For everyone in the government it is absolutely clear that we want to have Jewish and Muslim religious life in Germany. Circumcision carried out in a responsible manner must be possible in this country without punishment.".
In July, a group of rabbis, imams, and others said that they view the ruling against circumcision "an affront on our basic religious and human rights." The joint statement was signed by leaders of groups including Germany's Turkish-Islamic Union for Religious Affairs, the Islamic Center Brussels, the Rabbinical Centre of Europe, the European Jewish Parliament and the European Jewish Association, who met with members of European Parliament from Germany, Finland, Belgium, Italy, and Poland. European rabbis, who urged Jews to continue circumcision, planned further talks with Muslim and Christian leaders to determine how they can oppose the ban together. The Jewish Hospital of Berlin suspended the practice of male circumcision. On 19 July 2012, a joint resolution of the CDU/CSU, SPD and FDP factions in the Bundestag requesting the executive branch to draft a law permitting circumcision of boys to be performed without unnecessary pain in accordance with best medical practice carried with a broad majority.
The New York Times reported that the German Medical Association "condemned the ruling for potentially putting children at risk by taking the procedure out of the hands of doctors, but it also warned surgeons not to perform circumcisions for religious reasons until legal clarity was established." The ruling was supported by Deutsche Kinderhilfe, a German child rights organization, which asked for a two-year moratorium to discuss the issue and pointed out that religious circumcision may contravene the Convention on the Rights of the Child (Article 24.3: "States Parties shall take all effective and appropriate measures with a view to abolishing traditional practices prejudicial to the health of children.").
The German Academy for Pediatric and Adolescent Medicine (Deutsche Akademie für Kinder- und Jugendmedizin e.V., DAKJ), the German Association for Pediatric Surgery (Deutsche Gesellschaft für Kinderchirurgie, DGKCH) and the Professional Association of Pediatric and Adolescent Physicians (Berufsverband der Kinder- und Jugendärzte) took a firm stand against non-medical routine infant circumcision.
In July, in Berlin, a criminal complaint was lodged against Rabbi Yitshak Ehrenberg for "causing bodily harm" by performing religious circumcision, and for vocal support of the continuation of the practice. In September, the prosecutors dismissed the complaint, concluding that "there is no proof to establish that the rabbi's conduct met the 'condition of a criminal' violation."
In September, Reuters reported "Berlin's senate said doctors could legally circumcise infant boys for religious reasons in its region, given certain conditions."
On 12 December 2012, following a series of hearings and consultations, the Bundestag adopted the proposed law explicitly permitting non-therapeutic circumcision to be performed under certain conditions; it is now §1631(d) in the German Civil Code. The vote tally was 434 ayes, 100 noes, and 46 abstentions. Following approval by the Bundesrat and signing by the Bundespräsident, the new law became effective on 28 December 2012 a day after its publication in the Federal Gazette.
Ireland.
In October 2005 a Nigerian man was cleared of a charge of reckless endangerment over the death of a baby from haemorrhage and shock after he had circumcised the child. The judge directed the jury not to "bring what he called their white western values to bear when they were deciding this case" and after deliberating for an hour and a half they found the defendant not guilty.
Israel.
In Israel, Jewish circumcision is entirely legal, as is posthumous circumcision. In 1999, the Israeli Supreme Court overruled an attempt to have after-death circumcision outlawed. Though illegal, female circumcision is still practiced among the Negev Bedouin, and tribal secrecy among the Bedouin makes it difficult for authorities to enforce the ban. In 2013, Rabbinical court in Israel ordered a mother, Elinor Daniel, to circumcise her son or pay a fine of 500 Israeli Shekel for every day that the child is not circumcised. She appealed against the Rabbinical court ruling and the High Court ruled in her favour and levied the fine.
Netherlands.
When Ayaan Hirsi Ali was a Member of the Netherlands Parliament she asked it to consider making the circumcision of male children unlawful. In May 2008 a father who had his two sons, aged 3 and 6 circumcised against the will of their mother was found not guilty of causing them serious physical harm but was given a 6-week suspended jail sentence for taking the boys away from their mother against her will.
Norway and region.
In 2012, the Senterpartiet proposed a ban on circumcision on males under eighteen.
In September 2013, the Children's ombudsmen in all Nordic countries issued a statement by which they called for a ban on circumcision of minors for non-medical reasons, stating that such circumcisions violate the rights of children after the Convention on the Rights of the Child to co-determination and protection from harmful traditions.
South Africa.
The Children's Act 2005 makes the circumcision of male children under 16 unlawful except for religious or medical reasons. In the Eastern Cape province the Application of Health Standards in Traditional Circumcision Act, 2001, regulates traditional circumcision, which causes the death or mutilation of many youths by traditional surgeons each year. Among other provisions, the minimum age for circumcision is age 18.
In 2004, a 22-year-old Rastafarian convert was forcibly circumcised by a group of Xhosa tribal elders and relatives. When he first fled, two police returned him to those who had circumcised him. In another case, a medically circumcised Xhosa man was forcibly recircumcised by his father and community leaders. He laid a charge of unfair discrimination on the grounds of his religious beliefs, seeking an apology from his father and the Congress of Traditional Leaders of South Africa. According to South African newspapers, the subsequent trial became "a landmark case around forced circumcision." In October 2009, the Eastern Cape High Court at Bhisho (sitting as an Equality Court) clarified that circumcision is unlawful unless done with the full consent of the initiate.
Sweden.
In 2001, the Parliament of Sweden enacted a law allowing only persons certified by the National Board of Health to circumcise infants. It requires a medical doctor or an anesthesia nurse to accompany the circumciser and for anaesthetic to be applied beforehand. After the first two months of life circumcisions can only be performed by a physician. The stated purpose of the law was to increase the safety of the procedure.
Swedish Jews and Muslims objected to the law, and in 2001, the World Jewish Congress called it "the first legal restriction on Jewish religious practice in Europe since the Nazi era." The requirement for an anaesthetic to be administered by a medical professional is a major issue, and the low degree of availability of certified professionals willing to conduct circumcision has also been subject to criticism. According to a survey, two out of three paediatric surgeons said they refuse to perform non-therapeutic circumcision, and less than half of all county councils offer it in their hospitals. However, in 2006, the U.S. State Department stated, in a report on Sweden, that most Jewish mohels had been certified under the law and 3000 Muslim and 40–50 Jewish boys were circumcised each
year. An estimated 2000 of these are performed by persons who are neither physicians nor have officially recognised certification.
The Swedish National Board of Health and Welfare reviewed the law in 2005 and recommended that it be maintained, but found that the law had failed with regard to the intended consequence of increasing the safety of circumcisions. A later report by the Board criticised the low level of availability of legal circumcisions, partly due to reluctance among health professionals. To remedy this, the report suggested a new law obliging all county councils to offer non-therapeutic circumcision in their hospitals, but this was later abandoned in favour of a non-binding recommendation.
United States.
Circumcision of adults who grant personal informed consent for the surgical operation is legal. 
In the United States, non-therapeutic circumcision of male children has long been assumed to be lawful in every jurisdiction provided that one parent grants surrogate informed consent. Adler (2013) has recently challenged the validity of this assumption. As with every country, doctors who circumcise children must take care that all applicable rules regarding informed consent and safety are satisfied.
While anti-circumcision groups have occasionally proposed legislation banning non-therapeutic child circumcision, it has not been supported in any legislature. After a failed attempt to adopt a local ordinance banning circumcision on a San Francisco ballot, the state of California enacted in October 2011 a law protecting circumcision from local attempts to ban the practice.
In 2012, New York City required those performing "metzitzah b'peh", a part of circumcision required by some Hasidim, to obey stringent consent requirements, including documentation. Agudath Israel of America and other Jewish groups have planned to sue the city in response.
Disputes between parents
Occasionally the courts are asked to make a ruling when parents cannot agree on whether or not to circumcise a child.
In January 2001 a dispute between divorcing parents in New Jersey was resolved when the mother, who sought to have the boy circumcised withdrew her request. The boy had experienced two instances of foreskin inflammation and she wanted to have him circumcised. The father, who had experienced a traumatic circumcision as a child objected and they turned to the courts for a decision. The Medical Society of New Jersey and the Urological Society of New Jersey both opposed any court ordered medical treatment. As the parties came to an agreement, no precedent was set. In June 2001 a Nevada court settled a dispute over circumcision between two parents but put a strict gag order on the terms of the settlement. In July 2001 a dispute between parents in Kansas over circumcision was resolved when the mother's request to have the infant circumcised was withdrawn. In this case the father opposed circumcision while the mother asserted that not circumcising the child was against her religious beliefs. (The woman's pastor had stated that circumcision was "important" but was not necessary for salvation.) On 24 July 2001 the parents reached agreement that the infant would not be circumcised.
On 14 July 2004 a mother appealed to the Missouri Supreme Court to prevent the circumcision of her son after a county court and the Court of Appeals had denied her a writ of prohibition. However, in early August 2004, before the Supreme Court had given its ruling, the father, who had custody of the boy, had him circumcised.
In October 2006 a judge in Chicago granted an injunction blocking the circumcision of a 9-year-old boy. In granting the injunction the judge stated that "the boy could decide for himself whether to be circumcised when he turns 18."
In November 2007, the Oregon Supreme Court heard arguments from a divorced Oregon couple over the circumcision of their son. The father wanted his son, who turned 13 on 2 March 2008, to be circumcised in accordance with the father's religious views; the child's mother opposes the procedure. The parents dispute whether the boy is in favor of the procedure. A group opposed to circumcision filed briefs in support of the mother's position, while some Jewish groups filed a brief in support of the father. On 25 January 2008, the Court returned the case to the trial court with instructions to determine whether the child agrees or objects to the proposed circumcision. The father appealed to the US Supreme Court to allow him to have his son circumcised but his appeal was rejected. The case then returned to the trial court. When the trial court interviewed the couple's son, now 14 years old, the boy stated that he did not want to be circumcised. This also provided the necessary circumstances to allow the boy to change residence to live with his mother. The boy was not circumcised.
Other disputes
In September 2004 the North Dakota Supreme Court rejected a mother's attempt to prosecute her doctor for circumcising her child without fully informing her of the consequences of the procedure. The judge and jury found that the defendants were adequately informed of possible complications, and the jury further found that it is not incumbent on the doctors to describe every "insignificant" risk.
In March 2009 a Fulton County, Ga., State Court jury awarded $2.3 million in damages to a 4-year-old boy and his mother for a botched circumcision in which too much tissue was removed causing permanent disfigurement.
In August 2010 an eight-day-old boy was circumcised in a Florida hospital against the stated wishes of the parents. The hospital admitted that the boy was circumcised by mistake; the mother has sued the hospital and the doctor involved in the case.
USSR.
Before glasnost, according to an article in The Jewish Press, Jewish ritual circumcision was forbidden in the USSR. However, David E. Fishman, professor of Jewish History at the Jewish Theological Seminary of America, states that, whereas the "heder" and "yeshiva", the organs of Jewish education, "were banned by virtue of the law separating church and school, and subjected to tough police and administrative actions," circumcision was not proscribed by law or suppressed by executive measures.
Jehoshua A. Gilboa writes that while circumcision was not officially or explicitly banned, pressure was exerted to make it difficult. "Mohels" in particular were concerned that they could be punished for any health issue that might develop, even if it arose some time after the circumcision.

</doc>
<doc id="7449" url="https://en.wikipedia.org/wiki?curid=7449" title="Called to Common Mission">
Called to Common Mission

Called to Common Mission is an agreement between The Episcopal Church and the Evangelical Lutheran Church in America (ELCA), establishing full communion between them. It was ratified by the ELCA in 1999, the ECUSA in 2000, after the narrow failure of a previous agreement. Its principal author on the Episcopal side was the Rev. Canon J. Robert Wright. Under the agreement, they recognize the validity of each other's baptisms and ordinations. The agreement provided that the ELCA would accept the historical episcopate, something which became controversial in the ELCA. In response to concerns about the meaning of CCM, bishops in the ELCA drafted Tucson Resolution, which presented the official ELCA position.
Some within the ELCA argued that requiring the historic episcopate would contradict the traditional Lutheran doctrine that the church exists wherever the Word is preached and Sacraments are practiced. Others objected on the grounds that adopting the Episcopalian priesthood and hierarchical structure was contrary to the Lutheran concept of the priesthood of all believers, which holds that all Christians stand on equal footing before God. They argued that the Old Covenant required a priest to mediate between God and humanity, but that New Covenant explicitly abolishes the need for priestly role by making every Christian a priest with direct access to God's grace. Still others objected because of the implied directive that lay presidency would be abolished. This was a particularly issue for rural congregations that periodically "called" a congregation member to conduct communion services in the absence of ordained clergy.

</doc>
<doc id="7450" url="https://en.wikipedia.org/wiki?curid=7450" title="Context menu">
Context menu

A context menu (also called contextual, shortcut, and popup or pop-up menu) is a menu in a graphical user interface (GUI) that appears upon user interaction, such as a right-click mouse operation. A context menu offers a limited set of choices that are available in the current state, or context, of the operating system or application. Usually the available choices are actions related to the selected object. From a technical point of view, such a context menu is a graphical control element.
History.
Context menus first appeared in the Smalltalk environment on the Xerox Alto computer, where they were called "pop-up menus." The NEXTSTEP operating system further developed the idea, incorporating a feature whereby the right or middle mouse button brought the main menu (which was vertical and automatically changed depending on context) to the location of the mouse, thereby eliminating the need to move the mouse pointer all the way across the large (for the time) NextStep screen.
Implementation.
Context menus are opened via various forms of user interaction that target a region of the GUI that supports context menus. The specific form of user interaction and the means by which a region is targeted vary:
Windows mouse click behavior is such that the context menu doesn't open while the mouse button is pressed, but only opens the menu when the button is released, so the user has to click again (this time with the first mouse button) to select a context menu item. This behavior differs from Mac OS X, and most Linux distributions.
Context menus are sometimes hierarchically organized, allowing navigation through different levels of the menu structure. The implementations differ: Microsoft Word was one of the first applications to only show sub-entries of some menu entries after clicking an arrow icon on the context menu, otherwise executing an action associated with the parent entry. This makes it possible to quickly repeat an action with the parameters of the previous execution, and to better separate options from actions.
X Window Managers.
The following window managers provide context menu functionality:
Usability.
Context menus have received some criticism from usability analysts when improperly used, as some applications make certain features "only" available in context menus, which may confuse even experienced users (especially when the context menus can only be activated in a limited area of the application's client window).
Context menus usually open in a fixed position under the pointer, but when the pointer is near a screen edge the menu will be displaced - thus reducing consistency and impeding use of muscle memory. If the context menu is being triggered by keyboard, such as by using Shift + F10, the context menu appears near the focused widget instead of the position of the pointer, to save recognition efforts.
In documentation.
Microsoft's guidelines call for always using the term "context menu", and explicitly deprecate "shortcut menu".

</doc>
<doc id="7451" url="https://en.wikipedia.org/wiki?curid=7451" title="Jews as the chosen people">
Jews as the chosen people

In Judaism, "chosenness" is the belief that the Jews, via descent from the ancient Israelites, are the chosen people, i.e. chosen to be in a covenant with God. The idea of the Israelites being chosen by God is found most directly in the Book of Deuteronomy as the verb "bahar" (), and is alluded to elsewhere in the Hebrew Bible using other terms such as "holy people". Much is written about these topics in rabbinic literature. The three largest Jewish denominations— Orthodox Judaism, Conservative Judaism and Reform Judaism—maintain the belief that the Jews have been chosen by God for a purpose. Sometimes this choice is seen as charging the Jewish people with a specific mission — to be a light unto the nations, and to exemplify the covenant with God as described in the Torah.
This view, however, did not preclude a belief that God has a relationship with other peoples — rather, Judaism held that God had entered into a covenant with all humankind, and that Jews and non-Jews alike have a relationship with God. Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh" (), and the Tanakh (Hebrew Bible) also identifies prophets outside the community of Israel. Based on these statements, some rabbis theorized that, in the words of Nethanel ibn Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others... God sends a prophet to every people according to their own language."(Levine, 1907/1966) The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other." (Mishnah Sanhedrin 4:5) The Mishnah continues, and states that anyone who kills or saves a single human, not Jewish, life, has done the same (save or kill) to an entire world. The Tosefta, a collection of important post-Talmudic discourses, also states: "Righteous people of all nations have a share in the world to come" (Sanhedrin 105a).
According to the Israel Democracy Institute, approximately two thirds of Israeli Jews believe that Jews are the "chosen people".
In the Bible.
According to the traditional Jewish interpretation of the Bible, Israel's character as the chosen people is unconditional as it says in , 
The Torah also says, 
God promises that he will never exchange his people with any other:
Other Torah verses about chosenness, 
The obligation imposed upon the Israelites was emphasized by the prophet Amos (): 
Rabbinic views.
Sometimes this choice is seen as charging the Jewish people with a specific mission — to be a light unto the nations, and to exemplify the covenant with God as described in the Torah. This view, however, did not preclude a belief that God has a relationship with other peoples — rather, Judaism held that God had entered into a covenant with all humankind, and that Jews and non-Jews alike have a relationship with God.
Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh" (), and the Tanakh (Hebrew Bible) also identifies prophets outside the community of Israel. Based on these statements, some rabbis theorized that, in the words of Nethanel ibn Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others... God sends a prophet to every people according to their own language."(Levine, 1907/1966) The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other." (Mishnah Sanhedrin 4:5) The Mishnah continues, and states that anyone who kills or saves a single human, not Jewish, life, has done the same (save or kill) to an entire world. The Tosefta, a collection of important post-Talmudic discourses, also states: "Righteous people of all nations have a share in the world to come" (Sanhedrin 105a).
The idea of chosenness has traditionally been interpreted by Jews in two ways: one way is that God chose the Israelites, while the other is that the Israelites chose God. Although collectively this choice was made freely, religious Jews believe that it created individual obligation for the descendants of the Israelites. Another opinion is that the choice was free in a limited context, thus: although the Jews chose to follow precepts ordained by God, the Kabbalah and Tanya teach that even prior to creation, the "Jewish soul" was already chosen.
Crucial to the Jewish notion of chosenness is that it creates obligations exclusive to Jews, while non-Jews receive from God other covenants and other responsibilities. Generally, it does not entail exclusive rewards for Jews. Classical rabbinic literature in the Mishnah Avot 3:14 has this teaching:
Rabbi Akiva used to say, "Beloved is man, for he was created in God's image; and the fact that God made it known that man was created in His image is indicative of an even greater love. As the verse states [], 'In the image of God, man was created.'" The mishna goes on to say, "Beloved are the people Israel, for they are called children of God; it is even a greater love that it was made known to them that they are called children of God, as it said, 'You are the children of the Lord, your God. Beloved are the people Israel, for a precious article Torah was given to them ...
Most Jewish texts do not state that "God chose the Jews" by itself. Rather, this is usually linked with a mission or purpose, such as proclaiming God's message among all the nations, even though Jews cannot become "unchosen" if they shirk their mission. This implies a special duty, which evolves from the belief that Jews have been pledged by the covenant which God concluded with the biblical patriarch Abraham, their ancestor, and again with the entire Jewish nation at Mount Sinai. In this view, Jews are charged with living a holy life as God's priest-people.
In the Jewish prayerbook (the Siddur), chosenness is referred to in a number of ways. The blessing for reading the Torah reads
In the "Kiddush", a prayer of sanctification, in which the Sabbath is inaugurated over a cup of wine, the text reads, 
In the "Kiddush" recited on festivals it says, 
The Aleinu prayer refers to the concept of Jews as a chosen people:
An earlier form of this prayer, in use during the medieval era, contained an extra sentence:
It is our duty to praise the Master of all, to exalt the Creator of the Universe, who has not made us like the nations of the world and has not placed us like the families of the earth; who has not designed our destiny to be like theirs, nor our lot like that of all their multitude, "who worship mist and emptiness and pray to a god who cannot save."
This sentence in italics is an allusion to the Bible, Isaiah (). 
In the medieval era some within the Christian community came to believe that this line referred to Christians worshipping Jesus; they demanded that it be excised. Ismar Elbogen, a historian of the Jewish liturgy, held that the early form of the prayer pre-dated Christianity, and could not possibly have referred to it.
Further interpretations.
According to the Rabbis, "Israel is of all nations the most willful or headstrong one, and the Torah was to give it the right scope and power of resistance, or else the world could not have withstood its fierceness."
"The Lord offered the Law to all nations; but all refused to accept it except Israel."
How do we understand "A Gentile who consecrates his life to the study and observance of the Law ranks as high as the high priest", says R. Meïr, by deduction from Lev. xviii. 5; II Sam. vii. 19; Isa. xxvi. 2; Ps. xxxiii. 1, cxviii. 20, cxxv. 4, where all stress is laid not on Israel, but on man or the righteous one.
The Gemara states this regarding a non-Jew who studies Torah 7 mitzvot and regarding this, see Shita Mekubetzes, Bava Kama 38a who says that this is an exaggeration. In any case, this statement was not extolling the non-Jew. The Rishonim explain that it is extolling the Torah.
Tosfos explains that it uses the example of a "kohen gadol" (high priest), because this statement is based on the verse, ""y'kara hi mipnimim"" (it is more precious than pearls). This is explained elsewhere in the Gemara to mean that the Torah is more precious "pnimim" (translated here as "inside" instead of as "pearls"; thus that the Torah is introspectively absorbed into the person), which refers to "lifnai v'lifnim" (translated as "the most inner of places"), that is the Holy of Holies where the "kahon gadol" went.
In any case, in Midrash Rabba (Bamidbar 13:15) this statement is made with an important addition: a non-Jew who converts and studies Torah etc.
The Nation of Israel is likened to the olive. Just as this fruit yields its precious oil only after being much pressed and squeezed, so Israel's destiny is one of great oppression and hardship, in order that it may thereby give forth its illuminating wisdom. Poverty is the quality most befitting Israel as the chosen people (Ḥag. 9b). Only on account of its good works is Israel among the nations "as the lily among thorns", or "as wheat among the chaff."
Chassidism.
Bal Shem Tov considered important every Jewish person. Chassidut teaches that God loves all Israel people.
Modern Orthodox views.
Rabbi Lord Immanuel Jakobovits, former Chief Rabbi of the United Synagogue of Great Britain (Modern Orthodox Judaism), describes chosenness in this way:
Yes, I do believe that the chosen people concept as affirmed by Judaism in its holy writ, its prayers, and its millennial tradition. In fact, I believe that every people—and indeed, in a more limited way, every individual—is "chosen" or destined for some distinct purpose in advancing the designs of Providence. Only, some fulfill their mission and others do not. Maybe the Greeks were chosen for their unique contributions to art and philosophy, the Romans for their pioneering services in law and government, the British for bringing parliamentary rule into the world, and the Americans for piloting democracy in a pluralistic society. The Jews were chosen by God to be 'peculiar unto Me' as the pioneers of religion and morality; that was and is their national purpose.
Rabbi Norman Lamm, a leader of Modern Orthodox Judaism writes:
Conservative views.
Conservative Judaism, views the concept of chosenness in this way:
Rabbi Reuven Hammer comments on the excised sentence in the Aleinu prayer mentioned above:
Reform views.
Reform Judaism views the concept of chosenness in this way:
In 1999 the Reform movement stated:
Alternative views.
Equality of souls.
Many Kabbalistic sources, notably the Tanya, contain statements to the effect that the Jewish soul is qualitatively different from the non-Jewish soul. Some prominent Kabbalists rejected this idea and believed in essential equality of all human souls. Menahem Azariah da Fano, in his book "Reincarnations of souls", provides many examples of non-Jewish Biblical figures being reincarnated as Jews, and vice versa. Abraham Cohen de Herrera, another Kabbalist of the same school, quotes Greek, Christian and various Oriental mystics and philosophers without hesitation, and does not mention anything specific about the Jewish souls.
A number of known Chabad rabbis offered alternative readings of the Tanya, did not take this teaching literally, and even managed to reconcile it with the leftist ideas of internationalism and class struggle. The original text of the Tanya refers to the "idol worshippers" and does not mention the "nations of the world" at all, although such interpretation was endorsed by Menachem Mendel Schneerson and is popular in contemporary Chabad circles. Hillel of Parich, an early Tanya commentator, wrote that the souls of righteous Gentiles are more similar to the Jewish souls, and are generally good and not egoistic. This teaching was accepted by Schneerson and is considered normative in Chabad.
Different in character but not value.
According to the author of the Tanya himself, a righteous non-Jew can achieve a high level of spiritually, similar to an angel, although his soul is still fundamentally different in character, but not value, from a Jewish one. Tzemach Tzedek, the third rebbe of Chabad, wrote that the Muslims are naturally good-hearted people. Rabbi Yosef Jacobson, a popular contemporary Chabad lecturer, teaches that in today's world most non-Jews belong to the category of righteous Gentiles, effectively rendering the Tanya's attitude anachronistic.
Dov Ber Pinson, a contemporary Chabad mystic, denies the idea that there is any essential difference between the Jews and non-Jews. According to his theory, every person has a lower animalistic and higher Godly soul. The Tanya does not talk about Jews and non-Jews as social groups, but describes the internal struggle between the materialistic "Gentile" and spiritual "Jewish" levels of consciousness within every human soul.
Altruism.
An anti-Zionist interpretation of Tanya was offered by Abraham Yehudah Khein, a prominent Ukrainian Chabad rabbi, who supported anarchist communism and considered Peter Kropotkin a great Tzaddik. Khein basically read the Tanya backwards; since the souls of idol worshipers are known to be evil, according to the Tanya, while the Jewish souls are known to be good, he concluded that truly altruistic people are really Jewish, in a spiritual sense, while Jewish nationalists and class oppressors are not. By this logic, he claimed that Vladimir Solovyov and Rabindranath Tagore probably have Jewish souls, while Leon Trotsky and other totalitarians do not, and many Zionists, whom he compared to apes, are merely "Jewish by birth certificate".
Righteous non-Jews.
Nachman of Breslov also believed that Jewishness is a level of consciousness, and not an intrinsic inborn quality. He wrote that, according to the Book of Malachi, one can find "potential Jews" among all nations, whose souls are illuminated by the leap of "holy faith", which "activated" the Jewishness in their soul. These people would otherwise convert to Judaism, but prefer not to do so. Instead, they recognize the Divine unity within their pagan religions.
Isaac Arama, an influential philosopher and mystic of the 15th century, believed that righteous non-Jews are spiritually identical to the righteous Jews. Rabbi Menachem Meiri, a famous Catalan Talmudic commentator and Maimonidian philosopher, considered all people, who sincerely profess an ethical religion, to be part of a greater "spiritual Israel". He explicitly included Christian and Muslims in this category. Meiri rejected all Talmudic laws that discriminate between the Jews and non-Jews, claiming that they only apply to the ancient idolators, who had no sense of morality. The only exception are a few laws related directly or indirectly to intermarriage, which Meiri did recognize.
Meiri applied his idea of "spiritual Israel" to the Talmudic statements about unique qualities of the Jewish people. For example, he believed that the famous saying that Israel is above astrological predestination ("Ein Mazal le-Israel") also applied to the followers of other ethical faiths. He also considered countries, inhabited by decent moral non-Jews, such as Languedoc, as a spiritual part of the Holy Land.
Spinoza.
One Jewish critic of chosenness was the philosopher Baruch Spinoza. In the third chapter of his "Theologico-Political Treatise", Spinoza mounts an argument against a naive interpretation of God's choice of the Jews. Bringing evidence from the Bible itself, he argues that God's choice of Israel was not unique (he had chosen other nations before choosing the Hebrew nation) and that the choice of the Jews is neither inclusive (it does not include all of the Jews, but only the 'pious' ones) nor exclusive (it also includes 'true gentile prophets'). Finally, he argues that God's choice is not unconditional. Recalling the numerous times God threatened the complete destruction of the Hebrew nation, he asserts that this choice is neither absolute, nor eternal, nor necessary. Moreover, in aphorism 12 he writes, "Thus the Jews today have absolutely nothing that they can attribute to themselves but not to other peoples..."
Reconstructionist criticism.
Reconstructionist Judaism rejects the concept of chosenness. Its founder, Rabbi Mordecai Kaplan, said that the idea that God chose the Jewish people leads to racist beliefs among Jews, and thus must be excised from Jewish theology. This rejection of chosenness is made explicit in the movement's siddurim (prayer books).
For example, the original blessing recited before reading from the Torah contains the phrase, "asher bahar banu mikol ha’amim"—"Praised are you Lord our God, ruler of the Universe, "who has chosen us from among all peoples" by giving us the Torah." The Reconstructionist version is rewritten as "asher kervanu la’avodato", "Praised are you Lord our God, ruler of the Universe, "who has drawn us to your service" by giving us the Torah."
In the mid-1980s, the Reconstructionist movement issued its "Platform on Reconstructionism". It states that the idea of chosenness is "morally untenable", because anyone who has such beliefs "implies the superiority of the elect community and the rejection of others."
Not all Reconstructionists accept this view. The newest siddur of the movement, "Kol Haneshamah", includes the traditional blessings as an option, and some modern Reconstructionist writers have opined that the traditional formulation is not racist, and should be embraced.
An original prayer book, by Reconstructionist feminist poet Marcia Falk, "The Book of Blessings", has been widely accepted by both Reform and Reconstructionist Jews. Falk rejects all concepts relating to hierarchy or distinction; she sees any distinction as leading to the acceptance of other kinds of distinctions, thus leading to prejudice. She writes that as a politically liberal feminist, she must reject distinctions made between men and women, homosexuals and heterosexuals, Jews and non-Jews, and to some extent even distinctions between the Sabbath and the other six days of the week. She thus rejects idea of chosenness as unethical. She also rejects Jewish theology in general, and instead holds to a form of religious humanism. Falk writes:
Reconstructionist author Judith Plaskow also criticises the idea of chosenness, for many of the same reasons as Falk. A politically liberal lesbian, Plaskow rejects most distinctions made between men and women, homosexuals and heterosexuals, and Jews and non-Jews. In contrast to Falk, Plaskow does not reject all concepts of difference as inherently leading to unethical beliefs, and holds to a more classical form of Jewish theism than Falk.
A number of responses to these views have been made by Reform and Conservative Jews; they hold that these criticisms are against teachings that do not exist within liberal forms of Judaism, and which are rare in Orthodox Judaism (outside certain Haredi communities, such as Chabad). A separate criticism stems from the very existence of feminist forms of Judaism in all denominations of Judaism, which do not have a problem with the concepts of chosenness.
Views from other religions.
Islam.
The children of Israel enjoy a special status in the Islamic book, the Quran:
O children of Israel, remember my favor which I bestowed upon you, and that I favored you above all creation. (Qur'an 2:47). 2:122).
However, Muslim scholars point out that this status did not confer upon Israelites any racial superiority, and was only valid so long as the Israelites maintain their covenant with God,
Indeed God had taken the covenant from the Children of Israel, and We appointed twelve leaders among them. And God said: "I am with you if you establish the prayer and offer the Zakat (compulsory charity) and believe in My Messengers; honor and assist them, and lend to God a good loan. Verily, I will remit your sins and admit you to Gardens under which rivers flow (in Paradise). But if any of you after this, disbelieve, he has indeed gone astray from the Straight Path." (Quran 5:12)
Christianity.
Some Christians believe that the Jews were God's chosen people (), but because of Jewish Rejection of Jesus, the Christians in turn received that special status (). This doctrine is known as Supersessionism.
However, most other Christians are of the view that all people who turn to Christ as their personal saviour are 'chosen' in the context of John 15:16 whereby Jesus referred to God's plan of salvation as his great redeeming work on the cross, that all who come to faith in him does so freely and are 'chosen' to bear 'fruit that lasts'. 1 Peter 2:9 refers to these (Christians) as 'chosen people, a royal priesthood, a holy nation, God's special possession' .
Influence on relations with other religions.
Avi Beker, an Israeli scholar and former Secretary General of the World Jewish Congress, regarded the idea of the chosen people as Judaism's defining concept and "the central unspoken psychological, historical, and theological problem at the heart of Jewish-Gentile relations." In his book "The Chosen: The History of an Idea, and the Anatomy of an Obsession", Beker views the concept of chosenness as the driving force behind Jewish-Gentile relations, explaining both the admiration and, more pointedly, the envy and hatred the world has felt for the Jews in religious and also secular terms. Beker argues that while Christianity has modified its doctrine on the displacement of the Jews, Islam has neither reversed nor reformed its theology concerning the succession of both the Jews and the Christians. According to Beker, this presents a major barrier to conflict resolution in the Arab-Israeli conflict.
Ethnocentrism.
Israeli philosopher Ze’ev Levy writes that chosenness can be "(partially) justified only from the historical angle" with respect to its spiritual and moral contribution to Jewish life thorough centuries, "a powerful agent of consolation and hope". He points out however that modern anthropological theories "do not merely proclaim the inherent universal equality of all people human beings; they also stress the "equivalence" of all human cultures." (emphasis in original) He continues that "there are no inferior and superior people or cultures but only different, "other", ones." He concludes that the concept of chosenness entails ethnocentrism, "which does not go hand in hand with otherness, that is, with unconditional respect of otherness".
Some people have claimed that Judaism's chosen people concept is racist because it implies that Jews are superior to non-Jews. The Anti-Defamation League, and other authorities, assert that the concept of a chosen people within Judaism has nothing to do with racial superiority, but rather is a description of the special relationship between God and Jews.

</doc>
<doc id="7453" url="https://en.wikipedia.org/wiki?curid=7453" title="Christian persecution">
Christian persecution

Christian persecution could refer to:
More generally, see:

</doc>
<doc id="7455" url="https://en.wikipedia.org/wiki?curid=7455" title="Chaparral">
Chaparral

Chaparral is a shrubland or heathland plant community found primarily in the U.S. state of California and in the northern portion of the Baja California Peninsula, Mexico. It is shaped by a Mediterranean climate (mild, wet winters and hot dry summers) and wildfire, featuring summer-drought-tolerant plants with hard sclerophyllous evergreen leaves, as contrasted with the associated soft-leaved, drought-deciduous, scrub community of coastal sage scrub, found below the chaparral biome. Chaparral covers 5 percent of the state of California, and associated Mediterranean shrubland an additional 3.5 percent. The name comes from the Spanish word for scrub oak, "chaparro".
Introduction.
In its natural state, chaparral is characterized by infrequent fires, with intervals ranging between 10–15 years to over a hundred years. Mature chaparral (stands that have been allowed greater intervals between fires) is characterized by nearly impenetrable, dense thickets (except the more open chaparral of the desert). These plants are highly flammable. They grow as woody shrubs with hard and small leaves, are non-leaf-dropping (non-deciduous), and are drought-tolerant. After the first rains following a fire, the landscape is dominated by soft-leaved non-woody annual plants, known as fire followers, which die back with the summer dry period.
Similar plant communities are found in the four other Mediterranean climate regions around the world, including the Mediterranean Basin (where it is known as maquis), central Chile (where it is called matorral), the South African Cape Region (known there as fynbos), and in Western and Southern Australia (as kwongan). According to the California Academy of Sciences, Mediterranean shrubland contains more than 20 percent of the world's plant diversity. The word "chaparral" is a loan word from Spanish "chaparro", meaning both "small" and "dwarf" evergreen oak, which itself comes from the Basque word "txapar", with exactly the same meaning.
Conservation International and other conservation organizations consider the chaparral to be a biodiversity hotspot - a biological community with a large number of different species - that is under threat by human activity.
California chaparral.
California chaparral and woodlands ecoregion.
The California chaparral and woodlands ecoregion, of the Mediterranean forests, woodlands, and scrub biome, has three sub-ecoregions with ecosystem—plant community subdivisions:
Chaparral and woodlands biota.
For the numerous individual plant and animal species found within the California chaparral and woodlands ecoregion, see:
Some of the indicator plants of the California chaparral and woodlands ecoregion include:
California cismontane and transmontane chaparral subdivisions.
Another phytogeography system uses two California chaparral and woodlands subdivisions: the cismontane chaparral and the transmontane (desert) chaparral.
California cismontane chaparral.
Cismontane chaparral ("this side of the mountain") refers to the chaparral ecosystem in the Mediterranean forests, woodlands, and scrub biome in California, growing on the western (and coastal) sides of large mountain range systems, such as the western slopes of the Sierra Nevada in the San Joaquin Valley foothills, western slopes of the Peninsular Ranges and California Coast Ranges, and south-southwest slopes of the Transverse Ranges in the Central Coast and Southern California regions.
Cismontane chaparral plant species.
In Central and Southern California chaparral forms a dominant habitat. Members of the chaparral biota native to California, all of which tend to regrow quickly after fires, include:
Cismontane chaparral bird species.
The complex ecology of chaparral habitats supports a very large number of animal species. The following is a short list of birds which are an integral part of the cismontane chaparral ecosystems.
California transmontane (desert) chaparral.
Transmontane chaparral or desert chaparral — "transmontane" ("the other side of the mountain") "chaparral" — refers to the desert shrubland habitat and chaparral plant community growing in the rainshadow of these ranges. Transmontane chaparral features xeric desert climate, not Mediterranean climate habitats, and is also referred to as desert chaparral. Desert chaparral is a regional ecosystem subset of the deserts and xeric shrublands biome, with some plant species from the California chaparral and woodlands ecoregion. Unlike cismontane chaparral, which forms dense, impenetrable stands of plants, desert chaparral is open, with only about 50 percent of the ground covered. Individual shrubs can reach up to in height.
Transmontane chaparral or desert chaparral is found on the eastern slopes of major mountain range systems on the western sides of the deserts of California. The mountain systems include the southeastern Transverse Ranges (the San Bernardino and San Gabriel Mountains) in the Mojave Desert north and northeast of the Los Angeles basin and Inland Empire; and the northern Peninsular Ranges (San Jacinto, Santa Rosa, and Laguna Mountains), which separate the Colorado Desert (western Sonoran Desert) from lower coastal Southern California. It is distinguished from the cismontane chaparral found on the coastal side of the mountains, which experiences higher winter rainfall. Naturally, desert chaparral experiences less winter rainfall than cismontane chaparral. Plants in this community are characterized by small, hard (sclerophyllic) evergreen (non-dropping; non-deciduous) leaves. Desert chaparral grows above California's desert cactus scrub plant community and below the pinyon-juniper woodland. It is further distinguished from the deciduous sub-alpine scrub above the pinyon-juniper woodlands on the same side of the Peninsular ranges.
Transmontane chaparral distribution.
Transmontane (desert) chaparral typically grows on the lower ( elevation) northern slopes of the southern Transverse Ranges (running east to west in San Bernardino and Los Angeles counties) and on the lower () eastern slopes of the Peninsular Ranges (running south to north from lower Baja California to Riverside and Orange counties and the Transverse Ranges). It can also be found in higher-elevation sky islands in the interior of the deserts, such as in the upper New York Mountains within the Mojave National Preserve in the Mojave Desert.
The California transmontane (desert) chaparral is found in the rain shadow deserts of the:
Transmontane chaparral animals.
There is overlap of animals with those of the adjacent desert and pinyon-juniper communities.
Chaparral and wildfires.
The Chaparral is a coastal biome with hot, dry summers and mild, rainy winters. The Chaparral area receives about of precipitation a year. This makes the chaparral most vulnerable to fire in the late summer and fall.
The chaparral ecosystem as a whole is adapted to be able to recover from infrequent wildfires (fires occurring a minimum of 15 years apart); indeed, chaparral regions are known culturally and historically for their impressive fires. (This does create a conflict with human development adjacent to and expanding into chaparral systems.) Additionally, Native Americans burned chaparral to promote grasslands for textiles and food (Vale 2002). Before a major fire, typical chaparral plant communities are dominated by manzanita, chamise (also called greasewood or "Adenostoma fasciculatum") and "Ceanothus" species, toyon (which can sometimes be interspersed with scrub oaks), and other drought-resistant shrubs with hard (sclerophyllous) leaves; these plants resprout (see resprouter) from underground burls after a fire. Some chaparral plant communities may grow so dense and tall that it becomes difficult for large animals and humans to penetrate, but may be teeming with smaller fauna in the understory. Many chaparral plant species require some fire cue (heat, smoke, or charred wood, and chemical changes in the soil following fires) for germination. Others, such as annual and herbaceous species like "Phacelia" require fires to allow sunlight to reach them, and are known as fire followers. During the time shortly after a fire, chaparral communities may contain soft-leaved herbaceuous annual plants that dominate the community for the first few years - until the burl resprouts and seedlings of chaparral perennials create an overstory, blocking the sunlight from other plants in the community. When the overstory regrows, seeds of annuals and smaller plants may lie dormant until the next fire creates the conditions required for germination. Mid-sized plants such as "Ceonothus" fix nitrogen, while others cannot, which, together with the need for exposure to the sun, creates a symbiotic relationship of the entire community with infrequent fires.
Because of the hot, dry conditions that exist in the California summer and fall, chaparral is one of the most fire-prone plant communities in North America. Some fires are caused by lightning, but these are usually during periods of high humidity and low winds and are easily controlled. Nearly all of the very large wildfires are caused by human activity during periods of very hot, dry easterly Santa Ana winds. These man-made fires are commonly caused by power line failures, vehicle fires and collisions, sparks from machinery, arson, or campfires.
Though adapted to infrequent fires, chaparral plant communities can be exterminated by frequent fires. Today, frequent accidental ignitions can convert chaparral from a native shrubland to nonnative annual grassland and drastically reduce species diversity, especially under global-change-type drought (Syphard et al. 2007, Pratt et al. 2013).
Wildfire debate.
There are two assumptions relating to California chaparral fire regimes that appear to have caused considerable debate, and sometimes confusion and controversy, within the fields of wildfire ecology and land management. 
The perspective that older chaparral is unhealthy or unproductive may have originated during the 1940s when studies were conducted measuring the amount of forage available to deer populations in chaparral stands. However, according to recent studies, California chaparral is extraordinarily resilient to very long periods without fire and continues to maintain productive growth throughout pre-fire conditions. Seeds of many chaparral plants actually require 30 years or more worth of accumulated leaf litter before they will successfully germinate (e.g. scrub oak: "Quercus berberidifolia", toyon: "Heteromeles arbutifolia", and holly-leafed cherry: "Prunus ilicifolia"). When intervals between fires drop below 10 to 15 years, many chaparral species are eliminated and the system is typically replaced by non-native, invasive, weedy grassland.
The idea that older chaparral is responsible for causing large fires was originally proposed in the 1980s by comparing wildfires in Baja California and southern California . It was suggested that fire suppression activities in southern California allowed more fuel to accumulate, which in turn led to larger fires (in Baja, fires often burn without active suppression efforts ). This is similar to the argument that fire suppression in western United States has allowed ponderosa pine forests to become “overstocked”. In the past, surface-fires burned through these forests at intervals of anywhere between 4 and 36 years, clearing out the understory and creating a more ecologically balanced system. However, chaparral has a crown-fire regime, meaning that fires consume the entire system whenever they burn. In one study, a detailed analysis of historical fire data concluded that fire suppression activities have failed to exclude fire from southern California chaparral, as they have in ponderosa pine forests. In addition, the number of fires is increasing in step with population growth. Chaparral stand age does not have a significant correlation to its tendency to burn. Low humidity, low fuel moisture, and high winds appear to be the primary factors in determining when, where, and how large a chaparral fire burns. 

</doc>
<doc id="7456" url="https://en.wikipedia.org/wiki?curid=7456" title="CJD">
CJD

CJD can mean:

</doc>
<doc id="7460" url="https://en.wikipedia.org/wiki?curid=7460" title="Clinker">
Clinker

Clinker may refer to:
Clinker may also be used for:

</doc>
<doc id="7461" url="https://en.wikipedia.org/wiki?curid=7461" title="Clipper">
Clipper

A clipper was a very fast sailing ship of the middle third of the 19th century. They were fast, yacht like vessels, with three masts and a square rig. They were generally narrow for their length, could carry limited bulk freight, small by later 19th century standards, and had a large total sail area. Clipper ships were mostly constructed in British and American shipyards, though France, Brazil, the Netherlands and other nations also produced some. Clippers sailed all over the world, primarily on the trade routes between the United Kingdom and its colonies in the east, in trans-Atlantic trade, and the New York-to-San Francisco route round Cape Horn during the California Gold Rush. Dutch clippers were built beginning in the 1850s for the tea trade and passenger service to Java.
The boom years of the clipper ship era began in 1843 as a result of a growing demand for a more rapid delivery of tea from China. It continued under the stimulating influence of the discovery of gold in California and Australia in 1848 and 1851, and ended with the opening of the Suez Canal in 1869.
Origin and usage of "clipper".
The term "clipper" most likely derives from the verb "clip", which in former times meant, among other things, to run or fly swiftly. Dryden, the English poet, used the word "clip" to describe the swift flight of a falcon in the 17th century when he said "And, with her eagerness the quarry missed, Straight flies at check, and clips it down the wind." The ships appeared to clip along the ocean water. The term "clip" became synonymous with "speed" and was also applied to fast horses and sailing ships. "To clip it," and "going at a good clip," are familiar expressions to this day.
While the first application of the term "clipper" in a nautical sense is by no means certain, it seems to have had an American origin when applied to the Baltimore clippers of the late 18th century. When these vessels of a new model were built, which were intended to "clip" over the waves rather than plough through them, the improved type of craft became known as "clippers" because of their speed.
In England the nautical term "clipper" appeared a little later. The "Oxford English Dictionary" says its earliest quotation for "clipper" is from 1830. This does not mean, however, that little British opium clippers from prior to 1830 were not called "opium clippers" just as they are today. Carl C. Cutler reports the first newspaper appearance was in 1835, and by then the term was apparently familiar. An undated painting of the British "Water Witch" built in 1831 is labeled "OPIUM CLIPPER "WATER WITCH"" so the term had at least passed into common usage during the time that this ship sailed.
There is no single definition of the characteristics of a clipper ship, but mariner and author Alan Villiers describes them as follows:To sailors, three things made a ship a clipper. She must be sharp-lined, built for speed. She must be tall-sparred and carry the utmost spread of canvas. And she must "use" that sail, day and night, fair weather and foul. Optimized for speed, they were too fine-lined to carry much cargo. Clippers typically carried extra sails such as skysails and moonrakers on the masts, and studdingsails on booms extending out from the hull or yards, which required extra hands to handle them. And in conditions where other ships would shorten sail, clippers drove on, heeling so much that their lee rails were in the water.
History.
The first ships to which the term "clipper" seems to have been applied were the Baltimore clippers. Baltimore clippers were topsail schooners developed in the Chesapeake Bay before the American Revolution, and which reached their zenith between 1795 and 1815. They were small, rarely exceeding 200 tons OM, and modelled after French luggers. Some were lightly armed in the War of 1812, sailing under Letters of Marque and Reprisal, when the type—exemplified by "Chasseur", launched at Fells Point, Baltimore in 1814 became known for her incredible speed; the deep draft enabled the Baltimore clipper to sail close to the wind. Clippers, running the British blockade of Baltimore, came to be recognized for speed rather than cargo space.
Speed was also required for the Chinese opium trade between England, India and China. Small, sharp-bowed British vessels were the result. An early example, which is today known as an opium clipper, was "Transit" of 1819. She was followed by many more.
Meanwhile, Baltimore Clippers still continued to be built, and were built specifically for the China opium trade running opium between India and China, a trade that only became unprofitable for American shipowners in 1849.
"Ann McKim" is generally known as the original clipper ship. She was built in Baltimore in 1833 and was the first attempt at building a larger swift vessel in the United States. "Ann McKim" 494 tons OM, was built on the enlarged lines of a Baltimore clipper, with sharply raked stem, counter stern and square rig. She was built in Baltimore in 1833 by the Kennard & Williamson shipyard. Although "Ann McKim" was the first large clipper ship ever constructed, it cannot be said that she founded the clipper ship era, or even that she directly influenced shipbuilders, since no other ship was built like her; but she may have suggested the clipper design in vessels of ship rig. She did, however, influence the building of "Rainbow" in 1845, the first extreme clipper ship.
In Aberdeen, Scotland, the shipbuilders Alexander Hall and Sons developed the "Aberdeen" clipper bow in the late 1830s: the first was "Scottish Maid" launched in 1839. "Scottish Maid", 150 tons OM, was the first British clipper ship. ""Scottish Maid" was intended for the Aberdeen-London trade, where speed was crucial to compete with steamships. The Hall brothers tested various hulls in a water tank and found the clipper design most effective. The design was influenced by tonnage regulations. Tonnage measured a ship's cargo capacity and was used to calculate tax and harbour dues. The new 1836 regulations measured depth and breadth with length measured at half midship depth. Extra length above this level was tax-free and became a feature of clippers. "Scottish Maid" proved swift and reliable and the design was widely copied." The earliest British clipper ships were built for trade amongst the British Isles. Then followed the vast clipper trade of tea, opium, spices and other goods from the Far East to Europe, and the ships became known as "tea clippers".
From 1839, larger American clipper ships started to be built beginning with "Akbar", 650 tons OM, in 1839, and including the 1844-built Houqua, 581 tons OM. These larger vessels were built predominantly for use in the China tea trade and known as "tea clippers". Smaller clipper vessels also continued to be built predominantly for the China opium trade and known as "opium clippers" such as the 1842 built "Ariel", 100 tons OM.
Then in 1845 "Rainbow", 757 tons OM, the first extreme clipper was launched in New York. These American clippers were larger vessels designed to sacrifice cargo capacity for speed. They had a bow lengthened above the water, a drawing out and sharpening of the forward body, and the greatest breadth further aft. Extreme clippers were built in the period 1845 to 1855. From 1851 or earlier another type of clipper ship was also being built in American shipyards, the medium clipper. The medium clipper, though still very fast, had comparatively more allowance for cargo. After 1854 extreme clippers were replaced in American shipbuilding yards by medium clippers.
The Flying Cloud was a clipper ship that set the world's sailing record for the fastest passage between New York and San Francisco, 89 days 8 hours. She held this record for over 100 years, from 1854 to 1989.
Flying Cloud was the most famous of the clippers built by Donald McKay. She was known for her extremely close race with the Hornet in 1853; for having a woman navigator, Eleanor Creesy, wife of Josiah Perkins Creesy who skippered the Flying Cloud on two record-setting voyages from New York to San Francisco; and for sailing in the Australia and timber trades.
Clipper ships largely ceased being built in American shipyards in 1859 when, unlike the earlier boom years, only 4 clipper ships were built. That is except for a small number built in the 1860s, and the last American clipper ship from the East Boston shipyard of Donald McKay in 1869, "Glory of the Seas".
During the time from 1859 British clipper ships continued to be built. Earlier British clipper ships had become known as extreme clippers, and were considered to be "as sharp as the American" built ships. From 1859 a new design was developed for British clipper ships that was nothing like the American clippers. These ships built from 1859 continued to be called extreme clippers. The new design had a sleek graceful appearance, less sheer, less freeboard, lower bulwarks, and smaller breadth. They were built for the China tea trade and began with "Falcon" in 1859, and finished with the last ships built in 1870. It is estimated that 25 to 30 of these ships were built, and no more than 4–5 per year. The earlier ships were made from wood, though some were made from iron, just as some British clippers had been made from iron prior to 1859. In 1863 the first tea clippers of composite construction were brought out, combining the best of both worlds. Composite clippers had the strength of iron spars with wooden hulls, and copper sheathing could be added to prevent the fouling that occurred on iron hulls.
After 1869 with the opening of the Suez Canal that allowed competition with steam vessels, the tea trade then collapsed for clippers. From the late 1860s-early 1870's the clipper trade increasingly focused on trade and the carrying of immigrants between England and Australia and New Zealand, a trade that had begun earlier with the Australian Gold Rush in the 1850s. British-built clipper ships were used for this trade, as were many American-built ships which were sold to British owners. Even in the 1880s, sailing ships were still the main carriers of cargoes to and from Australia and New Zealand. Eventually, however, even this trade became unprofitable, and the aging clipper fleet became unseaworthy.
China clippers and the apogee of sail.
Among the most notable clippers were the China clippers, also called Tea clippers or Opium clippers, designed to ply the trade routes between Europe and the East Indies. The last example of these still in reasonable condition was "Cutty Sark", preserved in dry dock at Greenwich, United Kingdom. Damaged by fire on 21 May 2007 while undergoing conservation, the ship was permanently elevated three meters above the dry dock floor in 2010 as part of a plan for long-term preservation.
Before the early 18th century, the East India Company paid for its tea mainly in silver. However, when the Chinese Emperor chose to embargo European manufactured commodities and demand payment for all Chinese goods in silver, the price rose, restricting free trade. The East India Company began to manufacture a product that was desired by the Chinese as much as tea was by the British: opium. This had a significant influence on both India and China. Opium was also imported into Britain and was not prohibited because it was thought to be medically beneficial. Laudanum, which was made from opium was also used as a pain killer, to induce sleep and to suppress anxiety. The famous literary opium addicts Thomas De Quincey, Samuel Taylor Coleridge and Wilkie Collins also took it for its pleasurable effects. The Limehouse area in London was notorious for its opium dens, many of which catered for Chinese sailors as well as English addicts.
Clippers were built for seasonal trades such as tea, where an early cargo was more valuable, or for passenger routes. One passenger ship survives, the City of Adelaide designed by William Pile of Sunderland. The fast ships were ideally suited to low-volume, high-profit goods, such as tea, opium, spices, people, and mail. The return could be spectacular. The "Challenger" returned from Shanghai with ""the most valuable cargo of tea and silk ever to be laden in one bottom"". Competition among the clippers was public and fierce, with their times recorded in the newspapers. The ships had short-expected lifetimes and rarely outlasted two decades of use before they were broken up for salvage. Given their speed and maneuverability, clippers frequently mounted cannon or carronades and were used for piracy, privateering, smuggling, or interdiction service.
The last China clippers were acknowledged as the fastest sail vessels. When fully rigged and riding a tradewind, they had peak average speeds over . The Great Tea Race of 1866 showcased their speed. China clippers are also the fastest commercial sailing vessels ever made. Their speeds have been exceeded many times by modern yachts, but never by a commercial sail vessel. Only the fastest windjammers could attain similar speeds.
The 24h record of the "Champion of the Seas" wasn't broken until 1984 (by a multihull), or 2001 (by another monohull).
Decline.
Decline in the use of clippers started with the economic slump following the Panic of 1857 and continued with the gradual introduction of the steamship. Although clippers could be much faster than early steamships, they depended on the vagaries of the wind, while steamers could keep to a schedule. The "steam clipper" was developed around this time, and had auxiliary steam engines which could be used in the absence of wind. An example was "Royal Charter", built in 1857 and wrecked on the coast of Anglesey in 1859. The final blow was the Suez Canal, opened in 1869, which provided a great shortcut for steamships between Europe and Asia, but was difficult for sailing ships to use. With the absence of the tea trade, some clippers began operating in the wool trade, between Britain and Australia.
Surviving ships.
Although many clipper ships were built in the mid-19th century, "Cutty Sark" was, perhaps until recently, the only intact survivor. Other surviving examples of clippers of the era are less well preserved, for example the oldest surviving clipper "City of Adelaide"" (a.k.a. S.V. "Carrick").
"Falls of Clyde" is a well-preserved example of a more conservatively designed, slower contemporary of the clippers, which was built for general freight in 1878.
Clipper ship sailing cards.
Departures of clipper ships, mostly from New York and Boston to San Francisco, were advertised by clipper ship sailing cards. These cards, slightly larger than today’s postcards, were produced by letterpress and wood engraving on coated card stock. Most clipper cards were printed in the 1850s and 1860s, and represented the first pronounced use of color in American advertising art.
Relatively few (perhaps 3,500) cards survive today. With their stunning appearance, rarity, and importance as artifacts of nautical, Western, and printing history, clipper cards are highly prized by both private collectors and institutions.

</doc>
<doc id="7462" url="https://en.wikipedia.org/wiki?curid=7462" title="Clive Anderson">
Clive Anderson

Clive Anderson (born 10 December 1952) is an English television and radio presenter, comedy writer and former barrister. Winner of a British Comedy Award in 1991, Anderson began experimenting with comedy and writing comedic scripts during his 15-year legal career, before starring in "Whose Line Is It Anyway?" on BBC Radio 4, then later Channel 4. He has also been successful with a number of radio programmes, television interviews and guest appearances on "Have I Got News for You", "Mock the Week" and "QI". 
Early life.
Anderson was educated at Stanburn Primary School and Harrow County School for Boys where his group of friends included Geoffrey Perkins and Michael Portillo. His Scottish father was manager of the Midland Bank's Wembley branch. Anderson attended Selwyn College, Cambridge, where, from 1974 to 1975, he was President of Footlights. He was called to the bar at the Middle Temple in 1976 and became a practising barrister, specialising in criminal law.
Career.
Television.
Anderson was involved in the fledgling alternative comedy scene in the early 1980s and was the first act to come on stage at The Comedy Store when it opened in 1979. He made his name as host of the improvised television comedy show "Whose Line Is It Anyway?", which ran for 10 series.
Anderson hosted his own chat-show, "Clive Anderson Talks Back", on Channel 4, which ran for 10 series. Anderson moved to the BBC in 1996 and the show's name changed to "Clive Anderson All Talk" and was aired on BBC1. In one incident in 1996, Anderson interviewed the Bee Gees, and throughout the interview he repeatedly joked about their life and career, ultimately prompting the band to walk out. Anderson once had a glass of water poured over his head by a perturbed Richard Branson. He also famously said to Jeffrey Archer, "There's no beginning to your talents." Archer retorted that "The old jokes are always the best," for Anderson to reply "Yes, I've read your books." The last series of "Clive Anderson All Talk" aired in 2001.
He has been a frequent participant on "Have I Got News for You", making ten appearances in total. He has also frequently appeared on "QI". In 2007, he featured as a regular panellist on the ITV comedy show "News Knight". One of his most memorable exchanges on "HIGNFY" occurred when he scathingly joked to fellow guest Piers Morgan that the "Daily Mirror" was now, thanks to Morgan (then its editor), almost as good as "The Sun". When asked by Morgan, "What do you know about editing newspapers?", he swiftly replied, "About as much as you do."
As a journalist for the BBC, he travelled around the world looking at problems "in out-of-the-way places," though mostly arguing about whether they could film there. "Our Man in..." featured episodes on monkeywrenching in American logging and 419 scams in Nigeria.
In 2005 he presented the short-lived Celador panel game "Back in the Day" for Channel 4.
In January 2008, he appeared on the second episode of "Thank God You're Here" and won.
On 25 February 2008, he started presenting "Brainbox Challenge", a new game show, for BBC Two.
In 2008, he presented a reality TV talent show-themed television series produced by the BBC entitled "Maestro", starring eight celebrities who are "famous amateurs with a passion for classical music."
In 2009, Anderson was the television host of the BBC's "Last Night of the Proms".
TV presenting.
Shows he has presented include:
Appearances.
Radio.
In recent years, Clive Anderson has combined his continuing interest in the law with his role as a radio presenter in the regular series "Unreliable Evidence" on Radio 4. He also covered the Sunday morning 11 AM-1 PM show on BBC Radio 2 through the end of January 2008.
It was announced in April 2008 that Anderson, who had previously filled in for host Ned Sherrin from 2006 until his death from throat cancer in 2007, would be taking over as permanent host of "Loose Ends". He also hosted six series of "Clive Anderson's Chat Room" on BBC Radio 2 from 2004–2009. Clive Anderson has appeared on BBC Radio 4's "The Unbelievable Truth" hosted by David Mitchell.
Clive also presents "The Guessing Game (radio)" on BBC Radio Scotland.
Comedy and newspaper writing.
Anderson is a comedy sketch writer who has written for Frankie Howerd, "Not the Nine O'Clock News", and Griff Rhys Jones/Mel Smith. One of his early comedy writing projects was "Black Cinderella Two Goes East" with Rory McGrath for BBC Radio 4 in 1978. He is famous for his fast, nervous delivery and close-to-the-knuckle witticisms.
As well as writing comedy, Anderson is also a frequent contributor to newspapers, and was a regular columnist in the "Sunday Correspondent".
Personal life.
Anderson lives in Highbury, north London, with his wife, Jane, and three children. He supports Arsenal, and Rangers FC and is President of the Woodland Trust and Vice Patron of the Solicitors' Benevolent Association.
He also has a holiday home in Dalmally, Argyll.
Awards.
The show "Whose Line is it Anyway?" won a BAFTA award in 1990. Later, Clive Anderson won both the "Top Entertainment Presenter" and "Top Radio Comedy Personality" at the British Comedy Awards in 1991.

</doc>
<doc id="7463" url="https://en.wikipedia.org/wiki?curid=7463" title="Cold fusion">
Cold fusion

Cold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. This is compared with the "hot" fusion which takes place naturally within stars, under immense pressure and at temperatures of millions of degrees, and distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model which would allow cold fusion to occur.
In 1989 Martin Fleischmann (then one of the world's leading electrochemists) and Stanley Pons reported that their apparatus had produced anomalous heat ("excess heat") of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.
Many scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion.
A small community of researchers continues to investigate cold fusion, now often preferring the designation low-energy nuclear reactions (LENR) or condensed matter nuclear science (CMNS). Since cold fusion articles are rarely published in peer-reviewed mainstream scientific journals, they do not attract the level of scrutiny expected for mainstream scientific publications.
History.
Nuclear fusion is normally understood to occur at temperatures in the tens of millions of degrees. Since the 1920s, there has been speculation that nuclear fusion might be possible at much lower temperatures by catalytically fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before the majority of scientists criticized their claim as incorrect after many found they could not replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of researchers who believe that such reactions happen and hope to gain wider recognition for their experimental evidence.
Early research.
The ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, saying that the helium they measured was due to background from the air.
In 1927, Swedish scientist John Tandberg reported that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for "a method to produce helium and useful reaction energy". Due to Paneth and Peters's retraction and his inability to explain the physical process, his patent application was denied. After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.
The term "cold fusion" was used as early as 1956 in a "New York Times" article about Luis Alvarez's work on muon-catalyzed fusion. Paul Palmer and then Steven Jones of Brigham Young University used the term "cold fusion" in 1986 in an investigation of "geo-fusion", the possible existence of fusion involving hydrogen isotopes in a planetary core. In his original paper on this subject with Clinton Van Siclen, submitted in 1985, Jones had coined the term "piezonuclear fusion".
Fleischmann–Pons experiment.
The most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program to France after the controversy erupted.
Events preceding announcement.
Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.
In 1988, Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled "Cold nuclear fusion" that had been published in "Scientific American" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable "excess energy", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their 6 March meeting differ.
Announcement.
In mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to "Nature" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the "Journal of Electroanalytical Chemistry" on 11 March, and disclosing their work via a press release and press conference on 23 March. Jones, upset, faxed in his paper to "Nature" after the press conference.
Fleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. And many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.
The announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: "What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics."
Response and fallout.
Although the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to "Nature" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal "Fusion Technology". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that "essentially all" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as "cold fusion" or "fusion confusion" in the news.
In April 1989, Fleischmann and Pons published a "preliminary note" in the "Journal of Electroanalytical Chemistry". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.
Nevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.
On 30 April 1989, cold fusion was declared dead by the "New York Times". The "Times" called it a circus the same day, and the "Boston Herald" attacked cold fusion the following day.
On 1 May 1989, the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of ""the incompetence and delusion of Pons and Fleischmann,"" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.
On 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.
From 8 May only the A&M tritium results kept cold fusion afloat.
In July and November 1989, "Nature" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including "Science", "Physical Review Letters", and "Physical Review C" (nuclear physics).
In August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.
The United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of "focused experiments within the general funding system." Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of "pathological science".
In March 1990 Dr. Michael H. Salamon, a Utah physicist, and nine co-authors reported negative results. University faculty were then "stunned" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.
In early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in "Science" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.
On 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.
On 1 January 1991, Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischman resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.
Mostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.
Subsequent research.
A 1991 review by a cold fusion proponent had calculated "about 600 scientists" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis set-ups in spite of the rejection by the mainstream community. "The Boston Globe" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India.
Current research.
Cold fusion research continues today in a few specific venues, but the wider scientific community has generally marginalized the research being done and researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR), Chemically Assisted Nuclear Reactions (CANR), Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) or Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with "cold fusion". The new names avoid making bold implications, like implying that fusion is actually occurring.
The researchers who continue acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated for increased attention from mainstream researchers and described cold fusion as:
United States.
United States Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002, they released a two-volume report, "Thermal and nuclear aspects of the Pd/D2O system," with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.
In August 2003, the U.S. Secretary of Energy, Spencer Abraham, ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were "split approximately evenly" on whether the experiments had produced energy in the form of heat, but "most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented.'" In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they didn't recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research "could be helpful in resolving some of the controversies in the field". They summarized its conclusions thus:
Cold fusion researchers placed a "rosier spin" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused "a huge upswing in interest in funding cold fusion research." However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as "cold fusion" even if they are not, in order to attract the attention of journalists.
In February 2012, millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news-show "60 Minutes", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum under extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which Prelas says bursts of millions of neutrons a second were recorded, which was stopped because "his research account had been frozen". He claims that the new experiment has already seen "neutron emissions at similar levels to the 1991 observation".
Italy.
Since the Fleischmann and Pons announcement, the Italian National agency for new technologies, Energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomena to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA started a research program which claimed to have found excess power of up to 500 percent, and in 2009, ENEA hosted the 15th cold fusion conference.
Japan.
Between 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a "New Hydrogen Energy (NHE)" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated "We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future." In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher is Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and McKubre at SRI.
India.
In the 1990s, India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended that the Indian government revive this research. Projects were commenced at the Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research has stalled since the 1990s. A special section in the Indian multidisciplinary journal "Current Science" published 33 cold fusion papers in 2015 by major cold fusion researchers including several Indian researchers.
Reported results.
A cold fusion experiment usually includes:
Electrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also "heat-after-death" experiments, where the evolution of heat is monitored after the electric current is turned off.
The most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear—this is known as the "loading time," the time required to saturate the palladium electrode with hydrogen (see "Loading ratio" section).
The Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.
Excess heat and energy production.
An excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, an electrolysis cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.
Unable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments. In 1993, after their original report, Fleischmann reported "heat-after-death" experiments—where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report has also become part of subsequent cold fusion claims.
Helium, heavy elements, and neutrons.
Known instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detection of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 1012 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.
Several medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the United States Department of Energy (DOE) in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.
In response to doubts about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring 4He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for 4He; with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.
One of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and nuclear transmutations. Some researchers also claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.
Proposed mechanisms.
Researchers in the field do not agree on a theory for cold fusion. One proposal considers that hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes, however, not enough by a factor of ten to create the fusion rates claimed in the original experiment. It was proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations not convincing and inconsistent with current physics theories.
Criticism.
Criticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility that fusion reactions have occurred in electrolysis set-ups or criticizing the excess heat measurements as being spurious, erroneous, or due to poor methodology or controls. There are a couple of reasons why known fusion reactions are an unlikely explanation for the excess heat and associated cold fusion claims.
Repulsion forces.
Because nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this charged repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat. In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.
Paneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium, which at the time was needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.
This was also the belief of geologist Palmer, who convinced Steven Jones that the helium-3 occurring naturally in Earth perhaps came from fusion involving hydrogen isotopes inside catalysts like nickel and palladium. This led their team in 1986 to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had much the same belief, but they calculated the pressure to be of 1027 atmospheres, when cold fusion experiments only achieve a loading ratio of one to one, which only has between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.
Lack of expected reaction products.
Conventional deuteron fusion is a two-step process, in which an unstable high energy intermediary is formed:
Experiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:
Only about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (1 eV = 1.602 x 10−19 joule) of nuclear power were produced from deuteron fusion consistent with known branching ratios, the resulting neutron and tritium (3H) production would be easily measured. Some researchers reported detecting 4He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of 4He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.
The known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then we would see measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment only cause small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.
Setup of experiments.
Cold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and have asserted that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Critics have further taken issue with what they describe as mistakes or errors of interpretation that cold fusion researchers have made in calorimetry analyses and energy budgets.
Reproducibility.
In 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups, however, reported successful reproductions of cold fusion during this time. In July 1989, an Indian group from the Bhabha Atomic Research Centre (P. K. Iyengar and M. Srinivasan) and in October 1989, John Bockris' group from Texas A&M University reported on the creation of tritium. In December 1990, professor Richard Oriani of the University of Minnesota reported excess heat.
Groups that did report successes found that some of their cells were producing the effect, while other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:
Loading ratio.
Cold fusion researchers (McKubre since 1994, ENEA in 2011) have speculated that a cell that is loaded with a deuterium/palladium ratio lower than 100% (or 1:1) will not produce excess heat. Since most of the negative replications from 1989–1990 did not report their ratios, this has been proposed as an explanation for failed replications. This loading ratio is hard to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells, there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier uses now a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.
Misinterpretation of data.
Some research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.
Calorimetry errors.
The calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.
One assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid D2O into gaseous D2 and O2. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.
Another assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.
Publications.
The ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper "Cold Fusion: A Hypothesis" in "Physical Review Letters", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of "PRL") in protest.
The number of papers sharply declined after 1990 because of two simultaneous phenomena: scientists abandoning the field and journal editors declining to review new papers, and cold fusion fell off the ISI charts. Researchers who got negative results abandoned the field, while others kept publishing. A 1993 paper in "Physics Letters A" was the last paper published by Fleischmann, and "one of the last reports to be formally challenged on technical grounds by a cold fusion skeptic".
The "Journal of Fusion Technology" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.
The decline of publications in cold fusion has been described as a "failed information epidemic". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to "normal" science more difficult.
Cold fusion reports continued to be published in a small cluster of specialized journals like "Journal of Electroanalytical Chemistry" and "Il Nuovo Cimento". Some papers also appeared in "Journal of Physical Chemistry", "Physics Letters A", "International Journal of Hydrogen Energy", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, "Naturwissenschaften" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board. In 2015 the Indian multidisciplinary journal "Current Science" published a special section devoted entirely to cold fusion related papers.
In the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as "Fusion Facts", "Cold Fusion Magazine", "Infinite Energy Magazine" and "New Energy Times" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.
Conferences.
Cold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The first International Conference on Cold Fusion (ICCF) was held in 1990, and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics; thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science (the reasons are explained in the subsequent research section), but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as "low-energy nuclear reactions", or LENR, but according to sociologist Bart Simon the "cold fusion" label continues to serve a social function in creating a collective identity for the field.
Since 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include "invited symposium(s)" on cold fusion. An ACS program chair said that without a proper forum the matter would never be discussed and, "with the world facing an energy crisis, it is worth exploring all possibilities."
On 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis set-up and a CR-39 detector, a result previously published in "Naturwissenschaften". The authors claim that these neutrons are indicative of nuclear reactions; without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.
Patents.
Although details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from the 5 to 12 April. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and in March 1998 it said that it would no longer defend its patents.
The U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is "useful", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being "inoperative" are rare, since such rejections need to demonstrate "proof of total incapacity", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.
A U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more in the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving "new nuclear physics" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of "cold fusion" from the patent description to avoid having it rejected outright.
At least one patent related to cold fusion has been granted by the European Patent Office.
A patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.
Cultural references.
In "Undead Science", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in "Murphy Brown" and "The Simpsons". It was adopted as a software product name Adobe ColdFusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.
The plot of "The Saint", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. The film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.
"Final Exam", the 16th episode of season 4 of "The Outer Limits", depicts a student named Todtman who has invented a cold fusion weapon, and attempts to use it as a tool for revenge on people who have wronged him over the years. Despite the secret being lost with his death at the end of the episode, it is implied that another student elsewhere is on a similar track, and may well repeat Todtman's efforts.

</doc>
<doc id="7466" url="https://en.wikipedia.org/wiki?curid=7466" title="Coal tar">
Coal tar

Coal tar is a brown or black liquid of extremely high viscosity. Coal tar is among the by-products when coal is 
carbonized to make coke or gasified to make coal gas. Coal tars are complex and variable mixtures of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds.
It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.
Uses.
Pavement sealcoat.
Coal tar is incorporated into some parking-lot sealcoat products, which are used to protect and beautify the underlying pavement. Sealcoat products that are coal-tar based typically contain 20 to 35 percent coal-tar pitch. Research shows it is used in United States states from Alaska to Florida and several areas have banned its use in sealcoat products
Industrial.
Being flammable, coal tar is sometimes used for heating or to fire boilers. Like most heavy oils, it must be heated before it will flow easily.
Coal tar was a component of the first sealed roads. In its original development by Edgar Purnell Hooley, tarmac was tar covered with granite chips. Later the filler used was industrial slag. Today, petroleum derived binders and sealers are more commonly used. These sealers are used to extend the life and reduce maintenance cost associated with asphalt pavements, primarily in asphalt road paving, car parks and walkways.
A large part of the binders used in the graphite industry for making "green blocks" are coke oven volatiles (COV). A considerable portion of these COV used as binders is coal tar. During the baking process of the green blocks as a part of commercial graphite production, most of the coal tar binders are vaporised and are generally burned in an incinerator to prevent release into the atmosphere, as COV and coal tar can be injurious to health.
Coal tar is also used to manufacture paints, synthetic dyes, and photographic materials.
Medical.
It can be used in medicated shampoo, soap and ointment, as a treatment for dandruff and psoriasis, as well as being used to kill and repel head lice. When used as a medication in the U.S., coal tar preparations are considered over-the-counter drug pharmaceuticals and are subject to regulation by the USFDA. Named brands include Denorex, Balnetar, Psoriasin, Tegrin, T/Gel, and Neutar. When used in the extemporaneous preparation of topical medications, it is supplied in the form of coal tar topical solution USP, which consists of a 20% w/v solution of coal tar in alcohol, with an additional 5% w/v of polysorbate 80 USP; this must then be diluted in an ointment base such as petrolatum.
Pine tar has historically also been used for this purpose. Though it is frequently cited online as having been banned as a medical product by the FDA due to a "lack of evidence having been submitted for proof of effectiveness", pine tar is included in the Code of Federal Regulations, subchapter D: Drugs for Human Use, as an OTC treatment for "Dandruff/seborrheic dermatitis/psoriasis".
Various phenolic coal tar derivatives have analgesic (pain-killer) properties. These included acetanilide, phenacetin, and paracetamol (acetaminophen). Paracetamol is the only coal-tar derived analgesic still in use today, but industrial phenol is now usually synthesized from crude oil rather than coal tar.
Safety.
According to the International Agency for Research on Cancer, preparations that include more than five percent of crude coal tar are Group 1 carcinogens.
According to the National Psoriasis Foundation and the FDA, coal tar is a valuable, safe and inexpensive treatment option for millions of people with psoriasis and other scalp or skin conditions. Coal tar concentrations between 0.5% and 5% are safe and effective for psoriasis, and no scientific evidence suggests that the coal tar in the concentrations seen in non-prescription treatments is (or is not) carcinogenic because there are too few studies and insufficient data to make a judgement. Coal tar contains approximately 10,000 chemicals, of which only about 50% have been identified, and the composition of coal tar varies with its origin and type of coal (for example,: lignite, bituminous or anthracite) used to make it.
Coal tar causes increased sensitivity to sunlight, so skin treated with topical coal tar preparations should be protected from sunlight.
The residue from the distillation of high-temperature coal tar, primarily a complex mixture of three or more membered condensed ring aromatic hydrocarbons, was listed on 28 October 2008 as a substance of very high concern by the European Chemicals Agency.
People can be exposed to coal tar pitch volatiles in the workplace by breathing them in, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for coal tar pitch volatiles exposure in the workplace as 0.2 mg/m3 benzene-soluble fraction over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m3 cyclohexane-extractable fraction over an 8-hour workday. At levels of 80 mg/m3, coal tar pitch volatiles are immediately dangerous to life and health.
Coal tar distillers.
In the coal gas era, there were many companies in Britain whose business was to distill coal tar to separate the higher-value fractions, such as naphtha, creosote and pitch. A great many industrial chemicals were first isolated from coal tar during this time. These companies included:
Names.
Also known as liquor carbonis detergens (LCD), and liquor picis carbonis (LPC) by BP.,

</doc>
<doc id="7467" url="https://en.wikipedia.org/wiki?curid=7467" title="Cobbler">
Cobbler

Cobbler(s) may refer to:

</doc>
<doc id="7471" url="https://en.wikipedia.org/wiki?curid=7471" title="Catherine of Siena">
Catherine of Siena

Saint Catherine of Siena, T.O.S.D. (March 25, 1347 in Siena – April 29, 1380 in Rome), was a tertiary of the Dominican Order and a Scholastic philosopher and theologian. She also worked to bring the papacy of Gregory XI back to Rome from its displacement in France and to establish peace among the Italian city-states. Since 18 June 1939, she is one of the two patron saints of Italy, together with St. Francis of Assisi. On 3 October 1970, she was proclaimed a Doctor of the Church by Pope Paul VI, and on 1 October 1999, Pope John Paul II named her as one of the six patron saints of Europe, together with Benedict of Nursia, Saints Cyril and Methodius, Bridget of Sweden and Edith Stein.
Life.
Caterina di Giacomo di Benincasa was born on 25 March 1347 in Black Death-ravaged Siena, Italy, to Giacomo di Benincasa, a cloth dyer who ran his enterprise with the help of his sons, and Lapa Piagenti, the daughter of a local poet. The house where Catherine grew up is still in existence. Lapa was about forty years old when she prematurely gave birth to twin daughters, Catherine and Giovanna. She had already borne 22 children, but half of them had died. Giovanna was handed over to a wet-nurse and presently died. Catherine was nursed by her mother and developed into a healthy child. She was two years old when Lapa had her 25th child, another daughter named Giovanna. As a child Catherine was so merry that the family gave her the pet name of "Euphrosyne", which is Greek for "joy" and the name of an early Christian saint.
Catherine is said by her confessor and biographer Raymond of Capua O.P.'s "Life" to have had her first vision of Christ when she was the age of five or six. With her brother, she was on the way home from a visit to a married sister and is said to have experienced a vision of Christ seated in glory with the Apostles Peter, Paul, and John. Raymond continues that at age seven, Catherine vowed to give her whole life to God.
Her older sister Bonaventura died in childbirth. While tormented with sorrow, sixteen-year-old Catherine was now faced with her parents' wish that she marry Bonaventura's widower. Absolutely opposed to this, she started a massive fast, something she had learnt from Bonaventura, whose husband had not been considerate in the least. Bonaventura had changed her husband's attitude by refusing to eat until he showed better manners. This had taught Catherine the power of fasting. She disappointed her mother by cutting off her long hair as a protest against being overly encouraged to improve her appearance to attract a husband.
Catherine would later advise Raymond of Capua to do during times of trouble what she did now as a teenager: "Build a cell inside your mind, from which you can never flee." In this inner cell she made her father into a representation of Christ, her mother Lapa into the Blessed Virgin Mary, and her brothers into the apostles. Serving them humbly became an opportunity for spiritual growth. Catherine resisted the accepted course of marriage and motherhood, on the one hand, or a nun's veil, on the other. She chose to live an active and prayerful life outside a convent’s walls following the model of the Dominicans. Eventually her father gave up and permitted her to live as she pleased.
A vision of St. Dominic gave strength to Catherine, but her wish to join his Order was no comfort to Lapa, who took her daughter with her to the baths in Bagno Vignoni to improve her health. Catherine fell seriously ill with a violent rash, fever and pain, which conveniently made her mother accept her wish to join the "Mantellate", the local association of Dominican tertiaries. Lapa went to the Sisters of the Order and persuaded them to take in her daughter. Within days, Catherine seemed entirely restored, rose from bed and donned the black and white habit of the Third Order of St. Dominic. Catherine received the habit of a Dominican tertiary from the friars of the Order after vigorous protests from the tertiaries themselves, who up to that point had been only widows. As a tertiary, she lived outside the convent, at home with her family like before. The Mantellate taught Catherine how to read, and she lived in almost total silence and solitude in the family home.
Her custom of giving away clothing and food without asking anyone's permission cost her family significantly, but she demanded nothing for herself. By staying in their midst, she could live out her rejection of them more strongly. She did not want their food, referring to the table laid for her in Heaven with her real family. 
In about 1368, age twenty-one, Catherine experienced what she described in her letters as a "Mystical Marriage" with Jesus, later a popular subject in art as the "Mystic marriage of Saint Catherine". Caroline Walker Bynum, one of the most prominent medieval historians of her generation, explains one surprising and controversial aspect of this marriage that occurs both in artistic representations of the event and in some early accounts of her life: "Underlining the extent to which the marriage was a fusion with Christ's physicality [...] Catherine received, not the ring of gold and jewels that her biographer reports in his bowdlerized version, but the ring of Christ's foreskin." Catherine herself mentions the foreskin-as-wedding ring motif in one of her letters (#221), equating the wedding ring of a virgin with a foreskin; she typically claimed that her own wedding ring to Christ was simply invisible. Raymond also records that she was told by Christ to leave her withdrawn life and enter the public life of the world. Catherine rejoined her family and began helping the ill and the poor, where she took care of them in hospitals or homes. Her early pious activities in Siena attracted a group of followers, women and men, who gathered around her.
As social and political tensions mounted in Siena, Catherine found herself drawn to intervene in wider politics. She made her first journey to Florence in 1374, probably to be interviewed by the Dominican authorities at the General Chapter held in Florence in May 1374, though this is controverted (if she was interviewed, then the absence of later evidence suggests she was deemed sufficiently orthodox). It seems that at this time she acquired Raymond of Capua as her confessor and spiritual director.
After this visit, she began travelling with her followers throughout northern and central Italy advocating reform of the clergy and advising people that repentance and renewal could be done through "the total love for God." In Pisa, in 1375, she used what influence she had to sway that city and Lucca away from alliance with the anti-papal league whose force was gaining momentum and strength. She also lent her enthusiasm towards promoting the launch of a new crusade. It was in Pisa in 1375 that, according to Raymond of Capua's biography, she received the stigmata (visible, at Catherine's request, only to herself).
Physical travel was not the only way in which Catherine made her views known. From 1375 onwards, she began dictating letters to scribes. These letters were intended to reach men and women of her circle, increasingly widening her audience to include figures in authority as she begged for peace between the republics and principalities of Italy and for the return of the Papacy from Avignon to Rome. She carried on a long correspondence with Pope Gregory XI, asking him to reform the clergy and the administration of the Papal States.
Towards the end of 1375, she returned to Siena, to assist a young political prisoner, Niccolò di Tuldo, at his execution. In June 1376 Catherine went to Avignon as ambassador of Florence to make peace with the Papal States (on 31 March 1376 Gregory XI had placed Florence under interdict). She was unsuccessful and was disowned by the Florentine leaders, who sent ambassadors to negotiate on their own terms as soon as Catherine's work had paved the way for them. Catherine sent an appropriately scorching letter back to Florence in response. While in Avignon, Catherine also tried to convince Pope Gregory XI to return to Rome. Gregory did indeed return his administration to Rome in January 1377; to what extent this was due to Catherine’s influence is a topic of much modern debate.
Catherine returned to Siena and spent the early months of 1377 founding a women's monastery of strict observance outside the city in the old fortress of Belcaro. She spent the rest of 1377 at Rocca d'Orcia, about twenty miles from Siena, on a local mission of peace-making and preaching. During this period, in autumn 1377, she had the experience which led to the writing of her "Dialogue" and learned to write, although she still seems to have chiefly relied upon her secretaries for her correspondence.
Late in 1377 or early in 1378 Catherine again travelled to Florence, at the order of Gregory XI, to seek peace between Florence and Rome. Following Gregory's death in March 1378 riots, the revolts of the Ciompi, broke out in Florence on 18 June, and in the ensuing violence she was nearly assassinated. Eventually, in July 1378, peace was agreed between Florence and Rome; Catherine returned quietly to Florence.
In late November 1378, with the outbreak of the Western Schism, the new Pope, Urban VI, summoned her to Rome. She stayed at Pope Urban VI's court and tried to convince nobles and cardinals of his legitimacy, both meeting with individuals at court and writing letters to persuade others.
For many years she had accustomed herself to a rigorous abstinence. She received the Holy Communion almost daily. This extreme fasting appeared unhealthy in the eyes of the clergy and her own sisterhood. Her confessor, Blessed Raymond, ordered her to eat properly. But Catherine claimed that she was unable to, describing her inability to eat as an "infermità" (illness). From the beginning of 1380, Catherine could neither eat nor swallow water. On February 26 she lost the use of her legs.
St Catherine died in Rome, on 29 April 1380, at the age of thirty-three, having suffered a stroke eight days earlier.
Sources of her life.
There is some internal evidence of Catherine's personality, teaching and work in her nearly four hundred letters, her "Dialogue", and her prayers.
Much detail about her life has also, however, been drawn from the various sources written shortly after her death in order to promote her cult and canonisation. Though much of this material is heavily hagiographic, it has been an important source for historians seeking to reconstruct Catherine's life. Various sources are particularly important, especially the works of Raymond of Capua, who was Catherine's spiritual director and close friend from 1374 until her death, and himself became Master General of the Order in 1380. Raymond began writing what is known as the "Legenda Major", his "Life" of Catherine, in 1384, and completed it in 1395.
Another important work written after Catherine's death was "Libellus de Supplemento" ("Little Supplement Book"), written between 1412 and 1418 by Tommaso d'Antonio Nacci da Siena (commonly called Thomas of Siena, or Tommaso Caffarini): the work is an expansion of Raymond's "Legenda Major" making heavy use of the notes of Catherine's first confessor, Tommaso della Fonte (notes that do not survive anywhere else). Caffarini later published a more compact account of Catherine's life, entitled the "Legenda Minor".
From 1411 onwards, Caffarini also co-ordinated the compiling of the "Processus" of Venice, the set of documents submitted as part of the process of canonisation of Catherine, which provides testimony from nearly all of Catherine's disciples. There is also an anonymous piece entitled "Miracoli della Beata Caterina" ("Miracle of Blessed Catherine"), written by an anonymous Florentine. A few other relevant pieces survive.
Works.
Three genres of work by Catherine survive:
Veneration.
She was buried in the (Roman) cemetery of Santa Maria sopra Minerva which lies near the Pantheon. After miracles were reported to take place at her grave, Raymond moved her inside the Basilica of Santa Maria sopra Minerva, where she lies to this day.
Her head however, was parted from her body and inserted in a gilt bust of bronze. This bust was later taken to Siena, and carried through that city in a procession to the Dominican church. Behind the bust walked Lapa, Catherine's mother, who lived until she was 89 years old. By then she had seen the end of the wealth and the happiness of her family, and followed most of her children and several of her grandchildren to the grave. She helped Raymond of Capua write his biography of her daughter, and said, "I think God has laid my soul athwart in my body, so that it can't get out." The incorruptible head and thumb were entombed in the Basilica of San Domenico at Siena, where they remain.
Pope Pius II, himself from Siena, canonized St Catherine on 29 June 1461.
On 3 October 1970, Pope Paul VI named Catherine a Doctor of the Church; this title was almost simultaneously given to Saint Teresa of Ávila (27 September 1970), making them the first women to receive this honour.
Initially however, her feast day was not included in the General Roman Calendar. When it was added in 1597, it was put on the day of her death, April 29; however, because this conflicted with the feast of Saint Peter of Verona which also fell on the 29th of April, Catherine's feast day was moved in 1628 to the new date of April 30. In the 1969 revision of the calendar, it was decided to leave the celebration of the feast of St Peter of Verona to local calendars, because he was not as well known worldwide, and Saint Catherine's feast was restored to its traditional date of April 29.
Patronage.
In his decree of 13 April 1866, Pope Pius IX declared Catherine of Siena to be a co-patroness of Rome. On 18 June 1939 Pope Pius XII named her a joint Patron Saint of Italy along with Saint Francis of Assisi.
On 1 October 1999, Pope John Paul II made her one of Europe's patron saints, along with Edith Stein and Bridget of Sweden. She is also the patroness of the historically Catholic American woman's fraternity, Theta Phi Alpha.
Iconography.
The people of Siena wished to have St. Catherine's body. A story is told of a miracle whereby they were partially successful: knowing that they could not smuggle her whole body out of Rome, they decided to take only her head which they placed in a bag. When stopped by the Roman guards, they prayed to St Catherine to help them, confident that she would rather have her body (or at least part thereof) in Siena. When they opened the bag to show the guards, it appeared no longer to hold her head but to be full of rose petals. Once back at Siena, as they reopened the bag her head was visible once more. Due to this story, St Catherine is often seen holding a rose.
Legacy.
Catherine ranks high among the mystics and spiritual writers of the Church. She remains a greatly respected figure for her spiritual writings, and political boldness to "speak truth to power"— it being exceptional for a woman, in her time period, to have had such influence in politics and on world history.
The St. Catherine of Siena Medical Center is located in Smithtown, Long Island, New York. Only the church and a memorial garden survive of St Catherine's Convent in Bow, London, whose members moved to Stone, Staffordshire in 1926.
Modern editions and English translations.
English translations of The "Dialogue" include:
The Letters are translated into English as:
The Prayers are translated into English as:
Raymond of Capua's "Life" was translated into English in 1493 and 1609, and in Modern English is translated as:

</doc>
<doc id="7472" url="https://en.wikipedia.org/wiki?curid=7472" title="Charles Lyell">
Charles Lyell

Sir Charles Lyell, 1st Baronet, (14 November 1797 – 22 February 1875) was a British lawyer and the foremost geologist of his day. He is best known as the author of "Principles of Geology", which popularized James Hutton's concepts of uniformitarianism—the idea that the Earth was shaped by the same processes still in operation today. "Principles of Geology" also challenged theories popularized by Georges Cuvier, which were the most accepted and circulated ideas about geology in England at the time.
His scientific contributions included an explanation of earthquakes, the theory of gradual "backed up-building" of volcanoes, and in stratigraphy the division of the Tertiary period into the Pliocene, Miocene, and Eocene. He also coined the currently-used names for geological eras, Paleozoic, Mesozoic and Cenozoic. He wrongly conjectured that icebergs might transport glacial erratics, and that silty loess deposits might have settled out of flood waters.
Lyell was one of the first to believe that the world is older than 300 million years, on the basis of its geological anomalies. He was a close friend of Charles Darwin, and contributed significantly to Darwin's thinking on the processes involved in evolution. He helped to arrange the simultaneous publication in 1858 of papers by Darwin and Alfred Russel Wallace on natural selection, despite his personal religious qualms about the theory. He later published evidence from geology of the time man had existed on Earth.
Biography.
Lyell was born November 14, 1797 in Scotland about 15 miles north of Dundee in Kinnordy, near Kirriemuir in Forfarshire (now in Angus). He was the eldest of ten children. Lyell's father, also named Charles Lyell, was a lawyer and botanist of minor repute: it was he who first exposed his son to the study of nature.
The house/place of his birth is located in the north-west of the Central Lowlands in the valley of the Highland Boundary Fault. Round the house, in the rift valley, is farmland, but within a short distance to the north-west, on the other side of the fault, are the Grampian Mountains in the Highlands. His family's second home was in a completely different geological and ecological area: he spent much of his childhood at Bartley Lodge in the New Forest, England.
Lyell entered Exeter College, Oxford, in 1816, and attended William Buckland's lectures. He graduated BA second class in classics, December 1819, and M.A. 1821.
After graduation he took up law as a profession, entering Lincoln's Inn in 1820. He completed a circuit through rural England, where he could observe geological phenomena. In 1821 he attended Robert Jameson's lectures in Edinburgh, and visited Gideon Mantell at Lewes, in Sussex. In 1823 he was elected joint secretary of the Geological Society. As his eyesight began to deteriorate, he turned to geology as a full-time profession. His first paper, "On a recent formation of freshwater limestone in Forfarshire", was presented in 1822. By 1827, he had abandoned law and embarked on a geological career that would result in fame and the general acceptance of uniformitarianism, a working out of the ideas proposed by James Hutton a few decades earlier.
In 1832, Lyell married Mary Horner in Bonn, daughter of Leonard Horner (1785–1864), also associated with the Geological Society of London. The new couple spent their honeymoon in Switzerland and Italy on a geological tour of the area.
During the 1840s, Lyell travelled to the United States and Canada, and wrote two popular travel-and-geology books: "Travels in North America" (1845) and "A Second Visit to the United States" (1849). After the Great Chicago Fire, Lyell was one of the first to donate books to help found the Chicago Public Library. In 1866, he was elected a foreign member of the Royal Swedish Academy of Sciences.
Lyell's wife died in 1873, and two years later (in 1875) Lyell himself died as he was revising the twelfth edition of "Principles". He is buried in Westminster Abbey. Lyell was knighted (Kt) in 1848, and later, in 1864, made a baronet (Bt), which is an hereditary honour. He was awarded the Copley Medal of the Royal Society in 1858 and the Wollaston Medal of the Geological Society in 1866. Mount Lyell, the highest peak in Yosemite National Park, is named after him; the crater Lyell on the Moon and a crater on Mars were named in his honour; Mount Lyell in western Tasmania, Australia, located in a profitable mining area, bears Lyell's name; and the Lyell Range in north-west Western Australia is named for him as well. In Southwest Nelson in the South Island of New Zealand, the Lyell Range, Lyell River and the gold mining town of Lyell (now only a camping site) were all named after Lyell. The jawless fish "Cephalaspis lyelli", from the Old Red Sandstone of southern Scotland, was named by Louis Agassiz in honour of Lyell.
Career and major writings.
Lyell had private means, and earned further income as an author. He came from a prosperous family, worked briefly as a lawyer in the 1820s, and held the post of Professor of Geology at King's College London in the 1830s. From 1830 onward his books provided both income and fame. Each of his three major books was a work continually in progress. All three went through multiple editions during his lifetime, although many of his friends (such as Darwin) thought the first edition of the "Principles" was the best written. Lyell used each edition to incorporate additional material, rearrange existing material, and revisit old conclusions in light of new evidence.
"Principles of Geology", Lyell's first book, was also his most famous, most influential, and most important. First published in three volumes in 1830–33, it established Lyell's credentials as an important geological theorist and propounded the doctrine of uniformitarianism. It was a work of synthesis, backed by his own personal observations on his travels.
The central argument in "Principles" was that "the present is the key to the past" – a concept of the Scottish Enlightenment which David Hume had stated as "all inferences from experience suppose ... that the future will resemble the past", and James Hutton had described when he wrote in 1788 that "from what has actually been, we have data for concluding with regard to that which is to happen thereafter." Geological remains from the distant past can, and should, be explained by reference to geological processes now in operation and thus directly observable. Lyell's interpretation of geologic change as the steady accumulation of minute changes over enormously long spans of time was a powerful influence on the young Charles Darwin. Lyell asked Robert FitzRoy, captain of HMS "Beagle", to search for erratic boulders on the survey voyage of the "Beagle", and just before it set out FitzRoy gave Darwin Volume 1 of the first edition of Lyell's "Principles". When the "Beagle" made its first stop ashore at St Jago, Darwin found rock formations which seen "through Lyell's eyes" gave him a revolutionary insight into the geological history of the island, an insight he applied throughout his travels.
While in South America Darwin received Volume 2 which considered the ideas of Lamarck in some detail. Lyell rejected Lamarck's idea of organic evolution, proposing instead "Centres of Creation" to explain diversity and territory of species. However, as discussed below, many of his letters show he was fairly open to the idea of evolution. In geology Darwin was very much Lyell's disciple, and brought back observations and his own original theorising, including ideas about the formation of atolls, which supported Lyell's uniformitarianism. On the return of the "Beagle" (October 1836) Lyell invited Darwin to dinner and from then on they were close friends. Although Darwin discussed evolutionary ideas with him from 1842, Lyell continued to reject evolution in each of the first nine editions of the "Principles". He encouraged Darwin to publish, and following the 1859 publication of "On the Origin of Species", Lyell finally offered a tepid endorsement of evolution in the tenth edition of "Principles".
"Elements of Geology" began as the fourth volume of the third edition of "Principles": Lyell intended the book to act as a suitable field guide for students of geology. The systematic, factual description of geological formations of different ages contained in "Principles" grew so unwieldy, however, that Lyell split it off as the "Elements" in 1838. The book went through six editions, eventually growing to two volumes and ceasing to be the inexpensive, portable handbook that Lyell had originally envisioned. Late in his career, therefore, Lyell produced a condensed version titled "Student's Elements of Geology" that fulfilled the original purpose.
"Geological Evidences of the Antiquity of Man" brought together Lyell's views on three key themes from the geology of the Quaternary Period of Earth history: glaciers, evolution, and the age of the human race. First published in 1863, it went through three editions that year, with a fourth and final edition appearing in 1873. The book was widely regarded as a disappointment because of Lyell's equivocal treatment of evolution. Lyell, a devout Christian, had great difficulty reconciling his beliefs with natural selection.
Scientific contributions.
Lyell's geological interests ranged from volcanoes and geological dynamics through stratigraphy, palaeontology, and glaciology to topics that would now be classified as prehistoric archaeology and paleoanthropology. He is best known, however, for his role in popularising the doctrine of uniformitarianism.
Uniformitarianism.
From 1830 to 1833 his multi-volume "Principles of Geology" was published. The work's subtitle was "An attempt to explain the former changes of the Earth's surface by reference to causes now in operation", and this explains Lyell's impact on science. He drew his explanations from field studies conducted directly before he went to work on the founding geology text. He was, along with the earlier John Playfair, the major advocate of James Hutton's idea of uniformitarianism, that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. This was in contrast to catastrophism, a geologic idea of abrupt changes, which had been adapted in England to support belief in Noah's flood. Describing the importance of uniformitarianism on contemporary geology, Lyell wrote,
Never was there a doctrine more calculated to foster indolence, and to blunt the keen edge of curiosity, than this assumption of the discordance between the former and the existing causes of change... The student was taught to despond from the first. Geology, it was affirmed, could never arise to the rank of an exact science... catastrophism we see the ancient spirit of speculation revived, and a desire manifestly shown to cut, rather than patiently untie, the Gordian Knot.-Sir Charles Lyell, "Principles of Geology", 1854 edition, p.196; quoted by Stephen Jay Gould.
Lyell saw himself as "the spiritual saviour of geology, freeing the science from the old dispensation of Moses." The two terms, "uniformitarianism" and "catastrophism", were both coined by William Whewell; in 1866 R. Grove suggested the simpler term "continuity" for Lyell's view, but the old terms persisted. In various revised editions (12 in all, through 1872), "Principles of Geology" was the most influential geological work in the middle of the 19th century, and did much to put geology on a modern footing. For his efforts he was knighted in 1848, then made a baronet in 1864.
Geological Surveys.
Lyell noted the "economic advantages" that geological surveys could provide, citing their felicity in mineral-rich countries and provinces. Modern surveys, like the US Geological Survey, map and exhibit the natural resources within the country. So, in endorsing surveys, as well as advancing the study of geology, Lyell helped to forward the business of modern extractive industries, such as the coal and oil industry.
Volcanoes and geological dynamics.
Before the work of Lyell, phenomena such as earthquakes were understood by the destruction that they brought. One of the contributions that Lyell made in "Principles" was to explain the cause of earthquakes. Lyell, in contrast focused on recent earthquakes (150 yrs), evidenced by surface irregularities such as faults, fissures, stratigraphic displacements and depressions.
Lyell's work on volcanoes focused largely on Vesuvius and Etna, both of which he had earlier studied. His conclusions supported gradual building of volcanoes, so-called "backed up-building", as opposed to the upheaval argument supported by other geologists.
Stratigraphy.
Lyell's most important specific work was in the field of stratigraphy. From May 1828, until February 1829, he travelled with Roderick Impey Murchison (1792–1871) to the south of France (Auvergne volcanic district) and to Italy. In these areas he concluded that the recent strata (rock layers) could be categorised according to the number and proportion of marine shells encased within. Based on this he proposed dividing the Tertiary period into three parts, which he named the Pliocene, Miocene, and Eocene. He also renamed the traditional "Primary", "Secondary" and "Tertiary" periods (now called eras) to Paleozoic, Mesozoic and Cenozoic, which nomenclature was gradually accepted worldwide.
Glaciers.
In "Principles of Geology" (first edition, vol. 3, Ch. 2, 1833) Lyell proposed that icebergs could be the means of transport for erratics. During periods of global warming, ice breaks off the poles and floats across submerged continents, carrying debris with it, he conjectured. When the iceberg melts, it rains down sediments upon the land. Because this theory could account for the presence of diluvium, the word "drift" became the preferred term for the loose, unsorted material, today called "till". Furthermore, Lyell believed that the accumulation of fine angular particles covering much of the world (today called loess) was a deposit settled from mountain flood water. Today some of Lyell's mechanisms for geologic processes have been disproven, though many have stood the test of time. His observational methods and general analytical framework remain in use today as foundational principles in geology.
Evolution.
Lyell first received a copy of one of Lamarck's books from Mantell in 1827, when he was on circuit. He thanked Mantell in a letter which includes this enthusiastic passage:
In the second volume of the first edition of "Principles" Lyell explicitly rejected the "mechanism" of Lamark on the transmutation of species, and was doubtful whether species were mutable. However, privately, in letters, he was more open to the possibility of evolution:
This letter makes it clear that his equivocation on evolution was, at least at first, a deliberate tactic. As a result of his letters and, no doubt, personal conversations, Huxley and Haeckel were convinced that, at the time he wrote "Principles", he believed new species had arisen by natural methods. Both Whewell and Sedgwick wrote worried letters to him about this.
During the "Beagle" survey expedition from 1831 to 1836, Darwin read Lyell's "Principles" as they were published, and made geological findings supporting Lyell's ideas. On return, he became a close personal friend, and Lyell was one of the first scientists to support "On the Origin of Species", though he did not subscribe to all its contents. Lyell was also a friend of Darwin's closest colleagues, Hooker and Huxley, but unlike them he struggled to square his religious beliefs with evolution. This inner struggle has been much commented on. He had particular difficulty in believing in natural selection as the main motive force in evolution.
Lyell and Hooker were instrumental in arranging the peaceful co-publication of the theory of natural selection by Darwin and Alfred Russel Wallace in 1858: each had arrived at the theory independently. Lyell's data on stratigraphy were important because Darwin thought that populations of an organism changed slowly, requiring "geologic time".
Although Lyell did not publicly accept evolution (descent with modification) at the time of writing the "Principles", after the Darwin–Wallace papers and the "Origin" Lyell wrote in his notebook:
Lyell's acceptance of natural selection, Darwin's proposed mechanism for evolution, was equivocal, and came in the tenth edition of "Principles". "The Antiquity of Man" (published in early February 1863, just before Huxley's "Man's place in nature") drew these comments from Darwin to Huxley:
Quite strong remarks: no doubt Darwin resented Lyell's repeated suggestion that he owed a lot to Lamarck, whom he (Darwin) had always specifically rejected. Darwin's daughter Henrietta (Etty) wrote to her father: "Is it fair that Lyell always calls your theory a modification of Lamarck's?" 
In other respects "Antiquity" was a success. It sold well, and it "shattered the tacit agreement that mankind should be the sole preserve of theologians and historians". But when Lyell wrote that it remained a profound mystery how the huge gulf between man and beast could be bridged, Darwin wrote "Oh!" in the margin of his copy.
Legacy.
Places named after Lyell:

</doc>
<doc id="7473" url="https://en.wikipedia.org/wiki?curid=7473" title="Chelsea F.C.">
Chelsea F.C.

Chelsea Football Club () is a professional football club based in Fulham, London, that competes in the Premier League of England. Founded in 1905, the club's home ground since then has been Stamford Bridge.
Chelsea had their first major success in 1955, when they won the league championship. They won various cup competitions between 1965 and 1990. The club's greatest period of success has been the last two decades, winning 17 major trophies since 1997. Chelsea have won five national league titles, seven FA Cups, five League Cups and four FA Community Shields, one UEFA Champions League, two UEFA Cup Winners' Cups, one UEFA Europa League and one UEFA Super Cup. Chelsea are the only London club to win the UEFA Champions League, and one of four clubs, and the only British club, to have won all three main UEFA club competitions.
Chelsea's regular kit colours are royal blue shirts and shorts with white socks. The club's crest has been changed several times in attempts to re-brand the club and modernise its image. The current crest, featuring a ceremonial lion rampant regardant holding a staff, is a modification of the one introduced in the early 1950s. The club have the sixth-highest average all-time attendance in English football. Their average home gate for the 2014–15 season was 41,546, the seventh highest in the Premier League. Since 2003, Chelsea have been owned by Russian billionaire Roman Abramovich. In 2015, they were ranked by "Forbes" magazine as the sixth most valuable football club in the world, at £898 million ($1.37 billion).
History.
In 1904, Gus Mears acquired the Stamford Bridge athletics stadium with the aim of turning it into a football ground. An offer to lease it to nearby Fulham was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like "Kensington FC", "Stamford Bridge FC" and "London FC" were also considered. Chelsea were founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook), opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards.
The club won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in their early years. They reached the 1915 FA Cup Final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point. Chelsea attracted large crowds and had a reputation for signing big-name players, but success continued to elude the club in the inter-war years.
Former Arsenal and England centre-forward Ted Drake became manager in 1952 and proceeded to modernise the club. He removed the club's Chelsea pensioner crest, improved the youth set-up and training regime, rebuilt the side with shrewd signings from the lower divisions and amateur leagues, and led Chelsea to their first major trophy success – the League championship – in 1954–55. The following season saw UEFA create the European Champions' Cup, but after objections from The Football League and the FA Chelsea were persuaded to withdraw from the competition before it started. Chelsea failed to build on this success, and spent the remainder of the 1950s in mid-table. Drake was dismissed in 1961 and replaced by player-coach Tommy Docherty.
Docherty built a new team around the group of talented young players emerging from the club's youth set-up and Chelsea challenged for honours throughout the 1960s, enduring several near-misses. They were on course for a treble of League, FA Cup and League Cup going into the final stages of the 1964–65 season, winning the League Cup but faltering late on in the other two. In three seasons the side were beaten in three major semi-finals and were FA Cup runners-up. Under Docherty's successor, Dave Sexton, Chelsea won the FA Cup in 1970, beating Leeds United 2–1 in a final replay. Chelsea took their first European honour, a UEFA Cup Winners' Cup triumph, the following year, with another replayed win, this time over Real Madrid in Athens.
The late 1970s through to the 1980s was a turbulent period for Chelsea. An ambitious redevelopment of Stamford Bridge threatened the financial stability of the club, star players were sold and the team were relegated. Further problems were caused by a notorious hooligan element among the support, which was to plague the club throughout the decade. In 1982, Chelsea were, at the nadir of their fortunes, acquired by Ken Bates for the nominal sum of £1, although by now the Stamford Bridge freehold had been sold to property developers, meaning the club faced losing their home. On the pitch, the team had fared little better, coming close to relegation to the Third Division for the first time, but in 1983 manager John Neal put together an impressive new team for minimal outlay. Chelsea won the Second Division title in 1983–84 and established themselves in the top division, before being relegated again in 1988. The club bounced back immediately by winning the Second Division championship in 1988–89.
After a long-running legal battle, Bates reunited the stadium freehold with the club in 1992 by doing a deal with the banks of the property developers, who had been bankrupted by a market crash. Chelsea's form in the new Premier League was unconvincing, although they did reach the 1994 FA Cup Final with Glenn Hoddle. It was not until the appointment of Ruud Gullit as player-manager in 1996 that their fortunes changed. He added several top international players to the side, as the club won the FA Cup in 1997 and established themselves as one of England's top sides again. Gullit was replaced by Gianluca Vialli, who led the team to victory in the League Cup Final, the UEFA Cup Winners' Cup Final and the UEFA Super Cup in 1998, the FA Cup in 2000 and their first appearance in the UEFA Champions League. Vialli was sacked in favour of Claudio Ranieri, who guided Chelsea to the 2002 FA Cup Final and Champions League qualification in 2002–03.
In June 2003, Bates sold Chelsea to Russian billionaire Roman Abramovich for £140 million. Over £100 million was spent on new players, but Ranieri was unable to deliver any trophies, and was replaced by José Mourinho. Under Mourinho, Chelsea became the fifth English team to win back-to-back league championships since the Second World War (2004–05 and 2005–06), in addition to winning an FA Cup (2007) and two League Cups (2005 and 2007). Mourinho was replaced by Avram Grant, who led the club to their first UEFA Champions League final, which they lost on penalties to Manchester United.
In 2009, Guus Hiddink guided Chelsea to another FA Cup success. In 2009–10, his successor Carlo Ancelotti led them to their first Premier League and FA Cup "Double", and becoming the first English top-flight club to score 100 league goals in a season since 1963. In 2012, caretaker manager Roberto Di Matteo led Chelsea to their seventh FA Cup, and their first UEFA Champions League title, beating Bayern Munich 4–3 on penalties, the first London club to win the trophy. In 2013, interim manager Rafael Benítez guided Chelsea to win the UEFA Europa League against Benfica, becoming the first club to hold two major European titles simultaneously and one of four clubs, and the only British club, to have won all three of UEFA's major club competitions. In the summer of 2013, Mourinho returned as manager, leading Chelsea to League Cup success in March 2015, and their fifth league title two months later.
Stadium.
Chelsea have only had one home ground, Stamford Bridge, where they have played since the team's foundation. It was officially opened on 28 April 1877 and for the first 28 years of its existence it was used almost exclusively by the London Athletic Club as an arena for athletics meetings and not at all for football. In 1904 the ground was acquired by businessman Gus Mears and his brother Joseph, who had also purchased nearby land (formerly a large market garden) with the aim of staging football matches on the now 12.5 acre (51,000 m²) site. Stamford Bridge was designed for the Mears family by the noted football architect Archibald Leitch, who had also designed Ibrox, Craven Cottage and Hampden Park. Most football clubs were founded first, and then sought grounds in which to play, but Chelsea were founded for Stamford Bridge.
Starting with an open bowl-like design and one covered terrace, Stamford Bridge had an original capacity of around 100,000. The early 1930s saw the construction of a terrace on the southern part of the ground with a roof that covered around one fifth of the stand. It eventually became known as the "Shed End", the home of Chelsea's most loyal and vocal supporters, particularly during the 1960s, 70s and 80s. The exact origins of the name are unclear, but the fact that the roof looked like a corrugated iron shed roof played a part.
In the early 1970s, the club's owners announced a modernisation of Stamford Bridge with plans for a state-of-the-art 50,000 all-seater stadium. Work began on the East Stand in 1972 but the project was beset with problems and was never completed; the cost brought the club close to bankruptcy, culminating in the freehold being sold to property developers. Following a long legal battle, it was not until the mid-1990s that Chelsea's future at the stadium was secured and renovation work resumed. The north, west and southern parts of the ground were converted into all-seater stands and moved closer to the pitch, a process completed by 2001.
When Stamford Bridge was redeveloped in the Bates era many additional features were added to the complex including two hotels, apartments, bars, restaurants, the Chelsea Megastore, and an interactive visitor attraction called Chelsea World of Sport. The intention was that these facilities would provide extra revenue to support the football side of the business, but they were less successful than hoped and before the Abramovich takeover in 2003 the debt taken on to finance them was a major burden on the club. Soon after the takeover a decision was taken to drop the "Chelsea Village" brand and refocus on Chelsea as a football club. However, the stadium is sometimes still referred to as part of ""Chelsea Village"" or ""The Village"".
The Stamford Bridge freehold, the pitch, the turnstiles and Chelsea's naming rights are now owned by Chelsea Pitch Owners, a non-profit organisation in which fans are the shareholders. The CPO was created to ensure the stadium could never again be sold to developers. As a condition for using the Chelsea FC name, the club has to play its first team matches at Stamford Bridge, which means that if the club moves to a new stadium, they may have to change their name.
Chelsea's training ground is located in Cobham, Surrey. Chelsea moved to Cobham in 2004. Their previous training ground in Harlington was taken over by QPR in 2005. The new training facilities in Cobham were completed in 2007.
Stamford Bridge has been used for a variety of other sporting events since 1905. It hosted the FA Cup Final from 1920 to 1922, has held ten FA Cup semi-finals (most recently in 1978), ten FA Charity Shield matches (the last in 1970), and three England international matches, the last in 1932; it was also the venue for an unofficial "Victory International" in 1946. The 2013 UEFA Women's Champions League Final was played at Stamford Bridge. In October 1905 it hosted a rugby union match between the All Blacks and Middlesex, and in 1914 hosted a baseball match between the touring New York Giants and the Chicago White Sox. It was the venue for a boxing match between world flyweight champion Jimmy Wilde and Joe Conn in 1918. The running track was used for dirt track racing between 1928 and 1932, greyhound racing from 1933 to 1968, and Midget car racing in 1948. In 1980, Stamford Bridge hosted the first international floodlit cricket match in the UK, between Essex and the West Indies. It was also the home stadium of the London Monarchs American Football team for the 1997 season.
The current club ownership have stated that a larger stadium is necessary in order for Chelsea to stay competitive with rival clubs who have significantly larger stadia, such as Arsenal and Manchester United. Owing to its location next to a main road and two railway lines, fans can only enter the ground via the Fulham Road exits, which places constraints on expansion due to health and safety regulations. The club have consistently affirmed their desire to keep Chelsea at their current home, but have nonetheless been linked with a move to various nearby sites, including the Earls Court Exhibition Centre, Battersea Power Station and the Chelsea Barracks. In October 2011, a proposal from the club to buy back the freehold to the land on which Stamford Bridge sits was voted down by Chelsea Pitch Owners shareholders. In May 2012, the club made a formal bid to purchase Battersea Power Station, with a view to developing the site into a new stadium, but lost out to a Malaysian consortium. The club subsequently announced plans to redevelop Stamford Bridge into a 60,000 seater stadium.
Crest and colours.
Crest.
Chelsea have had four main crests, which all underwent minor variations. The first, adopted when the club was founded, was the image of a Chelsea pensioner, the army veterans who reside at the nearby Royal Hospital Chelsea. This contributed to the club's original "pensioner" nickname, and remained for the next half-century, though it never appeared on the shirts. When Ted Drake became Chelsea manager in 1952, he began to modernise the club. Believing the Chelsea pensioner crest to be old-fashioned, he insisted that it be replaced. A stop-gap badge which comprised the initials C.F.C. was adopted for a year. In 1953, the club crest was changed to an upright blue lion looking backwards and holding a staff. It was based on elements in the coat of arms of the Metropolitan Borough of Chelsea with the "lion rampant regardant" taken from the arms of then club president Viscount Chelsea and the staff from the Abbots of Westminster, former Lords of the Manor of Chelsea. It also featured three red roses, to represent England, and two footballs. This was the first Chelsea crest to appear on the shirts, in the early 1960s.
In 1986, with Ken Bates now owner of the club, Chelsea's crest was changed again as part of another attempt to modernise and because the old rampant lion badge could not be trademarked. The new badge featured a more naturalistic non-heraldic lion, in white and not blue, standing over the C.F.C. initials. This lasted for the next 19 years, with some modifications such as the use of different colours, including red from 1987 to 1995, and yellow from 1995 until 1999, before the white returned. With the new ownership of Roman Abramovich, and the club's centenary approaching, combined with demands from fans for the popular 1950s badge to be restored, it was decided that the crest should be changed again in 2005. The new crest was officially adopted for the start of the 2005–06 season and marked a return to the older design, used from 1953 to 1986, featuring a blue heraldic lion holding a staff. For the centenary season this was accompanied by the words '100 YEARS' and 'CENTENARY 2005–2006' on the top and bottom of the crest respectively.
Colours.
Chelsea have always worn blue shirts, although they originally used the paler eton blue, which was taken from the racing colours of then club president, Earl Cadogan, and was worn with white shorts and dark blue or black socks. The light blue shirts were replaced by a royal blue version in around 1912. In the 1960s Chelsea manager Tommy Docherty changed the kit again, switching to blue shorts (which have remained ever since) and white socks, believing it made the club's colours more modern and distinctive, since no other major side used that combination; this kit was first worn during the 1964–65 season. Since then Chelsea have always worn white socks with their home kit apart from a short spell from 1985 to 1992, when blue socks were reintroduced.
Chelsea's away colours are usually all yellow or all white with blue trim. More recently, the club have had a number of black or dark blue away kits. As with most teams, they have also had some more unusual ones. At Docherty's behest, in the 1966 FA Cup semi-final they wore blue and black stripes, based on Inter Milan's kit. In the mid-1970s, the away strip was a red, white and green kit inspired by the Hungarian national side of the 1950s. Other memorable away kits include an all jade strip worn from 1986–89, red and white diamonds from 1990–92, graphite and tangerine from 1994–96, and luminous yellow from 2007–08. The graphite and tangerine strip often appears in lists of the worst football kits ever.
Support.
Chelsea are one of the most widely supported football clubs in the world. They have the sixth highest average all-time attendance in English football and regularly attract over 40,000 fans to Stamford Bridge; they were the seventh best-supported Premier League team in the 2013–14 season, with an average gate of 41,572. Between 2007 and 2012, Chelsea were ranked fourth worldwide in annual replica kit sales, with an average of 910,000. Chelsea's official Twitter account has 6.29 million followers, the fifth highest among football clubs.
At matches, Chelsea fans sing chants such as ""Carefree"" (to the tune of Lord of the Dance, whose lyrics were probably written by supporter Mick Greenaway), ""Ten Men Went to Mow"", ""We All Follow the Chelsea"" (to the tune of "Land of Hope and Glory"), ""Zigga Zagga"", and the celebratory ""Celery"", with the latter often resulting in fans ritually throwing celery. The vegetable was banned inside Stamford Bridge after an incident involving Arsenal midfielder Cesc Fàbregas at the 2007 League Cup Final.
During the 1970s and 1980s in particular, Chelsea supporters were associated with football hooliganism. The club's "football firm", originally known as the Chelsea Shed Boys, and subsequently as the Chelsea Headhunters, were nationally notorious for football violence, alongside hooligan firms from other clubs such as West Ham United's Inter City Firm and Millwall's Bushwackers, before, during and after matches. The increase of hooligan incidents in the 1980s led chairman Ken Bates to propose erecting an electric fence to deter them from invading the pitch, a proposal that the Greater London Council rejected.
Since the 1990s, there has been a marked decline in crowd trouble at matches, as a result of stricter policing, CCTV in grounds and the advent of all-seater stadia. In 2007, the club launched the 'Back to the Shed' campaign to improve the atmosphere at home matches, with notable success. According to Home Office statistics, 126 Chelsea fans were arrested for football-related offences during the 2009–10 season, the third highest in the division, and 27 banning orders were issued, the fifth-highest in the division.
Rivalries.
Chelsea have long-standing rivalries with North London clubs Arsenal and Tottenham Hotspur. A strong rivalry with Leeds United dates back to several heated and controversial matches in the 1960s and 1970s, particularly the 1970 FA Cup Final. More recently a rivalry with Liverpool has grown following repeated clashes in cup competitions. Chelsea's fellow West London sides Brentford, Fulham and Queens Park Rangers are generally not considered major rivals, as matches have only taken place intermittently due to the clubs often being in separate divisions. A 2004 survey by "Planetfootball.com" found that Chelsea fans consider their main rivalries to be with (in order): Arsenal, Tottenham Hotspur and Manchester United. In the same survey, fans of six clubs (Arsenal, Fulham, Leeds United, QPR, Tottenham and West Ham United) named Chelsea as one of their three main rivals. In a 2008 poll conducted by the Football Fans Census, Chelsea fans named Liverpool, Arsenal and Manchester United as their most disliked clubs.
Records.
Chelsea's highest appearance-maker is ex-captain Ron Harris, who played in 795 competitive games for the club between 1961 and 1980. The record for a Chelsea goalkeeper is held by Harris's contemporary, Peter Bonetti, who made 729 appearances (1959–79). With 103 caps (101 while at the club), Frank Lampard of England is Chelsea's most capped international player.
Frank Lampard is Chelsea's all-time top goalscorer, with 211 goals in 648 games (2001–2014); he passed Bobby Tambling's longstanding record of 202 in May 2013. Seven other players have also scored over 100 goals for Chelsea: George Hilsdon (1906–12), George Mills (1929–39), Roy Bentley (1948–56), Jimmy Greaves (1957–61), Peter Osgood (1964–74 and 1978–79), Kerry Dixon (1983–92) and Didier Drogba (2004–12 and 2014–2015). Greaves holds the record for the most goals scored in one season (43 in 1960–61).
Chelsea's biggest winning scoreline in a competitive match is 13–0, achieved against Jeunesse Hautcharage in the Cup Winners' Cup in 1971. The club's biggest top-flight win was an 8–0 victory against Wigan Athletic in 2010, which was matched in 2012 against Aston Villa. Chelsea's biggest loss was an 8–1 reverse against Wolverhampton Wanderers in 1953. Officially, Chelsea's highest home attendance is 82,905 for a First Division match against Arsenal on 12 October 1935. However, an estimated crowd of over 100,000 attended a friendly match against Soviet team Dynamo Moscow on 13 November 1945. The modernisation of Stamford Bridge during the 1990s and the introduction of all-seater stands mean that neither record will be broken for the foreseeable future. The current legal capacity of Stamford Bridge is 41,837. Every starting player in Chelsea's 57 games of the 2013–14 season was a full international – a new club record.
Chelsea hold the English record for the highest ever points total for a league season (95), the fewest goals conceded during a league season (15), the highest number of Premier League victories in a season (29), the highest number of clean sheets overall in a Premier League season (25) (all set during the 2004–05 season), and the most consecutive clean sheets from the start of a league season (6, set during the 2005–06 season). The club's 21–0 aggregate victory over Jeunesse Hautcharage in the UEFA Cup Winners' Cup in 1971 remains a record in European competition. Chelsea hold the record for the longest streak of unbeaten matches at home in the English top-flight, which lasted 86 matches from 20 March 2004 to 26 October 2008. They secured the record on 12 August 2007, beating the previous record of 63 matches unbeaten set by Liverpool between 1978 and 1980. Chelsea's streak of eleven consecutive away league wins, set between 5 April 2008 and 6 December 2008, is also a record for the English top flight. Their £50 million purchase of Fernando Torres from Liverpool in January 2011 was the record transfer fee paid by a British club until Ángel di María signed for Manchester United in August 2014 for £59.7 million.
Chelsea, along with Arsenal, were the first club to play with shirt numbers, on 25 August 1928 in their match against Swansea Town. They were the first English side to travel by aeroplane to a domestic away match, when they visited Newcastle United on 19 April 1957, and the first First Division side to play a match on a Sunday, when they faced Stoke City on 27 January 1974. On 26 December 1999, Chelsea became the first British side to field an entirely foreign starting line-up (no British or Irish players) in a Premier League match against Southampton.
In May 2007, Chelsea were the first team to win the FA Cup at the new Wembley Stadium, having also been the last to win it at the old Wembley. They were the first English club to be ranked #1 under UEFA's five-year coefficient system in the 21st century. They were the first team in Premier League history to score at least 100 goals in a single season, reaching the milestone on the final day of the 2009–10 season. Chelsea are the only London club to win the UEFA Champions League, after beating Bayern Munich in the 2012 final. Upon winning the 2012–13 UEFA Europa League, Chelsea became the first English club to win all four European trophies and the only club to hold the Champions League and the Europa League at the same time.
Ownership and finances.
Chelsea Football Club were founded by Gus Mears in 1905. After his death in 1912, his descendents continued to own the club until 1982, when Ken Bates bought the club from Mears' great-nephew Brian Mears for £1. Bates bought a controlling stake in the club and floated Chelsea on the AIM stock exchange in March 1996. In July 2003, Roman Abramovich purchased just over 50% of Chelsea Village plc's share capital, including Bates' 29.5% stake, for £30 million and over the following weeks bought out most of the remaining 12,000 shareholders at 35 pence per share, completing a £140 million takeover. Other shareholders at the time of the takeover included the Matthew Harding estate (21%), BSkyB (9.9%) and various anonymous offshore trusts. After passing the 90% share threshold, Abramovich took the club back into private hands, delisting it from the AIM on 22 August 2003. He also took on responsibility for the club's debt of £80 million, quickly paying most of it.
Thereafter, Abramovich changed the ownership name to Chelsea FC plc, whose ultimate parent company is Fordstam Limited, which is controlled by him. Chelsea are additionally funded by Abramovich via interest free soft loans channelled through his holding company Fordstam Limited. The loans stood at £709 million in December 2009, when they were all converted to equity by Abramovich, leaving the club themselves debt free, although the debt remains with Fordstam. Since 2008 the club have had no external debt.
Chelsea did not turn a profit in the first nine years of Abramovich's ownership, and made record losses of £140m in June 2005. In November 2012, Chelsea announced a profit of £1.4 million for the year ending 30 June 2012, the first time the club had made a profit under Abramovich's ownership. This was followed by a loss in 2013 and then their highest ever profit of £18.4 million for the year to June 2014.
Chelsea have been described as a global brand; a 2012 report by Brand Finance ranked Chelsea fifth among football brands and valued the club's brand value at US $398 million – an increase of 27% from the previous year, also valuing them at US $10 million more than the sixth best brand, London rivals Arsenal – and gave the brand a strength rating of AA (very strong). In 2015, "Forbes" magazine ranked Chelsea the sixth most valuable football club in the world, at £898 million ($1.37 billion). As of 2016, Chelsea are ranked eighth in the Deloitte Football Money League with an annual commercial revenue of £322.59 million.
Sponsorship.
Chelsea's kit has been manufactured by Adidas since 2006, which is contracted to supply the club's kit from 2006 to 2018. The partnership was extended in October 2010 in a deal worth £160 million over eight years. This deal was again extended in June 2013 in a deal worth £300 million over another ten years. Previously, the kit was manufactured by Umbro (1975–81), Le Coq Sportif (1981–86), The Chelsea Collection (1986–87) and Umbro again (1987–2006).
Chelsea's first shirt sponsor was Gulf Air, agreed during the 1983–84 season. The club were then sponsored by Grange Farms, Bai Lin Tea and Simod before a long-term deal was signed with Commodore International in 1989; Amiga, an offshoot of Commodore, also appeared on the shirts. Chelsea were subsequently sponsored by Coors beer (1994–97), Autoglass (1997–2001), Emirates (2001–05), Samsung Mobile (2005–08) and Samsung (2008–15). Chelsea's current shirt sponsor is the Yokohama Rubber Company. Worth £40 million-per-year, the deal is second in English football to Chevrolet's £50 million-per-year sponsorship of Manchester United.
The club has a variety of other sponsors and official partners, which include Gazprom, Delta Air Lines, Sauber, Audi, Singha, EA Sports, Dolce & Gabbana, Barbados Tourism Authority, Atlas, AZIMUT Hotels, BNI, Indosat, Vietinbank, Nitto Tire, Orico, Guangzhou R&F, Coca-Cola, Grand Royal, Digicel, Lucozade Sport, and Viagogo.
Popular culture.
In 1930, Chelsea featured in one of the earliest football films, "The Great Game". One-time Chelsea centre forward, Jack Cock, who by then was playing for Millwall, was the star of the film and several scenes were shot at Stamford Bridge, including the pitch, the boardroom, and the dressing rooms. It included guest appearances by then-Chelsea players Andrew Wilson, George Mills, and Sam Millington. Owing to the notoriety of the Chelsea Headhunters, a football firm associated with the club, Chelsea have also featured in films about football hooliganism, including 2004's "The Football Factory". Chelsea also appear in the Hindi film "Jhoom Barabar Jhoom". In April 2011, Montenegrin comedy series "Nijesmo mi od juče" made an episode in which Chelsea play against FK Sutjeska Nikšić for qualification of the UEFA Champions League.
Up until the 1950s, the club had a long-running association with the music halls; their underachievement often provided material for comedians such as George Robey. It culminated in comedian Norman Long's release of a comic song in 1933, ironically titled "On the Day That Chelsea Went and Won the Cup", the lyrics of which describe a series of bizarre and improbable occurrences on the hypothetical day when Chelsea finally won a trophy. In Alfred Hitchcock's 1935 film "The 39 Steps", Mr Memory claims that Chelsea last won the Cup in 63 BC, "in the presence of the Emperor Nero." Scenes in a 1980 episode of "Minder" were filmed during a real match at Stamford Bridge between Chelsea and Preston North End with Terry McCann (played by Dennis Waterman) standing on the terraces.
The song "Blue is the Colour" was released as a single in the build-up to the 1972 League Cup Final, with all members of Chelsea's first team squad singing; it reached number five in the UK Singles Chart. The song has since been adopted as an anthem by a number of other sports teams around the world, including the Vancouver Whitecaps (as "White is the Colour") and the Saskatchewan Roughriders (as "Green is the Colour"). In the build-up to the 1997 FA Cup Final, the song "Blue Day", performed by Suggs and members of the Chelsea squad, reached number 22 in the UK charts. Bryan Adams, a fan of Chelsea, dedicated the song "We're Gonna Win" from the album "18 Til I Die" to the club.
Chelsea Ladies.
Chelsea also operate a women's football team, Chelsea Ladies. They have been affiliated to the men's team since 2004 and are part of the club's Community Development programme. They play their home games at Wheatsheaf Park, the home ground of Conference South club Staines Town. The club were promoted to the Premier Division for the first time in 2005 as Southern Division champions and won the Surrey County Cup in 2003–04, 2006–10, 2012, and 2013. In 2010 Chelsea Ladies were one of the eight founder members of the FA Women's Super League. In 2015, Chelsea Ladies won the FA Women's Cup for the first time, beating Notts County Ladies at Wembley Stadium, and a month later clinched their first FA WSL title to complete a league and cup double. John Terry, the current captain of the Chelsea men's team, is the President of Chelsea Ladies.
Players.
First team squad.
"For recent transfers, see 2015–16 Chelsea F.C. season."
Reserves and Academy.
"For further information: Chelsea F.C. Reserves and Academy"
Player of the Year.
Source: Chelsea F.C.
Notable managers.
The following managers won at least one trophy when in charge of Chelsea:
Management team.
!Position
!Staff
Club personnel.
Chelsea FC plc is the company which owns Chelsea Football Club. The ultimate parent company of Chelsea FC plc is Fordstam Limited and the ultimate controlling party of Fordstam Limited is Roman Abramovich.
On 22 October 2014, Chelsea announced that Ron Gourlay, after ten successful years at the club including five as Chief Executive, is leaving Chelsea in order to pursue new business opportunities. On 27 October 2014, Chelsea announced that Christian Purslow is joining the club to run global commercial activities and the club do not expect to announce any other senior appointments in the near future having Chairman Bruce Buck and Director Marina Granovskaia assumed the executive responsibilities.
Chelsea Ltd.
Chelsea F.C. plc Board
Executive Board
Life President
Vice-Presidents
Source: Chelsea F.C.
Honours.
Upon winning the 2012–13 UEFA Europa League, Chelsea became the fourth club in history to have won the "European Treble" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax and Bayern Munich. Chelsea are the first English club to have won all three major UEFA trophies.
European.
Source: Chelsea F.C.

</doc>
<doc id="7475" url="https://en.wikipedia.org/wiki?curid=7475" title="CANDU reactor">
CANDU reactor

The CANDU reactor (short for CANada Deuterium Uranium) is a Canadian-developed, pressurized heavy water reactor used for generating electric power. The acronym refers to its deuterium-oxide (heavy water) moderator and its use of (originally, natural) uranium fuel. CANDU reactors were first developed in the late 1950s and 1960s by a partnership between Atomic Energy of Canada Limited (AECL), the Hydro-Electric Power Commission of Ontario, Canadian General Electric, and other companies.
All power reactors built in Canada are of the CANDU type. The reactor is also marketed abroad and there are CANDU-type units operating in India, Pakistan, Argentina, South Korea, Romania and China. In October 2011, the Canadian Federal Government licensed the CANDU design to Candu Energy (a wholly owned subsidiary of SNC-Lavalin), which also acquired the former reactor development and marketing division of AECL at that time.
Design.
Basic design and operation.
[[File:CANDU Reactor Schematic.svg|thumb|400px|Schematic diagram of a CANDU reactor: The primary heavy-water loop is in yellow and orange, the secondary light-water loop in blue and red. The cool heavy water moderator in the calandria can be seen in pink, along with partially inserted adjuster rods (as CANDU control rods are known).
Fission reactions in the reactor core heat pressurized heavy water in a "primary cooling loop". A heat exchanger, also known as a steam generator, transfers the heat to a light-water "secondary cooling loop", which powers a steam turbine with an electrical generator attached to it (for a typical Rankine thermodynamic cycle). The exhaust steam from the turbines is then condensed and returned as feedwater to the steam generator, often using cooling water from a nearby source, such as a lake, river, or ocean. Newer CANDU plants, such as the Darlington Nuclear Generating Station near Toronto, Ontario, use a diffuser to spread the warm outlet water over a larger volume and limit the effects on the environment. A cooling tower can be used, but it reduces efficiency and increases costs considerably.
Some of the unique features of the CANDU design are listed below:
In a light water reactor (LWR), the entire reactor core is a single large pressure vessel containing the light water, which acts as moderator and coolant, and the fuel arranged in a series of long bundles running the length of the core. At the time of CANDU's design, Canada lacked the heavy industry to cast and machine the pressure vessels. In CANDU the pressure (and the fuel bundles) is contained in much smaller (10 cm diameter), easier-to-fabricate tubes. Each bundle is a cylinder assembled from alloy tubes containing ceramic pellets of fuel. In older designs the assembly had 28 or 37 half-meter-long fuel tubes with 12 such assemblies lying end to end in a pressure tube. The newer CANFLEX bundle has 43 tubes, with two pellet sizes (so the power rating can be increased without melting the hottest pellets). It is about in diameter, long and weighs about and replaces the 37-tube bundle. To allow the neutrons to flow freely between the bundles, the tubes and bundles are made of neutron-transparent zircaloy (zirconium + 2.5% wt niobium).
The zircaloy tubes are surrounded by a much larger low-pressure tank known as a calandria, which contains the majority of the moderator. To keep the hot coolant from boiling the moderator, a calandria tube surrounds each pressure tube, with insulating carbon dioxide gas in between. Slowing down neutrons releases energy, so a cooling system dissipates the heat. The moderator is actually a large heat sink that acts as an additional safety feature. The use of individual high pressure fuel channels passing through the CANDU's low-pressure moderator calandria makes it easier to refuel: a pressure-vessel reactor must be shut down, the pressure dropped, the lid removed, and a sizeable fraction of the fuel, e.g. one-third, replaced all at once. In CANDU, individual channels can be refuelled without taking the reactor offline, improving the capacity factor. One fueling machine inserts new fuel into one end of the channel while the other receives discharged fuel from the opposite end. One significant operational advantage of online refuelling is that a failed or leaking fuel bundle can be removed from the core once it has been located, thus reducing the radiation fields in the primary systems.
Purpose of using heavy water.
Natural uranium is a mix of isotopes—mainly uranium-238, with 0.72% (by weight) fissile uranium-235. A reactor aims for a steady rate of fission over time (criticality), where the neutrons released by fission cause an equal number of fissions in other atoms. These neutrons are fairly energetic and don't readily react with (get "captured" by) the surrounding fissile material—they must have their energy "moderated" (i.e., be slowed down ) as much as possible, ideally to the same energy as the atoms themselves ("thermal neutrons") or lower. During moderation it helps to separate the neutrons and uranium, since 238U has a large affinity for intermediate-energy neutrons ("resonance" absorption), but is only easily fissioned by the few energetic neutrons above ≈1.5–2 MeV. Since most of the fuel is usually 238U, most reactor designs are based on thin fuel rods separated by moderator, allowing the neutrons to travel in the moderator before entering the fuel again. More neutrons are released than is needed to maintain the chain reaction; when uranium-238 absorbs just the excess, plutonium is created which helps to make up for the depletion of uranium-235. Eventually the build-up of fission products that are even more neutron-absorbing than 238U slows the reaction and calls for refuelling.
Light water makes an excellent moderator—the light hydrogen atoms are very close in mass to a neutron and can absorb a lot of energy in a single collision (like a collision of two billiard balls). However, light hydrogen is also fairly effective at "absorbing" neutrons, and there will be too few left over to react with the small amount of 235U in natural uranium, preventing criticality. In order to allow criticality, the fuel must be "enriched", increasing the amount of 235U to an acceptable level. In light water reactors, the fuel is typically enriched to between 2% and 5% 235U (the leftover fraction with less 235U is called depleted uranium). Enrichment facilities are expensive to build and operate. They are also a proliferation concern as they can be used to enrich the 235U much further, up to weapons-grade material (90% or more 235U). However, this can be remedied if the fuel is supplied and reprocessed by an internationally approved supplier.
The main advantage of heavy water moderator over light water is reduced absorption of the neutrons that sustain the chain reaction, allowing a lower concentration of active atoms (to the point of using unenriched natural uranium fuel). Deuterium ("heavy hydrogen") already has the extra neutron that light hydrogen would absorb, reducing the tendency to capture neutrons. However, deuterium is twice the mass of a single neutron (vs light hydrogen which is about the same mass); the mismatch means more collisions are needed to moderate the neutrons, requiring a larger thickness of moderator between the fuel rods. This increases the size of the reactor core and the leakage of neutrons. It is also the practical reason for the calandria design, otherwise a very large pressure vessel would be needed. The low 235U density in natural uranium also implies that less of the fuel will be consumed before the fission rate drops too low to sustain criticality, because the ratio of 235U to fission products+238U is lower. However, in CANDU most of the moderator is at lower temperatures than in other designs, reducing the spread of speeds and the overall speed of the moderator particles. This means most of the neutrons will end up at a lower energy and be more likely to cause fission, so CANDU not only "burns" natural uranium, but it does so more effectively as well. Overall, CANDU reactors use 30–40% less mined uranium than light-water reactors per unit of electricity produced. This is a major advantage to the heavy water design; it not only requires less fuel, but as the fuel does not have to be enriched, it is much less expensive as well.
A further unique feature of heavy-water moderation is the greater stability of the chain reaction. This is due to the relatively low binding energy of the deuterium nucleus (2.2 MeV), leading to some energetic neutrons and especially gamma rays breaking the deuterium nuclei apart to produce extra neutrons. Both gammas produced directly by fission and by the decay of fission fragments have enough energy, and the half-lives of the fission fragments range from seconds to hours or even years. The slow response of these gamma-generated neutrons delays the response of the reactor and gives the operators extra time in case of an emergency. Since gamma rays travel for meters through water, an increased rate of chain reaction in one part of the reactor will produce a response from the rest of the reactor, allowing various negative feedbacks to stabilize the reaction.
On the other hand, the fission neutrons are thoroughly slowed down before they reach another fuel rod, meaning that it takes neutrons a longer time to get from one part of the reactor to the other. Thus if the chain reaction accelerates in one section of the reactor, the change will propagate itself only slowly to the rest of the core, giving time to respond in an emergency. The independence of the neutrons' energies from the nuclear fuel used is what allows for such fuel flexibility in a CANDU reactor, since every fuel bundle will experience the same environment and affect its neighbors in the same way, whether the fissile material is uranium-235, uranium-233 or plutonium.
Canada developed the heavy water moderated design in the post-World War II era to explore nuclear energy while lacking access to enrichment facilities. War-era enrichment systems were extremely expensive to build and operate, whereas the heavy water solution allowed the use of natural uranium in the experimental ZEEP reactor. A much less expensive enrichment system was developed, but the United States classified work on the cheaper gas centrifuge process. The CANDU was therefore designed to use natural uranium.
Safety features.
The CANDU includes a number of active and passive safety features in its design. Some of these are a side-effect of the physical layout of the system.
CANDU designs have a positive void coefficient as well as a small power coefficient, normally considered bad in reactor design. This implies that steam generated in the coolant will "increase" the reaction rate, which in turn would generate more steam. This is one of the many reasons for the cooler mass of moderator in the calandria, as even a serious steam incident in the core would not have a major impact on the overall moderation cycle. Only if the moderator itself starts to boil would there be any significant effect, and the large thermal mass ensures this will occur slowly. The deliberately "sluggish" response of the fission process in CANDU allows controllers more time to diagnose and deal with problems.
The fuel channels can only maintain criticality if they are mechanically sound. If the temperature of the fuel bundles increases to the point where they are mechanically unstable, their horizontal layout means they will bend under gravity, shifting the layout of the bundles and reducing the efficiency of the reactions. Because the original fuel arrangement is optimum for a chain reaction and the natural uranium fuel has little excess reactivity, any significant deformation will stop the inter-fuel pellet fission reaction. This will not stop heat production from fission product decay, which would continue to supply a considerable heat output. If this process further weakens the fuel bundles, they will eventually bend far enough to touch the calandria tube, allowing heat to be efficiently transferred into the moderator tank. The moderator vessel has a considerable thermal capability on its own, and is normally kept relatively cool.
Heat generated by fission products would initially be at about 7% of full reactor power, which requires significant cooling. The CANDU designs have several emergency cooling systems, as well as having limited self-pumping capability through thermal means (the steam generator is well above the reactor). Even in the event of a catastrophic accident and core meltdown, it is important to remember that the fuel is not critical in light water. This means that cooling the core with water from nearby sources will not add to the reactivity of the fuel mass.
Normally the rate of fission is controlled by light-water compartments called liquid zone controllers, which absorb excess neutrons, and by adjuster rods which can be raised or lowered in the core to control the neutron flux. These are used for normal operation, allowing the controllers to adjust reactivity across the fuel mass as different portions would normally burn at different rates depending on their position. The adjuster rods can also be used to slow or stop criticality. Because these rods are inserted into the low-pressure calandria, not the high-pressure fuel tubes, they would not be "ejected" by steam, a design issue for many pressurized-water reactors.
There are two independent, fast-acting safety shutdown systems as well. Shutoff rods are held above the reactor by electromagnets, and drop under gravity into the core to quickly end criticality. This system works even in the event of a complete power failure, as the electromagnets only hold the rods out of the reactor when power is available. A secondary system injects a high-pressure gadolinium nitrate neutron absorber solution into the calandria.
Fuel cycles.
A heavy water design can sustain a chain reaction with a lower concentration of fissile atoms than light water reactors, allowing it to use some alternative fuels, e.g., "recovered uranium" (RU) from used LWR fuel can be used. CANDU was designed for natural uranium with only 0.7% U-235, so RU with 0.9% U-235 is a rich fuel. This extracts a further 30–40% energy from the uranium. The DUPIC ("Direct Use of spent PWR fuel In CANDU") process under development can recycle it even without reprocessing. The fuel is sintered in air (oxidized), then in hydrogen (reduced) to break it into a powder, which is then formed into CANDU fuel pellets.
CANDU can also breed fuel from the more abundant thorium. This is being investigated by India to take advantage of its natural thorium reserves.
Even better than LWRs, CANDU can utilize a mix of uranium and plutonium oxides (MOX fuel), the plutonium either from dismantled nuclear weapons or reprocessed reactor fuel. The mix of isotopes in reprocessed plutonium is not attractive for weapons, but can be used as fuel (instead of being simply nuclear waste), while burning weapons-grade plutonium eliminates a proliferation hazard. If the aim is explicitly to burn plutonium or other actinides from spent fuel, then special inert-matrix fuels are proposed to do this more efficiently than MOX. Since they contain no uranium, these fuels do not breed any extra plutonium.
Economics.
The neutron economy of heavy water moderation and precise control of on-line refueling allow CANDU to use a great range of fuels other than enriched uranium, e.g., natural uranium, reprocessed uranium, thorium, plutonium, and used LWR fuel. Given the expense of enrichment, this can make fuel much cheaper. There is however an initial investment into the tonnes of 99.75% pure heavy water to fill the core and heat transfer system. In the case of the Darlington plant costs released as part of a freedom of information act request put the overnight cost of the plant (four reactors totalling 3,512 MWe net capacity) at $5.117 billion CAD (about $4.2 billion USD at early 1990s exchange rates). Total capital costs including interest were $14.319 billion CAD (about $11.9 billion USD) with the heavy water accounting for $1.528 billion, or 11%, of this.
Since heavy water is less efficient at slowing neutrons, CANDU needs a larger moderator to fuel ratio and a larger core for the same power output. Although a calandria-based core is cheaper to build, its size increases the cost for standard features like the containment building. Generally nuclear plant construction and operations are ≈65% of overall lifetime cost; for CANDU costs are dominated by construction even more. Fueling CANDU is cheaper than other reactors, costing only ≈10% of the total, so the overall price per kWh electricity is comparable. The next-generation Advanced CANDU Reactor (ACR) mitigates these disadvantages by having light water coolant and using a more compact core with less moderator.
When first introduced, CANDUs offered much better capacity factor (ratio of power generated to what would be generated by running at full power, 100% of the time) than LWRs of a similar generation. The light-water designs spent, on average, about half the time being refueled or maintained. However, since the 1980s dramatic improvements in LWR outage management have narrowed the gap, with several units achieving capacity factors ~90% and higher, with an overall fleet performance of 92% in 2010. The latest-generation CANDU 6 reactors have an 88–90% CF, but overall performance is dominated by the older Canadian units with CFs on the order of 80%. Refurbished units have demonstrated poor performance to date, on the order of 65%.
Some CANDU plants suffered from cost overruns during construction, often from external factors such as government action. For instance, a number of imposed construction delays led to roughly a doubling of the cost of the Darlington Nuclear Generating Station near Toronto, Ontario. Technical problems and redesigns added about another billion to the resulting $14.4 billion price. In contrast, in 2002 two CANDU 6 reactors at Qinshan in China were completed on-schedule and on-budget, an achievement attributed to tight control over scope and schedule.
Nuclear nonproliferation.
In terms of safeguards against nuclear weapons proliferation, CANDUs meet a similar level of international certification as other reactors. There is a common misconception that the plutonium for India's first nuclear detonation, Operation Smiling Buddha in 1974, was produced in a CIRUS design. In fact, it was produced in the safeguarded indigenously built PHWR reactor. In addition to its two PHWR reactors, India has some safeguarded pressurised heavy water reactors (PHWRs) based on the CANDU design, and two safeguarded light-water reactors supplied by the US. Plutonium has been extracted from the spent fuel from all of these reactors; however India mainly relies on an Indian designed and built military reactor called Dhruva. The design is believed to be derived from the CIRUS reactor, with the Dhruva being scaled-up for more efficient plutonium production. It is this reactor which is thought to have produced the plutonium for India's more recent (1998) Operation Shakti nuclear tests.
Although heavy water is relatively immune to neutron capture, a small amount of the deuterium turns into tritium in this way. Tritium+deuterium mix undergoes nuclear fusion more easily than any other substance. Tritium can be used in both the "fusion boost" of a boosted fission weapon and the main fusion process of an H-bomb. However, in an H-bomb, it is usually created "in situ" by neutron irradiation of lithium-6.
Tritium is extracted from some CANDU plants in Canada, mainly to improve safety in case of heavy-water leakage. The gas is stockpiled and used in a variety of commercial products, notably "powerless" lighting systems and medical devices. In 1985 what was then Ontario Hydro sparked controversy in Ontario due to its plans to sell tritium to the U.S. The plan, by law, involved sales to non-military applications only, but some speculated that the exports could have freed American tritium for the U.S. nuclear weapons program. Future demands appear to outstrip production, in particular the demands of future generations of experimental fusion reactors like ITER. Currently between 1.5 and 2.1 kg of tritium are recovered yearly at the Darlington separation facility, of which a minor fraction is sold.
The 1998 Operation Shakti test series in India included one bomb of about 45 kt yield that India has publicly claimed was a hydrogen bomb. An offhand comment in the BARC publication "Heavy Water — Properties, Production and Analysis" appears to suggest that the tritium was extracted from the heavy water in the CANDU and PHWR reactors in commercial operation. "Janes Intelligence Review" quotes the Chairman of the Indian Atomic Energy Commission as admitting to the tritium extraction plant, but refusing to comment on its use. However India is also capable of creating tritium more efficiently by irradiation of lithium-6 in reactors.
Tritium emissions.
Tritium is a radioactive form of hydrogen (H-3), with a half-life of 12.3 years. It is produced in small amounts in nature (about 4 kg/year globally), by cosmic ray interactions in the upper atmosphere. Tritium is considered a weak radionuclide because of its low-energy radioactive emissions (beta particle energy up to 18.6 keV). The beta particles travel 6 mm in air and only penetrate skin up to 6 micrometers. The biological half-life of inhaled, ingested, or absorbed tritium is 10–12 days.
Tritium is generated in the fuel of all reactors; however, CANDU reactors generate tritium also in their coolant and moderator, due to neutron capture in heavy hydrogen. Some of this tritium escapes into containment and is generally recovered; however a small percentage (about 1%) escapes containment and is considered a routine radioactive emission (also higher than from an LWR of comparable size). Responsible operation of a CANDU plant therefore includes monitoring tritium in the surrounding environment (and publishing the results).
In some CANDU reactors the tritium is periodically extracted. Typical emissions from CANDU plants in Canada are less than 1% of the national regulatory limit, which is based on International Commission on Radiological Protection (ICRP) guidelines (for example, the maximum permitted drinking water concentration for tritium in Canada, 7,000 Bq/L, corresponds to 1/10 of the ICRP's dose limit for members of the public). Tritium emissions from other CANDU plants are similarly low.
In general there is significant public controversy about radioactive emissions from nuclear power plants, and for CANDU plants one of the main concerns is tritium. In 2007 Greenpeace published a critique of tritium emissions from Canadian nuclear power plants by Dr. Ian Fairlie. This report was criticized by Dr. Richard Osborne.
History.
Evolving designs.
The CANDU development effort has gone through four major stages over time. The first systems were experimental and prototype machines of limited power. These were replaced by a second generation of machines of 500 to 600 MWe (the CANDU6), a series of larger machines of 900 MWe, and finally developing into the CANDU9 and current ACR-1000 effort.
Early efforts.
The first heavy water moderated design in Canada was the ZEEP, which started operation just after the end of World War II. ZEEP was joined by several other experimental machines, including the NRX in 1947 and NRU in 1957. These efforts led to the first CANDU-type reactor, the Nuclear Power Demonstration (NPD), in Rolphton, Ontario. It was intended as a proof-of-concept and rated for only 22 MWe, a very low power for a commercial power reactor. NPD produced the first nuclear-generated electricity in Canada, and ran successfully from 1962 to 1987.
The second CANDU was the Douglas Point reactor, a more powerful version rated at roughly 200 MWe and located near Kincardine, Ontario. It went into service in 1968, and ran until 1984. Uniquely among CANDU stations, Douglas Point had an oil-filled window with a view of the east reactor face, even when the reactor was operating. Douglas Point was originally planned to be a two-unit station, but the second unit was cancelled because of the success of the larger 515 MWe units at Pickering.
In parallel with the classic CANDU design, experimental variants were being developed. WR-1, located at the AECL's Whiteshell Laboratories in Pinawa, Manitoba, used vertical pressure tubes and organic oil as the primary coolant. The oil used has a higher boiling point than water, allowing the reactor to operate at higher temperatures and lower pressures than a conventional reactor. WR-1 operated successfully for many years, and promised a significantly higher efficiency than water-cooled versions.
600 MWe designs.
The successes at NPD and Douglas Point led to the decision to construct the first multi-unit station in Pickering, Ontario. Pickering A, consisting of Units 1 to 4, went into service in 1971. Pickering B with units 5 to 8 came online in 1983, giving a full-station capacity of 4,120 MWe. The station is very close to the city of Toronto, in order to reduce transmission costs.
A series of improvements to the basic Pickering design led to the CANDU 6 design, which first went into operation in the early 1980s. CANDU 6 was essentially a version of the Pickering power plant that was re-designed to be able to be built in single-reactor units. CANDU 6 was used in several installations outside Ontario, including the Gentilly-2 in Quebec, and Point Lepreau Nuclear Generating Station in New Brunswick. CANDU 6 forms the majority of foreign CANDU systems, including the designs exported to Argentina, Romania, China and South Korea. Only India operates a CANDU system that is not based on the CANDU 6 design.
900 MWe designs.
The economics of nuclear power plants generally scale well with size. However, this improvement at larger sizes is offset by the sudden appearance of large quantities of power on the grid, which leads to a lowering of electricity prices through supply and demand effects. Predictions in the late 1960s suggested that growth in electricity demand would overwhelm these downward pricing pressures, leading most designers to introduce plants in the 1000 MWe range.
Pickering A was quickly followed by such an upscaling effort for the Bruce Nuclear Generating Station, constructed in stages between 1970 and 1987. It is the largest nuclear facility in North America, and second largest in the world (after Kashiwazaki-Kariwa in Japan), with eight reactors at around 800 MWe each, in total 6,232 MW (net) and 7,276 MW (gross). Another, smaller, upscaling led to the Darlington Nuclear Generating Station design, similar to the Bruce plant, but delivering about 880 MWe per reactor.
As was the case for the development of the Pickering design into the CANDU 6, the Bruce design was also developed into the similar CANDU 9. Like the CANDU 6, the CANDU 9 is essentially a re-packaging of the Bruce design so it can be built as a single-reactor unit. However, no CANDU 9 reactors have been built.
Generation III+ designs.
Through the 1980s and 90s the nuclear power market suffered a major crash, with few new plants being constructed in North America or Europe. Design work continued throughout, however, and a number of new design concepts were introduced that dramatically improved safety, capital costs, economics and overall performance. These Generation III+ and Generation IV machines became a topic of considerable interest in the early 2000s as it appeared a nuclear renaissance was underway and large numbers of new reactors would be built over the next decade.
AECL had been working on a design known as the ACR-700, using elements of the latest versions of the CANDU 6 and CANDU 9, with a design power of 700 MWe. During the nuclear renaissance, the upscaling seen in the earlier years re-expressed itself, and the ACR-700 was developed into the 1200 MWe ACR-1000. ACR-1000 is the next-generation (officially, "Generation III+") CANDU technology which makes some significant modifications to the existing CANDU design.
The main change, and the most radical among the CANDU generations, is the use of pressurized light water as the coolant. This significantly reduces the cost of implementing the primary cooling loop, which no longer has to be filled with expensive heavy water. The ACR-1000 uses about 1/3rd the heavy water needed in earlier generation designs. It also eliminates tritium production in the coolant loop, the major source of tritium leaks in operational CANDU designs. The redesign also allows for a slightly negative void reactivity, a major design goal of all Gen III+ machines.
However, the design also requires the use of slightly enriched uranium, enriched by about 1 or 2%. The main reason for this is to increase the burn-up ratio, allowing bundles to remain in the reactor longer, so that only a third as much spent fuel is produced. This also has effects on operational costs and timetables, as the refuelling frequency is reduced. As is the case with earlier CANDU designs, the ACR-1000 also offers online refuelling.
Outside of the reactor, the ACR-1000 has a number of design changes that are expected to dramatically lower capital and operational costs. Primary among these changes is the design lifetime of 60 years, which dramatically lowers the price of the electricity generated over the lifetime of the plant. The design also has an expected capacity factor of 90%. Higher pressure steam generators and turbines improve efficiency downstream of the reactor.
Many of the operational design changes were also applied to the existing CANDU 6 to produce the Enhanced CANDU 6. Also known as CANDU 6e or EC 6, this was an evolutionary upgrade of the CANDU 6 design with a gross output of 740 MWe per unit. The reactors are designed with a lifetime of over fifty years, with a mid-life program to replace some of the key components e.g. the fuel channels. The projected average annual capacity factor is more than ninety percent. Improvements to construction techniques (including modular, open-top assembly) decrease construction costs. The CANDU 6e is designed to operate at power settings as low as 50%, allowing them to adjust to load demand much better than the previous designs.
Sales efforts.
In Ontario.
By most measures, the CANDU is "the Ontario reactor". The system was developed almost entirely in Ontario, and only two experimental designs were built in other provinces. Of the 29 commercial CANDU reactors built, 22 are in Ontario. Of these 22, a number of reactors have been removed from service. Two new CANDU reactors have been proposed for Darlington with Canadian government help with financing.
In Canada.
AECL has heavily marketed CANDU within Canada, but has found a limited reception. To date, only two non-experimental reactors have been built in other provinces, one each in Quebec and New Brunswick, other provinces have concentrated on hydro and coal-fired plants. Several Canadian provinces have developed large amounts of hydro power. Alberta and Saskatchewan do not have extensive hydro resources, and use mainly fossil fuels to generate electric power.
Interest has been expressed in Western Canada, where CANDU reactors are being considered as heat and electricity sources for the energy-intensive oil sands extraction process, which currently uses natural gas. Energy Alberta Corporation announced 27 August 2007 that they had applied for a licence to build a new nuclear plant at Lac Cardinal (30 km west of the town of Peace River, Alberta), with two ACR-1000 reactors going online in 2017 producing 2.2 gigawatts (electric). However, a 2007 parliamentary review suggested placing the development efforts on hold. The company was later purchased by Bruce Power, who proposed expanding the plant to four units of a total 4.4 gigawatts. However, these plans were upset and Bruce later withdrew its application for the Lac Cardinal, proposing instead a new site about 60 km away.
Foreign sales.
During the 1970s the international nuclear sales market was extremely competitive, with many national nuclear companies being supported by their governments' foreign embassies. In addition, the pace of construction in the United States had meant that cost overruns and delayed completion was generally over, and subsequent reactors would be cheaper. Canada, a relatively new player on the international market, had numerous disadvantages in these efforts. However, the CANDU was deliberately designed to reduce the need for very large machined parts, making it suitable for construction by countries without a major industrial base. Sales efforts have had their most success in countries that could not locally build designs from other firms.
In the late 1970s, AECL noted that each reactor sale would employ 3,600 Canadians and result in $300 million in balance-of-payments income. However, these sales efforts were aimed primarily at countries being run by dictatorships or similar, a fact that led to serious concerns in parliament. These efforts also led to a scandal when it was discovered millions of dollars had been given to foreign sales agents, with little or no record of who they were, or what they did to earn the money. This led to a Royal Canadian Mounted Police investigation after questions were raised about sales efforts in Argentina, and new regulations on full disclosure of fees for future sales.
CANDU's first success was the sale of early CANDU designs to India. In 1963, an agreement was signed for export of a 200 MWe power reactor based on the Douglas Point reactor. The success of the deal led to the 1966 sale of a second reactor of the same design. The first reactor, then known as RAPP-1 for "Rajasthan Atomic Power Project", began operation in 1972. However, a serious problem with cracking of the reactor's end shield led to the reactor being shut down for long periods, and the reactor was finally downrated to 100 MW. Construction of the RAPP-2 reactor was still underway when India detonated its first atomic bomb in 1974, leading to Canada ending nuclear dealings with the country. Part of the sales agreement was a technology transfer process. When Canada withdrew from development, India continued construction of CANDU-like plants across the country. By 2010, CANDU-based reactors were operational at the following sites: Kaiga (3), Kakrapar (2), Madras (2), Narora (2), Rajasthan (6), and Tarapur (2).
In Pakistan the Karachi Nuclear Power Plant with a gross capacity of 137 MWe was built between 1966 and 1971.
In 1972, AECL submitted a design based on the Pickering plant to Argentina's Comision Nacional de Energia Atomica process, in partnership with the Italian company Italimpianti. High inflation during construction led to massive losses, and efforts to re-negotiate the deal were interrupted by the March 1976 coup led by General Videla. The Embalse Nuclear Power Station began commercial operation in January 1984. There have been ongoing negotiations to open more CANDU 6 reactors in the country, including a 2007 deal between Canada, China and Argentina, but to date no firm plans have been announced.
A licensing agreement with Romania was signed in 1977, selling the CANDU 6 design for $5 million per reactor for the first four reactors, and then $2 million each for the next twelve. In addition, Canadian companies would supply a varying amount of equipment for the reactors, about $100 million of the first reactor's $800 million price tag, and then falling over time. In 1980 Nicolae Ceaușescu asked for a modification to provide goods instead of cash, in exchange the amount of Canadian content was increased and a second reactor would be built with Canadian help. Economic troubles in the country worsened throughout the construction phase. The first reactor of the Cernavodă Nuclear Power Plant only came online in April 1996, a decade after its December 1985 predicted startup. Further loans were arranged for completion of the second reactor, which went online in November 2007.
In January 1975 a deal was announced for a single CANDU 6 reactor to be built in South Korea, now known as the Wolsong-1 Power Reactor. Construction started in 1977 and commercial operation began in April 1983. In December 1990 a further deal was announced for three additional units at the same site, which began operation in the period 1997–1999. However, South Korea also negotiated development and technology transfer deals with Westinghouse for their advanced System-80 reactor design, and all future development is based on locally built versions of this reactor.
In June 1998 construction started on a CANDU 6 reactor in Qinshan China Qinshan Nuclear Power Plant, as Phase III (Units 4 and 5) of the planned 11 unit facility. Commercial operation began on Dec 2002 and July 2003 respectively. These are the first heavy water reactors in China. Qinshan is the first CANDU-6 project to use open-top reactor building construction, and the first project where commercial operation began earlier than the projected date.
Future sales.
CANDU Energy is continuing marketing efforts in China. In addition, China and Argentina are negotiating over the possible construction of a CANDU-derived reactor 
Economic performance.
The cost of electricity from any power plant can be calculated by roughly the same selection of factors: capital costs for construction or the payments on loans made to secure that capital, the cost of fuel on a per-watt-hour basis, and fixed and variable maintenance fees. In the case of nuclear power, one normally includes two additional costs, the cost of permanent waste disposal, and the cost of decommissioning the plant when its useful lifetime is over. Generally, the capital costs dominate the price of nuclear power, as the amount of power produced is so large that it overwhelms the cost of fuel and maintenance. The World Nuclear Association calculates that the cost of fuel, including all processing, accounts for less than one cent per kWh.
Information on economic performance on CANDU is somewhat lopsided; the majority of reactors are in Ontario, which is also the "most public" among the major CANDU operators, so their performance dominates the available information. Based on Ontario's record, the economic performance of the CANDU system is quite poor. Although much attention has been focussed on the problems with the Darlington plant, every CANDU design in Ontario went over budget by at least 25%, and average over 150% higher than estimated. Darlington was the worst, at 350% over budget, but this project was stopped in-progress thereby incurring additional interest charges during a period of high interest rates, which is a special situation that was not expected to repeat itself.
In the 1980s, the pressure tubes in the Pickering A reactors were replaced ahead of design life due to unexpected deterioration caused by hydrogen embrittlement. Extensive inspection and maintenance has avoided this problem in later reactors.
All the Pickering A and Bruce A reactors were shut down in 1999 in order to focus on restoring operational performance in the later generations at Pickering, Bruce, and Darlington. Before restarting the Pickering A reactors, OPG undertook a limited refurbishment program. The original cost and time estimates based on inadequate project scope development were greatly below the actual time and cost and it was determined that Pickering Units 2 and 3 would not be restarted for commercial reasons. Despite this refurbishment, the reactors have not performed well since the restart.
These overruns were repeated at Bruce, with Units 3 and 4 running 90% over budget. Similar overruns were experienced at Point Lepreau, and Gentilly-2 plant was shut down on December 28, 2012.
Based on the projected capital costs, and the low cost of fuel and in-service maintenance, in 1994 power from CANDU was predicted to be well under 5 cents/kWh. In 1998, Ontario Hydro calculated that the cost of generation from CANDU was 7.7 cents/kWh, whereas hydropower was only 1.1 cents, and their coal-fired plants were 4.3 cents. As Hydro received a regulated price averaging 6.3 cents/kWh for power in this period, the revenues from the other forms of generation were being used to fund the operating losses of the nuclear plants. The debt left over from the nuclear construction could not be included in the rate base until the reactors were declared in service, thereby exacerbating the total capital cost of construction with unpaid interest, at that time around $15 billion, and another $3.5 billion in debts throughout the system was held by a separate entity and repaid through a standing charge on electricity bills.
In 1999, Ontario Hydro was broken up and its generation facilities re-formed into Ontario Power Generation (OPG). In order to make the successor companies more attractive for private investors, $19.4 billion in "stranded debt" was placed in the control of the Ontario Electricity Financial Corporation. This debt is slowly paid down through a variety of sources, including a 0.7-cent/kWh tariff on all power, all income taxes paid by all operating companies, and all dividends paid by the OPG and Hydro One. Even with these sources of income, the amount of debt has grown on several occasions, and in 2010 stood at almost $15 billion. This is in spite of total payments on the order of $19 billion, ostensibly enough to have paid off the debt entirely if interest repayment requirements are ignored.
Darlington is currently in the process of considering a major re-build of several units, as it too is reaching its design mid-life time. The budget is currently estimated to be between $8.5 and $14 billion, and produce power at 6 to 8 cents/kWh. However, this prediction is based on three assumptions that appear to have never been met in operation: that the rebuild will be completed on-budget, that the system will operate at an average capacity utilization of 82%, and that the Ontario taxpayer will pay 100% of any cost overruns. Although Darlington Units 1, 3 and 4 have operated with an average lifetime annual capacity factor of 85% and Unit 2 with a capacity factor of 78%, refurbished units at Pickering and Bruce have lifetime capacity factors between 59 and 69%. However, this includes periods of several years while the units were shut down for the retubing and refurbishing. In 2009, Bruce A Units 3 and 4 had capacity factors of 80.5% and 76.7%, respectively, in a year when they had a major Vacuum Building outage.
Active CANDU reactors.
Today there are 29 CANDU reactors in use around the world, and 13 "CANDU-derivatives" in India, developed from the CANDU design. After India detonated a nuclear bomb in 1974, Canada stopped nuclear dealings with India. The breakdown is:

</doc>
<doc id="7477" url="https://en.wikipedia.org/wiki?curid=7477" title="Cuitláhuac">
Cuitláhuac

Cuitláhuac () (c. 1476 – 1520) or Cuitláhuac (in Spanish orthography; , , honorific form Cuitlahuatzin) was the 10th "tlatoani" (ruler) of the Aztec city of Tenochtitlan for 80 days during the year Two Flint (1520).
Cuitláhuac was the eleventh son of the ruler Axayacatl and a younger brother of Moctezuma II, the previous ruler of Tenochtitlan. His mother's father, also called Cuitlahuac, had been ruler of Iztapalapa, and the younger Cuitláhuac also ruled there initially.
Cuitláhuac was made "tlatoani" of Tenochtitlan during the Spanish conquest of Mexico; After Pedro de Alvarado had ordered the Massacre in the Great Temple, the Aztecs were very upset and started to fight and put a siege to the Spaniards. Hernán Cortés ordered Moctezuma to ask his people to stop fighting. Moctezuma told him that they would not listen to him and suggested Cortés free Cuitláhuac so that he could convince them to dispose of their arms and not fight anymore. Cortés then freed Cuitláhuac and once Cuitláhuac was free he led his people against the conquistadors. He succeeded and the Spaniards were driven out of Tenochtitlan on June 30, 1520. Cuitláhuac was ritually married to Moctezuma's eldest daughter, a ten- or eleven-year-old girl who later was called Isabel Moctezuma.
After having ruled for just 80 days, Cuitláhuac died of smallpox that had been introduced to the New World by the Europeans. His elder brother Matlatzincatzin, who had been "cihuacoatl" ("president"), resigned upon Cuitláhuac's death. As soon as Cuitláhuac died, Cuauhtémoc was made the next "tlatoani".
The modern Mexican municipality of Cuitláhuac, Veracruz and the Mexico City Metro station Metro Cuitláhuac are named in honor of Cuitláhuac. The asteroid 2275 Cuitláhuac is also named after this ruler.
There is an Avenue in Mexico City Called Cuitláhuac (Eje 3 Norte) that runs from Avenue Insurgentes to Avenue Mexico-Tacuba and that is part of an inner ring; also many streets in other towns and villages in Mexico are so called.

</doc>
<doc id="7478" url="https://en.wikipedia.org/wiki?curid=7478" title="Cuauhtémoc">
Cuauhtémoc

Cuauhtémoc (, also known as Cuauhtemotzin, Guatimozin or Guatemoc; c. 1495) was the Mexica ruler ("tlatoani") of Tenochtitlan from 1520 to 1521, making him the last Aztec Emperor. The name Cuāuhtemōc means "one who has descended like an eagle", and is commonly rendered in English as "Descending Eagle," as in the moment when an eagle folds its wings and plummets down to strike its prey. This is a name that implies aggressiveness and determination.
Cuauhtémoc took power in 1520 as successor of Cuitláhuac and was a cousin of the late emperor Moctezuma II. His young wife, who was later known as Isabel Moctezuma, was one of Moctezuma's daughters. He ascended to the throne when he was around 25 years old, while Tenochtitlan was being besieged by the Spanish and devastated by an epidemic of smallpox brought to the New World by the invaders. After the killings in the Great Temple, there were probably few Aztec captains available to take the position.
Early life and rule.
Cuauhtemoc's date of birth is unknown and he does not enter the historical record until he became emperor. He was the eldest legitimate son of emperor Ahuitzotl and may well have attended the last New Fire ceremony marking the beginning of a new 52-year cycle in the Aztec calendar. Like the rest of Cuauhtemoc's early biography, this is inferred from knowledge of his age, and the likely events and life path of someone of his rank. Following education in the calmecac, the school for elite boys, and then military service, he was named ruler of Tlatelolco, with the title "cuauhtlatoani" ("eagle ruler") in 1515. To have reached this position of rulership, Cuauhtemoc had to be a male of high birth, and a warrior who had captured enemies for sacrifice.
When Cuauhtemoc was elected tlatoani in 1520, Tenochtitlan had already been rocked by the invasion of the Spanish and their indigenous allies, the death of Moctezuma II, and the death of Moctezuma's brother Cuitlahuac, who succeeded him as ruler, but died of smallpox shortly afterwards. In keeping with traditional practice, the most able candidate among the high noblemen was chosen by vote of the highest noblemen, Cuauhtemoc assumed the rulership. Although under Cuitlahuac Tenochtitlan began mounting a defense against the invaders, it was increasingly isolated militarily and largely faced the crisis alone, as the numbers of Spanish allies increased with the desertion of many polities previously under its control.
Cuauhtémoc called for reinforcements from the countryside to aid the defense of Tenochtitlán, after eighty days of warfare against the Spanish. Of all the Nahuas, only Tlatelolcas remained loyal, and the surviving Tenochcas looked for refuge in Tlatelolco, where even women took part in the battle. Cuauhtémoc was captured on August 13, 1521, while fleeing Tenochtitlán by crossing Lake Texcoco with his wife, family, and friends.
He surrendered to Hernán Cortés along with the surviving "pipiltin" (nobles) and, according to Spanish sources, he asked Cortés to take his knife and "strike me dead immediately". According to the same Spanish accounts, Cortés refused this offer and treated his foe magnanimously. "You have defended your capital like a brave warrior," he declared, "A Spaniard knows how to respect valor, even in an enemy." At Cuauhtémoc's request, Cortés also allowed the defeated Mexica to depart the city unmolested. Subsequently, however, when the booty found did not measure up to the Spaniards' expectations, Cuauhtémoc was tortured in an unsuccessful attempt to discover its whereabouts. On the statue to Cuauhtemoc on the Paseo de la Reforma in Mexico City, there is a bas relief showing the Spaniards' torture of the emperor. Eventually some gold was recovered, though far less than Cortés and his men expected.
Cuauhtémoc continued to hold his position under the Spanish, keeping the title of tlatoani, but he was no longer the sovereign ruler. He ordered the construction of a renaissance-style two-storied stone palace in Tlatelolco, in which he settled after the destruction of Mexico City; the building survived and was known as the Tecpan or palace.
Execution.
In 1525, Cortés took Cuauhtémoc and several other indigenous nobles on his expedition to Honduras, fearing that Cuauhtémoc could have led an insurrection in his absence. While the expedition was stopped in the Chontal Maya capital of Itzamkanac, known as Acalan in Nahuatl, Cortés had Cuauhtémoc executed for allegedly conspiring to kill him and the other Spaniards.
There are a number of discrepancies in the various accounts of the event. According to Cortés himself, on 27 February 1525 it was revealed to him by a citizen of Tenochtitlan named Mexicalcingo that Cuauhtémoc, Coanacoch (the ruler of Texcoco) and Tetlepanquetzal (the ruler of Tlacopan) were plotting his death. Cortés interrogated them until each confessed, and then had Cuauhtémoc, Tetlepanquetzal, and another lord named Tlacatlec hanged. Cortés wrote that the other lords would be too frightened to plot against him again, as they believed he had uncovered the plan through magic powers. Cortés's account is supported by the historian Francisco López de Gómara.
According to Bernal Díaz del Castillo, a conquistador serving under Cortés who recorded his experiences in his book "The True History of the Conquest of New Spain", the supposed plot was revealed by two men, named Tapia and Juan Velásquez. Díaz portrays the executions as unjust and based on no evidence, and admits to having liked Cuauhtémoc personally. He also records Cuauhtémoc giving the following speech to Cortés, through his interpreter Malinche:
Díaz wrote that afterwards, Cortés suffered from insomnia due to guilt, and badly injured himself while wandering at night.
Fernando de Alva Cortés Ixtlilxóchitl, a Mestizo historian and descendant of Coanacoch, wrote an account of the executions in the 17th century partly based on Texcocan oral tradition. According to Ixtlilxóchitl the three lords were joking cheerfully with each other, due to a rumor that Cortés had decided to return the expedition to Mexico, when Cortés asked a spy to tell him what they were talking about. The spy reported honestly, but Cortés invented the plot himself. Cuauhtémoc, Coanacoch and Tetlepanquetzal were all hanged, as well as eight others. However, Cortés cut down Coanacoch, the last to be hanged, after his brother began rallying his warriors. Coanacoch did not have long to enjoy his reprieve—Ixtlilxóchitl wrote that he died a few days later.
Tlacotzin, Cuauhtémoc's "cihuacoatl", was appointed his successor as "tlatoani". He died the next year before returning to Tenochtitlan.
Cuauhtemoc's bones.
The modern-day town of Ixcateopan in the state of Guerrero is home to an ossuary purportedly containing Cuauhtémoc's remains. Archeologist Eulalia Guzmán, a "passionate indigenista", excavated the bones in 1949, which were discovered shortly after bones found in Mexico City of Cortés had been authenticated by the Instituto Nacional de Antropología e Historia (INAH). Initially Mexican scholars congratulated Guzmán, but after a similar examination by scholars at INAH, their authenticity as Cuauhtemoc's was rejected - the bones in the ossuary belonged to several different persons, several of them seemingly women. This finding caused a public uproar. A panel assembled by Guzmán gave support to the initial contention. The Secretariat of Public Education (SEP) had another panel examine the bones, which gave support to INAH's original finding, but did not report on the finding publicly. A scholarly study of the controversy was published in 2011 arguing that the available data suggests that the grave is an elaborate hoax prepared by a local of Ichcateopan as a way of generating publicity, and subsequently supported by Mexican nationalists such as Guzman who wished to use the find for political purposes.
Legacy.
Cuauhtemoc is the embodiment of indigenist nationalism in Mexico, being the only Aztec emperor who survived the conquest by the Spanish Empire (and their native allies). He is honored by a monument on the Paseo de la Reforma, his face has appeared on Mexican banknotes, and he is celebrated in paintings, music, and popular culture.
Many places in Mexico are named in honour of Cuauhtémoc. These include Ciudad Cuauhtémoc in Chihuahua and the Cuauhtémoc borough of the Mexican Federal District, as well as Ciudad Cuauhtémoc, in the state of Veracruz.
There is a Cuauhtémoc station on Line 1 of the Mexico City metro as well as one for Moctezuma, but none for Hernán Cortés. There is also a metro station in Monterrey named after him.
Cuauhtémoc is also one of the few non-Spanish given names for Mexican boys that is perennially popular.
Cuauhtémoc Cárdenas Solórzano, a prominent Mexican politician, is named after him. In the Aztec campaign of the PC game ', the player plays as Cuauhtémoc, despite the name "Montezuma" for the campaign itself, and Cuauhtémoc narrates the openings and closings to each scenario. In the next installment to the series, ', Cuauhtémoc was the leader of Aztecs. The Mexican football player Cuauhtémoc Blanco was also named after him.
In the 1996 Rage Against The Machine single "People of the Sun", lyricist Zack De La Rocha rhymes "When the fifth sun sets get back reclaimed, The spirit of Cuauhtémoc alive and untamed".
Cuauhtémoc, in the name Guatemoc, is portrayed sympathetically in the adventure novel "Montezuma's Daughter", by H. Rider Haggard. First appearing in Chapter XIV, he becomes friends with the protagonist after they save each other's lives. His coronation, torture, and death are described in the novel.

</doc>
<doc id="7480" url="https://en.wikipedia.org/wiki?curid=7480" title="Cross section (physics)">
Cross section (physics)

The cross section is an effective area that quantifies the intrinsic likelihood of a scattering event when an incident beam strikes a target object, made of discrete particles. The cross section of a particle is the same as the cross section of a hard object, if the probabilities of hitting them with a ray are the same. It is typically denoted and measured in units of area.
In scattering experiments, one is often interested in knowing how likely a given event occurs. However, the rate depends strongly on experimental variables such as the density of the target material, the intensity of the beam, or the area of overlap between the beam and the target material. To control for these mundane differences, one can factor out these variables, resulting in an area-like quantity known as the cross section.
Definition.
Cross section is associated with a particular event (e.g. elastic collision, a specific chemical reaction, a specific nuclear reaction) involving a certain combination of beam (e.g. light, elementary particles, nuclei) and target material (e.g. colloids, gases, atoms, nuclei). Often there are additional factors that can affect the cross section in complicated ways, such as the energy of the beam.
For a given event, the cross section is given by
where
Equivalently, if the target material is a thin slab placed perpendicular to the beam, one may express the cross section in terms of flux:
where
For discrete events involving a beam of particles, the cross section is given by:
where
Schematically, an event is said to have a cross section of if its rate is equal to that of collisions in an idealized classical experiment where:
with all other experimental variables kept the same as the original experiment.
Differential cross section.
The cross section is a scalar that only quantifies the intrinsic rate of an event. In contrast, the differential cross section is a function that quantifies the intrinsic rate at which the scattered projectiles can be detected at a given angle (where represents solid angle).
Conventionally, a spherical coordinate system is used, with the target placed at the origin and the -axis of this coordinate system aligned with the incident beam. The angle is the scattering angle, measured between the incident beam and the scattered beam and the is the azimuthal angle. Many types of scattering processes possess azimuthal symmetry and therefore do not depend on .
The differential cross section at an angle is related to the rate of detection at that angle by
where is the angular span of the detector (SI unit: sr), which is assumed to be small and have perfect detection ratio.
The cross section may be recovered by integrating the differential cross section over the full solid angle ( steradians):
It is common to omit the “differential” qualifier when the type of cross section can be inferred from context. In this case, may be referred to as the "integral cross section" or "total cross section". The latter term may be confusing in contexts where multiple events are involved, since “total” can also refer to the sum of cross sections over all events.
The differential cross section is extremely useful quantity in many fields of physics, as measuring it can reveal a great amount of information about the internal structure of the target particles. For example, the differential cross section of Rutherford scattering provided strong evidence for the existence of the atomic nucleus.
Units.
Although the SI unit of total cross sections is square meter, smaller units are usually used in practice.
When the scattered radiation is visible light, it is conventional to measure the path length in centimetres. To avoid the need for conversion factors, the scattering cross section is expressed in and the number concentration in . The measurement of the scattering of visible light is known as nephelometry, and is effective for particles of in diameter: as such, it is widely used in meteorology and in the measurement of atmospheric pollution.
The scattering of X-rays can also be described in terms of scattering cross sections, in which case is a convenient unit: .
In nuclear and particle physics, the conventional unit is barn (unit), where . Smaller prefixed units such as and are also widely used. Correspondingly, the differential cross section can be measured in units such as .
Classical scattering.
In a simple classical experiment where a single particle is scattered off a rigid target,
the impact parameter is the perpendicular offset of the trajectory of the incoming particle. The differential of the cross section is the area element in the plane of the impact parameter, i.e. formula_6, where formula_7 is the impact parameter. The differential cross section is the differential quotient of this area element by the solid angle element in the direction of the particle exit trajectory:
It describes the change in the impact parameter necessary to cause a given change in the exit trajectory direction. The definition is slightly counter-intuitive in that the independent variable (in the denominator) describes the effect and the dependent variable (in the numerator) the initial condition. The differential cross section is always taken to be positive, even though in the most frequent case of limited-range repulsive interactions, larger impact parameters cause less deflection. In rotationally symmetric problems, the azimuthal angle formula_9 is not changed by the scattering process, and the differential cross section becomes
where formula_11 is the angle between the incident and exit direction of the scattered particle, as shown in the figure.
Quantum scattering.
In time-independent formalism of quantum scattering, the initial wave function (before scattering) is taken to be a plane wave with definite momentum :
where and ) are the "relative" coordinates between the projectile and the target. The arrow indicates that this only describes the "asymptotic behavior" of the wave function when the projectile and target are too far apart for the interaction to have any effect.
After the scattering takes place, it is expected that the wave function takes on the following asymptotic form:
where is some function of the angular coordinates known as the scattering amplitude. This general form is valid for any short-ranged, energy-conserving interaction. It is not true for long-ranged interactions, so there are additional complications when dealing with electromagnetic interactions.
The full wave function of the system behaves asymptotically as the sum,
The differential cross section is related to the scattering amplitude:
This has the simple interpretation as the probability of finding the scattered projectile within a given solid angle.
A cross section is therefore a measure of the effective surface area seen by the impinging particles, and as such is expressed in units of area. The cross section of two particles (i.e. observed when the two particles are colliding with each other) is a measure of the interaction event between the two particles. The cross section is proportional to the probability that an interaction will occur; for example in a simple scattering experiment the number of particles scattered per unit of time (current of scattered particles formula_16) depends only on the number of incident particles per unit of time (current of incident particles formula_17), the characteristics of target (for example the number of particles per unit of surface N), and the type of interaction. For formula_18 we have
Relation to the S-matrix.
If the reduced masses and momenta of the colliding system are "mi", p"i" and "mf", p"f" before and after the collision respectively, the differential cross section is given by
where the on-shell "T" matrix is defined by
in terms of the S-matrix. Here, formula_23 is the Dirac delta function. The computation of the S-matrix is the main goal of the scattering theory.
Attenuation.
If a beam enters a thin layer of material of thickness , the flux of the beam will decrease according to:
where is the "total" cross section of "all" events, including scattering, or to absorption, or transformation to another species. Solving this equation leads to the exponentially decaying behavior:
where is the initial flux. For light, this is called the Beer–Lambert law. 
This basic concept can then extended to the cases where the interaction probability in the targeted area assumes intermediate values, because the target itself is not homogeneous, or because the interaction is mediated by a non-uniform field. 
Scattering of light.
In general, the scattering cross section is different from the geometrical cross section of a particle, and it depends upon the wavelength of light and the permittivity, shape and size of the particle. The total amount of scattering in a sparse medium is determined by the product of the scattering cross section and the number of particles present. In terms of area, the "total cross section" (σ) is the sum of the cross sections due to absorption, scattering and luminescence
The total cross section is related to the absorbance of the light intensity through Beer–Lambert law, which says absorbance is proportional to concentration:
where "A"λ is the absorbance at a given wavelength "λ", "C" is the concentration as a number density, and formula_28 is the path length. The absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance:
Scattering of light on extended bodies.
In the context of scattering light on extended bodies, the scattering cross section, σscat, describes the likelihood of light being scattered by a macroscopic particle. In general, the scattering cross section is different from the geometrical cross section of a particle as it depends upon the wavelength of light and the permittivity in addition to the shape and size of the particle. The total amount of scattering in a sparse medium is determined by the product of the scattering cross section and the number of particles present. In terms of area, the "total cross section" (σ) is the sum of the cross sections due to absorption, scattering and luminescence
The total cross section is related to the absorbance of the light intensity through Beer-Lambert's law, which says absorbance is proportional to concentration: formula_31, where "Aλ" is the absorbance at a given wavelength "λ", "C" is the concentration as a number density, and "l" is the path length. The extinction or absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance:
Relation to physical size.
There is no simple relationship between the scattering cross section and the physical size of the particles, as the scattering cross section depends on the wavelength of radiation used. This can be seen when driving in foggy weather: the droplets of water (which form the fog) scatter red light less than they scatter the shorter wavelengths present in white light, and the red rear fog light can be distinguished more clearly than the white headlights of an approaching vehicle. That is to say that the scattering cross section of the water droplets is smaller for red light than for light of shorter wavelengths, even though the physical size of the particles is the same.
Meteorological range.
The scattering cross section is related to the meteorological range, "L"V:
The quantity "C σ"scat is sometimes denoted "b"scat, the scattering coefficient per unit length.
Applications.
Differential and total scattering cross sections are among the most important measurable quantities in nuclear and particle physics. Instead of the solid angle, the momentum transfer may be used as the independent variable of differential cross sections.
Differential cross sections in inelastic scattering contain resonance peaks that indicate the creation of metastable states and contain information about their energy and lifetime.
The total cross section in inelastic scattering is the sum of the total cross sections of all allowed individual processes. As a consequence, total cross sections of the creation of hadrons (i.e., strongly interacting particles) receive a factor of 3 from the quarks' color symmetry, allowing scientists to discover this symmetry.
Examples.
Example 1: elastic collision of two hard spheres.
The elastic collision of two hard spheres is an instructive example that demonstrates the sense of calling this quantity a cross section. formula_34 and formula_35 are the radii of the scattering center and scattered sphere, respectively, formula_7 the impact parameter and formula_11 the polar angle of the exit trajectory as above. Then the differential scattering cross section is
The total cross section is
So in this case the total scattering cross section is equal to the area of the circle (with radius formula_40) within which the center of mass of the incoming sphere has to arrive for it to be deflected, and outside which it passes by the stationary scattering center.
Example 2: differential cross section for the geometric light scattering from the circle mirror.
Another example illustrates the details of the calculation of a simple light scattering model obtained by a reduction of the dimension. For simplicity, we will consider the scattering of a beam of light on a plane treated as a uniform density of parallel rays and within the framework of geometrical optics from a circle with radius formula_35 with a perfectly reflecting boundary. Its three dimensional equivalent is therefore the more difficult problem of a laser or flashlight light scattering from the mirror sphere, for example from the mechanical bearing ball. The unit of cross section in one dimension is the unit of length, e. g. one meter. Let formula_42 be the angle between the light ray and the radius joining the reflection point of the light ray with the center point of the circle mirror. Then the increase of the length element perpendicular to the light beam is expressed by this angle as
the reflection angle of this ray with respect to the incoming ray is then formula_44 and the scattering angle is
The energy or the number of photons reflected from the light beam with the intensity or density of photons formula_46 on the length formula_47 is 
The differential cross section is therefore formula_49
As it is seen from the behaviour of the sine function this quantity has the maximum for the front backward scattering (formula_51) (the light is reflected perpendicularly and it returns) and the zero minimum for the scattering from the edge of the circle directly straight (formula_52). It confirms the intuitive expectations that the mirror circle acts like a diverging lens and a thin beam is more diluted the closer it is from the edge defined with respect to the incoming direction. The total cross section can be obtained by summing (integrating) the differential section of the entire range of angles:
so it is equal as much as the circular mirror is totally screening the two-dimensional space for the beam of light. In three dimensions for the mirror ball with the radius formula_35 it is therefore equal formula_55.
Example 3: differential cross section for the geometric light scattering from the perfectly spherical mirror.
We can now use the result from the Example 2 to calculate the differential cross section for the light scattering from the perfectly reflecting sphere in three dimensions. Let us denote now the radius of the sphere as formula_56. Let us parametrize the plane perpendicular to the incoming light beam by the cylindrical coordinates formula_35 and formula_58. In any plane of the incoming and the reflected ray we can write now from the previous example:
while the impact area element is 
Using the relation for the solid angle in the spherical coordinates:
and the trigonometric identity:
we obtain
while the total cross section as we expected is
As one can see it also agrees with the result from the Example 1 while photon is assumed to be a rigid sphere of the zero radius.
References.
Notes
Sources

</doc>
