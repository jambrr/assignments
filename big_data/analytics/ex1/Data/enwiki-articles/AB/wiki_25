<doc id="8267" url="https://en.wikipedia.org/wiki?curid=8267" title="Dimensional analysis">
Dimensional analysis

In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their fundamental dimensions (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometers, or pounds vs. kilograms vs. grams) and tracking these dimensions as calculations or comparisons are performed. Converting from one dimensional unit to another is often somewhat complex. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.
The concept of physical dimension was introduced by Joseph Fourier in 1822. Physical quantities that are commensurable have the same dimension; if they have different dimensions, they are incommensurable. For example, it is meaningless to ask whether a kilogram is less, the same, or more than an hour.
Any physically meaningful equation (and likewise any inequality and inequation) will have the same dimensions on the left and right sides, a property known as "dimensional homogeneity". Checking this is a common application of dimensional analysis. Dimensional analysis is also routinely used as a check on the plausibility of derived equations and computations. It is generally used to categorize types of physical quantities and units based on their relationship to or dependence on other units.
Concrete numbers and base units.
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number – a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 miles per hour or 1.4 km per second. Compound relations with "per" are expressed with division, e.g. 60 mi/1 h. Other relations can involve multiplication (often shown with · or juxtaposition), powers (like m2 for square meters), or combinations thereof.
A unit of measure that is in a conventionally chosen subset of a given system of units, where no unit in the set can be expressed in terms of the others, is known as a base unit. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus being derived or compound units.
Sometimes the names of units obscure that they are derived units. For example, an ampere is a unit of electric current, which is equivalent to electric charge per unit time and is measured in coulombs (a unit of electrical charge) per second, so . One newton is 1 kg⋅m/s2.
Percentages and derivatives.
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "1/100", since .
Derivatives with respect to a quantity add the dimensions of the variable one is differentiating with respect to on the denominator. Thus:
In economics, one distinguishes between stocks and flows: a stock has units of "units" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of "units/time" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency) – but one may argue that in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance), and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.
Conversion factor.
In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression "" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, because , and bar/bar cancels out, so .
Dimensional homogeneity.
The most basic rule of dimensional analysis is that of dimensional homogeneity. Only "commensurable" quantities (quantities with the same dimensions) may be "compared," "equated," "added," or "subtracted."
However, the dimensions form a "multiplicative group" and consequently:
For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometer, as these have different dimensions, nor to add 1 hour to 1 kilometer. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if "m"man, "m"rat and "L"man denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, "m"man/"L"2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometers. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in meters.
The factor-label method for converting units.
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:
It can be seen that each conversion factor is equivalent to the value of one. For example, starting with 1 mile = 1609 meters and dividing both sides of the equation by 1 mile yields 1 mile / 1 mile = 1609 meters / 1 mile, which when simplified yields 1 = 1609 meters / 1 mile.
So, when the units "mile" and "hour" are cancelled out and the arithmetic is done, 10 miles per hour converts to 4.47 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., NOx) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of NOx by using the following information as shown below:
After cancelling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.
Checking equations that involve dimensions.
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not guarantee that the equation is correct, but having different units on the two sides of an equation does guarantee that the equation is wrong.
For example, check the Universal Gas Law equation of "P·V = n·R·T", when:
As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units.
Limitations.
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (formula_4, rather than a linear transform formula_5) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature "T"in degrees Fahrenheit to a numerical quantity value "T"[C in degrees Celsius, this formula may be used:
To convert "T"in degrees Celsius to "T"[F in degrees Fahrenheit, this formula may be used:
Applications.
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.
Mathematics.
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an "n"-ball (the solid ball in "n" dimensions), or the area of its surface, the "n"-sphere: being an "n"-dimensional figure, the volume scales as formula_6 while the surface area, being formula_7-dimensional, scales as formula_8 Thus the volume of the "n"-ball in terms of the radius is formula_9 for some constant formula_10 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.
Finance, economics, and accounting.
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.
Fluid mechanics.
Common dimensionless groups in fluid mechanics include:
History.
The origins of dimensional analysis have been disputed by historians. The 19th-century French mathematician Joseph Fourier is generally credited with having made important contributions based on the idea that physical laws like Newton's laws of motion#Newton's second law should be independent of the units employed to measure the physical variables. This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually formalized in the Buckingham π theorem. However, the first application of dimensional analysis has been credited to the Italian scholar François Daviet de Foncenex (1734–1799). It was published in 1761, 61 years before the publication of Fourier’s work. 
James Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "the three fundamental units", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant "G" is taken as unity, giving . By assuming a form of Coulomb's law in which Coulomb's constant "k"e is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book "The Theory of Sound".
The original meaning of the word dimension, in Fourier's "Theorie de la Chaleur", was the numerical value of the exponents of the base units. For example, acceleration had the dimension 1 with respect to the unit of length, and the dimension -2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT-2, instead of just the exponents.
Mathematical examples.
The Buckingham π theorem describes how every physically meaningful equation involving "n" variables can be equivalently rewritten as an equation of dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.
Definition.
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions length, mass, time, electric charge, and absolute temperature, represented by sans-serif roman symbols L, M, T, Q, and Θ, respectively, each raised to a rational power.
The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J).
The term "dimension" is more abstract than "scale" unit: "mass" is a dimension, while kilogram is a scale unit (choice of standard) in the mass dimension.
As examples, the dimension of the physical quantity speed is "length"/"time" (L/T, or LT−1), and the dimension of the physical quantity force is "mass × acceleration" or "mass×(length/time)/time" (ML/T2, or MLT−2). In principle, other dimensions of physical quantity could be defined as "fundamental" (such as momentum or energy or electric current) instead of some of those shown above. Most physicists do not recognize temperature, Θ, as a fundamental dimension of physical quantity, since it essentially expresses the energy per degree of freedom, which can be expressed in terms of energy (or mass, length, and time). Still others do not recognize electric current, I, as a separate fundamental dimension of physical quantity, since it has been expressed in terms of mass, length, and time in unit systems such as the cgs system. There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.
The unit of a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g. length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to measure it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change a quantity. Dimensional symbols do not have conversion factors.
Mathematical properties.
The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; , and the inverse to L is 1/L or L−1. L raised to any rational power "p" is a member of the group, having an inverse of L−"p" or 1/Lp. The operation of the group is multiplication, having the usual rules for handling exponents ().
This group can be described as a vector space over the rational numbers, with for example dimensional symbol M"i"L"j"T"k" corresponding to the vector . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for a given vector space of dimensional symbols is called a set of fundamental units or fundamental dimensions, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The kernel describes some number (e.g., "m") of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., π"m"}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity "X" can be expressed in the general form
Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form
Knowing this restriction can be a powerful tool for obtaining new insight into the system.
Mechanics.
In mechanics, the dimension of any physical quantity can be expressed in terms of the fundamental dimensions (or "base dimensions") M, L, and T – these form a 3-dimensional vector space. This is not the only possible choice, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is, thus, partly a convention, resulting in increased utility and familiarity. It is, however, important to note that the choice of the set of dimensions cannot be chosen arbitrarily – it is not "just" a convention – because the dimensions must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form an equivalent basis to M, L, T: the former can be expressed as = ML/T2, L, M, while the latter can be expressed as M, L, = (ML/F)1/2.
On the other hand, using length, velocity and time as base dimensions will not work well (they do not form a set of fundamental dimensions), for two reasons:
Other fields of physics and chemistry.
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry the number of moles of substance (the number of molecules divided by Avogadro's constant, ≈ 6.02 × 1023) is defined as a base unit as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.
Polynomials and transcendental functions.
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
2, and so forth – the overall output would not scale as a particular dimension.
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does "not" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials ("x""n") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for "x"2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for "x"2 + "x", the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,
This is the height to which an object rises in time "t" if the acceleration of gravity is 32 feet per second per second and the initial upward speed is 500 feet per second. It is not even necessary for "t" to be in "seconds". For example, suppose "t" = 0.01 minutes. Then the first term would be
Incorporating units.
The value of a dimensional physical quantity "Z" is written as the product of a unit ["Z"] within the dimension and a dimensionless numerical factor, "n".
When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:
The factor formula_18 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Position vs displacement.
Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).
Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
but for temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −273.15 °C, or the temperature difference equal to 1 °C.
Orientation and frame of reference.
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.
Examples.
A simple example: period of a harmonic oscillator.
What is the period of oscillation formula_19 of a mass formula_20 attached to an ideal linear spring with spring constant formula_21 suspended in gravity of strength formula_22? That period is the solution for formula_19 of some dimensionless equation in the variables formula_19, formula_20, formula_21, and formula_22.
The four quantities have the following dimensions: formula_19 formula_20 [M; formula_21 and formula_22 [L/T2. From these we can form only one dimensionless product of powers of our chosen variables, formula_32 = formula_33 , and putting formula_34 for some dimensionless constant formula_35 gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.
Note that the variable formula_22 does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines formula_22 with formula_21, formula_20, and formula_19, because formula_22 is the only quantity that involves the dimension L. This implies that in this problem the formula_22 is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of formula_22: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_44, for some dimensionless constant κ (equal to formula_45 from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (formula_22, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as κ.
A more complex example: energy of a vibrating wire.
Consider the case of a vibrating wire of length "ℓ" (L) vibrating with an amplitude "A" (L). The wire has a linear density "ρ" (M/L) and is under tension "s" (ML/T2), and we want to know the energy "E" (ML2/T2) in the wire. Let "π"1 and "π"2 be two dimensionless products of powers of the variables chosen, given by
The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation
where "F" is some unknown function, or, equivalently as
where "f" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function "f". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to "ℓ", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.
Extensions.
Huntley's extension: directed dimensions.
Huntley has pointed out that it is sometimes productive to refine our concept of dimension. Two possible refinements are:
As an example of the usefulness of the first refinement, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_50 and a horizontal velocity component formula_51, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_51, formula_50, both dimensioned as LT−1, "R", the distance travelled, having dimension L, and "g" the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range "R" may be written:
Or dimensionally
from which we may deduce that formula_56 and formula_57, which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then formula_51 will be dimensioned as LxT−1, formula_50 as LyT−1, "R" as Lx and "g" as LyT−2. The dimensional equation becomes:
and we may solve completely as formula_61, formula_62 and formula_63. The increase in deductive power gained by the use of directed length dimensions is apparent.
In a similar manner, it is sometimes found useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of quantity (substantial mass). For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables
There are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_69 and formula_70 and we may express the dimensional equation as
where "C" and "a" are undetermined constants. If we draw a distinction between inertial mass with dimension formula_72 and substantial mass with dimension formula_73, then mass flow rate and density will use substantial mass as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:
where now only "C" is an undetermined constant (found to be equal to formula_75 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Siano's extension: orientational analysis.
Huntley's extension has some serious drawbacks:
It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries? Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's addition to real problems.
Angles are, by convention, considered to be dimensionless variables, and so the use of angles as physical variables in dimensional analysis can give less meaningful results. As an example, consider the projectile problem mentioned above. Suppose that, instead of the x- and y-components of the initial velocity, we had chosen the magnitude of the velocity "v" and the angle "θ" at which the projectile was fired. The angle is, by convention, considered to be dimensionless, and the magnitude of a vector has no directional quality, so that no dimensionless variable can be composed of the four variables "g", "v", "R", and "θ". Conventional analysis will correctly give the powers of "g" and "v", but will give no information concerning the dimensionless angle "θ".
Note that the orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of 1z. For angles, consider an angle "θ" that lies in the z-plane. Form a right triangle in the z-plane with "θ" being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y. Then, since tan("θ") = 1y/1x = "θ" + ... we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable. Analogous reasoning forces the conclusion that sin("θ") has orientation 1z while cos("θ") has orientation 10. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where "a" and "b" are real scalars. Note that an expression such as formula_77 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:
which for formula_79 and formula_80 yields formula_81. Physical quantities may be expressed as complex numbers (e.g. formula_82) which imply that the complex quantity "i" has an orientation equal to that of the angle it is associated with (1z in the above example).
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into "normal form". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, "θ", being in the xy-plane will thus have dimension 1z and the range of the projectile "R" will be of the form:
Dimensional homogeneity will now correctly yield and , and orientational homogeneity requires that "c" be an odd integer. In fact the required function of theta will be sin("θ")cos("θ") which is a series of odd powers of "θ".
It is seen that the Taylor series of sin("θ") and cos("θ") are orientationally homogeneous using the above multiplication table, while expressions like and exp("θ") are not, and are (correctly) deemed unphysical.
It should be clear that the multiplication rule used for the orientational symbols is not the same as that for the cross product of two vectors. The cross product of two identical vectors is zero, while the product of two identical orientational symbols is the identity element.
Dimensionless concepts.
Constants.
The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_84 in the spring problems discussed above come from a more detailed analysis of the underlying physics, and often arises from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.
Formalisms.
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_85 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be formula_86 where formula_87 is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: "c", "ħ", and "G", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants "ħ", "c", and "G" (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_88, formula_89 and formula_90. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.
Dimensional equivalences.
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.
Natural units.
If , where "c" is the speed of light and "ħ" is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length "L", mass "M" and time "T" can be expressed (dimensionally) as a power of energy "E", because length, mass and time can be expressed using speed "v", action "S", and energy "E":
though speed and action are dimensionless ( and ) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:
This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge "e" though other choices are possible.

</doc>
<doc id="8270" url="https://en.wikipedia.org/wiki?curid=8270" title="December 25">
December 25


</doc>
<doc id="8271" url="https://en.wikipedia.org/wiki?curid=8271" title="Digital television">
Digital television

Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signal, in contrast to the totally analog and channel separated signals used by analog television. Digital TV can support more than one program in the same channel bandwidth. It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s. Several regions of the world are in different stages of adaptation and are implementing different broadcasting standards. Below are the different widely used digital television broadcasting standards (DTB):
History.
Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It wasn't until the 1990s that digital TV became a real possibility.
In the mid-1980s as Japanese consumer electronics firms forged ahead with the development of HDTV technology, and as the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies. Until June 1990, the Japanese MUSE standard—based on an analog system—was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.
In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images. Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels. The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.
The final standard adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This outcome resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—is superior. Interlaced scanning, which is used in televisions worldwide, scans even-numbered lines first, then odd-numbered ones. Progressive scanning, which is the format used in computers, scans lines in sequences, from top to bottom. The computer industry argued that progressive scanning is superior because it does not "flicker" in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format.
Digital television transition started in the late 2000s. All the governments across the world set the deadline for analog shutdown by the 2010s. Initially the adoption rate was low. But soon, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.
Technical information.
Formats and bandwidth.
Digital television supports many different picture formats defined by the broadcast television systems which are a combination of size, aspect ratio (width to height ratio).
With digital terrestrial television (DTT) broadcasting, the range of formats can be broadly divided into two categories: high definition television (HDTV) for the transmission of high-definition video and standard-definition television (SDTV). These terms by themselves are not very precise, and many subtle intermediate cases exist.
One of several different HDTV formats that can be transmitted over DTV is: 1280 × 720 pixels in progressive scan mode (abbreviated "720p") or 1920 × 1080 pixels in interlaced video mode ("1080i"). Each of these uses a aspect ratio. (Some televisions are capable of receiving an HD resolution of 1920 × 1080 at a 60 Hz progressive scan frame rate — known as 1080p.) HDTV cannot be transmitted over analog television channels because of channel capacity issues.
Standard definition TV (SDTV), by comparison, may use one of several different formats taking the form of various aspect ratios depending on the technology used in the country of broadcast. For aspect-ratio broadcasts, the 640 × 480 format is used in NTSC countries, while 720 × 576 is used in PAL countries. For broadcasts, the 720 × 480 format is used in NTSC countries, while 720 × 576 is used in PAL countries. However, broadcasters may choose to reduce these resolutions to reduce bit rate (e.g., many DVB-T channels in the United Kingdom use a horizontal resolution of 544 or 704 pixels per line).
Each commercial broadcasting terrestrial television DTV channel in North America is permitted to be broadcast at a bit rate up to 19 megabits per second. However, the broadcaster does not need to use this entire bandwidth for just one broadcast channel. Instead the broadcast can use the channel to include PSIP and can also subdivide across several video subchannels (a.k.a. feeds) of varying quality and compression rates, including non-video datacasting services that allow one-way high-bit-rate streaming of data to computers like National Datacast.
A broadcaster may opt to use a standard-definition (SDTV) digital signal instead of an HDTV signal, because current convention allows the bandwidth of a DTV channel (or "multiplex") to be subdivided into multiple digital subchannels, (similar to what most FM radio stations offer with HD Radio), providing multiple feeds of entirely different television programming on the same channel. This ability to provide either a single HDTV feed or multiple lower-resolution feeds is often referred to as distributing one's "bit budget" or multicasting. This can sometimes be arranged automatically, using a statistical multiplexer (or "stat-mux"). With some implementations, image resolution may be less directly limited by bandwidth; for example in DVB-T, broadcasters can choose from several different modulation schemes, giving them the option to reduce the transmission bit rate and make reception easier for more distant or mobile viewers.
Receiving digital signal.
There are several different ways to receive digital television. One of the oldest means of receiving DTV (and TV in general) is using an antenna (known as an "aerial" in some countries). This way is known as Digital terrestrial television (DTT or DVB-Terrestrial, DVB-T), which can be in two types: DVB-T and DVB-T2. With DTT, viewers are limited to whatever channels the antenna picks up. Signal quality will also vary. Regardless of what sale ads try to lead the public to believe, there is no such thing as a specialized DTV antenna. ANY Over the Air antenna that worked for analog TV should work for Digital TV (But DTV signal levels are lower thus requiring actually a bigger antenna with more gain unless you are visually close to the transmitting towers).
Other ways have been devised to receive digital television. Among the most familiar to people are digital cable and digital satellite. In some countries where transmissions of TV signals are normally achieved by microwaves, digital MMDS is used. Other standards, such as Digital multimedia broadcasting (DMB) and DVB-H, have been devised to allow handheld devices such as mobile phones to receive TV signals. Another way is IPTV, that is receiving TV via Internet Protocol, relying on digital subscriber line (DSL) or optical cable line. Finally, an alternative way is to receive digital TV signals via the open Internet (Internet television), whether from a central streaming service or a P2P (peer-to-peer) system.
Some signals carry encryption and specify use conditions (such as "may not be recorded" or "may not be viewed on displays larger than 1 m in diagonal measure") backed up with the force of law under the World Intellectual Property Organization Copyright Treaty (WIPO Copyright Treaty) and national legislation implementing it, such as the U.S. Digital Millennium Copyright Act. Access to encrypted channels can be controlled by a removable smart card, for example via the Common Interface (DVB-CI) standard for Europe and via Point Of Deployment (POD) for IS or named differently CableCard.
Disadvantages.
While poor signal analog TV quality could be evaluated by the user by the amount of noise on the screen, digital TV has no grey areas, it either works or does not when the signal is not strong enough.
Protection parameters for terrestrial DTV broadcasting.
Digital television signals must not interfere with each other, and they must also coexist with analog television until it is phased out.
The following table gives allowable signal-to-noise and signal-to-interference ratios for various interference scenarios. This table is a crucial regulatory tool for controlling the placement and power levels of stations. Digital TV is more tolerant of interference than analog TV, and this is the reason a smaller range of channels can carry an all-digital set of television stations.
Interaction.
People can interact with a DTV system in various ways. One can, for example, browse the Electronic program guide. Modern DTV systems sometimes use a return path providing feedback from the end user to the broadcaster. This is possible with a coaxial or fiber optic cable, a dialup modem, or Internet connection but is not possible with a standard antenna.
Some of these systems support video on demand using a communication channel localized to a neighborhood rather than a city (terrestrial) or an even larger area (satellite).
1-segment broadcasting.
1seg (1-segment) is a special form of ISDB. Each channel is further divided into 13 segments. The 12 segments of them are allocated for HDTV and remaining segment, the 13th, is used for narrow-band receivers such as mobile television or cell phone.
Comparison of analog vs digital.
DTV has several advantages over analog TV, the most significant being that digital channels take up less bandwidth, and the bandwidth needs are continuously variable, at a corresponding reduction in image quality depending on the level of compression as well as the resolution of the transmitted image. This means that digital broadcasters can provide more digital channels in the same space, provide high-definition television service, or provide other non-television services such as multimedia or interactivity. DTV also permits special services such as multiplexing (more than one program on the same channel), electronic program guides and additional languages (spoken or subtitled). The sale of non-television services may provide an additional revenue source.
Digital and analog signals react to interference differently. For example, common problems with analog television include ghosting of images, noise from weak signals, and many other potential problems which degrade the quality of the image and sound, although the program material may still be watchable. With digital television, the audio and video must be synchronized digitally, so reception of the digital signal must be very nearly complete; otherwise, neither audio nor video will be usable. Short of this complete failure, "blocky" video is seen when the digital signal experiences interference.
Analog TV started off with monophonic sound, and later evolved to stereophonic sound with two independent audio signal channels. DTV will allow up to 5 audio signal channels plus a sub-woofer bass channel, with broadcasts similar in quality to movie theaters and DVDs.
Compression artifacts and allocated bandwidth.
DTV images have some picture defects that are not present on analog television or motion picture cinema, because of present-day limitations of bit rate and compression algorithms such as MPEG-2. This defect is sometimes referred to as "mosquito noise".
Because of the way the human visual system works, defects in an image that are localized to particular features of the image or that come and go are more perceptible than defects that are uniform and constant. However, the DTV system is designed to take advantage of other limitations of the human visual system to help mask these flaws, e.g. by allowing more compression artifacts during fast motion where the eye cannot track and resolve them as easily and, conversely, minimizing artifacts in still backgrounds that may be closely examined in a scene (since time allows).
Effects of poor reception.
Changes in signal reception from factors such as degrading antenna connections or changing weather conditions may gradually reduce the quality of analog TV. The nature of digital TV results in a perfectly decodable video initially, until the receiving equipment starts picking up interference that overpowers the desired signal or if the signal is too weak to decode. Some equipment will show a garbled picture with significant damage, while other devices may go directly from perfectly decodable video to no video at all or lock up. This phenomenon is known as the digital cliff effect.
For remote locations, distant channels that, as analog signals, were previously usable in a snowy and degraded state may, as digital signals, be perfectly decodable or may become completely unavailable. The use of higher frequencies will add to these problems, especially in cases where a clear line-of-sight from the receiving antenna to the transmitter is not available.
Effect on old analog technology.
Television sets with only analog tuners cannot decode digital transmissions. When analog broadcasting over the air ceases, users of sets with analog-only tuners may use other sources of programming (e.g. cable, recorded media) or may purchase set-top converter boxes to tune in the digital signals. In the United States, a government-sponsored coupon was available to offset the cost of an external converter box. Analog switch-off (of full-power stations) took place on December 11, 2006 in The Netherlands, June 12, 2009 in the United States for full-power stations, July 24, 2011 in Japan, August 31, 2011 in Canada, February 13, 2012 in Arab states, May 1, 2012 in Germany, October 24, 2012 in the United Kingdom and Ireland, October 31, 2012 in selected Indian cities, and December 10, 2013 in Australia. Completion of analog switch-off is scheduled for December 31, 2014 in the whole of India, by 2015 in the Philippines and Uruguay, by September 1, 2015 for low-power stations in the United States, and by 2017 in Costa Rica.
Disappearance of TV-audio receivers.
Prior to the conversion to digital TV, analog television broadcast audio for TV channels on a separate FM carrier signal from the video signal. This FM audio signal could be heard using standard radios equipped with the appropriate tuning circuits.
However, after the transition of many countries to digital TV, no portable radio manufacturer has yet developed an alternative method for portable radios to play just the audio signal of digital TV channels. (DTV radio is not the same thing.)
Environmental issues.
The adoption of a broadcast standard incompatible with existing analog receivers has created the problem of large numbers of analog receivers being discarded during digital television transition. One superintendent of Public Works was quoted in 2009 as saying, "Some of the studies I’ve read in the trade magazines say up to a quarter of American households could be throwing a TV out in the next two years following the regulation change". In 2009, an estimated 99 million analog TV receivers were sitting unused in homes in the US alone and, while some obsolete receivers are being retrofitted with converters, many more are simply dumped in landfills where they represent a source of toxic metals such as lead as well as lesser amounts of materials such as barium, cadmium and chromium.
According to one campaign group, a CRT computer monitor or TV contains an average of of lead. According to another source, the lead in glass of a CRT varies from 1.08 lb to 11.28 lb, depending on screen size and type, but the lead is in the form of "stable and immobile" lead oxide mixed into the glass. It is claimed that the lead can have long-term negative effects on the environment if dumped as landfill. However, the glass envelope can be recycled at suitably equipped facilities. Other portions of the receiver may be subject to disposal as hazardous material.
Local restrictions on disposal of these materials vary widely; in some cases second-hand stores have refused to accept working color television receivers for resale due to the increasing costs of disposing of unsold TVs. Those thrift stores which are still accepting donated TVs have reported significant increases in good-condition working used television receivers abandoned by viewers who often expect them not to work after digital transition.
In Michigan in 2009, one recycler estimated that as many as one household in four would dispose of or recycle a TV set in the following year. The digital television transition, migration to high-definition television receivers and the replacement of CRTs with flatscreens are all factors in the increasing number of discarded analog CRT-based television receivers.

</doc>
<doc id="8274" url="https://en.wikipedia.org/wiki?curid=8274" title="Declaration of Arbroath">
Declaration of Arbroath

The Declaration of Arbroath is a declaration of Scottish independence, made in 1320. It is in the form of a letter in Latin submitted to Pope John XXII, dated 6 April 1320, intended to confirm Scotland's status as an independent, sovereign state and defending Scotland's right to use military action when unjustly attacked.
Generally believed to have been written in the Arbroath Abbey by Bernard of Kilwinning, then Chancellor of Scotland and Abbot of Arbroath, and sealed by fifty-one magnates and nobles, the letter is the sole survivor of three created at the time. The others were a letter from the King of Scots, Robert I, and a letter from four Scottish bishops which all presumably made similar points.
Overview.
The Declaration was part of a broader diplomatic campaign which sought to assert Scotland's position as an independent kingdom, rather than being a feudal land controlled by England's Norman kings, as well as lift the excommunication of Robert the Bruce. The Pope had recognised Edward I of England's claim to overlordship of Scotland in 1305 and Bruce was excommunicated by the Pope for murdering John Comyn before the altar in Greyfriars Church in Dumfries in 1306.
The Declaration made a number of rhetorical points: that Scotland had always been independent, indeed for longer than England; that Edward I of England had unjustly attacked Scotland and perpetrated atrocities; that Robert the Bruce had delivered the Scottish nation from this peril; and, most controversially, that the independence of Scotland was the prerogative of the Scottish people, rather than the King of Scots. In fact it stated that the nobility would choose someone else to be king if Bruce proved to be unfit in maintaining Scotland's independence. Some have interpreted this last point as an early expression of 'popular sovereignty' – that government is contractual and that kings can be chosen by the community rather than by God alone.
It has also been argued that the Declaration was not a statement of popular sovereignty (and that its signatories would have had no such concept) but a statement of royal propaganda supporting Bruce's faction. A justification had to be given for the rejection of King John in whose name William Wallace and Andrew de Moray had rebelled in 1297. The reason given in the Declaration is that Bruce was able to defend Scotland from English aggression whereas, by implication, King John could not.
Whatever the true motive, the idea of a contract between King and people was advanced to the Pope as an excuse for Bruce's coronation whilst John de Balliol still lived in Papal custody.
There are 39 names (eight earls and thirty one barons) at the start of the document, all of whom may have had their seals appended, probably over the space of some weeks and months, with nobles sending in their seals to be used. (On the extant copy of the Declaration there are only 19 seals, and of those 19 people only 12 are named within the document.) It is thought likely that at least 11 more seals than the original 39 might have been appended.) The Declaration was then taken to the papal court at Avignon by Bishop Kininmund, Sir Adam Gordon and Sir Odard de Maubuisson.
The Pope heeded the arguments contained in the Declaration, influenced by the offer of support from the Scots for his long-desired crusade if they no longer had to fear English invasion. He exhorted Edward II in a letter to make peace with the Scots, but the following year was again persuaded by the English to take their side and issued six bulls to that effect. It was only in October 1328, after a short-lived peace treaty between Scotland and England, the Treaty of Edinburgh-Northampton (which renounced all English claims to Scotland and was signed by the new English king, Edward III, on 1 March 1328), that the interdict on Scotland and the excommunication of its king were finally removed.
The original copy of the Declaration that was sent to Avignon is lost. A copy of the Declaration survives among Scotland's state papers, held by the National Archives of Scotland in Edinburgh. The most widely known English language translation was made by Sir James Fergusson, formerly Keeper of the Records of Scotland, from text that he reconstructed using this extant copy and early copies of the original draft. One passage in particular, strongly suggesting Sallust (86–35 BC) as the direct source, is often quoted from the Fergusson translation:
Legacy.
US Senate Resolution 155 of 10 November 1997 states that the Declaration of Arbroath, the Scottish Declaration of Independence , was signed on April 6, 1320 and the American Declaration of Independence was modeled on that inspirational document. However, this influence is accepted by few historians and actively disputed by others. Even advocates of the link concede that it is speculative and not based on any verifiable sources.

</doc>
<doc id="8276" url="https://en.wikipedia.org/wiki?curid=8276" title="Digital data">
Digital data

Digital data, in information theory and information systems, are discrete, discontinuous representations of information or works, as contrasted with continuous, or analog signals which behave in a continuous manner, or represent information using a continuous function.
Although digital representations are the subject matter of discrete mathematics, the information represented can be either discrete, such as numbers and letters, or it can be continuous, such as sounds, images, and other measurements.
The word "digital" comes from the same source as the words digit and "digitus" (the Latin word for "finger"), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word "digital" in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942. The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.
Symbol to digital conversion.
Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.
A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.
For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.
Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.
A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as 'ß' needs to be converted but is not in the standard.
It is estimated that in the year 1986 less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%. The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog format (the "beginning of the digital age").
Properties of digital information.
All digital information possesses common properties that distinguish it from analog data with respect to communications:
Historical digital systems.
Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.

</doc>
<doc id="8278" url="https://en.wikipedia.org/wiki?curid=8278" title="Deduction">
Deduction

Deduction may refer to:

</doc>
<doc id="8280" url="https://en.wikipedia.org/wiki?curid=8280" title="Demon">
Demon

A demon (from Koine Greek δαιμόνιον "daimonion"), or fiend is a supernatural, often malevolent being prevalent in religion, occultism, literature, fiction, mythology and folklore.
The original Greek word "daimon" does not carry the negative connotation initially understood by implementation of the Koine δαιμόνιον ("daimonion"), and later ascribed to any cognate words sharing the root.
In Ancient Near Eastern religions as well as in the Abrahamic traditions, including ancient and medieval Christian demonology, a demon is considered an unclean spirit, a fallen angel, or a spirit of unknown type which may cause demonic possession, calling for an exorcism. In Western occultism and Renaissance magic, which grew out of an amalgamation of Greco-Roman magic, Jewish Aggadah and Christian demonology, a demon is believed to be a spiritual entity that may be conjured and controlled.
Etymology.
The Ancient Greek word "daimōn" denotes a spirit or divine power, much like the Latin "genius" or "numen". "Daimōn" most likely came from the Greek verb "daiesthai" (to divide, distribute). The Greek conception of a "daimōns" notably appears in the works of Plato, where it describes the divine inspiration of Socrates. To distinguish the classical Greek concept from its later Christian interpretation, the former is anglicized as either "daemon" or "daimon" rather than "demon".
The Greek terms do not have any connotations of evil or malevolence. In fact, εὐδαιμονία "eudaimonia", (literally good-spiritedness) means happiness. By the early Roman Empire, cult statues were seen, by pagans and their Christian neighbors alike, as inhabited by the numinous presence of the gods: "Like pagans, Christians still sensed and saw the gods and their power, and as something, they had to assume, lay behind it, by an easy traditional shift of opinion they turned these pagan "daimones" into malevolent 'demons', the troupe of Satan... Far into the Byzantine period Christians eyed their cities' old pagan statuary as a seat of the demons' presence. It was no longer beautiful, it was infested." The term had first acquired its negative connotations in the Septuagint translation of the Hebrew Bible into Greek, which drew on the mythology of ancient Semitic religions. This was then inherited by the Koine text of the New Testament. The Western medieval and neo-medieval conception of a "demon" derives seamlessly from the ambient popular culture of Late (Roman) Antiquity. The Hellenistic "daemon" eventually came to include many Semitic and Near Eastern gods as evaluated by Christianity.
The supposed existence of demons remains an important concept in many modern religions and occultist traditions. Demons are still feared largely due to their alleged power to possess living creatures. In the contemporary Western occultist tradition (perhaps epitomized by the work of Aleister Crowley), a demon (such as Choronzon, the Demon of the Abyss) is a useful metaphor for certain inner psychological processes (inner demons), though some may also regard it as an objectively real phenomenon. Some scholars believe that large portions of the demonology (see Asmodai) of Judaism, a key influence on Christianity and Islam, originated from a later form of Zoroastrianism, and were transferred to Judaism during the Persian era. 
Ancient Near East.
Mesopotamia.
According to the Jewish Encyclopedia, "In Chaldean mythology the seven evil deities were known as "shedu", storm-demons, represented in ox-like form." They were represented as winged bulls, derived from the colossal bulls used as protective jinn of royal palaces.
From Chaldea, the term "shedu" traveled to the Israelites. The writers of the Tanach applied the word as a dialogism to Canaanite deities.
There are indications that demons in popular Hebrew mythology were believed to come from the nether world. Various diseases and ailments were ascribed to them, particularly those affecting the brain and those of internal nature. Examples include the catalepsy, headache, epilepsy and nightmares. There also existed a demon of blindness, "Shabriri" (lit. "dazzling glare") who rested on uncovered water at night and blinded those who drank from it.
Demons supposedly entered the body and caused the disease while overwhelming or "seizing" the victim. To cure such diseases, it was necessary to draw out the evil demons by certain incantations and talismanic performances, at which the Essenes excelled. Josephus, who spoke of demons as "spirits of the wicked which enter into men that are alive and kill them", but which could be driven out by a certain root, witnessed such a performance in the presence of the Emperor Vespasian and ascribed its origin to King Solomon. In mythology, there were few defences against Babylonian demons. The mythical mace Sharur had the power to slay demons such as Asag, a legendary gallu or edimmu of hideous strength.
Judaism.
As referring to the existence or non-existence of "shedim" (Hebr. for "demons", "spirits") there are converse opinions in Judaism. There are "practically nil" roles assigned to demons in the Jewish Bible. In Judaism today, beliefs in "shedim" ("demons" or "evil spirits") are either midot hasidut (Hebr. for "customs of the pious"), and therefore not halachah, or notions based on a superstition that are non-essential, non-binding parts of Judaism, and therefore not normative Jewish practice. In conclusion, Jews are not obligated to believe in the existence of "shedim", as posek rabbi David Bar-Hayim points out.
Tanach.
The word "shedim" (Hebr. for "demons" or "spirits") appears only in two places in the Tanakh (, ). In both places, the term appears in a scriptural context of animal or child sacrifice to non-existent false gods that are called "shedim".
Talmudic tradition.
In the Jerusalem Talmud notions of "shedim" ("demons" or "evil spirits") are almost unknown or occur only very rarely, whereas in the Babylon Talmud there are many references to "shedim" and magical incantations. The existence of "shedim" in general was not questioned by most of the Babylonian Talmudists. As a consequence of the rise of influence of the Babylonian Talmud over that of the Jerusalem Talmud, late rabbis in general took as fact the existence of "shedim", nor did most of the medieval thinkers question their reality. However, rationalists like Maimonides, Saadia Gaon and Abraham ibn Ezra and others explicitly denied their existence, and completely rejected concepts of demons, evil spirits, negative spiritual influences, attaching and possessing spirits. Their point of view eventually became mainstream Jewish understanding.
Kabbalah.
Some benevolent "shedim" were used in kabbalistic ceremonies (as with the "golem" of Rabbi Yehuda Loevy) and malevolent "shedim" ("mazikin", from the root meaning "to damage") were often credited with possession.
Aggadah.
Aggadic tales from the Persian tradition describe the "shedim", the " mazziḳim" ("harmers"), and the " ruḥin" ("spirits"). There were also "lilin" ("night spirits"), " ṭelane" ("shade", or "evening spirits"), " ṭiharire" ("midday spirits"), and " ẓafrire" ("morning spirits"), as well as the "demons that bring famine" and "such as cause storm and earthquake". According to some aggadic stories about demons is told that they were under the dominion of a king or chief, either Asmodai or, in the older Aggadah, Samael ("the angel of death"), who killed via poison. Stories in the fashion of this kind of folklore never became an essential feature of Jewish theology. Although occasionally an angel is called "satan" in the Babylon Talmud, this does not refer to a demon: "Stand not in the way of an ox when coming from the pasture, for Satan dances between his horns".
Second Temple period texts.
To the Qumran community during the Second Temple period this apotropaic prayer was assigned, stating: "And, I the Sage, declare the grandeur of his radiance in order to frighten and terri all the spirits of the ravaging angels and the bastard spirits, demons, Liliths, owls" ("Dead Sea Scrolls", "Songs of the Sage," Lines 4–5).
In the Dead Sea Scrolls, there exists a fragment entitled “Curses of Belial” ("Curses of Belial (Dead Sea Scrolls, 394, 4Q286(4Q287, fr. 6)=4QBerakhot)"). This fragment holds much rich language that reflects the sentiment shared between the Qumran towards Belial. In many ways this text shows how these people thought Belial influenced sin through the way they address him and speak of him. By addressing “Belial and all his guilty lot,” (4Q286:2) they make it clear that he is not only impious, but also guilty of sins. Informing this state of uncleanliness are both his “hostile” and “wicked design” (4Q286:3,4). Through this design, Belial poisons the thoughts of those who are not necessarily sinners. Thus a dualism is born from those inclined to be wicked and those who aren’t. It is clear that Belial directly influences sin by the mention of “abominable plots” and “guilty inclination” (4Q286:8,9). These are both mechanisms by which Belial advances his evil agenda that the Qumran have exposed and are calling upon God to protect them from. There is a deep sense of fear that Belial will “establish in their heart their evil devices” (4Q286:11,12). This sense of fear is the stimulus for this prayer in the first place. Without the worry and potential of falling victim to Belial’s demonic sway, the Qumran people would never feel impelled to craft a curse. This very fact illuminates the power Belial was believed to hold over mortals, and the fact that sin proved to be a temptation that must stem from an impure origin.
In Jubilees 1:20, Belial’s appearance continues to support the notion that sin is a direct product of his influence. Moreover, Belial’s presence acts as a placeholder for all negative influences or those that would potentially interfere with God’s will and a pious existence. Similarly to the “gentiles…cause them to sin against you” (Jubilees 1:19), Belial is associated with a force that drives one away from God. Coupled in this plea for protection against foreign rule, in this case the Egyptians, is a plea for protection from “the spirit of Belial” (Jubilees 1:19). Belial’s tendency is to “ensnare [you from every path of righteousness” (Jubilees 1:19). This phrase is intentionally vague, allowing room for interpretation. Everyone, in one way or another, finds themselves straying from the path of righteousness and by pawning this transgression off on Belial, he becomes a scapegoat for all misguidance, no matter what the cause. By associating Belial with all sorts of misfortune and negative external influence, the Qumran people are henceforth allowed to be let off for the sins they commit.
Belial’s presence is found throughout the War Scrolls, located in the Dead Sea Scrolls, and is established as the force occupying the opposite end of the spectrum of God. In Col. I, verse 1, the very first line of the document, it is stated that “the first attack of the Sons of Light shall be undertaken against the forces of the Sons of Darkness, the army of Belial” (1Q33;1:1). This dichotomy sheds light on the negative connotations that Belial held at the time. Where God and his Sons of Light are forces that protect and promote piety, Belial and his Sons of Darkness cater to the opposite, instilling the desire to sin and encouraging destruction. This opposition is only reinforced later in the document; it continues to read that the “holy ones” will “strike a blow at wickedness,” ultimately resulting in the “annihilation of the Sons of Darkness” (1Q33:1:13). This epic battle between good and evil described in such abstract terms, however it is also applicable to everyday life and serves as a lens through which the Qumran see the world. Every day is the Sons of Light battle evil and call upon God to help them overcome evil in ways small and large.
Belial’s influence is not taken lightly. In Col. XI, verse 8, the text depicts God conquering the “hordes of Belial” (1Q33;11:8). This defeat is indicative of God’s power over Belial and his forces of temptation. However the fact that Belial is the leader of hordes is a testament to how persuasive he can be. If Belial was obviously an arbiter of wrongdoing and was blatantly in the wrong, he wouldn’t be able to amass an army. This fact serves as a warning message, reasserting God’s strength, while also making it extremely clear the breadth of Belial’s prowess. Belial’s “council is to condemn and convict,” so the Qumran feel strongly that their people are not only aware of his purpose, but also equipped to combat his influence (1Q33;13:11).
In the Damascus Document, Belial also makes a prominent appearance, being established as a source of evil and an origin of several types of sin. In Column 4, the first mention of Belial reads: “Belial shall be unleashed against Israel” (4Q266). This phrase is able to be interpreted myriad different ways. Belial is characterized in a wild and uncontrollable fashion, making him seem more dangerous and unpredictable. The notion of being unleashed is such that once he is free to roam; he is unstoppable and able to carry out his agenda uninhibited. The passage then goes to enumerate the “three nets” (4Q266;4:16) by which Belial captures his prey and forces them to sin. “Fornication…, riches..., the profanation of the temple” (4Q266;4:17,18) make up the three nets. These three temptations were three agents by which people were driven to sin, so subsequently, the Qumran people crafted the nets of Belial to rationalize why these specific temptations were so toxic. Later in Column 5, Belial is mentioned again as one of “the removers of bound who led Israel astray” (4Q266;5:20). This statement is a clear display of Belial’s influence over man regarding sin. The passage goes on to state: “they preached rebellion against...God” (4Q266;5:21,22). Belial’s purpose is to undermine the teachings of God, and he achieves this by imparting his nets on humans, or the incentive to sin.
In the War of the Sons of Light Against the Sons of Darkness, Belial controls scores of demons, which are specifically allotted to him by God for the purpose of performing evil. Belial, despite his malevolent disposition, is considered an angel.
Christianity.
Christian Bible.
Old Testament.
Demons in the Old Testament of the Christian Bible are of two classes: the "satyrs" or "shaggy goats" (from Hebr. "se'irim" "hairy beings" and Greek Old Testament σάτυρος "satyros", "satyr"; , ) and the "demons" (from Hebr. "shedim", and Koine Greek δαιμόνιον "daimonion"; , ).
New Testament.
The term "demon" (from the Greek New Testament δαιμόνιον "daimonion") appears 63 times in the New Testament of the Christian Bible.
Pseudepigrapha and Deuterocanonical books.
Demons are sometimes included into biblical interpretation. In the story of Passover, the Bible tells the story as "the Lord struck down all the firstborn in Egypt" (Exodus 12:21–29). In the Book of Jubilees, which is considered canonical only by the Ethiopian Orthodox Church, this same event is told slightly differently: "All the powers of demon Mastema had been let loose to slay all the first-born in the land of Egypt...And the powers of the Lord did everything according as the Lord commanded them" (Jubilees 49:2–4).
In the Genesis flood narrative the author explains how God was noticing "how corrupt the earth had become, for all the people on earth had corrupted their ways" (Genesis 6:12). In Jubilees the sins of man are attributed to "the unclean demons began to lead astray the children of the sons of Noah, and to make to err and destroy them" (Jubilees 10:1). In Jubilees Mastema questions the loyalty of Abraham and tells God to "bid him offer him as a burnt offering on the altar, and Thou wilt see if he will do this command" (Jubilees 17:16). The discrepancy between the story in Jubilees and the story in Genesis 22 exists with the presence of Mastema. In Genesis, God tests the will of Abraham merely to determine whether he is a true follower, however; in Jubilees Mastema has an agenda behind promoting the sacrifice of Abraham’s son, "an even more demonic act than that of the Satan in Job." In Jubilees, where Mastema, an angel tasked with the tempting of mortals into sin and iniquity, requests that God give him a tenth of the spirits of the children of the watchers, demons, in order to aid the process. These demons are passed into Mastema’s authority, where once again, an angel is in charge of demonic spirits.
The sources of demonic influence were thought to originate from the Watchers or Nephilim, who are first mentioned in Genesis 6 and are the focus of 1 Enoch Chapters 1–16, and also in Jubilees 10. The Nephilim were seen as the source of the sin and evil on earth because they are referenced in Genesis 6:4 before the story of the Flood. In Genesis 6:5, God sees evil in the hearts of men. The passage states, “the wickedness of humankind on earth was great”, and that “Every inclination of the thoughts of their hearts was only continually evil” (Genesis 5). The mention of the Nephilim in the preceding sentence connects the spread of evil to the Nephilim. Enoch is a very similar story to Genesis 6:4–5, and provides further description of the story connecting the Nephilim to the corruption of humans. In Enoch, sin originates when angels descend from heaven and fornicate with women, birthing giants as tall as 300 cubits. The giants and the angels’ departure of Heaven and mating with human women are also seen as the source of sorrow and sadness on Earth. The book of Enoch shows that these fallen angels can lead humans to sin through direct interaction or through providing forbidden knowledge. In Enoch, Semyaz leads the angels to mate with women. Angels mating with humans is against God’s commands and is a cursed action, resulting in the wrath of God coming upon Earth. Azazel indirectly influences humans to sin by teaching them divine knowledge not meant for humans. Asael brings down the “stolen mysteries” (Enoch 16:3). Asael gives the humans weapons, which they use to kill each other. Humans are also taught other sinful actions such as beautification techniques, alchemy, astrology and how to make medicine (considered forbidden knowledge at the time). Demons originate from the evil spirits of the giants that are cursed by God to wander the earth. These spirits are stated in Enoch to “corrupt, fall, be excited, and fall upon the earth, and cause sorrow” (Enoch 15:11).
The Book of Jubilees conveys that sin occurs when Cainan accidentally transcribes astrological knowledge used by the Watchers (Jubilees 8). This differs from Enoch in that it does not place blame on the Angels. However, in Jubilees 10:4 the evil spirits of the Watchers are discussed as evil and still remain on earth to corrupt the humans. God binds only 90 percent of the Watchers and destroys them, leaving 10 percent to be ruled by Mastema. Because the evil in humans is great, only 10 percent would be needed to corrupt and lead humans astray. These spirits of the giants also referred to as “the bastards” in the Apotropaic prayer Songs of the Sage, which lists the names of demons the narrator hopes to expel.
Christian demonology.
In the Christian Bible, the deities of other religions are sometimes interpreted or created as "demons" (from the Greek Old Testament δαιμόνιον "daimonion"). The evolution of the Christian Devil and pentagram are examples of early rituals and images that showcase evil qualities, as seen by the Christian churches.
Since Early Christianity, demonology has developed from a simple acceptance of demons to a complex study that has grown from the original ideas taken from Jewish demonology and Christian scriptures. Christian demonology is studied in depth within the Roman Catholic Church, although many other Christian churches affirm and discuss the existence of demons.
Building upon the few references to "daemons" in the New Testament, especially the poetry of the Book of Revelation, Christian writers of apocrypha from the 2nd century onwards created a more complicated tapestry of beliefs about "demons" that was largely independent of Christian scripture.
The contemporary Roman Catholic Church unequivocally teaches that angels and demons are real beings rather than just symbolic devices. The Catholic Church has a cadre of officially sanctioned exorcists which perform many exorcisms each year. The exorcists of the Catholic Church teach that demons attack humans continually but that afflicted persons can be effectively healed and protected either by the formal rite of exorcism, authorized to be performed only by bishops and those they designate, or by prayers of deliverance, which any Christian can offer for themselves or others.
At various times in Christian history, attempts have been made to classify demons according to various proposed demonic hierarchies.
In the Gospels, particularly the Gospel of Mark, Jesus cast out many demons from those afflicted with various ailments. He also lent this power to some of his disciples ().
Apuleius, by Augustine of Hippo, is ambiguous as to whether "daemons" had become 'demonized' by the early 5th century:
Islam.
The numerous mentions of jinn in the Quran and testimony of both pre-Islamic and Islamic literature indicate that the belief in spirits was prominent in pre-Islamic Bedouin religion. There is evidence that the word jinn is derived from Aramaic, where it was used by Christians to designate pagan gods reduced to the status of demons, and was introduced into Arabic folklore only late in the pre-Islamic era. Julius Wellhausen has observed that such spirits were thought to inhabit desolate, dingy and dark places and that they were feared.
Islam recognizes the existence of jinn, which are sentient beings with free will that can co-exist with humans (though not the genies of modern lore). In Islam, evil jinn are referred to as the "shayātīn" or demons/devils, with Iblis (Satan) as their chief. Iblis was one of the first jinn; he disobeyed Allah and did not bow down before Adam refusing to acknowledge a creature made of "clay". Thus, Iblis was banished from Jannah (Heaven). He asked for respite until the Last Day (Judgement Day), when he vowed to make mankind fall and deny the existence of their creator, to which Allah replied that Iblis would only be able to mislead those who were not righteous believers, warning that Iblis and all who followed him in evil would be punished in Hell.
Hinduism.
Hinduism includes numerous varieties of spirits that might be classified as demons, including Vetalas, Bhutas and Pishachas. Rakshasas and Asuras are often also taken as demons.
Asuras.
Originally, "Asura", in the earliest hymns of the Rig Veda, meant any supernatural spirit, either good or bad. Since the /s/ of the Indic linguistic branch is cognate with the /h/ of the Early Iranian languages, the word "Asura", representing a category of celestial beings, became the word "Ahura" (Mazda), the Supreme God of the monotheistic Zoroastrians. Ancient Hinduism tells that Devas (also called "suras") and Asuras are half-brothers, sons of the same father Kashyapa; although some of the Devas, such as Varuna, are also called Asuras. Later, during Puranic age, Asura and Rakshasa came to exclusively mean any of a race of anthropomorphic, powerful, possibly evil beings. Daitya (lit. sons of the mother "Diti"), Rakshasa (lit. from "harm to be guarded against"), and Asura are incorrectly translated into English as "demon".
In Hindu scriptures, pious, highly enlightened Asuras, such as Prahlada and Vibhishana, are not uncommon. The Asura are not fundamentally against the gods, nor do they tempt humans to fall. Many people metaphorically interpret the Asura as manifestations of the ignoble passions in the human mind and as a symbolic devices. There were also cases of power-hungry Asuras challenging various aspects of the Gods, but only to be defeated eventually and seek forgiveness—see Surapadman and Narakasura.
Evil spirits.
Hinduism advocates the reincarnation and transmigration of souls according to one's karma. Souls (Atman) of the dead are adjudged by the Yama and are accorded various purging punishments before being reborn. Humans that have committed extraordinary wrongs are condemned to roam as lonely, often evil, spirits for a length of time before being reborn. Many kinds of such spirits (Vetalas, Pishachas, Bhūta) are recognized in the later Hindu texts. These beings, in a limited sense, can be called demons.
Bahá'í Faith.
In the Bahá'í Faith, demons are not regarded as independent evil spirits as they are in some faiths. Rather, evil spirits described in various faiths' traditions, such as Satan, fallen angels, demons and jinns, are metaphors for the base character traits a human being may acquire and manifest when he turns away from God and follows his lower nature. Belief in the existence of ghosts and earthbound spirits is rejected and considered to be the product of superstition.
Ceremonial magic.
While some people fear demons, or attempt to exorcise them, others willfully attempt to summon them for knowledge, assistance, or power. The ceremonial magician usually consults a grimoire, which gives the names and abilities of demons as well as detailed instructions for conjuring and controlling them. Grimoires aren't limited to demons – some give the names of angels or spirits which can be called, a process called theurgy. The use of ceremonial magic to call demons is also known as goetia, the name taken from a section in the famous grimoire the "Lesser Key of Solomon".
Wicca.
According to Rosemary Ellen Guiley, "Demons are not courted or worshipped in contemporary Wicca and Paganism. The existence of negative energies is acknowledged."
Modern interpretations.
Psychologist Wilhelm Wundt remarked that "among the activities attributed by myths all over the world to demons, the harmful predominate, so that in popular belief bad demons are clearly older than good ones." Sigmund Freud developed this idea and claimed that the concept of demons was derived from the important relation of the living to the dead: "The fact that demons are always regarded as the spirits of those who have died "recently" shows better than anything the influence of mourning on the origin of the belief in demons."
M. Scott Peck, an American psychiatrist, wrote two books on the subject, "People of the Lie: The Hope For Healing Human Evil" and "Glimpses of the Devil: A Psychiatrist's Personal Accounts of Possession, Exorcism, and Redemption". Peck describes in some detail several cases involving his patients. In "People of the Lie" he provides identifying characteristics of an evil person, whom he classified as having a character disorder. In "Glimpses of the Devil" Peck goes into significant detail describing how he became interested in exorcism in order to debunk the "myth" of possession by evil spirits – only to be convinced otherwise after encountering two cases which did not fit into any category known to psychology or psychiatry. Peck came to the conclusion that possession was a rare phenomenon related to evil, and that possessed people are not actually evil; rather, they are doing battle with the forces of evil.
Although Peck's earlier work was met with widespread popular acceptance, his work on the topics of evil and possession has generated significant debate and derision. Much was made of his association with (and admiration for) the controversial Malachi Martin, a Roman Catholic priest and a former Jesuit, despite the fact that Peck consistently called Martin a liar and manipulator. Richard Woods, a Roman Catholic priest and theologian, has claimed that Dr. Peck misdiagnosed patients based upon a lack of knowledge regarding dissociative identity disorder (formerly known as multiple personality disorder), and had apparently transgressed the boundaries of professional ethics by attempting to persuade his patients into accepting Christianity. Father Woods admitted that he has never witnessed a genuine case of demonic possession in all his years.
According to the "Hong Kong Journal of Psychiatry", God is shown sending a demon against Saul in 1 Samuel 16 and 18 in order to punish him for the failure to follow God’s instructions, showing God as having the power to use demons for his own purposes, putting the demon under his divine authority. According to the "Britannica Concise Encyclopedia", demons, despite being typically associated with evil, are often shown to be under divine control, and not acting of their own devices.

</doc>
<doc id="8286" url="https://en.wikipedia.org/wiki?curid=8286" title="Domino effect">
Domino effect

A domino effect or chain reaction is the cumulative effect produced when one event sets off a chain of similar events. The term is best known as a mechanical effect, and is used as an analogy to a falling row of dominoes. It typically refers to a linked sequence of events where the time between successive events is relatively small. It can be used literally (an observed series of actual collisions) or metaphorically (causal linkages within systems such as global finance or politics).
See also.
Relevant physical theory:
Mathematical theory
Political theory

</doc>
<doc id="8293" url="https://en.wikipedia.org/wiki?curid=8293" title="Diffusion pump">
Diffusion pump

Diffusion pumps use a high speed jet of vapor to direct gas molecules in the pump throat down into the bottom of the pump and out the exhaust. Invented in 1915 by Wolfgang Gaede using mercury vapor, and improved by Irving Langmuir and W. Crawford, they were the first type of high vacuum pumps operating in the regime of free molecular flow, where the movement of the gas molecules can be better understood as diffusion than by conventional fluid dynamics. Gaede used the name diffusion pump since his design was based on the finding that gas cannot diffuse against the vapor stream, but will be carried with it to the exhaust. However, the principle of operation might be more precisely described as gas-jet pump, since diffusion plays a role also in other high vacuum pumps. In modern text books, the diffusion pump is categorized as a momentum transfer pump.
The diffusion pump is widely used in both industrial and research applications. Most modern diffusion pumps use silicone oil or polyphenyl ethers as the working fluid. Cecil Reginald Burch discovered the possibility of using silicone oil in 1928.
Oil diffusion pumps.
An oil diffusion pump is used to achieve higher vacuum (lower pressure) than is possible by use of positive displacement pumps alone. Although its use has been mainly associated within the high-vacuum range (down to 10−9 mbar), diffusion pumps today can produce pressures approaching 10−10 mbar when properly used with modern fluids and accessories. The features that make the diffusion pump attractive for high and ultra-high vacuum use are its high pumping speed for all gases and low cost per unit pumping speed when compared with other types of pump used in the same vacuum range. Diffusion pumps cannot discharge directly into the atmosphere, so a mechanical forepump is typically used to maintain an outlet pressure around 0.1 mbar.
The oil diffusion pump is operated with an oil of low vapor pressure. The high speed jet is generated by boiling the fluid and directing the vapor through a jet assembly. Note that the oil is gaseous when entering the nozzles. Within the nozzles, the flow changes from laminar, to supersonic and molecular. Often, several jets are used in series to enhance the pumping action. The outside of the diffusion pump is cooled using either air flow or a water line. As the vapor jet hits the outer cooled shell of the diffusion pump, the working fluid condenses and is recovered and directed back to the boiler. The pumped gases continue flowing to the base of the pump at increased pressure, flowing out through the diffusion pump outlet, where they are compressed to ambient pressure by the secondary mechanical forepump and exhausted. 
Unlike turbomolecular pumps and cryopumps, diffusion pumps have no moving parts and as a result are quite durable and reliable. They can function over pressure ranges of 10−10 to 10−2 mbar. They are driven only by convection and thus have a very low energy efficiency.
One major disadvantage of diffusion pumps is the tendency to backstream oil into the vacuum chamber. This oil can contaminate surfaces inside the chamber or upon contact with hot filaments or electrical discharges may result in carbonaceous or siliceous deposits. Due to backstreaming, oil diffusion pumps are not suitable for use with highly sensitive analytical equipment or other applications which require an extremely clean vacuum environment, but mercury diffusion pumps may be in the case of ultra high vacuum chambers used for metal deposition. Often cold traps and baffles are used to minimize backstreaming, although this results in some loss of pumping ability.
The oil of a diffusion pump cannot be exposed to the atmosphere when hot. If this occurs, the oil will burn and has to be replaced.
Steam ejectors.
The steam ejector is a popular form of diffusion pump for vacuum distillation and freeze-drying. A jet of steam entrains the vapour that must be removed from the vacuum chamber. Steam ejectors can have single or multiple stages, with and without condensers in between the stages.
Compressed-air ejectors.
One class of diffusion vacuum pumps is the multistage compressed-air driven ejector. It is very popular in applications where objects are moved around using suction cups and vacuum lines.

</doc>
<doc id="8295" url="https://en.wikipedia.org/wiki?curid=8295" title="Declarative memory">
Declarative memory

Declarative memory (sometimes referred to as explicit memory) is one of two types of long-term human memory. Declarative memory refers to memories that can be consciously recalled such as facts and "verbal" knowledge. Declarative memory can be divided into two categories: "episodic memory", which stores specific personal experiences, and "semantic memory", which stores factual information. 
Declarative memory's counterpart is known as non-declarative or procedural memory, which refers to unconscious memories such as skills (e.g. learning to ride a bicycle). 
While declarative memory is similar to explicit memory, declarative memory is memory that a person can state in words, while explicit memory is the deliberate recall of information that the person recognizes as a memory.
Types.
Semantic memories are those memories that store general factual knowledge that is independent of personal experience. This includes world knowledge, object knowledge, language knowledge, and conceptual priming. Some examples of semantic memory include types of food, capital cities of a geographic region, or the lexicon of a common language, such as a person's vocabulary.
Episodic memories are those memories that store chunks of observational information attached to a specific event. Some examples of episodic memory include the memory of entering a specific classroom for the first time, the memory of storing your carry-on baggage while boarding a plane headed to a specific destination on a specific day and time, the memory of being notified that you are being terminated from your job, or the memory of notifying a subordinate that they are being terminated from their job. The retrieval of these episodic memories can be thought of as the action of mentally reliving in detail the past events that they concern. Episodic memory is believed to be the system that provides the basic support for semantic memory.
History.
The study of human memory stretches back over the last 2000 years. An early attempt to understand memory can be found in Aristotle’s major treatise, On the Soul, in which he compares the human mind to a blank slate. He theorized that all humans are born free of any knowledge and are the sum of their experiences. It wasn’t until the late 1800s, however, that a young German philosopher by the name of Herman Ebbinghaus developed the first scientific approach to studying memory. While some of his findings have endured and remain relevant to this day (Learning Curve), his greatest contribution to the field of memory research was demonstrating that memory can be studied scientifically. In 1972, Endel Tulving proposed the distinction between episodic and semantic memory. This was quickly adopted and is now widely accepted. Following this, in 1985, Daniel Schacter proposed a more general distinction between explicit (declarative) and implicit (procedural) memory With the recent advances in neuroimaging technology, there have been a multitude of findings linking specific brain areas to declarative memory. Despite these advances in Cognitive psychology, there is still much to be discovered in terms of the operating mechanisms of declarative memory. It is unclear whether declarative memory is mediated by a particular “memory system” or if it is more accurately classified as a “type of knowledge” and it is not known how or why declarative memory evolved to begin with.
Neuropsychology.
Normal brain function.
Hippocampus.
Although many psychologists believe that the entire brain is involved with memory, the "hippocampus" and surrounding structures appear to be most important in declarative memory specifically. The ability to retain and recall episodic memories is highly dependent on the hippocampus, whereas the formation of new declarative memories relies on both the hippocampus and "parahippocampus" Other studies have found that the parahippocampal cortices were related to superior "Recognition Memory".
The Three Stage Model was developed by Eichenbaum, et. Al (2001), and proposes that the hippocampus does three things with episodic memory:
To support this model, a version of Piaget’s Transitive Inference Task was used to show that the hippocampus is in fact used as the memory space.
When experiencing an event for the first time, a link is formed in the hippocampus allowing us to recall that event in the future. Separate links are also made for features related to that event. For example, when you meet someone new, a unique link is created for them. More links are then connected to that person’s link so you can remember what colour their shirt was, what the weather was like when you met them, etc. Specific episodes are made easier to remember and recall by repeatedly exposing oneself to them (which strengthens the links in the memory space) allowing for faster retrieval when remembering.
Hippocampal cells ("neurons") are activated depending on what information one is exposed to at that moment. Some cells are specific to spatial information, certain stimuli (smells, etc.), or behaviours as has been shown in a "Radial Maze Task". It is therefore the hippocampus that allows us to recognize certain situations, environments, etc. as being either distinct or similar to others. However, the Three Stage Model does not incorporate the importance of other cortical structures in memory.
The anatomy of the hippocampus is largely conserved across mammals, and the role of these areas in declarative memory are conserved across species as well. The organization and neural pathways of the hippocampus are very similar in humans and other mammal species. In humans and other mammals, a cross-section of the hippocampus shows the dentate gyrus as well as the dense cell layers of the CA fields. The intrinsic connectivity of these areas are also conserved.
Results from an experiment by Davachi, Mitchell, and Wagner (2003) and numerous subsequent studies (Davachi, 2006) show that activation in the hippocampus during encoding is related to a subject's ability to recall prior events or later relational memories. These tests did not differentiate between individual test items later seen and those forgotten.
Prefrontal cortex.
The lateral Prefrontal cortex (PFC) is essential for remembering contextual details of an experience rather than for memory formation. The PFC is also more involved with episodic memory than semantic memory, although it does play a small role in semantics.
Using PET studies and word stimuli, Endel Tulving found that remembering is an automatic process. It is also well documented that a hemispheric asymmetry occurs in the PFC: When encoding memories, the Left Dorsolateral PFC (LPFC) is activated, and when retrieving memories, activation is seen in the Right Dorsolateral PFC (RPFC).
Studies have also shown that the PFC is extremely involved with autonoetic consciousness (See Tulving's theory). This is responsible for humans’ recollective experiences and ‘mental time travelling’ abilities (characteristics of episodic memory).
Amygdala.
The amygdala is believed to be involved in the encoding and retrieval of emotionally charged memories. Much of the evidence for this has come from research on a phenomenon known as flashbulb memories. These are instances in which memories of powerful emotional events are more highly detailed and enduring than regular memories (e.g. September 11 attacks, assassination of JFK). These memories have been linked to increased activation in the amygdala. Recent studies of patients with damage to the amygdala suggest that it is involved in memory for general knowledge, and not for specific information.
Other structures involved.
The regions of the Diencephalon have shown brain activation when a remote memory is being recovered and the Occipital lobe, Ventral Temporal lobe, and Fusiform gyrus all play a role in memory formation.
Lesion studies.
Lesion studies are commonly used in cognitive neuroscience research. Lesions can occur naturally through trauma or disease, or they can be surgically induced by researchers. In the study of declarative memory, the hippocampus and the amygdala are two structures frequently examined using this technique.
Hippocampal lesion studies.
The "Morris water navigation task" tests spatial learning in rats. In this test rats learn to escape from a pool by swimming toward a platform submerged just below the surface of the water. Visual cues that surround the pool (e.g. a chair or window) help the rat to locate the platform on subsequent trials. The rats' use of specific events, cues and places are all forms of declarative memory. Two groups of rats are observed: a control group with no lesions and an experimental group with hippocampal lesions. In this task created by Morris, "et al.", rats are placed in the pool at the same position for 12 trials. Each trial is timed and the path taken by the rats is recorded. Rats with hippocampal lesions successfully learn to find the platform. If the starting point is moved, the rats with hippocampal lesions typically fail to locate the platform. The control rats, however, are able to find the platform using the cues acquired during the learning trials. This demonstrates the involvement of the hippocampus in declarative memory.
The "Odor-odor Recognition Task", devised by Bunsey and Eichenbaum, involves a social encounter between two rats (a "subject" and a "demonstrator"). The demonstrator, after eating a specific type of food, interacts with the subject rat, who then smells the food odor on the other's breath. The experimenters then present the subject rat with a decision between two food options; the food previously eaten by the demonstrator, and a novel food. The researchers found that when there was no time delay, both control rats and rats with lesions chose the familiar food. After 24 hours, however, the rats with hippocampal lesions were just as likely to eat both types of food, while control rats chose the familiar food. This can be attributed to the inability to form episodic memories due to lesions in the hippocampus. The effects of this study can be observed in humans with amnesia, indicating the role of the hippocampus in developing episodic memories that can be generalized to similar situations.
Henry Molaison, previously known as H.M., had parts of both his left and right medial temporal lobes (hippocampi) removed which resulted in the loss of the ability to form new memories. The long-term declarative memory was crucially affected when the structures from the medial temporal lobe were removed, including the ability to form new semantic knowledge and memories. The dissociation in Molaison between the acquisition of declarative memory and other kinds of learning was seen initially in motor learning. Molaison's declarative memory was not functioning, as was seen when Molaison completed the task of repetition priming. His performance does improve over trials, however, his scores were inferior to those of control participants. In the condition of Molaison the same results from this priming task are reflected when looking at the other basic memory functions like remembering, recall and recognizing. Lesions should not be interpreted as an all-or-nothing condition, in the case of Molaison not all memory and recognition is lost, although the declarative memory is severely damaged he still has a sense of self and memories that were developed before the lesion occurred.
Patient R.B. was another clinical case reinforcing the role of the hippocampus in declarative memory. After suffering an ischemic episode during a cardiac bypass operation, Patient R.B. awoke with a severe anterograde amnesic disorder. IQ and cognition were unaffected, but declarative memory deficits were observed (although not to the extent of that seen in Molaison). Upon death, an autopsy revealed that Patient R.B. had bilateral lesions of the CA1 cell region along the whole length of the hippocampus.
Amygdala lesion studies.
Adolph, Cahill and Schul completed a study showing that emotional arousal facilitates the encoding of material into long term declarative memory. They selected two subjects with bilateral damage to the amygdala, as well as six control subjects and six subjects with brain damage. All subjects were shown a series of twelve slides accompanied by a narrative. The slides varied in the degree to which they evoked emotion - slides 1 through 4 and slides 9 through 12 contain non-emotional content. Slides 5 through 8 contain emotional material, and the seventh slide contained the most emotionally arousing image and description (a picture of surgically repaired legs of a car crash victim).
The emotionally arousing slide (slide 7) was remembered no better by the bilateral damage participants than any of the other slides. All other participants notably remembered the seventh slide the best and in most detail out of all the other slides. This shows that the amygdala is necessary to facilitate encoding of declarative knowledge regarding emotionally arousing stimuli, but is not required for encoding knowledge of emotionally neutral stimuli.
Factors that affect declarative memory.
Stress.
Stress may have an effect on the recall of declarative memories. Lupien, et al. completed a study that had 3 phases for participants to take part in. Phase 1 involved memorizing a series of words, phase 2 entailed either a stressful (public speaking) or non-stressful situation (an attention task), and phase 3 required participants to recall the words they learned in phase 1. There were signs of decreased declarative memory performance in the participants that had to complete the stressful situation after learning the words. Recall performance after the stressful situation was found to be worse overall than after the non-stressful situation, where performance differed based on whether the participant responded to the stressful situation with an increase in measured levels of salivary cortisol.
Posttraumatic stress disorder (PTSD) emerges after exposure to a traumatic event eliciting fear, horror or helplessness that involves bodily injury, the threat of injury, or death to one’s self or another person The chronic stress in PTSD contributes to an observed decrease in hippocampal volume and declarative memory deficits.
Neurochemical factors of stress on the brain.
In the brain, Glucocorticoids (GC's) modulate the ability of the hippocampus and PFC to process memories. Cortisol is one of the most common GC’s in the human body, and hydrocortisone (a derivative of cortisol) decreases brain activity in the above areas during declarative memory retrieval.
Elevations in cortisol occur during stress, and long-term stress impairs declarative memory this way. A study done by Damoiseaux et al. (2007) evaluated the effect of glucocorticoids on MTL and PFC activation in young men. They found that GC’s given to participants 1 hour before retrieval of information impairs free recall of words, yet when administered before or after learning they had no effect. Although it is not known exactly how GC’s influence memory, there are Glucocorticoid receptors in the hippocampus and PFC that tell us these structures are targets for the circulating hormone. However, it is known that cortisone impairs memory function by reducing the blood flow in the right parahippocampal gyrus, left visual cortex, and the Cerebellum.
Note: This study involved only male subjects, which may be significant as sex steroids may have different effects in the responses to cortisol administration. Men and women also respond to emotional stimuli differently, and this may affect cortisol levels. Also, this study was the first Functional magnetic resonance imaging(fMRI) study to be done involving GC's and more research is necessary to support these findings.
Declarative memory consolidation during sleep.
It is believed that sleep plays an active role in consolidation of declarative memory. Specifically, sleep’s unique properties enhance "memory consolidation", such as the reactivation of newly learned memories during sleep. For example, it has been suggested that the central mechanism for consolidation of declarative memory during sleep is the reactivation of hippocampal memory representations. This reactivation transfers information to neocortical networks where it is integrated into long-term representations. Studies on rats involving maze learning found that hippocampal neuronal assemblies that are used in the encoding of spatial information are reactivated in the same temporal order. Similarly, positron emission tomography (PET) has shown reactivation of the "hippocampus" in slow-wave sleep (SWS) after spatial learning. Together these studies show that newly learned memories are reactivated during sleep and through this process new memory traces are consolidated. In addition, researchers have identified three types of sleep (SWS, sleep spindle and REM) in which declarative memory is consolidated.
Slow-Wave Sleep, often referred to as deep sleep, plays the most important role in consolidation of declarative memory and there is a large amount of evidence to support this claim. One study found that the first 3.5 hours of sleep offer the greatest performance enhancement on memory recall tasks because the first couple of hours are dominated by SWS. Additional hours of sleep do not add to the initial level of performance. Thus this study suggests that full sleep may not be important for optimal performance of memory. Another study shows that people who experience SWS during the first half of their sleep cycle compared to subjects who did not, showed better recall of information. However this is not the case for subjects who were tested for the second half of their sleep cycle, as they experience less SWS.
Another key piece of evidence regarding SWS’s involvement in declarative memory consolidation is a finding that people with pathological conditions of sleep, such as insomnia, exhibit both reduction in "Slow-Wave Sleep" and also have impaired consolidation of declarative memory during sleep. Another study found that middle aged people compared to young group had a worse retrieval of memories. This in turn indicated that SWS is associated with poor declarative memory consolidation but not with age itself.
Some researchers suggest that "sleep spindle", a burst of brain activity occurring during stage 2 sleep, plays a role in boosting consolidation of declarative memories. Critics point out that spindle activity is positively correlated with intelligence. In contrast, Schabus and Gruber point out that sleep spindle activity only relates to performance on newly learned memories and not to absolute performance. This supports the hypothesis that sleep spindle helps to consolidate recent memory traces but not memory performance in general. The relationship between sleep spindles and declarative memory consolidation is not yet fully understood.
There is a relatively small body of evidence that supports the idea that "REM sleep" helps consolidate highly emotional declarative memories. For instance Wagner, et al. compared memory retention for emotional versus neutral text over two instances; early sleep that is dominated by SWS and late sleep that is dominated by REM phase. This study found that sleep improved memory retention of emotional text only during late sleep phase, which was primarily REM. Similarly, Hu & Stylos-Allen, et al. performed a study with emotional versus neutral pictures and concluded that REM sleep facilitates consolidation of emotional declarative memories.
The view that sleep plays an "active" role in declarative memory consolidation is not shared by all researchers. For instance Ellenbogen, et al. argue that sleep actively protects declarative memory from associative interference. Furthermore, Wixted believes that the sole role of sleep in declarative memory consolidation is nothing more but creating ideal conditions for memory consolidation. For example, when awake, people are bombarded with mental activity which interferes with effective consolidation. However, during sleep, when interference is minimal, memories can be consolidated without associative interference. More research is needed to make a definite statement whether sleep creates favourable conditions for consolidation or it actively enhances declarative memory consolidation.
In popular culture.
Amnesiacs are frequently portrayed in television and movies. Some of the better-known examples include:
In the romantic comedy "50 First Dates" (2004), Adam Sandler plays veterinarian Henry Roth, who falls for Lucy Whitmore, played by Drew Barrymore. Having lost her short term memory in a car crash, Lucy can only remember the current day's events until she falls asleep. When she wakes up the next morning, she has no recollection of the previous day's experiences. These experiences would normally be transferred into declarative knowledge, allowing them to be recalled in the future. Although this movie is not the most accurate representation of a true amnesic patient, it is useful for informing viewers of the detrimental effects of amnesia.
"Memento" (2000) a film inspired by the case of Henry Molaison (H.M.). Guy Pearce plays an ex-insurance investigator suffering from severe anterograde amnesia caused by a head injury. Unlike most amnesiacs, Leonard retains his identity and the memories of events that occurred before the injury, but loses all ability to form new memories. This loss of ability to form new memories indicates that the head injury affected the medial temporal lobe of the brain resulting in the inability for Leonard to form declarative memory.
"Finding Nemo" features a reef fish named Dory with an inability to develop declarative memory. This prevents her from learning or retaining any new information such as names or directions. The exact origin of Dory's impairment is not mentioned in the film, but her memory loss accurately portrays the difficulties facing amnesiacs.

</doc>
<doc id="8299" url="https://en.wikipedia.org/wiki?curid=8299" title="Domenico Alberti">
Domenico Alberti

Domenico Alberti (c. 1710 – 14 October 1740) was an Italian singer, harpsichordist, and composer.
Alberti was born in Venice and studied music with Antonio Lotti. He wrote operas, songs, and sonatas for keyboard instruments, for which he is best known today. These sonatas frequently employ a particular kind of arpeggiated accompaniment in the left hand that is now known as the "Alberti bass". It consists of regular broken chords, with the lowest note sounding first, then the highest, then the middle and then the highest again. This pattern is repeated. Today, Alberti is regarded as a minor composer, and his works are played or recorded only irregularly. The Alberti bass was used by many later composers, and it became an important element in much keyboard music of the Classical music era.
An example of Alberti bass (Mozart's "Piano Sonata, K 545"):
In his own lifetime, Alberti was known as a singer. He often used to accompany himself on the harpsichord. In 1736, he served as a page for Pietro Andrea Cappello, the Venetian ambassador to Spain. While at the Spanish court, the famous castrato singer Farinelli heard him sing. Farinelli was said to have been impressed, although Alberti was an amateur.
Alberti's best known pieces are his keyboard sonatas, although even they are very rarely performed. It is thought he wrote around 36 sonatas, of which 14 have survived. They all have two movements, each in binary form.
It is probable that Mozart's first violin sonatas, written at the age of seven, were modeled on Alberti's work.
Alberti died in 1740 in Rome.

</doc>
<doc id="8300" url="https://en.wikipedia.org/wiki?curid=8300" title="Doris Day">
Doris Day

Doris Day (born Doris Mary Ann Kappelhoff; April 3, 1922 or 1924) is an American actress, singer, and animal welfare activist.
Day began her career as a big band singer in 1939. Her popularity began to rise after her first hit recording "Sentimental Journey", in 1945. After leaving Les Brown & His Band of Renown to embark on a solo career, Day started her long-lasting partnership with Columbia Records, which remained her only recording label. The contract lasted from 1947 to 1967 and included more than 650 recordings, making Day one of the most popular and acclaimed singers of the 20th century. She received the Grammy Lifetime Achievement Award and a Legend Award from the Society of Singers. In 2011, she released her 29th studio album, "My Heart", which debuted at No. 9 on the UK Top 40 charts. As of January 2014, Day is the oldest living artist to score a UK Top 10 with an album featuring new material.
In 1948, Day was persuaded by songwriters Sammy Cahn and Jule Styne and by Al Levy, her agent at the time, to audition for "Romance on the High Seas", which led to a 20-year career in film. She became well known for her string of musicals with Gordon MacRae in the early 1950s, and later, romantic comedies with handsome leading men such as Clark Gable in "Teacher's Pet" (1958), Rock Hudson in "Pillow Talk" (1959), "Lover Come Back" (1961) and "Send Me No Flowers" (1964), Cary Grant in "That Touch of Mink" (1962), and James Garner in "The Thrill of It All" and "Move Over, Darling" (1963). She was ranked the biggest box-office star, the only woman appearing on that list in the era, for four years (1960, 1962, 1963 and 1964), ranking in the top 10 for ten years (1951–52, and 1959–66). She became the top-ranking female box-office star of all time and is currently ranked sixth among the top 10 box office performers (male and female), as of 2012.
Day received an Academy Award nomination for her performance in "Pillow Talk", won three Henrietta Awards (World Film Favorite), and received the Los Angeles Film Critics Association's Career Achievement Award. In 1989, she received the Cecil B. DeMille Award for lifetime achievement in motion pictures. She made her last film in 1968.
Her strong commitment to animal welfare began in 1971, when she co-founded Actors and Others for Animals. She started her own non-profit organization in the 1970s, the Doris Day Animal Foundation and, later, the Doris Day Animal League (DDAL). Establishing the annual observance Spay Day USA in 1995, the Doris Day Animal League now partners with The Humane Society of the United States and continues to be a leading advocacy organization. In 2004, she received the Presidential Medal of Freedom from President George W. Bush in recognition of her distinguished service to the country. Day is retired from acting and performing, but has continued her work in animal rights and welfare causes.
Early life.
Doris Mary Ann Kappelhoff was born on April 3, 1922 or 1924 in Cincinnati, the daughter of Alma Sophia (née Welz), a housewife, and William Joseph Kappelhoff, a music teacher and choir master. All of her grandparents were German immigrants.
The youngest of three siblings, she had two older brothers: Richard (who died before her birth) and Paul, several years older. Due to her father's alleged infidelity, her parents separated. She developed an early interest in dance, and in the mid-1930s formed a dance duo with Jerry Doherty that performed locally in Cincinnati. A car accident on October 13, 1937, injured her legs and curtailed her prospects as a professional dancer.
Career.
Early career (1938–1947).
While recovering, Day started to sing along with the radio and discovered a talent she did not know she had. Day said: "During this long, boring period, I used to while away a lot of time listening to the radio, sometimes singing along with the likes of Benny Goodman, Duke Ellington, Tommy Dorsey, and Glenn Miller [...]. But the one radio voice I listened to above others belonged to Ella Fitzgerald. There was a quality to her voice that fascinated me, and I'd sing along with her, trying to catch the subtle ways she shaded her voice, the casual yet clean way she sang the words."
Observing her daughter rekindled Alma's interest in show business, and she decided to give Doris singing lessons. She engaged a teacher, Grace Raine. After three lessons, Raine told Alma that young Doris had "tremendous potential", which led Alma to give her daughter three lessons a week for the price of one. Years later, Day said that Raine had the biggest effect on her singing style and career.
During the eight months she was taking singing lessons, Day had her first professional jobs as a vocalist, on the WLW radio program "Carlin's Carnival", and in a local restaurant, Charlie Yee's Shanghai Inn. During her radio performances, Day first caught the attention of Barney Rapp, who was looking for a girl vocalist and asked if Day would like to audition for the job. According to Rapp, he had auditioned about 200 singers when Day got the job.
While working for Rapp in 1939, she adopted the stage surname "Day", at Rapp's suggestion. Rapp felt that "Kappelhoff" was too long for marquees, and he admired her rendition of the song "Day After Day". After working with Rapp, Day worked with bandleaders Jimmy James, Bob Crosby, and Les Brown.
While working with Brown, Day scored her first hit recording, "Sentimental Journey", released in early 1945. It soon became an anthem of the desire of World War II demobilizing troops to return home. This song is still associated with Day, and she rerecorded it on several occasions, including a version in her 1971 television special. At one point in 1945–46, Day (as vocalist with the Les Brown Band) had six other top ten hits on the" Billboard" chart: "My Dreams Are Getting Better All the Time", "'Tain't Me", "Till The End of Time", "You Won't Be Satisfied (Until You Break My Heart)", "The Whole World is Singing My Song", and "I Got the Sun in the Mornin'". By the time she left Brown's band in August 1946, she was the highest paid female band vocalist in the world.
Early film career (1948–1954).
While singing with the Les Brown band and for nearly two years on Bob Hope's weekly radio program, she toured extensively across the United States. Her popularity as a radio performer and vocalist, which included a second hit record "My Dreams Are Getting Better All the Time", led directly to a career in films. In 1941, Day appeared as a singer with the Les Brown band in a soundie (a Cinemasters production). During her separation from her second husband, George Weidler, in 1947, Day reportedly intended to leave Los Angeles and return to Cincinnati. Her agent Al Levy convinced her to attend a party at the home of composer Jule Styne. Her performance of the song "Embraceable You" impressed Styne and his partner, Sammy Cahn, and they recommended her for a role in "Romance on the High Seas", which they were working on for Warner Brothers. Betty Hutton's withdrawal due to pregnancy left the main role to be re-cast, and Day got the part after auditioning for Michael Curtiz. The film provided her with her first #1 hit recording as a soloist, "It's Magic", which followed by two months her first #1 hit ("Love Somebody" in 1948) recorded as a duet with Buddy Clark. Day recorded the song "Someone like You", written by Ralph Blane and composed by Harry Warren, in circa 1948–49 before the 1949 film "My Dream Is Yours", which featured the song. The song was given an 81 overall rating with conductor George Siravo's score praised as "splendid". It was subsequently recorded by Ella Fitzgerald in 1949 and Peggy Lee.
In 1950, U.S. servicemen in Korea voted her their favorite star. She continued to make minor and frequently nostalgic period musicals such as "Starlift", "The West Point Story", "On Moonlight Bay", "By the Light of the Silvery Moon", and "Tea For Two" for Warner Brothers. Her most commercially successful film for Warners was "I'll See You in My Dreams", which broke box-office records of 20 years' standing during its premiere engagement at Radio City Music Hall in 1951. In 1953, Day appeared as the title character in the comedic western-themed musical, "Calamity Jane", winning the Academy Award for Best Original Song for "Secret Love" (her recording of which became her fourth #1 hit single in the U.S.). Between 1950 and 1953, the albums from six of her movie musicals charted in the Top 10, three of them at #1. After filming "Lucky Me" with Phil Silvers and "Young at Heart" (both 1954) with Frank Sinatra, Day chose not to renew her contract with Warner Brothers. She elected to work under the advice and management of her third husband, Marty Melcher, whom she married in Burbank on April 3, 1951.
Breakthrough (1955–1958).
Having become primarily recognized as a musical-comedy actress, Day gradually took on more dramatic roles to broaden her range. Her dramatic star-turn as singer Ruth Etting in "Love Me or Leave Me" (1955), co-starring James Cagney, received critical and commercial success, becoming Day's biggest hit thus far. Day would later call it, in her autobiography, her best film performance. Producer Joe Pasternak said, "I was stunned that Doris didn't get an Oscar nomination." The soundtrack album from that movie was a No. 1 hit that stayed charted for twenty-eight weeks and became the recording industry's third biggest selling album of the entire decade.
Day starred in Alfred Hitchcock's suspense film, "The Man Who Knew Too Much" (1956) with James Stewart. She sang only two songs in the film, "Que Sera, Sera (Whatever Will Be, Will Be)", which won an Academy Award for Best Original Song, and "We'll Love Again". During the filming, Day became concerned about Hitchcock's lack of direction. She recalled being worried if she was pleasing him and confronted him on her performance. He told her, "If you weren't doing what I liked, you'd know." At the premiere, Hitchcock was asked how he got such a great performance from Day. He replied, "It wasn't me; it was Doris." The film was Day's 10th movie to be in the Top 10 at the box office. Day played the title role in the thriller/noir "Julie" (1956) with Louis Jourdan. The film received poor press acclaim and was unpopular with audiences.
After three successive dramatic films, Day returned to her musical/comedic roots in 1957's "The Pajama Game" with John Raitt. The film was based on the Broadway play of the same name. She worked with Paramount Pictures for the comedy "Teacher's Pet" (1958), alongside Clark Gable and Gig Young. She co-starred with Richard Widmark and Gig Young in the romantic comedy film, "The Tunnel of Love" (1958), but found scant success opposite Jack Lemmon in "It Happened to Jane" (1959).
"Billboard" annual nationwide poll of disc jockeys had ranked Day as the No. 1 female vocalist nine times in ten years (1949 through 1958), but her success and popularity as a singer was now being overshadowed by her box-office appeal.
Box-office success (1959–1968).
In 1959, Day entered her most successful phase as a film actress with a series of romantic comedies. This success began with "Pillow Talk" (1959), co-starring Rock Hudson, who became a lifelong friend, and Tony Randall. Day received a nomination for an Academy Award for Best Actress. Day, Hudson, and Randall made two more films together, "Lover Come Back" (1961) and "Send Me No Flowers" (1964).
She also starred with David Niven and Janis Paige in the hit "Please Don't Eat the Daisies". In 1962, Day appeared with Cary Grant in the comedy "That Touch of Mink", the first film in history ever to gross $1 million in one theatre (Radio City Music Hall). Day was in the Top 10 at the box office ten times. During 1960 and the 1962 to 1964 period, she ranked number one at the box office, the second woman to be number one four times. She set an unprecedented record that has yet to be equaled, receiving seven consecutive Laurel Awards as the top female box office star.
Day teamed up with James Garner, starting with "The Thrill of It All", followed by "Move Over, Darling" (both 1963). "Move Over, Darling" was originally titled "Something's Got to Give", a 1962 comeback vehicle for Marilyn Monroe. Filming was suspended following Monroe's dismissal and her subsequent death. A year later, filming resumed with Day recast as the leading lady. The film's theme song, "Move Over, Darling", was co-written by her son specifically for her and charted at #8 in the U.K. In between these comedic roles, Day co-starred with Rex Harrison in the movie thriller "Midnight Lace" (1960), an updating of the classic stage thriller, "Gaslight".
By the late 1960s, the sexual revolution of the baby boomer generation had refocused public attitudes about sex. Times changed, but Day's films did not. Day's next film, "Do Not Disturb" (1965), was popular with audiences, but her popularity soon waned. Critics and comics dubbed Day "The World's Oldest Virgin", and audiences began to shy away from her films. As a result, she slipped from the list of top box-office stars, last appearing in the top ten in 1966 with the hit film "The Glass Bottom Boat". One of the roles she turned down was that of "Mrs. Robinson" in "The Graduate", a role that eventually went to Anne Bancroft. In her published memoirs, Day said she had rejected the part on moral grounds: she found the script "vulgar and offensive".
She starred in the western film "The Ballad of Josie" (1967). That same year, Day recorded "The Love Album", although it was not released until 1994. The following year (1968), she starred in the comedy film "Where Were You When the Lights Went Out?" which centers on the Northeast blackout of November 9, 1965. Her final feature, the comedy "With Six You Get Eggroll", was released in 1968.
From 1959-70, Day received nine Laurel Award nominations (and won four times) for best female performance in eight comedies and one drama. From 1959 through 1969, she received six Golden Globe nominations for best female performance in three comedies, one drama ("Midnight Lace"), one musical ("Jumbo"), and her television series.
Bankruptcy and television career.
When her third husband Martin Melcher died on April 20, 1968, a shocked Day discovered that Melcher and his business partner Jerome Bernard Rosenthal had squandered her earnings, leaving her deeply in debt. Rosenthal had been her attorney since 1949, when he represented her in her uncontested divorce action against her second husband, saxophonist George W. Weidler. In February 1969, Day filed suit against Rosenthal and won the then-largest civil judgment (over $20 million) in the state of California. (She later settled for about one-quarter of the amount originally awarded.)
Day also learned that Melcher had committed her to a television series, which became "The Doris Day Show".
Day hated the idea of performing on television, but felt obliged to it. "There was a contract. I didn't know about it. I never wanted to do TV, but I gave it 100 percent anyway. That's the only way I know how to do it." The first episode of "The Doris Day Show" aired on September 24, 1968, and, from 1968 to 1973, employed "Que Sera, Sera" as its theme song. Day grudgingly persevered (she needed the work to help pay off her debts), but only after CBS ceded creative control to her and her son. The successful show enjoyed a five-year run, and functioned as a curtain-raiser for the popular "Carol Burnett Show". It is remembered today for its abrupt season-to-season changes in casting and premise. It was not widely syndicated as many of its contemporaries were, and was re-broadcast very little outside the United States, Australia and the UK. By the end of its run in 1973, public tastes had changed and her firmly established persona was regarded as passé. She largely retired from acting after "The Doris Day Show", but did complete two television specials, "The Doris Mary Anne Kappelhoff Special" (1971) and "Doris Day to Day" (1975). She appeared in a John Denver TV special in 1974.
In the 1985–86 season, Day hosted her own television talk show, "Doris Day's Best Friends", on CBN. The network canceled the show after 26 episodes, despite the worldwide publicity it received.
1970s.
On September 18, 1974, courts awarded Day $22,835,646 for fraud and malpractice in an hour-long oral decision by Superior Judge Lester E. Olson, ending a 99-day trial that involved 18 consolidated lawsuits and countersuits filed by Day and Rosenthal that involved Rosenthal's handling of her finances after she terminated him in July 1968. The civil trial included 14,451 pages of transcript from 67 witnesses. Represented by attorney Robert Winslow and the law firm of Mitchell, Silberberg & Knupp LLP, Day was awarded $1 million punitive damages, $5.6 million plus $2 million interest for losses incurred in a sham oil venture; $3.4 million plus $1.2 million interest over a hotel venture; $2.2 million plus $793,800 interest for duplicate or unnecessary fees paid to Rosenthal; more than $2 million to recoup loans to Rosenthal; $3.9 million plus $1 million interest for fraud, and $850,000 attorney fees for Day. Olson enjoined Rosenthal from filing any further lawsuits against Day or her business operations. (Rosenthal had filed more than 20 suits from 1969 to 1974). Olson, an expert in complex financial marital settlements, read every page of 3,275 individual exhibits and 68 boxes of miscellaneous financial records. In October 1979, Rosenthal's liability insurer settled with Day for about $6 million payable in 23 annual installments.
Rosenthal filed an appeal in the 2nd District Court of Appeal. He filed six additional lawsuits related to the case. Two were libel suits, one against Day and her publishers over comments she made about Rosenthal in her book in which he sought damages. The others sought court determinations that insurance companies and individual lawyers failed to defend Rosenthal properly before Olson and in appellate stages. In April 1979, he filed an unsuccessful suit to set aside the $6 million settlement with Day and recover damages from everyone involved in agreeing, supposedly without his permission, to the payment.
1980s and 1990s.
In October 1985, the California Supreme Court rejected Rosenthal's appeal of the multimillion-dollar judgment against him for legal malpractice, and upheld conclusions of a trial court and a Court of Appeal that Rosenthal acted improperly. In April 1986, the U.S. Supreme Court refused to review the lower court's judgment. In June 1987, Rosenthal filed a $30 million lawsuit against lawyers he claimed cheated him out of millions of dollars in real estate investments. He named Day as a co-defendant, describing her as an "unwilling, involuntary plaintiff whose consent cannot be obtained". Rosenthal claimed that millions of dollars Day lost were in real estate sold after Melcher died in 1968, in which Rosenthal asserted that the attorneys gave Day bad advice, telling her to sell, at a loss, three hotels, in Palo Alto, California, Dallas, Texas and Atlanta, Georgia and some oil leases in Kentucky and Ohio. Rosenthal claimed he had made the investments under a long-term plan, and did not intend to sell them until they appreciated in value. Two of the hotels sold in 1970 for about $7 million, and their estimated worth in 1986 was $50 million. In July 1984, after a hearing panel of the State Bar Court, after 80 days of testimony and consideration of documentary evidence, the panel accused Rosenthal of 13 separate acts of misconduct and urged his disbarment in a 34-page unsigned opinion. The State Bar Court's review department upheld the panel's findings, which asked the justices to order Rosenthal's disbarment. He continued representing clients in federal courts until the U.S. Supreme Court ruled against him on March 21, 1988. Disbarment by the Ninth U.S. Circuit Court of Appeals followed on August 19, 1988. The Supreme Court of California, in affirming the disbarment, held that Rosenthal had engaged in transactions involving undisclosed conflicts of interest, took positions adverse to his former clients, overstated expenses, double-billed for legal fees, failed to return client files, failed to provide access to records, failed to give adequate legal advice, failed to provide clients with an opportunity to obtain independent counsel, filed fraudulent claims, gave false testimony, engaged in conduct designed to harass his clients, delayed court proceedings, obstructed justice and abused legal process. Rosenthal died August 15, 2007, at the age of 96.
Terry Melcher stated that his father's premature death saved Day from financial ruin. It remains unresolved whether Marty Melcher had himself also been duped. Day stated publicly that she believed her husband innocent of any deliberate wrongdoing, stating that he "simply trusted the wrong person".
According to Day's autobiography, as told to A. E. Hotchner, the usually athletic and healthy Martin Melcher had an enlarged heart. Most of the interviews on the subject given to Hotchner (and included in Day's autobiography) paint an unflattering portrait of Melcher. Author David Kaufman asserts that one of Day's costars, actor Louis Jourdan, maintained that Day herself disliked her husband, but Day's public statements regarding Melcher appear to contradict that assertion.
Day was scheduled to present, along with Patrick Swayze and Marvin Hamlisch, the Best Original Score Oscar at the 61st Annual Academy Awards (March 1989) but she suffered a deep leg cut and was unable to attend. She had been walking through the gardens of her hotel when she cut her leg on a sprinkler. The cut required stitches.
Day was inducted into the Ohio Women's Hall of Fame in 1981 and received the Cecil B. DeMille Award for career achievement in 1989. In 1994, Day's Greatest Hits album became another entry into the British charts. The song "Perhaps, Perhaps, Perhaps" was included in the soundtrack of the Australian film "Strictly Ballroom", the theme song for the British TV show "Coupling", with Mari Wilson performing the song for the title sequence and in the promo for the fourth season of the FX series "Louie".
2000s.
In 2000, Day received the Ohio Medal of Honor, that state's highest civilian award. In 2006, Day recorded a commentary for the DVD release of the fifth (and final) season of her television show. Day has participated in telephone interviews with a radio station that celebrates her birthday with an annual Doris Day music marathon. In July 2008, she appeared on the Southern California radio show of longtime friend, newscaster George Putnam, as reported in the "Los Angeles Times".
Day turned down a tribute offer from the American Film Institute and from the Kennedy Center Honors because they require attendance in person. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush for her achievements in the entertainment industry and for her work on behalf of animals. President Bush stated, "It was a good day for our fellow creatures when she gave her good heart to the cause of animal welfare." Day declined to attend the ceremony due to her fear of flying.
Both columnist Liz Smith and film critic Rex Reed have mounted vigorous campaigns to gather support for an honorary Academy Award for Day to herald her film career and her status as the top female box-office star of all time. Day received a Grammy for Lifetime Achievement in Music in 2008, albeit again in absentia.
She received three Grammy Hall of Fame Awards, in 1998, 1999 and 2012 for her recordings of "Sentimental Journey", "Secret Love", and "Que Sera, Sera", respectively. Day was inducted into the Hit Parade Hall of Fame in 2007, and in 2010 received the first Legend Award ever presented by the Society of Singers.
2010s.
Day, aged 89, released "My Heart" in the United Kingdom on September 5, 2011, her first new album in nearly two decades, since the release of "The Love Album", which, although recorded in 1967, was not released until 1994. The album is a compilation of previously unreleased recordings produced by Day's son, Terry Melcher, before his death in 2004. Tracks include the 1970s Joe Cocker hit "You Are So Beautiful", The Beach Boys' "Disney Girls" and jazz standards such as "My Buddy", which Day originally sang in her 1951 film "I'll See You in My Dreams".
Day has dedicated this song to her son. The disc was released in the US via City Hall Records on December 6, and within two weeks had climbed to No. 12 on Amazon's bestseller list in spite of being priced over 25% higher than most CDs to raise funds for the Doris Day Animal League. It debuted at 135 on the Billboard 200, Day's first entry on that chart since "Love Him" (1963). Day became the oldest artist to score a UK Top 10 with an album featuring new material, according to the Official Charts Company, entering at No. 9. (British singer Vera Lynn reached the top of the chart in August 2009 at age 92, but with a greatest hits album, "We'll Meet Again – The Very Best of Vera Lynn".)
In January 2012, the Los Angeles Film Critics Association presented Day with a Lifetime Achievement Award.
Personal life.
Since her retirement from films, Day has lived in Carmel-by-the-Sea, California. She has many pets and adopts stray animals.
Day is a Republican. Her only child, music producer and songwriter Terry Melcher, who had a hit in the 1960s with "Hey Little Cobra" under the name The Rip Chords, died of melanoma in 2004, about five months after Day had received the Presidential Medal of Freedom. She owns a hotel in Carmel-by-the-Sea, the Cypress Inn, which Melcher had co-owned with Day.
Marriages.
In 1975, Day published her autobiography, "Doris Day: Her Own Story", an "as-told-to" work with A. E. Hotchner. The book detailed her first three marriages:
After publishing her autobiography, Day married one last time:
Animal welfare activism.
Day's interest in animal welfare and related issues apparently dates to her teen years. While recovering from an automobile accident, she took her dog Tiny for a walk without a leash. Tiny ran into the street and was killed by a passing car. Day later confessed guilt and loneliness about Tiny's untimely death. In 1971, she co-founded Actors and Others for Animals, and appeared in a series of newspaper advertisements denouncing the wearing of fur, alongside Mary Tyler Moore, Angie Dickinson, and Jayne Meadows. Day's friend, Cleveland Amory, wrote about these events in "Man Kind? Our Incredible War on Wildlife" (1974).
In 1978, Day founded the Doris Day Pet Foundation, now the Doris Day Animal Foundation (DDAF). A non-profit 501(c)(3) grant-giving public charity, DDAF funds other non-profit causes throughout the US that share DDAF's mission of helping animals and the people who love them. The DDAF continues to operate independently under Day's personal supervision.
To complement the Doris Day Animal Foundation, Day formed the Doris Day Animal League (DDAL) in 1987, a national non-profit citizen's lobbying organization whose mission is to reduce pain and suffering and protect animals through legislative initiatives. Day actively lobbied the United States Congress in support of legislation designed to safeguard animal welfare on a number of occasions and in 1995 she originated the annual Spay Day USA. The DDAL merged into The Humane Society of the United States (HSUS) in 2006. Staff members of DDAL took positions within The HSUS, and Day recorded public service announcements for the organization. The HSUS now manages World Spay Day, the annual one-day spay/neuter event that Day originated.
A facility to help abused and neglected horses opened in 2011 and bears her name—the Doris Day Horse Rescue and Adoption Center, located in Murchison, Texas, on the grounds of an animal sanctuary started by her late friend, author Cleveland Amory. Day contributed $250,000 toward the founding of the center.

</doc>
<doc id="8301" url="https://en.wikipedia.org/wiki?curid=8301" title="Distillation">
Distillation

Distillation is a process of separating the component substances from a liquid mixture by selective evaporation and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components of the mixture. In either case the process exploits differences in the volatility of mixture's components. In industrial chemistry, distillation is a unit operation of practically universal importance, but it is a physical separation process and not a chemical reaction.
Commercially, distillation has many applications. For example:
An installation for distillation, especially of alcohol, is a distillery. The distillation equipment is a still.
History.
Aristotle wrote about the process in his "Meteorologica" and even that "ordinary wine possesses a kind of exhalation, and that is why it gives out a flame". Later evidence of distillation comes from Greek alchemists working in Alexandria in the 1st century AD. Distilled water has been known since at least c. 200, when Alexander of Aphrodisias described the process. Distillation in China could have begun during the Eastern Han Dynasty (1st–2nd centuries), but archaeological evidence indicates that actual distillation of beverages began in the Jin and Southern Song dynasties. A still was found in an archaeological site in Qinglong, Hebei province dating to the 12th century. Distilled beverages were more common during the Yuan dynasty. Arabs learned the process from the Alexandrians and used it extensively in their chemical experiments.
Clear evidence of the distillation of alcohol comes from the School of Salerno in the 12th century. Fractional distillation was developed by Tadeo Alderotti in the 13th century.
In 1500, German alchemist Hieronymus Braunschweig published "Liber de arte destillandi" (The Book of the Art of Distillation) the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published The Art of Distillation the first major English compendium of practice, though it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.
As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle which acted as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols such as cognac, Scotch whisky, tequila and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for the domestic production of flower water or essential oils.
Early forms of distillation were batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists were reported to carry out as many as 500 to 600 distillations in order to obtain a pure compound.
In the early 19th century the basics of modern techniques including pre-heating and reflux were developed, particularly by the French, then in 1830 a British Patent was issued to Aeneas Coffey for a whisky distillation column, which worked continuously and may be regarded as the archetype of modern petrochemical units. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation and the same and subsequent years saw developments of this theme for oil and spirits.
With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods such as the McCabe–Thiele method and the Fenske equation. The availability of powerful computers has also allowed direct computer simulation of distillation columns.
Applications of distillation.
The application of distillation can roughly be divided in four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that in the processing of beverages and herbs, the distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate.
The main difference between laboratory scale distillation and industrial distillation is that laboratory scale distillation is often performed batch-wise, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions which are collected sequentially from most volatile to less volatile, with the bottoms (remaining least or non-volatile fraction) removed at the end. The still can then be recharged and the process repeated.
In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a better control of the separation process.
Idealized distillation model.
The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.
It is a common misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure and the vapors of each component will collect separately and purely. This, however, does not occur even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law, and assume that vapor–liquid equilibria are attained.
Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.
Dalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. Note that a mixture with a given composition has one boiling point at a given pressure, when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.
An implication of one boiling point is that lighter components never cleanly "boil first". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and thus are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily evaporate also, albeit being less concentrated in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch evaporates, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling "point", although this is because its composition changes: each intermediate mixture has its own, singular boiling point.
The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is a composition with a boiling point higher or lower than the boiling point of each separate liquid. Virtually all liquids, when mixed and heated, will display azeotropic behaviour. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.
It is not possible to "completely" purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is evaporated and the other component, e.g. a salt, has zero partial pressure for practical purposes, the process is simpler and is called evaporation in engineering.
Batch distillation.
Heating an ideal mixture of two volatile substances A and B (with A having the higher volatility, or lower boiling point) in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid which contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid: the ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This in turn means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than the starting liquid).
The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which in turn results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio A : B in the distillate.
If the difference in vapor pressure between the two components A and B is large (generally expressed as the difference in boiling points), the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.
Continuous distillation.
Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor, and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.
Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.
General improvements.
Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.
There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include
Laboratory scale distillation.
Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a "still", consists at a minimum of a reboiler or "pot" in which the source material is heated, a condenser in which the heated vapour is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also distillation types).
Simple distillation.
In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.
As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.
Fractional distillation.
For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used in order to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.
As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors are determined once again by Raoult's law. Each vaporization-condensation cycle (called a "theoretical plate") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; "theoretical plate" is thus a concept rather than an accurate description.
More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.
Steam distillation.
Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element, and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.
Steam distillation of various aromatic herbs and flowers can result in two products; an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.
Vacuum distillation.
Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.
This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.
Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e. the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.
Air-sensitive vacuum distillation.
Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.
The Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.
Short path distillation.
Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr is a kind of a short path distillation apparatus which often contain multiple chambers to collect distillate fractions.
Zone distillation.
Zone distillation is a distillation process in long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with number of repetitions of the process.
Zone distillation is a distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization with various numbers of iteration of process – with replacement distribution efficient k of crystallization on separation factor α of distillation.
Other types.
The unit process of evaporation may also be called "distillation":
Other uses:
Azeotropic distillation.
Interactions between the components of the solution create properties unique to the solution, as most processes entail nonideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not effect separation. For example, ethyl alcohol and water form an azeotrope of 95.6% at 78.1 °C.
If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a pure distillate. This set of techniques are known as azeotropic distillation. Some techniques achieve this by "jumping" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.
Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.
High boiling azeotropes, such as a 20 weight percent mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.
To break azeotropic distillations and cross distillation boundaries, such as in the DeRosier Problem, it is necessary to increase the composition of the light key in the distillate.
Breaking an azeotrope with unidirectional pressure manipulation.
The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapour pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to be remain identical further along the pressure axis either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.
This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.
Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapours being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.
Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.
A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.
Pressure-swing distillation.
Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.
This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.
One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.
Industrial distillation.
Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.
To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.
Industrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 65 centimeters to 16 meters and heights ranging from about 6 meters to 90 meters or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different "fractions" or products having different boiling points or boiling ranges. The "lightest" products (those with the lowest boiling point) exit from the top of the columns and the "heaviest" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.
Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.
Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.
Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as "plates" or "trays") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.
In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.
This packing material can either be random dumped packing (1–3" wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of "theoretical stages" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both of these factors affect packing performance.
Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work as been done on this topic by Fractionation Research, Inc. (commonly known as FRI).
Multi-effect distillation.
The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m3 of water recovered figure, and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m3.
There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.
Distillation in food processing.
Distilled beverages.
Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.

</doc>
<doc id="8302" url="https://en.wikipedia.org/wiki?curid=8302" title="David Hilbert">
David Hilbert

David Hilbert (; 23 January 1862 – 14 February 1943) was a German mathematician. He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory and the axiomatization of geometry. He also formulated the theory of Hilbert spaces, one of the foundations of functional analysis.
Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.
Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.
Life.
Early life and education.
Hilbert, the first of two children of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, Kingdom of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth.
In late 1872, Hilbert entered the Friedrichskolleg Gymnasium ("Collegium fridericianum", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the "Albertina". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but so talented he had graduated early from his gymnasium and gone to Berlin for three semesters), returned to Königsberg and entered the university. "Hilbert knew his luck when he saw it. In spite of his father's disapproval, he soon became friends with the shy, gifted Minkowski".
Career.
In 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled "Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen" ("On the invariant properties of special binary forms, in particular the spherical harmonic functions").
Hilbert remained at the University of Königsberg as a "Privatdozent" (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen. During the Klein and Hilbert years, Göttingen became the preeminent institution in the mathematical world. He remained there for the rest of his life.
Göttingen school.
Among Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.
Among his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the "Mathematische Annalen", the leading mathematical journal of the time.
Later years.
Hilbert lived to see the Nazis purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book "Grundlagen der Mathematik" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert-Ackermann book "Principles of Mathematical Logic" from 1928. Hermann Weyl's successor was Helmut Hasse.
About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether "the "Mathematical Institute" really suffered so much because of the departure of the Jews". Hilbert replied,
"Suffered? It doesn't exist any longer, does it!"
By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world six months after he had died.
Hilbert was baptized and raised in the Reformed Protestant Church. He later on left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other "a priori" assumptions.
The epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: "Ignoramus et ignorabimus" or "We do not know, we shall not know":
In English:
The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a round table discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem.
Personal life.
In 1892, Hilbert married Käthe Jerosch (1864–1945), "the daughter of a Königsberg merchant, an outspoken young lady with an independence of mind that matched his own". While at Königsberg they had their one child, Franz Hilbert (1893–1969).
Hilbert's son Franz suffered throughout his life from an undiagnosed mental illness. His inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at Göttingen.
Hilbert considered the mathematician Hermann Minkowski to be his "best and truest friend".
Hilbert solves Gordan's Problem.
Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous "finiteness theorem". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. In order to solve what had become known in some circles as "Gordan's Problem", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated "Hilbert's basis theorem", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof — it did not display "an object" — but rather, it was an existence proof and relied on use of the law of excluded middle in an infinite extension.
Hilbert sent his results to the "Mathematische Annalen". Gordan, the house expert on the theory of invariants for the "Mathematische Annalen", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:
Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the "Annalen". After having read the manuscript, Klein wrote to him, saying:
Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:
For all his successes, the nature of his proof stirred up more trouble than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that "many different constructions are subsumed under one fundamental idea" — in other words (to quote Reid): "Through a proof of existence, Hilbert had been able to obtain a construction"; "the proof" (i.e. the symbols on the page) "was" "the object". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist "school", much to Hilbert's torment in his later years. Indeed, Hilbert would lose his "gifted pupil" Weyl to intuitionism — "Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert would respond:
Axiomatization of geometry.
The text "Grundlagen der Geometrie" (tr.: "Foundations of Geometry") published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the "Grundlagen" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.
Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat "things", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.
Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.
The 23 problems.
Hilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.
After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell-Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.
The problem set was launched as a talk "The Problems of Mathematics" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:
He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.
Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.
Formalism.
In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.
Hilbert's program.
In 1920 he proposed explicitly a research project (in "metamathematics", as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:
He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the "ignorabimus", still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.
This program is still recognizable in the most popular philosophy of mathematics, where it is usually called "formalism". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.
Hilbert wrote in 1919:
Hilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.
Gödel's work.
Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.
Gödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.
Nevertheless, the subsequent achievements of proof theory at the very least "clarified" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in Alonzo Church and Alan Turing, also grew directly out of this 'debate'.
Functional analysis.
Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction.
Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.
Physics.
Until 1912, Hilbert was almost exclusively a "pure" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.
In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a "physics tutor" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.
By 1907 Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915 Einstein published several papers culminating in "The Field Equations of Gravitation" (see Einstein field equations). Nearly simultaneously David Hilbert published "The Foundations of Physics", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.
Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation and his namesake Hilbert space plays an important part in quantum theory. In 1926 von Neumann showed that if atomic states were understood as vectors in Hilbert space, then they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.
Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher math, physicists tended to be "sloppy" with it. To a "pure" mathematician like Hilbert, this was both "ugly" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic "Methoden der mathematischen Physik" (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said "Physics is too hard for physicists", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.
Number theory.
Hilbert unified the field of algebraic number theory with his 1897 treatise "Zahlbericht" (literally "report on numbers"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.
He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.
Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.

</doc>
<doc id="8303" url="https://en.wikipedia.org/wiki?curid=8303" title="Down syndrome">
Down syndrome

Down syndrome (DS or DNS), also known as trisomy 21, is a genetic disorder caused by the presence of all, or part of a third copy of chromosome 21. It is typically associated with physical growth delays, characteristic facial features, and mild to moderate intellectual disability. The average IQ of a young adult with Down syndrome is 50, equivalent to the mental age of an 8- or 9-year-old child, but this can vary widely.
The parents of the affected individual are typically genetically normal. The extra chromosome occurs by random chance. There is no known behavior or environmental factor that changes the risk. Down syndrome can be identified during pregnancy by prenatal screening followed by diagnostic testing, or after birth by direct observation and genetic testing. Since the introduction of screening, pregnancies with the diagnosis are often terminated. Regular screening for health problems common in Down syndrome is recommended throughout the person's life.
There is no cure for Down syndrome. Education and proper care have been shown to improve quality of life. Some children with Down syndrome are educated in typical school classes, while others require more specialized education. Some individuals with Down syndrome graduate from high school and a few attend post-secondary education. In adulthood, about 20% in the United States do paid work in some capacity with many requiring a sheltered work environment. Support in financial and legal matters is often needed. Life expectancy is around 50 to 60 years in the developed world with proper health care.
Down syndrome is one of the most common chromosome abnormalities in humans. It occurs in about one per 1000 babies born each year. In 2013, Down syndrome was present in 8.5 million individuals and resulted in 36,000 deaths down from 43,000 deaths in 1990. It is named after John Langdon Down, the British doctor who fully described the syndrome in 1866. Some aspects of the condition were described earlier by Jean-Étienne Dominique Esquirol in 1838 and Édouard Séguin in 1844. The genetic cause of Down syndrome—an extra copy of chromosome 21—was identified by French researchers in 1959.
Signs and symptoms.
Those with Down syndrome nearly always have physical and intellectual disabilities. As adults, their mental abilities are typically similar to those of an 8- or 9-year-old. They also typically have poor immune function and generally reach developmental milestones at a later age. They have an increased risk of a number of other health problems, including congenital heart defect, epilepsy, leukemia, thyroid diseases, and mental disorders, among others.
Physical.
People with Down syndrome may have some or all of these physical characteristics: a small chin, slanted eyes, poor muscle tone, a flat nasal bridge, a single crease of the palm, and a protruding tongue due to a small mouth and relatively large tongue. These airway changes lead to obstructive sleep apnea in around half of those with Down syndrome. Other common features include: a flat and wide face, a short neck, excessive joint flexibility, extra space between big toe and second toe, abnormal patterns on the fingertips and short fingers. Instability of the atlantoaxial joint occurs in about 20% and may lead to spinal cord injury in 1–2%. Hip dislocations may occur without trauma in up to a third of people with Down syndrome.
Growth in height is slower, resulting in adults who tend to have short stature—the average height for men is 154 cm (5 ft 1 in) and for women is 142 cm (4 ft 8 in). Individuals with Down syndrome are at increased risk for obesity as they age. Growth charts have been developed specifically for children with Down syndrome.
Neurological.
Most individuals with Down syndrome have mild (IQ: 50–70) or moderate (IQ: 35–50) intellectual disability with some cases having severe (IQ: 20–35) difficulties. Those with mosaic Down syndrome typically have IQ scores 10–30 points higher. As they age, people with Down syndrome typically perform less well than their same-age peers. Some after 30 years of age may lose their ability to speak. This syndrome causes about a third of cases of intellectual disability. Many developmental milestones are delayed with the ability to crawl typically occurring around 8 months rather than 5 months and the ability to walk independently typically occurring around 21 months rather than 14 months.
Commonly, individuals with Down syndrome have better language understanding than ability to speak. Between 10 and 45% have either a stutter or rapid and irregular speech, making it difficult to understand them. They typically do fairly well with social skills. Behavior problems are not generally as great an issue as in other syndromes associated with intellectual disability. In children with Down syndrome, mental illness occurs in nearly 30% with autism occurring in 5–10%. People with Down syndrome experience a wide range of emotions. While people with Down syndrome are generally happy, symptoms of depression and anxiety may develop in early adulthood.
Children and adults with Down syndrome are at increased risk of epileptic seizures which occur in 5–10% of children and up to 50% of adults. This includes an increased risk of a specific type of seizure called infantile spasms. Many (15%) who live 40 years or longer develop Alzheimer disease. In those who reach 60 years of age, 50–70% have the disease.
Senses.
Hearing and vision disorders occur in more than half of people with Down syndrome. 
Vision problems occur in 38 to 80%. Between 20 and 50% have strabismus, in which the two eyes do not move together. Cataracts (cloudiness of the lens of the eye) occur in 15%, and may be present at birth. Keratoconus (a thin, cone-shaped cornea) and glaucoma (increased eye pressure) are also more common, as are refractive errors requiring glasses or contacts. Brushfield spots (small white or grayish/brown spots on the outer part of the iris) are present in 38 to 85% of individuals.
Hearing problems are found in 50–90% of children with Down syndrome. This is often the result of otitis media with effusion which occurs in 50–70% and chronic ear infections which occur in 40 to 60%. Ear infections often begin in the first year of life and are partly due to poor eustachian tube function. Excessive ear wax can also cause hearing loss due to obstruction of the outer ear canal. Even a mild degree of hearing loss can have negative consequences for speech, language understanding, and academics. Additionally, it is important to rule out hearing loss as a factor in social and cognitive deterioration. Age-related hearing loss of the sensorineural type occurs at a much earlier age and affects 10–70% of people with Down syndrome.
Heart.
The rate of congenital heart disease in newborns with Down syndrome is around 40%. Of those with heart disease, about 80% have an atrioventricular septal defect or ventricular septal defect with the former being more common. Mitral valve problems become common as people age, even in those without heart problems at birth. Other problems that may occur include tetralogy of Fallot and patent ductus arteriosus. People with Down syndrome have a lower risk of hardening of the arteries.
Cancer.
Although the overall risk of cancer is not changed, the risk of leukemia and testicular cancer is increased and risk of solid cancers is reduced. Solid cancers are believed to be less common due to increased expression of tumor suppressor genes present on chromosome 21.
Cancers of the blood are 10 to 15 times more common in children with Down syndrome. In particular, acute lymphoblastic leukemia is 20 times more common and the megakaryoblastic form of acute myeloid leukemia is 500 times more common. Transient myeloproliferative disease, a disorder of blood cell production that does not occur outside of Down syndrome, affects 3–10% of infants. The disorder is typically not serious but occasionally can be. It resolves most times without treatment; however, in those who have had it, a 20 to 30% risk of developing acute lymphoblastic leukemia at a later time exists.
Endocrine.
Problems of the thyroid gland occur in 20–50% of individuals with Down syndrome. Low thyroid is the most common form, occurring in almost half of all individuals. Thyroid problems can be due to a poorly or nonfunctioning thyroid at birth (known as congenital hypothyroidism) which occurs in 1% or can develop later due to an attack on the thyroid by the immune system resulting in Graves' disease or autoimmune hypothyroidism. Type 1 diabetes mellitus is also more common.
Gastrointestinal.
Constipation occurs in nearly half of people with Down syndrome and may result in changes in behavior. One potential cause is Hirschsprung's disease, occurring in 2–15%, which is due to a lack of nerve cells controlling the colon. Other frequent congenital problems include duodenal atresia, pyloric stenosis, Meckel diverticulum, and imperforate anus. Celiac disease affects about 7–20% and gastroesophageal reflux disease is also more common.
Teeth.
Individuals with Down syndrome tend to be more susceptible to gingivitis as well as early, severe periodontal disease, necrotising ulcerative gingivitis, and early tooth loss, especially in the lower front teeth. While plaque and poor oral hygiene are contributing factors, the severity of these periodontal disease cannot be explained solely by external factors. Research suggests that the severity is likely a result of a weakened immune system. The weakened immune system also contributes to increased incidence of yeast infections in the mouth (from Candida albicans).
Individuals with Down syndrome also tend to have a more alkaline saliva resulting in a greater resistance to tooth decay, despite decreased quantities of saliva, less effective oral hygiene habits and higher plaque indexes.
Higher rates of tooth wear and bruxism are also common. Other common oral manifestations of Down syndrome include enlarged hypotonic tongue, crusted and hypotonic lips, mouth breathing, narrow palate with crowded teeth, class III malocclusion with an underdeveloped maxilla and posterior crossbite, delayed exfoliation of baby teeth and delayed eruption of adult teeth, shorter roots on teeth, and often missing and malformed (usually smaller) teeth. Less common manifestations include cleft lip and palate, enamel hypocalcification (20% prevalence).
Fertility.
Males with Down syndrome usually do not father children, while females have lower rates of fertility relative to those who are unaffected. Fertility is estimated to be present in 30–50% of females. Menopause typically occurs at an earlier age. The poor fertility in males is thought to be due to problems with sperm development; however, it may also be related to not being sexually active. As of 2006, three instances of males with Down syndrome fathering children and 26 cases of females having children have been reported. Without assisted reproductive technologies, around half of the children of someone with Down syndrome will also have the syndrome.
Genetics.
Down syndrome is caused by having three copies of the genes on chromosome 21, rather than the usual two. The parents of the affected individual are typically genetically normal. Those who have one child with Down syndrome have about a 1% risk of having a second child with the syndrome, if both parents are found to have normal karyotypes.
The extra chromosome content can arise through several different ways. The most common cause (about 92–95% of cases) is a complete extra copy of chromosome 21, resulting in trisomy 21. In 1.0 to 2.5% of cases, some of the cells in the body are normal and others have trisomy 21, known as mosaic Down syndrome. The other common mechanisms that can give rise to Down syndrome include: a Robertsonian translocation, isochromosome, or ring chromosome. These contain additional material from chromosome 21 and occur in about 2.5% of cases. An isochromosome results when the two long arms of a chromosome separate together rather than the long and short arm separating together during egg or sperm development.
Trisomy 21.
Trisomy 21 (also known by the karyotype 47,XX,+21 for females and 47,XY,+21 for males) is caused by a failure of the 21st chromosome to separate during egg or sperm development. As a result, a sperm or egg cell is produced with an extra copy of chromosome 21; this cell thus has 24 chromosomes. When combined with a normal cell from the other parent, the baby has 47 chromosomes, with three copies of chromosome 21. About 88% of cases of trisomy 21 result from nonseparation of the chromosomes in the mother, 8% from nonseparation in the father, and 3% after the egg and sperm have merged.
Translocation.
The extra chromosome 21 material may also occur due to a Robertsonian translocation in 2–4% cases. In this situation, the long arm of chromosome 21 is attached to another chromosome, often chromosome 14. In a male affected with Down syndrome, it results in a karyotype of 46XY,t(14q21q). This may be a new mutation or previously present in one of the parents. The parent with such a translocation is usually normal physically and mentally; however, during production of egg or sperm cells, a higher chance of creating reproductive cells with extra chromosome 21 material exists. This results in a 15% chance of having a child with Down syndrome when the mother is affected and a less than 5% probability if the father is affected. The probability of this type of Down syndrome is not related to the mother's age. Some children without Down syndrome may inherit the translocation and have a higher probability of having children of their own with Down syndrome. In this case it is sometimes known as familial Down syndrome.
Mechanism.
The extra genetic material present in DS results in overexpression of a portion of the 310 genes located on chromosome 21. This overexpression has been estimated at around 50%. Some research has suggested the Down syndrome critical region is located at bands 21q22.1–q22.3, with this area including genes for amyloid, superoxide dismutase, and likely the ETS2 proto oncogene. Other research, however, has not confirmed these findings. microRNAs is also proposed to be involved.
The dementia which occurs in Down syndrome is due to an excess of amyloid beta peptide produced in the brain and is similar to Alzheimer's disease. This peptide is processed from amyloid precursor protein, the gene for which is located on chromosome 21. Senile plaques and neurofibrillary tangles are present in nearly all by 35 years of age, though dementia may not be present. Those with DS also lack a normal number of lymphocytes and produce less antibodies which contributes to their increased risk of infection.
Epigenetics.
Down syndrome is associated with an increased risk of many chronic diseases that are typically associated with older age such as Alzheimer's disease. The accelerated aging suggest that trisomy 21 increases the biological age of tissues, but molecular evidence for this hypothesis is sparse. According to a biomarker of tissue age known as epigenetic clock, trisomy 21 increases the age of blood and brain tissue (on average by 6.6 years).
Screening.
Guidelines recommend screening for Down syndrome to be offered to all pregnant women, regardless of age. A number of tests are used, with varying levels of accuracy. They are typically used in combination to increase the detection rate. None can be definitive, thus if screening is positive, either amniocentesis or chorionic villous sampling is required to confirm the diagnosis. Screening in both the first and second trimesters is better than just screening in the first trimester. The different screening techniques in use are able to pick up 90 to 95% of cases with a false-positive rate of 2 to 5%.
Ultrasound.
Ultrasound imaging can be used to screen for Down syndrome. Findings that indicate increased risk when seen at 14 to 24 weeks of gestation include a small or no nasal bone, large ventricles, nuchal fold thickness, and an abnormal right subclavian artery, among others. The presence or absence of many markers is more accurate. Increased fetal nuchal translucency (NT) indicates an increased risk of Down syndrome picking up 75–80% of cases and being falsely positive in 6%.
Blood tests.
Several blood markers can be measured to predict the risk of Down syndrome during the first or second trimester. Testing in both trimesters is sometimes recommended and test results are often combined with ultrasound results. In the second trimester, often two or three tests are used in combination with two or three of: α-fetoprotein, unconjugated estriol, total hCG, and free βhCG detecting about 60–70% of cases.
Testing of the mother's blood for fetal DNA is being studied and appears promising in the first trimester. The International Society for Prenatal Diagnosis considers it a reasonable screening option for those women whose pregnancies are at a high risk for trisomy 21. Accuracy has been reported at 98.6% in the first trimester of pregnancy. Confirmatory testing by invasive techniques (amniocentesis, CVS) is still required to confirm the screening result.
Diagnosis.
Before birth.
When screening tests predict a high risk of Down syndrome, a more invasive diagnostic test (amniocentesis or chorionic villus sampling) is needed to confirm the diagnosis. If Down syndrome occurs in one in 500 pregnancies and the test used has a 5% false-positive rate, this means, of 28 women who test positive on screening, only one will have Down syndrome confirmed. If the screening test has a 2% false-positive rate, this means one of 10 who test positive on screening have a fetus with DS. Amniocentesis and chorionic villus sampling are more reliable tests, but they increase the risk of miscarriage between 0.5 and 1%. The risk of limb problems is increased in the offspring due to the procedure. The risk from the procedure is greater the earlier it is performed, thus amniocentesis is not recommended before 15 weeks gestational age and chorionic villus sampling before 10 weeks gestational age.
Abortion rates.
About 92% of pregnancies in Europe with a diagnosis of Down syndrome are terminated. In the United States, termination rates are around 67%, but this rate varies significantly depending upon the population evaluated. When nonpregnant people are asked if they would have a termination if their fetus tested positive, 23–33% said yes, when high-risk pregnant women were asked, 46–86% said yes, and when women who screened positive are asked, 89–97% say yes.
After birth.
The diagnosis can often be suspected based on the child's physical appearance at birth. An analysis of the child's chromosomes is needed to confirm the diagnosis, and to determine if a translocation is present, as this may help determine the risk of the child's parents having further children with Down syndrome. Parents generally wish to know the possible diagnosis once it is suspected and do not wish pity. The National Down Syndrome Society have developed information regarding the positive aspects of life with Down Syndrome.
Management.
Efforts such as early childhood intervention, screening for common problems, medical treatment where indicated, a good family environment, and work-related training can improve the development of children with Down syndrome. Education and proper care can improve quality of life. Raising a child with Down syndrome is more work for parents than raising an unaffected child. Typical childhood vaccinations are recommended.
Health screening.
A number of health organizations have issued recommendations for screening those with Down syndrome for particular diseases. This is recommended to be done systematically.
At birth, all children should get an electrocardiogram and ultrasound of the heart. Surgical repair of heart problems may be required as early as three months of age. Heart valve problems may occur in young adults, and further ultrasound evaluation may be needed in adolescents and in early adulthood. Due to the elevated risk of testicular cancer, some recommend checking the person's testicles yearly.
Cognitive development.
Hearing aids or other amplification devices can be useful for language learning in those with hearing loss. Speech therapy may be useful and is recommended to be started around 9 months of age. As those with Down's typically have good hand-eye coordination, learning sign language may be possible. Augmentative and alternative communication methods, such as pointing, body language, objects, or pictures, are often used to help with communication. Behavioral issues and mental illness are typically managed with counseling or medications.
Education programs before reaching school age may be useful. School-age children with Down syndrome may benefit from inclusive education (whereby students of differing abilities are placed in classes with their peers of the same age), provided some adjustments are made to the curriculum. Evidence to support this, however, is not very strong. In the United States, the Individuals with Disabilities Education Act of 1975 requires public schools generally to allow attendance by students with Down's.
Other.
Tympanostomy tubes are often needed and often more than one set during the person's childhood. Tonsillectomy is also often done to help with sleep apnea and throat infections. Surgery, however, does not always address the sleep apnea and a continuous positive airway pressure (CPAP) machine may be useful. Physical therapy and participation in physical education may improve motor skills. Evidence to support this in adults, however, is not very good.
Efforts to prevent respiratory syncytial virus (RSV) infection with human monoclonal antibodies should be considered, especially in those with heart problems. In those who develop dementia there is no evidence for memantine, donepezil, rivastigmine, or galantamine.
Plastic surgery has been suggested as a method of improving the appearance and thus the acceptance of people with Down's. It has also been proposed as a way to improve speech. Evidence, however, does not support a meaningful difference in either of these outcomes. Plastic surgery on children with Down syndrome is uncommon, and continues to be controversial. The U.S. National Down Syndrome Society views the goal as one of mutual respect and acceptance, not appearance.
Many alternative medical techniques are used in Down syndrome; however, they are poorly supported by evidence. These include: dietary changes, massage, animal therapy, chiropractics and naturopathy, among others. Some proposed treatments may also be harmful.
Prognosis.
Between 5 and 15% of children with Down syndrome in Europe attend regular school. Some graduate from high school; however, most do not. Of those with intellectual disability in the United States who attended high school about 40% graduated. Many learn to read and write and some are able to do paid work. In adulthood about 20% in the United States do paid work in some capacity. In Europe, however, less than 1% have regular jobs. Many are able to live semi-independently, but they often require help with financial, medical, and legal matters. Those with mosaic Down syndrome usually have better outcomes.
Individuals with Down syndrome have a higher risk of early death than the general population. This is most often from heart problems or infections. Following improved medical care, particularly for heart and gastrointestinal problems, the life expectancy has increased. This increase has been from 12 years in 1912, to 25 years in the 1980s, to 50 to 60 years in the developed world in the 2000s. Currently between 4 and 12% die in the first year of life. The probability of long-term survival is partly determined by the presence of heart problems. In those with congenital heart problems 60% survive to 10 years and 50% survive to 30 years of age. In those without heart problems 85% survive to 10 years and 80% survive to 30 years of age. About 10% live to 70 years of age.
Epidemiology.
Globally, as of 2010, Down syndrome occurs in about 1 per 1000 births and results in about 17,000 deaths. More children are born with Down syndrome in countries where abortion is not allowed and in countries where pregnancy more commonly occurs at a later age. About 1.4 per 1000 live births in the United States and 1.1 per 1000 live births in Norway are affected. In the 1950s, in the United States, it occurred in 2 per 1000 live births with the decrease since then due to prenatal screening and abortions. The number of pregnancies with Down syndrome is more than two times greater with many spontaneously aborting. It is the cause of 8% of all congenital disorders.
Maternal age affects the chances of having a pregnancy with Down syndrome. At age 20, the chance is one in 1441; at age 30, it is one in 959; at age 40, it is one in 84; and at age 50 it is one in 44. Although the probability increases with maternal age, 70% of children with Down syndrome are born to women 35 years of age and younger, because younger people have more children. The father's older age is also a risk factor in women older than 35, but not in women younger than 35, and may partly explain the increase in risk as women age.
History.
English physician John Langdon Down first characterized Down syndrome as a separate form of mental disability in 1862, and in a more widely published report in 1866. Édouard Séguin described it as separate from cretinism in 1944. By the 20th century, Down syndrome had become the most recognizable form of mental disability.
In ancient times, many infants with disabilities were either killed or abandoned. A number of historical pieces of art are believed to portray Down syndrome, including pottery from AD 500 from South America and the 16th-century painting "The Adoration of the Christ Child".
In the 20th century, many individuals with Down syndrome were institutionalized, few of the associated medical problems were treated, and most died in infancy or early adult life. With the rise of the eugenics movement, 33 of the then 48 U.S. states and several countries began programs of forced sterilization of individuals with Down syndrome and comparable degrees of disability. Action T4 in Nazi Germany made public policy of a program of systematic involuntary euthanization.
With the discovery of karyotype techniques in the 1950s, it became possible to identify abnormalities of chromosomal number or shape. In 1959, Jérôme Lejeune reported the discovery that Down syndrome resulted from an extra chromosome. However, Lejeune's claim to the discovery has been disputed, and in 2014, the Scientific Council of the French Federation of Human Genetics unanimously awarded its Grand Prize to his colleague Marthe Gautier for this discovery. As a result of this discovery, the condition became known as trisomy 21. Even before the discovery of its cause, the presence of the syndrome in all races, its association with older maternal age, and its rarity of recurrence had been noticed. Medical texts had assumed it was caused by a combination of inheritable factors that had not been identified. Other theories had focused on injuries sustained during birth.
Society and culture.
Name.
Due to his perception that children with Down syndrome shared facial similarities with those of Blumenbach's Mongolian race, John Langdon Down used the term 'mongoloid'. While the term mongoloid (also mongolism, Mongolian imbecility or idiocy) continued to be used until the early 1970s, it is now considered unacceptable and is no longer in common use. In 1961, 19 scientists suggested that "mongolism" had "misleading connotations" and had become "an embarrassing term". The World Health Organization (WHO) dropped the term in 1965 after a request by the Mongolian delegate.
In 1975, the United States National Institutes of Health (NIH) convened a conference to standardize the naming and recommended eliminating the possessive form, Down's syndrome, although both the possessive and nonpossessive forms are used by the general population. The term "trisomy 21" is also used frequently.
Ethics.
Some argue that not to offer screening for Down syndrome is unethical. As it is a medically reasonable procedure, per informed consent, people should at least be given information about it. It will then be the woman's choice, based on her personal beliefs, how much or how little screening she wishes. When results from testing become available, it is also considered unethical not to give the results to the person in question.
Some deem it reasonable for parents to select a child who would have the highest well-being. One criticism of this reasoning is it often values those with disabilities less. Others argue that Down syndrome shouldn't be prevented or cured and that eliminating Down syndrome amounts to genocide. The disability rights movement does not have a position on screening, although some members consider testing and abortion discriminatory. Some in the United States who are pro-life support abortion if the fetus is disabled, while others do not. Of a group of 40 mothers in the United States who have had one child with Down syndrome, half agreed to screening in the next pregnancy.
Within Christianity, some Protestants denominations see abortion as acceptable when a fetus has Down syndrome, while Orthodox Christians and Roman Catholics often do not. Some of those against screening refer to it as a form of "eugenics". Disagreement exists within Islam regarding the acceptability of abortion in those carrying a fetus with Down syndrome. Some Islamic countries allow abortion, while others do not. Women may face stigmatization whichever decision they make.
Advocacy groups.
Advocacy groups for Down syndrome formed after the Second World War. These were organizations advocating for the inclusion of people with Down syndrome into the general school system and for a greater understanding of the condition among the general population, as well as groups providing support for families with children with Down syndrome. Organizations included the Royal Society for Handicapped Children and Adults founded in the UK in 1946 by Judy Fryd, Kobato Kai founded in Japan in 1964, the National Down Syndrome Congress founded in the United States in 1973 by Kathryn McGee and others, and the National Down Syndrome Society founded in 1979 in the United States.
The first World Down Syndrome Day was held on 21 March 2006. The day and month were chosen to correspond with 21 and trisomy, respectively. It was recognized by the United Nations General Assembly in 2011.
Research.
Efforts are underway to determine how the extra chromosome 21 material causes Down syndrome, as currently this is unknown, and to develop treatments to improve intelligence in those with the syndrome. One hope is to use stem cells. Other methods being studied include the use of antioxidants, gamma secretase inhibition, adrenergic agonists, and memantine. Research is often carried out on an animal model, the Ts65Dn mouse.

</doc>
<doc id="8305" url="https://en.wikipedia.org/wiki?curid=8305" title="Dyslexia">
Dyslexia

Dyslexia, also known as reading disorder, is characterized by trouble with reading unrelated to problems with overall intelligence. Different people are affected to varying degrees. Problems may include difficulties in spelling words, reading quickly, writing words, "sounding out" words in the head, pronouncing words when reading aloud and understanding what one reads. Often these difficulties are first noticed at school. When someone who previously could read loses their ability, it is known as alexia. The difficulties are involuntary and people with this disorder have an unaffected desire to learn.
The cause of dyslexia is believed to involve both genetic and environmental factors. Some cases run in families. It often occurs in people with attention deficit hyperactivity disorder (ADHD) and is associated with similar difficulties with numbers. It may begin in adulthood as the result of a traumatic brain injury, stroke, or dementia. The underlying mechanisms are problems within the brain's language processing. Dyslexia is diagnosed through a series of tests of memory, spelling, vision, and reading skills. Dyslexia is separate from reading difficulties caused by insufficient teaching; or either hearing or vision problems.
Treatment involves adjusting teaching methods to meet the person's needs. While not curing the underlying problem, it may decrease the degree of symptoms. Treatments targeting vision are not effective. Dyslexia is the most common learning disability, affecting 3–7 % of the population; however, up to 20% may have some degree of symptoms. While dyslexia is more often diagnosed in men, it has been suggested that it affects men and women equally. Dyslexia occurs in all areas of the world. Some believe that dyslexia should be best considered as a different way of learning, with both benefits and downsides.
Classification.
Dyslexia is thought to have two types of cause, one related to language processing and another to visual processing. It is considered a cognitive disorder, not a problem with intelligence. However, emotional problems often arise because of it. Some published definitions are purely descriptive, whereas others propose causes. The latter usually cover a variety of reading skills and deficits, and difficulties with distinct causes rather than a single condition. The National Institute of Neurological Disorders and Stroke definition describes dyslexia as "difficulty with spelling, phonological processing (the manipulation of sounds), or rapid visual-verbal responding". The British Dyslexia Association definition describes dyslexia as "a learning difficulty that primarily affects the skills involved in accurate and fluent word reading and spelling" and is characterized by "difficulties in phonological awareness, verbal memory and verbal processing speed".
Acquired dyslexia or alexia may be caused by brain damage due to stroke or atrophy. Forms of alexia include pure alexia, surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia.
Definition.
There is some variability in the definition of dyslexia. Some sources, such as the U.S. National Institutes of Health, define it specifically as a learning disorder. Other sources, however, define it simply as inability to read in the context of normal intelligence, and distinguish between "developmental dyslexia" (a learning disorder) and "acquired dyslexia" (loss of the ability to read caused by brain damage). ICD 10, the manual of medical diagnosis used in much of the world, includes separate diagnoses for "developmental dyslexia" (81.0) and for "dyslexia and alexia" (48.0). DSM 5, the manual of psychiatric diagnosis used in the United States, does not specifically define dyslexia, justifying this decision by stating that "the many definitions of dyslexia and dyscalculia meant those terms would not be useful as disorder names or in the diagnostic 
criteria". Instead it includes dyslexia in a category called specific learning disorders.
Signs and symptoms.
In early childhood, symptoms that correlate with a later diagnosis of dyslexia include delayed onset of speech, difficulty distinguishing left from right, difficulty with direction, as well as being easily distracted by background noise. The reversal of letters or words and mirror writing are behaviors sometimes seen in people with dyslexia, but are not considered to be defining characteristics of the disorder.
Dyslexia and attention deficit hyperactivity disorder (ADHD) commonly occur together; about 15% of people with dyslexia also have ADHD and 35% of those with ADHD have dyslexia.
School-age dyslexic children may exhibit signs of difficulty in identifying or generating rhyming words, or counting the number of syllables in words – both of which depend on phonological awareness. They may also show difficulty in segmenting words into individual sounds or may blend sounds when producing words, indicating reduced phonemic awareness. Difficulties with word retrieval or naming things is also associated with dyslexia. Dyslexics are commonly poor spellers, a feature sometimes called dysorthographia or dysgraphia, which depends on orthographic coding.
Problems persist into adolescence and adulthood and may accompany difficulties with summarizing stories, memorization, reading aloud, or learning foreign languages. Adult dyslexics can often read with good comprehension, though they tend to read more slowly than non-dyslexics and perform worse in spelling tests or when reading nonsense words – a measure of phonological awareness.
A common myth about dyslexia is that its defining feature is reading or writing letters or words backwards, but this is true of many children as they learn to read and write.
Language.
The orthographic complexity of a language directly impacts how difficult learning to read the language is. English and French have comparatively "deep" phonemic orthographies within the Latin alphabet writing system, with complex structures employing spelling patterns on several levels: letter-sound correspondence, syllables, and morphemes. Languages such as Spanish, Italian and Finnish have mostly alphabetic orthographies, which primarily employ letter-sound correspondence – so-called shallow orthographies – which for dyslexics makes them easier to learn. Logographic writing systems, such as Chinese characters, have extensive symbol use, and pose problems for dyslexic learners.
Associated conditions.
Dyslexia is often accompanied by several learning disabilities, but it is unclear whether they share underlying neurological causes. These associated disabilities include:
Causes.
Researchers have been trying to find the neurobiological basis of dyslexia since the condition was first identified in 1881. For example, some have tried to associate the common problem among dyslexics of not being able to see letters clearly to abnormal development of their visual nerve cells.
Neuroanatomy.
Modern neuroimaging techniques such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have shown a correlation between both functional and structural differences in the brains of children with reading difficulties. Some dyslexics show less electrical activation in parts of the left hemisphere of the brain involved with reading, such as the inferior frontal gyrus, inferior parietal lobule, and the middle and ventral temporal cortex. Over the past decade, brain activation studies using PET to study language have produced a breakthrough in the understanding of the neural basis of language. Neural bases for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e. functional rather than structural). fMRIs in dyslexics have provided important data which point to the interactive role of the cerebellum and cerebral cortex as well as other brain structures.
The cerebellar theory of dyslexia proposes that impairment of cerebellum-controlled muscle movement affects the formation of words by the tongue and facial muscles, resulting in the fluency problems that are characteristic of some dyslexics. The cerebellum is also involved in the automatization of some tasks, such as reading. The fact that some dyslexic children have motor task and balance impairments has been used as evidence for a cerebellar role in their reading difficulties. However, the cerebellar theory is not supported by controlled research studies.
Genetics.
Research into potential genetic causes of dyslexia has its roots in post-autopsy examination of the brains of people with dyslexia. Observed anatomical differences in the language centers of such brains include microscopic cortical malformations known as ectopias, more rarely, vascular micro-malformations, and microgyrus. The previously cited studies and others suggest that abnormal cortical development presumed to occur before or during the sixth month of fetal brain development was the cause of the abnormalities. Abnormal cell formations in dyslexics have also been reported in non-language cerebral and subcortical brain structures. Several genes have been associated with dyslexia, including DCDC2 and KIAA0319 on chromosome 6, and DYX1C1 on chromosome 15.
Gene–environment interaction.
The contribution of gene–environment interaction to reading disability has been intensely studied using twin studies, which estimate the proportion of variance associated with a person's environment and the proportion associated with their genes. Studies examining the influence of environmental factors such as parental education and teacher quality have determined that genetics have greater influence in supportive, rather than less optimal, environments. However, more optimal conditions may just allow those genetic risk factors to account for more of the variance in outcome because the environmental risk factors have been minimized. As environment plays a large role in learning and memory, it is likely that epigenetic modifications play an important role in reading ability. Animal experiments and measures of gene expression and methylation in the human periphery are used to study epigenetic processes; however, both types of study have many limitations in the extrapolation of results for application to the human brain.
Mechanisms.
The dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud. One mechanism is the lexical route, which is the process whereby skilled readers can recognize known words by sight alone, through a "dictionary" lookup procedure. The other mechanism is the nonlexical or sublexical route, which is the process whereby the reader can "sound out" a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and applying knowledge of how these parts are associated with each other, for example, how a string of neighboring letters sound together. The dual-route system could explain the different rates of dyslexia occurrence between different languages (e.g. the Spanish language dependence on phonological rules accounts for the fact that Spanish-speaking children show a higher level of performance in non-word reading, when compared to English-speakers).
Dyslexia disorder is not caused by mutation in one gene; in fact, it appears to involve the combined effects of several genes. Studying the cognitive problems associated with other disorders helps to better understand the genotype-phenotype link of dyslexia. Neurophysiological and imaging procedures are being used to ascertain phenotypic characteristics in dyslexics, thus identifying the effects of certain genes.
Diagnosis.
There are tests that can indicate with high probability whether a person is a dyslexic. If diagnostic testing indicates that a person may be dyslexic, such tests are often followed up with a full diagnostic assessment to determine the extent and nature of the disorder. Tests can be administered by a teacher or computer. Some test results indicate how to carry out teaching strategies.
Central dyslexias.
Central dyslexias include surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia. ICD-10 reclassified the previous distinction between dyslexia (315.02 in ICD-9) and alexia (315.01 in ICD-9) into a single classification as R48.0. The terms are applied to developmental dyslexia and inherited dyslexia along with developmental aphasia and inherited alexia, which are considered synonymous.
Surface dyslexia.
In surface dyslexia, words with regular pronunciations (highly consistent with their spelling, e.g. "mint") are read more accurately than words with irregular pronunciation, such as "colonel". Difficulty distinguishing homophones is a diagnostic used for some forms of surface dyslexia. This disorder is usually accompanied by surface agraphia and fluent aphasia. Acquired surface dyslexia arises when a previously literate person experiences brain damage, which results in pronunciation errors that indicate impairment of the lexical route.
Phonological dyslexia.
In phonological dyslexia, sufferers can read familiar words but have difficulty with unfamiliar words, such as invented pseudo-words. Phonological dyslexia is associated with lesions in the parts of the brain supplied with blood by the middle cerebral artery. The superior temporal lobe is often also involved. Furthermore, dyslexics compensate by overusing a front-brain region called Broca's area, which is associated with aspects of language and speech. The Lindamood Phoneme Sequencing Program (LiPS) is used to treat phonological dyslexia. This system is based on a three-way sensory feedback process, using auditory, visual, and oral skills to learn to recognize words and word patterns. Case studies with a total of three patients found a significant improvement in spelling and reading ability after using LiPS.
Deep dyslexia.
Individuals with deep dyslexia experience both semantic paralexia (para-dyslexia) and phonological dyslexia, which causes the person to read a word and then say a related meaning instead of the denoted meaning. Deep alexia is associated with clear phonological processing impairments. Deep dyslexia is caused by widespread damage to the brain that often includes the left hemisphere. The "continuum" hypothesis claims that deep dyslexia develops from phonological dyslexia.
Peripheral dyslexias.
Peripheral dyslexias have been described as affecting the visual analysis of letters as a result of brain injury. Hemianopsia, a visual field loss on the left/right side of the vertical midline, is associated with this condition.
Pure dyslexia.
Pure, or phonologically-based, dyslexia, also known as agnosic dyslexia, dyslexia without agraphia, and pure word blindness, is dyslexia due to difficulty in recognizing written sequences of letters (such as words), or sometimes even letters. It is considered '"pure" because it is not accompanied by other significant language-related impairments. Pure dyslexia does not affect speech, handwriting style, language or comprehension impairments. Pure dyslexia is caused by lesions on the visual word form area (VWFA). The VWFA is composed of the left lateral occipital sulcus and is activated during reading. A lesion in the VWFA stops transmission between the visual cortex and the left angular gyrus. It can also be caused by a lesion involving the left occipital lobe or the splenium. It is usually accompanied by a homonymous hemianopsia in the right side of the visual field. Multiple oral re-reading (MOR) is a treatment for pure dyslexia. It is considered a top-down processing technique in which affected individuals read and reread texts a predetermined number of times or until reading speed or accuracy improves a predetermined amount.
Hemianopic dyslexia.
Hemianopic dyslexia is commonly considered to derive from visual field loss due to damage to the primary visual cortex. Sufferers may complain of abnormally slow reading but are able to read individual words normally. This is the most common form of peripheral alexia, and the form with the best evidence of effective treatments.
Neglect dyslexia.
In neglect dyslexia, some letters, most commonly those at the beginning or left side of a word, are skipped or misread during reading. This alexia is associated with right parietal lesions. The use of prism glasses has been shown to substantially mitigate this condition.
Attentional dyslexia.
People with attentional dyslexia complain of letter-crowding or migration, sometimes blending elements of two words into one. Sufferers read better when words are presented in isolation rather than flanked by other words and letters. Using a large magnifying glass may help mitigate this condition by reducing the effects of flanking from nearby words; however, no trials of this or indeed any other therapy for left parietal syndromes have been published as of 2014.
Management.
Through the use of compensation strategies, therapy and educational support, dyslexic individuals can learn to read and write. There are techniques and technical aids which help to manage or conceal symptoms of the disorder. Removing stress and anxiety alone can sometimes improve written comprehension. For dyslexia intervention with alphabet-writing systems, the fundamental aim is to increase a child's awareness of correspondences between graphemes (letters) and phonemes (sounds), and to relate these to reading and spelling by teaching how sounds blend into words. It has been found that reinforced collateral training focused on reading and spelling yields longer-lasting gains than oral phonological training alone. Early intervention – that done while the language areas of the brain are still developing – is the most successful in reducing the long-term impacts of dyslexia. There is some evidence that the use of specially-tailored fonts may mitigate the effects of dyslexia. These fonts, which include Dyslexie, OpenDyslexic, and Lexia Readable, were created based on the idea that many of the letters of the Latin alphabet are visually similar and may therefore confuse dyslexics. Dyslexie and OpenDyslexic both put emphasis on making each letter more distinctive in order to be more easily identified. Font design can have an effect on reading, reading time, and the perception of legibility of all readers, not only those with dyslexia.
There have been many studies conducted regarding intervention in dyslexia. Among these studies one meta-analysis found that there was functional activation as a result.
Prognosis.
Dyslexic children require special instruction for word analysis and spelling from an early age. However, there are fonts that can help dyslexics better understand writing. The prognosis, generally speaking, is positive for individuals who are identified in childhood and receive support from friends and family.
Epidemiology.
The percentage of people with dyslexia is unknown, but it has been estimated to be as low as 5% and as high as 17% of the population. While it is diagnosed more often in males, some believe that it affects males and females equally.
There are different definitions of dyslexia used throughout the world, but despite significant differences in writing systems, dyslexia occurs in different populations. Dyslexia is not limited to difficulty in converting letters to sounds, and Chinese dyslexics may have difficulty converting Chinese characters into their meanings. The Chinese vocabulary uses logographic, monographic, non-alphabet writing where one character can represent an individual phoneme.
The phonological-processing hypothesis attempts to explain why dyslexia occurs in a wide variety of languages. Furthermore, the relationship between phonological capacity and reading appears to be influenced by orthography.
History.
Dyslexia was identified by Oswald Berkhan in 1881, but the term "dyslexia" was coined in 1887 by Rudolf Berlin, an ophthalmologist in Stuttgart. He used the term to refer to the case of a young boy who had a severe impairment in learning to read and write, despite showing typical intelligence and physical abilities in all other respects. In 1896, W. Pringle Morgan, a British physician from Seaford, East Sussex, published a description of a reading-specific learning disorder in a report to the "British Medical Journal" titled "Congenital Word Blindness". The distinction between phonological and surface types of dyslexia is only descriptive, and without any etiological assumption as to the underlying brain mechanisms. However, studies have alluded to potential differences due to variation in performance.
Research and society.
The majority of currently available dyslexia research relates to alphabetic writing systems, and especially to European languages. However, substantial research is also available regarding dyslexics who speak Arabic, Chinese, Hebrew or other languages.
As is the case with any disorder, society often makes an assessment based on incomplete information. Before the 1980s, dyslexia was thought to be a consequence of education, rather than a basic disability. As a result, society often misjudges those with the disorder. There is also sometimes a workplace stigma and negative attitude towards those with dyslexia. If a dyslexic's instructors lack the necessary training to support a child with the condition, there is often a negative effect on the student's learning participation. There is no evidence demonstrating that the use of music education is effective in improving dyslexic adolescents' reading skills.

</doc>
<doc id="8308" url="https://en.wikipedia.org/wiki?curid=8308" title="Delft">
Delft

Delft () is a city and a municipality in the Netherlands. It is located in the province of South Holland, where it is situated north of Rotterdam and south of The Hague.
Delft is known for its historic town centre with canals, Delft Blue pottery, the Delft University of Technology, painter Johannes Vermeer and scientist Antony van Leeuwenhoek, and its association with the royal House of Orange-Nassau.
History.
Early history.
The city of Delft came into being aside a canal, the 'Delf', which comes from the word "delven", meaning delving or digging, and led to the name Delft. It presumably started around the 11th century as a landlord court.
From a rural village in the early Middle Ages Delft developed to a city, that in the 13th century (1246) received its charter. (For some more information about the early development, see Gracht)."
The town's association with the House of Orange started when William of Orange (Willem van Oranje), nicknamed William the Silent (Willem de Zwijger), took up residence in 1572. At the time he was the leader of growing national Dutch resistance against Spanish occupation of the country, which struggle is known as the Eighty Years' War. By then Delft was one of the leading cities of Holland and it was equipped with the necessary city walls to serve as a headquarters.
After the Act of Abjuration was proclaimed in 1581 Delft became the "de facto" capital of the newly independent Netherlands, as the seat of the Prince of Orange.
When William was shot dead in 1584, by Balthazar Gerards in the hall of the Prinsenhof, the family's traditional burial place in Breda was still in the hands of the Spanish. Therefore, he was buried in the Delft Nieuwe Kerk (New Church), starting a tradition for the House of Orange that has continued to the present day.
Delft Explosion.
The Delft Explosion, also known in history as the Delft Thunderclap, occurred on 12 October 1654 when a gunpowder store exploded, destroying much of the city. Over a hundred people were killed and thousands wounded.
About of gunpowder were stored in barrels in a magazine in a former Clarissen convent in the Doelenkwartier district. Cornelis Soetens, the keeper of the magazine, opened the store to check a sample of the powder and a huge explosion followed. Luckily, many citizens were away, visiting a market in Schiedam or a fair in The Hague. Artist Carel Fabritius was wounded in the explosion and died of his injuries. Later on, Egbert van der Poel painted several pictures of Delft showing the devastation.
Sights.
The city centre retains a large number of monumental buildings, whereas in many streets there are canals of which the borders are connected by typical bridges, altogether making this city a notable tourist destination.
Historical buildings and other sights of interest include:
Culture.
Delft is well known for the Delft pottery ceramic products which were styled on the imported Chinese porcelain of the 17th century. The city had an early start in this area since it was a home port of the Dutch East India Company. It can still be seen at the pottery factories De Koninklijke Porceleyne Fles (or Royal Delft) and De Delftse Pauw.
The painter Johannes Vermeer (1632–1675) was born in Delft. Vermeer used Delft streets and home interiors as the subject or background of his paintings.
Several other famous painters lived and worked in Delft at that time, such as Pieter de Hoogh, Carel Fabritius, Nicolaes Maes, Gerard Houckgeest and Hendrick Cornelisz. van Vliet. They all were members of the Delft School. The Delft School is known for its images of domestic life, views of households, church interiors, courtyards, squares and the streets of Delft. The painters also produced pictures showing historic events, flower paintings, portraits for patrons and the court, and decorative pieces of art.
Education.
Delft University of Technology (TU Delft) is one of three universities of technology in the Netherlands. It was founded as an academy for civil engineering in 1842 by King William II. Today well over 20,000 students are enrolled.
The UNESCO-IHE Institute for Water Education, providing postgraduate education for people from developing countries, draws on the strong tradition in water management and hydraulic engineering of the Delft university.
Economy.
In the local economic field essential elements are:
Nature and recreation.
East of Delft a relatively vast nature and recreation area called the "Delftse Hout" ("Delft Wood") is situated. Apart from a forest, through which bike-, horseride- and footpaths are leading, it also comprises a vast lake (suitable for swimming and windsurfing), narrow beaches, a restaurant, community gardens, plus campground and other recreational and sports facilities. (There is a possibility to rent bikes at the station).
Inside the city apart from a central park there are also several smaller town parks, like "Nieuwe Plantage", "Agnetapark", "Kalverbos" and others.
Furthermore, there's a Botanical Garden of the TU and an arboretum in Delftse Hout.
Famous persons.
Delft was the birthplace of:
Before 1900
After 1900
Otherwise related
International relations.
Twin towns — Sister cities.
Delft is twinned with:
Transport.
Trains stopping at these stations connect Delft with, among others, nearby cities of Rotterdam and The Hague, up to every five minutes, for most of the day.
There are several bus routes from Delft to similar destinations. Trams frequently travel between Delft and The Hague via special double tracks crossing the city. One of those two lines (19) is still under construction inside Delft and is meant to connect The Hague with a science park, which being developed on the southern (Rotterdam) side of Delft and is a joint project by the Delft and Rotterdam municipalities.

</doc>
<doc id="8309" url="https://en.wikipedia.org/wiki?curid=8309" title="Duesberg hypothesis">
Duesberg hypothesis

The Duesberg hypothesis is the claim, associated with University of California, Berkeley professor Peter Duesberg, that various noninfectious factors such as recreational and pharmaceutical drug use are the cause of AIDS, and that HIV (human immunodeficiency virus) is merely a harmless passenger virus. The most prominent supporters of this hypothesis are Duesberg himself, biochemist and vitamin proponent David Rasnick, and journalist Celia Farber. The scientific community contends that Duesberg's arguments are the result of cherry-picking predominantly outdated scientific data and selectively ignoring evidence in favor of HIV's role in AIDS. The scientific consensus is that the Duesberg hypothesis is incorrect and that HIV is the cause of AIDS.
Role of legal and illegal drug use.
Duesberg argues that there is a statistical correlation between trends in recreational drug use and trends in AIDS cases. He argues that the epidemic of AIDS cases in the 1980s corresponds to a supposed epidemic of recreational drug use in the United States and Europe during the same time frame.
These claims are not supported by epidemiologic data. The average yearly increase in opioid-related deaths from 1990 to 2002 was nearly three times the yearly increase from 1979–90, with the greatest increase in 2000–02, yet AIDS cases and deaths fell dramatically during the mid-to-late-1990s. Duesberg's claim that recreational drug use, rather than HIV, was the cause of AIDS has been specifically examined and found to be false. Cohort studies have found that only HIV-positive drug users develop opportunistic infections; HIV-negative drug users do not develop such infections, indicating that HIV rather than drug use is the cause of AIDS.
Duesberg has also argued that nitrite inhalants were the cause of the epidemic of Kaposi sarcoma (KS) in gay men. However, this argument has been described as an example of the fallacy of a statistical confounding effect; it is now known that a herpesvirus, potentiated by HIV, is responsible for AIDS-associated KS.
Moreover, in addition to recreational drugs, Duesberg argues that anti-HIV drugs such as zidovudine (AZT) can cause AIDS. Duesberg's claim that antiviral medication causes AIDS is regarded as disproven by the scientific community. Placebo-controlled studies have found that AZT as a single agent produces modest and short-lived improvements in survival and delays the development of opportunistic infections; it certainly did not cause AIDS, which develops in both treated and untreated study patients. With the subsequent development of protease inhibitors and highly active antiretroviral therapy, numerous studies have documented the fact that anti-HIV drugs prevent the development of AIDS and substantially prolong survival, further disproving the claim that these drugs "cause" AIDS.
Scientific study and rejection of Duesberg's risk-AIDS hypothesis.
Several studies have specifically addressed Duesberg's claim that recreational drug abuse or sexual promiscuity were responsible for the manifestations of AIDS. An early study of his claims, published in "Nature" in 1993, found Duesberg's drug abuse-AIDS hypothesis to have "no basis in fact."
A large prospective study followed a group of 715 homosexual men in the Vancouver, Canada, area; approximately half were HIV-seropositive or became so during the follow-up period, and the remainder were HIV-seronegative. After more than 8 years of follow-up, despite similar rates of drug use, sexual contact, and other supposed risk factors in both groups, only the HIV-positive group suffered from opportunistic infections. Similarly, CD4 counts dropped in the patients who were HIV-infected, but remained stable in the HIV-negative patients, despite similar rates of risk behavior. The authors concluded that "the risk-AIDS hypothesis ... is clearly rejected by our data," and that "the evidence supports the hypothesis that HIV-1 has an integral role in the CD4 depletion and progressive immune dysfunction that characterise AIDS."
Similarly, the Multicenter AIDS Cohort Study (MACS) and the Women's Interagency HIV Study (WIHS)—which between them observed more than 8,000 Americans—demonstrated that "the presence of HIV infection is the only factor that is strongly and consistently associated with the conditions that define AIDS." A 2008 study found that recreational drug use (including cannabis, cocaine, poppers, and amphetamines) had no effect on CD4 or CD8 T-cell counts, providing further evidence against a role of recreational drugs as a cause of AIDS.
Current AIDS definitions.
Duesberg argued in 1989 that a significant number of AIDS victims had died without proof of HIV infection. However, with the use of modern culture techniques and polymerase chain reaction testing, HIV can be demonstrated in virtually all patients with AIDS. Since AIDS is now defined partially by the presence of HIV, Duesberg claims it is impossible by definition to offer evidence that AIDS doesn't require HIV. However, the first definitions of AIDS mentioned no cause and the first AIDS diagnoses were made before HIV was discovered. The addition of HIV positivity to surveillance criteria as an absolutely necessary condition for case reporting occurred only in 1993, after a scientific consensus was established that HIV caused AIDS.
AIDS in Africa.
According to the Duesberg hypothesis, AIDS is not found in Africa. What Duesberg calls "the myth of an African AIDS epidemic," among people" exists for several reasons, including:
Duesberg states that African AIDS cases are "a collection of long-established, indigenous diseases, such as chronic fevers, weight loss, alias "slim disease," diarrhea, and tuberculosis" that result from malnutrition and poor sanitation. African AIDS cases, though, have increased in the last three decades as HIV's prevalence has increased but as malnutrition percentages and poor sanitation have declined in many African regions. In addition, while HIV and AIDS are more prevalent in urban than in rural settings in Africa, malnutrition and poor sanitation are found more commonly in rural than in urban settings.
According to Duesberg, common diseases are easily misdiagnosed as AIDS in Africa because "the diagnosis of African AIDS is arbitrary" and does not include HIV testing. A definition of AIDS agreed upon in 1985 by the World Health Organization in Bangui did not require a positive HIV test, but since 1985, many African countries have added positive HIV tests to the Bangui criteria for AIDS or changed their definitions to match those of the U.S. Centers for Disease Control. One of the reasons for using more HIV tests despite their expense is that, rather than overestimating AIDS as Duesberg suggests, the Bangui definition alone excluded nearly half of African AIDS patients."
Duesberg notes that diseases associated with AIDS differ between African and Western populations, concluding that the causes of immunodeficiency must be different. Tuberculosis is much more commonly diagnosed among AIDS patients in Africa than in Western countries, while PCP conforms to the opposite pattern. Tuberculosis, though, had higher prevalence in Africa than in the West before the spread of HIV. In Africa and the United States, HIV has spurred a similar percentage increase in tuberculosis cases. PCP may be underestimated in Africa: since machinery "required for accurate testing is relatively rare in many resource-poor areas, including large parts of Africa, PCP is likely to be underdiagnosed in Africa. Consistent with this hypothesis, studies that report the highest rates of PCP in Africa are those that use the most advanced diagnostic methods" Duesberg also claims that Kaposi's Sarcoma is "exclusively diagnosed in male homosexual risk groups using nitrite inhalants and other psychoactive drugs as aphrodisiacs", but the cancer is fairly common among heterosexuals in some parts of Africa, and is found in heterosexuals in the United States as well.
Because reported AIDS cases in Africa and other parts of the developing world include a larger proportion of people who do not belong to Duesberg's preferred risk groups of drug addicts and male homosexuals, Duesberg writes on his website that "There are no risk groups in Africa, like drug addicts and homosexuals." However, many studies have addressed the issue of risk groups in Africa and concluded that the risk of AIDS is not equally distributed. In addition, AIDS in Africa largely kills sexually active working-age adults.
Duesberg claims that retroviruses like HIV must be harmless to survive.
Duesberg argues that retroviruses like HIV must be harmless to survive: they do not kill cells and they do not cause cancer, he maintains. Duesberg writes, "retroviruses do not kill cells because they depend on viable cells for the replication of their RNA from viral DNA integrated into cellular DNA." Duesberg elsewhere states that "the typical virus reproduces by entering a living cell and commandeering the cell's resources in order to make new virus particles, a process that ends with the disintegration of the dead cell."
Duesberg also rejects the involvement of retroviruses and other viruses in cancer. To him, virus-associated cancers are "freak accidents of nature" that do not warrant research programs such as the War on Cancer. Duesberg rejects a role in cancer for numerous viruses, including leukemia viruses, Epstein-Barr Virus, Human Papilloma Virus, Hepatitis B, Feline Leukemia Virus, and Human T-lymphotropic virus.
Duesberg claims that the supposedly innocuous nature of all retroviruses is supported by what he considers to be their normal mode of proliferation: infection from mother to child "in utero". Duesberg does not suggest that HIV is an endogenous retrovirus, a virus integrated into the germ line and genetically heritable:
Scientific response to the Duesberg hypothesis.
The consensus in the scientific community is that the Duesberg hypothesis has been refuted by a large and growing mass of evidence showing that HIV causes AIDS, that the amount of virus in the blood correlates with disease progression, that a plausible mechanism for HIV's action has been proposed, and that anti-HIV medication decreases mortality and opportunistic infection in people with AIDS.
In the 9 December 1994 issue of "Science" (Vol. 266, No. 5191), Duesberg's methods and claims were evaluated in a group of articles. The authors concluded that
Effectiveness of antiretroviral medication.
The vast majority of people with AIDS have never received antiretroviral drugs, including those in developed countries prior to the licensure of AZT (zidovudine) in 1987, and people in developing countries today where very few individuals have access to these medications.
The NIAID reports that "in the mid-1980s, clinical trials enrolling patients with AIDS found that AZT given as single-drug therapy conferred a modest survival advantage compared placebo. Among HIV-infected patients who had not yet developed AIDS, placebo-controlled trials found that AZT given as single-drug therapy delayed, for a year or two, the onset of AIDS-related illnesses. Significantly, long-term follow-up of these trials did not show a prolonged benefit of AZT, but also did not indicate that the drug increased disease progression or mortality. The lack of excess AIDS cases and death in the AZT arms of these placebo-controlled trials in effect counters the argument that AZT causes AIDS. Subsequent clinical trials found that patients receiving two-drug combinations had up to 50 percent improvements in time to progression to AIDS and in survival when compared with people receiving single-drug therapy. In more recent years, three-drug combination therapies have produced another 50 to 80 percent improvement in progression to AIDS and in survival when compared with two-drug regimens in clinical trials." "Use of potent anti-HIV combination therapies has contributed to dramatic reductions in the incidence of AIDS and AIDS-related deaths in populations where these drugs are widely available, an effect which clearly would not be seen if antiretroviral drugs caused AIDS."
Opponents claim that nearly all HIV-positive people will develop AIDS.
Duesberg claims as support for his idea that many drug-free HIV-positive people have not yet developed AIDS; HIV/AIDS scientists note that many drug-free HIV-positive people have developed AIDS, and that, in the absence of medical treatment or rare genetic factors postulated to delay disease progression, it is very likely that nearly all HIV-positive people will eventually develop AIDS. Scientists also note that HIV-negative drug users do not suffer from immune system collapse.

</doc>
<doc id="8310" url="https://en.wikipedia.org/wiki?curid=8310" title="DSL (disambiguation)">
DSL (disambiguation)

DSL or digital subscriber line is a family of technologies that provide digital data transmission over the wires of a local telephone network.
DSL may also refer to:

</doc>
<doc id="8311" url="https://en.wikipedia.org/wiki?curid=8311" title="Dinosaur">
Dinosaur

Dinosaurs are a diverse group of animals of the clade Dinosauria. They first appeared during the Triassic period, 231.4 million years ago, and were the dominant terrestrial vertebrates for 135 million years, from the start of the Jurassic (about 200 million years ago) until the end of the Cretaceous (66 million years ago), when the Cretaceous–Paleogene extinction event led to the extinction of most dinosaur groups at the end of the Mesozoic Era. Until the late 20th century, all groups were believed to be extinct; however, the fossil record indicates that birds are modern feathered dinosaurs, having evolved from theropod ancestors during the Jurassic Period. As such, birds were the only dinosaurs to survive the mass extinction event.
Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species and fossil remains. Some are herbivorous, others carnivorous. While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. Evidence suggests that egg laying and nest building are additional traits shared by all dinosaurs. While modern dinosaurs (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of and heights of and were the largest land animals of all time. Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: "Xixianykus", for example, was only about long.
Although the word "dinosaur" means "terrible lizard", the name is somewhat misleading, as dinosaurs are not lizards. Instead, they represent a separate group of reptiles that, like many extinct forms, did not exhibit characteristics traditionally seen as reptilian, such as a sprawling limb posture or ectothermy. Additionally, many prehistoric animals, including mosasaurs, ichthyosaurs, pterosaurs, plesiosaurs, and "Dimetrodon", are popularly conceived of as dinosaurs, but are not taxonomically classified as dinosaurs. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction.
Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as "Jurassic Park". Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.
Etymology.
The taxon Dinosauria was formally named in 1842 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived from the Greek words δεινός ("deinos", meaning "terrible", "potent", or "fearfully great") and σαῦρος ("sauros", meaning "lizard" or "reptile"). Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it merely to evoke their size and majesty.
Definition.
Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of "Triceratops", Neornithes birds, their most recent common ancestor (MRCA), and all descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of "Megalosaurus" and "Iguanodon", because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing theropods (mostly bipedal carnivores and birds), ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).
Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic taxonomy; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.
General description.
Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and "Dimetrodon", but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a cat, and were generally rodent-sized carnivores of small prey.
Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avian dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3400 dinosaur genera existed, including many that would not have been preserved in the fossil record. By September 17, 2008, 1047 different species of dinosaurs had been named. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as "Ammosaurus" and "Iguanodon", could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the early Jurassic period. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as "Microraptor") could fly or at least glide, and others, such as spinosaurids, had semi-aquatic habits.
Distinguishing anatomical features.
While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the last common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.
A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known:
Nesbitt found a number of further potential synapomorphies, and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others.
A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of temporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in "Herrerasaurus"); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in "Saturnalia", for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the late Triassic are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature.
Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.
Evolutionary history.
Origins and early evolution.
Dinosaurs diverged from their archosaur ancestors during the middle to late Triassic period, roughly 20 million years after the Permian–Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus "Eoraptor" at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus "Nyasasaurus" from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives.
When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 200 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early-mid Norian and early Rhaetian, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct.
Evolution and paleobiogeography.
Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the late Triassic and early Jurassic, the continents were connected as the single landmass Pangaea, and there was a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the middle and late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like "Mamenchisaurus". Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians.
By the early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like "Psittacosaurus" became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late early Cretaceous or early late Cretaceous. A major change in the early Cretaceous, which would be amplified in the late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with tooth batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid "Nigersaurus".
There were three general dinosaur faunas in the late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.
The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous period, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.
The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of "Gastornis", eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often cited that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still being fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, only becoming extinct very recently after over 20 million years of co-existence with many mammal groups.
Classification.
Dinosaurs are archosaurs, like modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.
Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with "Triceratops" than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek "sauros" (σαῦρος) meaning "lizard" and "ischion" (ἰσχίον) meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups ("Herrerasaurus", therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).
By contrast, ornithischians—"bird-hipped", from the Greek "ornitheios" (ὀρνίθειος) meaning "of a bird" and "ischion" (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubis bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species which were primarily herbivores. (NB: the terms "lizard hip" and "bird hip" are misnomers – birds evolved from dinosaurs with "lizard hips".)
Taxonomy.
The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2007). A more detailed version can be found at Dinosaur classification.
The dagger (†) is used to signify groups with no living members.
Biology.
Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.
Size.
Current evidence suggests that dinosaur average size varied through the Triassic, early Jurassic, late Jurassic and Cretaceous periods. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the category. The mode of Mesozoic dinosaur body masses is between one and ten metric tonnes. This contrasts sharply with the size of Cenozoic mammals, estimated by the National Museum of Natural History as about .
The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest were an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as "Paraceratherium" (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.
Largest and smallest.
Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals ever fossilize, and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.
The tallest and heaviest dinosaur known from good skeletons is "Giraffatitan brancai" (previously classified as a species of "Brachiosaurus"). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde Berlin; this mount is tall and long, and would have belonged to an animal that weighed between and  kilograms ( and  lb). The longest complete dinosaur is the long "Diplodocus", which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Natural History Museum in 1907.
There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were all discovered in the 1970s or later, and include the massive "Argentinosaurus", which may have weighed to  kilograms (90 to 110 short tons); some of the longest were the long "Diplodocus hallorum" (formerly "Seismosaurus") and the long "Supersaurus"; and the tallest, the tall "Sauroposeidon", which could have reached a sixth-floor window. The heaviest and longest dinosaur may have been "Amphicoelias fragillimus", known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been long and weighed kg ( lb). However, as no further evidence of sauropods of this size has been found, and the discoverer, Edward Cope, had made typographic errors before, it is likely to have been an extreme overestimation. The largest known carnivorous dinosaur was "Spinosaurus", reaching a length of , and weighing 7–20.9 tonnes (7.7–23 short tons). Other large carnivorous theropods included "Giganotosaurus", "Carcharodontosaurus" and "Tyrannosaurus". "Therizinosaurus" and "Deinocheirus" were among the tallest of the theropods.
The smallest dinosaur known is the bee hummingbird, with a length of only and mass of around . The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, "Anchiornis huxleyi" is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of 110 grams and a total skeletal length of . The smallest herbivorous non-avialan dinosaurs included "Microceratus" and "Wannanosaurus", at about long each.
Behavior.
Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.
The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 "Iguanodon bernissartensis", ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-bills (hadrosaurids) may have moved in great herds, like the American bison or the African springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 "Sinornithomimus", from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as "Deinonychus" and "Allosaurus" can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators.
The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.
From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a "Velociraptor" attacking a "Protoceratops", providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an "Edmontosaurus", a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod "Majungasaurus".
Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, "Juravenator", and "Megapnosaurus" were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian "Agilisaurus" was inferred to be diurnal.
Based on current fossil evidence from dinosaurs such as "Oryctodromeus", some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as "Microraptor") most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, in particular, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.
Communication.
Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups suggests that visual communication has always been important in dinosaur biology. However, the evolution of dinosaur vocalization is less certain. In 2008, paleontologist Phil Senter examined the evidence for vocalization in Mesozoic animal life, including dinosaurs. Senter found that, contrary to popular depictions of roaring dinosaurs in motion pictures, it is likely that most Mesozoic dinosaurs were not capable of creating any vocalizations (though the hollow crests of the lambeosaurines could have functioned as resonance chambers used for a wide range of vocalizations). To draw this conclusion, Senter studied the distribution of vocal organs in modern reptiles and birds. He found that vocal cords in the larynx probably evolved multiple times among reptiles, including crocodilians, which are able to produce guttural roars. Birds, on the other hand, lack a larynx. Instead, bird calls are produced by the syrinx, a vocal organ found only in birds, and which is not related to the larynx, meaning it evolved independently from the vocal organs in reptiles. The syrinx depends on the air sac system in birds to function; specifically, it requires the presence of a "clavicular air sac" near the wishbone or collar bone. This air sac leaves distinctive marks or opening on the bones, including a distinct opening in the upper arm bone ("humerus"). While extensive air sac systems are a unique characteristic of saurischian dinosaurs, the clavicular air sac necessary to vocalize does not appear in the fossil record until the enantiornithines (one exception, "Aerosteon", probably evolved its clavicular air sac independently of birds for reasons other than vocalization).
The most primitive dinosaurs with evidence of a vocalizing syrinx are the enantironithine birds. Any bird-line archosaurs more primitive than this probably did not make vocal calls. Rather, several lines of evidence suggest that early dinosaurs used primarily visual communication, in the form of distinctive-looking (and possibly brightly colored) horns, frills, crests, sails and feathers. This is similar to some modern reptile groups such as lizards, in which many forms are largely silent (though like dinosaurs they possess well-developed senses of hearing) but use complex coloration and display behaviors to communicate.
In addition, other, non-vocal, methods of producing sound for communication include hissing, jaw grinding or clapping, use of environment (such as splashing), and wing beating (possible in winged maniraptoran dinosaurs).
Reproductive biology.
All dinosaurs lay amniotic eggs with hard shells made mostly of calcium carbonate. Eggs are usually laid in a nest. Most species create somewhat elaborate nests, which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as "Troodon", exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.
When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a "Tyrannosaurus rex" skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur "Allosaurus" and the ornithopod "Tenontosaurus". Because the line of dinosaurs that includes "Allosaurus" and "Tyrannosaurus" diverged from the line that led to "Tenontosaurus" very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs.
Another widespread trait among modern birds is parental care for young after hatching. Jack Horner's 1978 discovery of a "Maiasaura" ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods, suggesting this behavior might also have been common to all dinosaurs. There is evidence that other non-theropod dinosaurs, like Patagonian titanosaurian sauropods, also nested in large groups. A specimen of the Mongolian oviraptorid "Citipati osmolskae" was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. Parental care being a trait common to all dinosaurs is supported by other finds. For example, a dinosaur embryo (pertaining to the prosauropod "Massospondylus") was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland. Nests and eggs have been found for most major groups of dinosaurs, and it appears likely that all dinosaurs cared for their young to some extent either before or shortly after hatching.
Physiology.
Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.
After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic and sluggish remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.
Modern evidence indicates that even non-avian dinosaurs and birds thrived in cooler temperate climates, and that at least some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved.
In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than mammals of similar size and build could sustain. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.
Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. They also excrete creatine, rather than creatinine like mammals. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous period.
Number of Species.
As of March 2016 the estimated number of dinosaur species that existed in the Mesozoic era is 1,543–2,468 species.
Origin of birds.
The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in "Oviraptor", but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives.
They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.
Feathers.
Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has also revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching.
"Archaeopteryx" was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Darwin's seminal "The Origin of Species", its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for "Compsognathus". Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.
The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.
Skeleton.
Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.
Soft anatomy.
Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described "Aerosteon riocoloradensis", the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT-scanning of "Aerosteon"'s fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.
Behavioral evidence.
Fossils of the troodonts "Mei" and "Sinornithoides" demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.
Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.
Extinction of major groups.
The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which led to their downfall when large-bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.
At the peak of the Mesozoic, there were no polar ice caps, and sea levels are estimated to have been from higher than they are today. The planet's temperature was also much more uniform, with only separating average polar temperatures from those at the equator. On average, atmospheric temperatures were also much higher; the poles, for example, were warmer than today.
The atmosphere's composition during the Mesozoic is a matter for debate. While some academics argue that oxygen levels were much higher than today, others argue that biological adaptations seen in birds and dinosaurs indicate that respiratory systems evolved beyond what would be necessary if oxygen levels were high. By the late Cretaceous, the environment was changing dramatically. Volcanic activity was decreasing, which led to a cooling trend as levels of atmospheric carbon dioxide dropped. Oxygen levels in the atmosphere also started to fluctuate and would ultimately fall considerably. Some scientists hypothesize that climate change, combined with lower oxygen levels, might have led directly to the demise of many species.
Impact event.
The asteroid collision theory, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous period to a bolide impact approximately 66 million years ago. Alvarez "et al." proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately Chicxulub Crater and triggering the mass extinction. Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this theory is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.
Deccan Traps.
Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.
The Deccan Traps could have caused extinction through several mechanisms, including the release into the air of dust and sulfuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of in the last half million years before the ]."
In the years when the Deccan Traps theory was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.
Possible Paleocene survivors.
Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2001, paleontologists Zielinski and Budahn reported the discovery of a single hadrosaur leg-bone fossil in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half million years into the Cenozoic Era. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to above the Cretaceous–Paleogene boundary, representing  years of elapsed time. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. Direct dating of the bones themselves has supported the later date, with U–Pb dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.
History of study.
Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese, whose modern word for dinosaur is "kǒnglóng" (恐龍, or "terrible dragon"), considered them to be dragon bones and documented them as such. For example, "Hua Yang Guo Zhi", a book written by Chang Qu during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines, a practice that continues today. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.
Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a "Megalosaurus", was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his "Natural History of Oxfordshire" in 1677. He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He therefore concluded it to be the thigh bone of a giant human similar to those mentioned in the Bible. In 1699, Edward Lhuyd, a friend of Sir Isaac Newton, was responsible for the first published scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, "Rutellum implicatum", that had been found in Caswell, near Witney, Oxfordshire.
Between 1815 and 1824, the Rev William Buckland, a professor of geology at Oxford, collected more fossilized bones of "Megalosaurus" and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, "Iguanodon", was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.
The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, "Iguanodon", "Megalosaurus" and "Hylaeosaurus", shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.
In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named "Hadrosaurus foulkii". It was an extremely important find: "Hadrosaurus" was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of dinosaur mania in the United States.
Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an "Elasmosaurus" skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Marsh 'won' the contest primarily because he was better funded through a relationship with the US Geological Survey. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones (a method modern paleontologists would find appalling). Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History in New York, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.
After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid "Antarctopelta oliveroi", was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod "Cryolophosaurus ellioti", was formally named and described in a scientific journal.
Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China in particular has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.
"Dinosaur renaissance".
The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of "Deinonychus", an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the amazingly well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.
Soft tissue and DNA.
One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in Pietraroia, Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, "Scipionyx samniticus". The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.
In the March 2005 issue of "Science", the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old "Tyrannosaurus rex" leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team. When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear.
In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, "Brachylophosaurus canadensis", found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the "Tyrannosaurus" bone. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.
The extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.
In 2015, researchers reported finding structures similar to blood cells and collagen fibers, preserved in the bone fossils of six Cretaceous dinosaur specimens, which are approximately 75 million years old.
Cultural depictions.
By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. Entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.
Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures were unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.
The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens "Bleak House", dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel "Journey to the Center of the Earth", Sir Arthur Conan Doyle's 1912 book "The Lost World", the iconic 1933 film "King Kong", the 1954 "Godzilla" and its many sequels, the best-selling 1990 novel "Jurassic Park" by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.

</doc>
<doc id="8315" url="https://en.wikipedia.org/wiki?curid=8315" title="Diamagnetism">
Diamagnetism

Diamagnetic materials create an induced magnetic field in a direction opposite to an externally applied magnetic field, and are repelled by the applied magnetic field. In contrast, the opposite behavior is exhibited by paramagnetic materials. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism the material is called a "diamagnet". Unlike a ferromagnet, a diamagnet is not a permanent magnet. Its magnetic permeability is less than μ0, the permeability of vacuum. In most materials diamagnetism is a weak effect, but a superconductor repels the magnetic field entirely, apart from a thin layer at the surface.
Diamagnets were first discovered when Sebald Justinus Brugmans observed in 1778 that bismuth and antimony were repelled by magnetic fields. In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. He adopted the term "diamagnetism" after it was suggested to him by William Whewell.
Materials.
Diamagnetism, to a greater or lesser degree, is a property of all materials and always makes a weak contribution to the material's response to a magnetic field. For materials that show some other form of magnetism (such as ferromagnetism or paramagnetism), the diamagnetic contribution becomes negligible. Substances that mostly display diamagnetic behaviour are termed diamagnetic materials, or diamagnets. Materials called diamagnetic are those that laymen generally think of as "non-magnetic", and include water, wood, most organic compounds such as petroleum and some plastics, and many metals including copper, particularly the heavy ones with many core electrons, such as mercury, gold and bismuth. The magnetic susceptibility values of various molecular fragments are called Pascal's constants.
Diamagnetic materials, like water, or water based materials, have a relative magnetic permeability that is less than or equal to 1, and therefore a magnetic susceptibility less than or equal to 0, since susceptibility is defined as . This means that diamagnetic materials are repelled by magnetic fields. However, since diamagnetism is such a weak property its effects are not observable in everyday life. For example, the magnetic susceptibility of diamagnets such as water is . The most strongly diamagnetic material is bismuth, , although pyrolytic carbon may have a susceptibility of in one plane. Nevertheless, these values are orders of magnitude smaller than the magnetism exhibited by paramagnets and ferromagnets. Note that because χv is derived from the ratio of the internal magnetic field to the applied field, it is a dimensionless value.
All conductors exhibit an effective diamagnetism when they experience a changing magnetic field. The Lorentz force on electrons causes them to circulate around forming eddy currents. The eddy currents then produce an induced magnetic field opposite the applied field, resisting the conductor's motion.
Superconductors.
Superconductors may be considered perfect diamagnets (), because they expel all fields (except in a thin surface layer) due to the Meissner effect. However this effect is not due to eddy currents, as in ordinary diamagnetic materials (see the article on superconductivity).
Demonstrations.
Curving water surfaces.
If a powerful magnet (such as a supermagnet) is covered with a layer of water (that is thin compared to the diameter of the magnet) then the field of the magnet significantly repels the water. This causes a slight dimple in the water's surface that may be seen by its reflection.
Levitation.
Diamagnets may be levitated in stable equilibrium in a magnetic field, with no power consumption. Earnshaw's theorem seems to preclude the possibility of static magnetic levitation. However, Earnshaw's theorem only applies to objects with positive susceptibilities, such as ferromagnets (which have a permanent positive moment) and paramagnets (which induce a positive moment). These are attracted to field maxima, which do not exist in free space. Diamagnets (which induce a negative moment) are attracted to field minima, and there can be a field minimum in free space.
A thin slice of pyrolytic graphite, which is an unusually strong diamagnetic material, can be stably floated in a magnetic field, such as that from rare earth permanent magnets. This can be done with all components at room temperature, making a visually effective demonstration of diamagnetism.
The Radboud University Nijmegen, the Netherlands, has conducted experiments where water and other substances were successfully levitated. Most spectacularly, a live frog (see figure) was levitated.
In September 2009, NASA's Jet Propulsion Laboratory in Pasadena, California announced they had successfully levitated mice using a superconducting magnet, an important step forward since mice are closer biologically to humans than frogs. They hope to perform experiments regarding the effects of microgravity on bone and muscle mass.
Recent experiments studying the growth of protein crystals has led to a technique using powerful magnets to allow growth in ways that counteract Earth's gravity.
A simple homemade device for demonstration can be constructed out of bismuth plates and a few permanent magnets that levitate a permanent magnet.
Theory.
The electrons in a material generally circulate in orbitals, with effectively zero resistance and act like current loops. Thus it might be imagined that diamagnetism effects in general would be very, very common, since any applied magnetic field would generate currents in these loops that would oppose the change, in a similar way to superconductors, which are essentially perfect diamagnets. However, since the electrons are rigidly held in orbitals by the charge of the protons and are further constrained by the Pauli exclusion principle, many materials exhibit diamagnetism, but typically respond very little to the applied field.
The Bohr–van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. However, the classical theory for Langevin diamagnetism gives the same prediction as the quantum theory. The classical theory is given below.
Langevin diamagnetism.
The Langevin theory of diamagnetism applies to materials containing atoms with closed shells (see dielectrics). A field with intensity , applied to an electron with charge and mass , gives rise to Larmor precession with frequency . The number of revolutions per unit time is , so the current for an atom with electrons is (in SI units)
The magnetic moment of a current loop is equal to the current times the area of the loop. Suppose the field is aligned with the axis. The average loop area can be given as formula_2, where formula_3 is the mean square distance of the electrons perpendicular to the axis. The magnetic moment is therefore
If the distribution of charge is spherically symmetric, we can suppose that the distribution of coordinates are independent and identically distributed. Then formula_5, where formula_6 is the mean square distance of the electrons from the nucleus. Therefore, formula_7. If formula_8 is the number of atoms per unit volume, the diamagnetic susceptibility in SI units is
In metals.
The Langevin theory does not apply to metals because they have non-localized electrons. The theory for the diamagnetism of a free electron gas is called Landau diamagnetism, and instead considers the weak counter-acting field that forms when their trajectories are curved due to the Lorentz force. Landau diamagnetism, however, should be contrasted with Pauli paramagnetism, an effect associated with the polarization of delocalized electrons' spins.

</doc>
<doc id="8317" url="https://en.wikipedia.org/wiki?curid=8317" title="Duke of Marlborough (title)">
Duke of Marlborough (title)

Duke of Marlborough ( ) is a title in the Peerage of England. It was created by Queen Anne in 1702 for John Churchill, 1st Earl of Marlborough (1650–1722), the noted military leader. The name of the dukedom refers to Marlborough in Wiltshire. It is one of the few titles in the peerage which allows for "suo jure" female inheritance, and the only current dukedom to do so.
History of the Dukedom.
Churchill had been made "Lord Churchill of Eyemouth" (1682) in the Scottish peerage, "Baron Churchill" of Sandridge (1685), and "Earl of Marlborough" (1689) in the Peerage of England. Shortly after her accession to the throne in 1702, Queen Anne made Churchill the first "Duke of Marlborough" and granted him the subsidiary title "Marquess of Blandford".
In 1678, Churchill married Sarah Jennings (1660–1744), a courtier and influential favourite of the queen. They had seven children, of whom four daughters married into some of the most important families in Great Britain; one daughter and one son died in infancy. He was pre-deceased by his son, John Churchill, Marquess of Blandford, in 1703; so, to prevent the extinction of the titles, a special Act of Parliament was passed. When the 1st Duke of Marlborough died in 1722 his title as "Lord Churchill of Eyemouth" in the Scottish peerage became extinct and the Marlborough titles passed, according to the Act, to his eldest daughter Henrietta (1681-1733), the 2nd Duchess of Marlborough. She was married to the 2nd Earl of Godolphin and had a son who predeceased her.
When Henrietta died in 1733, the Marlborough titles passed to her nephew Charles Spencer (1706–1758), the third son of her late sister Anne (1683-1716), who had married the 3rd Earl of Sunderland in 1699. After his older brother's death in 1729, Charles Spencer had already inherited the Spencer family estates and the titles of "Earl of Sunderland" (1643) and "Baron Spencer" of Wormleighton (1603), all in the Peerage of England. Upon his maternal aunt Henrietta's death in 1733, Charles Spencer succeeded to the Marlborough family estates and titles and became the 3rd Duke. When he died in 1758, his titles passed to his eldest son George (1739–1817), who was succeeded by his eldest son George, the 5th Duke (1766–1840). In 1815, Francis Spencer (the younger son of the 4th Duke) was created "Baron Churchill" in the Peerage of the United Kingdom. In 1902, his grandson, the 3rd Baron Churchill, was created "Viscount Churchill".
In 1817, the 5th Duke obtained permission to assume and bear the surname of Churchill in addition to his surname of Spencer, to perpetuate the name of his illustrious great-great-grandfather. At the same time he received Royal Licence to quarter the coat of arms of Churchill with his paternal arms of Spencer. The modern Dukes thus originally bore the surname "Spencer": the double-barrelled surname of "Spencer-Churchill" as used since 1817 remains in the family, though some members have preferred to style themselves "Churchill".
The 7th Duke was the paternal grandfather of the British Prime Minister Sir Winston Churchill, born at Blenheim Palace on 30 November 1874.
The 11th duke, John Spencer-Churchill died in 2014, having assumed the title in 1972. The 12th and present duke is Charles James Spencer-Churchill.
Family seat.
The family seat is Blenheim Palace in Woodstock, Oxfordshire.
After his leadership in the victory against the French in the Battle of Blenheim on 13 August 1704, the 1st Duke was honoured by Queen Anne granting him the royal manor of Woodstock, and building him a house at her expense to be called Blenheim. Construction started in 1705 and the house was completed in 1722, the year of the 1st Duke's death. Blenheim Palace has since remained in the Churchill and Spencer-Churchill family.
With the exception of the 10th Duke and his first wife, the Dukes and Duchesses of Marlborough are buried in Blenheim Palace's chapel. Most other members of the Spencer-Churchill family are interred in St. Martin's parish churchyard at Bladon, a short distance from the palace.
Succession to the title.
The dukedom is the only one in the United Kingdom that can still pass through a female line. However, unlike the remainder to heirs general found in most other peerages that allow male-preference primogeniture, the grant does not allow for abeyance and follows a more restrictive Semi-Salic formula designed to keep succession wherever possible in the male line. The succession is as follows:
Succession to the title under the first and second contingencies have lapsed; holders of the title from the 3rd Duke trace their status from the third contingency.
It is now very unlikely that the Dukedom will be passed to a woman or through a woman, since all the male-line descendants of the 1st Duke's second daughter Anne Spencer, Countess of Sunderland - including the lines of the Viscounts Churchill and Barons Churchill of Whichwood and of the Earls Spencer and the entire Spencer-Churchill and Spencer family - would have to become extinct.
If that were to happen, the Churchill titles would pass to the Earl of Jersey and his family, the heir-male of the 1st Duke's granddaughter Anne Villiers, Countess of Jersey, daughter of Elizabeth Egerton, Duchess of Bridgewater, the third daughter of the first Duke.
The next heir would be the Duke of Buccleuch and his family, the heir-male of the 1st Duke's great-granddaughter Elizabeth Montagu, Duchess of Buccleuch, the daughter of Mary Montagu, Duchess of Montagu (1766 creation), the daughter of the 1st Duke's youngest daughter Mary, Duchess of Montagu (1705 creation).
The fourth surviving line is represented by the Earl of Chichester and his family, the heir-male of the 1st Duke's most senior great-great-granddaughter Mary Henrietta Osborne, Countess of Chichester, daughter of Francis Osborne, 5th Duke of Leeds, only child of Mary Godolphin, Duchess of Leeds, daughter of the 1st Duke's eldest daughter Henrietta Godolphin, 2nd Duchess of Marlborough by her husband Francis Godolphin, 2nd Earl of Godolphin.
Other titles of the Dukes.
Subsidiary titles.
The Duke holds subsidiary titles: "Marquess of Blandford" (created in 1702 for John Churchill), "Earl of Sunderland" (created in 1643 for the Spencer family), "Earl of Marlborough" (created in 1689 for John Churchill), "Baron Spencer" of Wormleighton (created in 1603 for the Spencer family), and "Baron Churchill" of Sandridge (created in 1685 for John Churchill), all in the Peerage of England.
The title "Marquess of Blandford" is used as the courtesy title for the Duke's eldest son and heir. The Duke's eldest son's eldest son can use the courtesy title "Earl of Sunderland", and the duke's eldest son's eldest son's eldest son (eldest great-grandson) the title "Lord Spencer of Wormleighton" (not to be confused with Earl Spencer).
The title of "Earl of Marlborough", created for John Churchill in 1689, had previously been created for James Ley, in 1626, becoming extinct in 1679.
Foreign titles.
The 1st Duke was honoured with land and titles in the Holy Roman Empire: Emperor Joseph I created him a Prince in 1704, and in 1705 he was given the principality of Mindelheim (once the lordship of the noted soldier Georg von Frundsberg). He was obliged to surrender Mindelheim in 1714 by the Treaty of Utrecht, which returned it to Bavaria. He tried to obtain Nellenburg in Austria in exchange, which at that time was only a county ('Landgrafschaft'), but this failed, partially because Austrian law did not allow for Nellenburg being converted into a sovereign principality. Some authors misread the name of the county as Mellenburg. The 1st Duke's principality title of Mindelheim became extinct either on the return of the land to Bavaria or on his death, as the Empire operated Salic Law, which prevented female succession.
Coats of arms.
Original arms of the Churchill family.
The original arms of Sir Winston Churchill (1620–1688), father of the 1st Duke of Marlborough, were simple and in use by his own father in 1619. The shield was Sable a lion rampant Argent, debruised by a bendlet Gules. The addition of a canton of Saint George (see below) rendered the distinguishing mark of the bendlet unnecessary.
The Churchill crest is blazoned as a lion couchant guardant Argent, supporting with its dexter forepaw a banner Gules, charged with a dexter hand appaumée of the first, staff Or.
In recognition of Sir Winston's services to King Charles I as Captain of the Horse, and his loyalty to King Charles II as a Member of Parliament, he was awarded an augmentation of honour to his arms around 1662. This rare mark of royal favour took the form of a canton of Saint George. At the same time, he was authorised to omit the bendlet, which had served the purpose of distinguishing this branch of the Churchill family from others which bore an undifferenced lion.
Arms of the 1st Duke of Marlborough.
Sir Winston's shield and crest were inherited by his son John Churchill, 1st Duke of Marlborough. Minor modifications reflected the bearer's social rise: the helm was now shown in profile and had a closed grille to signify the bearer's rank as a peer, and there were now supporters placed on either side of the shield. They were the mythical Griffin (part lion, part eagle) and Wyvern (a dragon without hind legs). The supporters were derived from the arms of the family of the 1st Duke's mother, Drake of Ash (Argent, a wyvern gules; these arms can be seen on the monument in Musbury Church to Sir Bernard Drake, d.1586).
The motto was "Fiel pero desdichado" (Spanish for "Faithful but unfortunate"). The 1st Duke was also entitled to a coronet indicating his rank.
When the 1st Duke was made a Prince of the Holy Roman Empire in 1705, two unusual features were added: the Imperial Eagle and a Princely Coronet. His estates in Germany, such as Mindelheim, were represented in his arms by additional quarterings.
Arms of the Spencer-Churchill family.
In 1817, the 5th Duke received Royal Licence to place the quarter of Churchill ahead of his paternal arms of Spencer. The shield of the Spencer family arms is: quarterly Argent and Gules, in the second and third quarters a fret Or, over all on a bend Sable three escallops of the first. The Spencer crest is: out of a ducal coronet Or, a griffin's head between two wings expanded Argent, gorged with a collar gemel and armed Gules. Paul Courtenay observes that "It would be normal in these circumstances for the paternal arms (Spencer) to take precedence over the maternal (Churchill), but because the Marlborough dukedom was senior to the Sunderland earldom, the procedure was reversed in this case."
Also in 1817, a further augmentation of honour was added to his armorial achievement. This incorporated the bearings from the standard of the Manor of Woodstock and was borne on an escutcheon, displayed over all in the centre chief point, as follows: Argent a cross of Saint George surmounted by an inescutcheon Azure, charged with three fleurs-de-lys Or, two over one. This inescutcheon represents the royal arms of France.
The resulting heraldic achievement is:
These quartered arms, incorporating the two augmentations of honour, have been the arms of all subsequent Dukes of Marlborough.
Motto.
The motto "Fiel pero desdichado" is Spanish for "Faithful though unhappy". "Desdichado" means without happiness or without joy, alluding to the first Duke's father, Winston, who was a royalist and faithful supporter of the king during the English Civil War but was not compensated for his losses after the restoration. Charles II created Winston Churchill and other Civil War royalists knights but did not compensate them for their wartime losses, thereby inducing Winston to adopt the motto. It is unusual for the motto of an Englishman of the era to be in Spanish rather than Latin, and it is not known why this is the case.
List of title holders.
Dukes of Marlborough (1702).
The heir apparent to the Dukedom is George John Godolphin Spencer-Churchill, Marquess of Blandford (b. 1992), eldest son of the 12th Duke.

</doc>
<doc id="8322" url="https://en.wikipedia.org/wiki?curid=8322" title="December 17">
December 17


</doc>
<doc id="8324" url="https://en.wikipedia.org/wiki?curid=8324" title="Difference engine">
Difference engine

A difference engine is an automatic mechanical calculator designed to tabulate polynomial functions. The name derives from the method of divided differences, a way to interpolate or tabulate functions by using a small set of polynomial coefficients. Most mathematical functions commonly used by engineers, scientists and navigators, including logarithmic and trigonometric functions, can be approximated by polynomials, so a difference engine can compute many useful tables of numbers.
The historical difficulty in producing error-free tables by teams of mathematicians and human "computers" spurred Charles Babbage's desire to build a mechanism to automate the process. It is considered to be the world's first computer.
History.
J. H. Müller, an engineer in the Hessian army, conceived of the idea of a difference machine. This was described in a book published in 1786, but Müller was unable to obtain funding to progress with the idea.
On 14 June 1822, Charles Babbage proposed the use of such a machine in a paper to the Royal Astronomical Society, entitled "Note on the application of machinery to the computation of astronomical and mathematical tables". This machine used the decimal number system and was powered by cranking a handle. The British government was interested, since producing tables was time-consuming and expensive and they hoped the difference engine would make the task more economical.
In 1823, the British government gave Babbage £1700 to start work on the project. Although Babbage's design was feasible, the metalworking techniques of the era could not economically make parts in the precision and quantity required. Thus the implementation proved to be much more expensive and doubtful of success than the government's initial bargain. By the time the government abandoned the project in 1842, Babbage had received and spent over £17,000 on development, which still fell short of achieving a working engine. The government valued only the machine's output (economically produced tables), not the development (at unknown and unpredictable cost to complete) of the machine itself. Babbage did not, or was unwilling to, recognize that predicament. Meanwhile, Babbage's attention had moved on to developing an analytical engine, further undermining the government’s confidence in the eventual success of the difference engine. By improving the concept as an analytical engine, Babbage had made the difference engine concept obsolete, and the project to implement it an utter failure in the view of the government.
Babbage went on to design his much more general analytical engine, but later produced an improved "Difference Engine No. 2" design, between 1847 and 1849. Babbage was able to take advantage of ideas developed for the analytical engine to make the new difference engine calculate more quickly while using fewer parts. Inspired by Babbage's difference engine plans, Per Georg Scheutz built several difference engines from 1855 onwards, one of which was sold to the British government in 1859. Martin Wiberg improved Scheutz's construction but used his device only for producing and publishing printed logarithmic tables.
Construction of two working No. 2 difference engines.
During the 1980s, Allan G. Bromley, an associate professor at the University of Sydney, Australia, studied Babbage's original drawings for the Difference and Analytical Engines at the Science Museum library in London. This work led the Science Museum to construct a working difference engine No. 2 from 1989 to 1991, under Doron Swade, the then Curator of Computing. This was to celebrate the 200th anniversary of Babbage's birth in 2001. In 2000, the printer which Babbage originally designed for the difference engine was also completed. The conversion of the original design drawings into drawings suitable for engineering manufacturers' use revealed some minor errors in Babbage's design (possibly introduced as a protection in case the plans were stolen), which had to be corrected. Once completed, both the engine and its printer worked flawlessly, and still do. The difference engine and printer were constructed to tolerances achievable with 19th-century technology, resolving a long-standing debate as to whether Babbage's design would actually have worked. (One of the reasons formerly advanced for the non-completion of Babbage's engines had been that engineering methods were insufficiently developed in the Victorian era.)
The printer's primary purpose is to produce stereotype plates for use in printing presses, which it does by pressing type into soft plaster to create a flong. Babbage intended that the Engine's results be conveyed directly to mass printing, having recognized that many errors in previous tables were not the result of human calculating mistakes but from error in the manual typesetting process. The printer's paper output is mainly a means of checking the Engine's performance.
In addition to funding the construction of the output mechanism for the Science Museum's Difference Engine No. 2, Nathan Myhrvold commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the Computer History Museum in Mountain View, California until 31 January 2016.
Operation.
The difference engine consists of a number of columns, numbered from 1 to N. The machine is able to store one decimal number in each column. The machine can only add the value of a column "n" + 1 to column "n" to produce the new value of "n". Column "N" can only store a constant, column 1 displays (and possibly prints) the value of the calculation on the current iteration.
The engine is programmed by setting initial values to the columns. Column 1 is set to the value of the polynomial at the start of computation. Column 2 is set to a value derived from the first and higher derivatives of the polynomial at the same value of X. Each of the columns from 3 to "N" is set to a value derived from the formula_1 first and higher derivatives of the polynomial.
Timing.
In the Babbage design, one iteration (i.e., one full set of addition and carry operations) happens for each rotation of the main shaft. Odd and even columns alternately perform an addition in one cycle. The sequence of operations for column formula_2 is thus:
Steps 1,2,3,4 occur for every odd column, while steps 3,4,1,2 occur for every even column.
While Babbage's original design placed the crank directly on the main shaft, it was later realized that the force required to crank the machine would have been too great for a human to handle comfortably. Therefore, the two models that were built incorporate a 4:1 reduction gear at the crank, and four revolutions of the crank are required to perform one full cycle.
Steps.
Each iteration creates a new result, and is accomplished in four steps corresponding to four complete turns of the handle shown at the far right in the picture below. The four steps are:
Subtraction.
The engine represents negative numbers as ten's complements. Subtraction amounts to addition of a negative number. This works in the same manner that modern computers perform subtraction, known as two's complement.
Method of differences.
The principle of a difference engine is Newton's method of divided differences. If the initial value of a polynomial (and of its finite differences) is calculated by some means for some value of X, the difference engine can calculate any number of nearby values, using the method generally known as the method of finite differences. For example, consider the quadratic polynomial
with the goal of tabulating the values "p"(0), "p"(1), "p"(2), "p"(3), "p"(4), and so forth. The table below is constructed as follows: the second column contains the values of the polynomial, the third column contains the differences of the two left neighbors in the second column, and the fourth column contains the differences of the two neighbors in the third column:
The numbers in the third values-column are constant. In fact, by starting with any polynomial of degree "n", the column number "n" + 1 will always be constant. This is the crucial fact behind the success of the method.
This table was built from left to right, but it is possible to continue building it from right to left down a diagonal in order to compute more values. To calculate "p"(5) use the values from the lowest diagonal. Start with the fourth column constant value of 4 and copy it down the column. Then continue the third column by adding 4 to 11 to get 15. Next continue the second column by taking its previous value, 22 and adding the 15 from the third column. Thus "p"(5) is 22 + 15 = 37. In order to compute "p"(6), we iterate the same algorithm on the "p"(5) values: take 4 from the fourth column, add that to the third column's value 15 to get 19, then add that to the second column's value 37 to get 56, which is "p"(6). This process may be continued ad infinitum. The values of the polynomial are produced without ever having to multiply. A difference engine only needs to be able to add. From one loop to the next, it needs to store 2 numbers—in this example (the last elements in the first and second columns). To tabulate polynomials of degree "n", one needs sufficient storage to hold "n" numbers.
Babbage's difference engine No. 2, finally built in 1991, could hold 8 numbers of 31 decimal digits each and could thus tabulate 7th degree polynomials to that precision. The best machines from Scheutz could store 4 numbers with 15 digits each.
Initial values.
The initial values of columns can be calculated by first manually calculating N consecutive values of the function and by backtracking, i.e. calculating the required differences.
Col formula_6 gets the value of the function at the start of computation formula_7. Col formula_8 is the difference between formula_9 and formula_7…
If the function to be calculated is a polynomial function, expressed as
the initial values can be calculated directly from the constant coefficients "a"0, "a"1,"a"2, …, "an" without calculating any data points. The initial values are thus:
Use of derivatives.
Many commonly used functions are analytic functions, which can be expressed as power series, for example as a Taylor series. The initial values can be calculated to any degree of accuracy; if done correctly the engine will give exact results for first N steps. After that, the engine will only give an approximation of the function.
The Taylor series expresses the function as a sum obtained from its derivatives at one point. For many functions the higher derivatives are trivial to obtain; for instance, the sine function at 0 has values of 0 or formula_19 for all derivatives. Setting 0 as the start of computation we get the simplified Maclaurin series
The same method of calculating the initial values from the coefficients can be used as for polynomial functions. The polynomial constant coefficients will now have the value
Curve fitting.
The problem with the methods described above is that errors will accumulate and the series will tend to diverge from the true function. A solution which guarantees a constant maximum error is to use curve fitting. A minimum of N values are calculated evenly spaced along the range of the desired calculations. Using a curve fitting technique like Gaussian reduction an N-1th degree polynomial interpolation of the function is found. With the optimized polynomial, the initial values can be calculated as above.

</doc>
<doc id="8326" url="https://en.wikipedia.org/wiki?curid=8326" title="Draupnir">
Draupnir

In Norse mythology, Draupnir (Old Norse "the dripper") is a gold ring possessed by the god Odin with the ability to multiply itself: Every ninth night, eight new rings 'drip' from Draupnir, each one of the same size and weight as the original.
Draupnir was forged by the dwarven brothers Brokkr and Eitri (or Sindri). Brokkr and Eitri made this ring as one of a set of three gifts which included Mjöllnir and Gullinbursti. They made these gifts in accordance with a wager Loki made saying that Brokk and Eitri could not make better gifts than the three made by the Sons of Ivaldi. In the end, Mjöllnir, Thor's hammer, won the contest for Brokkr and Eitri. Loki used a loophole to get out of the wager for his head (the wager was for Loki's head only, but he argued that, to remove his head, they would have to injure his neck, which was not in the bargain) and Brokkr punished him by sealing his lips shut with wire.
The ring was placed by Odin on the funeral pyre of his son Baldr:
The ring was subsequently retrieved by Hermóðr. It was offered as a gift by Freyr's servant Skírnir in the wooing of Gerðr, which is described in the poem "Skírnismál".
Draupnir in popular culture.
"DRAUPNIR" was revealed as the password to a website that Neal Caffrey and Mozzie used to view their stolen Nazi U-boat treasure in "Taking Account", the seventh episode of the third season of "White Collar".
Draupnir is represented as a card in the Yu-Gi-Oh Trading Card Game. It has an effect that mimics the multiplication ability of the mythological version. If it is destroyed by another card's effect, you can add another "Nordic Relic" card to your hand. The art represents it as an arm brace, with another brace seemingly growing from it, once again mimicking the story.
It also appeared in episode 11 of as a tool to seal Loki's spirit.

</doc>
<doc id="8328" url="https://en.wikipedia.org/wiki?curid=8328" title="Divergence">
Divergence

In vector calculus, divergence is a vector operator that produces a signed scalar field giving the quantity of a vector field's source at each point. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.
As an example, consider air as it is heated or cooled. The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region. The divergence of the velocity field in that region would thus have a positive value. While the air is cooled and thus contracting, the divergence of the velocity has a negative value.
Definition of divergence.
In physical terms, the divergence of a three-dimensional vector field is the extent to which the vector field flow behaves like a source at a given point. It is a local measure of its "outgoingness" – the extent to which there is more exiting an infinitesimal region of space than entering it. If the divergence is nonzero at some point then there must be a source or sink at that position. (Note that we are imagining the vector field to be like the velocity vector field of a fluid (in motion) when we use the terms "flow", "source" and so on.)
More rigorously, the divergence of a vector field F at a point "p" is defined as the limit of the net flow of F across the smooth boundary of a three-dimensional region "V" divided by the volume of "V" as "V" shrinks to "p". Formally,
where is the volume of "V", "S"("V") is the boundary of "V", and the integral is a surface integral with n being the outward unit normal to that surface. The result, div F, is a function of "p". From this definition it also becomes explicitly visible that can be seen as the "source density" of the flux of F.
In light of the physical interpretation, a vector field with zero divergence everywhere is called "incompressible" or "solenoidal" – in this case, no net flow can occur across any closed surface.
The intuition that the sum of all sources minus the sum of all sinks should give the net flow outwards of a region is made precise by the divergence theorem.
Application in Cartesian coordinates.
Let "x", "y", "z" be a system of Cartesian coordinates in 3-dimensional Euclidean space, and let i, j, k be the corresponding basis of unit vectors.
The divergence of a continuously differentiable vector field is equal to the scalar-valued function:
Although expressed in terms of coordinates, the result is invariant under orthogonal transformations, as the physical interpretation suggests.
The common notation for the divergence is a convenient mnemonic, where the dot denotes an operation reminiscent of the dot product: take the components of the ∇ operator (see del), apply them to the components of F, and sum the results. Because applying an operator is different from multiplying the components, this is considered an abuse of notation.
The divergence of a continuously differentiable second-order tensor field formula_3 is a first-order tensor field:
Cylindrical coordinates.
For a vector expressed in cylindrical coordinates as
where e"a" is the unit vector in direction "a", the divergence is
Spherical coordinates.
In spherical coordinates, with formula_7 the angle with the "z" axis and formula_8 the rotation around the "z" axis, the divergence reads
Decomposition theorem.
It can be shown that any stationary flux v(r) that is at least twice continuously differentiable in formula_10 and vanishes sufficiently fast for can be decomposed into an "irrotational part" E(r) and a "source-free part" B(r). Moreover, these parts are explicitly determined by the respective "source densities" (see above) and "circulation densities" (see the article Curl):
For the irrotational part one has
with
The source-free part, B, can be similarly written: one only has to replace the "scalar potential" Φ(r) by a "vector potential" A(r) and the terms −∇Φ by , and the source density 
by the circulation-density .
This "decomposition theorem" is a by-product of the stationary case of electrodynamics. It is a special case of the more general Helmholtz decomposition which works in dimensions greater than three as well.
Properties.
The following properties can all be derived from the ordinary differentiation rules of calculus. Most importantly, the divergence is a linear operator, i.e.
for all vector fields F and G and all real numbers "a" and "b".
There is a product rule of the following type: if formula_14 is a scalar valued function and F is a vector field, then
or in more suggestive notation
Another product rule for the cross product of two vector fields F and G in three dimensions involves the curl and reads as follows:
or
The Laplacian of a scalar field is the divergence of the field's gradient:
The divergence of the curl of any vector field (in three dimensions) is equal to zero: 
If a vector field F with zero divergence is defined on a ball in R3, then there exists some vector field G on the ball with F = curl(G). For regions in R3 more topologically complicated than this, the latter statement might be false (see Poincaré lemma). The degree of "failure" of the truth of the statement, measured by the homology of the chain complex
(where the first map is the gradient, the second is the curl, the third is the divergence) serves as a nice quantification of the complicatedness of the underlying region "U". These are the beginnings and main motivations of de Rham cohomology.
Relation with the exterior derivative.
One can express the divergence as a particular case of the exterior derivative, which takes a 2-form to a 3-form in R3.
Define the current two-form as
It measures the amount of "stuff" flowing through a surface per unit time in a "stuff fluid" of density formula_26 moving with local velocity F. Its exterior derivative formula_27 is then given by
Thus, the divergence of the vector field F can be expressed as:
Here the superscript formula_30 is one of the two musical isomorphisms, and formula_31 is the Hodge dual. Working with the current two-form and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.
Generalizations.
The divergence of a vector field can be defined in any number of dimensions. If 
in a Euclidean coordinate system where formula_33 and formula_34, define
The appropriate expression is more complicated in curvilinear coordinates.
In the case of one dimension, a F reduces to a regular function, and the divergence reduces to the derivative.
For any "n", the divergence is a linear operator, and it satisfies the "product rule"
for any scalar-valued function formula_37.
The divergence of a vector field extends naturally to any differentiable manifold of dimension "n" with a volume form (or density) formula_38 e.g. a Riemannian or Lorentzian manifold. Generalising the construction of a two-form for a vector field on formula_39, on such a manifold a vector field "X" defines an -form formula_40 obtained by contracting "X" with formula_38. The divergence is then the function defined by
Standard formulas for the Lie derivative allow us to reformulate this as
This means that the divergence measures the rate of expansion of a volume element as we let it flow with the vector field.
On a pseudo-Riemannian manifold, the divergence with respect to the metric volume form can be computed in terms of the Levi-Civita connection formula_44:
where the second expression is the contraction of the vector field valued 1-form formula_46 with itself and the last expression is the traditional coordinate expression from Ricci calculus.
An equivalent expression without using connection is
where formula_48 is the metric and formula_49 denotes partial derivative with respect to coordinate formula_50.
Divergence can also be generalised to tensors. In Einstein notation, the divergence of a contravariant vector formula_51 is given by
where formula_53 denotes the covariant derivative.
Equivalently, some authors define the divergence of a mixed tensor by using the musical isomorphism :
If "T" is a -tensor ("p" for the contravariant vector and "q" for the covariant one), then we define the "divergence of T" to be the -tensor
that is we trace the covariant derivative on the "first two" covariant indices.

</doc>
<doc id="8334" url="https://en.wikipedia.org/wiki?curid=8334" title="December 18">
December 18


</doc>
<doc id="8336" url="https://en.wikipedia.org/wiki?curid=8336" title="Decision problem">
Decision problem

In computability theory and computational complexity theory, a decision problem is a question in some formal system with a yes-or-no answer, depending on the values of some input parameters. Decision problems typically appear in mathematical questions of decidability, that is, the question of the existence of an effective method to determine the existence of some object or its membership in a set; some of the most important problems in mathematics are undecidable.
For example, the problem "given two numbers "x" and "y", does "x" evenly divide "y"?" is a decision problem. The answer can be either 'yes' or 'no', and depends upon the values of "x" and "y". A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem "given two numbers "x" and "y", does "x" evenly divide "y"?" would give the steps for determining whether "x" evenly divides "y", given "x" and "y". One such algorithm is long division, taught to many school children. If the remainder is zero the answer produced is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm, such as this example, is called "decidable".
The field of computational complexity categorizes "decidable" decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes "undecidable" decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution. Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'. A corresponding function problem is "given two numbers "x" and "y", what is "x" divided by "y"?". They are also related to optimization problems, which are concerned with finding the "best" answer to a particular problem. There are standard techniques for transforming function and optimization problems into decision problems, and vice versa, that do not significantly change the computational difficulty of these problems. For this reason, research in computability theory and complexity theory have typically focused on decision problems.
Definition.
A "decision problem" is any arbitrary yes-or-no question on an infinite set of inputs. Because of this, it is traditional to define the decision problem equivalently as: the set of inputs for which the problem returns "yes".
These inputs can be natural numbers, but may also be values of some other kind, such as strings over the binary alphabet {0,1} or over some other finite set of symbols. The subset of strings for which the problem returns "yes" is a formal language, and often decision problems are defined in this way as formal languages.
Alternatively, using an encoding such as Gödel numberings, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers.
Examples.
A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.
Decidability.
A decision problem "A" is called "decidable" or "effectively solvable" if "A" is a recursive set. A problem is called "partially decidable", "semidecidable", "solvable", or "provable" if "A" is a recursively enumerable set. Problems that are not decidable are called "undecidable".
The halting problem is an important undecidable decision problem; for more examples, see list of undecidable problems.
Complete problems.
Decision problems can be ordered according to many-one reducibility and related feasible reductions such as polynomial-time reductions. A decision problem "P" is said to be "complete" for a set of decision problems "S" if "P" is a member of "S" and every problem in "S" can be reduced to "P". Complete decision problems are used in computational complexity to characterize complexity classes of decision problems. For example, the Boolean satisfiability problem is complete for the class NP of decision problems under polynomial-time reducibility.
Equivalence with function problems.
A function problem consists of a partial function "f"; the informal "problem" is to compute the values of "f" on the inputs for which it is defined.
Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function. (The graph of a function "f" is the set of pairs ("x","y") such that "f"("x") = "y".) If this decision problem were effectively solvable then the function problem would be as well. This reduction does not respect computational complexity, however. For example, it is possible for the graph of a function to be decidable in polynomial time (in which case running time is computed as a function of the pair ("x","y") ) when the function is not computable in polynomial time (in which case running time is computed as a function of "x" alone). The function "f"("x") = "2""x" has this property.
Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem. If this function is computable then the associated decision problem is decidable. However, this reduction is more liberal than the standard reduction used in computational complexity (sometimes called polynomial-time many-one reduction); for example, the complexity of the characteristic functions of an NP-complete problem and its co-NP-complete complement is exactly the same even though the underlying decision problems may not be considered equivalent in some typical models of computation.

</doc>
<doc id="8339" url="https://en.wikipedia.org/wiki?curid=8339" title="Domain Name System">
Domain Name System

The Domain Name System (DNS) is a hierarchical decentralized naming system for computers, services, or any resource connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for the purpose of locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System is an essential component of the functionality of the Internet.
The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over sub-domains of their allocated name space to other name servers. This mechanism provides distributed and fault tolerant service and was designed to avoid a single large central database.
The Domain Name System also specifies the technical functionality of the database service which is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet Protocol Suite. Historically, other directory services preceding DNS were not scalable to large or global directories as they were originally based on text files, prominently the HOSTS.TXT resolver. The Domain Name System has been in use since the 1980s.
The Internet maintains two principal namespaces, the domain name hierarchy and the Internet Protocol (IP) address spaces. The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System. A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.
The most common types of records stored in the DNS database are for DNS zone authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS can store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as "responsible person" (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list. The DNS database is traditionally stored in a structured zone file.
Function.
An often-used analogy to explain the Domain Name System is that it serves as the phone book for the Internet by translating human-friendly computer hostnames into IP addresses. For example, the domain name www.example.com translates to the addresses 93.184.216.119 (IPv4) and 2606:2800:220:6d:26bf:1447:1097:aa7 (IPv6). Unlike a phone book, DNS can be quickly updated, allowing a service's location on the network to change without affecting the end users, who continue to use the same host name. Users take advantage of this when they use meaningful Uniform Resource Locators (URLs), and e-mail addresses without having to know how the computer actually locates the services.
Additionally, DNS reflects administrative partitioning. For zones operated by a registry, also known as public suffix zones, administrative information is often complemented by the registry's RDAP and WHOIS services. That data can be used to gain insight on, and track responsibility for, a given host on the Internet.
An important and ubiquitous function of DNS is its central role in distributed Internet services such as cloud services and content delivery networks. When a user accesses a distributed Internet service using a URL, the domain name of the URL is translated to the IP address of a server that is proximal to the user. The key functionality of DNS exploited here is that different users can "simultaneously" receive different translations for the "same" domain name, a key point of divergence from a traditional "phone book" view of DNS. This process of using DNS to assign proximal servers to users is key to providing faster response times on the Internet and is widely used by most major Internet services today.
History.
Using a simpler, more memorable name in place of a host's numerical address dates back to the ARPANET era. The Stanford Research Institute (now SRI International) maintained a text file named HOSTS.TXT that mapped host names to the numerical addresses of computers on the ARPANET. Host operators obtained copies of the master file. The rapid growth of the emerging network required an automated system for maintaining the host names and addresses.
Paul Mockapetris designed the Domain Name System at the University of California, Irvine in 1983, and wrote the first implementation at the request of Jon Postel from ISI. The Internet Engineering Task Force published the original specifications in RFC 882 and RFC 883 in November 1983, which established the concepts that still guide DNS development.
In 1984, four UC Berkeley students—Douglas Terry, Mark Painter, David Riggle, and Songnian Zhou—wrote the first Unix name server implementation, called the Berkeley Internet Name Domain (BIND) Server. In 1985, Kevin Dunlap of DEC substantially revised the DNS implementation. Mike Karels, Phil Almquist, and Paul Vixie have maintained BIND since then. BIND was ported to the Windows NT platform in the early 1990s. BIND was widely distributed, especially on Unix systems, and is still the most widely used DNS software on the Internet.
In November 1987, RFC 1034 and RFC 1035 superseded the 1983 DNS specifications. Several additional Request for Comments have proposed extensions to the core DNS protocols.
Structure.
Domain name space.
The domain name space consists of a tree data structure. Each node or leaf in the tree has a "label" and zero or more "resource records" (RR), which hold information associated with the domain name. The domain name itself consists of the label, possibly concatenated with the name of its parent node on the right, separated by a dot.
The tree sub-divides into "zones" beginning at the root zone. A DNS zone may consist of only one domain, or may consist of many domains and sub-domains, depending on the administrative choices of the zone manager. DNS can also be partitioned according to "class"; the separate classes can be thought of as an array of parallel namespace trees.
Administrative responsibility over any zone may be divided by creating additional zones. Authority over the new zone is said to be "delegated" to a designated name server. The parent zone ceases to be authoritative for the new zone.
Domain name syntax.
The definitive descriptions of the rules for forming domain names appear in RFC 1035, RFC 1123, and RFC 2181.
A domain name consists of one or more parts, technically called "labels", that are conventionally concatenated, and delimited by dots, such as example.com.
The right-most label conveys the top-level domain; for example, the domain name www.example.com belongs to the top-level domain "com".
The hierarchy of domains descends from right to left; each label to the left specifies a subdivision, or subdomain of the domain to the right. For example: the label "example" specifies a subdomain of the "com" domain, and "www" is a subdomain of example.com. This tree of subdivisions may have up to 127 levels.
A label may contain zero to 63 characters. The null label, of length zero, is reserved for the root zone. The full domain name may not exceed the length of 253 characters in its textual representation. In the internal binary representation of the DNS the maximum length requires 255 octets of storage, since it also stores the length of the name.
Although domain names may theoretically consist of any character representable in an octet, host names use a preferred format and character set. The characters allowed in their labels are a subset of the ASCII character set, consisting of characters "a" through "z", "A" through "Z", digits "0" through "9", and hyphen. This rule is known as the "LDH rule" (letters, digits, hyphen). Domain names are interpreted in case-independent manner. Labels may not start or end with a hyphen. An additional rule requires that top-level domain names should not be all-numeric.
Internationalized domain names.
The limited set of ASCII characters permitted in the DNS prevented the representation of names and words of many languages in their native alphabets or scripts. To make this possible, ICANN approved the Internationalizing Domain Names in Applications (IDNA) system, by which user applications, such as web browsers, map Unicode strings into the valid DNS character set using Punycode. In 2009 ICANN approved the installation of internationalized domain name country code top-level domains ("ccTLD"s). In addition, many registries of the existing top level domain names ("TLD"s) have adopted the IDNA system.
Name servers.
The Domain Name System is maintained by a distributed database system, which uses the client–server model. The nodes of this database are the name servers. Each domain has at least one authoritative DNS server that publishes information about that domain and the name servers of any domains subordinate to it. The top of the hierarchy is served by the root name servers, the servers to query when looking up ("resolving") a TLD.
Authoritative name server.
An "authoritative" name server is a name server that gives answers that have been configured by an original source, for example, the domain administrator or by dynamic DNS methods, in contrast to answers that were obtained via a regular DNS query to another name server. An authoritative-only name server only returns answers to queries about domain names that have been specifically configured by the administrator.
In other words, an authoritative name server lets recursive name servers know what DNS data (the IPv4 IP, the IPv6 IP, a list of incoming mail servers, etc.) a given host name (such as "www.example.com") has. As just one example, the authoritative name server for "example.com" tells recursive name servers that "www.example.com" has the IPv4 IP address 192.0.43.10.
An authoritative name server can either be a "master" server or a "slave" server. A master server is a server that stores the original ("master") copies of all zone records. A slave server uses an automatic updating mechanism of the DNS protocol in communication with its master to maintain an identical copy of the master records.
A set of authoritative name servers has to be assigned for every DNS zone. An NS record about addresses of that set must be stored in the parent zone and servers themselves (as self-reference).
When domain names are registered with a domain name registrar, their installation at the domain registry of a top level domain requires the assignment of a "primary" name server and at least one "secondary" name server. The requirement of multiple name servers aims to make the domain still functional even if one name server becomes inaccessible or inoperable. The designation of a primary name server is solely determined by the priority given to the domain name registrar. For this purpose, generally only the fully qualified domain name of the name server is required, unless the servers are contained in the registered domain, in which case the corresponding IP address is needed as well.
Primary name servers are often master name servers, while secondary name servers may be implemented as slave servers.
An authoritative server indicates its status of supplying definitive answers, deemed "authoritative", by setting a software flag (a protocol structure bit), called the "Authoritative Answer" ("AA") bit in its responses. This flag is usually reproduced prominently in the output of DNS administration query tools (such as dig) to indicate "that the responding name server is an authority for the domain name in question."
Operation.
Address resolution mechanism.
Domain name resolvers determine the domain name servers responsible for the domain name in question by a sequence of queries starting with the right-most (top-level) domain label.
For proper operation of its domain name resolver, a network host is configured with an initial cache ("hints") of the known addresses of the root name servers. The hints are updated periodically by an administrator by retrieving a dataset from a reliable source.
Assuming the resolver has no cached records to accelerate the process, the resolution process starts with a query to one of the root servers. In typical operation, the root servers will not answer directly, but will respond with a referral to more authoritative servers, e.g. a query for "www.wikipedia.org" will be referred to the ".org" servers. The resolver will then query the servers it finds itself referred to, and iteratively repeat this process until it receives a concrete (non-referral) answer. The diagram illustrates this process for the host www.wikipedia.org.
A common misconception is that a DNS resolver will "split up" a domain name — for example, that a query for "www.wikipedia.org" will query the root servers for the ".org" servers, and then query the ".org" servers for the "wikipedia.org" servers. While the chain of servers may be correct, the key distinction is that the resolver is making the specific request for "www.wikipedia.org" at every level of resolution, and receiving referrals only because the servers have chosen that referral structure. There is no reason that the root servers could not directly return an IP answer, bypassing the referral process entirely. Similarly, a query for "a.b.c.example.com" can often be answered directly by the "example.com" servers, without the lengthy hypothetical process of querying nameservers for "c.example.com" and "b.c.example.com".
This mechanism would place a large traffic burden on the root servers, as every resolution on the Internet would require them. In practice caching is used in DNS servers to off-load the root servers, and as a result, root name servers actually are involved in only a fraction of all requests.
Recursive and caching name server.
In theory, authoritative name servers are sufficient for the operation of the Internet. However, with only authoritative name servers operating, every DNS query must start with recursive queries at the root zone of the Domain Name System and each user system would have to implement resolver software capable of recursive operation.
To improve efficiency, reduce DNS traffic across the Internet, and increase performance in end-user applications, the Domain Name System supports DNS cache servers which store DNS query results for a period of time determined in the configuration (time-to-live) of the domain name record in question.
Typically, such "caching" DNS servers, also called "DNS caches", also implement the recursive algorithm necessary to resolve a given name starting with the DNS root through to the authoritative name servers of the queried domain. With this function implemented in the name server, user applications gain efficiency in design and operation.
As one example, if a client wants to know the address for "www.example.com", it might send, to a recursive caching name server, a DNS request stating "I would like the IPv4 address for 'www.example.com'." The recursive name server will then query authoritative name servers until it gets an answer to that query (or return an error if it's not possible to get an answer)—in this case 192.0.43.10.
On a subsequent query for "www.example.com", this process can be greatly accelerated:
The combination of DNS caching and recursive functions in a name server is not mandatory; the functions can be implemented independently in servers for special purposes.
Internet service providers typically provide recursive and caching name servers for their customers. In addition, many home networking routers implement DNS caches and recursors to improve efficiency in the local network.
DNS resolvers.
The client side of the DNS is called a DNS resolver. A resolver is responsible for initiating and sequencing the queries that ultimately lead to a full resolution (translation) of the resource sought, e.g., translation of a domain name into an IP address. An individual DNS query may be either "non-recursive", "recursive", or "iterative", or a combination of these.
Circular dependencies and glue records.
Name servers in delegations are identified by name, rather than by IP address. This means that a resolving name server must issue another DNS request to find out the IP address of the server to which it has been referred. If the name given in the delegation is a subdomain of the domain for which the delegation is being provided, there is a circular dependency. In this case the name server providing the delegation must also provide one or more IP addresses for the authoritative name server mentioned in the delegation. This information is called "glue". The delegating name server provides this glue in the form of records in the "additional section" of the DNS response, and provides the delegation in the "authority section" of the response.
For example, if the authoritative name server for example.org is ns1.example.org, a computer trying to resolve www.example.org first resolves ns1.example.org. Since ns1 is contained in example.org, this requires resolving example.org first, which presents a circular dependency. To break the dependency, the name server for the top level domain org includes glue along with the delegation for example.org. The glue records are address records that provide IP addresses for ns1.example.org. The resolver uses one or more of these IP addresses to query one of the domain's authoritative servers, which allows it to complete the DNS query.
Record caching.
The DNS Resolution Process reduces the load on individual servers by "caching" DNS request records for a period of time after a response. This entails the local recording and subsequent consultation of the copy instead of initiating a new request upstream. The time for which a resolver caches a DNS response is determined by a value called the time to live (TTL) associated with every record. The TTL is set by the administrator of the DNS server handing out the authoritative response. The period of validity may vary from just seconds to days or even weeks.
As a noteworthy consequence of this distributed and caching architecture, changes to DNS records do not propagate throughout the network immediately, but require all caches to expire and refresh after the TTL. RFC 1912 conveys basic rules for determining appropriate TTL values.
Some resolvers may override TTL values, as the protocol supports caching for up to 68 years or no caching at all. Negative caching, i.e. the caching of the fact of non-existence of a record, is determined by name servers authoritative for a zone which must include the Start of Authority (SOA) record when reporting no data of the requested type exists. The value of the "minimum" field of the SOA record and the TTL of the SOA itself is used to establish the TTL for the negative answer.
Reverse lookup.
A reverse lookup is a query of the DNS for domain names when the IP address is known. Multiple domain names may be associated with an IP address. The DNS stores IP addresses in the form of domain names as specially formatted names in pointer (PTR) records within the infrastructure top-level domain arpa. For IPv4, the domain is in-addr.arpa. For IPv6, the reverse lookup domain is ip6.arpa. The IP address is represented as a name in reverse-ordered octet representation for IPv4, and reverse-ordered nibble representation for IPv6.
When performing a reverse lookup, the DNS client converts the address into these formats before querying the name for a PTR record following the delegation chain as for any DNS query. For example, assuming the IPv4 address 208.80.152.2 is assigned to Wikimedia, it is represented as a DNS name in reverse order: 2.152.80.208.in-addr.arpa. When the DNS resolver gets a pointer (PTR) request, it begins by querying the root servers, which point to the servers of American Registry for Internet Numbers (ARIN) for the 208.in-addr.arpa zone. ARIN's servers delegate 152.80.208.in-addr.arpa to Wikimedia to which the resolver sends another query for 2.152.80.208.in-addr.arpa, which results in an authoritative response.
Client lookup.
Users generally do not communicate directly with a DNS resolver. Instead DNS resolution takes place transparently in applications such as web browsers, e-mail clients, and other Internet applications. When an application makes a request that requires a domain name lookup, such programs send a resolution request to the DNS resolver in the local operating system, which in turn handles the communications required.
The DNS resolver will almost invariably have a cache (see above) containing recent lookups. If the cache can provide the answer to the request, the resolver will return the value in the cache to the program that made the request. If the cache does not contain the answer, the resolver will send the request to one or more designated DNS servers. In the case of most home users, the Internet service provider to which the machine connects will usually supply this DNS server: such a user will either have configured that server's address manually or allowed DHCP to set it; however, where systems administrators have configured systems to use their own DNS servers, their DNS resolvers point to separately maintained name servers of the organization. In any event, the name server thus queried will follow the process outlined above, until it either successfully finds a result or does not. It then returns its results to the DNS resolver; assuming it has found a result, the resolver duly caches that result for future use, and hands the result back to the software which initiated the request.
Broken resolvers.
Some large ISPs have configured their DNS servers to violate rules, such as by disobeying TTLs, or by indicating that a domain name does not exist just because one of its name servers does not respond.
Some applications, such as web browsers, maintain an internal DNS cache to avoid repeated lookups via the network. This practice can add extra difficulty when debugging DNS issues, as it obscures the history of such data. These caches typically use very short caching times – in the order of one minute.
Internet Explorer represents a notable exception: versions up to IE 3.x cache DNS records for 24 hours by default. Internet Explorer 4.x and later versions (up to IE 8) decrease the default time out value to half an hour, which may be changed by modifying default configuration.
Other applications.
The Domain Name System includes several other functions and features.
No requirement exists that hostnames and IP addresses must match in a one-to-one fashion. Multiple hostnames may correspond to a single IP address and vice versa. Virtual hosting in which a single address is associated with multiple hostnames permits a single server to serve many web sites. Alternatively, a single hostname may correspond to many IP addresses to facilitate fault tolerance and load distribution.
DNS serves other purposes in addition to translating names to IP addresses. For instance, mail transfer agents use DNS to find the best mail server to deliver e-mail. The domain to mail exchanger mapping provided by MX records may present an additional layer of fault tolerance and load distribution.
The DNS is used for efficient storage and distribution of IP addresses of blacklisted e-mail hosts. The usual method is placing the IP address of the subject host into the sub-domain of a higher level domain name, and resolving that name to different records to indicate a positive or a negative indications. Here is a hypothetical example blacklist:
The Sender Policy Framework and DomainKeys were designed to take advantage of another DNS record type, the TXT record, but have since been assigned specific record types.
To provide resilience in the event of computer failure, multiple DNS servers are usually provided for coverage of each domain, and at the top level, thirteen very powerful root name servers exist, with additional "copies" of several of them distributed worldwide via anycast.
Dynamic DNS (DDNS) allows clients to update a DNS entry when their IP address changes, for example, when moving between ISPs or mobile hot spots.
DNS message format.
The DNS protocol uses two types of DNS messages, queries and replies, and they both have the same format. Each message consists of a header and four sections: question, answer, authority, and an additional space. A header field ("flags") controls the content of these four sections.
The header section contains the following fields: "Identification", "Flags", "Number of questions", "Number of answers", "Number of authority resource records" (RRs), and "Number of additional RRs". The identification field can be used to match responses with queries. The flag field consists of several sub-fields. The first is a single bit which indicates if the message is a query (0) or a reply (1). The second sub-field consists of four bits; if the value is 1, the present packet is a reply; if it is 2, the present packet is a status; if the value is 0, the present packet is a request. A single-bit sub-field indicates if the DNS server is authoritative for the queried hostname. Another single-bit sub-field indicates if the client wants to send a recursive query ("RD"). The next single-bit sub-field indicates if the replying DNS server supports recursion ("RA"), since not all DNS servers are configured to do this task. Another sub-field indicates if the request was truncated for some reason ("TC"), and a four-bit sub-field indicates status. The "question" section contains the domain name and type of record (A, AAAA, MX, TXT, etc.) being resolved. The domain name is broken into discrete labels which are concatenated; each label is prefixed by the length of that label. The "answer" section has the resource records of the queried name. A domain name may occur in multiple records if it has multiple IP addresses associated.
Protocol transport.
DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests. DNS queries consist of a single UDP request from the client followed by a single UDP reply from the server. The Transmission Control Protocol (TCP) is used when the response data size exceeds 512 bytes, or for tasks such as zone transfers. Some resolver implementations use TCP for all queries.
DNS resource records.
The Domain Name System specifies a set of various types of resource records (RRs), which are the basic information elements of the domain name system. Each record has a type (name and number), an expiration time (time to live), a class, and type-specific data. Resource records of the same type are described as a "resource record set" (RRset). The order of resource records in a set, which is returned by a resolver to an application, is undefined, but often servers implement round-robin ordering to achieve load balancing. The Domain Name System Security Extensions (DNSSEC), however, work on the complete set of resource record in canonical order.
When sent over an Internet Protocol network, all records use the common format specified in RFC 1035:
"NAME" is the fully qualified domain name of the node in the tree. On the wire, the name may be shortened using label compression where ends of domain names mentioned earlier in the packet can be substituted for the end of the current domain name. A free standing "@" is used to denote the current origin.
"TYPE" is the record type. It indicates the format of the data and it gives a hint of its intended use. For example, the "A" record is used to translate from a domain name to an IPv4 address, the "NS" record lists which name servers can answer lookups on a DNS zone, and the "MX" record specifies the mail server used to handle mail for a domain specified in an e-mail address.
"RDATA" is data of type-specific relevance, such as the IP address for address records, or the priority and hostname for MX records. Well known record types may use label compression in the RDATA field, but "unknown" record types must not (RFC 3597).
The "CLASS" of a record is set to IN (for "Internet") for common DNS records involving Internet hostnames, servers, or IP addresses. In addition, the classes Chaos (CH) and Hesiod (HS) exist. Each class is an independent name space with potentially different delegations of DNS zones.
In addition to resource records defined in a zone file, the domain name system also defines several request types that are used only in communication with other DNS nodes ("on the wire"), such as when performing zone transfers (AXFR/IXFR) or for EDNS (OPT).
Wildcard DNS records.
The domain name system supports wildcard DNS records which specify names that start with the "asterisk label", '*', e.g., *.example. DNS records belonging to wildcard domain names specify rules for generating resource records within a single DNS zone by substituting whole labels with matching components of the query name, including any specified descendants.
For example, in the DNS zone "x.example", the following configuration specifies that all subdomains, including subdomains of subdomains, of "x.example" use the mail exchanger "a.x.example". The records for "a.x.example" are needed to specify the mail exchanger. As this has the result of excluding this domain name and its subdomains from the wildcard matches, all subdomains of "a.x.example" must be defined in a separate wildcard statement.
The role of wildcard records was refined in RFC 4592, because the original definition in RFC 1034 was incomplete and resulted in misinterpretations by implementers.
Protocol extensions.
The original DNS protocol had limited provisions for extension with new features. In 1999, Paul Vixie published in RFC 2671 an extension mechanism, called Extension mechanisms for DNS (EDNS) that introduced optional protocol elements without increasing overhead when not in use. This was accomplished through the OPT pseudo-resource record that only exists in wire transmissions of the protocol, but not in any zone files. Initial extensions were also suggested (EDNS0), such as increasing the DNS message size in UDP datagrams.
Dynamic zone updates.
Dynamic DNS updates use the UPDATE DNS opcode to add or remove resource records dynamically from a zone database maintained on an authoritative DNS server. The feature is described in RFC 2136. This facility is useful to register network clients into the DNS when they boot or become otherwise available on the network. Since a booting client may be assigned a different IP address each time from a DHCP server, it is not possible to provide static DNS assignments for such clients.
Security issues.
Originally, security concerns were not major design considerations for DNS software or any software for deployment on the early Internet, as the network was not open for participation by the general public. However, the expansion of the Internet into the commercial sector in the 1990s changed the requirements for security measures to protect data integrity and user authentication.
Several vulnerability issues were discovered and exploited by malicious users. One such issue is DNS cache poisoning, in which data is distributed to caching resolvers under the pretense of being an authoritative origin server, thereby polluting the data store with potentially false information and long expiration times (time-to-live). Subsequently, legitimate application requests may be redirected to network hosts operated with malicious intent.
DNS responses are traditionally not cryptographically signed, leading to many attack possibilities; the Domain Name System Security Extensions (DNSSEC) modify DNS to add support for cryptographically signed responses. DNSCurve has been proposed as an alternative to DNSSEC. Other extensions, such as TSIG, add support for cryptographic authentication between trusted peers and are commonly used to authorize zone transfer or dynamic update operations.
Some domain names may be used to achieve spoofing effects. For example, and paypa1.com are different names, yet users may be unable to distinguish them in a graphical user interface depending on the user's chosen typeface. In many fonts the letter "l" and the numeral "1" look very similar or even identical. This problem is acute in systems that support internationalized domain names, since many character codes in ISO 10646, may appear identical on typical computer screens. This vulnerability is occasionally exploited in phishing.
Techniques such as forward-confirmed reverse DNS can also be used to help validate DNS results.
Domain name registration.
The right to use a domain name is delegated by domain name registrars which are accredited by the Internet Corporation for Assigned Names and Numbers (ICANN) or other organizations such as OpenNIC, that are charged with overseeing the name and number systems of the Internet. In addition to ICANN, each top-level domain (TLD) is maintained and serviced technically by an administrative organization, operating a registry. A "registry" is responsible for operating the database of names within its authoritative zone, although the term is most often used for TLDs. A "registrant" is a person or organization who asked for domain registration. The registry receives registration information from each domain name "registrar", which is authorized (accredited) to assign names in the corresponding TLD and publishes the information using WHOIS protocol. As of 2015, usage of RDAP is being considered.
ICANN publishes the complete list of TLDs, TLD registries, and domain name registrars. Registrant information associated with domain names is maintained in an online database accessible with the WHOIS service. For most of the more than 290 country code top-level domains (ccTLDs), the domain registries maintain the WHOIS (Registrant, name servers, expiration dates, etc.) information. For instance, DENIC, Germany NIC, holds the DE domain data. Since about 2001, most Generic top-level domain (gTLD) registries have adopted this so-called "thick" registry approach, i.e. keeping the WHOIS data in central registries instead of registrar databases.
For COM and NET domain names, a "thin" registry model is used. The domain registry (e.g., VeriSign) holds basic WHOIS data (i.e., registrar and name servers, etc.) One can find the detailed WHOIS (registrant, name servers, expiry dates, etc.) at the registrars.
Some domain name registries, often called "network information centers" (NIC), also function as registrars to end-users. The major generic top-level domain registries, such as for the domains COM, NET, ORG, INFO, use a registry-registrar model consisting of many domain name registrars. In this method of management, the registry only manages the domain name database and the relationship with the registrars. The "registrants" (users of a domain name) are customers of the registrar, in some cases through additional layers of resellers.
RFC documents.
Standards.
The Domain Name System is defined by Request for Comments (RFC) documents published by the Internet Engineering Task Force (Internet standards). The following is a list of RFCs that define the DNS protocol.
Informational.
These RFCs are advisory in nature, but may provide useful information despite defining neither a standard or BCP. (RFC 1796)
Unknown.
These RFCs have an official status of Unknown, but due to their age are not clearly labeled as such.

</doc>
<doc id="8340" url="https://en.wikipedia.org/wiki?curid=8340" title="David Letterman">
David Letterman

David Michael Letterman (born April 12, 1947) is an American former television and radio host, comedian, writer, producer, and actor.
He hosted a late night television talk show for 33 years, beginning with the February 1, 1982, debut of "Late Night with David Letterman" on NBC, and ending with the May 20, 2015, broadcast of the "Late Show with David Letterman" on CBS. In total, Letterman hosted 6,028 episodes of "Late Night" and "Late Show", surpassing friend and mentor Johnny Carson as the longest-serving late night talk show host in American television history. In 1996, Letterman was ranked 45 on "TV Guide"s 50 Greatest TV Stars of All Time.
Letterman is also a television and film producer. His company, Worldwide Pants, produced his show and formerly produced "The Late Late Show with Craig Ferguson". Worldwide Pants has also produced several prime-time comedies, the most successful of which was "Everybody Loves Raymond", currently in syndication.
Late-night hosts Conan O'Brien (Letterman's successor on "Late Night") and Jimmy Kimmel cite Letterman's influence.
Early life and career.
Letterman was born in Indianapolis, Indiana. His father, Harry Joseph Letterman (April 15, 1915 – February 13, 1973), was a florist. His mother, Dorothy Marie (Hofert), a church secretary, was an occasional figure on Letterman's show, usually at holidays and birthdays. His mother is of German descent, and his father had English, Scots-Irish, and German ancestry.
He lived on the north side of Indianapolis (Broad Ripple area), not far from Speedway, Indiana, and the Indianapolis Motor Speedway; and he enjoyed collecting model cars, including racers. In 2000, he told an interviewer for "Esquire" that, while growing up, he admired his father's ability to tell jokes and be the life of the party. Harry Joseph Letterman survived a heart attack at age 36, when David was a young boy. The fear of losing his father was constantly with Letterman as he grew up. The elder Letterman died of a second heart attack at age 57.
Letterman attended his hometown's Broad Ripple High School at the same time as Marilyn Tucker (future wife of Dan Quayle) and worked as a stock boy at the local Atlas Supermarket. According to the "Ball State Daily News", he originally had wanted to attend Indiana University, but his grades were not good enough, so he instead attended Ball State University, in Muncie, Indiana. He is a member of the Sigma Chi fraternity, and he graduated in 1969 from what was then the Department of Radio and Television. A self-described average student, Letterman later endowed a scholarship for what he called "C students" at Ball State.
Though he registered for the draft and passed his physical after graduating from college, he was not drafted for service in Vietnam because of receiving a draft lottery number of 346 (out of 366).
Letterman began his broadcasting career as an announcer and newscaster at the college's student-run radio station—WBST—a 10-watt campus station which now is part of Indiana Public Radio. He was fired for treating classical music with irreverence. He then became involved with the founding of another campus station—WAGO-AM 570 (now WWHI, 91.3).
He credits Paul Dixon, host of the "Paul Dixon Show", a Cincinnati-based talk show also shown in Indianapolis while he was growing up, for inspiring his choice of career:
I was just out of college 1969, and I really didn't know what I wanted to do. And then all of a sudden I saw him doing it TV. And I thought: That's really what I want to do!
Weatherman.
Letterman began his career as a radio talk show host on WNTS (AM) and on Indianapolis television station WLWI (which changed its call sign to WTHR in 1976) as an anchor, and weatherman. He received some attention for his unpredictable on-air behavior, which included congratulating a tropical storm for being upgraded to a hurricane and predicting hail stones "the size of canned hams." He would also occasionally report the weather and the day's very high and low temps for fictitious cities ("Eight inches of snow in Bingree and surrounding areas") while on another occasion saying that a state border had been erased when a satellite map accidentally omitted the state border between Indiana and Ohio, attributing it to dirty political dealings. ("The higher-ups have removed the border between Indiana and Ohio making it one giant state. Personally, I'm against it. I don't know what to do about it.") He also starred in a local kiddie show, made wisecracks as host of a late night TV show called "Freeze-Dried Movies" (he once acted out a scene from "Godzilla" using plastic dinosaurs), and hosted a talk show that aired early on Saturday mornings called "Clover Power", in which he interviewed 4-H members about their projects.
In 1971, Letterman appeared as a pit road reporter for ABC Sports' tape-delayed coverage of the Indianapolis 500. Letterman was initially introduced as Chris Economaki, although this was corrected at the end of the interview. Letterman interviewed Mario Andretti, who had just crashed out of the race.
Move to Los Angeles.
In 1975, encouraged by his then-wife Michelle and several of his Sigma Chi fraternity brothers, Letterman moved to Los Angeles, California, with hope of becoming a comedy writer. He and Michelle packed their belongings in his pickup truck and headed west. As of 2012, he still owned the truck. In Los Angeles, he began performing comedy at The Comedy Store. Jimmie Walker saw him on stage; with an endorsement from George Miller, Letterman joined a group of comedians whom Walker hired to write jokes for his stand-up act, a group that at various times would also include Jay Leno, Paul Mooney, Robert Schimmel, Richard Jeni, Louie Anderson, Elayne Boosler, Byron Allen, Jack Handey, and Steve Oedekerk.
By the summer of 1977, Letterman was a writer and regular on the six-week summer series "The Starland Vocal Band Show", broadcast on CBS. He hosted a 1977 pilot for a game show entitled "The Riddlers" (that was never picked up), and co-starred in the Barry Levinson-produced comedy special "Peeping Times" that aired in January 1978. Later that year, Letterman was a cast member on Mary Tyler Moore's variety show, "Mary". Letterman made a guest appearance on "Mork & Mindy" (as a parody of EST leader Werner Erhard) and appearances on game shows such as "The $20,000 Pyramid", "The Gong Show", "Hollywood Squares", "Password Plus" and "Liar's Club", as well as talk shows such as "The Mike Douglas Show" (3 April 1979 and 7 February 1980). He was also screen tested for the lead role in the 1980 film "Airplane!", a role that eventually went to Robert Hays.
His dry, sarcastic humor caught the attention of scouts for "The Tonight Show Starring Johnny Carson", and Letterman was soon a regular guest on the show. Letterman became a favorite of Carson and was a regular guest host for the show beginning in 1978. Letterman credits Carson as the person who influenced his career the most.
NBC.
Morning show.
On June 23, 1980, Letterman was given his own morning comedy show on NBC, "The David Letterman Show". It was originally 90 minutes long, but was shortened to 60 minutes in August 1980. The show was a critical success, winning two Emmy Awards, but was a ratings disappointment and was canceled in October 1980.
"Late Night with David Letterman".
NBC kept Letterman under contract to try him in a different time slot. "Late Night with David Letterman" debuted February 1, 1982; the first guest on the first show was Bill Murray. Murray later went on to become one of Letterman's most recurrent guests, guesting on the show's 30th anniversary episode, which aired January 31, 2012 and on the very last show, which aired May 20, 2015. The show ran Monday through Thursday at 12:30 a.m. Eastern Time, immediately following "The Tonight Show Starring Johnny Carson" (a Friday night broadcast was added in June 1987). It was seen as being edgy and unpredictable, and soon developed a cult following (particularly among college students). Letterman's reputation as an acerbic interviewer was borne out in verbal sparring matches with Cher (who even called him an asshole on the show), Shirley MacLaine, Charles Grodin, and Madonna. The show also featured comedy segments and running characters, in a style heavily influenced by the 1950s and 1960s programs of Steve Allen. Although Ernie Kovacs is often cited as an influence on the show, Letterman has denied this.
The show often featured quirky, genre-mocking regular features, including "Stupid Pet Tricks" (which had its origins on Letterman's morning show), Stupid Human Tricks, dropping various objects off the roof of a five-story building, demonstrations of unorthodox clothing (such as suits made of Alka-Seltzer, Velcro and suet), a recurring Top 10 list, the Monkey-Cam (and the Audience Cam), a facetious letter-answering segment, several "Film by My Dog Bob" in which a camera was mounted on Letterman's own dog (often with comic results) and Small Town News, all of which would eventually move with Letterman to CBS.
Other memorable moments included Letterman using a bullhorn to interrupt a live interview on "The Today Show", announcing that he was the NBC News president while not wearing any pants; and staging "elevator races", complete with commentary by NBC Sports' Bob Costas. In one infamous appearance, in 1982, Andy Kaufman (who was already wearing a neck brace) appeared; interrupting Al Roker on WNBC-TV's broadcast of "Live at Five" by walking into their studio (which occupied the same floor of 30 Rockefeller Plaza as Letterman's studio) to be slapped and knocked to the ground by professional wrestler Jerry Lawler (though Lawler and Kaufman's friend Bob Zmuda later revealed that the event was staged). In another memorable exchange, sex expert Dr. Ruth Westheimer included cucumbers in a list of handy sex objects that women could find at home. The following night, guest Ted Koppel asked Letterman "May I insert something here?" and Letterman responded "OK, as long as it's not a cucumber."
Coach Toast Pilot (1984)
David Letterman appeared in the pilot episode of the short run series "Coach Toast." Letterman starred as "Mr. Melvins," a grouchy driver's education teacher who was the antagonist to Coach Toast.
CBS.
"Late Show with David Letterman".
In 1992, Johnny Carson retired, and many fans believed that Letterman would become host of "The Tonight Show". When NBC instead gave the job to Jay Leno, Letterman departed NBC to host his own late-night show on CBS, opposite "The Tonight Show" at 11:30 p.m., called the "Late Show with David Letterman". The new show debuted on August 30, 1993, and was taped at the historic Ed Sullivan Theater, where Ed Sullivan broadcast his eponymous variety series from 1948 to 1971. For Letterman's arrival, CBS spent  million in renovations. In addition to that cost, CBS also signed Letterman to a lucrative three-year,  million/year contract, doubling his "Late Night" salary. The total cost for everything (renovations, negotiation right paid to NBC, signing Letterman, announcer Bill Wendell, Shaffer, the writers and the band) was over  million.
But while the expectation was that Letterman would retain his unique style and sense of humor with the move, "Late Show" was not an exact replica of his old NBC program. Recognizing the more formal mood (and wider audience) of his new time slot and studio, Letterman eschewed his trademark blazer with khaki pants and white wrestling shoes wardrobe combination in favor of expensive shoes, tailored suits and light-colored socks. The monologue was lengthened. Paul Shaffer and the "World's Most Dangerous Band" followed Letterman to CBS, but they added a brass section and were rebranded the "CBS Orchestra" as a short monologue and a small band were mandated by Carson while Letterman occupied the 12:30 slot. Additionally, because of intellectual property disagreements, Letterman was unable to import many of his "Late Night" segments verbatim, but he sidestepped this problem by simply renaming them (the "Top Ten List" became the "Late Show Top Ten", "Viewer Mail" became the "CBS Mailbag", etc.)
Popularity.
The main competitor of the "Late Show" is NBC's "The Tonight Show", which was hosted by Jay Leno for 22 years, but from June 1, 2009, to January 22, 2010, was hosted by Conan O'Brien. In 1993 and 1994, the "Late Show" consistently gained higher ratings than "The Tonight Show". But in 1995, ratings dipped and Leno's show consistently beat Letterman's in the ratings from the time that Hugh Grant came on Leno's show after Grant's arrest for soliciting a prostitute; Leno typically attracted about five million nightly viewers between 1999 and 2009. The "Late Show" lost nearly half its audience during its competition with Leno, attracting 7.1 million viewers nightly in its 1993–94 season and about 3.8 million per night as of Leno's departure in 2009. In the final months of his first stint as host of "The Tonight Show", Leno beat Letterman in the ratings by a 1.3 million viewer margin (5.2 million to 3.9 million), and "Nightline" and the "Late Show" were virtually tied. Once O'Brien took over "Tonight", however, Letterman closed the gap in the ratings. O'Brien initially drove the median age of "Tonight Show" viewers from 55 to 45, with most older viewers opting to watch the "Late Show" instead.
Following Leno's return to "The Tonight Show", however, Leno regained his lead.
Letterman's shows have garnered both critical and industry praise, receiving 67 Emmy Award nominations, winning 12 times in his first 20 years in late night television. From 1993 to 2009, Letterman ranked higher than Leno in the annual Harris Poll of "Nation's Favorite TV Personality" 12 times. For example, in 2003 and 2004 Letterman ranked second in that poll, behind only Oprah Winfrey, a year that Leno was ranked fifth. Leno was higher than Letterman on that poll three times during the same period, in 1998, 2007, and 2008.
Hosting the Academy Awards.
On March 27, 1995, Letterman acted as the host for the 67th Academy Awards ceremony. Critics blasted Letterman for what they deemed a poor hosting of the Oscars, noting that his irreverent style undermined the traditional importance and glamor of the event. In a joke about their unusual names (inspired by a celebrated comic essay in "The New Yorker", "Yma Dream" by Thomas Meehan), he started off by introducing Uma Thurman to Oprah Winfrey, and then both of them to Keanu Reeves: "Oprah...Uma. Uma...Oprah," "Have you kids met Keanu?" This and many of his other jokes fell flat. Although Letterman attracted the highest ratings to the annual telecast since 1983, many felt that the bad publicity garnered by Letterman's hosting caused a decline in the "Late Show"'s ratings.
Letterman recycled the apparent debacle into a long-running gag. On his first show after the Oscars, he joked, "Looking back, I had no idea that thing was being televised." He lampooned his stint two years later, during Billy Crystal's opening Oscar skit, which also parodied the plane-crashing scenes from that year's chief nominated film, "The English Patient".
For years afterward, Letterman recounted his hosting the Oscars, although the Academy of Motion Picture Arts and Sciences continued to hold Letterman in high regard and they had invited him to host the Oscars again. On September 7, 2010, he made an appearance on the premiere of the 14th season of "The View", and confirmed that he had been considered for hosting again.
Heart surgery hiatus.
On January 14, 2000, a routine check-up revealed that an artery in Letterman's heart was severely obstructed. He was rushed to emergency surgery for a quintuple bypass.
During the initial weeks of his recovery, reruns of the "Late Show" were shown and introduced by friends of Letterman including Drew Barrymore, Ray Romano, Robin Williams, Bonnie Hunt, Megan Mullally, Bill Murray, Regis Philbin, Charles Grodin, Nathan Lane, Julia Roberts, Bruce Willis, Jerry Seinfeld, Martin Short, Steven Seagal, Hillary Rodham Clinton, Danny DeVito, Steve Martin, and Sarah Jessica Parker.
Subsequently, while still recovering from surgery, Letterman revived the late night tradition that had virtually disappeared on network television during the 1990s of 'guest hosts' by allowing Bill Cosby, Kathie Lee Gifford, Dana Carvey, Janeane Garofalo, and others to host new episodes of the "Late Show".
Upon his return to the show on February 21, 2000, Letterman brought all but one of the doctors and nurses on stage who had participated in his surgery and recovery (with extra teasing of a nurse who had given him bed baths—"This woman has seen me naked!"), including Dr. O. Wayne Isom and physician Louis Aronne, who frequently appears on the show. In a show of emotion, Letterman was nearly in tears as he thanked the health care team with the words "These are the people who saved my life!" The episode earned an Emmy nomination. For a number of episodes, Letterman continued to crack jokes about his bypass, including saying, "Bypass surgery: it's when doctors surgically create new blood flow to your heart. A bypass is what happened to me when I didn't get "The Tonight Show!" It's a whole different thing." In a later running gag he lobbied his home state of Indiana to rename the freeway circling Indianapolis (I-465) "The David Letterman Bypass." He also featured a montage of faux news coverage of his bypass surgery, which included a clip of Letterman's heart for sale on the Home Shopping Network. Letterman became friends with his doctors and nurses. In 2008, a "Rolling Stone" interview stated he hosted a doctor and nurse who'd helped perform the emergency quintuple-bypass heart surgery that saved his life in 2000. 'These are people who were complete strangers when they opened my chest,' he says. 'And now, eight years later, they're among my best friends.'
Additionally, Letterman invited the band Foo Fighters to play "Everlong", introducing them as "my favorite band, playing my favorite song." During a later Foo Fighters appearance, Letterman said that Foo Fighters had been in the middle of a South American tour which they canceled to come play on his comeback episode.
Letterman again handed over the reins of the show to several guest hosts (including Bill Cosby, Brad Garrett, Elvis Costello, John McEnroe, Vince Vaughn, Will Ferrell, Bonnie Hunt, Luke Wilson and bandleader Paul Shaffer) in February 2003, when he was diagnosed with a severe case of shingles. Later that year, Letterman made regular use of guest hosts—including Tom Arnold and Kelsey Grammer—for new shows broadcast on Fridays. In March 2007, Adam Sandler—who had been scheduled to be the lead guest—served as a guest host while Letterman was ill with a stomach virus.
Re-signing with CBS.
In March 2002, as Letterman's contract with CBS neared expiration, ABC offered him the time slot for long-running news program "Nightline" with Ted Koppel. Letterman was interested as he believed he could never match Leno's ratings at CBS due to Letterman's complaint of weaker lead-ins from the network's late local news programs, but was reluctant to replace Koppel. Letterman addressed his decision to re-sign on the air, stating that he was content at CBS and that he had great respect for Koppel.
On December 4, 2006, CBS revealed that Letterman signed a new contract to host "Late Show with David Letterman" through the fall of 2010. "I'm thrilled to be continuing on at CBS," said Letterman. "At my age you really don't want to have to learn a new commute." Letterman further joked about the subject by pulling up his right pants leg, revealing a tattoo, presumably temporary, of the ABC logo.
"Thirteen years ago, David Letterman put CBS late night on the map and in the process became one of the defining icons of our network," said Leslie Moonves, president and CEO of CBS Corporation. His presence on our air is an ongoing source of pride, and the creativity and imagination that the "Late Show" puts forth every night is an ongoing display of the highest quality entertainment. We are truly honored that one of the most revered and talented entertainers of our time will continue to call CBS 'home.'
According to a 2007 article in "Forbes" magazine, Letterman earned  million a year. A 2009 article in "The New York Times", however, said his salary was estimated at  million per year. In June 2009, Letterman's Worldwide Pants and CBS reached agreement to continue the "Late Show" until at least August 2012. The previous contract had been set to expire in 2010, and the two-year extension is shorter than the typical three-year contract period negotiated in the past. Worldwide Pants agreed to lower its fee for the show, though it had remained a "solid moneymaker for CBS" under the previous contract.
On the February 3, 2011, edition of the "Late Show", during an interview with Howard Stern, Letterman said he would continue to do his talk show for "maybe two years, I think."
In April 2012, CBS announced it had extended its contract with Letterman through 2014. His contract was subsequently extended to 2015.
Retirement from "Late Show".
During the taping of his April 3, 2014, show, Letterman announced that he had informed CBS president Leslie Moonves that he would retire from hosting "Late Show" by May 20, 2015. It was announced soon after that comedian and political satirist Stephen Colbert would succeed Letterman. Letterman's last episode aired on May 20, 2015, and opened with a presidential send off featuring the four of the five living presidents, George H.W. Bush, Bill Clinton, George W. Bush and Barack Obama, each mimicking the late president Gerald Ford's statement that "Our long national nightmare is over." It also featured cameos from "The Simpsons" and "Wheel of Fortune" (the latter with a puzzle saying "Good riddance to David Letterman"), a Top Ten List of "things I wish I could have said to David Letterman" performed by regular guests including Alec Baldwin, Barbara Walters, Steve Martin, Jerry Seinfeld, Jim Carrey, Chris Rock, Julia Louis-Dreyfus, Peyton Manning, Tina Fey, and Bill Murray, and closed with a montage of scenes from both his CBS and NBC series set to a live performance of "Everlong" by Foo Fighters.
The final episode of "Late Show with David Letterman" was watched by 13.76 million viewers in the United States with an audience share of 9.3/24, earning the show its highest ratings since following the 1994 Olympics on February 25, 1994, and the show's highest demo numbers (4.1 in adults 25-54 and 3.1 in adults 18-49) since Oprah Winfrey's first "Late Show" appearance following the ending of her feud with Letterman on December 1, 2005. In a rarity for a late-night show, it was also the highest-rated program on network television that night, beating out all prime-time shows. In total, Letterman hosted 6,028 episodes of "Late Night" and "Late Show", surpassing friend and mentor Johnny Carson as the longest-serving late night talk show host in American television history.
Post-"Late Show".
In the months following the end of "Late Show" Letterman has been seen occasionally at sports events such as the Indianapolis 500, during which he submitted to an interview with a local publication. He made a surprise appearance on stage in San Antonio, Texas when he was invited up for an extended segment during Steve Martin and Martin Short's "A Very Stupid Conversation" show saying "I retired, and...I have no regrets," Letterman told the crowd after walking on stage. "I was happy. I'll make actual friends. I was complacent. I was satisfied. I was content, and then a couple of days ago Donald Trump said he was running for president. I have made the biggest mistake of my life, ladies and gentlemen" and then delivering a Top Ten List roasting Donald Trump's presidential campaign followed by an on-stage conversation with Martin and Short. Cell phone recordings of the appearance were posted on YouTube by audience members and were widely reported in the media.
Notable exchanges and incidents.
NBC and Johnny Carson.
In spite of Johnny Carson's clear intention to pass his title to Letterman, NBC selected Jay Leno to host "The Tonight Show" after Carson's departure. Letterman maintained a close relationship with Carson through his break with NBC. Three years after he left for CBS, HBO produced a made-for-television movie called "The Late Shift", based on a book by "The New York Times" reporter Bill Carter, chronicling the battle between Letterman and Leno for the coveted "Tonight Show" hosting spot.
Carson later made a few cameo appearances as a guest on Letterman's show. Carson's final television appearance came May 13, 1994, on a "Late Show" episode taped in Los Angeles, when he made a surprise appearance during a 'Top 10 list' segment. In early 2005, it was revealed that Carson occasionally sent jokes to Letterman, who used these jokes in his monologue; according to CBS senior vice president Peter Lassally (a one-time producer for both men), Carson got "a big kick out of it." Letterman would do a characteristic Johnny Carson golf swing after delivering one of Carson's jokes. In a tribute to Carson, all of the opening monologue jokes during the first show following Carson's death were written by Carson.
Lassally also claimed that Carson had always believed Letterman, not Leno, to be his "rightful successor." During the early years of the "Late Show"s run, Letterman occasionally used some of Carson's trademark bits, including "Carnac the Magnificent" (with Paul Shaffer as Carnac), "Stump the Band", and the "Week in Review."
Oprah Winfrey.
Oprah Winfrey appeared on Letterman's show when he was hosting NBC's "Late Night" on May 2, 1989. Following that appearance, the two had a 16-year feud which according to Letterman started when he and his girlfriend decided to skip out on a bill, tricking the waiter into thinking Oprah agreed to pay it.
The feud apparently ended in 2005 when Winfrey appeared on CBS's "Late Show with David Letterman" on December 2, in an event Letterman jokingly referred to as "the Super Bowl of Love".
Winfrey and Letterman also appeared together in a "Late Show" promo that aired during CBS's coverage of Super Bowl XLI in February 2007, with the two sitting next to each other on the couch watching the game. Since the game was played between the Indianapolis Colts and Chicago Bears, the Indianapolis-born Letterman wears a Peyton Manning jersey, while Winfrey—who tapes her show in Chicago—is in a Brian Urlacher jersey. On September 10, 2007, Letterman made his first appearance on "The Oprah Winfrey Show" at Madison Square Garden in New York City.
Three years later, during CBS's coverage of Super Bowl XLIV, the two appeared again in a "Late Show" promo, this time with Winfrey sitting on a couch between Letterman and Jay Leno. This time Letterman was wearing the retired 70 jersey of Colts' Hall of Fame and Letterman regular guest, Art Donovan. The appearance was Letterman's idea: Leno flew to New York City on an NBC corporate jet, sneaking into the Ed Sullivan Theater during the "Late Show"'s February 4 taping wearing a disguise, meeting Winfrey and Letterman at a living room set created in the theater's balcony where they taped their promo.
Winfrey interviewed Letterman in January 2013 on "Oprah's Next Chapter". Winfrey and Letterman discussed their feud during the interview and Winfrey revealed that she had had a "terrible experience" while appearing on Letterman's show years earlier. Letterman could not recall the incident but apologized.
2007–08 writers' strike.
"Late Show" went off air for eight weeks during the months of November and December because of the Writers Guild of America strike. Letterman's production company, Worldwide Pants, was the first company to make an individual agreement with the WGA, thus allowing his show to come back on air on January 2, 2008. On his first episode since being off air, he surprised the viewing audience with his newly grown beard, which signified solidarity with the strike. His beard was shaved off during the show on January 7, 2008.
Palin joke.
On June 8 and June 9, 2009, Letterman told a sexually themed joke on his show each night about a daughter of Sarah Palin. Palin was in New York City at the time with her fourteen-year-old daughter, Willow, and the jokes were said to be aimed at the daughter, never named, who was visiting New York City with her mother. Palin criticized the jokes, saying in a statement posted on the internet that "I doubt he'd ever dare make such comments about anyone else's daughter," and "laughter incited by sexually perverted comments made by a 62-year-old male celebrity aimed at a 14-year-old girl" is "disgusting." On June 10, Letterman responded to the controversy on his show by stating that the jokes were meant to be about Palin's eighteen-year-old daughter, Bristol, whose pregnancy as an unmarried teenager had caused controversy during the 2008 Presidential election, and that "(t)hese are not jokes made about (Palin's) 14-year-old daughter. I would never, never make jokes about raping or having sex of any description with a 14-year-old girl." His remarks did not put an end to the public criticism, however, with the National Organization for Women, who supported Palin in a statement, noting he had given only "something of an apology." With the controversy not subsiding, Letterman addressed the issue again on his June 15 show, faulting himself for the error and apologizing "especially to the two daughters involved, Bristol and Willow, and also to the governor and her family and everybody else who was outraged by the joke."
Al-Qaeda death threat.
On August 17, 2011, it was reported that an Islamist militant had posted a death threat against Letterman on a website frequented by Al-Qaeda supporters, calling on American Muslims to kill Letterman for making a joke about the death of an Al-Qaeda leader killed in a drone strike in Pakistan in June 2011, Ilyas Kashmiri. In his show on August 22, Letterman joked about the threat, saying "State Department authorities are looking into this. They're not taking this lightly. They're looking into it. They're questioning, they're interrogating, there's an electronic trail—but everybody knows it's Leno."
Appearances in other media.
Letterman appeared in issue 239 of the Marvel comic book "The Avengers", in which the title characters are guests on "Late Night". A parody of Letterman, named "David Endochrine", is gassed to death along with his bandleader named "Paul" and their audience in Frank Miller's "The Dark Knight Returns".
Letterman appeared in the pilot episode of the short-lived 1986 series "Coach Toast", and he appears with a bag over his head as a guest on Bonnie Hunt's ca. 1993 sitcom "The Building". He also appears in "The Simpsons" as himself in a couch gag when the Simpsons find themselves (and the couch) in "Late Night with David Letterman". He had a cameo in the feature film "Cabin Boy", with Chris Elliott, who worked as a writer on Letterman's show. In this and other appearances, Letterman is listed in the credits as "Earl Hofert", the name of Letterman's maternal grandfather. He also appeared as himself in the Howard Stern biographical film "Private Parts" as well as the 1999 Andy Kaufman biopic "Man on the Moon", in a few episodes of Garry Shandling's 1990s TV series "The Larry Sanders Show" and in "The Abstinence", a 1996 episode of the sitcom "Seinfeld".
Letterman provided vocals for the Warren Zevon song "Hit Somebody" from "My Ride's Here", and provided the voice for Butt-head's father in the 1996 animated film "Beavis and Butt-Head Do America".
In 2010, a documentary "Dying to do Letterman" was released directed by Joke Fincioen and Biagio Messina featuring Steve Mazan, a stand up comic, who has cancer and wants to appear on the Letterman show. The film won best documentary and jury awards at the Cinequest Film Festival. Steve Mazan published a same-titled book (full title, "Dying to Do Letterman: Turning Someday into Today") about his own saga.
Known for rarely giving interviews, Letterman appeared as a guest on CNN's "Piers Morgan Tonight" on May 29, 2012, when he was interviewed by Regis Philbin, the guest host and long-time friend. Philbin again interviewed Letterman (and Shaffer) while guest-hosting CBS' "The Late Late Show" (between the tenures of Craig Ferguson and James Corden) on January 27, 2015.
In June 2013, he appeared in the second episode of season two of "Comedians in Cars Getting Coffee".
On November 5, 2013, Letterman and Bruce McCall published a fiction satire book titled "This Land Was Made for You and Me (But Mostly Me)". ISBN 0-399-16368-9
Other projects.
Worldwide Pants.
Letterman started his production company—Worldwide Pants Incorporated—which produced his show and several others, including "Everybody Loves Raymond"; "The Late Late Show" and two television series for Bonnie Hunt. Worldwide Pants also produced the dramedy program "Ed" which aired on NBC from 2000–2004. It was Letterman's first association with NBC since he left the network in 1993. During the run of "Ed," the star, Tom Cavanagh, appeared as a guest on the "Late Show" several times.
In 2005, Worldwide Pants produced its first feature film, "Strangers with Candy", which was a prequel to the Comedy Central TV series of the same title. In 2007, Worldwide Pants produced the ABC comedy series, "Knights of Prosperity."
Worldwide Pants made significant news in December 2007 when it was announced that Letterman's company had independently negotiated its own contract with the Writers Guild of America, East, thus allowing Letterman, Craig Ferguson, and their writers to return to work, while the union continued its strike against production companies, networks and studios who had not reached an agreement.
Record company.
In late April 2010, several music industry websites reported that Letterman started a record label named Clear Entertainment/C.E. Music and signed his first artist, Runner Runner. Lucy Walsh announced on her MySpace page that she has been signed by Letterman and Clear Entertainment/C.E. Music and is working on her album.
Rahal Letterman Lanigan Racing.
Rahal Letterman Lanigan Racing (RLLR) is an auto racing team that currently races in the United SportsCar Championship (formerly the American Le Mans Series), and full-time in the Verizon IndyCar Series. It is co-owned by 1986 Indianapolis 500 winner Bobby Rahal, businessman Mike Lanigan, and Letterman himself, and is based in Hilliard, Ohio. The team won the 2004 Indianapolis 500 with driver Buddy Rice.
Charitable foundation.
The Letterman Foundation for Courtesy and Grooming is a private foundation through which Letterman has donated millions of dollars to charities and other non-profits in Indiana and Montana, celebrity-affiliated organizations such as Paul Newman's Hole in the Wall Gang Camp, universities such as Ball State, and other organizations such as the American Cancer Society, Salvation Army, and Doctors Without Borders.
Personal life.
In 2015, Forbes estimated that Letterman's annual income was $35 million.
Marriages, relationships, and family.
On July 2, 1968, Letterman married his college sweetheart Michelle Cook (born July 2, 1946) in Muncie, Indiana; the marriage ended in divorce by October 1977. He also had a long-term relationship and lived with former head writer and producer on "Late Night", Merrill Markoe (born August 13, 1948) from 1978 to 1988. Markoe was the mind behind several "Late Night" staples, such as "Stupid Pet/Human Tricks".
Letterman and Regina Lasko (born November 20, 1960) started dating in February 1986, while he was still living with Markoe. He has a son, Harry Joseph Letterman (born November 3, 2003), with her. Harry is named after Letterman's father. In 2005, police discovered a plot to kidnap Harry Letterman and ransom him for  million. Kelly Frank, a house painter who had worked for Letterman, was charged in the conspiracy.
Letterman and Lasko wed on March 19, 2009, during a quiet courthouse civil ceremony in Choteau, Montana, where he purchased a ranch in 1999. Letterman announced the marriage during the taping of his March 23 show, shortly after congratulating Bruce Willis for getting married the previous week. Letterman told the audience he nearly missed the ceremony because his truck became stuck in mud two miles from their house. The family resides in North Salem, New York, on a 108-acre estate.
Letterman suffers from tinnitus (ringing in the ears), which is a symptom of hearing loss. On the "Late Show" in 1996, Letterman talked about his tinnitus in an interview he did with actor William Shatner, who has severe tinnitus himself, caused from an on-set explosion. Letterman said at first he could not figure out where the noise in his head was coming from and that he hears constant noises and ringing in his ears 24 hours a day.
Letterman no longer drinks alcohol and has on more than one occasion said that he had once been a "horrible alcoholic" and had begun drinking around the age of 13 until the age of 34, in 1981: "I was drunk 80% of the time... I loved it. I was one of those guys, I looked around, and everyone else had stopped drinking and I couldn't understand why." When he is shown on the "Late Show" (or, before that, on "Late Night") drinking what appears to be alcohol, it is actually substituted with apple juice by the crew. In 2015, he said that "For years and years and years – 30, 40 years – I was anxious and hypochondriacal and an alcoholic, and many, many other things that made me different from other people." He became calmer through a combination of transcendental meditation and low doses of medication.
Letterman has implied that he is a Lutheran.
Stalker.
Beginning in May 1988, Letterman was stalked by Margaret Mary Ray, a woman suffering from schizophrenia. She stole his Porsche, camped out on his tennis court, and repeatedly broke into his house. Her exploits drew national attention, with Letterman occasionally joking about her on his show, although he never referred to her by name. After she committed suicide in October 1998, Letterman told "The New York Times" that he had great compassion for her. A spokesperson for Letterman said: "This is a sad ending to a confused life."
Extramarital affairs and blackmail attempt.
On October 1, 2009, Letterman announced on his show that he had been the victim of a blackmail attempt by someone threatening to reveal that he had had sex with several of his female employees, and at the same time, he confirmed that he had had such relationships. He stated that three weeks earlier (on September 9, 2009) someone had left a package in his car with material he said he would write into a screenplay and a book if Letterman did not pay him  million. Letterman said that he contacted the Manhattan District Attorney's office, ultimately cooperating with them to conduct a sting operation involving giving the man a phony check. Subsequently, Robert J. "Joe" Halderman, a producer of the CBS true crime journalism series "48 Hours", was arrested after trying to deposit the check. He was indicted by a Manhattan grand jury and pleaded not guilty to a charge of attempted grand larceny on October 2, 2009. Eventually, on March 9, 2010, he pleaded guilty to this same felony and served a six-month jail sentence, followed by probation and community service.
A central figure in the case and one of the women with whom Letterman had had a sexual relationship was his longtime personal assistant Stephanie Birkitt, who often appeared with him on his show. She had also worked for "48 Hours". Until a month prior to the revelations, she had shared a residence with Halderman, who allegedly had copied her personal diary and used it, along with private emails, in the blackmail package.
In the days following the initial announcement of the affairs and the arrest, several prominent women, including Kathie Lee Gifford, co-host of NBC's "Today Show", and NBC news anchor Ann Curry questioned whether Letterman's affairs with subordinates created an unfair working environment. A spokesman for Worldwide Pants said that the company's sexual harassment policy did not prohibit sexual relationships between managers and employees. According to business news reporter Eve Tahmincioglu, "CBS suppliers are supposed to follow the company's business conduct policies" and the CBS 2008 Business Conduct Statement states that "If a consenting romantic or sexual relationship between a supervisor and a direct or indirect subordinate should develop, CBS requires the supervisor to disclose this information to his or her Company's Human Resources Department...".
On October 3, 2009, a former CBS employee, Holly Hester, announced that she and Letterman had engaged in a year-long "secret" affair in the early 1990s while she was his intern and a student at New York University.
On October 5, 2009, Letterman devoted a segment of his show to a public apology to his wife and staff. Three days later, Worldwide Pants announced that Birkitt had been placed on a "paid leave of absence" from the "Late Show". On October 15, CBS News announced that the company's Chief Investigative Correspondent, Armen Keteyian, had been assigned to conduct an "in-depth investigation" into Letterman.
Awards and honors.
David Letterman Communication and Media Building.
On September 7, 2007, Letterman visited his "alma mater", Ball State University in Muncie, Indiana, for the dedication of a communications facility named in his honor for his dedication to the university. The  million, David Letterman Communication and Media Building opened for the 2007 fall semester. Thousands of Ball State students, faculty, and local residents welcomed Letterman back to Indiana. Letterman's emotional speech touched on his struggles as a college student and his late father, and also included the "top ten good things about having your name on a building", finishing with "if reasonable people can put my name on a  million building, anything is possible." Over many years Letterman "has provided substantial assistance to State's Department of Telecommunications, including an annual scholarship that bears his name."
At the same time, Letterman also received a Sagamore of the Wabash award given by Indiana Governor Mitch Daniels, which recognizes distinguished service to the state of Indiana.
Awards and nominations.
In his capacities as either a writer, producer, performer, or as part of a writing team, Letterman is among the most nominated people in Emmy Award history with 52 nominations, winning two Daytime Emmys and ten Primetime Emmys since 1981. He won four American Comedy Awards and in 2011 became the first recipient of the Johnny Carson Award for Comedic Excellence at The Comedy Awards.
Letterman was a recipient of the 2012 Kennedy Center Honors, where he was called "one of the most influential personalities in the history of television, entertaining an entire generation of late-night viewers with his unconventional wit and charm."

</doc>
<doc id="8341" url="https://en.wikipedia.org/wiki?curid=8341" title="Delroy Lindo">
Delroy Lindo

Delroy George Lindo (born 18 November 1952) is an British-American actor and theatre director. Lindo has been nominated for the Tony and Screen Actors Guild awards and has won a Satellite Award. He is perhaps best known for his roles in a trio of Spike Lee films, especially as West Indian Archie in Lee's "Malcolm X" (1992) and Woody Carmichael in "Crooklyn" (1994), Catlett in "Get Shorty", Arthur Rose in "The Cider House Rules", and Detective Castlebeck in "Gone in 60 Seconds" (2000). Lindo starred as Alderman Ronin Gibbons in the TV series "The Chicago Code" (2011) and as Winter on the series "Believe," which premiered in 2014.
Early life.
Delroy Lindo was born in 1952 in Eltham, south-east London, the son of Jamaican parents who had emigrated to Britain. He was brought up in nearby Lewisham and got interested in acting as a child in a Nativity play. His mother was a nurse and his father worked in various jobs. As a teenager, he and his mother moved to Toronto, Canada. When he was sixteen, they moved to San Francisco. At the age of 24, Lindo started acting studies at the American Conservatory Theater, graduating in 1979.
Career.
Lindo's film debut came in 1976 with the British comedy "Find the Lady", followed by two other roles in films, including an Army Sergeant in "More American Graffiti" (1979). 
He quit film for 10 years to concentrate on theatre acting. In 1982 he debuted on Broadway in ""Master Harold"...and the Boys," directed by the play's South African author Athol Fugard. By 1988 Lindo had earned a Tony nomination for his portrayal of Herald Loomis in August Wilson's "Joe Turner's Come and Gone".
Lindo returned to film in the 1990s, acting alongside Rutger Hauer and Joan Chen in the cult science fiction film "Salute of the Jugger" (1990), which has become a cult classic. Although he had turned down Spike Lee for a role in his debut "Do the Right Thing", Lee cast him as Woody Carmichael in the drama "Crooklyn" (1994), which brought him notice. Together with his other roles with Lee - as the West Indian Archie, a psychotic gangster, in "Malcolm X", and a starring role as a neighbourhood drug dealer in "Clockers" - he became established in his film career. 
Other films in which he has starring roles are Barry Sonnenfeld's "Get Shorty" (1995), Ron Howard's "Ransom" (1996) and "Soul of the Game" (1996), as the baseball player Satchel Paige. As a character actor, Lindo has readily taken on roles as treacherous bad guys as well as those of trustworthy professionals. 
In 1998 Lindo co-starred as African-American explorer Matthew Henson, in the TV film "Glory & Honor", directed by Kevin Hooks. It portrayed his nearly 20-year partnership with Commander Robert Peary in Arctic exploration and their effort to find the Geographic North Pole in 1909. He received a Satellite Award as best actor. Lindo continues to work in television and was most recently seen on the short-lived NBC drama "Kidnapped".
Lindo played an angel in the comedy film "A Life Less Ordinary" (1997), in which Dan Hedaya played the angel Gabriel, and Lindo's boss. He guest-starred on "The Simpsons" in the episode "Brawl in the Family", playing a similar character named Gabriel. 
Lindo had a small role in the 1995 science fiction/action film "Congo," playing the corrupt Captain Wanta. Lindo was not credited for the role, but one of his lines in the film, ""Stop eating my sesame cake!"", has become an internet meme.
In the British film, "Wondrous Oblivion" (2003), directed by Paul Morrison, he starred as Dennis Samuels, the father of a Jamaican immigrant family in London in the 1950s; he coaches his children and the son of a neighbour Jewish family in cricket, earning their admiration in a time of strained social relations. Lindo said he made the film in honour of his parents, who had similarly moved to London in those years.
In 2007, Lindo began an association with Berkeley Repertory Theatre in Berkeley, California, when he directed Tanya Barfield's play "The Blue Door". In the autumn of 2008, Lindo revisited August Wilson's play, "Joe Turner's Come and Gone", directing a production at the Berkeley Rep. In 2010, he played the role of elderly seer Bynum in David Lan's production of "Joe Turner" at the Young Vic Theatre in London.
Lindo is poised to play Marcus Garvey in an upcoming biopic of the black nationalist historical figure.

</doc>
<doc id="8343" url="https://en.wikipedia.org/wiki?curid=8343" title="David Janssen">
David Janssen

David Janssen (March 27, 1931 – February 13, 1980) was an American film and television actor who is best known for his starring role as Dr. Richard Kimble in the television series "The Fugitive" (1963–1967). Janssen also had the title roles in three other series: "Richard Diamond, Private Detective"; "Harry O"; and "O'Hara, U.S. Treasury".
In 1996 "TV Guide" ranked him number 36 on its 50 Greatest TV Stars of All Time list.
Early life.
Janssen was born as David Harold Meyer in Naponee, a village in Franklin County in southern Nebraska, to Harold Edward Meyer, a banker (May 12, 1906 – November 4, 1990) and Berniece Graf (May 11, 1910 – November 26, 1995). Janssen was of Irish and Jewish descent. Following his parents' divorce in 1935, his mother moved with five-year-old David to Los Angeles, California, and later married Eugene Janssen (February 18, 1918 – March 30, 1996) in 1940 in Los Angeles. Young David used his stepfather's name after he entered show business as a child.
He attended Fairfax High School in Los Angeles, where he excelled on the basketball court setting a school scoring record that lasted over 20 years. His first film part was at the age of thirteen, and by the age of twenty-five he had appeared in twenty films and served two years as an enlisted man in the United States Army. During his Army days, Janssen became friends with fellow enlistees Martin Milner and Clint Eastwood while posted at Fort Ord, California.
Acting career.
Janssen appeared in many television series before he landed programs of his own. In 1956, he and Peter Breck appeared in John Bromfield's syndicated series "Sheriff of Cochise" in the episode "The Turkey Farmers". Later, he guest starred on NBC's medical drama "The Eleventh Hour" in the role of Hal Kincaid in the 1962 episode "Make Me a Place", with series co-stars Wendell Corey and Jack Ging. He joined friend Martin Milner in a 1962 episode of "Route 66" as the character Kamo in the episode "One Tiger to a Hill."
Janssen starred in four television series of his own:
At the time, the final episode of "The Fugitive" held the record for the greatest number of American homes with television sets to watch a series finale, at 72 percent in August 1967.
His films include "To Hell and Back", the biography of Audie Murphy, who was the most decorated American soldier of World War II; John Wayne's Vietnam war film "The Green Berets"; opposite Gregory Peck in the space story "Marooned", in which Janssen played an astronaut sent to rescue three stranded men in space, and "The Shoes of the Fisherman", as a television journalist in Rome reporting on the election of a new Pope (Anthony Quinn).
He starred as a Los Angeles police detective trying to clear himself in the killing of an apparently innocent doctor in the 1968 film "Warning Shot".
Janssen played an alcoholic in the 1977 TV movie "A Sensitive, Passionate Man", which co-starred Angie Dickinson, and an engineer who devises an unbeatable system for blackjack in the 1978 made-for-TV movie "Nowhere to Run", co-starring Stefanie Powers and Linda Evans. Janssen's impressively husky voice was used to good effect as the narrator for the TV mini-series "Centennial" (1978–79); he also appeared in the final episode.
Though Janssen's scenes were cut from the final release, he also appeared as a journalist in the film "Inchon", which he accepted to work with Laurence Olivier who played General Douglas MacArthur. At the time of his death, Janssen had just begun filming a television movie playing the part of Father Damien, the priest who dedicated himself to the leper colony on the island of Molokai, Hawaii. The part was eventually reassigned to actor Ken Howard of the CBS series "The White Shadow".
In 1996 "TV Guide" ranked him number 36 on its 50 Greatest TV Stars of All Time list.
Personal life.
Janssen was married twice. His first marriage was to model and interior decorator Ellie Graham, whom he married in Las Vegas on August 25, 1958. They divorced in 1968. In 1975, he married actress and model Dani Crayne Greco. They remained married until Janssen's death.
Death.
Janssen died of a heart attack in the early morning of February 13, 1980, at his home in Malibu, California at age 48. At the time of his death, Janssen was filming the television movie "Father Damien". Janssen was buried at the Hillside Memorial Park Cemetery in Culver City, California. A non-denominational funeral was held at the Jewish chapel of the cemetery on February 17. Suzanne Pleshette delivered the eulogy at the request of Janssen's widow. Johnny Carson, Rod Stewart and Gregory Peck were among Janssen's pallbearers. Honorary pallbearers included Jack Lemmon, George Peppard, James Stewart and Danny Thomas.
For his contribution to the television industry, David Janssen has a star on the Hollywood Walk of Fame located on the 7700 block of Hollywood Boulevard.

</doc>
<doc id="8344" url="https://en.wikipedia.org/wiki?curid=8344" title="Docetism">
Docetism

In Christian terminology, docetism (from the Greek "dokeĩn" (to seem) /"dókēsis" (apparition, phantom), according to Norbert Brox, is defined narrowly as "the doctrine according to which the phenomenon of Christ, his historical and bodily existence, and thus above all the human form of Jesus, was altogether mere semblance without any true reality." Broadly it is taken as the belief that Jesus only seemed to be human, and that his human form was an illusion. The word "Dokētaí" (illusionists) referring to early groups who denied Jesus' humanity, first occurred in a letter by Bishop Serapion of Antioch (197–203), who discovered the doctrine in the Gospel of Peter, during a pastoral visit to a Christian community using it in Rhosus, and later condemned it as a forgery. It appears to have arisen over theological contentions concerning the meaning, figurative or literal, of a sentence from the Gospel of John: "the Word was made Flesh".
Docetism was unequivocally rejected at the First Council of Nicaea in 325 and is regarded as heretical by the Catholic Church, Orthodox Church, and Coptic Church.
Definitions.
Docetism is broadly defined as any teaching that claims that Jesus' body was either absent or illusory. The term ‘docetic’ should be used with caution, since its use is rather nebulous. For Robert Price "docetism", together with "encratism", "Gnosticism" and "adoptionism", has been employed "far beyond what historically descriptive usage would allow". Two varieties were widely known. In one version, as in Marcionism, Christ was so divine he could not have been human, since God lacked a material body, which therefore could not physically suffer. Jesus only appeared to be a flesh-and-blood man; his body was a phantasm. Other groups who were accused of docetism held that Jesus was a man in the flesh, but Christ was a separate entity who entered Jesus's body in the form of a dove at his baptism, empowered him to perform miracles, and abandoned him upon his death on the cross.
Christology and theological implications.
Docetism's origin within Christianity is obscure. Ernst Käsemann controversially defined the Christology of St John’s Gospel as "naïve docetism" in 1968. The ensuing debate reached an impasse as awareness grew that the very term "docetism", like "gnosticism", was difficult to define within the religio-historical framework of the debate. It has occasionally been argued that its origins were in heterodox Judaism or Oriental and Grecian philosophies. The alleged connection with Jewish Christianity would have reflected Jewish Christian concerns with the inviolability of (Jewish) monotheism. Docetic opinions seem to have circulated from very early times, 1 John 4:2 appearing explicitly to reject them. Some 1st century Christian groups developed docetic interpretations partly as a way to make Christian teachings more acceptable to pagan ways of thinking about divinity.
In his critique of the theology of Clement of Alexandria, Photius in his Myriobiblon held that Clement's views reflected a quasi-docetic view of the nature of Christ, writing that " hallucinates that the Word was not incarnate but "only seems to be"." (ὀνειροπολεῖ καὶ μὴ σαρκωθῆναι τὸν λόγον ἀλλὰ "δόξαι".) In Clement’s time, some disputes contended over whether Christ assumed the "psychic" flesh of mankind as heirs to Adam, or the "spiritual" flesh of the resurrection. Docetism largely died out during the first millennium AD.
The opponents against whom Ignatius of Antioch inveighs are often taken to be Monophysite docetists. In his letter to the Smyrnaeans, 7:1, written around 110 AD, he writes:
They abstain from the Eucharist and from prayer, because they confess not the Eucharist to be the flesh of our Saviour Jesus Christ, which suffered for our sins, and which the Father, of His goodness, raised up again. They who deny the gift of God are perishing in their disputes. 
While these characteristics fit a Monophysite framework, a slight majority of scholars consider that Ignatius was waging a polemic on two distinct fronts, one Jewish, the other docetic; a minority holds that he was concerned with a group that commingled Judaism and docetism. Other, however, doubt that there was actual docetism threatening the churches, arguing that he was merely criticizing Christians who lived Jewishly or that his critical remarks were directed at an Ebionite or Cerinthian possessionist Christology, according to which Christ was a heavenly spirit that temporarily possessed Jesus.
Islam and docetism.
The Qur'an has a docetic Christology, viewing Jesus as a divine illuminator rather than the redeemer (as he is viewed in Christianity). However, the Islamic docetism is not focused on the general life and person of Jesus or the Christ. In Islam "the Christ" ("al-masīḥ") is not generally viewed as distinct from humanity nor a special spirit being as in docetism or some gnosticisms. Islamic docetism focuses on a denial of the crucifixion of Jesus. Sura 4:157–158 reads:
And because of their saying: We slew the Messiah, Jesus son of Mary, Allah's messenger — they slew him not nor crucified him, but it appeared so unto them; and lo! those who disagree concerning it are in doubt thereof; they have no knowledge thereof save pursuit of a conjecture; they slew him not for certain. But Allah took him up unto Himself. Allah was ever Mighty, Wise.
Docetism and Christ myth theory.
Since Arthur Drews published his "The Christ Myth" (Die Christusmythe) in 1909, occasional connections have been drawn between the modern idea that Christ was a myth and docetist theories. Shailer Mathews called Drews' theory a "modern docetism". Frederick Cornwallis Conybeare thought any connection to be based on a misunderstanding of docetism. The idea recurred in Classicist Michael Grant's 1977 review of the evidence for Jesus, who compared modern scepticism about an historical Jesus to the ancient docetic idea that Jesus only "seemed" to come into the world "in the flesh". Modern theories did away with "seeming".

</doc>
<doc id="8347" url="https://en.wikipedia.org/wiki?curid=8347" title="Greek drachma">
Greek drachma

Drachma ( , ; pl. "drachmae" or "drachmas") was the currency used in Greece during several periods in its history:
It was also a small unit of weight.
Ancient drachma.
The name "drachma" is derived from the verb δράσσομαι ("drássomai", "(I) grasp"). It is believed that the same word with the meaning of "handful" or "handle" is found in Linear B tablets of the Mycenean Pylos. Initially a drachma was a fistful (a "grasp") of six "oboloí" or "obeloí" (metal sticks, literally "spits") used as a form of currency as early as 1100 BC and being a form of "bullion": bronze, copper, or iron ingots denominated by weight. A hoard of over 150 rod-shaped obeloi was uncovered at Heraion of Argos in Peloponnese. Six of them are displayed at the Numismatic Museum of Athens.
It was the standard unit of silver coinage at most ancient Greek mints, and the name "obol" was used to describe a coin that was one-sixth of a drachma. The notion that "drachma" derived from the word for fistful was recorded by Herakleides of Pontos (387–312 BC) who was informed by the priests of Heraion that Pheidon, king of Argos, dedicated rod-shaped obeloi to Heraion. Similar information about Pheidon's obeloi was also recorded at the Parian Chronicle.
Ancient Greek coins normally had distinctive names in daily use. The Athenian tetradrachm was called owl, the Aeginetic stater was called chelone, the Corinthian stater was called "hippos" (horse) an so on. Each city would mint its own and have them stamped with recognizable symbols of the city, known as badge in numismatics, along with suitable inscriptions, and they would often be referred to either by the name of the city or of the image depicted. The exact exchange value of each was determined by the quantity and quality of the metal, which reflected on the reputation of each mint.
Among the Greek cities that used the drachma were: Abdera, Abydos, Alexandria, Aetna, Antioch, Athens, Chios, Cyzicus, Corinth, Ephesus, Eretria, Gela, Catana, Kos, Maronia, Naxos, Pella, Pergamum, Rhegion, Salamis, Smyrni, Sparta, Syracuse, Tarsus, Thasos, Tenedos, Troy and more.
The 5th century BC Athenian "tetradrachm" ("four drachmae") coin was perhaps the most widely used coin in the Greek world prior to the time of Alexander the Great (along with the Corinthian stater). It featured the helmeted profile bust of Athena on the obverse (front) and an owl on the reverse (back). In daily use they were called "glaukes" (owls), hence the proverb , 'an owl to Athens', referring to something that was in plentiful supply, like 'coals to Newcastle'. The reverse is featured on the national side of the modern Greek 1 euro coin.
Drachmae were minted on different weight standards at different Greek mints. The standard that came to be most commonly used was the Athenian or Attic one, which weighed a little over 4.3 grams.
After Alexander the Great's conquests, the name "drachma" was used in many of the Hellenistic kingdoms in the Middle East, including the Ptolemaic kingdom in Alexandria and the Parthian Empire based in what is modern-day Iran. The Arabic unit of currency known as "dirham" (in the Arabic language, درهم), known from pre-Islamic times and afterwards, inherited its name from the drachma or didrachm (, 2 drachmae); the dirham is still the name of the official currencies of Morocco and the United Arab Emirates. The Armenian dram also derives its name from the drachma.
Value.
It is difficult to estimate comparative exchange rates with modern currency because the range of products produced by economies of centuries gone by were different from today, which makes purchasing power parity (PPP) calculations very difficult; however, some historians and economists have estimated that in the 5th century BC a drachma had a rough value of 25 U.S. dollars (in the year 1990 – equivalent to 46.50 USD in 2015), whereas classical historians regularly say that in the heyday of ancient Greece (the fifth and fourth centuries) the daily wage for a skilled worker or a hoplite was one drachma, and for a heliast (juror) half a drachma since 425 BC.
Modern commentators derived from Xenophon that half a drachma per day (360 days per year) would provide "a comfortable subsistence" for "the poor citizens" (for the head of a household in 355 BC). Earlier in 422 BC, we also see in Aristophanes ("Wasps", line 300–302) that the daily half-drachma of a juror is just enough for the daily subsistence of a family of three.
A modern person might think of one drachma as the rough equivalent of a skilled worker's daily pay in the place where they live, which could be as low as $1 USD, or as high as $100 USD, depending on the country.
Fractions and multiples of the drachma were minted by many states, most notably in Ptolemaic Egypt, which minted large coins in gold, silver and bronze.
Notable Ptolemaic coins included the gold "pentadrachm" and "octadrachm", and silver "tetradrachm", "decadrachm" and "pentakaidecadrachm". This was especially noteworthy as it would not be until the introduction of the Guldengroschen in 1486 that coins of substantial size (particularly in silver) would be minted in significant quantities.
For the Roman successors of the drachma, see Roman provincial coins.
Denominations of Ancient Greek drachma.
The weight of the silver drachma was approximately 4.3 grams, although weights varied significantly from one city-state to another. It was divided into six obols of 0.72 grams, which were subdivided into four tetartemoria of 0.18 grams, one of the smallest coins ever struck, approximately 5–7 mm in diameter.
Historic currency divisions.
Minae and talents were never actually minted: they represented weight measures used for commodities (e.g. grain) as well as metals like silver or gold. The New Testament mentions both didrachma and, by implication, tetradrachma in context of the Temple tax. Luke's Gospel includes a parable told by Jesus of a woman with 10 drachmae, who lost one and searched her home until she found it.
Modern drachma.
First modern drachma.
The drachma was reintroduced in May 1832, shortly before the establishment of the modern state of Greece (with the exception of the subdivision Taurus). It replaced the "phoenix" at par. The drachma was subdivided into 100 lepta.
Coins.
The first coinage consisted of copper denominations of 1, 2, 5 and 10 lepta, silver denominations of , , 1 and 5 drachmae and a gold coin of 20 drachmae. The drachma coin weighed 4.5 g and contained 90% silver, with the 20-drachma coin containing 5.8 g of gold.
In 1868, Greece joined the Latin Monetary Union and the drachma became equal in weight and value to the French franc. The new coinage issued consisted of copper coins of 1, 2, 5 and 10 lepta, with the 5- and 10-lepta coins bearing the names "obolos" () and "diobolon" (), respectively; silver coins of 20 and 50 lepta, 1, 2 and 5 drachmae and gold coins of 5, 10 and 20 drachmae. (Very small numbers of 50- and 100-drachma coins in gold were also issued.)
In 1894, cupro-nickel 5-, 10- and 20-lepta coins were introduced. No 1-lepton or 2-lepta coin had been issued since the late 1870s. Silver coins of 1 and 2 drachmae were last issued in 1911, and no coins were issued between 1912 and 1922, during which time the Latin Monetary Union collapsed due to World War I.
Between 1926 and 1930, a new coinage was introduced for the new Hellenic Republic, consisting of cupro-nickel coins in denominations of 20 lepta, 50 lepta, 1 drachma, and 2 drachmae; nickel coins of 5 drachmae; and silver coins of 10 and 20 drachmae. These were the last coins issued for the first modern drachma, and none were issued for the second.
Notes.
Notes were issued by the National Bank of Greece from 1841 until 2001 when Greece joined the Euro. Early denominations ranged from 10 to 500 drachmae. Smaller denominations (1, 2, 3 and 5 drachmae) were issued from 1885, with the first 5-drachma notes being made by cutting 10-drachma notes in half.
When Greece finally achieved its independence from the Ottoman Empire in 1828, the phoenix was introduced as the monetary unit; its use was short-lived, however, and in 1832 the phoenix was replaced by the drachma, adorned with the image of King Otto of Greece, who reigned as modern Greece’s first king from 1832 to 1862. The drachma was divided into 100 lepta. In 2002 the drachma ceased to be legal tender after the euro, the monetary unit of the European Union, became Greece’s sole currency.
Between 1917 and 1920, the Greek government issued paper money in denominations of 10 lepta, 50 lepta, 1 drachma, 2 drachmae, and 5 drachmae. The National Bank of Greece introduced 1000-drachma notes in 1901, and the Bank of Greece introduced 5000-drachma notes in 1928. The Greek government again issued notes between 1940 and 1944, in denominations ranging from 50 lepta to 20 drachmae.
During the German-Italian occupation of Greece from 1941 to 1944, catastrophic hyperinflation and Nazi looting of the Greek treasury caused much higher denominations to be issued, culminating in 100,000,000,000-drachma notes in 1944.
Second modern drachma.
In November 1944, after Greece was liberated from Germany, old drachmae were exchanged for new ones at the rate of 50,000,000,000 to 1. Only paper money was issued. The government issued notes of 1, 5, 10 and 20 drachmae, with the Bank of Greece issuing 50-, 100-, 500-, 1000-, 5000-, and 10,000-drachma notes. This drachma also suffered from high inflation. The government later issued 100-, 500-, and 1000-drachma notes, and the Bank of Greece issued 20,000-and 50,000-drachma notes.
Third modern drachma.
In 1953, in an effort to halt inflation, Greece joined the Bretton Woods system. In 1954, the drachma was revalued at a rate of 1000 to 1. The new currency was pegged at 30 drachmae = 1 United States dollar. In 1973, the Bretton Woods System was abolished; over the next 25 years the official exchange rate gradually declined, reaching 400 drachmae to 1 U. S. dollar. On 1 January 2002, the Greek drachma was officially replaced as the circulating currency by the euro, and it has not been legal tender since 1 March 2002.
Third modern drachma coins.
The first issue of coins minted in 1954 consisted of holed aluminium 5-, 10- and 20-lepton pieces, with 50-lepton, 1-, 2-, 5- and 10-drachma pieces in cupro-nickel. A silver 20-drachma piece was issued in 1960, replacing the 20-drachma banknote. Coins in denominations from 50 lepta to 20 drachmae carried a portrait of King Paul (1947–1964). New coins were introduced in 1966, ranging from 50 lepta to 10 drachmae, depicting King Constantine II (1964–1974). The reverse of all coins was altered in 1971 to reflect the military junta which was in power from 1967 to 1974. This design included a soldier standing in front of the flames of the rising phoenix.
A 20-drachmae coin in cupro-nickel with an image of Europa on the obverse was issued in 1973. In the latter part of 1973, several new coin types were introduced: unholed aluminium (10 and 20 lepta), nickel-brass (50 lepta, 1 drachma, and 2 drachmae) and cupro-nickel (5, 10, and 20 drachmae). These provisional coins carried the design of the phoenix rising from the flame on the obverse, and used the country's new designation as the "Hellenic Republic", replacing the coins also issued in 1973 as the Kingdom of Greece with King Constantine II's portrait. A new series of all 8 denominations was introduced in 1976 carrying images of early national heroes on the smaller values.
Cupro-nickel 50-drachmae coins were introduced in 1980. In 1986, nickel-brass 50-drachma coins were introduced, followed by copper 1- and 2-drachma pieces in 1988 and nickel-brass coins of 20 and 100 drachmae in 1990. In 2000, a set of 6 themed 500-drachma coins was issued to commemorate the 2004 Athens Olympic Games.
Coins in circulation at the time of the adoption of the euro were
Banknotes.
The first issues of banknotes were in denominations of 10, 20 and 50 drachmae, soon followed by 100, 500 and 1000 drachmae by 1956. 5000-drachma notes were introduced in 1984, followed by 10,000-drachma notes in 1995 and 200-drachma notes in 1997.
Banknotes in circulation at the time of the adoption of the euro were
Encoding.
In Unicode, the currency symbol is . There is a special Attic numeral, for the value of one drachma but it fails to render in most browsers.
Restoration.
The Drachmi Greek Democratic Movement Five Stars which was founded in 2013, aims to restore the Drachma, as Greece's currency.

</doc>
