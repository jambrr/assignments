<doc id="7617" url="https://en.wikipedia.org/wiki?curid=7617" title="Corum Jhaelen Irsei">
Corum Jhaelen Irsei

Corum Jhaelen Irsei ("the Prince in the Scarlet Robe") is the name of a fictional fantasy hero in a series of two trilogies written by author Michael Moorcock.
Plot summary.
Corum is the last survivor of the Vadhagh race and an incarnation aspect of the Eternal Champion, a being that exists in all worlds to ensure there is "Cosmic Balance".
"Corum: The Prince in the Scarlet Robe" (The Swords Trilogy).
Corum is a Vadhagh, one of a race of long-lived beings with limited magical abilities dedicated to peaceful pursuits such as art and poetry. A group of "Mabden" (men) led by the savage Earl Glandyth-a-Krae raid the family castle and slaughter everyone with the exception of Corum, who escapes. Arming himself, Corum attacks and kills several of the Mabden before being captured and tortured. After having his left hand cut off and right eye put out, Corum escapes by moving into another plane of existence, becoming invisible to the Mabden. They depart and Corum is found by The Brown Man, a dweller of the forest of Laar able to see Corum while out of phase. The Brown Man takes Corum to a being called Arkyn, who treats his wounds and explains he has a higher purpose. 
Travelling to Moidel's Castle, Corum encounters his future lover, the Margravine Rhalina. Rhalina uses sorcery (a ship summoned from the depths of the ocean and manned by her drowned dead husband and crew) to ward off an attack by Glandyth-a-Krae. Determined to restore himself, Corum and Rhalina travel to the island of Shool, a near immortal and mad sorcerer. During the journey Corum observes a mysterious giant who trawls the ocean with a net. On arrival at the island Shool takes Rhalina hostage, and then provides Corum with two artifacts to replace his lost hand and eye: the Hand of Kwll and the Eye of Rhynn. The Eye of Rhynn allows Corum to see into an undead netherworld where the last beings killed by Corum exist until summoned by the Hand of Kwll. 
Shool then explains that Corum's ill fortune has been caused by the Chaos God Arioch, the Knight of the Swords. When Arioch and his fellow Chaos Lords conquered the Fifteen Planes, the balance between the forces of Law and Chaos tipped in favor of Chaos, and their minions - such as Glandyth-a-Krae - embarked on a bloody rampage. Shool sends Corum to Arioch's fortress to steal the Heart of Arioch, which the sorcerer intends to use to attain greater power. Corum confronts Arioch, and learns Shool is nothing more than a pawn of the Chaos God. Arioch then ignores Corum, who discovers the location of the Heart. Corum is then attacked by Arioch, but the Hand of Kwll crushes the Heart and banishes the Chaos God forever. Before fading from existence, Arioch warns Corum that he has now earned the enmity of the Sword Rulers. Corum returns to the island to rescue Rhalina, and observes Shool has become a powerless moron, and is devoured by his own creations soon afterwards. Corum learns Arkyn is in fact a Lord of Law, and that this is the first step towards Law regaining control of the Fifteen Planes.
On another five planes, the forces of Chaos - led by Xiombarg, Queen of the Swords - reign supreme and are on the verge on eradicating the last resistance from the forces of Law. The avatars of the Bear and Dog gods plot with Earl Glandyth-a-Krae to murder Corum and return Arioch to the Fifteen Planes. Guided by Arkyn, Corum, Rhalina and companion Jhary-a-Conel cross the planes and encounter the King Without A Country, the last of his people who in turn is seeking the City in the Pyramid. The group locate the City, which is in fact a floating arsenal powered by advanced technology and inhabited by a people originally from Corum's world and his distant kin. 
Besieged by the forces of Chaos, the City requires certain rare minerals to continue to power their weapons. Corum and Jhary attempt to locate the minerals and also encounter Xiombarg, who learns of Corum's identity. Corum slows Xiombarg's forces by defeating their leader, Prince Gaynor the Damned. Xiombarg is goaded into attacking the City directly in revenge for Arioch's banishment. Arkyn provides the minerals and confronts Xiombarg, who has manifested in a vulnerable state. As Arkyn banishes Xiombarg, Corum and his allies devastate the forces of Chaos. Glandyth-a-Krae, however, escapes, and seeks revenge.
A spell - determined to have been cast by the forces of Chaos - forces the inhabitants of Corum's plane to war with each other (including the City in the Pyramid). Desperate to stop the slaughter, Corum, Rhalina and Jhary Corum travel to the last five planes, ruled by Mabelode, the King of the Swords. Rhalina is taken hostage by the forces of Chaos and Corum has several encounters with the forces of Chaos, including Earl Glandyth-a-Krae. 
Corum also meets two other aspects of the Eternal Champion: Elric and ErekosÃ«, with all three seeking the mystical city of Tanelorn for their own purposes. After a brief adventure in the "Vanishing Tower", the other heroes depart and Corum and Jhary arrive at their version of Tanelorn. Corum discovers one of the "Lost Gods", the being Kwll, who is imprisoned and cannot be freed until whole. Corum offers Kwll his hand, on the condition that he aid them against Mabelode. Kwll accepts the terms, but reneges on the bargain until persuaded to assist. Corum is also stripped of his artificial eye, which belongs to Rhynn - actually the mysterious giant Corum had previously encountered. Kwll transports Corum and Jhary to the court of Mabelode, with the pair fleeing with Rhalina when Kwll directly challenges the Chaos God.
Having found out Corum's location by torturing and killing the Brown Man of Laar, Glandyth-a-Krae marshalled his allies to Moidel's Castle. Glandyth had kept Corum's former hand and eye as souvenirs, and showed them to Corum to provoke a reaction.
In a final battle Corum avenged his family by killing Glandyth-a-Krae and decimating the last of Chaos' mortal forces. Kwll later located Corum and revealed that all the gods - of both Chaos and Law - have been slain in order to free humanity and allow it to shape its own destiny.
"Corum: The Prince with the Silver Hand".
Set eighty years after the defeat of the Sword Rulers, Corum has become despondent and alone since the death of his Mabden bride Rhalina. Plagued by voices at night, Corum believes he has gone insane until old friend Jhary-a-Conel advises Corum it is in fact a summons from another world. Listening to the voices allows Corum to pass to the other world, which is in fact the distant future. Rhalina's descendants, the Tuha-na-Cremm Croich (who call Corum "Corum Llew Ereint") face extinction from the Fhoi Myore: seven giants who with their allies conquered the land and plunged it into eternal winter. Allying himself with King Mannach, Corum falls in love with his daughter Medhbh.
Corum also hears the prophecy of a seeress, who claims Corum should fear a brother (who will apparently slay him), a harp and above all, beauty. Corum seeks the lost artifacts of the Tuha-na-Cremm Croich - a sacred Bull, a spear, an oak, a ram and a stallion - which will restore the land. Together with new allies Goffanon (a blacksmith and diminutive giant) and Goffanon's cousin and true giant Illbrec battles the Fhoi Myore and their own allies, a returned Prince Gaynor, the wizard Calatin and his clone of Corum, the Brothers of the Pine, the undead Ghoolegh and a host of giant wolves. After being instrumental in the death of two of the Fhoi Myore and restoring the High King of the Tuha-na-Cremm Croich, Corum and his allies have a final battle in which all their foes are destroyed.
Corum decides not to return his own world, and is attacked by his clone, whom he defeats with the aid of a spell placed on his silver hand by Medhbh. Medhbh, however, attacks and wounds Corum, having been told by the being the Dagdah that their world must be free of all gods and demi-gods if they are to flourish as a people. Corum is then killed with his own sword by his animated silver hand, thereby fulfilling the prophecy.
In other media.
First Comics published "The Chronicles of Corum", a twelve issue limited series (Jan. 1986 - Dec. 1988) that adapted the "Swords Trilogy", and was followed by the four issue limited series "Corum: The Bull and the Spear" (Jan. - July (bi-monthly) 1989), which adapted the first book in the second trilogy. 
Darcsyde Productions produced a supplement for use with Chaosium's "Stormbringer" (2001) role-playing game adapting the characters and settings from the "Corum" series for role-playing.
Gollancz have announced plans to release the entire Corum stories in both print and ebook form, commencing in 2013. The ebooks will be available via Gollancz's SF Gateway site.
Bibliography.
First trilogy:
Second trilogy:
Additional appearances:
Note: In the United Kingdom the first trilogy has been collected as an omnibus edition titled "Corum", "Swords of Corum" and most recently "Corum: The Prince in the Scarlet Robe" (vol. 30 of Orion's Fantasy Masterworks series). In the United States the first trilogy has been published as "Corum: The Coming of Chaos". The second trilogy was titled "The Prince with the Silver Hand" (United Kingdom) and "The Chronicles of Corum" (United States) respectively.

</doc>
<doc id="7618" url="https://en.wikipedia.org/wiki?curid=7618" title="Cumberland (disambiguation)">
Cumberland (disambiguation)

Cumberland is one of the historic counties of England.
Cumberland may also refer to:

</doc>
<doc id="7619" url="https://en.wikipedia.org/wiki?curid=7619" title="Capella (disambiguation)">
Capella (disambiguation)

Capella, meaning "small she goat" in classical Latin, and chapel in medieval Latin, may refer to:

</doc>
<doc id="7622" url="https://en.wikipedia.org/wiki?curid=7622" title="Complex instruction set computing">
Complex instruction set computing

Complex instruction set computing (CISC ) is a processor design where single instructions can execute several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multi-step operations or addressing modes within single instructions. The term was retroactively coined in contrast to reduced instruction set computer (RISC) and has therefore become something of an umbrella term for everything that is not RISC, i.e. everything from large and complex mainframes to simplistic microcontrollers where memory load and store operations are not separated from arithmetic instructions. 
A modern RISC processor can therefore be much more complex than, say, a modern microcontroller using a CISC-labeled instruction set, especially in terms of implementation (electronic circuit complexity), but also in terms of the number of instructions or the complexity of their encoding patterns. The only differentiating characteristic (nearly) "guaranteed" is the fact that most RISC designs use uniform instruction length for (almost) all instructions and employ strictly separate load/store-instructions.
Examples of instruction set architectures that have been retroactively labeled CISC are System/360 through z/Architecture, the PDP-11 and VAX architectures, Data General Nova and many others. Well known microprocessors and microcontrollers that have also been labeled CISC in many academic publications include the Motorola 6800, 6809 and 68000-families; the Intel 8080, iAPX432 and x86-family; the Zilog Z80, Z8 and Z8000-families; the National Semiconductor 32016 and NS320xx-line; the MOS Technology 6502-family; the Intel 8051-family; and others.
Some designs have been regarded as borderline cases by some writers. For instance, the Microchip Technology PIC has been labeled RISC in some circles and CISC in others and the 6502 and 6809 have both been described as "RISC-like", although they have complex addressing modes as well as arithmetic instructions that access memory, contrary to the RISC-principles.
Historical design context.
Incitements and benefits.
Before the RISC philosophy became prominent, many computer architects tried to bridge the so-called semantic gap, i.e. to design instruction sets that directly supported high-level programming constructs such as procedure calls, loop control, and complex addressing modes, allowing data structure and array accesses to be combined into single instructions. Instructions are also typically highly encoded in order to further enhance the code density. The compact nature of such instruction sets results in smaller program sizes and fewer (slow) main memory accesses, which at the time (early 1960s and onwards) resulted in a tremendous savings on the cost of computer memory and disc storage, as well as faster execution. It also meant good programming productivity even in assembly language, as high level languages such as Fortran or Algol were not always available or appropriate (microprocessors in this category are sometimes still programmed in assembly language for certain types of critical applications).
New instructions.
In the 1970s, analysis of high level languages indicated some complex machine language implementations and it was determined that new instructions could improve performance. Some instructions were added that were never intended to be used in assembly language but fit well with compiled high-level languages. Compilers were updated to take advantage of these instructions. The benefits of semantically rich instructions with compact encodings can be seen in modern processors as well, particularly in the high-performance segment where caches are a central component (as opposed to most embedded systems). This is because these fast, but complex and expensive, memories are inherently limited in size, making compact code beneficial. Of course, the fundamental reason they are needed is that main memories (i.e. dynamic RAM today) remain slow compared to a (high performance) CPU core.
Design issues.
While many designs achieved the aim of higher throughput at lower cost and also allowed high-level language constructs to be expressed by fewer instructions, it was observed that this was not "always" the case. For instance, low-end versions of complex architectures (i.e. using less hardware) could lead to situations where it was possible to improve performance by "not" using a complex instruction (such as a procedure call or enter instruction), but instead using a sequence of simpler instructions.
One reason for this was that architects (microcode writers) sometimes "over-designed" assembly language instructions, i.e. including features which were not possible to implement efficiently on the basic hardware available. This could, for instance, be "side effects" (above conventional flags), such as the setting of a register or memory location that was perhaps seldom used; if this was done via ordinary (non duplicated) internal buses, or even the "external" bus, it would demand extra cycles every time, and thus be quite inefficient.
Even in balanced high-performance designs, highly encoded and (relatively) high-level instructions could be complicated to decode and execute efficiently within a limited transistor budget. Such architectures therefore required a great deal of work on the part of the processor designer in cases where a simpler, but (typically) slower, solution based on decode tables and/or microcode sequencing is not appropriate. At a time when transistors and other components were a limited resource, this also left fewer components and less opportunity for other types of performance optimizations.
The RISC idea.
The circuitry that performs the actions defined by the microcode in many (but not all) CISC processors is, in itself, a processor which in many ways is reminiscent in structure to very early CPU designs. In the early 1970s, this gave rise to ideas to return to simpler processor designs in order to make it more feasible to cope without ("then" relatively large and expensive) ROM tables and/or PLA structures for sequencing and/or decoding. The first (retroactively) RISC-"labeled" processor (IBM 801 IBM's Watson Research Center, mid-1970s) was a tightly pipelined simple machine originally intended to be used as an internal microcode kernel, or engine, in CISC designs, but also became the processor that introduced the RISC idea to a somewhat larger public. Simplicity and regularity also in the visible instruction set would make it easier to implement overlapping processor stages (pipelining) at the machine code level (i.e. the level seen by compilers). However, pipelining at that level was already used in some high performance CISC "supercomputers" in order to reduce the instruction cycle time (despite the complications of implementing within the limited component count and wiring complexity feasible at the time). Internal microcode execution in CISC processors, on the other hand, could be more or less pipelined depending on the particular design, and therefore more or less akin to the basic structure of RISC processors.
Superscalar.
In a more modern context, the complex variable-length encoding used by some of the typical CISC architectures makes it complicated, but still feasible, to build a superscalar implementation of a CISC programming model "directly"; the in-order superscalar original Pentium and the out-of-order superscalar Cyrix 6x86 are well known examples of this. The frequent memory accesses for operands of a typical CISC machine may limit the instruction level parallelism that can be extracted from the code, although this is strongly mediated by the fast cache structures used in modern designs, as well as by other measures. Due to inherently compact and semantically rich instructions, the average amount of work performed per machine code unit (i.e. per byte or bit) is higher for a CISC than a RISC processor, which may give it a significant advantage in a modern cache based implementation.
Transistors for logic, PLAs, and microcode are no longer scarce resources; only large high-speed cache memories are limited by the maximum number of transistors today. Although complex, the transistor count of CISC decoders do not grow exponentially like the total number of transistors per processor (the majority typically used for caches). Together with better tools and enhanced technologies, this has led to new implementations of highly encoded and variable length designs without load-store limitations (i.e. non-RISC). This governs re-implementations of older architectures such as the ubiquitous x86 (see below) as well as new designs for microcontrollers for embedded systems, and similar uses. The superscalar complexity in the case of modern x86 was solved by converting instructions into one or more micro-operations and dynamically issuing those micro-operations, i.e. indirect and dynamic superscalar execution; the Pentium Pro and AMD K5 are early examples of this. It allows a fairly simple superscalar design to be located after the (fairly complex) decoders (and buffers), giving, so to speak, the best of both worlds in many respects.
CISC and RISC terms.
The terms CISC and RISC have become less meaningful with the continued evolution of both CISC and RISC designs and implementations. The first highly (or tightly) pipelined x86 implementations, the 486 designs from Intel, AMD, Cyrix, and IBM, supported every instruction that their predecessors did, but achieved "maximum efficiency" only on a fairly simple x86 subset that was only a little more than a typical RISC instruction set (i.e. without typical RISC "load-store" limitations). The Intel P5 Pentium generation was a superscalar version of these principles. However, modern x86 processors also (typically) decode and split instructions into dynamic sequences of internally buffered micro-operations, which not only helps execute a larger subset of instructions in a pipelined (overlapping) fashion, but also facilitates more advanced extraction of parallelism out of the code stream, for even higher performance.
Contrary to popular simplifications (present also in some academic texts), not all CISCs are microcoded or have "complex" instructions. As CISC became a catch-all term meaning anything that's not a load-store (RISC) architecture, it's not the number of instructions, nor the complexity of the implementation or of the instructions themselves, that define CISC, but the fact that arithmetic instructions also perform memory accesses. Compared to a small 8-bit CISC processor, a RISC floating-point instruction is complex. CISC does not even need to have complex addressing modes; 32 or 64-bit RISC processors may well have more complex addressing modes than small 8-bit CISC processors.
A PDP-10, a PDP-8, an Intel 386, an Intel 4004, a Motorola 68000, a System z mainframe, a Burroughs B5000, a VAX, a Zilog Z80000, and a MOS Technology 6502 all vary wildly in the number, sizes, and formats of instructions, the number, types, and sizes of registers, and the available data types. Some have hardware support for operations like scanning for a substring, arbitrary-precision BCD arithmetic, or transcendental functions, while others have only 8-bit addition and subtraction. But they are all in the CISC category because they have "load-operate" instructions that load and/or store memory contents within the same instructions that perform the actual calculations. For instance, the PDP-8, having only 8 fixed-length instructions and no microcode at all, is a CISC because of "how" the instructions work, PowerPC, which has over 230 instructions (more than some VAXes) and complex internals like register renaming and a reorder buffer is a RISC, while Minimal CISC has 8 instructions, but is clearly a CISC because it combines memory access and computation in the same instructions.
Some of the problems and contradictions in this terminology will perhaps disappear as more systematic terms, such as (non) load/store, become more popular and eventually replace the imprecise and slightly counter-intuitive RISC/CISC terms.

</doc>
<doc id="7624" url="https://en.wikipedia.org/wiki?curid=7624" title="CISC">
CISC

CISC may refer to:

</doc>
<doc id="7626" url="https://en.wikipedia.org/wiki?curid=7626" title="Cetacea">
Cetacea

Cetacea (), (from Latin "cetus" "whale" and Greek "ketos" "huge-fish") are a widely distributed and diverse clade of carnivorous, finned, aquatic marine mammals. They comprise the extant parvorders Odontoceti (toothed whales including dolphins and porpoises), Mysticeti (the baleen whales), and Archaeoceti (the ancestors of modern whales, and is now extinct). There are around 89 species of cetaceans, and more than 70 belonging to Odontoceti. While cetaceans were historically thought to have descended from mesonychids, molecular evidence supports them as a relative of Artiodactyls (even-toed ungulates). Cetaceans belong to the order Cetartiodactyla (formed by combining Cetacea + Artiodactyla) and their closest living relatives are hippopotamuses and other hoofed mammals (camels, pigs, and ruminants), having diverged about 50 million years ago.
Cetaceans range in size from the and Maui's dolphin to the and blue whale, which is also the largest creature alive. Several species exhibit sexual dimorphism. They have streamlined bodies and two (external) limbs that are modified into flippers. Though not as fast in the water as dolphins, seals are more flexible and agile Though not as flexible or agile as seals, cetaceans can swim very fast, with the killer whale able to travel at in short bursts and the fin whale able to cruise at . The hindlimbs of cetaceans are internal, and are thought to be vestigial. Dolphins are able to make very tight turns while swimming at high speeds. Baleen whales have short hairs on their mouth, unlike the toothed whales. Cetaceans have well-developed sensesâtheir eyesight and hearing are adapted for both air and water, and baleen whales have a tactile system in their vibrissae. Some species are well adapted for diving to great depths. They have a layer of fat, or blubber, under the skin to keep warm in the cold water.
Although pinnipeds are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres. They spend their lives in the water, having to mate, give birth, molt or escape from predators, like killer whales, underwater. This has drastically affected their anatomy to be able to do so. They feed largely on fish and marine invertebrates; but a few, like the killer whale, feed on large mammals and birds, such as penguins and seals. Baleen whales (mainly gray whales and right whales) are specialised for feeding on benthic creatures. Male cetaceans typically mate with more than one female (polygyny), although the degree of polygyny varies with the species. Cetaceans are not shown to have pair bonds. Male cetacean strategies for reproductive success vary between herding females, defending potential mates from other males, or whale song which attracts mates. Calves are typically born in the fall and winter months, and females bear almost all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively short period of time, which is more typical of baleen whales as their main food source (invertebrates) aren't found in their breeding and calving grounds (tropics). Cetaceans produce a number of vocalizations, notably the clicks and whistles of dolphins, the moaning songs of the humpback whale.
The meat, blubber and oil of cetaceans have traditionally been used by indigenous peoples of the Arctic. Cetaceans have been depicted in various cultures worldwide. Dolphins are commonly kept in captivity and are even sometimes trained to perform tricks and tasks, other cetaceans aren't as often kept in captivity (with usually unsuccessful attempts). Once relentlessly hunted by commercial industries for their products, cetaceans are now protected by international law. The baiji (Chinese river dolphin) has become extinct in the past century, while the vaquita and Yangtze finless porpoise are ranked Critically Endangered by the International Union for Conservation of Nature. Besides hunting, cetaceans also face threats from accidental trapping, marine pollution, and ongoing climate change.
Baleen whales and toothed whales.
The two parvorders, baleen whales (Mysticeti) and toothed whales (Odontoceti), are thought to have diverged around thirty-four million years ago.
Baleen whales have bristles made of keratin instead of teeth. The bristles filter krill and other small invertebrates from seawater. Grey whales feed on bottom-dwelling mollusks. Rorqual family (balaenopterids) use throat pleats to expand their mouths to take in food and sieve out the water. Balaenids (right whales and bowhead whales) have massive heads that can make up 40% of their body mass. Most mysticetes prefer the food-rich colder waters of the Northern and Southern Hemispheres, migrating to the Equator to give birth. During this process, they are capable of fasting for several months, relying on their fat reserves.
The parvorder of Odontocetes - the toothed whales - include sperm whales, beaked whales, killer whales, dolphins and porpoises. They have conical teeth designed for catching fish or squid. A few, such as the killer whale, feed on mammals, such as pinnipeds and other whales. They have well-developed sensesâtheir eyesight and hearing are adapted for both air and water, and they have advanced sonar capabilities using their melon. Their hearing is so well-adapted for both air and water that some blind specimens can survive. Some species, such as sperm whales, are well adapted for diving to great depths. Several species of odontocetes show sexual dimorphism, in which the males differ from the females, usually for purposes of sexual display or aggression. Toothed whales feed largely on fish and marine invertebrates.
Anatomy.
Cetacean bodies are generally similar to that of fish, which can be attributed to their lifestyle and the habitat conditions. Their body is well-adapted to their habitat, although they share essential characteristics with other higher mammals (Eutheria):
They have a streamlined shape, and their forelimbs are flippers. Almost all have a dorsal fin on their backs that can take on many forms depending on the species. A few species, such as the beluga whale, lack them. Both the flipper and the fin are for stabilization and steering in the water.
The male genitals and mammary glands of females are sunken into the body.
The body is wrapped in a thick layer of fat, known as blubber, used for thermal insulation and gives cetaceans their smooth, streamlined body shape. In larger species, it can reach a thickness up to half a meter (1.6Â ft).
Sexual dimorphism evolved in many toothed whales. Sperm whales, narwhals, many members of the beaked whale family, several species of the porpoise family, killer whales, pilot whales, eastern spinner dolphins and northern right whale dolphins show this characteristic. Males in these species developed external features absent in females that are advantageous in combat or display. For example, male sperm whales are up to 63% percent larger than females, and many beaked whales possess tusks used in competition among males.
Fluke.
They have a cartilaginous fluke at the end of their tails that is used for propulsion. The fluke is set horizontally on the body, unlike fish, which have vertical tails.
Hind legs are not present in cetaceans, nor are any other external body attachments such as a pinna and hair.
Head.
Whales have an elongated head, especially baleen whales, due to the wide overhanging jaw. Bowhead whale plates can be long. Their nostril(s) make up the blowhole, with one in toothed whales and two in baleen whales.
The nostrils are located on top of the head above the eyes so that the rest of the body can remain submerged while surfacing for air. The back of the skull is significantly shortened and deformed. By shifting the nostrils to the top of the head, the nasal passages extend perpendicularly through the skull. The teeth or baleen in the upper jaw sit exclusively on the maxilla. The braincase is concentrated through the nasal passage to the front and is correspondingly higher, with individual cranial bones that overlap. The bony otic capsule, the petrosal, is connected to the skull with cartilage, so that it can swing independently.
In toothed whales, connective tissue exists in the melon as a head buckle. This is filled with air sacs and fat that aid in buoyancy and biosonar. The sperm whale has a particularly pronounced melon; this is called the spermaceti organ and contains the eponymous spermaceti, hence the name "sperm whale". Even the long tusk of the narwhal is a vice-formed tooth. In many toothed whales, the depression in their skull is due to the formation of a large melon and multiple, asymmetric air bags.
River dolphins, unlike most other cetaceans, can turn their head 90Â°. Other cetaceans have fused neck vertebrae and are unable to turn their head at all.
The baleen of baleen whales consists of long, fibrous strands of keratin. Located in place of the teeth, it has the appearance of a huge fringe and is used to sieve the water for plankton and krill.
Brain.
The neocortex of many cetaceans is home to elongated spindle neurons that, prior to 2007, were known only in hominids. In humans, these cells are involved in social conduct, emotions, judgment and theory of mind. Cetacean spindle neurons are found in areas of the brain homologous to where they are found in humans, suggesting they perform a similar function.
Brain size was previously considered a major indicator of intelligence. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately two-thirds or three-quarter exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such an analysis provides an encephalization quotient that can be used as an indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males. The brain to body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans. In some whales, however, it is less than half that of humans: 0.9% versus 2.1%. The sperm whale ("Physeter macrocephalus") is the largest of all toothed predatory animals and possesses the largest brain.
Skeleton.
The cetacean skeleton is largely made up of cortical bone, which stabilizes the animal in the water. For this reason, the usual terrestrial compact bones, which are finely woven cancellous bone, are replaced with lighter and more elastic material. In many places, bone elements are replaced by cartilage and even fat, thereby improving their hydrostatic qualities. The ear and the muzzle contain a bone shape that is exclusive to cetaceans with a high density, resembling porcelain. This conducts sound better than other bones, thus aiding biosonar.
The number of vertebrae that make up the spine varies by species, ranging from forty to ninety-three. The cervical spine, found in all mammals, consists of seven vertebrae which, however, are reduced or fused. This gives stability during swimming at the expense of mobility. The fins are carried by the thoracic vertebrae, ranging from nine to seventeen individual vertebrae. The sternum is cartilaginous. The last two to three pairs of ribs are not connected and hang freely in the body wall. The stable lumbar and tail include the other vertebrae. Below the caudal vertebrae is the chevron bone; the vortex developed provides additional attachment points for the tail musculature.
The front limbs are paddle-shaped with shortened arms and elongated finger bones, to support movement. They are connected by cartilage. The second and third fingers display a proliferation of the finger members, a so-called hyperphalangy. The shoulder joint is the only functional join in all cetaceans except for the Amazon river dolphin. The collarbone is completely absent. The rear limbs are vestigial, without connections to the spine.
Physiology.
Circulation.
Cetaceans have powerful hearts. Blood oxygen is distributed effectively throughout the body. They are warm-blooded, i.e., they hold a nearly constant body temperature.
Respiration.
Cetaceans have lungs, meaning they breathe air. An individual can last without a breath from a few minutes to over two hours depending on the species. Cetacea are deliberate breathers who must be awake to inhale and exhale. When stale air, warmed from the lungs, is exhaled, it condenses as it meets colder external air. As with a terrestrial mammal breathing out on a cold day, a small cloud of 'steam' appears. This is called the 'spout' and varies across species in shape, angle and height. Species can be identified at a distance using this characteristic.
The structure of the respiratory and circulatory systems are of particular importance for the life of marine mammals. The oxygen balance is effective. Each breath can replace up to 90% of the total lung volume. For land mammals, in comparison, this value is usually about 15%. During inhalation, about twice as much oxygen is absorbed by the lung tissue as in a land mammal. As with all mammals, the oxygen is stored in the blood and the lungs, but in cetaceans, it is also stored in various tissues, mainly in the muscles. The muscle pigment, myoglobin, provides an effective bond. This additional oxygen storage is vital for deep diving, since beyond a depth around , the lung tissue is almost completely compressed by the water pressure.
Organs.
The stomach consists of three chambers. The first region is formed by a loose gland and a muscular forestomach (missing in beaked whales), which is then followed by the main stomach and the pylorus. Both are equipped with glands to help digestion. A bowel adjoins the stomachs, whose individual sections can only be distinguished histologically. The liver is large and separate from the gall bladder.
The kidneys are long and flattened. The salt concentration in cetacean blood is lower than that in seawater, requiring kidneys to excrete salt. This allows the animals to drink seawater.
Senses.
Cetacean eyes are set on the sides rather than the front of the head. This means only species with pointed 'beaks' (such as dolphins) have good binocular vision forward and downward. Tear glands secrete greasy tears, which protect the eyes from the salt in the water. The lens is almost spherical, which is most efficient at focusing the minimal light that reaches deep water. Cetaceans make up for their generally poor vision (except dolphins) with excellent hearing.
At least one species, the tucuxi or Guiana dolphin, is able to use electroreception to sense prey.
Teeth/baleen.
While the teeth are divided into incisors, canines and molars among terrestrial archaeocetes, the teeth of modern cetaceans are brought into line with each other, which can be seen among the fish-eating odontocetes (transition from heterodont to homodont).
Ears.
The external ear has lost the pinna (visible ear), but still retains a narrow external auditory meatus. To register sounds, instead, the posterior part of the mandible has a thin lateral wall (the pan bone) fronting a concavity that houses a fat pad. The pad passes anteriorly into the greatly enlarged mandibular foramen to reach in under the teeth and posteriorly to reach the thin lateral wall of the ectotympanic. The ectotympanic offers a reduced attachment area for the tympanic membrane. The connection between this auditory complex and the rest of the skull is reducedâto a single, small cartilage in oceanic dolphins.
In odontocetes, the complex is surrounded by spongy tissue filled with air spaces, while in mysticetes, it is integrated into the skull as with land mammals. In odontocetes, the tympanic membrane (or ligament) has the shape of a folded-in umbrella that stretches from the ectotympanic ring and narrows off to the malleus (quite unlike the flat, circular membrane found in land mammals.) In mysticetes, it also forms a large protrusion (known as the "glove finger"), which stretches into the external meatus and the stapes are larger than in odontocetes. In some small sperm whales, the malleus is fused with the ectotympanic.
The ear ossicles are pachyosteosclerotic (dense and compact) and differently shaped than land mammals (other aquatic mammals, such as sirenians and earless seals, have also lost their pinnae). T semicircular canals are much smaller relative to body size than in other mammals.
The auditory bulla is separated from the skull and composed of two compact and dense bones (the periotic and tympanic) referred to as the tympanoperiotic complex. This complex is located in a cavity in the middle ear, which, in the Mysticeti, is divided by a bony projection and compressed between the exoccipital and squamosal, but in the odontoceti, is large and completely surrounds the bulla (hence called "peribullar"), which is, therefore, not connected to the skull except in physeterids. In the Odontoceti, the cavity is filled with a dense foam in which the bulla hangs suspended in five or more sets of ligaments. The pterygoid and peribullar sinuses that form the cavity tend to be more developed in shallow water and riverine species than in pelagic Mysticeti. In Odontoceti, the composite auditory structure is thought to serve as an acoustic isolator, analogous to the lamellar construction found in the temporal bone in bats.
Cetaceans use sound to communicate, using groans, moans, whistles, clicks or the 'singing' of the humpback whale.
Echolocation.
Odontoceti are generally capable of echolocation. They can discern the size, shape, surface characteristics, distance and movement of an object. They can search for, chase and catch fast-swimming prey in total darkness. Most Odontoceti can distinguish between prey and nonprey (such as humans or boats); captive Odontoceti can be trained to distinguish between, for example, balls of different sizes or shapes.
Mysticeti have exceptionally thin, wide basilar membranes in their cochleae without stiffening agents, making their ears adapted for processing low to infrasonic frequencies. Echolocation clicks also contain characteristic details unique to each animal, which may suggest that toothed whales can discern between their own click and that of others.
Chromosomes.
The initial karyotype includes a set of chromosomes from 2n = 44. They have four pairs of telocentric chromosomes (whose centromeres sit at one of the telomeres), two to four pairs of subtelocentric and one or two large pairs of submetacentric chromosomes. The remaining chromosomes are metacentric - the centromere is approximately in the middle - and are rather small. Sperm whales, beaked whales and right whales converge to a reduction in the number of chromosomes to 2n = 42.
Ecology.
Range and habitat.
Cetaceans are found in all oceans. River dolphin species live exclusively in fresh water. While many marine species, such as the blue whale, the humpback whale and the killer whale, have a distribution area that includes nearly the entire ocean, some species occur only locally or in broken populations. These include the vacquita, which inhabits a small part of the Gulf of California and Hector's dolphin, which lives in some coastal waters in New Zealand. Both species prefer deeper marine areas and species that live frequently or exclusively in coastal and shallow water areas.
Many species inhabit specific latitudes, often in tropical or subtropical waters, such as Bryde's whale or Risso's dolphin. Others are found only in a specific body of water. The southern right whale dolphin and the hourglass dolphin live only in the Southern Ocean. The narwhal and the beluga live only in the Arctic Ocean. Sowerby's beaked whale and the Clymene dolphin exist only in the Atlantic and the Pacific white-sided dolphin and the northern straight dolphin live only in the North Pacific.
Cosmopolitan species may be found in the Pacific, Atlantic and Indian Oceans. However, northern and southern populations become genetically separated over time. In some species, this separation leads eventually to a divergence of the species, such as produced the southern right whale, North Pacific right whale and North Atlantic right whale. Migratory species' reproductive sites often lie in the tropics and their feeding grounds in polar regions.
Thirty-two species are found in European waters, including twenty-five toothed and seven baleen species.
Life history.
Sleep.
Conscious breathing cetaceans sleep, but cannot afford to be unconscious for long; because they may drown. While knowledge of sleep in wild cetaceans is limited, toothed cetaceans in captivity have been recorded to exhibit unihemispheric slow-wave sleep (USWS), which means they sleep with one side of their brain at a time, so that they may swim, breathe consciously and avoid both predators and social contact during their period of rest.
A 2008 study found that sperm whales sleep in vertical postures just under the surface in passive shallow 'drift-dives', generally during the day, during which whales do not respond to passing vessels unless they are in contact, leading to the suggestion that whales possibly sleep during such dives.
Diving.
While diving, the animals reduce their oxygen consumption by lowering the heart activity and blood circulation; individual organs receive no oxygen during this time. Some rorquals can dive for up to 40 minutes, sperm whales between 60 and 90 minutes and bottlenose whales for two hours. Diving depths average about . Species such as sperm whales can dive to , although more commonly .
Social relations.
Most whales are social animals, although a few species live in pairs or are solitary. A group, known as a pod, usually consists of ten to fifty animals, but on occasion, such as mass availability of food or during mating season, groups may encompass more than one thousand individuals. Inter-species socialization can occur.
Pods have a fixed hierarchy, with the priority positions determined by biting, pushing or ramming. The behavior in the group is aggressive only in situations of stress such as lack of food, but usually it is peaceful. Contact swimming, mutual fondling and nudging are common. The playful behavior of the animals, which is manifested in air jumps, somersaults, surfing, or fin hitting, occurs more often than not in smaller cetaceans, such as dolphins and porpoises.
Whale song.
Males in some baleen species communicate via whale song, sequences of high pitched sounds. These "songs" can be heard for hundreds of kilometers. Each population generally shares a distinct song, which evolves over time. Sometimes, an individual can be identified by its distinctive vocals, such as the 52-hertz whale that sings at a higher frequency than other whales. Some individuals are capable of generating over 600 distinct sounds. In baleen species such as humpbacks, blues and fins, male-specific song is believed to be used to attract and display fitness to females.
Hunting.
Pod groups also hunt, often with other species. Many species of dolphins hunt accompany large tunas on hunting expeditions, following large schools of fish. The killer whale hunts in pods and targets belugas and even larger whales. Humpback whales, among others, form in collaboration bubble carpets to herd krill or plankton into bait balls before lunging at them.
Intelligence.
Cetacea are known to teach, learn, cooperate, scheme and grieve.
Smaller cetaceans, such as dolphins and porpoises, engage in complex play behavior, including such things as producing stable underwater toroidal air-core vortex rings or "bubble rings". The two main methods of bubble ring production are rapid puffing of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex rings, so that they burst into many separate bubbles and then rise quickly to the surface. Whales produce bubble nets to aid in herding prey.
Larger whales are also thought to engage in play. The southern right whale elevates its tail fluke above the water, remaining in the same position for a considerable time. This is known as "sailing". It appears to be a form of play and is most commonly seen off the coast of Argentina and South Africa. Humpback whales also display this behaviour.
Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning (thinking about thinking) that humans exploit. Cetaceans appear to possess self-awareness. The most widely used test for self-awareness in animals is the mirror test, in which a temporary dye is placed on an animal's body and the animal is then presented with a mirror. Researchers then explore whether the animal shows signs of self-recognition.
Critics claim that the results of these tests are susceptible to the Clever Hans effect. This test is much less definitive than when used for primates. Primates can touch the mark or the mirror, while cetaceans cannot, making their alleged self-recognition behavior less certain. Skeptics argue that behaviors said to identify self-awareness resemble existing social behaviors, so researchers could be misinterpreting self-awareness for social responses. Advocates counter that the behaviors are different from normal responses to another individual. Cetaceans show less definitive behavior of self-awareness, because they have no pointing ability.
In 1995, Marten and Psarakos used video to test dolphin self-awareness. They showed dolphins real-time footage of themselves, recorded footage and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been replicated, dolphins later "passed" the mirror test.
Reproduction and brooding.
Most cetaceans sexually mature at seven to 10 years. An exception to this is the La Plata dolphin, which is sexually mature at two years, but lives only to about 20. The sperm whale reaches sexual maturity within about 20 years and a lifespan between 50 and 100 years.
For most species, reproduction is seasonal. Ovulation coincides with male fertility. This cycle is usually coupled with seasonal movements that can be observed in many species. Most toothed whales have no fixed bonds. In many species, females choose several partners during a season. Baleen whales are largely monogamous within each reproductive period.
Gestation ranges from 9 to 16 months. Duration is not necessarily a function of size. Porpoises and blue whales gestate for about 11 months. During gestation, the embryo is fed by a special nutritive tissue, the placenta.
Cetaceans usually bear one calf. In the case of twins, one usually dies, because the mother cannot produce sufficient milk for both. The fetus is stpositioned for a tail-first delivery, so that the risk of drowning during delivery is minimal. After birth, the mother carries the infant to the surface for its first breath. At birth they are about one-third of their adult length and tend to be independently active, comparable to terrestrial mammals.
Suckling.
Like other placental mammals, cetaceans give birth to well-developed calves and nurse them with milk from their mammary glands. When suckling, the mother actively splashes milk into the mouth of the calf, using the muscles of her mammary glands, as the calf has no lips. This milk usually has a high fat content, ranging from 16 to 46%, causing the calf to increase rapidly in size and weight.
In many small cetaceans, suckling lasts for about four months. In large species, it lasts for over a year and involves a strong bond between mother and offspring.
The mother is solely responsible for brooding. In some species, so-called "aunts" occasionally suckle the young.
This reproductive strategy provides a few offspring that have a high survival rate.
Lifespan.
Among cetaceans, whales are distinguished by an unusual longevity compared to other higher mammals. Some species, such as the bowhead whale ("Balaena mysticetus"), can reach over 200 years. Based on the annual rings of the bony otic capsule, the age of the oldest known specimen is a male determined to be 211 years at the time of death.
Death.
Upon death, whale carcasses fall to the deep ocean and provide a substantial habitat for marine life. Evidence of whale falls in present-day and fossil records shows that deep-sea whale falls support a rich assemblage of creatures, with a global diversity of 407 species, comparable to other neritic biodiversity hotspots, such as cold seeps and hydrothermal vents.
Deterioration of whale carcasses happens though three stages. Initially, organisms such as sharks and hagfish scavenge the soft tissues at a rapid rate over a period of months and as long as two years. This is followed by the colonization of bones and surrounding sediments (which contain organic matter) by enrichment opportunists, such as crustaceans and polychaetes, throughout a period of years. Finally, sulfophilic bacteria reduce the bones releasing hydrogen sulfide enabling the growth of chemoautotrophic organisms, which in turn, support organisms such as mussels, clams, limpets and sea snails. This stage may last for decades and supports a rich assemblage of species, averaging 185 per site.
Disease.
Brucellosis affects almost all mammals. It is distributed worldwide, while fishing and pollution have caused porpoise population density pockets, which risks further infection and disease spreading. "Brucella ceti", most prevalent in dolphins, has been shown to cause chronic disease, increasing the chance of failed birth and miscarriages, male infertility, neurobrucellosis, cardiopathies, bone and skin lesions, strandings and death. Until 2008, no case had ever been reported in porpoises, but isolated populations have an increased risk and consequentially a high mortality rate.
Evolution.
Phylogenetics.
Molecular biology and immunology show that cetaceans are phylogenetically closely related with the even-toed ungulates (Artiodactyla). Whales direct lineage began in the early Eocene, more than 50 million years ago, with early artiodactyls. Fossil discoveries at the beginning of the 21st century confirmed this.
Most molecular biological evidence suggests that hippos are the closest living relatives. Common anatomical features include similarities in the morphology of the posterior molars, and the bony ring on the temporal bone (bulla) and the involucre, a skull feature that was previously associated only with cetaceans. The fossil record, however, does not support this relationship, because the hippo lineage dates back only about 15 million years. The most striking common feature is the talus, a bone in the upper ankle. Early cetaceans, archaeocetes, show double castors, which only occur in even-toed ungulates. Corresponding findings are from Tethys Sea deposits in northern India and Pakistan. The Tethys Sea was a shallow sea between the Asian continent and northward-bound Indian plate.
Mysticetes evolved baleen around 25 million years ago and lost their teeth.
Development.
Ancestors.
The direct ancestors of today's cetaceans are probably found within the Dorudontidae whose most famous member, "Dorudon atrox", lived at the same time as "Basilosaurus". Both groups had already developed the typical anatomical features of today's whales, such as hearing. Life in the water for a formerly terrestrial creature required significant adjustments such as the fixed bulla, which replaces the mammalian eardrum, as well as sound-conducting elements for submerged directional hearing. Their wrists were stiffened and probably contributed to the typical build of flippers. The hind legs existed, however, but were significantly reduced in size and with a vestigial pelvis connection.
Transition from land to sea.
The fossil record traces the gradual transition from terrestrial to aquatic life. The regression of the hind limbs allowed greater flexibility of the spine. This made it possible for whales to move around with the vertical tail hitting the water. The front legs transformed into flippers, costing them their mobility on land.
One of the oldest members of ancient cetaceans (Archaeoceti) is "Pakicetus" from the Middle Eocene. This animal is the size of a wolf, whose skeleton is known only partially. It had functioning legs and lived near the shore. This suggests the animal could still move on land. The long snout had carnivorous dentition.
The transition from land to sea dates to about 49 million years ago, with the "Ambulocetus" ("running whale"), discovered in Pakistan. It was up to long. The limbs of this archaeocete were adapted to swimming, but terrestrial locomotion was still possible. It probably crawled like a seal or crocodile. The snout was elongated with overhead nostrils and eyes. The tail was strong and supported movement through water. "Ambulocetus" probably lived in mangroves in brackish water and fed in the riparian zone as a predator of fish and other vertebrates.
Dating from about 45 million years ago are species such as "Indocetus", "Kutchicetus", "Rodhocetus" and "Andrewsiphius", all of which were adapted to life in water. The hind limbs of these species were regressed and their body shapes resemble modern whales. Protocetidae family member "Rodhocetus" is considered the first to be fully aquatic. The body was streamlined and delicate with extended hand and foot bones. The merged pelvic lumbar spine was present, making it possible to support the floating movement of the tail. It was likely a good swimmer, but could probably move only clumsily on land, much like a modern seal.
Marine animals.
Since the late Eocene, about 40 million years ago, cetaceans populated the subtropical oceans and no longer emerged on land. An example is the 18-m-long "Basilosaurus", sometimes referred to as "Zeuglodon". The transition from land to water was completed in about 10 million years. The Wadi Al-Hitan ("Whale Valley") in Egypt contains numerous skeletons of "Basilosaurus", as well as other marine vertebrates.
Taxonomy.
Baleen whales (Mysticeti) owe their name to their baleen. Toothed whales (Odontoceti), which include the dolphins and porpoises, have conical teeth or spade-shaped teeth and can perceive their environment through biosonar.
The infraorder comprises the families Balaenidae (right and bowhead whales), Balaenoptera (rorquals), Eschrichtiidae (the gray whale), Delphinidae (oceanic dolphins), Monodontidae (Arctic whales), Phocoenidae (porpoises), Physeteridae (sperm whales), Kogiidae (lesser sperm whales), Platanistidae (Old World river dolphins), Iniidae (New World river dolphins), Pontoporiidae (the La plata dolphin) and Ziphidae (beaked whales).
â Recently extinct
Status.
Threats.
The primary threats to cetaceans come from people, both directly from whaling or drive hunting and indirect threats from fishing and pollution.
Whaling.
Whaling is the practice of hunting whales, mainly baleen and sperm whales. This activity has gone on since the Stone Age.
In the Middle Ages, reasons for whaling included their meat, oil usable as fuel and the jawbone, which was used in house construction. At the end of the Middle Ages, early whaling fleets aimed at baleen whales, such as bowheads. In the 16th and 17th centuries, the Dutch fleet had about 300 whaling ships with 18,000 crewmen.
In the 18th and 19th centuries, baleen whales especially were hunted for their baleen, which was used as a replacement for wood, or in products requiring strength and flexibility such as corsets and crinoline skirts. In addition, the spermaceti found in the sperm whale was used as a machine lubricant and the ambergris as a material for pharmaceutical and perfume industries. In the second half of the 19th century, the explosive harpoon was invented, leading to a massive increase in the catch size.
Large ships were used as "mother" ships for the whale handlers. In the first half of the 20th century, whales were of great importance as a supplier of raw materials. Whales were intensively hunted during this time; in the 1930s, 30,000 whales were killed. This increased to over 40,000 animals per year up to the 1960s, when stocks of large baleen whales collapsed.
Most hunted whales are now threatened, with some great whale populations exploited to the brink of extinction. Atlantic and Korean gray whale populations were completely eradicated and the North Atlantic right whale population fell to some 300-600. The blue whale population is estimated to be around 14,000.
The first efforts to protect whales came in 1931. Some particularly endangered species, such as the humpback whale (which then numbered about 100 animals), were placed under international protection and the first protected areas were established. In 1946, the International Whaling Commission (IWC) was established, to monitor and secure whale stocks. Whaling for commercial purposes was prohibited worldwide by this organization from 1985 to 2005.
The stocks of species such as humpback and blue whales have recovered, though they are still threatened. The United States Congress passed the Marine Mammal Protection Act of 1972 sustain the marine mammal population. It prohibits the taking of marine mammals. Japanese whaling ships are allowed to hunt whales of different species for ostensibly scientific purposes. Aboriginal whaling is still permitted, but under limited circumstances as defined by IWC. Iceland and Norway do not recognize the ban and operate commercial whaling. Norway and Japan are committed to ending the ban.
Dolphins and other smaller cetaceans are hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats, usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru and Japan (the most well-known practitioner). Dolphins are mostly hunted for their meat, though some end up in dolphinaria. Despite the controversy thousands of dolphins are caught in drive hunts each year.
Fishing.
Dolphin pods often reside near large tuna shoals. This is known to fishermen, who look for dolphins to catch tuna. Dolphins are much easier to spot from a distance than tuna, since they regularly breathe. The fishermen pull their nets hundreds of meters wide in a circle around the dolphin groups, in the expectation that they will net a tuna shoal. When the nets are pulled together, the dolphins become entangled under water and drown. Line fisheries in larger rivers are threats to river dolphins.
A greater threat than by-catch for small cetaceans is targeted hunting. In Southeast Asia, they are sold as fish-replacement to locals, since the region's edible fish promise higher revenues from exports. In the Mediterranean, small cetaceans are targeted to ease pressure on edible fish.
Strandings.
A stranding is when a cetacean leaves the water to lie on a beach. In some cases, groups of whales strand together. The best known are mass strandings of pilot whales and sperm whales. Stranded cetaceans usually die, because their as much as body weight compresses their lungs or breaks their ribs. Smaller whales can die of heatstroke because of their thermal insulation.
The causes are not clear. Possible reasons for mass beachings are:
Since 2000, whale strandings frequently occurred following military sonar testing. In December 2001, the US Navy admitted partial responsibility for the beaching and the deaths of several marine mammals in March 2000. The coauthor of the interim report stated that animals killed by active sonar of some Navy ships were injured. Generally, underwater noise, which is still on the increase, is increasingly tied to strandings; because it impairs communication and sense of direction.
Climate change influences the major wind systems and ocean currents, which also lead to cetacean strandings. Researchers studying strandings on the Tasmanian coast from 1920-2002 found that greater strandings occurred at certain time intervals. Years with increased strandings were associated with severe storms, which initiated cold water flows close to the coast. In nutrient-rich, cold water, cetaceans expect large prey animals, so they follow the cold water currents into shallower waters, where the risk is higher for strandings. Whales and dolphins who live in pods may accompany sick or debilitated pod members into shallow water, stranding them at low tide. Once stranded, large whales are crushed by their own body weight, if they cannot quickly return to the water. In addition, body temperature regulation is compromised.
Environmental hazards.
Heavy metals, residues of many plant and insect venoms and plastic waste flotsam are not biodegradable. Sometimes, cetaceans consume these hazardous materials, mistaking them for food items. As a result, the animals are more susceptible to disease and have fewer offspring.
Damage to the ozone layer reduces plankton reproduction because of its resulting radiation. This shrinks the food supply for many marine animals, but the filter-feeding baleen whales are most impacted. Even the Nekton is, in addition to intensive exploitation, damaged by the radiation.
Food supplies are also reduced long-term by ocean acidification due to increased absorption of increased atmospheric carbon dioxide. The CO2 reacts with water to form carbonic acid, which reduces the construction of the calcium carbonate skeletons of food supplies for zooplankton that baleen whales depend on.
The military and resource extraction industries operate strong sonar and blasting operations. Vessel traffic also increases noise in the oceans. Such noise can disrupt cetacean behavior such as their use of biosonar for orientation and communication. Sever instances can panic them, driving them to the surface. This leads to bubbles in blood gases and can cause decompression sickness. Naval exercises with sonar regularly results in fallen cetaceans that wash up with fatal decompression. Sounds can be disruptive at distances of more than . Damage varies across frequency and species.
Relationship to humans.
Research history.
In Aristotle's time, the 4th century BCE, whales were regarded as fish due to their superficial similarity. Aristotle, however, observed many physiological and anatomical similarities with the terrestrial vertebrates, such as blood (circulation), lungs, uterus and fin anatomy. His detailed descriptions were assimilated by the Romans, but mixed with a more accurate knowledge of the dolphins, as mentioned by Pliny the Elder in his "Natural history". In the art of this and subsequent periods, dolphins are portrayed with a high-arched head (typical of porpoises) and a long snout. The harbour porpoise was one of the most accessible species for early cetologists; because it could be seen close to land, inhabiting shallow coastal areas of Europe. Much of the findings that apply to all cetaceans were first discovered in porpoises. One of the first anatomical descriptions of the airways of a harbor porpoise dates from 1671 by John Ray. It nevertheless referred to the porpoise as a fish.
In the 10th edition of Systema Naturae (1758), Swedish biologist and taxonomist Carl Linnaeus asserted that cetaceans were mammals and not fish. His groundbreaking binomial system formed the basis of modern whale classification.
Culture.
Cetaceans play a role in human culture.
Prehistoric.
Stone Age petroglyphs , such as those in Roddoy and Reppa (Norway), depict them. Whale bones were used for many purposes. In the Neolithic settlement of Skara Brae on Orkney sauce pans were made from whale vertebrae.
Antiquity.
The whale was first mentioned in ancient Greece by Homer. There, it is called Ketos, a term that initially included all large marine animals. From this was derived the Roman word for whale, Cetus. Other names were phÃ¡laina (Aristotle, Latin form of ballaena) for the female and, with an ironic characteristic style, musculus (Mouse) for the male. North Sea whales were called Physeter, which was meant for the sperm whale "Physter macrocephalus". Whales are described in particular by Aristotle, Pliny and Ambrose. All mention both live birth and suckling. Pliny describes the problems associated with the lungs with spray tubes and Ambrose claimed that large whales would take their young into their mouth to protect them.
In the Bible especially, the leviathan plays a role as a sea monster. The essence, which features a giant crocodile or a dragon and a whale, was created according to the Bible by God () and should again be destroyed by him ( and ). In the Book of Job, the leviathan is described in more detail ( to ).
In - is a more recognizable description of a whale alongside the prophet Jonah, who, on his flight from the city of Nineveh is swallowed by a whale.
Dolphins are mentioned far more often than whales. Aristotle discusses the sacred animals of the Greeks in his "Historia Animalium" and gives details of their role as aquatic animals. The Greeks admired the dolphin as a "king of the aquatic animals" and referred to them erroneously as fish. Its intelligence was apparent both in its ability to escape from fishnets and in its collaboration with fishermen.
River dolphins are known from the Ganges and - erroneously - the Nile. In the latter case it was equated with sharks and catfish. Supposedly they attacked even crocodiles.
Dolphins appear in Greek mythology. Because of their intelligence, they rescued multiple people from drowning. They were said to love music - probably not least because of their own song - they saved, in the legends, famous musicians such as Arion of Lesbos from Methymna or Kairanos from Miletus. Because of their mental faculties, dolphins were considered for the god Dionysus.
Dolphins belong to the domain of Poseidon and led him to his wife Amphitrite. Dolphins are associated with other gods, such as Apollo, Dionysus and Aphrodite. The Greeks paid tribute to both whales and dolphins with their own constellation. The constellation of the Whale (Ketos, lat. Cetus) is located south of the Dolphin (Delphi, lat. Delphinus) north of the zodiac.
Ancient art often included dolphin representations, including the Cretan Minoans. Later they appeared on reliefs, gems, lamps, coins, mosaics and gravestones. A particularly popular representation is that of Arion or the Taras (mythology) riding on a dolphin. In early Christian art, the dolphin is a popular motif, at times used as a symbol of Christ.
Middle Ages to the 19th century.
St. Brendan described in his travel story "Navigatio Sancti Brendani" an encounter with a whale, between the years 565-573. He described how he and his companions entered a treeless island, which turned out to be a giant whale, which he called Jasconicus. He met this whale seven years later and rested on his back.
Most descriptions of large whales from this time to the whaling era, starting in the 17th century. They came from beached whales, which resembled no other animal. This was particularly true for the sperm whale, the most frequently stranded in larger groups. Raymond Gilmore documented seventeen sperm whales in the estuary of the Elbe from 1723 to 1959 and thirty-one animals on the coast of Great Britain in 1784. In 1827, a blue whale beached itself off the coast of Ostend. Whales were used as attractions in museums and traveling exhibitions.
Whalers in the 17-19th centuries depicted whales in drawings and recounted tales of their occupation. Although they knew that whales were harmless giants, they described battles with harpooned animals. These included descriptions of sea monsters, including huge whales, sharks, sea snakes, giant squid and octopuses.
Among the first whalers who described their experiences on whaling trips was Captain William Scoresby from Great Britain, who published the book "Northern Whale Fishery", describing the hunt for northern baleen whales. This was followed by Thomas Beale, a British surgeon, in his book "Some observations on the natural history of the sperm whale" in 1835; and Frederick Debell Bennett's "The tale of a whale hunt" in 1840. Whales were described in narrative literature and paintings, most famously in the novels "Moby Dick" by Herman Melville and "20,000 Leagues Under the Sea" by Jules Verne. In the 1882 children's book "Adventures of Pinocchio" by Carlo Collodi, the wooden figures Pinocchio and Geppettos' creators were swallowed by a whale.
Baleen was used to make vessel components such as the bottom of a bucket in the Scottish National Museum. The Norse crafted ornamented plates from baleen, sometimes interpreted as ironing boards.
In the Canadian Arctic (east coast) in Punuk and Thule culture (1000-1600 C.E.), I baleen was used to construct houses in place of wood as roof support for winter houses, with half of the building buried under the ground. The actual roof was probably made of animal skins that were covered with soil and moss.
Modern culture.
In the 20th century perceptions of cetaceans changed. They transformed from monsters into objects of wonder. As science revealed them to be intelligent and peaceful animals. Hunting was replaced by whale and dolphin tourism. This change is reflected in films and novels. For example, the protagonist of the series Flipper was a bottle-nose dolphin. The TV series SeaQuest DSV (1993-1996), the movies Free Willy, and the book series The Hitchhiker's Guide to the Galaxy by Douglas Adams are examples.
The study of whale song also produced a popular Judy Collins album, Songs of the Humpback Whale.
Captivity.
Whales and dolphins have been kept in captivity for use in education, research and entertainment since the 19th century.
Belugas.
Beluga whales were the first whales to be kept in captivity. Other species were too rare, too shy or too big. The first was shown at Barnum's Museum in New York City in 1861. For most of the 20th century, Canada was the predominant source. They were taken from the St. Lawrence River estuary until the late 1960s, after which they were predominantly taken from the Churchill River estuary until capture was banned in 1992. Russia then became the largest provider. Belugas are caught in the Amur Darya delta and their eastern coast and are transported domestically to aquaria or dolphinaria in Moscow, St. Petersburg and Sochi, or exported to countries such as Canada. They have not been domesticated.
As of 2006, 30 belugas lived in Canada and 28 in the United States. 42 deaths in captivity had been reported. A single specimen can reportedly fetch up to US$100,000 (UKÂ£64,160). The beluga's popularity is due to its unique color and its facial expressions. The latter is possible because while most cetacean "smiles" are fixed, the extra movement afforded by the beluga's unfused cervical vertebrae allows a greater range of apparent expression.
Killer whales.
The killer whale's intelligence, trainability, striking appearance, playfulness in captivity and sheer size have made it a popular exhibit at aquaria and aquatic theme parks. From 1976 to 1997, fifty-five whales were taken from the wild in Iceland, nineteen from Japan and three from Argentina. These figures exclude animals that died during capture. Live captures fell dramatically in the 1990s and by 1999, about 40% of the forty-eight animals on display in the world were captive-born.
Organizations such as World Animal Protection and the Whale and Dolphin Conservation Society campaign against the practice of keeping them in captivity.
In captivity, they often develop pathologies, such as the dorsal fin collapse seen in 60â90% of captive males. Captives have reduced life expectancy, on average only living into their 20s, although some live longer, including several over 30 years old and two, Corky II and Lolita, in their mid-40s. In the wild, females who survive infancy live 46 years on average and up to 70â80 years. Wild males who survive infancy live 31 years on average and can reach 50â60 years.
Captivity usually bears little resemblance to wild habitat and captive whales' social groups are foreign to those found in the wild. Critics claim captive life is stressful due to these factors and the requirement to perform circus tricks that are not part of wild killer whale behavior. Wild killer whales may travel up to in a day and critics say the animals are too big and intelligent to be suitable for captivity. Captives occasionally act aggressively towards themselves, their tankmates, or humans, which critics say is a result of stress. Killer whales are well known for their performances in shows, but the number of orcas kept in captivity is small, especially when compared to the number of bottlenose dolphins, with only forty-four captive orcas being held in aquaria as of 2012.
Each country has their own tank requirements; in the US, the minimum enclosure size is set by the Code of Federal Regulations, 9 CFR E Â§ 3.104, under the "Specifications for the Humane Handling, Care, Treatment and Transportation of Marine Mammals".
Aggression among captive killer whales is common. They attack each other and their trainers as well. In 2013, SeaWorld's treatment of killer whales in captivity was the basis of the movie "Blackfish", which documents the history of Tilikum, a killer whale at SeaWorld Orlando, who had been involved in the deaths of three people. The film was a sensation, leading the company to announce in 2016 that it would phase out its killer whale program after various unsuccessful attempts to restore its reputation and stock price.
Others.
Dolphins and porpoises are kept in captivity. Bottlenose dolphins are the most common, as they are relatively easy to train, have a long lifespan in captivity and have a friendly appearance. Bottlenose dolphins live in captivity across the world, though exact numbers are hard to determine. Other species kept in captivity are spotted dolphins, false killer whales and common dolphins, Commerson's dolphins, as well as rough-toothed dolphins, but all in much lower numbers. There are also fewer than ten pilot whales, Amazon river dolphins, Risso's dolphins, spinner dolphins, or tucuxi in captivity. Two unusual and rare hybrid dolphins, known as wolphins, are kept at Sea Life Park in Hawaii, which is a cross between a bottlenose dolphin and a false killer whale. Also, two xommon/bottlenose hybrids reside in captivity at Discovery Cove and SeaWorld San Diego.
In repeated attempts in the 1960s and 1970s, narwhals kept in captivity died within months. A breeding pair of pygmy right whales were retained in a netted area). They were eventually released in South Africa. In 1971, SeaWorld captured a California gray whale calf in Mexico at Scammon's Lagoon. The calf, later named Gigi, was separated from her mother using a form of lasso attached to her flukes. Gigi was displayed at SeaWorld San Diego for a year. She was then released with a radio beacon affixed to her back; however, contact was lost after three weeks. Gigi was the first captive baleen whale. JJ, another gray whale calf, was kept at SeaWorld San Diego. JJ was an orphaned calf that beached itself in April 1997 and was transported two miles to SeaWorld. The calf was a popular attraction and behaved normally, despite separation from his mother. A year later, the then whale though smaller than average, was too big to keep in captivity, and was released on April 1, 1998. A captive Amazon river dolphin housed at Acuario de Valencia is the only river dolphin in captivity.

</doc>
<doc id="7627" url="https://en.wikipedia.org/wiki?curid=7627" title="The Canterbury Tales">
The Canterbury Tales

The Canterbury Tales (Middle English: "Tales of Caunterbury") is a collection of 24 stories that runs to over 17,000 lines written in Middle English by Geoffrey Chaucer. In 1386 Chaucer became Controller of Customs and Justice of Peace and then three years later in 1389 Clerk of the King's work. It was during these years that Chaucer began working on his most famous text, "The Canterbury Tales". The tales (mostly written in verse, although some are in prose) are presented as part of a story-telling contest by a group of pilgrims as they travel together on a journey from London to Canterbury in order to visit the shrine of Saint Thomas Becket at Canterbury Cathedral. The prize for this contest is a free meal at the Tabard Inn at Southwark on their return.
After a long list of works written earlier in his career, including "Troilus and Criseyde", "House of Fame", and "Parliament of Fowls", "The Canterbury Tales" is near-unanimously seen as Chaucer's magnum opus. He uses the tales and the descriptions of its characters to paint an ironic and critical portrait of English society at the time, and particularly of the Church. Chaucer's use of such a wide range of classes and types of people was without precedent in English. Although the characters are fictional, they still offer a variety of insights into the customs and practices of the time. Often, such insight leads to a variety of discussions and disagreements among people in the 14th century. For example, although various social classes are represented in these stories and all of the pilgrims are on a spiritual quest, it is apparent that they are more concerned with worldly things than spiritual. Structurally, the collection resembles "The Decameron", which Chaucer may have read during his first diplomatic mission to Italy in 1372.
It is sometimes argued that the greatest contribution "The Canterbury Tales" made to English literature was in popularising the literary use of the vernacular, English, rather than French, Italian or Latin. English had, however, been used as a literary language centuries before Chaucer's time, and several of Chaucer's contemporariesâJohn Gower, William Langland, the Pearl Poet, and Julian of Norwichâalso wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend as opposed to simply being part of it.
While Chaucer clearly states the addressees of many of his poems, the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for nobility.
"The Canterbury Tales" was far from complete at the end of Chaucer's life. In the General Prologue, some thirty pilgrims are introduced. According to the Prologue, Chaucer's intention was to write two stories from the perspective of each pilgrim on the way to and from their ultimate destination, St. Thomas Becket's shrine (making for a total of four stories per pilgrim). Although perhaps incomplete, "The Canterbury Tales" is revered as one of the most important works in English literature. Not only do readers from all time periods find it entertaining, but it is also a work that is open to a range of interpretations.
Text.
The question of whether "The Canterbury Tales" is finished has not yet been answered. There are 83 known manuscripts of the work from the late medieval and early Renaissance periods, more than any other vernacular literary text with the exception of "The Prick of Conscience". This is taken as evidence of the tales' popularity during the century after Chaucer's death. Fifty-five of these manuscripts are thought to have been complete at one time, while 28 are so fragmentary that it is difficult to ascertain whether they were copied individually or as part of a set. The "Tales" vary in both minor and major ways from manuscript to manuscript; many of the minor variations are due to copyists' errors, while others suggest that Chaucer added to and revised his work as it was being copied and (possibly) distributed.
Even the earliest surviving manuscripts are not Chaucer's originals, the oldest being MS Peniarth 392 D (called "Hengwrt"), compiled by a scribe shortly after Chaucer's death. The most beautiful of the manuscripts of the tales is the Ellesmere Manuscript, and many editors have followed the order of the Ellesmere over the centuries, even down to the present day. The first version of "The Canterbury Tales" to be published in print was William Caxton's 1478 edition. Only 10 copies of this edition are known to exist, including one held by the British Library and one held by the Folger Shakespeare Library. Since this print edition was created from a now-lost manuscript, it is counted as among the 83 manuscripts. In 2004, Professor Linne Mooney was able to identify the scrivener who worked for Chaucer as an Adam Pinkhurst. Mooney, then a professor at the University of Maine and a visiting fellow at Corpus Christi College, Cambridge, was able to match Pinkhurst's signature, on an oath he signed, to his lettering on a copy of "The Canterbury Tales" that was transcribed from Chaucer's working copy.
Order.
No authorial, arguably complete version of the "Tales" exists and no consensus has been reached regarding the order in which Chaucer intended the stories to be placed.
Textual and manuscript clues have been adduced to support the two most popular modern methods of ordering the tales. Some scholarly editions divide the "Tales" into ten "Fragments". The tales that make up a Fragment are closely related and contain internal indications of their order of presentation, usually with one character speaking to and then stepping aside for another character. However, between Fragments, the connection is less obvious. Consequently, there are several possible orders; the one most frequently seen in modern editions follows the numbering of the Fragments (ultimately based on the Ellesmere order). Victorians frequently used the nine "Groups", which was the order used by Walter William Skeat whose edition "Chaucer: Complete Works" was used by Oxford University Press for most of the twentieth century, but this order is now seldom followed.
An alternative ordering (seen in an early manuscript containing "The Canterbury Tales", the early-fifteenth century Harley MS. 7334) places Fragment VIII before VI. Fragments I and II almost always follow each other, just as VI and VII, IX and X do in the oldest manuscripts. Fragments IV and V, by contrast, vary in location from manuscript to manuscript.
Language.
Chaucer wrote in late Middle English, which has clear differences from Modern English. From philological research, we know certain facts about the pronunciation of English during the time of Chaucer. Chaucer pronounced "-e" at the end of words, so that "care" was , not as in Modern English. Other silent letters were also pronounced, so that the word "knight" was , with both the "k" and the "gh" pronounced, not . In some cases, vowel letters in Middle English were pronounced very differently from Modern English, because the Great Vowel Shift had not yet happened. For instance, the long "e" in "wepyng" "weeping" was pronounced as , as in modern German or Italian, not as . Below is an IPA transcription of the opening lines of "The Merchant's Prologue":
Although no manuscript exists in Chaucer's own hand, two were copied around the time of his death by Adam Pinkhurst, a scribe with whom he seems to have worked closely before, giving a high degree of confidence that Chaucer himself wrote the "Tales". Because final "-e" was lost soon after Chaucer's time, scribes did not accurately copy it, and this gave scholars the impression that Chaucer himself was inconsistent in using it. It has now been established, however, that "-e" was an important part of Chaucer's grammar, and helped to distinguish singular adjectives from plural and subjunctive verbs from indicative.
Sources.
No other work prior to Chaucer's is known to have set a collection of tales within the framework of pilgrims on a pilgrimage. It is obvious, however, that Chaucer borrowed portions, sometimes very large portions, of his stories from earlier stories, and that his work was influenced by the general state of the literary world in which he lived. Storytelling was the main entertainment in England at the time, and storytelling contests had been around for hundreds of years. In 14th-century England the English Pui was a group with an appointed leader who would judge the songs of the group. The winner received a crown and, as with the winner of "The Canterbury Tales", a free dinner. It was common for pilgrims on a pilgrimage to have a chosen "master of ceremonies" to guide them and organise the journey. Harold Bloom suggests that the structure is mostly original, but inspired by the "pilgrim" figures of Dante and Virgil in "The Divine Comedy".
"The Decameron" by Giovanni Boccaccio contains more parallels to "The Canterbury Tales" than any other work. Like the "Tales", it features a number of narrators who tell stories along a journey they have undertaken (to flee from the Black Death). It ends with an apology by Boccaccio, much like Chaucer's Retraction to the "Tales". A quarter of the tales in "The Canterbury Tales" parallel a tale in the "Decameron", although most of them have closer parallels in other stories. Some scholars thus find it unlikely that Chaucer had a copy of the work on hand, surmising instead that he must have merely read the "Decameron" at some point. Each of the tales has its own set of sources that have been suggested by scholars, but a few sources are used frequently over several tales. They include poetry by Ovid, the Bible in one of the many vulgate versions in which it was available at the time (the exact one is difficult to determine), and the works of Petrarch and Dante. Chaucer was the first author to utilise the work of these last two, both Italians. Boethius' "Consolation of Philosophy" appears in several tales, as the works of John Gower do. Gower was a known friend to Chaucer. A full list is impossible to outline in little space, but Chaucer also, lastly, seems to have borrowed from numerous religious encyclopaedias and liturgical writings, such as John Bromyard's "Summa praedicantium", a preacher's handbook, and Jerome's "Adversus Jovinianum". Many scholars say there is a good possibility Chaucer met Petrarch or Boccaccio.
Genre and structure.
"The Canterbury Tales" is a collection of stories built around a frame narrative or frame tale, a common and already long established genre of its period. Chaucer's "Tales" differs from most other story "collections" in this genre chiefly in its intense variation. Most story collections focused on a theme, usually a religious one. Even in the "Decameron", storytellers are encouraged to stick to the theme decided on for the day. The idea of a pilgrimage to get such a diverse collection of people together for literary purposes was also unprecedented, though "the association of pilgrims and storytelling was a familiar one". Introducing a competition among the tales encourages the reader to compare the tales in all their variety, and allows Chaucer to showcase the breadth of his skill in different genres and literary forms.
While the structure of the "Tales" is largely linear, with one story following another, it is also much more than that. In the "General Prologue", Chaucer describes not the tales to be told, but the people who will tell them, making it clear that structure will depend on the characters rather than a general theme or moral. This idea is reinforced when the Miller interrupts to tell his tale after the Knight has finished his. Having the Knight go first gives one the idea that all will tell their stories by class, with the Monk following the Knight. However, the Miller's interruption makes it clear that this structure will be abandoned in favour of a free and open exchange of stories among all classes present. General themes and points of view arise as the characters tell their tales, which are responded to by other characters in their own tales, sometimes after a long lapse in which the theme has not been addressed.
Lastly, Chaucer does not pay much attention to the progress of the trip, to the time passing as the pilgrims travel, or to specific locations along the way to Canterbury. His writing of the story seems focused primarily on the stories being told, and not on the pilgrimage itself.
Style.
The variety of Chaucer's tales shows the breadth of his skill and his familiarity with many literary forms, linguistic styles, and rhetorical devices. Medieval schools of rhetoric at the time encouraged such diversity, dividing literature (as Virgil suggests) into high, middle, and low styles as measured by the density of rhetorical forms and vocabulary. Another popular method of division came from St. Augustine, who focused more on audience response and less on subject matter (a Virgilian concern). Augustine divided literature into "majestic persuades", "temperate pleases", and "subdued teaches". Writers were encouraged to write in a way that kept in mind the speaker, subject, audience, purpose, manner, and occasion. Chaucer moves freely between all of these styles, showing favouritism to none. He not only considers the readers of his work as an audience, but the other pilgrims within the story as well, creating a multi-layered rhetorical puzzle of ambiguities. Thus Chaucer's work far surpasses the ability of any single medieval theory to uncover.
With this, Chaucer avoids targeting any specific audience or social class of readers, focusing instead on the characters of the story and writing their tales with a skill proportional to their social status and learning. However, even the lowest characters, such as the Miller, show surprising rhetorical ability, although their subject matter is more lowbrow. Vocabulary also plays an important part, as those of the higher classes refer to a woman as a "lady", while the lower classes use the word "wenche", with no exceptions. At times the same word will mean entirely different things between classes. The word "pitee", for example, is a noble concept to the upper classes, while in the "Merchant's Tale" it refers to sexual intercourse. Again, however, tales such as the "Nun's Priest's Tale" show surprising skill with words among the lower classes of the group, while the "Knight's Tale" is at times extremely simple.
Chaucer uses the same meter throughout almost all of his tales, with the exception of "Sir Thopas" and his prose tales. It is a decasyllable line, probably borrowed from French and Italian forms, with riding rhyme and, occasionally, a caesura in the middle of a line. His meter would later develop into the heroic meter of the 15th and 16th centuries and is an ancestor of iambic pentameter. He avoids allowing couplets to become too prominent in the poem, and four of the tales (the Man of Law's, Clerk's, Prioress', and Second Nun's) use rhyme royal.
Historical context and themes.
"The Canterbury Tales" was written during a turbulent time in English history. The Catholic Church was in the midst of the Western Schism and, though it was still the only Christian authority in Europe, was the subject of heavy controversy. Lollardy, an early English religious movement led by John Wycliffe, is mentioned in the "Tales", which also mention a specific incident involving pardoners (who gathered money in exchange for absolution from sin) who nefariously claimed to be collecting for St. Mary Rouncesval hospital in England. "The Canterbury Tales" is among the first English literary works to mention paper, a relatively new invention that allowed dissemination of the written word never before seen in England. Political clashes, such as the 1381 Peasants' Revolt and clashes ending in the deposing of King Richard II, further reveal the complex turmoil surrounding Chaucer in the time of the "Tales"' writing. Many of his close friends were executed and he himself moved to Kent to get away from events in London.
While some readers look to interpret the characters of "The Canterbury Tales" as historical figures, other readers choose to interpret its significance in less literal terms. After analysis of Chaucer's diction and historical context, his work appears to develop a critique of society during his lifetime. Within a number of his descriptions, his comments can appear complimentary in nature, but through clever language, the statements are ultimately critical of the pilgrim's actions. It is unclear whether Chaucer would intend for the reader to link his characters with actual persons. Instead, it appears that Chaucer creates fictional characters to be general representations of people in such fields of work. With an understanding of medieval society, one can detect subtle satire at work.
Religion.
The "Tales" reflect diverse views of the Church in Chaucer's England. After the Black Death, many Europeans began to question the authority of the established Church. Some turned to lollardy, while others chose less extreme paths, starting new monastic orders or smaller movements exposing church corruption in the behaviour of the clergy, false church relics or abuse of indulgences. Several characters in the "Tales" are religious figures, and the very setting of the pilgrimage to Canterbury is religious (although the prologue comments ironically on its merely seasonal attractions), making religion a significant theme of the work.
Two characters, the Pardoner and the Summoner, whose roles apply the Church's secular power, are both portrayed as deeply corrupt, greedy, and abusive. A pardoner in Chaucer's day was a person from whom one bought Church "indulgences" for forgiveness of sins, but pardoners were often thought guilty of abusing their office for their own gain. Chaucer's Pardoner openly admits the corruption of his practice while hawking his wares. The Summoner is a Church officer who brought sinners to the Church court for possible excommunication and other penalties. Corrupt summoners would write false citations and frighten people into bribing them to protect their interests. Chaucer's Summoner is portrayed as guilty of the very kinds of sins for which he is threatening to bring others to court, and is hinted as having a corrupt relationship with the Pardoner. In The Friar's Tale, one of the characters is a summoner who is shown to be working on the side of the devil, not God.
Churchmen of various kinds are represented by the Monk, the Prioress, the Nun's Priest, and the Second Nun. Monastic orders, which originated from a desire to follow an ascetic lifestyle separated from the world, had by Chaucer's time become increasingly entangled in worldly matters. Monasteries frequently controlled huge tracts of land on which they made significant sums of money, while peasants worked in their employ. The Second Nun is an example of what a Nun was expected to be: her tale is about a woman whose chaste example brings people into the church. The Monk and the Prioress, on the other hand, while not as corrupt as the Summoner or Pardoner, fall far short of the ideal for their orders. Both are expensively dressed, show signs of lives of luxury and flirtatiousness and show a lack of spiritual depth. The Prioress's Tale is an account of Jews murdering a deeply pious and innocent Christian boy, a blood libel against Jews that became a part of English literary tradition. The story did not originate in the works of Chaucer and was well known in the 14th century.
Pilgrimage was a very prominent feature of medieval society. The ultimate pilgrimage destination was Jerusalem, but within England Canterbury was a popular destination. Pilgrims would journey to cathedrals that preserved relics of saints, believing that such relics held miraculous powers. Saint Thomas Becket, Archbishop of Canterbury, had been murdered in Canterbury Cathedral by knights of Henry II during a disagreement between Church and Crown. Miracle stories connected to his remains sprang up soon after his death, and the cathedral became a popular pilgrimage destination. The pilgrimage in the work ties all of the stories together and may be considered a representation of Christians' striving for heaven, despite weaknesses, disagreement, and diversity of opinion.
Social class and convention.
The upper class or nobility, represented chiefly by the Knight and his Squire, was in Chaucer's time steeped in a culture of chivalry and courtliness. Nobles were expected to be powerful warriors who could be ruthless on the battlefield yet mannerly in the King's Court and Christian in their actions. Knights were expected to form a strong social bond with the men who fought alongside them, but an even stronger bond with a woman whom they idealised to strengthen their fighting ability. Though the aim of chivalry was to noble action, its conflicting values often degenerated into violence. Church leaders frequently tried to place restrictions on jousts and tournaments, which at times ended in the death of the loser. The Knight's Tale shows how the brotherly love of two fellow knights turns into a deadly feud at the sight of a woman whom both idealise. To win her, both are willing to fight to the death. Chivalry was in Chaucer's day on the decline, and it is possible that The Knight's Tale was intended to show its flaws, although this is disputed. Chaucer himself had fought in the Hundred Years' War under Edward III, who heavily emphasised chivalry during his reign. Two tales, "Sir Topas" and "The Tale of Melibee" are told by Chaucer himself, who is travelling with the pilgrims in his own story. Both tales seem to focus on the ill-effects of chivalryâthe first making fun of chivalric rules and the second warning against violence.
The "Tales" constantly reflect the conflict between classes. For example, the division of the three estates: the characters are all divided into three distinct classes, the classes being "those who pray" (the clergy), "those who fight" (the nobility), and "those who work" (the commoners and peasantry). Most of the tales are interlinked by common themes, and some "quit" (reply to or retaliate against) other tales. Convention is followed when the Knight begins the game with a tale, as he represents the highest social class in the group. But when he is followed by the Miller, who represents a lower class, it sets the stage for the "Tales" to reflect both a respect for and a disregard for upper class rules. Helen Cooper, as well as Mikhail Bakhtin and Derek Brewer, call this opposition "the ordered and the grotesque, Lent and Carnival, officially approved culture and its riotous, and high-spirited underside." Several works of the time contained the same opposition.
Relativism versus realism.
Chaucer's characters each express differentâsometimes vastly differentâviews of reality, creating an atmosphere of testing, empathy, and relativism. As Helen Cooper says, "Different genres give different readings of the world: the fabliau scarcely notices the operations of God, the saint's life focuses on those at the expense of physical reality, tracts and sermons insist on prudential or orthodox morality, romances privilege human emotion." The sheer number of varying persons and stories renders the "Tales" as a set unable to arrive at any definite truth or reality.
Liminality.
The concept of liminality figures prominently within "The Canterbury Tales". A liminal space, which can be both geographical as well as metaphorical or spiritual, is the transitional or transformational space between a ârealâ (secure, known, limited) world and an unknown or imaginary space of both risk and possibility. The notion of a pilgrimage is itself a liminal experience, because it centers on travel between destinations and because pilgrims undertake it hoping to become more holy in the process. Thus, the structure of "The Canterbury Tales" itself is liminal; it not only covers the distance between London and Canterbury, but the majority of the tales refer to places entirely outside the geography of the pilgrimage. Jean Jost summarises the function of liminality in "The Canterbury Tales",
"Both appropriately and ironically in this raucous and subversive liminal space, a ragtag assembly gather together and tell their equally unconventional tales. In this unruly place, the rules of tale telling are established, themselves to be both disordered and broken; here the tales of game and earnest, solas and sentence, will be set and interrupted. Here the sacred and profane adventure begins, but does not end. Here, the condition of peril is as prominent as that of protection. The act of pilgrimaging itself consists of moving from one urban space, through liminal rural space, to the next urban space with an ever fluctuating series of events and narratives punctuating those spaces. The goal of pilgrimage may well be a religious or spiritual space at its conclusion, and reflect a psychological progression of the spirit, in yet another kind of emotional space."
Liminality is also evident in the individual tales. An obvious instance of this is the Friarâs Tale in which the yeoman devil is a liminal figure because of his transitory nature and function; it is his purpose to issue souls from their current existence to hell, an entirely different one. The Franklinâs Tale is a Breton Lai tale, which takes the tale into a liminal space by invoking both the interaction of the supernatural and the mortal, but the relation between the present and the imagined past.
Influence on literature.
It is sometimes argued that the greatest contribution that this work made to English literature was in popularising the literary use of the vernacular English, rather than French or Latin. English had, however, been used as a literary language for centuries before Chaucer's life, and several of Chaucer's contemporariesâJohn Gower, William Langland, and the Pearl Poetâalso wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend rather than simply being part of it. It is interesting to note that, although Chaucer had a powerful influence in poetic and artistic terms, which can be seen in the great number of forgeries and mistaken attributions (such as "The Floure and the Leafe", which was translated by John Dryden), modern English spelling and orthography owe much more to the innovations made by the Court of Chancery in the decades during and after his lifetime.
Reception.
While Chaucer clearly states the addressees of many of his poems (the "Book of the Duchess" is believed to have been written for John of Gaunt on the occasion of his wife's death in 1368), the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for the nobility. He is referred to as a noble translator and poet by Eustache Deschamps and by his contemporary John Gower. It has been suggested that the poem was intended to be read aloud, which is probable as this was a common activity at the time. However, it also seems to have been intended for private reading as well, since Chaucer frequently refers to himself as the writer, rather than the speaker, of the work. Determining the intended audience directly from the text is even more difficult, since the audience is part of the story. This makes it difficult to tell when Chaucer is writing to the fictional pilgrim audience or the actual reader.
Chaucer's works may have been distributed in some form during his lifetime in part or in whole. Scholars speculate that manuscripts were circulated among his friends, but likely remained unknown to most people until after his death. However, the speed with which copyists strove to write complete versions of his tale in manuscript form shows that Chaucer was a famous and respected poet in his own day. The Hengwrt and Ellesmere manuscripts are examples of the care taken to distribute the work. More manuscript copies of the poem exist than for any other poem of its day except "The Prick of Conscience", causing some scholars to give it the medieval equivalent of bestseller status. Even the most elegant of the illustrated manuscripts, however, is not nearly as highly decorated as the work of authors of more respectable works such as John Lydgate's religious and historical literature.
15th century.
John Lydgate and Thomas Occleve were among the first critics of Chaucer's "Tales", praising the poet as the greatest English poet of all time and the first to show what the language was truly capable of poetically. This sentiment was universally agreed upon by later critics into the mid-15th century. Glosses included in "The Canterbury Tales" manuscripts of the time praised him highly for his skill with "sentence" and rhetoric, the two pillars by which medieval critics judged poetry. The most respected of the tales was at this time the Knight's, as it was full of both.
Literary additions and supplements.
The incompleteness of the "Tales" led several medieval authors to write additions and supplements to the tales to make them more complete. Some of the oldest existing manuscripts of the tales include new or modified tales, showing that even early on, such additions were being created. These emendations included various expansions of the "Cook's Tale", which Chaucer never finished, "The Plowman's Tale", "The Tale of Gamelyn", the "Siege of Thebes", and the "Tale of Beryn".
The "Tale of Beryn", written by an anonymous author in the 15th century, is preceded by a lengthy prologue in which the pilgrims arrive at Canterbury and their activities there are described. While the rest of the pilgrims disperse throughout the town, the Pardoner seeks the affections of Kate the barmaid, but faces problems dealing with the man in her life and the innkeeper Harry Bailey. As the pilgrims turn back home, the Merchant restarts the storytelling with "Tale of Beryn". In this tale, a young man named Beryn travels from Rome to Egypt to seek his fortune only to be cheated by other businessmen there. He is then aided by a local man in getting his revenge. The tale comes from the French tale "BÃ©rinus" and exists in a single early manuscript of the tales, although it was printed along with the tales in a 1721 edition by John Urry.
John Lydgate wrote "The Siege of Thebes" in about 1420. Like the "Tale of Beryn", it is preceded by a prologue in which the pilgrims arrive in Canterbury. Lydgate places himself among the pilgrims as one of them and describes how he was a part of Chaucer's trip and heard the stories. He characterises himself as a monk and tells a long story about the history of Thebes before the events of the "Knight's Tale". John Lydgate's tale was popular early on and exists in old manuscripts both on its own and as part of the "Tales". It was first printed as early as 1561 by John Stow, and several editions for centuries after followed suit.
There are actually two versions of "The Plowman's Tale", both of which are influenced by the story "Piers Plowman", a work written during Chaucer's lifetime. Chaucer describes a Plowman in the "General Prologue" of his tales, but never gives him his own tale. One tale, written by Thomas Occleve, describes the miracle of the Virgin and the Sleeveless Garment. Another tale features a pelican and a griffin debating church corruption, with the pelican taking a position of protest akin to John Wycliffe's ideas.
"The Tale of Gamelyn" was included in an early manuscript version of the tales, Harley 7334, which is notorious for being one of the lower-quality early manuscripts in terms of editor error and alteration. It is now widely rejected by scholars as an authentic Chaucerian tale, although some scholars think he may have intended to rewrite the story as a tale for the Yeoman. Dates for its authorship vary from 1340 to 1370.
Literary adaptations.
Many literary works (both fiction and non-fiction alike) have used a similar frame narrative to "The Canterbury Tales" as an homage. Science-fiction writer Dan Simmons wrote his Hugo Award winning novel "Hyperion" based on an extra-planetary group of pilgrims. Evolutionary biologist Richard Dawkins used "The Canterbury Tales" as a structure for his 2004 non-fiction book about evolution titled "". His animal pilgrims are on their way to find the common ancestor, each telling a tale about evolution.
Henry Dudeney's book "The Canterbury Puzzles" contains a part reputedly lost from what modern readers know as Chaucer's tales.
Historical-mystery novelist P.C. Doherty wrote a series of novels based on "The Canterbury Tales", making use of both the story frame and Chaucer's characters.
Canadian author Angie Abdou translates "The Canterbury Tales" to a cross section of people, all snow-sports enthusiasts but from different social backgrounds, converging on a remote back-country ski cabin in British Columbia in the 2011 novel "The Canterbury Trail".
Adaptations and homages.
"The Two Noble Kinsmen", by William Shakespeare and John Fletcher, a retelling of "The Knight's Tale", was first performed in 1613 or 1614 and published in 1634. In 1961, Erik Chisholm completed his opera, "The Canterbury Tales". The opera is in three acts: The Wyf of Bathâs Tale, The Pardonerâs Tale and The Nunâs Priestâs Tale. Nevill Coghill's modern English version formed the basis of a musical version that was first staged in 1964.
"A Canterbury Tale", a 1944 film jointly written and directed by Michael Powell and Emeric Pressburger, is loosely based on the narrative frame of Chaucer's tales. The movie opens with a group of medieval pilgrims journeying through the Kentish countryside as a narrator speaks the opening lines of the "General Prologue". The scene then makes a now-famous transition to the time of World War II. From that point on, the film follows a group of strangers, each with his or her own story and in need of some kind of redemption, who are making their way to Canterbury together. The film's main story takes place in an imaginary town in Kent and ends with the main characters arriving at Canterbury Cathedral, bells pealing and Chaucer's words again resounding. "A Canterbury Tale" is recognised as one of the Powell-Pressburger team's most poetic and artful films. It was produced as wartime propaganda, using Chaucer's poetry, referring to the famous pilgrimage, and offering photography of Kent to remind the public of what made Britain worth fighting for. In one scene a local historian lectures an audience of British soldiers about the pilgrims of Chaucer's time and the vibrant history of England.
Pier Paolo Pasolini's 1972 film "The Canterbury Tales" features several of the tales, some of which keep close to the original tale and some of which are embellished. The "Cook's Tale", for instance, which is incomplete in the original version, is expanded into a full story, and the "Friar's Tale" extends the scene in which the Summoner is dragged down to hell. The film includes these two tales as well as the "Miller's Tale", the "Summoner's Tale", the "Wife of Bath's Tale", and the "Merchant's Tale".
On April 26, 1986, American radio personality Garrison Keillor opened "The News from Lake Wobegon" portion of the first live TV broadcast of his "A Prairie Home Companion" radio show with a reading of the original Middle English text of the General Prologue. He commented, "Although those words were written more than 600 years ago, they still describe spring."
English rock musician Sting paid tribute to Chaucer and the book with his 1993 concept album "Ten Summoner's Tales", which he described as ten songs (plus an epilogue number) with no theme or subject tying them together. Sting's real name is Gordon Sumner, hence the reference to the "Summoner" character in the record's title. In essence, the collection of songs was composed as "a musical Canterbury Tales".
Several more recent films, while they are not based on the tales, do have references to them. For example, in the 1995 film "Se7en", the "Parson's Tale" is an important clue to the methods of a serial killer who chooses his victims based on the seven deadly sins. The 2001 film "A Knight's Tale" took its name from "The Knight's Tale". Although it bears little resemblance to the tale, it does feature what Martha Driver and Sid Ray call an "MTV-generation" Chaucer who is a gambling addict with a way with words. Scattered references to the "Tales" include Chaucer's declaration that he will use his verse to vilify a summoner and a pardoner who have cheated him.
Television adaptations include Alan Plater's 1975 re-telling of the stories in a series of plays for BBC2: "Trinity Tales". In 2003, BBC again featured modern re-tellings of selected tales.
The 2014 young adult short novel "Anaheim Tales" by M.L. Millard was inspired by and references "The Canterbury Tales".

</doc>
<doc id="7628" url="https://en.wikipedia.org/wiki?curid=7628" title="Christine de Pizan">
Christine de Pizan

Christine de Pizan (also seen as de Pisan; 1364 â c. 1430) was an Italian French late medieval author. She served as a court writer for several dukes (Louis of Orleans, Philip the Bold of Burgundy, and John the Fearless of Burgundy) and the French royal court during the reign of Charles VI. She wrote both poetry and prose works such as biographies and books containing practical advice for women. She completed forty-one works during her 30-year career from 1399â1429. She married in 1380 at the age of 15, and was widowed 10 years later. Much of the impetus for her writing came from her need to earn a living to support her mother, a niece and her two surviving children. She spent most of her childhood and all of her adult life in Paris and then the abbey at Poissy, and wrote entirely in her adopted language, Middle French.
Her early courtly poetry is marked by her knowledge of aristocratic custom and fashion of the day, particularly involving women and the practice of chivalry. Her early and later allegorical and didactic treatises reflect both autobiographical information about her life and views and also her own individualized and humanist approach to the scholastic learned tradition of mythology, legend, and history she inherited from clerical scholars and to the genres and courtly or scholastic subjects of contemporary French and Italian poets she admired. Supported and encouraged by important royal French and English patrons, she influenced 15th-century English poetry. Her success stems from a wide range of innovative writing and rhetorical techniques that critically challenged renowned writers such as Jean de Meun, author of the "Romance of the Rose", which she criticized as immoral.
In recent decades, Christine de Pizan's work has been returned to prominence by the efforts of scholars such as Charity Cannon Willard, Earl Jeffrey Richards and Simone de Beauvoir. Certain scholars have argued that she should be seen as an early feminist who efficiently used language to convey that women could play an important role within society. This characterization has been challenged by other critics, who say that it is either an anachronistic use of the word or a misinterpretation of her writing and intentions.
Life.
Christine de Pizan was born in 1364 in Venice, Italy. She was the daughter of Tommaso di Benvenuto da Pizzano (Thomas de Pizan, named for the family's origins in the town of Pizzano, south east of Bologna), a physician, court astrologer, and Councillor of the Republic of Venice. Following her birth, Thomas de Pizan accepted an appointment to the court of Charles V of France, as the kingâs astrologer, alchemist, and physician. In this atmosphere, Christine was able to pursue her intellectual interests. She successfully educated herself by immersing herself in languages, in the rediscovered classics and humanism of the early Renaissance, and in Charles Vâs royal archive that housed a vast number of manuscripts. But she did not assert her intellectual abilities, or establish her authority as a writer until she was widowed at the age of 25.
She married Etienne du Castel, a royal secretary to the court, at the age of 15. She had three children, a daughter (who became a nun at the Dominican Abbey in Poissy in 1397 as a companion to the king's daughter, Marie), a son Jean, and another child who died in childhood. Christine's family life was threatened in 1387 when her husband, while in Beauvais on a mission with the king, suddenly died in an epidemic. Following Castelâs death, she was left to support her mother, a niece, and her two children. When she tried to collect money from her husbandâs estate, she faced complicated lawsuits regarding the recovery of salary due her husband. On 4 June 1389, in a judgment concerning a lawsuit filed against her by the archbishop of Sens and FranÃ§ois Chanteprime, councillors of the king, Christine was styled "damoiselle" and widow of "Estienne du Castel." Note that in letters he signed as secretary of the king in 1381 and 1382 the signature of Etienne was "Ste de Castel." The abbreviation of his first name could be read both as a phonetic abbreviation of Estienne and as the first letters of his name in latin: Stephanus.
In order to support herself and her family, Christine turned to writing. By 1393, she was writing love ballads, which caught the attention of wealthy patrons within the court. These patrons were intrigued by the novelty of a female writer and had her compose texts about their romantic exploits. Her output during this period was prolific. Between 1393 and 1412, she composed over 300 ballads, and many more shorter poems.
Christine's participation in a literary debate, in 1401â1402, allowed her to move beyond the courtly circles, and ultimately to establish her status as a writer concerned with the position of women in society. During these years, she involved herself in a renowned literary controversy, the âQuerelle du Roman de la Roseâ. She helped to instigate this debate by beginning to question the literary merits of Jean de Meunâs the "Romance of the Rose". Written in the 13th century, the "Romance of the Rose" satirizes the conventions of courtly love while critically depicting women as nothing more than seducers. Christine specifically objected to the use of vulgar terms in Jean de Meunâs allegorical poem. She argued that these terms denigrated the proper and natural function of sexuality, and that such language was inappropriate for female characters such as Madame Raison. According to her, noble women did not use such language. Her critique primarily stems from her belief that Jean de Meun was purposely slandering women through the debated text.
The debate itself was extensive and at its end, the principal issue was no longer Jean de Meunâs literary capabilities. The principal issue had shifted to the unjust slander of women within literary texts. This dispute helped to establish Christine's reputation as a female intellectual who could assert herself effectively and defend her claims in the male-dominated literary realm. She continued to counter abusive literary treatments of women.
Works.
Christine produced a large amount of vernacular works, in both prose and verse. Her works include political treatises, mirrors for princes, epistles, and poetry.
By 1405, Christine had completed her most famous literary works, "The Book of the City of Ladies" and "The Treasure of the City of Ladies". The first of these shows the importance of womenâs past contributions to society, and the second strives to teach women of all estates how to cultivate useful qualities. For example, one section of the book tells wives: "If she wants to act prudently and have the praise of both the world and her husband, she will be cheerful to him all the time"
In "The Book of the City of Ladies" Christine created a symbolic city in which women are appreciated and defended. She constructed three allegorical figures â Reason, Justice, and Rectitude â in the common pattern of literature in that era, when many books and poetry utilized stock allegorical figures to express ideas or emotions. She enters into a dialogue, a movement between question and answer, with these allegorical figures that is from a completely female perspective. Together, they create a forum to speak on issues of consequence to all women. Only female voices, examples and opinions provide evidence within this text. Christine, through Lady Reason in particular, argues that stereotypes of women can be sustained only if women are prevented from entering into the conversation. Overall, she hoped to establish truths about women that contradicted the negative stereotypes that she had identified in previous literature.
In "The Treasure of the City of Ladies", she highlights the persuasive effect of womenâs speech and actions in everyday life. In this particular text, Christine argues that women must recognize and promote their ability to make peace between people. This ability will allow women to mediate between husband and subjects. She also argues that slanderous speech erodes oneâs honor and threatens the sisterly bond among women. Christine then argues that "skill in discourse should be a part of every womanâs moral repertoire". She believed that a womanâs influence is realized when her speech accords value to chastity, virtue, and restraint. She argued that rhetoric is a powerful tool that women could employ to settle differences and to assert themselves. The "Treasure of the City of Ladies" provides glimpses into women's lives in 1400, from the great lady in the castle down to the merchant's wife, the servant, and the peasant. She offers advice to governesses, widows, and even prostitutes.
De Pizan was greatly interested in history, ranging from the Matter of Troy to the "founding of the royal house of France" (for her the latter was a consequence of the former). She obtained her knowledge of Troy from the "Histoire ancienne jusqu'Ã  CÃ©sar", and chose an anti-Trojan position. Hector especially served as a model and a measure of masculinity for her.
In the âQuerelle du Roman de la Rose,â she responded to Jean de Montreuil, who had sent her a treatise defending the sentiments expressed in the "Romance of the Rose". She begins by styling her opponent as an âexpert in rhetoricâ in contrast to herself, âa woman ignorant of subtle understanding and agile sentiment.â In this particular apologetic response, de Pizan belittles her own style. She is employing a rhetorical strategy by writing against the grain of her meaning, also known as antiphrasis. Her ability to employ rhetorical strategies continued when Christine began to compose literary texts following the âQuerelle du Roman de la Rose.â 
Her final work was a poem eulogizing Joan of Arc, the peasant girl who said God had commanded her to secure the French throne for Charles VII. Written in 1429, "The Poem of Joan of Arc" ("Ditie de Jehanne dArc") celebrates the appearance of a woman whom Christine describes in the poem as "a simple shepherdess" while commenting: "It is a fact well worth remembering That God should now have wished (and this is the truth!) to bestow such great blessings on France, through a young virgin", adding "For there will be a King of France called Charles son of Charles [VI, who will be supreme ruler over all Kings." After completing this particular poem, it seems that Christine de Pizan, at the age of 65, decided to end her literary career.
Christine specifically sought out other women to collaborate in the creation of her work. She makes special mention of a manuscript illustrator we know only as Anastasia, whom she described as the most talented of her day.
Influence.
In her own day, Christine de Pizan was primarily a court writer who wrote commissioned works for aristocratic families, as well as addressing literary debates of the era. In modern times, she has been labeled a poetic mediator who engaged with historical texts to interpolate her royal readers and encourage ethical and judicious conduct. Some rhetorical scholars have concluded, from studying her persuasive strategies, that she forged a rhetorical identity for herself and encouraged women to embrace this identity. Some have argued that Christine de Pizan âbegan her literary career by singing, alone in her room, and she finished by shouting in the public square.â She left an influential footprint in the field of rhetorical discourse in an otherwise male-dominated literary field. She left forty-one surviving poetic works and a number of prose books. Simone de Beauvoir wrote in 1949 that "ÃpÃ®tre au Dieu d'Amour" was "the first time we see a woman take up her pen in defence of her sex".
Tributes.
The artwork "The Dinner Party" features a place setting for Christine de Pizan.

</doc>
<doc id="7630" url="https://en.wikipedia.org/wiki?curid=7630" title="Catharism">
Catharism

Catharism (; from the Greek: , "katharoi", "the pure ") was a Christian dualist or Gnostic revival movement that thrived in some areas of Southern Europe, particularly northern Italy and southern France, between the 12th and 14th centuries. Cathar beliefs varied between communities, because Catharism was initially taught by ascetic priests, who had set few guidelines. The Catholic Church denounced its practices including the 'C"onsolamentum"' ritual, by which Cathar individuals were baptized and raised to the status of 'perfect'. 
Catharism had its roots in the Paulician movement in Armenia and eastern Byzantine Anatolia and the Bogomils of the First Bulgarian Empire, who were influenced by the Paulicians resettled in Thrace (Philipopolis) by the Byzantines. Though the term "Cathar" () has been used for centuries to identify the movement, whether the movement identified itself with this name is debatable. In Cathar texts, the terms "Good Men" ("Bons Hommes") or "Good Christians" are the common terms of self-identification. The idea of two Gods or principles, one being good and the other evil, was central to Cathar beliefs. The good God was the God of the New Testament and the creator of the spiritual realm, contrasted with the evil Old Testament Godâthe creator of the physical world whom many Cathars, and particularly their persecutors, identified as Satan. All visible matter, including the human body, was created by this evil god; it was therefore tainted with sin. This was the antithesis to the monotheistic Catholic Church, whose fundamental principle was that there was only one God, who created all things visible and invisible. Cathars thought human spirits were the genderless spirits of angels trapped within the physical creation of the evil god, cursed to be reincarnated until the Cathar faithful achieved salvation through a ritual called the consolamentum.
From the beginning of his reign, Pope Innocent III attempted to end Catharism by sending missionaries and by persuading the local authorities to act against them. In 1208 Innocent's papal legate Pierre de Castelnau was murdered while returning to Rome after excommunicating Count Raymond VI of Toulouse, who, in his view, was too lenient with the Cathars. Pope Innocent III then abandoned the option of sending Catholic missionaries and jurists, declared Pierre de Castelnau a martyr and launched the Albigensian Crusade.
Origins.
The origins of the Cathars' beliefs are unclear, but most theories agree they came from the Byzantine Empire, mostly by the trade routes and spread from the First Bulgarian Empire to the Netherlands. The name of Bulgarians ("Bougres") was also applied to the Albigenses, and they maintained an association with the similar Christian movement of the Bogomils ("Friends of God") of Thrace. "That there was a substantial transmission of ritual and ideas from Bogomilism to Catharism is beyond reasonable doubt." Their doctrines have numerous resemblances to those of the Bogomils and the Paulicians, who influenced them, as well as the earlier Marcionites, who were found in the same areas as the Paulicians, the Manicheans and the Christian Gnostics of the first few centuries AD, although, as many scholars, most notably Mark Pegg, have pointed out, it would be erroneous to extrapolate direct, historical connections based on theoretical similarities perceived by modern scholars. St John Damascene, writing in the 8th century AD, also notes of an earlier sect called the "Cathari", in his book "On Heresies", taken from the epitome provided by Epiphanius of Salamis in his "Panarion". He says of them: "They absolutely reject those who marry a second time, and reject the possibility of penance is, forgiveness of sins after baptism". These are probably the same Cathari who are mentioned in Canon 8 of the First Ecumenical Council of Nicaea in the year 325, which states "...those called Cathari come over [to the faith, let them first make profession that they are willing to communicate full communion with the twice-married, and grant pardon to those who have lapsed..."
It is likely that we have only a partial view of their beliefs, because the writings of the Cathars were mostly destroyed because of the doctrinal threat perceived by the Papacy; much of our existing knowledge of the Cathars is derived from their opponents. Conclusions about Cathar ideology continue to be fiercely debated with commentators regularly accusing their opponents of speculation, distortion and bias. There are a few texts from the Cathars themselves which were preserved by their opponents (the "Rituel Cathare de Lyon") which give a glimpse of the inner workings of their faith, but these still leave many questions unanswered. One large text which has survived, "The Book of Two Principles" ("Liber de duobus principiis"), elaborates the principles of dualistic theology from the point of view of some of the Albanenses Cathars.
It is now generally agreed by most scholars that identifiable historical Catharism did not emerge until at least 1143, when the first confirmed report of a group espousing similar beliefs is reported being active at Cologne by the cleric Eberwin of Steinfeld. A landmark in the "institutional history" of the Cathars was the Council, held in 1167 at Saint-FÃ©lix-Lauragais, attended by many local figures and also by the Bogomil "papa" Nicetas, the Cathar bishop of (northern) France and a leader of the Cathars of Lombardy.
The Cathars were largely a homegrown, Western European/Latin Christian phenomenon, springing up in the Rhineland cities (particularly Cologne) in the mid-12th century, northern France around the same time, and particularly southern France â the Languedoc â and the northern Italian cities in the mid-late 12th century. In the Languedoc and northern Italy, the Cathars attained their greatest popularity, surviving in the Languedoc, in much reduced form, up to around 1325 and in the Italian cities until the Inquisitions of the 1260sâ1300s finally rooted them out.
General beliefs.
Cathars, in general, formed an anti-sacerdotal party in opposition to the Catholic Church, protesting against what they perceived to be the moral, spiritual and political corruption of the Church.
G. K. Chesterton, the English Roman Catholic author, claimed: "... the medieval system began to be broken to pieces intellectually, long before it showed the slightest hint of falling to pieces morally. The huge early heresies, like the Albigenses, had not the faintest excuse in moral superiority."
Contemporary reports suggest otherwise, however. St Bernard of Clairvaux, for instance, although opposed to the Cathars, said of them in Sermon 65 on the Song of Songs:
When Bishop Fulk, a key leader of the anti-Cathar persecutions, excoriated the Languedoc Knights for not pursuing the heretics more diligently, he received the reply:
Sacraments.
In contrast to the Catholic Church, the Cathars had but one sacrament, the Consolamentum, or Consolation. This involved a brief spiritual ceremony to remove all sin from the believer and to induct him or her into the next higher level as a perfect. Unlike the Roman Catholic sacrament of Penance, the Consolamentum could be taken only once.
Thus it has been alleged that many believers would eventually receive the Consolamentum as death drew near, performing the ritual of liberation at a moment when the heavy obligations of purity required of Perfecti would be temporally short. Some of those who received the sacrament of the consolamentum upon their death-beds may thereafter have shunned further food or drink in order to speed death. This has been termed the "endura". It was claimed by some of the Catholic writers that when a Cathar, after receiving the Consolamentum, began to show signs of recovery he or she would be smothered in order to ensure his or her entry into paradise. Other than at such moments of "extremis", little evidence exists to suggest this was a common Cathar practice.
The Cathars also refused the Catholic Sacrament of the eucharist saying that it could not possibly be the body of Christ. They also refused to partake in the practice of Baptism by water. The following two quotes are taken from the Catholic Inquisitor Bernard Guiâs experiences with the Cathar practices and beliefs:
Theology.
Some believe that the Catharist conception of Jesus resembled nontrinitarian modalistic monarchianism (Sabellianism) in the West and adoptionism in the East.
Bernard of Clairvaux's biographer and other sources accuse some Cathars of Arianism, and some scholars see Cathar Christology as having traces of earlier Arian roots. According to some of their contemporary enemies Cathars did not accept the Trinitarian understanding of Jesus, but considered him the human form of an angel similar to Docetic Christology. ZoÃ© Oldenbourg (2000) compared the Cathars to "Western Buddhists" because she considered that their view of the doctrine of "resurrection" taught by Jesus was, in fact, similar to the Buddhist doctrine of reincarnation. The Cathars taught that to regain angelic status one had to renounce the material self completely. Until one was prepared to do so, he/she would be stuck in a cycle of reincarnation, condemned to live on the corrupt Earth.
The alleged sacred texts of the Cathars besides the New Testament, include The Gospel of the Secret Supper, or John's Interrogation and The Book of the Two Principles.
Social relationships.
Killing was abhorrent to the Cathars. Consequently, abstention from all animal food (sometimes exempting fish) was enjoined of the Perfecti. The Perfecti avoided eating anything considered to be a by-product of sexual reproduction. War and capital punishment were also condemnedâan abnormality in Medieval Europe. In a world where few could read, their rejection of oath-taking marked them as social outcasts.
Cathars also rejected marriage. Their theology was based principally on the belief that the physical world, including the flesh, was irredeemably evilâas it stemmed from the evil principle or "demiurge". Therefore, reproduction was viewed by them as a moral evil to be avoidedâas it continued the chain of reincarnation and suffering in the material world. It was claimed by their opponents that, given this loathing for procreation, they generally resorted to sodomy. Such was the situation that a charge of heresy leveled against a suspected Cathar was usually dismissed if the accused could show he was legally married.
Organization.
It has been alleged that the Cathar Church of the Languedoc had a relatively flat structure, distinguishing between "perfecti" (a term they did not use, instead "bonhommes") and "credentes". By about 1140, liturgy and a system of doctrine had been established. It created a number of bishoprics, first at Albi around 1165 and after the 1167 Council at Saint-FÃ©lix-Lauragais sites at Toulouse, Carcassonne, and Agen, so that four bishoprics were in existence by 1200.
In about 1225, during a lull in the Albigensian Crusade, the bishopric of Razes was added. Bishops were supported by their two assistants: a "filius maior" (typically the successor) and a "filius minor", who were further assisted by deacons. The "perfecti" were the spiritual elite, highly respected by many of the local people, leading a life of austerity and charity. In the apostolic fashion they ministered to the people and travelled in pairs.
Role of women and gender.
Catharism has been seen as giving women the greatest opportunities for independent action since women were found as being believers as well as Perfecti, who were able to administer the sacrament of the "consolamentum". The Cathars believed that one would be repeatedly reincarnated until one commits to the self-denial of the material world, which meant that a man could be reincarnated as a woman and vice versa, thereby rendering gender completely meaningless. The spirit was of utmost importance to the Cathars and was described as being immaterial and sexless. Because of this belief, the Cathars saw women equally capable of being spiritual leaders, which undermined the very concept of gender held by the Catholic Church and did not go unnoticed.
The women that were accused of being heretics in early medieval Christianity included those labeled Gnostics, Cathars, and Beguines, as well as several other groups that were sometimes "tortured and executed". The Cathars, like the Gnostics who preceded them, assigned more importance to the role of Mary Magdalene in the spread of early Christianity than the Church previously did. Her vital role as a teacher contributed to the Cathar belief that women could serve as spiritual leaders. Women were found to be included in the Perfecti in significant numbers, with numerous receiving the "consolamentum" after being widowed. Having reverence for the Gospel of John, the Cathars saw Mary Magdalene as perhaps even more important than Saint Peter, the founder of the Church.
The Cathar movement proved to be extremely successful in gaining female followers because of its proto-feminist teachings along with the general feeling of exclusion from the Catholic church. Catharism attracted numerous women with the promise of a sacerdotal role that the Catholic Church did not allow. Catharism let women become a perfect of the faith, a position of far more prestige than anything the Church offered. These female perfects were required to adhere to a strict and ascetic lifestyle, but were still able to have their own houses. Although many women found something attractive in Catharism, not all found its teachings convincing. A notable example is Hildegard of Bingen, who in 1163 gave a widely renowned sermon against the Cathars in Cologne. During this speech, Hildegard announced a state of eternal punishment and damnation to all those who accepted Cathar beliefs.
While women perfects rarely traveled to preach the faith, they still played a vital role in the spreading of the Catharism by establishing group homes for women. Though it was extremely uncommon, there were isolated cases of female Cathars departing from their homes to spread the faith. In the Cathar group homes, women were educated in the faith and these women would go on to bear children who would then also become believers. Through this pattern the faith grew exponentially through the efforts of women as each generation passed. Among some groups of Cathars there were even more women than there were men.
Despite women having an instrumental role in the growing of the faith, misogyny was not completely absent from the Cathar movement. Some seemingly misogynistic Cathar beliefs include that one's last incarnation had to be experienced as a man to break the cycle. This belief was inspired by later French Cathars, which taught that women must be reborn as men in order to achieve salvation. Another one is that the sexual allure of women impedes man's ability to reject the material world. Toward the end of the Cathar movement, French Catharism became more misogynistic and started the practice of excluding women perfects. However, the influence of these type of misogynistic beliefs and practices remained rather limited on the whole of Catharism as later Italian perfects still included women.
Suppression.
In 1147, Pope Eugene III sent a legate to the Cathar district in order to arrest the progress of the Cathars. The few isolated successes of Bernard of Clairvaux could not obscure the poor results of this mission, which clearly showed the power of the sect in the Languedoc at that period. The missions of Cardinal Peter of St. Chrysogonus to Toulouse and the Toulousain in 1178, and of Henry of Marcy, cardinal-bishop of Albano, in 1180â81, obtained merely momentary successes. Henry's armed expedition, which took the stronghold at Lavaur, did not extinguish the movement.
Decisions of Catholic Church councilsâin particular, those of the Council of Tours (1163) and of the Third Council of the Lateran (1179)âhad scarcely more effect upon the Cathars. When Pope Innocent III came to power in 1198, he was resolved to deal with them.
At first Innocent tried pacific conversion, and sent a number of legates into the Cathar regions. They had to contend not only with the Cathars, the nobles who protected them, and the people who respected them, but also with many of the bishops of the region, who resented the considerable authority the Pope had conferred upon his legates. In 1204, Innocent III suspended a number of bishops in Occitania; in 1205 he appointed a new and vigorous bishop of Toulouse, the former troubadour Foulques. In 1206 Diego of Osma and his canon, the future Saint Dominic, began a programme of conversion in Languedoc; as part of this, Catholic-Cathar public debates were held at Verfeil, Servian, Pamiers, MontrÃ©al and elsewhere.
Saint Dominic met and debated with the Cathars in 1203 during his mission to the Languedoc. He concluded that only preachers who displayed real sanctity, humility and asceticism could win over convinced Cathar believers. The institutional Church as a general rule did not possess these spiritual warrants. His conviction led eventually to the establishment of the Dominican Order in 1216. The order was to live up to the terms of his famous rebuke, "Zeal must be met by zeal, humility by humility, false sanctity by real sanctity, preaching falsehood by preaching truth." However, even St. Dominic managed only a few converts among the Cathari.
Albigensian Crusade.
In January 1208 the papal legate, Pierre de Castelnauâa Cistercian monk, theologian and canon lawyerâwas sent to meet the ruler of the area, Raymond VI, Count of Toulouse. Known for excommunicating noblemen who protected the Cathars, Castelnau excommunicated Raymond for abetting heresy following an allegedly fierce argument during which Raymond supposedly threatened Castelnau with violence. Shortly thereafter, Castelnau was murdered as he returned to Rome, allegedly by a knight in the service of Count Raymond. His body was returned and laid to rest in the Abbey at Saint Gilles.
As soon as he heard of the murder, the Pope ordered the legates to preach a crusade against the Cathars and wrote a letter to Philip Augustus, King of France, appealing for his interventionâor an intervention led by his son, Louis. This was not the first appeal but some see the murder of the legate as a turning point in papal policy. Others claim it as a fortuitous event in allowing the Pope to excite popular opinion and to renew his pleas for intervention in the south. The chronicler of the crusade which followed, Peter of Vaux de Cernay, portrays the sequence of events in such a way that, having failed in his effort to peaceably demonstrate the errors of Catharism, the Pope then called a formal crusade, appointing a series of leaders to head the assault. The French King refused to lead the crusade himself, and could not spare his son to do so eitherâdespite his victory against John, King of England, there were still pressing issues with Flanders and the empire and the threat of an Angevin revival. Philip did however sanction the participation of some of his more bellicose and ambitiousâsome might say dangerousâbarons, notably Simon de Montfort and Bouchard de Marly. There followed twenty years of war against the Cathars and their allies in the Languedoc: the Albigensian Crusade.
This war pitted the nobles of the north of France against those of the south. The widespread northern enthusiasm for the Crusade was partially inspired by a papal decree permitting the confiscation of lands owned by Cathars and their supporters. This not only angered the lords of the south but also the French King, who was at least nominally the suzerain of the lords whose lands were now open to despoliation and seizure. Philip Augustus wrote to Pope Innocent in strong terms to point this outâbut the Pope did not change his policy. As the Languedoc was supposedly teeming with Cathars and Cathar sympathisers, this made the region a target for northern French noblemen looking to acquire new fiefs. The barons of the north headed south to do battle.
Their first target was the lands of the Trencavel, powerful lords of Albi, Carcassonne and the Razesâbut a family with few allies in the Midi. Little was thus done to form a regional coalition and the crusading army was able to take Carcassonne, the Trencavel capital, incarcerating Raymond Roger in his own citadel where he died, allegedly of natural causes; champions of the Occitan cause from that day to this believe he was murdered. Simon de Montfort was granted the Trencavel lands by the Pope and did homage for them to the King of France, thus incurring the enmity of Peter II of Aragon who had held aloof from the conflict, even acting as a mediator at the time of the siege of Carcassonne. The remainder of the first of the two Cathar wars now essentially focused on Simon's attempt to hold on to his fabulous gains through winters where he was faced, with only a small force of confederates operating from the main winter camp at Fanjeau, with the desertion of local lords who had sworn fealty to him out of necessityâand attempts to enlarge his newfound domains in the summer when his forces were greatly augmented by reinforcements from northern France, Germany and elsewhere.
Summer campaigns saw him not only retake, sometimes with brutal reprisals, what he had lost in the 'close' season, but also seek to widen his sphere of operationâand we see him in action in the Aveyron at St. Antonin and on the banks of the Rhone at Beaucaire. Simon's greatest triumph was the victory against superior numbers at the Battle of Muretâa battle which saw not only the defeat of Raymond of Toulouse and his Occitan alliesâbut also the death of Peter of Aragonâand the effective end of the ambitions of the house of Aragon/Barcelona in the Languedoc. This was in the medium and longer term of much greater significance to the royal house of France than it was to de Montfortâand with the battle of Bouvines was to secure the position of Philip Augustus vis a vis England and the Empire. The Battle of Muret was a massive step in the creation of the unified French kingdom and the country we know todayâalthough Edward III, the Black Prince and Henry V would threaten later to shake these foundations.
Massacre.
The crusader army came under the command, both spiritually and militarily, of the papal legate Arnaud-Amaury, Abbot of CÃ®teaux. In the first significant engagement of the war, the town of BÃ©ziers was besieged on 22 July 1209. The Catholic inhabitants of the city were granted the freedom to leave unharmed, but many refused and opted to stay and fight alongside the Cathars.
The Cathars spent much of 1209 fending off the crusaders. The BÃ©ziers army attempted a sortie but was quickly defeated, then pursued by the crusaders back through the gates and into the city. Arnaud-Amaury, the Cistercian abbot-commander, is supposed to have been asked how to tell Cathars from Catholics. His reply, recalled by Caesarius of Heisterbach, a fellow Cistercian, thirty years later was ""Caedite eos. Novit enim Dominus qui sunt eius""â"Kill them all, the Lord will recognise His own". The doors of the church of St Mary Magdalene were broken down and the refugees dragged out and slaughtered. Reportedly, 7,000 people died there. Elsewhere in the town, many more thousands were mutilated and killed. Prisoners were blinded, dragged behind horses, and used for target practice. What remained of the city was razed by fire. Arnaud-Amaury wrote to Pope Innocent III, "Today your Holiness, twenty thousand heretics were put to the sword, regardless of rank, age, or sex." The permanent population of BÃ©ziers at that time was then probably no more than 5,000, but local refugees seeking shelter within the city walls could conceivably have increased the number to 20,000.
After the success of his siege of Carcassonne, which followed the Massacre at BÃ©ziers in 1209, Simon de Montfort was designated as leader of the Crusader army. Prominent opponents of the Crusaders were Raymond Roger Trencavel, viscount of Carcassonne, and his feudal overlord Peter II, the king of Aragon, who held fiefdoms and had a number of vassals in the region. Peter died fighting against the crusade on 12 September 1213 at the Battle of Muret. Simon de Montfort was killed on 25 June 1218 after maintaining a siege of Toulouse for nine months.
Treaty and persecution.
The official war ended in the Treaty of Paris (1229), by which the king of France dispossessed the house of Toulouse of the greater part of its fiefs, and that of the Trencavels (Viscounts of BÃ©ziers and Carcassonne) of the whole of their fiefs. The independence of the princes of the Languedoc was at an end. But in spite of the wholesale massacre of Cathars during the war, Catharism was not yet extinguished and Catholic forces would continue to pursue Cathars.
In 1215, the bishops of the Catholic Church met at the Fourth Council of the Lateran under Pope Innocent III; part of the agenda was combating the Cathar heresy.
The Inquisition was established in 1234 to uproot the remaining Cathars. Operating in the south at Toulouse, Albi, Carcassonne and other towns during the whole of the 13th century, and a great part of the 14th, it succeeded in crushing Catharism as a popular movement and driving its remaining adherents underground. Cathars who refused to recant were hanged, or burnt at the stake.
From May 1243 to March 1244, the Cathar fortress of MontsÃ©gur was besieged by the troops of the seneschal of Carcassonne and the archbishop of Narbonne. On 16 March 1244, a large and symbolically important massacre took place, where over 200 Cathar Perfects were burnt in an enormous pyre at the "prat dels cremats" ("field of the burned") near the foot of the castle. Moreover, the Church decreed lesser chastisements against laymen suspected of sympathy with Cathars, at the 1235 Council of Narbonne.
A popular though as yet unsubstantiated theory holds that a small party of Cathar Perfects escaped from the fortress before the massacre at "prat dels cremats". It is widely held in the Cathar region to this day that the escapees took with them "le trÃ©sor cathar". What this treasure consisted of has been a matter of considerable speculation: claims range from sacred Gnostic texts to the Cathars' accumulated wealth, which might have included the Holy Grail (see the Section on Historical Scholarship, below).
Hunted by the Inquisition and deserted by the nobles of their districts, the Cathars became more and more scattered fugitives: meeting surreptitiously in forests and mountain wilds. Later insurrections broke out under the leadership of Roger-Bernard II, Count of Foix, Aimery III of Narbonne and Bernard DÃ©licieux, a Franciscan friar later prosecuted for his adherence to another heretical movement, that of the Spiritual Franciscans at the beginning of the 14th century. But by this time the Inquisition had grown very powerful. Consequently, many presumed to be Cathars were summoned to appear before it. Precise indications of this are found in the registers of the Inquisitors, Bernard of Caux, Jean de St Pierre, Geoffroy d'Ablis, and others. The parfaits it was said only rarely recanted, and hundreds were burnt. Repentant lay believers were punished, but their lives were spared as long as they did not relapse. Having recanted, they were obliged to sew yellow crosses onto their outdoor clothing and to live apart from other Catholics, at least for a while.
Annihilation.
After several decades of harassment and re-proselytising, and perhaps even more importantly, the systematic destruction of their religious texts, the sect was exhausted and could find no more adepts. The leader of a Cathar revival in the Pyrenean foothills, Peire Autier was captured and executed in April 1310 in Toulouse. After 1330, the records of the Inquisition contain very few proceedings against Cathars. The last known Cathar perfectus in the Languedoc, Guillaume BÃ©libaste, was executed in the autumn of 1321.
From the mid-12th century onwards, Italian Catharism came under increasing pressure from the Pope and the Inquisition, "spelling the beginning of the end". Other movements, such as the Waldensians and the pantheistic Brethren of the Free Spirit, which suffered persecution in the same area, survived in remote areas and in small numbers into the 14th and 15th centuries. Some Waldensian ideas were absorbed into early Protestant sects, such as the Hussites, Lollards, and the Moravian Church (Herrnhuters of Germany).
Later history.
After the suppression of Catharism, the descendants of Cathars were at times required to live outside towns and their defences. They thus retained a certain Cathar identity, despite having returned to the Catholic religion.
Any use of the term "Cathar" to refer to people after the suppression of Catharism in the 14th century is a cultural or ancestral reference, and has no religious implication. Nevertheless, interest in the Cathars, their history, legacy and beliefs continues.
"Pays Cathare".
The term "Pays Cathare", French meaning "Cathar Country" is used to highlight the Cathar heritage and history of the region where Catharism was traditionally strongest. This area is centred around fortresses such as MontsÃ©gur and Carcassonne; also the French dÃ©partement of the Aude uses the title "Pays Cathare" in tourist brochures. These areas have ruins from the wars against the Cathars which are still visible today.
Some criticise the promotion of the identity of "Pays Cathare" as an exaggeration for tourist purposes. Actually, most of the promoted Cathar castles were not built by Cathars but by local lords and later many of them were rebuilt and extended for strategic purposes. Good examples of these are the magnificent castles of Queribus and Peyrepertuse which are both perched on the side of precipitous drops on the last folds of the Corbieres mountains. They were for several hundred years frontier fortresses belonging to the French crown and most of what is still there dates from a post-Cathar era. The Cathars sought refuge at these sites. Many consider the County of Foix to be the actual historical centre of Catharism.
Oldest account of ordinary people told in their own words.
In an effort to find the few remaining heretics in and around the village of Montaillou, Jacques Fournier, Bishop of Pamiers, future Pope Benedict XII, had those suspected of heresy interrogated in the presence of scribes who recorded their conversations. The late 13th- to early 14th-century document, discovered in the Vatican archives in the 1960s, and edited by Jean Duvernoy is the oldest known account of the daily lives of ordinary people told in their own words. It was translated by Emmanuel Le Roy Ladurie as "Montaillou: The Promised Land of Error". In the original, the book was entitled "Montaillou, Occitan Village".
Historical scholarship.
The publication of the early scholarly book "Crusade against the Grail" by the young German Otto Rahn in the 1930s rekindled interest in the connection between the Cathars and the Holy Grail, especially in Germany. Rahn was convinced that the 13th-century work "Parzival" by Wolfram von Eschenbach was a veiled account of the Cathars. The philosopher and Nazi government official Alfred Rosenberg speaks favourably of the Cathars in "The Myth of the Twentieth Century".
Academic books in English first appeared at the beginning of the millennium: for example, Malcolm Lambert's "The Cathars" and Malcolm Barber's "The Cathars".
Starting in the 1990s and continuing to the present day, historians like R.I Moore have radically challenged the extent to which Catharism, as an institutionalized religion, actually existed. Building off the work of French historians such as Monique Zerner and Uwe Brunn, Mooreâs "The War on Heresy" argues that Catharism was âcontrived from the resources of well-stocked imaginationsâ of churchmen, with occasional reinforcement from miscellaneous and independent manifestations of local anticlericalism or apostolic enthusiasm.â In short, Moore claims that the men and women persecuted as Cathars were not the followers of a secret religion imported from the East, instead they were part of a broader spiritual revival taking place in the later twelfth and early thirteenth century. Mooreâs work is indicative of a larger historiographical trend towards examination of how heresy was constructed by the Church.
In art and music.
The principal legacy of the Cathar movement is in the poems and songs of the Cathar troubadors, though this artistic legacy is only a smaller part of the wider Occitan linguistic and artistic heritage. Recent artistic projects concentrating on the Cathar element in ProvenÃ§al and troubador art include commercial recording projects by Thomas Binkley, electric hurdy-gurdy artist Valentin Clastrier and his CD Heresie dedicated to the church at Cathars, La Nef, and Jordi Savall.
In popular culture.
The Cathars have been depicted or re-interpreted in popular books, video games, and films such as "The Holy Blood and the Holy Grail", "The Bone Clocks", "Labyrinth", "", Paulo Coelho's Brida, Bernard Cornwell's "The Grail Quest" series and Theodore Roszak's "Flicker". A number of semi-fictional conspiracy theories have been published that integrate the Cathars into their ideas, especially in France and Germany.
Catharism, along with other Christian movements including Fraticelli, Waldensianism, and Lollardy, is featured in the grand strategy game "Crusader Kings II", which is notable as being the only Catholic heresy in-game that allows female priests; it also grants the option of absolute cognatic succession laws (such as absolute primogeniture) and the appointment of female generals and councilors.
References.
Notes
Bibliography

</doc>
<doc id="7632" url="https://en.wikipedia.org/wiki?curid=7632" title="Cerebrospinal fluid">
Cerebrospinal fluid

Cerebrospinal fluid (CSF) is a clear, colorless body fluid found in the brain and spine. It is produced in the choroid plexuses of the ventricles of the brain. It acts as a cushion or buffer for the brain's cortex, providing basic mechanical and immunological protection to the brain inside the skull. The CSF also serves a vital function in cerebral autoregulation of cerebral blood flow.
The CSF occupies the subarachnoid space (between the arachnoid mater and the pia mater) and the ventricular system around and inside the brain and spinal cord. It constitutes the content of the ventricles, cisterns, and sulci of the brain, as well as the central canal of the spinal cord.
There is also a connection from the subarachnoid space to the bony labyrinth of the inner ear via the perilymphatic duct where the perilymph is continuous with the cerebrospinal fluid.
Structure.
Production.
The brain produces roughly 500 mL of cerebrospinal fluid per day. This fluid is constantly reabsorbed, so that only 100-160 mL is present at any one time.
Ependymal cells of the choroid plexus produce more than two thirds of CSF. The choroid plexus is a venous plexus contained within the four ventricles of the brain, hollow structures inside the brain filled with CSF. The remainder of the CSF is produced by the surfaces of the ventricles and by the lining surrounding the subarachnoid space.
Ependymal cells actively secrete sodium into the lateral ventricles. This creates osmotic pressure and draws water into the CSF space. Chloride, with a negative charge, moves with the positively charged sodium and a neutral charge is maintained. As a result, CSF contains a higher concentration of sodium and chloride than blood plasma, but less potassium, calcium and glucose and protein.
The OreÅ¡koviÄ and Klarica hypothesis suggests that the CSF is not primarily produced by the choroid plexus, but is being permanently produced inside the entire CSF system, as a consequence of water filtration through the capillary walls into the interstitial fluid (ISF) of the surrounding brain tissue, regulated by AQP-4.
Circulation or movement.
CSF circulates within the ventricular system of the brain. The ventricles are a series of cavities filled with CSF, inside the brain. The majority of CSF is produced from within the two lateral ventricles. From here, the CSF passes through the interventricular foramina to the third ventricle, then the cerebral aqueduct to the fourth ventricle. The fourth ventricle is an outpouching on the posterior part of the brainstem. From the fourth ventricle, the fluid passes through three openings to enter the subarachnoid space â these are the median aperture, and the lateral apertures. The subarachnoid space covers the brain and spinal cord. There is connection from the subarachnoid space to the bony labyrinth of the inner ear making the cerebrospinal fluid continuous with the perilymph.
A new hypothesis (2014) by Klarica and Oreskovic, based on more than thirty years of continuous experimental research, suggests that there is no unidirectional CSF circulation, but cardiac cycle-dependent bi-directional systolic-diastolic to-and-fro cranio-spinal CSF movements. This is based on the fact that, in the upright position, the hydrostatic pressure gradient was observed and it was found that there is a long-lasting sub-atmospheric intracranial pressure, zero CSF pressure in the cervical region and +30Â cm H2O in the lumbar region, and CSF can only flow from a region of higher to a region of lower CSF pressure. The team concluded that the term "circulation" should be avoided and a more appropriate term "movement" of CSF would be advisable.
The CSF moves in a pulsatile manner throughout the CSF system with a nearly zero net flow, as shown on an MRI.
Reabsorption.
It had been thought that CSF returns to the vascular system by entering the dural venous sinuses via the arachnoid granulations (or villi). However, some have suggested that CSF flow along the cranial nerves and spinal nerve roots allow it into the lymphatic channels; this flow may play a substantial role in CSF reabsorbtion, in particular in the neonate, in which arachnoid granulations are sparsely distributed. The flow of CSF to the nasal submucosal lymphatic channels through the cribriform plate seems to be especially important. The OreÅ¡koviÄ and Klarica hypothesis, on the other hand, suggests that the CSF does not flow unidirectionally to cortical SAS to be passively absorbed through arachnoid villi, but is being permanently produced and absorbed inside the entire CSF system, as a consequence of water filtration and reabsorption through the capillary walls into the interstitial fluid (ISF) of the surrounding brain tissue.
Contents.
The CSF is created from blood plasma and is largely similar to it, except that CSF is nearly protein-free compared with plasma and has some modified electrolyte levels. CSF contains approximately 0.3% plasma proteins, or approximately 15 to 40Â mg/dL, depending on sampling site, and it is produced at a rate of 500 ml/day. Since the subarachnoid space around the brain and spinal cord can contain only 135 to 150 ml, large amounts are drained primarily into the blood through arachnoid granulations in the superior sagittal sinus. Thus the CSF turns over about 3.7 times a day. This continuous flow into the venous system dilutes the concentration of larger, lipid-insoluble molecules penetrating the brain and CSF.
Healthy cerebrospinal fluid is free of red blood cells, and at most contains only a few white blood cells. Any cell count higher than that constitutes pleocytosis, an excess of cells.
CSF pressure, as measured by lumbar puncture (LP), is 10-18 
cmH2O (8-15 mmHg or 1.1-2 kPa) with the patient lying on the side and 20-30 cmH2O (16-24 mmHg or 2.1-3.2 kPa) with the patient sitting up. In newborns, CSF pressure ranges from 8 to 10 cmH2O (4.4â7.3Â mmHg or 0.78â0.98Â kPa). Most variations are due to coughing or internal compression of jugular veins in the neck. When lying down, the cerebrospinal fluid as estimated by lumbar puncture is similar to the intracranial pressure.
There are quantitative differences in the distributions of a number of proteins in the CSF. In general, globular proteins and albumin are in lower concentration in ventricular CSF compared to lumbar or cisternal fluid. The "IgG index" of cerebrospinal fluid is a measure of the immunoglobulin G content, and is elevated in multiple sclerosis. It is defined as
"IgG index = (IgGCSF / IgGserum ) / (albuminCSF / albuminserum"). A cutoff value has been suggested to be 0.73, with a higher value indicating presence of multiple sclerosis.
Development.
Around the third week of development, the embryo is a three-layered disc, covered on the dorsal surface by a layer of endoderm. In the middle of this surface is a linear structure called the notochord. As the endoderm proliferates, the notochord is dragged into the middle of the developing embryo and becomes the neural canal.
As the brain develops, by the fourth week of embryological development three swellings have formed within the embryo around the canal, near where the head will develop. These swellings represent different components of the central nervous system: the prosencephalon, mesencephalon and rhombencephalon.
The developing forebrain surrounds the neural cord. As the forebrain develops, the neural cord within it becomes a ventricle, ultimately forming the lateral ventricles. Along the inner surface of both ventricles, the ventricular wall remains thin, and a choroid plexus develops, releasing CSF. The CSF quickly fills the neural canal.
Function.
CSF serves several purposes:
Clinical significance.
When CSF pressure is elevated, cerebral blood flow may be constricted. When disorders of CSF flow occur, they may therefore affect not only CSF movement but also craniospinal compliance and the intracranial blood flow, with subsequent neuronal and glial vulnerabilities. The venous system is also important in this equation. Infants and patients shunted as small children may have particularly unexpected relationships between pressure and ventricular size, possibly due in part to venous pressure dynamics. This may have significant treatment implications, but the underlying pathophysiology needs to be further explored.
CSF connections with the lymphatic system have been demonstrated in several mammalian systems. Preliminary data suggest that these CSF-lymph connections form around the time that the CSF secretory capacity of the choroid plexus is developing (in utero). There may be some relationship between CSF disorders, including hydrocephalus and impaired CSF lymphatic transport.
CSF can leak from the dura as a result of different causes such as physical trauma or a lumbar puncture, or from no known cause when it is termed spontaneous cerebrospinal fluid leak. The leakage can cause a lack of CSF pressure and volume which can allow the brain to descend through the foramen magnum in the occipital bone where the lower portion of the brain may impact on cranial nerve complexes causing a variety of sensory symptoms.
Hydrocephalus.
Hydrocephalus is an abnormal accumulation of cerebrospinal fluid (CSF) in the ventricles of the brain and can be caused by an impaired flow of cerebrospinal fluid, reabsorption, or excessive production of CSF. Hydrocephalus is colloquially termed as "water on the brain" and is of medical importance. Hydrocephalus may cause increased intracranial pressure inside the skull. It may lead to enlargement of the cranium if hydrocephalus occurs during fetal development. It is usually accompanied by mental disability, sometimes by convulsive episodes and also tunnel vision. Hydrocephalus may become fatal if it is not corrected quickly. It is more common in infants, and in older adults.
Lumbar puncture.
CSF can be tested for the diagnosis of a variety of neurological diseases, usually obtained by a procedure called lumbar puncture.
Lumbar puncture is carried out under sterile conditions by inserting a needle into the subarachnoid space, usually between the third and fourth lumbar vertebrae. CSF is extracted through the needle, and tested. Cells in the fluid are counted, as are the levels of protein and glucose. These parameters alone may be extremely beneficial in the diagnosis of subarachnoid hemorrhage and central nervous system infections (such as meningitis). Moreover, a CSF culture examination may yield the microorganism that has caused the infection. By using more sophisticated methods, such as the detection of the oligoclonal bands, an ongoing inflammatory condition (for example, multiple sclerosis) can be recognized. A beta-2 transferrin assay is highly specific and sensitive for the detection of CSF leakage.
Lumbar puncture can also be performed to measure the intracranial pressure, which might be increased in certain types of hydrocephalus. However a lumbar puncture should never be performed if increased intracranial pressure is suspected due to certain situations such as a tumour, because it can lead to brain herniation and ultimately death.
About one third of people experience a headache after lumbar puncture.
Baricity.
This fluid has an importance in anesthesiology. Baricity refers to the density of a substance compared to the density of human cerebrospinal fluid. Baricity is used in anesthesia to determine the manner in which a particular drug will spread in the intrathecal space.
Alzheimer's disease.
A 2010 study showed analysis of CSF for three protein biomarkers that can indicate the presence of Alzheimer's disease. The three biomarkers are CSF amyloid beta 1-42, total CSF tau protein and P-Tau181P. In the study, the biomarker test showed good sensitivity, identifying 90% of persons with Alzheimer's disease, but poor specificity, as 36% of control subjects were positive for the biomarkers. The researchers suggested the low specificity may be explained by developing but not yet symptomatic disease in controls.
CNS Tumors.
Wang et al. in their article show the utility of tapping CSF to identify somatic mutations causing Central Nervous System (CNS) tumors.
History.
Various comments by ancient physicians have been read as referring to CSF. Hippocrates discussed "water" surrounding the brain when describing congenital hydrocephalus, and Galen referred to "excremental liquid" in the ventricles of the brain, which he believed was purged into the nose. But for some 16 intervening centuries of ongoing anatomical study, CSF remains unmentioned in the literature. This is perhaps because of the prevailing autopsy technique, which involved cutting off the head, thereby removing evidence of the CSF before the brain was examined. The modern rediscovery of CSF is now credited to Emanuel Swedenborg. In a manuscript written between 1741 and 1744, unpublished in his lifetime, Swedenborg referred to CSF as "spirituous lymph" secreted from the roof of the fourth ventricle down to the medulla oblongata and spinal cord. This manuscript was eventually published in translation in 1887.
Albrecht von Haller, a Swiss physician and physiologist, made note in his 1747 book on physiology that the "water" in the brain was secreted into the ventricles and absorbed in the veins, and when secreted in excess, could lead to hydrocephalus.
Francois Magendie studied the properties of CSF by vivisection. He discovered the foramen Magendie, the opening in the roof of the fourth ventricle, but mistakenly believed that CSF was secreted by the pia mater.
Thomas Willis (noted as the discoverer of the circle of Willis) made note of the fact that the consistency of the CSF is altered in meningitis.
In 1891, W. Essex Wynter began treating tubercular meningitis by tapping the subarachnoid space, and Heinrich Quincke began to popularize lumbar puncture, which he advocated for both diagnostic and therapeutic purposes. In 19th and early 20th century literature, particularly German medical literature, "liquor cerebrospinalis" was a term used to refer to CSF.
In 1912, William Mestrezat gave the first accurate description of the chemical composition of the CSF. In 1914, Harvey W. Cushing published conclusive evidence that the CSF is secreted by the choroid plexus.

</doc>
<doc id="7633" url="https://en.wikipedia.org/wiki?curid=7633" title="Cordial">
Cordial

Cordial may refer to:
Food and drink:
Other uses:

</doc>
<doc id="7635" url="https://en.wikipedia.org/wiki?curid=7635" title="Charles F. Hockett">
Charles F. Hockett

Charles Francis Hockett (January 17, 1916 â November 3, 2000) was an American linguist who developed many influential ideas in American structuralist linguistics. He represents the post-Bloomfieldian phase of structuralism often referred to as "distributionalism" or "taxonomic structuralism". His academic career spanned over half a century at Cornell and Rice universities.
Professional and academic career.
Education.
At the age of 16, Hockett enrolled at Ohio State University in Columbus, Ohio where he received a Bachelor of Arts and Master of Arts in ancient history. While enrolled at Ohio State, Hockett became interested in the work of Leonard Bloomfield, a leading figure in the field of structural linguistics. Hockett continued his education at Yale University where he studied anthropology and linguistics and received his PhD in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as Edward Sapir, George P. Murdock, and Benjamin Whorf. Hockett's dissertation was based on his fieldwork in Potawatomi; his paper on Potawatomi syntax was published in "Language" in 1939. In 1948 his dissertation was published as a series in the International Journal of American Linguistics. Following fieldwork in Kickapoo and MichoacÃ¡n, Mexico, Hockett did two years of postdoctoral study with Leonard Bloomfield in Chicago and Michigan.
Career.
Hockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of Modern Languages at Cornell University where he was responsible for directing the Chinese language program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to emeritus status in 1982. In 1986, he took up an adjunct post at Rice University in Houston, Texas, where he remained active until his death in 2000.
Achievements.
Charles Hockett held membership among many academic institutions such as the National Academy of Sciences the American Academy of Arts and Sciences, and the Society of Fellows at Harvard University. He served as president of both the Linguistic Society of America and the Linguistic Association of Canada and the United States.
In addition to making many contributions to the field of structural linguistics, Hockett also considered such things as Whorfian Theory, jokes, the nature of writing systems, slips of the tongue, and animal communication and their relativeness to speech.
Outside the realm of linguistics and anthropology, Hockett practiced musical performance and composition. Hockett composed a full-length opera called "The Love of DoÃ±a Rosita" which was based on a play by Federico GarcÃ­a Lorca and premiered at Ithaca College by the Ithaca Opera.
Hockett and his wife Shirley were vital leaders in the development of the Cayuga Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.
View on linguistics.
In his "Note on Structure" he argues that linguistics can be seen as a game and as a science. A linguist as player has a freedom for experimentation on all the utterances of a language, but no criterion to compare his analysis with other linguists. Late in his career, he was known for his stinging criticism of Chomskyan linguistics.
Key contributions.
Comparative method of linguistics.
One of Hockettâs most important contributions was his development of the design-feature approach to comparative linguistics where he attempted to distinguish the similarities and differences among animal communication systems and human language.
Hockett initially developed seven features which were published in the 1959 paper âAnimal âLanguagesâ and Human Language.â However, after many revisions, he settled on 13 design-features which can be found in the Scientific American article âThe Origin of Speech.â
Hockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, this differentiates human spoken language from animal communication and other human communication systems such as written language.
Hockett's 13 design features of language.
While Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all of the 13 design features. Additionally, traditional transmission, and duality of patterning are key to human language.
Design feature representation in other communication systems.
Foraging honeybees communicate with other members of their hive when they have discovered a relevant source of pollen, nectar, or water. In an effort to convey information about the location and distance of such resources, honeybees participate in a particular figure-eight dance known as the waggle dance.
In Hockett's "The Origin of Speech", he determined that the honeybee communication system of the waggle dance holds the following design features:
Gibbons are small apes in the family Hylobatidae. While gibbons share the same kingdom, phylum, class, and order of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.
Gibbons possess the first nine design features, but do not possess the last four (displacement, productivity, traditional transmission, and duality of patterning).
Later additions to the features.
In a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more Design Features, bringing the total to 16. The additional three are:
Other additions.
Cognitive scientist and linguist at the University of Sussex Larry Trask (1944â2004) offered an alternative term and definition for number 14, Prevarication:
There has since been one more Feature added to the list, by Dr. William Taft Stuart, a director of the Undergraduate Studies program at the University of Maryland: College Parkâs Anthropology school, part of the College of Behavioral and Social Sciences. His âextraâ Feature is:
This follows the definition of Grammar and Syntax, as given by Merriam-Websterâs Dictionary:
Relationship between the design features and animal communication.
Additionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.
While there have been studies attempting to disprove Chomsky, Marcus states that, "An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans." This is in response to a study performed by psychologist Timothy Gentner of the University of California at San Diego. Gentnerâs study found that starling songbirds use recursive grammar to identify âoddâ statements within a given âsong.â However, the study does not necessarily debunk Chomskyâs observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.
There is also thought that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding âhumansâ would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, 
today.
Hockett's "design features" of language and other animal communication systems.
The University of Oxford: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:

</doc>
<doc id="7638" url="https://en.wikipedia.org/wiki?curid=7638" title="Consilience">
Consilience

In science and history, consilience (also convergence of evidence or concordance of evidence) refers to the principle that evidence from independent, unrelated sources can "converge" to strong conclusions. That is, when multiple sources of evidence are in agreement, the conclusion can be very strong even when none of the individual sources of evidence is significantly so on its own. Most established scientific knowledge is supported by a convergence of evidence: if not, the evidence is comparatively weak, and there will not likely be a strong scientific consensus.
The principle is based on the unity of knowledge; measuring the same result by several different methods should lead to the same answer. For example, it should not matter whether one measures the distance between the Great Pyramids of Giza by laser rangefinding, by satellite imaging, or with a meter stick - in all three cases, the answer should be approximately the same. For the same reason, different dating methods in geochronology should concur, a result in chemistry should not contradict a result in geology, etc.
The word "consilience" was originally coined as the phrase "consilience of inductions" by William Whewell ("consilience" refers to a "jumping together" of knowledge). The word comes from Latin "com-" "together" and "-siliens" "jumping" (as in resilience).
Description.
Consilience requires the use of independent methods of measurement, meaning that the methods have few shared characteristics. That is, the mechanism by which the measurement is made is different; each method is dependent on an unrelated natural phenomenon. For example, the accuracy of laser rangefinding measurements is based on the scientific understanding of lasers, while satellite pictures and meter sticks rely on different phenomena. Because the methods are independent, when one of several methods is in error, it is very unlikely to be in error in the "same way" as any of the other methods, and a difference between the measurements will be observed. If the scientific understanding of the properties of lasers were inaccurate, then the laser measurement would be inaccurate but the others would not.
As a result, when several different methods agree, this is strong evidence that "none" of the methods are in error and the conclusion is correct. This is because of a greatly reduced likelihood of errors: for a consensus estimate from multiple measurements to be wrong, the errors would have to be similar for all samples and all methods of measurement, which is extremely unlikely. Random errors will tend to cancel out as more measurements are made, due to regression to the mean; systematic errors will be detected by differences between the measurements (and will also tend to cancel out since the direction of the error will still be random). This is how scientific theories reach high confidence â over time, they build up a large degree of evidence which converges on the same conclusion.
When results from different strong methods do appear to conflict, this is treated as a serious problem to be reconciled. For example, in the 19th century, the Sun appeared to be no more than 20 million years old, but the Earth appeared to be no less than 300 million years (resolved by the discovery of nuclear fusion and radioactivity, and the theory of quantum mechanics); or current attempts to resolve theoretical differences between quantum mechanics and general relativity.
Significance.
Because of consilience, the strength of evidence for any particular conclusion is related to how many independent methods are supporting the conclusion, as well as how different these methods are. Those techniques with the fewest (or no) shared characteristics provide the strongest consilience and result in the strongest conclusions. This also means that confidence is usually strongest when considering evidence from different fields, because the techniques are usually very different.
For example, the theory of evolution is supported by a convergence of evidence from genetics, molecular biology, paleontology, geology, biogeography, comparative anatomy, comparative physiology, and many other fields. In fact, the evidence within each of these fields is itself a convergence providing evidence for the theory. (As a result, to disprove evolution, most or all of these independent lines of evidence would have to be found to be in error.) The strength of the evidence, considered together as a whole, results in the strong scientific consensus that the theory is correct. In a similar way, evidence about the history of the universe is drawn from astronomy, astrophysics, planetary geology, and physics.
Finding similar conclusions from multiple independent methods is also evidence for the reliability of the methods themselves, because consilience eliminates the possibility of all potential errors that do not affect all the methods equally. This is also used for the validation of new techniques through comparison with the consilient ones. If only partial consilience is observed, this allows for the detection of errors in methodology; any weaknesses in one technique can be compensated for by the strengths of the others. Alternatively, if using more than one or two techniques for every experiment is infeasible, some of the benefits of consilience may still be obtained if it is well-established that these techniques usually give the same result.
Consilience is important across all of science, including the social sciences, and is often used as an argument for scientific realism by philosophers of science. Each branch of science studies a subset of reality that depends on factors studied in other branches. Atomic physics underlies the workings of chemistry, which studies emergent properties that in turn are the basis of biology. Psychology is not separate from the study of properties emergent from the interaction of neurons and synapses. Sociology, economics, and anthropology are each, in turn, studies of properties emergent from the interaction of countless individual humans. The concept that all the different areas of research are studying one real, existing universe is an apparent explanation of why scientific knowledge determined in one field of inquiry has often helped in understanding other fields.
Deviations from consilience.
Consilience does not forbid deviations: in fact, since not all experiments are perfect, some deviations from established knowledge are expected. However, when the convergence is strong enough, then new evidence inconsistent with the previous conclusion is not usually enough to outweigh that convergence. Without an equally strong convergence on the new result, the weight of evidence will still favor the established result. This means that the new evidence is most likely to be wrong.
Science denialism (for example, AIDS denialism) is often based on a misunderstanding of this property of consilience. A denier may promote small gaps not yet accounted for by the consilient evidence, or small amounts of evidence contradicting a conclusion without accounting for the pre-existing strength resulting from consilience. More generally, to insist that all evidence converge precisely with no deviations would be naÃ¯ve falsificationism, equivalent to considering a single contrary result to falsify a theory when another explanation, such as equipment malfunction or misinterpretation of results, is much more likely.
In history.
Historical evidence also converges in an analogous way. For example: if five ancient historians, none of whom knew each other, all claim that Julius Caesar seized power in Rome in 49 BCE, this is strong evidence in favor of that event occurring even if each individual historian is only partially reliable. By contrast, if the same historian had made the same claim five times in five different places (and no other types of evidence were available), the claim is much weaker because it originates from a single source. The evidence from the ancient historians could also converge with evidence from other fields, such as archaeology: for example, evidence that many senators fled Rome at the time, that the battles of Caesarâs civil war occurred, and so forth.
Consilience has also been discussed in reference to Holocaust denial. 
That is, individually the evidence may underdetermine the conclusion, but together they overdetermine it. A similar way to state this is that to ask for one "particular" piece of evidence in favor of a conclusion is a flawed question.
Outside the sciences.
In addition to the sciences, consilience can be important to the arts, ethics,and religion. Both artists and scientists have identified the importance of biology in the process of artistic innovation.
History of the concept.
Consilience has its roots in the ancient Greek concept of an intrinsic orderliness that governs our cosmos, inherently comprehensible by logical process, a vision at odds with mystical views in many cultures that surrounded the Hellenes. The rational view was recovered during the high Middle Ages, separated from theology during the Renaissance and found its apogee in the Age of Enlightenment.
Whewellâs definition was that:
More recent descriptions include:
Edward O. Wilson.
Although the concept of consilience in Whewell's sense was widely discussed by philosophers of science, the term was unfamiliar to the broader public until the end of the 20th century, when it was revived in "Consilience: The Unity of Knowledge," a 1998 book by the humanist biologist Edward Osborne Wilson, as an attempt to bridge the culture gap between the sciences and the humanities that was the subject of C. P. Snow's "The Two Cultures and the Scientific Revolution" (1959).
Wilson held that with the rise of the modern sciences, the sense of unity gradually was lost in the increasing fragmentation and specialization of knowledge in the last two centuries. He asserted that the sciences, humanities, and arts have a common goal: to give a purpose to understanding the details, to lend to all inquirers "a conviction, far deeper than a mere working proposition, that the world is orderly and can be explained by a small number of natural laws." Wilson's concept is a much broader notion of consilience than that of Whewell, who was merely pointing out that generalizations invented to account for one set of phenomena often account for others as well.
A parallel view lies in the term universology, which literally means "the science of the universe." Universology was first advocated for the study of the interconnecting principles and truths of all domains of knowledge by Stephen Pearl Andrews, a 19th-century utopian futurist and anarchist.

</doc>
<doc id="7642" url="https://en.wikipedia.org/wiki?curid=7642" title="Clarence Brown">
Clarence Brown

Clarence Leon Brown (May 10, 1890 â August 17, 1987) was an American film director.
Career.
After serving in World War I, Brown was given his first co-directing credit (with Tourneur) for The Great Redeemer (1920). Later that year, he directed a major portion of "The Last of the Mohicans" after Tourneur was injured in a fall.
Brown moved to Universal in 1924, and then to MGM, where he stayed until the mid-1950s. At MGM he was one of the main directors of their female starsâhe directed Joan Crawford six times and Greta Garbo seven.
He was nominated five times (see below) for the Academy Award as a director, and once as a producer, but never received an Oscar. However, he did win Best Foreign Film for "Anna Karenina" starring Garbo at the 1935 Venice International Film Festival.
Brown's films gained a total of 38 Academy Award nominations and earned nine Oscars. Brown himself received six Academy Award nominations and in 1949 won the British Academy Award for the film version of William Faulkner's "Intruder in the Dust.
In 1957, Brown was awarded The George Eastman Award, given by George Eastman House for distinguished contribution to the art of film. Brown retired a wealthy man due to his real estate investments, but refused to watch new movies, as he feared they might cause him to restart his career.
The Clarence Brown Theater, on the campus of the University of Tennessee, is named in his honor. He is tied with Robert Altman and Alfred Hitchcock for the most Academy Award nominations for best director without a single win.
Death.
Brown died from kidney failure on August 17, 1987, at the age of 97. He is interred at Forest Lawn Memorial Park in Glendale, California. For his contribution to the motion picture industry, Clarence Brown has a star on the Hollywood Walk of Fame at 1700 Vine Street.
Selected filmography.
NOTE: In 1929/1930, Brown received one Academy Award nomination for two films. According to the Academy of Motion Picture Arts and Sciences, "As allowed by the award rules for this year, a single nomination could honor work in one or more films."

</doc>
<doc id="7643" url="https://en.wikipedia.org/wiki?curid=7643" title="Conciliation">
Conciliation

Conciliation is an alternative dispute resolution (ADR) process whereby the parties to a dispute use a conciliator, who meets with the parties both separately and together in an attempt to resolve their differences. They do this by lowering tensions, improving communications, interpreting issues, encouraging parties to explore potential solutions and assisting parties in finding a mutually acceptable outcome.
Conciliation differs from arbitration in that the conciliation process, in and of itself, has no legal standing, and the conciliator usually has no authority to seek evidence or call witnesses, usually writes no decision, and makes no award.
Conciliation differs from mediation in that in conciliation, often the parties are in need of restoring or repairing a relationship, either personal or business.
Effectiveness.
Recent studies in the processes of negotiation have indicated the effectiveness of a technique that deserves mention here. A conciliator assists each of the parties to independently develop a list of all of their objectives (the outcomes which they desire to obtain from the conciliation). The conciliator then has each of the parties separately prioritize their own list from most to least important. He/She then goes back and forth between the parties and encourages them to "give" on the objectives one at a time, starting with the least important and working toward the most important for each party in turn. The parties rarely place the same priorities on all objectives, and usually have some objectives that are not listed by the other party. Thus the conciliator can quickly build a string of successes and help the parties create an atmosphere of trust which the conciliator can continue to develop.
Most successful conciliators are highly skilled negotiators. Some conciliators operate under the auspices of any one of several non-governmental entities, and for governmental agencies such as the Federal Mediation and Conciliation Service in the United States.
Historical conciliation.
Historical conciliation is an applied conflict resolution approach that utilizes historical narratives to positively transform relations between societies in conflicts. Historical conciliation can utilize many different methodologies, including mediation, sustained dialogue, apologies, acknowledgement, support of public commemoration activities, and public diplomacy.
Historical conciliation is not an excavation of objective facts. The point of facilitating historical questions is not to discover all the facts in regard to who was right or wrong. Rather, the objective is to discover the complexity, ambiguity, and emotions surrounding both dominant and non-dominant cultural and individual narratives of history. It is also not a rewriting of history. The goal is not to create a combined narrative that everyone agrees upon. Instead, the aim is to create room for critical thinking and more inclusive understanding of the past and conceptions of âthe other.â
Conflicts that are addressed through historical conciliation have their roots in conflicting identities of the people involved. Whether the identity at stake is their ethnicity, religion or culture, it requires a comprehensive approach that takes peopleâs needs, hopes, fears, and concerns into account.
Japan.
Japanese law makes extensive use of in civil disputes. The most common forms are civil conciliation and domestic conciliation, both of which are managed under the auspice of the court system by one judge and two non-judge "conciliators."
Civil conciliation is a form of dispute resolution for small lawsuits, and provides a simpler and cheaper alternative to litigation. Depending on the nature of the case, non-judge experts (doctors, appraisers, actuaries, and so on) may be called by the court as conciliators to help decide the case.
Domestic conciliation is most commonly used to handle contentious divorces, but may apply to other domestic disputes such as the annulment of a marriage or acknowledgment of paternity. Parties in such cases are required to undergo conciliation proceedings and may only bring their case to court once conciliation has failed.

</doc>
<doc id="7645" url="https://en.wikipedia.org/wiki?curid=7645" title="Cyclone (programming language)">
Cyclone (programming language)

The Cyclone programming language is intended to be a safe dialect of the C language. Cyclone is designed to avoid buffer overflows and other vulnerabilities that are endemic in C programs, without losing the power and convenience of C as a tool for system programming.
Cyclone development was started as a joint project of AT&T Labs Research and Greg Morrisett's group at Cornell in 2001. Version 1.0 was released on May 8, 2006.
Language features.
Cyclone attempts to avoid some of the common pitfalls of C, while still maintaining its look and performance. To this end, Cyclone places the following limits on programs:
To maintain the tool set that C programmers are used to, Cyclone provides the following extensions:
For a better high-level introduction to Cyclone, the reasoning behind Cyclone and the source of these lists, see this paper.
Cyclone looks, in general, much like C, but it should be viewed as a C-like language.
Pointer/reference types.
Cyclone implements three kinds of reference (following C terminology these are called pointers):
The purpose of introducing these new pointer types is to avoid common problems when using pointers. Take for instance a function, called codice_17 that takes a pointer to an int:
Although the person who wrote the function codice_17 could have inserted codice_1 checks, let us assume that for performance reasons they did not. Calling codice_20 will result in undefined behavior (typically, although not necessarily, a SIGSEGV signal being sent to the application). To avoid such problems, Cyclone introduces the codice_14 pointer type, which can never be codice_1. Thus, the "safe" version of codice_17 would be:
This tells the Cyclone compiler that the argument to codice_17 should never be codice_1, avoiding the aforementioned undefined behavior. The simple change of codice_13 to codice_14 saves the programmer from having to write codice_1 checks and the operating system from having to trap codice_1 pointer dereferences. This extra limit, however, can be a rather large stumbling block for most C programmers, who are used to being able to manipulate their pointers directly with arithmetic. Although this is desirable, it can lead to buffer overflows and other "off-by-one"-style mistakes. To avoid this, the codice_16 pointer type is delimited by a known bound, the size of the array. Although this adds overhead due to the extra information stored about the pointer, it improves safety and security. Take for instance a simple (and naÃ¯ve) codice_31 function, written in C:
This function assumes that the string being passed in is terminated by NULL (codice_32). However, what would happen if codice_33 were passed to this string? This is perfectly legal in C, yet would cause codice_31 to iterate through memory not necessarily associated with the string codice_35. There are functions, such as codice_36 which can be used to avoid such problems, but these functions are not standard with every implementation of ANSI C. The Cyclone version of codice_31 is not so different from the C version:
Here, codice_31 bounds itself by the length of the array passed to it, thus not going over the actual length. Each of the kinds of pointer type can be safely cast to each of the others, and arrays and strings are automatically cast to codice_16 by the compiler. (Casting from codice_16 to codice_13 invokes a bounds check, and casting from codice_16 to codice_14 invokes both a codice_1 check and a bounds check. Casting from codice_13 or codice_16 results in no checks whatsoever; the resulting codice_16 pointer has a size of 1.)
Dangling pointers and region analysis.
Consider the following code, in C:
This returns an object that is allocated on the stack of the function codice_48, which is not available after the function returns. While gcc and other compilers will warn about such code, the following will typically compile without warnings:
Cyclone does regional analysis of each segment of code, preventing dangling pointers, such as the one returned from this version of codice_48. All of the local variables in a given scope are considered to be part of the same region, separate from the heap or any other local region. Thus, when analyzing codice_48, the compiler would see that codice_51 is a pointer into the local stack, and would report an error.
Examples.
The best example to start with is the classic Hello world program:
External links.
Presentations:

</doc>
<doc id="7646" url="https://en.wikipedia.org/wiki?curid=7646" title="Cognitivism">
Cognitivism

Cognitivism may refer to:

</doc>
<doc id="7647" url="https://en.wikipedia.org/wiki?curid=7647" title="Counter (digital)">
Counter (digital)

In digital logic and computing, a counter is a device which stores (and sometimes displays) the number of times a particular event or process has occurred, often in relationship to a clock signal. The most common type is a sequential digital logic circuit with an input line called the "clock" and multiple output lines. The values on the output lines represent a number in the binary or BCD number system. Each pulse applied to the clock input increments or decrements the number in the counter.
A counter circuit is usually constructed of a number of flip-flops connected in cascade. Counters are a very widely-used component in digital circuits, and are manufactured as separate integrated circuits and also incorporated as parts of larger integrated circuits.
Electronic counters.
In electronics, counters can be implemented quite easily using register-type circuits such as the flip-flop, and a wide variety of classifications exist:
Each is useful for different applications. Usually, counter circuits are digital in nature, and count in natural binary. Many types of counter circuits are available as digital building blocks, for example a number of chips in the 4000 series implement different counters.
Occasionally there are advantages to using a counting sequence other than the natural binary sequenceâsuch as the binary coded decimal counter, a linear feedback shift register counter, or a Gray-code counter.
Counters are useful for digital clocks and timers, and in oven timers, VCR clocks, etc.
Asynchronous (ripple) counter.
An asynchronous (ripple) counter is a single d-type flip-flop, with its J (data) input fed from its own inverted output. This circuit can store one bit, and hence can count from zero to one before it overflows (starts over from 0). This counter will increment once for every clock cycle and takes two clock cycles to overflow, so every cycle it will alternate between a transition from 0 to 1 and a transition from 1 to 0. Notice that this creates a new clock with a 50% duty cycle at exactly half the frequency of the input clock. If this output is then used as the clock signal for a similarly arranged D flip-flop (remembering to invert the output to the input), one will get another 1 bit counter that counts half as fast. Putting them together yields a two-bit counter:
You can continue to add additional flip-flops, always inverting the output to its own input, and using the output from the previous flip-flop as the clock signal. The result is called a ripple counter, which can count to where "n" is the number of bits (flip-flop stages) in the counter. Ripple counters suffer from unstable outputs as the overflows "ripple" from stage to stage, but they do find frequent application as dividers for clock signals, where the instantaneous count is unimportant, but the division ratio overall is (to clarify this, a 1-bit counter is exactly equivalent to a divide by two circuit; the output frequency is exactly half that of the input when fed with a regular train of clock pulses).
The use of flip-flop outputs as clocks leads to timing skew between the count data bits, making this ripple technique incompatible with normal synchronous circuit design styles.
Synchronous counter.
In synchronous counters, the clock inputs of all the flip-flops are connected together and are triggered by the input pulses. Thus, all the flip-flops change state simultaneously (in parallel). The circuit below is a 4-bit synchronous counter. The J and K inputs of FF0 are connected to HIGH. FF1 has its J and K inputs connected to the output of FF0, and the J and K inputs of FF2 are connected to the output of an AND gate that is fed by the outputs of FF0 and FF1.
A simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the image to the right) is for each bit to toggle when all of the less significant bits are at a logic high state. For example, bit 1 toggles when bit 0 is logic high; bit 2 toggles when both bit 1 and bit 0 are logic high; bit 3 toggles when bit 2, bit 1 and bit 0 are all high; and so on.
Synchronous counters can also be implemented with hardware finite-state machines, which are more complex but allow for smoother, more stable transitions.
Hardware-based counters are of this type. A simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the image to the right) is for each bit to toggle when all of the less significant bits are at a logic high state
Decade counter.
A decade counter is one that counts in decimal digits, rather than binary. A decade counter may have each (that is, it may count in binary-coded decimal, as the 7490 integrated circuit did) or other binary encodings. "A decade counter is a binary counter that is designed to count to 1010b (decimal 10). An ordinary four-stage counter can be easily modified to a decade counter by adding a NAND gate as in the schematic to the right. Notice that FF2 and FF4 provide the inputs to the NAND gate. The NAND gate outputs are connected to the CLR input of each of the FFs." 
A decade counter is one that counts in decimal digits, rather than binary. It counts from 0 to 9 and then resets to zero. The counter output can be set to zero by pulsing the reset line low. The count then increments on each clock pulse until it reaches 1001 (decimal 9). When it increments to 1010 (decimal 10) both inputs of the NAND gate go high. The result is that the NAND output goes low, and resets the counter to zero. D going low can be a CARRY OUT signal, indicating that there has been a count of ten.
Ring counter.
A ring counter is a circular shift register which is initiated such that only one of its flip-flops is the state one while others are in their zero states.
A ring counter is a Shift Register (a cascade connection of flip-flops) with the output of the last one connected to the input of the first, that is, in a ring. Typically, a pattern consisting of a single bit is circulated so the state repeats every n clock cycles if n flip-flops are used.
Johnson counter.
A Johnson counter (or switch-tail ring counter, twisted ring counter, walking ring counter, or MÃ¶bius counter) is a modified ring counter, where the output from the last stage is inverted and fed back as input to the first stage. The register cycles through a sequence of bit-patterns, whose length is equal to twice the length of the shift register, continuing indefinitely. These counters find specialist applications, including those similar to the decade counter, digital-to-analog conversion, etc. They can be implemented easily using D- or JK-type flip-flops.
It is also known as twisted ring counter.
Computer science counters.
In computability theory, a counter is considered a type of memory. A counter stores a single natural number (initially zero) and can be arbitrarily long. A counter is usually considered in conjunction with a finite-state machine (FSM), which can perform the following operations on the counter:
The following machines are listed in order of power, with each one being strictly more powerful than the one below it:
For the first and last, it doesn't matter whether the FSM is a deterministic finite automaton or a nondeterministic finite automaton. They have the same power. The first two and the last one are levels of the Chomsky hierarchy.
The first machine, an FSM plus two counters, is equivalent in power to a Turing machine. See the article on counter machines for a proof.
Web counter.
A web counter or hit counter is a computer software program that indicates the number of visitors, or hits, a particular webpage has received. Once set up, these counters will be incremented by one every time the web page is accessed in a web browser.
The number is usually displayed as an inline digital image or in plain text or on a physical counter such as a mechanical counter. Images may be presented in a variety of fonts, or styles; the classic example is the wheels of an odometer.
"Web counter" was popular in the 1980s and 1990s, later replaced by more detailed and complete web traffic measures.
Computer based counters.
Many automation systems use PC and laptops to monitor different parameters of machines and production data. Counters may count parameters such as the number of pieces produced, the production batch number, and measurements of the amounts of material used.
Mechanical counters.
Long before electronics became common, mechanical devices were used to count events. These are known as tally counters. They typically consist of a series of disks mounted on an axle, with the digits 0 through 9 marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were used as odometers for bicycles and cars and in tape recorders, fuel dispensers, in production machinery as well as in other machinery. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter.
Hand held tally counters are used mainly for stocktaking and for counting people attending events.
Electromechanical counters were used to accumulate totals in tabulating machines that pioneered the data processing industry.

</doc>
<doc id="7649" url="https://en.wikipedia.org/wiki?curid=7649" title="Cervical mucus method">
Cervical mucus method

Cervical mucus method may refer to a specific method of fertility awareness or natural family planning:

</doc>
<doc id="7651" url="https://en.wikipedia.org/wiki?curid=7651" title="Coleridge">
Coleridge

Coleridge may refer to:

</doc>
<doc id="7655" url="https://en.wikipedia.org/wiki?curid=7655" title="Clay Mathematics Institute">
Clay Mathematics Institute

The Clay Mathematics Institute (CMI) is a private, non-profit foundation, based in Peterborough, New Hampshire, United States. CMI's scientific activities are managed from the President's office in Oxford, United Kingdom. The institute is "dedicated to increasing and disseminating mathematical knowledge." It gives out various awards and sponsorships to promising mathematicians. The institute was founded in 1998 through the sponsorship of Boston businessman Landon T. Clay. Harvard mathematician Arthur Jaffe was the first president of CMI. 
While the institute is best known for its Millennium Prize Problems, it carries out a wide range of activities, including a postdoctoral program (ten Clay Research Fellows are supported currently), conferences, workshops, and summer schools.
Governance.
The institute is run according to a standard structure comprising a scientific advisory committee that decides on grant-awarding and research proposals, and a board of directors that oversees and approves the committee's decisions. , the board is made up of members of the Clay family, whereas the advisory committee is composed of leading authorities in mathematics, namely Sir Andrew Wiles, Michael Hopkins, Carlos Kenig, Andrei Okounkov, and Simon Donaldson. Nicholas Woodhouse is the current president of CMI.
Millennium Prize Problems.
The institute is best known for establishing the Millennium Prize Problems on May 24, 2000. These seven problems are considered by CMI to be "important classic questions that have resisted solution over the years." For each problem, the first person to solve it will be awarded $1,000,000 by the CMI. In announcing the prize, CMI drew a parallel to Hilbert's problems, which were proposed in 1900, and had a substantial impact on 20th century mathematics. Of the initial 23 Hilbert problems, most of which have been solved, only the Riemann hypothesis (formulated in 1859) is included in the seven Millennium Prize Problems.
For each problem, the Institute had a professional mathematician write up an official statement of the problem, which will be the main standard by which a given solution will be measured against. The seven problems are:
Some of the mathematicians who were involved in the selection and presentation of the seven problems were Atiyah, Bombieri, Connes, Deligne, Fefferman, Milnor, Mumford, Wiles, and Witten.
Other awards.
The Clay Research Award.
In recognition of major breakthroughs in mathematical research, the institute has an annual prize - the Clay Research Award. Its recipients to date are Ian Agol, Manindra Agrawal, Yves Benoist, Manjul Bhargava, Danny Calegari, Alain Connes, Nils Dencker, Alex Eskin, David Gabai, Ben Green, Larry Guth, Christopher Hacon, Richard Hamilton, Michael Harris, Jeremy Kahn, Nets Katz, Laurent Lafforgue, GÃ©rard Laumon, Vladimir Markovic, James McKernan, Maryam Mirzakhani, NgÃ´ Báº£o ChÃ¢u, Rahul Pandharipande, Jonathan Pila, Jean-FranÃ§ois Quint, Peter Scholze, Oded Schramm, Stanislav Smirnov, Terence Tao, Clifford Taubes, Richard Taylor, Claire Voisin, Jean-Loup Waldspurger, Andrew Wiles, and Edward Witten.
Other activities.
Besides the Millennium Prize Problems, the Clay Mathematics Institute supports mathematics via the awarding of research fellowships (which range from two to five years, and are aimed at younger mathematicians), as well as shorter-term scholarships for programs, individual research, and book writing. The institute also has a yearly Clay Research Award, recognizing major breakthroughs in mathematical research. Finally, the institute organizes a number of summer schools, conferences, workshops, public lectures, and outreach activities aimed primarily at junior mathematicians (from the high school to postdoctoral level). CMI publications are available in PDF form at most six months after they appear in print.

</doc>
<doc id="7659" url="https://en.wikipedia.org/wiki?curid=7659" title="Cerebral arteriovenous malformation">
Cerebral arteriovenous malformation

A cerebral arteriovenous malformation (AVM) is an abnormal connection between the arteries and veins in the brain.
Signs and symptoms.
The most frequently observed problems, related to an AVM, are headaches and seizures, backaches, neckaches and eventual nausea, as the coagulated blood makes its way down to be dissolved in the individual's spinal fluid. It is supposed that 15% of the population, at detection, have no symptoms at all. Other common symptoms are a pulsing noise in the head, progressive weakness and numbness and vision changes as well as debilitating, excruciating pain.
In serious cases, the blood vessels rupture and there is bleeding within the brain (intracranial hemorrhage). Nevertheless in more than half of patients with AVM, hemorrhage is the first symptom. Symptoms due to bleeding include loss of consciousness, sudden and severe headache, nausea, vomiting, incontinence, and blurred vision, amongst others. Impairments caused by local brain tissue damage on the bleed site are also possible, including seizure, one-sided weakness (hemiparesis), a loss of touch sensation on one side of the body and deficits in language processing (aphasia). Minor bleeding can occur with no noticeable symptoms. Following the bleed's cessation, most AVM victims return to normal, after the blood vessel has had time to repair itself.
AVMs in certain critical locations may stop the circulation of the cerebrospinal fluid, causing accumulation of the fluid within the skull and giving rise to a clinical condition called hydrocephalus. A stiff neck can occur as the result of increased pressure within the skull and irritation of the meninges.
Diagnosis.
An AVM diagnosis is established by neuroimaging studies after a complete neurological and physical examination. Three main techniques are used to visualize the brain and search for AVM: computed tomography (CT), magnetic resonance imaging (MRI), and cerebral angiography. A CT scan of the head is usually performed first when the subject is symptomatic. It can suggest the approximate site of the bleed. MRI is more sensitive than CT in the diagnosis of AVMs and provides better information about the exact location of the malformation. More detailed pictures of the tangle of blood vessels that compose an AVM can be obtained by using radioactive agents injected into the blood stream. If a CT is used in conjunctiangiogram, this is called a computerized tomography angiogram; while, if MRI is used it is called magnetic resonance angiogram. The best images of an AVM are obtained through cerebral angiography. This procedure involves using a catheter, threaded through an artery up to the head, to deliver a contrast agent into the AVM. As the contrast agent flows through the AVM structure, a sequence of X-ray images are obtained.
Grading.
A common method of grading cerebral AVMs is the Spetzler-Martin grade. This system was designed to assess the patient's risk of neurological deficit after open surgical resection (surgical morbidity), based on characteristics of the AVM itself. Based on this system, AVMs may be classified as grades 1 - 5. This system was not intended to characterize risk of hemorrhage. 
"Eloquent cortex" is a name used by neurologists for areas of cortex that, if removed will result in loss of sensory processing or linguistic ability, minor paralysis, or paralysis.
The risk of post-surgical neurological deficit (difficulty with language, motor weakness, vision loss) increases with increasing Spetzler-Martin grade.
Pathophysiology.
AVMs are an abnormal connection between the arteries and veins in the human brain. Arteriovenous malformations are most commonly of prenatal origin. The cause of AVMs remains unknown. In a normal brain oxygen enriched blood from the heart travels in sequence through smaller blood vessels going from arteries, to arterioles and then capillaries. Oxygen is removed in the latter vessel to be used by the brain. After the oxygen is removed blood reaches venules and later veins which will take it back to the heart and lungs. On the other hand when there is an AVM blood goes directly from arteries to veins through the abnormal vessels disrupting the normal circulation of blood.
Prognosis.
The main risk is intracranial hemorrhage. This risk is difficult to quantify since many patients with asymptomatic AVMs will never come to medical attention. Small AVMs tend to bleed more often than do larger ones, the opposite of cerebral aneurysms. If a rupture or bleeding incident occurs, the blood may penetrate either into the brain tissue (cerebral hemorrhage) or into the subarachnoid space, which is located between the sheaths (meninges) surrounding the brain (subarachnoid hemorrhage). Bleeding may also extend into the ventricular system (intraventricular hemorrhage). Cerebral hemorrhage appears to be most common.
One long-term study (mean follow up greater than 20 years) of over 150 symptomatic AVMs (either presenting with bleeding or seizures) found the risk of cerebral hemorrhage to be approximately 4% per year, slightly higher than the 2-3% seen in other studies. A simple, rough approximation of a patient's lifetime bleeding risk is 105 - (patient age in years). This equation assumes a 3% yearly bleeding risk. For example, a healthy 30-year-old patient would have approximately a 75% lifetime risk of at least one bleeding event.
Treatment.
Treatment depends on the location and size of the AVM and whether there is bleeding or not.
The treatment in the case of sudden bleeding is focused on restoration of vital function. Anticonvulsant medications such as phenytoin are often used to control seizure; medications or procedures may be employed to relieve intracranial pressure. Eventually, curative treatment may be required to prevent recurrent hemorrhage. However, any type of intervention may also carry a risk of creating a neurological deficit.
Preventive treatment of as yet unruptured brain AVMs has been controversial, as several studies suggested favorable long-term outcome for unruptured AVM patients not undergoing intervention. The NIH-funded longitudinal ARUBA study ("A Randomized trial of Unruptured Brain AVMs) compares the risk of stroke and death in patients with preventive AVM eradication versus those followed without intervention. Interim results suggest that fewer strokes occur as long as patients with unruptured AVM do not undergo intervention. Because of the higher than expected event rate in the interventional arm of the ARUBA study, NIH/NINDS has stopped patient enrollment in April 2013, while continuing to follow all participants to determine whether the difference in stroke and death in the two arms changes over time.
Surgical elimination of the blood vessels involved is the preferred curative treatment for many types of AVM. Surgery is performed by a neurosurgeon who temporarily removes part of the skull (craniotomy), separates the AVM from surrounding brain tissue, and resects the abnormal vessels. While surgery can result in an immediate, complete removal of the AVM, risks exist depending on the size and the location of the malformation. The preferred treatment of Spetzler-Martin grade 1 and 2 AVMs in young, healthy patients is surgical resection due to the relatively small risk of neurological damage compared to the high lifetime risk of hemorrhage. Grade 3 AVMs may or may not be amenable to surgery. Grade 4 and 5 AVMs are not usually surgically treated.
Radiosurgery has been widely used on small AVMs with considerable success. The Gamma Knife is an apparatus used to precisely apply a controlled radiation dosage to the volume of the brain occupied by the AVM. While this treatment does not require an incision and craniotomy (with their own inherent risks), three or more years may pass before the complete effects are known, during which time patients are at risk of bleeding. Complete obliteration of the AVM may or may not occur after several years, and repeat treatment may be needed. Radiosurgery is itself not without risk. In one large study, nine percent of patients had transient neurological symptoms, including headache, after radiosurgery for AVM. However, most symptoms resolved, and the long-term rate of neurological symptoms was 3.8%.
Embolization is the occlusion of blood vessels most commonly with a glue-like substance introduced by a radiographically guided catheter. Such glue blocks the vessel and reduces blood flow into the AVM. Embolization is frequently used as an adjunct to either surgery or radiation treatment. Before other treatments it reduces the size of the AVM while during surgery it reduces the risk of bleeding. However, embolization alone may completely obliterate some AVMs. In high flow intranidal fistula we can use balloon to reduce the flow so that embolization can be done safely.
Epidemiology.
The annual new detection rate incidence of AVMs is approximately 1 per 100,000 a year. The point prevalence in adults is approximately 18 per 100,000. AVMs are more common in males than females, although in females pregnancy may start or worsen symptoms due the increase in blood flow and volume it usually brings.
Research directions.
No randomized, controlled clinical trial has established a survival benefit for treating patients (either with open surgery or radiosurgery) with AVMs that have not yet bled.

</doc>
<doc id="7660" url="https://en.wikipedia.org/wiki?curid=7660" title="Comparative method (linguistics)">
Comparative method (linguistics)

In linguistics, the comparative method is a technique for studying the development of languages by performing a feature-by-feature comparison of two or more languages with common descent from a shared ancestor, as opposed to the method of internal reconstruction, which analyses the internal development of a single language over time. Ordinarily both methods are used together to reconstruct prehistoric phases of languages, to fill in gaps in the historical record of a language, to discover the development of phonological, morphological, and other linguistic systems, and to confirm or refute hypothesized relationships between languages.
The comparative method was developed over the 19th century. Key contributions were made by the Danish scholars Rasmus Rask and Karl Verner and the German scholar Jacob Grimm. The first linguist to offer reconstructed forms from a proto-language was August Schleicher, in his "Compendium der vergleichenden Grammatik der indogermanischen Sprachen", originally published in 1861. Here is Schleicherâs explanation of why he offered reconstructed forms:
In the present work an attempt is made to set forth the inferred Indo-European original language side by side with its really existent derived languages. Besides the advantages offered by such a plan, in setting immediately before the eyes of the student the final results of the investigation in a more concrete form, and thereby rendering easier his insight into the nature of particular Indo-European languages, there is, I think, another of no less importance gained by it, namely that it shows the baselessness of the assumption that the non-Indian Indo-European languages were derived from Old-Indian (Sanskrit).
Demonstrating genetic relationship.
The comparative method aims to prove that two or more historically attested languages are descended from a single proto-language by comparing lists of cognate terms. From them, regular sound correspondences between the languages are established, and a sequence of regular sound changes can then be postulated, which allows the proto-language to be reconstructed. Relation is deemed certain only if at least a partial reconstruction of the common ancestor is feasible, and if regular sound correspondences can be established with chance similarities ruled out.
Terminology.
"Descent" is defined as transmission across the generations: children learn a language from the parents' generation and after being influenced by their peers transmit it to the next generation, and so on. For example, a continuous chain of speakers across the centuries links Vulgar Latin to all of its modern descendants.
Two languages are "genetically related" if they descended from the same ancestor language. For example, Italian and French both come from Latin and therefore belong to the same family, the Romance languages.
However, it is possible for languages to have different degrees of relatedness. English, for example, is related to both German and Russian, but is more closely related to the former than it is to the latter. Although all three languages share a common ancestor, Proto-Indo-European, English and German also share a more recent common ancestor, Proto-Germanic, while Russian does not. Therefore, English and German are considered to belong to a different subgroup, the Germanic languages.
"Shared retentions" from the parent language are not sufficient evidence of a sub-group. For example, as a result of heavy borrowing from Arabic into Persian, Modern Persian in fact takes more of its vocabulary from Arabic than from its direct ancestor, Proto-Indo-Iranian. The division of related languages into sub-groups is more certainly accomplished by finding "shared linguistic innovations" from the parent language.
Origin and development of the method.
Languages have been compared since antiquity. For example, in the 1st century BC the Romans were aware of the similarities between Greek and Latin, which they explained mythologically, as the result of Rome being a Greek colony speaking a debased dialect. In the 9th or 10th century, Yehuda Ibn Quraysh compared the phonology and morphology of Hebrew, Aramaic, and Arabic, but attributed this resemblance to the Biblical story of Babel, with Abraham, Isaac and Joseph retaining Adam's language, with other languages at various removes becoming more altered from the original Hebrew.
In publications of 1647 and 1654, Marcus van Boxhorn first described a rigid methodology for historical linguistic comparisons and proposed the existence of an Indo-European proto-language (which he called "Scythian") unrelated to Hebrew, but ancestral to Germanic, Greek, Romance, Persian, Sanskrit, Slavic, Celtic and Baltic languages. The Scythian theory was further developed by Andreas JÃ¤ger (1686) and William Wotton (1713), who made first forays to reconstruct this primitive common language. In 1710 and 1723, Lambert ten Kate first formulated the regularity of sound laws, introducing among others, the term root vowel.
Another early systematic attempt to prove the relationship between two languages on the basis of similarity of grammar and lexicon was made by the Hungarian JÃ¡nos Sajnovics in 1770, when he attempted to demonstrate the relationship between Sami and Hungarian (work that was later extended to the whole Finno-Ugric language family in 1799 by his countryman Samuel Gyarmathi), But the origin of modern historical linguistics is often traced back to Sir William Jones, an English philologist living in India, who in 1786 made his famous 
âThe Sanscrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and the forms of grammar, than could possibly have been produced by accident; so strong indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists. There is a similar reason, though not quite so forcible, for supposing that both the Gothick and the Celtick, though blended with a very different idiom, had the same origin with the Sanscrit; and the old Persian might be added to the same family.â
The comparative method developed out of attempts to reconstruct the proto-language mentioned by Jones, which he did not name, but subsequent linguists named Proto-Indo-European (PIE). The first professional comparison between the Indo-European languages known then was made by the German linguist Franz Bopp in 1816. Though he did not attempt a reconstruction, he demonstrated that Greek, Latin and Sanskrit shared a common structure and a common lexicon. Friedrich Schlegel in 1808 first stated the importance of using the eldest possible form of a language when trying to prove its relationships; in 1818, Rasmus Christian Rask developed the principle of regular sound changes to explain his observations of similarities between individual words in the Germanic languages and their cognates in Greek and Jacob Grimm - better known for his "Fairy Tales" - in "Deutsche Grammatik" (published 1819-37 in four volumes) made use of the comparative method in attempting to show the development of the Germanic languages from a common origin, the first systematic study of diachronic language change.
Both Rask and Grimm were unable to explain apparent exceptions to the sound laws that they had discovered. Although Hermann Grassmann explained one of these anomalies with the publication of Grassmann's law in 1862, it was Karl Verner who in 1875 made a methodological breakthrough when he identified a pattern now known as Verner's law, the first sound law based on comparative evidence showing that a phonological change in one phoneme could depend on other factors within the same word, such as the neighbouring phonemes and the position of the accent, now called "conditioning environments".
Similar discoveries made by the "Junggrammatiker" (usually translated as Neogrammarians) at the University of Leipzig in the late 1800s led them to conclude that all sound changes were ultimately regular, resulting in the famous statement by Karl Brugmann and Hermann Osthoff in 1878 that "sound laws have no exceptions". This idea is fundamental to the modern comparative method, since the method necessarily assumes regular correspondences between sounds in related languages, and consequently regular sound changes from the proto-language. This "Neogrammarian Hypothesis" led to application of the comparative method to reconstruct Proto-Indo-European, with Indo-European being at that time by far the most well-studied language family. Linguists working with other families soon followed suit, and the comparative method quickly became the established method for uncovering linguistic relationships.
Application.
There is no fixed set of steps to be followed in the application of the comparative method, but some steps are suggested by Lyle Campbell and Terry Crowley, both authors of introductory texts in historical linguistics. The abbreviated summary below is based on their concepts of how to proceed.
Step 1, assemble potential cognate lists.
This step involves making lists of words that are likely cognates among the languages being compared. If there is a regularly recurring match between the phonetic structure of basic words with similar meanings a genetic kinship can probably be established. For example, looking at the Polynesian family linguists might come up with a list similar to the following (a list actually used by them would be much longer):
Borrowings or false cognates could skew or obscure the correct data. For example, English "taboo" () is like the six Polynesian forms due to borrowing from Tongan into English, and not because of a genetic similarity. This problem can usually be overcome by using basic vocabulary such as kinship terms, numbers, body parts, pronouns, and other basic terms. Nonetheless, even basic vocabulary can be sometimes borrowed. Finnish, for example, borrowed the word for "mother", "Ã¤iti", from Gothic "aiÃ¾ei". While English borrowed the pronouns "they", "them", and "their(s)" from Norse, Thomason and Everett argue that PirahÃ£, a Muran language of South America for which a number of controversial claims are made, borrowed all its pronouns from Nhengatu.
Step 2, establish correspondence sets.
The next step is to determine the regular sound correspondences exhibited by the potential cognates lists. Mere phonetic similarity, as between English "day" and Latin "dies" (both with the same meaning), has no probative value. English initial "d-" does "not" regularly match and whatever sporadic matches can be observed are due either to chance (as in the above example) or to borrowing (for example, Latin "diabolus" and English "devil", both ultimately of Greek origin). English and Latin "do" exhibit a regular correspondence of "t-" : "d-" (where the notation "A : B" means "A corresponds to B"); for example,
If there are many regular correspondence sets of this kind (the more the better), then a common origin becomes a virtual certainty, particularly if some of the correspondences are non-trivial or unusual.
Step 3, discover which sets are in complementary distribution.
During the late 18th to late 19th century, two major developments improved the method's effectiveness.
First, it was found that many sound changes are conditioned by a specific "context". For example, in both Greek and Sanskrit, an aspirated stop evolved into an unaspirated one, but only if a second aspirate occurred later in the same word; this is Grassmann's law, first described for Sanskrit by Sanskrit grammarian PÄá¹ini and promulgated by Hermann Grassmann in 1863.
Second, it was found that sometimes sound changes occurred in contexts that were later lost. For instance, in Sanskrit velars ("k"-like sounds) were replaced by palatals ("ch"-like sounds) whenever the following vowel was "*i" or "*e". Subsequent to this change, all instances of "*e" were replaced by "a". The situation would have been unreconstructable, had not the original distribution of "e" and "a" been recoverable from the evidence of other Indo-European languages. For instance, Latin suffix "que", "and", preserves the original "*e" vowel that caused the consonant shift in Sanskrit:
Verner's Law, discovered by Karl Verner in about 1875, is a similar case: the voicing of consonants in Germanic languages underwent a change that was determined by the position of the old Indo-European accent. Following the change, the accent shifted to initial position. Verner solved the puzzle by comparing the Germanic voicing pattern with Greek and Sanskrit accent patterns.
This stage of the comparative method, therefore, involves examining the correspondence sets discovered in step 2 and seeing which of them apply only in certain contexts. If two (or more) sets apply in complementary distribution, they can be assumed to reflect a single original phoneme: "some sound changes, particularly conditioned sound changes, can result in a proto-sound being associated with more than one correspondence set".
For example, the following potential cognate list can be established for Romance languages, which descend from Latin:
They evidence two correspondence sets, "k : k" and "k : :
Since French "" only occurs before "a" where the other languages also have "a", while French "k" occurs elsewhere, the difference is due to different environments (being before an "a" conditions the change) and the sets are complementary. They can therefore be assumed to reflect a single proto-phoneme (in this case "*k", spelled <c> in Latin). The original words are corpus, crudus, catena and captiare, all with an initial k-sound. If more evidence along these lines were given, one might conclude to an alteration of the original k because of a different environment.
A more complex case involves consonant clusters in Proto-Algonquian. The Algonquianist Leonard Bloomfield used the reflexes of the clusters in four of the daughter languages to reconstruct the following correspondence sets:
Although all five correspondence sets overlap with one another in various places, they are not in complementary distribution, and so Bloomfield recognized that a different cluster must be reconstructed for each set; his reconstructions were, respectively, "*hk", "*xk", "*Äk" (=), "*Å¡k" (=), and "Ã§k" (where "âxâ" and "âÃ§â" are arbitrary symbols, not attempts to guess the phonetic value of the proto-phonemes).
Step 4, reconstruct proto-phonemes.
Typology assists in deciding what reconstruction best fits the data. For example, the voicing of voiceless stops between vowels is common, but not the devoicing of voiced stops there. If a correspondence "-t-" : "-d-" between vowels is found in two languages, the proto-phoneme is more likely to be "*-t-", with a development to the voiced form in the second language. The opposite reconstruction would create a rare type.
However, unusual sound changes do occur. The Proto-Indo-European word for "two", for example, is reconstructed as "*dwÅ", which is reflected in Classical Armenian as "erku". Several other cognates demonstrate a regular change "*dw-" â "erk-" in Armenian. Similarly, in Bearlake, a dialect of the Athabaskan language of Slavey, there has been a sound change of Proto-Athabaskan "*ts" â Bearlake '. It is very unlikely that "*dw-" changed directly into "erk-" and "*ts" into ', but instead they must have gone through several intermediate steps to arrive at the later forms. It is not phonetic similarity which matters when utilizing the comparative method, but regular sound correspondences.
By the Principle of Economy, the reconstruction of a proto-phoneme should require as few sound changes as possible to arrive at the modern reflexes in the daughter languages. For example, Algonquian languages exhibit the following correspondence set:
The simplest reconstruction for this set would be either "*m" or "*b". Both "*m" â "b" and "*b" â "m" are likely. Because "m" occurs in five of the languages, and "b" in only one, if "*b" is reconstructed, then it is necessary to assume five separate changes of "*b" â "m", whereas if "*m" is reconstructed, it is only necessary to assume a single change of "*m" â "b". "*m" would be most economical. (This argument assumes that the languages other than Arapaho are at least partly independent of each other. If they all formed a common subgroup, the development "*b" â "m" would only have to be assumed having occurred once.)
Step 5, examine the reconstructed system typologically.
In the final step, the linguist checks to see how the proto-phonemes fit the known typological constraints. For example, in a hypothetical system,
there is only one voiced stop, "*b", and although there is an alveolar and a velar nasal, "*n" and "*Å", there is no corresponding labial nasal. However, languages generally (though not always) tend to maintain symmetry in their phonemic inventories. In this case, the linguist might attempt to investigate the possibilities that what was earlier reconstructed as "*b" is in fact "*m", or that the "*n" and "*Å" are in fact "*d" and "*g".
Even a symmetrical system can be typologically suspicious. For example, the traditional Proto-Indo-European stop inventory is:
An earlier voiceless aspirated row was removed on grounds of insufficient evidence. Since the mid-20th century, a number of linguists have argued that this phonology is implausible; that it is extremely unlikely for a language to have a voiced aspirated (breathy voice) series without a corresponding voiceless aspirated series. A potential solution was provided by Thomas Gamkrelidze and Vyacheslav Ivanov, who argued that the series traditionally reconstructed as plain voiced should in fact be reconstructed as glottalized â either implosive or ejective . The plain voiceless and voiced aspirated series would thus be replaced by just voiceless and voiced, with aspiration being a non-distinctive quality of both. This example of the application of linguistic typology to linguistic reconstruction has become known as the Glottalic Theory. It has a large number of proponents but is not generally accepted. As an alternative, the voiceless aspirated row was restored.
The reconstruction of proto-sounds logically precedes the reconstruction of grammatical morphemes (word-forming affixes and inflectional endings), patterns of declension and conjugation, and so on. The full reconstruction of an unrecorded protolanguage is an open-ended task.
Limitations.
Problems with the history of historical linguistics.
The limitations of the comparative method were recognized by the very linguists who developed it, but it is still seen as a valuable tool. In the case of Indo-European, the method seemed to at least partially validate the centuries-old search for an Ursprache, the original language. These others were presumed ordered in a family tree, becoming the Tree model of the neogrammarians.
The archaeologists followed suit, attempting to find archaeological evidence of a culture or cultures that could be presumed to have spoken a proto-language, such as Vere Gordon Childe's "The Aryans: a study of Indo-European origins", 1926. Childe was a philologist turned archaeologist. These views culminated in the "Siedlungsarchaologie", or "settlement-archaeology", of Gustaf Kossinna, becoming known as "Kossinna's Law." He asserted that cultures represent ethnic groups, including their languages. It was rejected as a law in the post-World-War-II era. The fall of Kossinna's Law removed the temporal and spatial framework previously applied to many proto-languages. Fox concludes:
The Comparative Method "as such" is not, in fact, historical; it provides evidence of linguistic relationships to which we may give a historical interpretation. ...increased knowledge about the historical processes involved has probably made historical linguists less prone to equate the idealizations required by the method with historical reality. ...Provided we keep interpretation of the results and the method itself apart, the Comparative Method can continue to be used in the reconstruction of earlier stages of languages.
Proto-languages can be verified in many historical instances, such as Latin. Although no longer a law, settlement-archaeology is known to be essentially valid for some cultures that straddle history and prehistory, such as the Celtic Iron Age (mainly Celtic) and Mycenaean civilization (mainly Greek). None of these models can be or have been completely rejected, and yet none alone are sufficient.
Problems with the neogrammarian hypothesis.
The foundation of the comparative method, and of comparative linguistics in general, is the Neogrammarians' fundamental assumption that "sound laws have no exceptions." When it was initially proposed, critics of the Neogrammarians proposed an alternate position, summarized by the maxim "each word has its own history". Several types of change do in fact alter words in non-regular ways. Unless identified, they may hide or distort laws and cause false perceptions of relationship.
Borrowing.
All languages borrow words from other languages in various contexts. They are likely to have followed the laws of the languages from which they were borrowed rather than the laws of the borrowing language.
Areal diffusion.
Borrowing on a larger scale occurs in areal diffusion, when features are adopted by contiguous languages over a geographical area. The borrowing may be phonological, morphological or lexical. A false proto-language over the area may be reconstructed for them or may be taken to be a third language serving as a source of diffused features.
Several areal features and other influences may converge to form a sprachbund, a wider region sharing features that appear to be related but are diffusional. For instance, the Mainland Southeast Asia linguistic area suggested several false classifications of such languages as Chinese, Thai and Vietnamese before it was recognized.
Random mutations.
Sporadic changes, such as irregular inflections, compounding, and abbreviation, do not follow any laws. For example, the Spanish words "palabra" ('word'), "peligro" ('danger') and "milagro" ('miracle') should have been "parabla", "periglo", "miraglo" by regular sound changes from the Latin "parabÅla", "perÄ«cÅ­lum" and "mÄ«rÄcÅ­lum", but the "r" and "l" changed places by sporadic metathesis.
Analogy.
Analogy is the sporadic change of a feature to be like another feature in the same or a different language. It may affect a single word or be generalized to an entire class of features, such as a verb paradigm. For example, the Russian word for "nine", by regular sound changes from Proto-Slavic, should have been , but is in fact . It is believed that the initial ' changed to ' under influence of the word for "ten" in Russian, .
Gradual application.
Students of contemporary language changes, such as William Labov, note that even a systematic sound change is at first applied in an unsystematic fashion, with the percentage of its occurrence in a person's speech dependent on various social factors. The sound change gradually spreads, a process known as lexical diffusion. While not invalidating the Neogrammarians' axiom that "sound laws have no exceptions", their gradual application shows that they do not always apply to all lexical items at the same time. Hock notes, "While it probably is true in the long run every word has its own history, it is not justified to conclude as some linguists have, that therefore the Neogrammarian position on the nature of linguistic change is falsified."
Problems with the Tree Model.
The comparative method is used to construct a Tree model (German "Stammbaum") of language evolution, in which daughter languages are seen as branching from the proto-language, gradually growing more distant from it through accumulated phonological, morpho-syntactic, and lexical changes.
The presumption of a well-defined node.
The tree model features nodes that are presumed to be distinct proto-languages existing independently in distinct regions during distinct historical times. The reconstruction of unattested proto-languages lends itself to that illusion: they cannot be verified and the linguist is free to select whatever definite times and places for them seem best. Right from the outset of Indo-European studies, however, Thomas Young said:It is not, however, very easy to say what the definition should be that should constitute a separate language, but it seems most natural to call those languages distinct, of which the one cannot be understood by common persons in the habit of speaking the other â¦ Still, however, it may remain doubtfull whether the Danes and the Swedes could not, in general, understand each other tolerably well â¦ nor is it possible to say if the twenty ways of pronouncing the sounds, belonging to the Chinese characters, ought or ought not to be considered as so many languages or dialectsâ¦ But, â¦ the languages so nearly allied must stand next to each other in a systematic orderâ¦
The assumption of uniformity in a proto-language, implicit in the comparative method, is problematic. Even in small language communities there are always dialect differences, whether based on area, gender, class, or other factors. The PirahÃ£ language of Brazil is spoken by only several hundred people, but it has at least two different dialects, one spoken by men and one by women. Campbell points out:
It is not so much that the comparative method 'assumes' no variation; rather, it is just that there is nothing built into the comparative method which would allow it to address variation directly...This assumption of uniformity is a reasonable idealization; it does no more damage to the understanding of the language than, say, modern reference grammars do which concentrate on a language's general structure, typically leaving out consideration of regional or social variation.
Different dialects, as they evolve into separate languages, remain in contact with one another and influence each other. Even after they are considered distinct, languages near to one another continue to influence each other, often sharing grammatical, phonological, and lexical innovations. A change in one language of a family may spread to neighboring languages; and multiple waves of change are communicated like waves across language and dialect boundaries, each with its own randomly delimited range. If a language is divided into an inventory of features, each with its own time and range (isoglosses), they do not all coincide. History and prehistory may not offer a time and place for a distinct coincidence, as may be the case for proto-Italic, in which case the proto-language is only a concept. However, Hock observes:
The discovery in the late nineteenth century that isoglosses can cut across well-established linguistic boundaries at first created considerable attention and controversy. And it became fashionable to oppose a wave theory to a tree theory... Today, however, it is quite evident that the phenomena referred to by these two terms are complementary aspects of linguistic change...
Subjectivity of the reconstruction.
The reconstruction of unknown proto-languages is inherently subjective. In the Proto-Algonquian example above, the choice of "*m" as the parent phoneme is only "likely", not "certain". It is conceivable that a Proto-Algonquian language with "*b" in those positions split into two branches, one which preserved "*b" and one which changed it to "*m" instead; and while the first branch only developed into Arapaho, the second spread out wider and developed into all the other Algonquian tribes. It is also possible that the nearest common ancestor of the Algonquian languages used some other sound instead, such as "*p", which eventually mutated to "*b" in one branch and to "*m" in the other. While examples of strikingly complicated and even circular developments are indeed known to have occurred (such as PIE "*t" > Pre-Proto-Germanic "*Ã¾" > PG "*Ã°" > Proto-West-Germanic "*d" > Old High German "t" in "fater" > Modern German "Vater"), in the absence of any evidence or other reason to postulate a more complicated development, the preference of a simpler explanation is justified by the principle of parsimony, also known as Occam's razor. Since reconstruction involves many of these choices, some linguists prefer to view the reconstructed features as abstract representations of sound correspondences, rather than as objects with a historical time and place.
The existence of proto-languages and the validity of the comparative method is verifiable in cases where the reconstruction can be matched to a known language, which may only be known as a shadow in the loanwords of another language. For example, Finnic languages such as Finnish have borrowed many words from an early stage of Germanic, and the shape of the loans matches the forms that have been reconstructed for Proto-Germanic. Finnish "kuningas" 'king' and "kaunis" 'beautiful' match the Germanic reconstructions *"kuningaz" and *"skauniz" (> German "KÃ¶nig" 'king', "schÃ¶n" 'beautiful').
Additional models.
The Wave model was developed in the 1870s as an alternative to the Tree model, in order to represent the historical patterns of language diversification. Both the tree-based and the wave-based representations are compatible with the Comparative Method.
By contrast, some approaches are incompatible with the Comparative method, including glottochronology and mass lexical comparison. Most historical linguists consider these to be flawed and unreliable.

</doc>
<doc id="7661" url="https://en.wikipedia.org/wiki?curid=7661" title="Council of Constance">
Council of Constance

The Council of Constance is the 15th century ecumenical council recognized by the Roman Catholic Church, held from 1414 to 1418. The council ended the Western Schism, by deposing or accepting the resignation of the remaining papal claimants and electing Pope Martin V.
The Council also condemned Jan Hus as a heretic and facilitated his execution by the civil authority. It also ruled on issues of national sovereignty, the rights of pagans, and just war in response to a conflict between the Kingdom of Poland and the Order of the Teutonic Knights. The Council is important for its relationship to ecclesial Conciliarism and Papal supremacy.
Origin and background.
The council's main purpose was to end the Papal schism which had resulted from the confusion following the Avignon Papacy. Pope Gregory XI's return to Rome in 1377, followed by his death and the controversial election of his successor, Pope Urban VI, resulted in the defection of a number of cardinals and the election of a rival pope based at Avignon in 1378. After thirty years of schism, the Council of Pisa had sought to resolve the situation by deposing the two claimant popes and elected a new pope, Alexander V. The council claimed that in such a situation, a council of bishops had greater authority than just one bishop, even if he were the bishop of Rome. Though Alexander and his successor, John XXIII, gained widespread support, especially at the cost of the Avignon pope, the schism remained, now involving not two but three claimants: Gregory XII at Rome, Benedict XIII at Avignon and John XXIII.
Therefore, many voices, including Sigismund, King of Germany and Hungary (and later Holy Roman Emperor), pressed for another council to resolve the issue. That council was called by John XXIII and was held from 16 November 1414 to 22 April 1418 in Constance, Germany. According to Joseph McCabe, the council was attended by roughly 29 cardinals, 100 "learned doctors of law and divinity", 134 abbots, and 183 bishops and archbishops.
Constance.
Sigismund arrived on Christmas Eve 1414 and exercised a profound and continuous influence on the course of the council in his capacity of imperial protector of the Church. An innovation at the Council was that instead of voting as individuals, the bishops voted in national blocks. The vote by nations was in great measure the work of the English, German, and French members. The legality of this measure, in imitation of the "nations" of the universities, was more than questionable, but during February 1415, it carried and thenceforth was accepted in practice, though never authorized by any formal decree of the council. The four "nations" consisted of England, France, Italy, and Germany, with Poles, Hungarians, Danes, and Scandinavians counted with the Germans. While the Italian representatives made up half of those in attendance, they were equal in influence to the English, who sent twenty deputies and three bishops.
Decrees and doctrinal status.
Many members of the new assembly (comparatively few bishops, but many doctors of theology and of canon and civil law, procurators of bishops, deputies of universities, cathedral chapters, provosts, etc., agents and representatives of princes, etc.) strongly favored the voluntary abdication of all three popes, as did King Sigismund.
Although the Italian bishops who had accompanied John XXIII in large numbers supported his legitimacy, he grew increasingly more suspicious of the council. Partly in response to a fierce anonymous attack on his character from an Italian source, on 2 March 1415 he promised to resign. However, on 20 March he secretly fled the city and took refuge at Schaffhausen in territory of his friend Frederick, Duke of Austria-Tyrol.
The famous decree "Haec Sancta Synodus," which gave primacy to the authority of the Council and thus became a source for ecclesial conciliarism, was promulgated in the fifth session, 6 April 1415:
"Haec Sancta Synodus" marks the high-water mark of the Conciliar movement of reform. This decree, however, is not considered valid by the Magisterium of the Catholic Church, since it was never approved by Pope Gregory XII or his successors, and was passed by the Council in a session before his confirmation. The Church declared the first sessions of the Council of Constance an invalid and illicit assembly of bishops, gathered under the authority of John XXIII.
The acts of the Council were not made public until 1442, at the behest of the Council of Basel; they were printed in 1500. The creation of a book on how to die was ordered by the council, and thus written in 1415 called "Ars moriendi".
Ending the Western Schism.
With the support of King Sigismund, enthroned before the high altar of the cathedral of Constance, the Council of Constance recommended that all three papal claimants abdicate, and that another be chosen. In part because of the constant presence of the King, other rulers demanded that they have a say in who would be pope.
Gregory XII then sent representatives to Constance, whom he granted full powers to summon, open and preside over an Ecumenical Council; he also empowered them to present his resignation to the Papacy. This would pave the way for the end of the Western Schism.
The legates were received by King Sigismund and by the assembled Bishops, and the King yielded the presidency of the proceedings to the papal legates, Cardinal Dominici of Ragusa and Prince Charles of Malatesta. On 4 July 1415 the Bull of Gregory XII which appointed Malatesta and Cardinal Dominici of Ragusa as his proxies at the council was formally read before the assembled Bishops. The cardinal then read a decree of Gregory XII which convoked the council and authorized its succeeding acts. Thereupon, the Bishops voted to accept the summons. Prince Malatesta immediately informed the Council that he was empowered by a commission from Pope Gregory XII to resign the Papal Throne on the Pontiff's behalf. He asked the Council whether they would prefer to receive the abdication at that point or at a later date. The Bishops voted to receive the Papal abdication immediately. Thereupon the commission by Gregory XII authorizing his proxy to resign the Papacy on his behalf was read and Malatesta, acting in the name of Gregory XII, pronounced the resignation of the papacy by Gregory XII and handed a written copy of the resignation to the assembly.
Former Pope Gregory XII was then created titular Cardinal Bishop of Porto and Santa Ruffina by the Council, with rank immediately below the Pope (which made him the highest-ranking person in the Church, since, due to his abdication, the See of Peter was vacant). Gregory XII's cardinals were accepted as true cardinals by the Council, but the members of the council delayed electing a new pope for fear that a new pope would restrict further discussion of pressing issues in the Church.
By the time the anti-popes were all deposed and the new Pope, Martin V, was elected, two years had passed since Gregory XII's abdication, and Gregory was already dead. The council took great care to protect the legitimacy of the succession, ratified all his acts and a new pontiff was chosen. The new pope, Martin V, elected November 1417, soon asserted the absolute authority of the papal office.
Condemnation of Jan Hus.
A second goal of the council was to continue the reforms begun at the Council of Pisa. These reforms were largely directed against John Wycliffe, mentioned in the opening session, and condemned in the eighth, 4 May 1415 and Jan Hus, and their followers. Jan Hus, summoned to Constance under a letter of safe conduct, was found guilty of heresy by the council and turned over to the secular court. "This holy synod of Constance, seeing that God's church has nothing more that it can do, relinquishes Jan Hus to the judgment of the secular authority and decrees that he is to be relinquished to the secular court." (Council of Constance-Session 15â6 July 1415). The secular court sentenced him to the stake.
Jerome of Prague, a supporter of Jan Hus, came to Constance, to offer assistance. But he was similarly arrested, judged, found guilty of heresy and turned over to the same secular court, with the same outcome as Hus. Poggio Bracciolini attended the Council and related the unfairness of the process against Jerome.
PolishâLithuanianâTeutonic conflict.
In 1411, the First Peace of Thorn ended the PolishâLithuanianâTeutonic War, in which the Teutonic Knights fought the Kingdom of Poland and Grand Duchy of Lithuania. However, the peace was not stable and further conflicts arose regarding demarcation of the Samogitian borders. The tensions erupted into the brief Hunger War in summer 1414. It was concluded that the disputes would be mediated by the Council of Constance.
The Polish position was defended by Paulus Vladimiri, rector of the Jagiellonian University, who challenged legality of the Teutonic crusade. He argued that a forced conversion was incompatible with free will, which was an essential component of a genuine conversion. Therefore, the Knights could only wage a defensive war if pagans violated natural rights of the Christians. Vladimiri further stipulated that infidels had rights which had to be respected, and neither the Pope nor the Holy Roman Emperor had the authority to violate them. Poles and Lithuanians also brought a group of Samogitian representatives to testify of atrocities committed by the Knights.
The Dominican theologian, John of Falkenberg, proved to be the fiercest opponent of the Poles. In his "Liber de doctrina", Falkenberg argued that ""the Emperor has the right to slay even peaceful infidels simply because they are pagans..."". The Poles deserve death for defending infidels, and should be exterminated even more than the infidels; they should be deprived of their sovereignty and reduced to slavery." In "Satira", he attacked Polish King Jogaila, calling him a "mad dog" unworthy to be king. Falkenberg was condemned and imprisoned for such libel, but was not officially accused of heresy. Other opponents included Grand Master's proctor Peter Wormditt, Dominic of San Gimignano, John Urbach, Ardecino de Porta of Novara, and Bishop of Ciudad Rodrigo Andrew Escobar. They argued that the Knights were perfectly justified in their crusade as it was a sacred duty of Christians to spread the true faith. Cardinal Pierre d'Ailly published an independent opinion that attempted to somewhat balance both Polish and Teutonic positions.
The Council did not make any political decisions. It established the Diocese of Samogitia, with its seat in Medininkai and subordinated to Lithuanian dioceses, and appointed Matthias of Trakai as the first bishop. Pope Martin V appointed Polish King Jogaila and Lithuanian Grand Duke Vytautas as vicars general in Pskov and Veliky Novgorod in recognition of their Catholicism. After another round of futile negotiations, the Gollub War broke out in 1422. It ended with the Treaty of Melno. Polish-Teutonic wars continued for another hundred years.

</doc>
<doc id="7662" url="https://en.wikipedia.org/wiki?curid=7662" title="Churches Uniting in Christ">
Churches Uniting in Christ

Churches Uniting in Christ (CUIC) is an ecumenical organization that brings together ten mainline American denominations (including both predominantly white and predominantly black churches), and was inaugurated on January 20, 2002 in Memphis, Tennessee on the balcony of the Lorraine Motel. It is the successor organization to the Consultation on Church Union.
History.
Origins.
CUIC is the successor organization to the Consultation on Church Union (COCU), which had been founded in 1962. The original task of COCU was to negotiate a consensus between its nine (originally four) member communions (it also included three "advisory participant" churches). However, it never succeeded in this goal, despite making progress on several ecumenical fronts. At COCU's 18th plenary meeting in St. Louis, Missouri (January 1999), CUIC was proposed as a new relationship among the nine member communions. Each member communion voted to join CUIC over the next few years.
Inauguration.
Heads of communion from each member of COCU (as well as the ELCA, a partner in mission and dialogue) inaugurated the group on the day before Martin Luther King, Jr. Day in 2002 at the motel where he was killed. This particular location highlighted the group's focus on racism as a major dividing factor between and among churches.
Task forces.
The Coordinating Council of CUIC created three task forces: Ministry, Racial Justice, and Local and Regional Ecumenism. Each task force represented an important part of early CUIC work. Local ecumenical liturgies were encouraged, and excitement initially built around "pilot programs" in Denver, Los Angeles, and Memphis. The Racial Justice task force created gatherings and discussions on racial justice. The Ministry task force received much of the attention from church structures, however. The group had been given a mandate to complete work on reconciliation by 2007, and in 2003 began working on a document entitled "Mutual Recognition and Mutual Reconciliation of Ministries."
Mutual Recognition and Mutual Reconciliation of Ministries (MRMRM).
One of the most difficult issues concerning recognition and reconciliation of ministries was that of the historic episcopate. This was one of the issues that defeated proposals for union by COCU as well. The group approached this problem through dialogue, soliciting information from each member communion on the particularities of their theology and ecclesiology in order to come to a mutually acceptable conclusion.
CUIC released the seventh and final draft of the MRMRM document in June 2005. Much work was done in 2006 on this document, which focused on "Episkope," the oversight of ministry. The work culminated in a consultation on episkope in St. Louis in October 2006 involving the heads of communion of the members of CUIC. At this consultation, the MRMRM document was met with resistance, and concern was raised in particular that CUIC was focusing too narrowly on reconciliation of ministries and "not taking seriously our commitment to working on those issues of systemic racism that remain at the heart of our continuing and separated life as churches here in the United States."
Moravian Church (Northern Province).
The nine churches which inaugurated CUIC in 2002 were joined by the Moravian Church, Northern Province. The Moravians had been partners in mission and dialogue since 2002, but joined as a member communion after the October 2006 consultation on Episkope.
Suspension of activities.
In 2007, the African Methodist Episcopal Zion Church and the African Methodist Episcopal Church withdrew from CUIC. Neither body sent representatives to the CUIC plenary on January 11â14, 2008, though the AME Council of Bishops never voted to suspend membership officially. They felt the other churches were not doing enough to counter the history of racial injustice between black and white churches. In response to this, the remaining churches in CUIC decided in 2008 to suspend their work while they seek reconciliation with these churches. This work began with a group of representatives who revisited the 1999 document "Call to Christian Commitment and Action to Combat Racism," which is available on the current CUIC website. This also meant eliminating the position of Director as well as the suspension of the work of the CUIC task forces. As of 2012, CUIC no longer has physical offices, opting instead for a virtual office and storing the archives of both CUIC and COCU at Princeton Seminary's Henry Luce III Library.
Reconciliation efforts.
The African Methodist Episcopal Church resumed its participation by the February 2010 plenary meeting, where CUIC moved to refocus on its eight marks of commitment and a shared concern for racial justice as a major dividing factor facing ecumenism. Although the African Methodist Episcopal Zion Church has not rejoined the group, efforts have continued to bring this communion back into membership. The Rev. Staccato Powell, an AMEZ pastor, preached at the 2011 CUIC plenary in Ft. Lauderdale, Florida as a part of these reconciliation efforts. Combating racism has again become a priority of CUIC. Concerns over the historic episcopate have been sidelined since 2008, though they may re-emerge. The group's focus on mutual reconciliation of ministries has been revisited in the light of racism and the impact that racism may have on exchanging ministers between denominations. Therefore, the coordinating council of CUIC created a consultation on race and ministry while also choosing to partner with the Samuel Dewitt Proctor Conference, a social justice organization involved in African American faith communities.
Purpose.
The purpose of CUIC has always been unity (as reflected in their current slogan, "reconciling the baptized, seeking unity with justice"). This reflects one of the core scripture passages in the ecumenical movement, Jesus' prayer in John 17:21, "That they all may be one". CUIC has approached this goal of unity in various ways throughout its history.
Racism.
Racism has been a primary focus of CUIC since 2002 (and, indeed, a primary focus of COCU alongside other forms of exclusion and prejudice, such as sexism and ableism). According to Dan Krutz, former president of CUIC, "Overcoming racism has been a focal point of CUIC since its beginning... Racism may be the biggest sin that divides churches." Even before the absence of the AME and AMEZ churches at the January 2011 plenary, some in CUIC had noticed the lack of commitment to racial reconciliation. Since 2008, however, racism has become an even more pressing concern. This has led CUIC to address issues of racism in the public sphere, including the killing of Trayvon Martin and the recovery from the 2010 Haiti earthquake.
Marks of Commitment.
According to their website, one of the reasons for transitioning from COCU to CUIC is so that member churches "stop 'consulting' and start living their unity in Christ more fully." This means that each member communion in CUIC agrees to abide by the eight Marks of Commitment, which are summarized as follows:

</doc>
<doc id="7663" url="https://en.wikipedia.org/wiki?curid=7663" title="Canadian Unitarian Council">
Canadian Unitarian Council

Canadian Unitarian Council () (CUC) formed on May 14, 1961 to be the national organization for Canadians who belong to the Unitarian Universalist Association (UUA) (the UUA formed a day later, on May 15, 1961). Until 2002, almost all members congregations of the CUC were also members of the UUA, and most services to CUC member congregations were provided by the UUA. However, after an agreement between the CUC and the UUA, most services since 2002 have been provided by the CUC to its own member congregations, with the UUA continuing to provide ministerial settlement services. Some Canadian congregations have continued to be members of both the CUC and UUA, while others are members of only the CUC.
The CUC is currently the only national body for Unitarian Universalist congregations in Canada, and is a member of the International Council of Unitarians and Universalists.
Organization.
The CUC is made up of 46 member congregations and emerging groups, who are the legal owners of the organization, and who are, for governance and service delivery, divided into four regions: "BC" (British Columbia), "Western" (Alberta to Thunder Bay), "Central" (between Thunder Bay and Kingston), and "Eastern" (Kingston, Ottawa and everything east of that). However, for youth ministry, the "Central" and "Eastern" regions are combined to form a youth region known as "QuOM" (Quebec, Ontario and the Maritimes), giving the youth only three regions for their activities. The organization as a whole is governed by the CUC Board of Trusties (Board), whose mandate it is to govern in the best interests of the CUC's owners. The Board is made up of 8 members who are elected by congregational delegates at the CUC's Annual General Meeting. This consists of two Trustees from each region, who are eligible to serve a maximum of two three-year terms. Board meetings also include Official Observers to the Board, who participate without a vote and represent UU Youth and Ministers.
Service delivery.
As members of the CUC, congregations and emerging groups are served by volunteer Service Consultants, Congregational Networks, and a series of other committees. There are two directors of regional services, one for the Western two regions, and one for the Eastern two regions. The Director of Lifespan Learning oversees development of religious exploration programming and youth and young adults are served by a Youth and Young Adult Ministry Development staff person.
Annual Conference and Meeting.
Policies and business of the CUC are determined at the Annual Conference and Meeting (ACM), consisting of the Annual Conference, in which workshops are held, and the Annual General Meeting, in which business matters and plenary meetings are performed. The ACM features two addresses, a Keynote and a Confluence Lecture. The Confluence Lecture is comparable to the UUA's Ware Lecture in prestige. In early days this event simply consisted of the Annual General Meeting component as the Annual Conference component was not added to much later. Past ACMs have been held in the following locations:
^Not an ACM, but an "Annual General Meeting" and "Symposium", and unlike ACMs it was organized by the CUC and the Unitarian Universalist Ministers of Canada instead of a local congregation.<br>#Not a keynote presenter or lecturer, rather a symposium "Provocateur".<br>*Upcoming locations
Principles and sources.
The CUC does not have a central creed in which members are required to believe, but they have found it useful to articulate their common values in what has become known as The Principles and Sources of our Religious Faith, which are currently based on the UUA's Principles and Purposes. The CUC had a task force whose mandate was to consider revising them.
The principles and sources as published in church literature and on the CUC website:
Formation and relationship to the Unitarian Universalist Association.
The CUC formed on May 14, 1961 to be the national organization for Canadians within the about to form, UUA (they formed a day later on May 15, 1961). And until 2002, almost all member congregations of the CUC were also members of the UUA and most services to CUC member congregations were provided by the UUA. However, after an agreement between the UUA and the CUC, since 2002 most services have been provided by the CUC to its own member congregations, with the UUA continuing to provide ministerial settlement services. And also since 2002, some Canadian congregations have continued to be members of both the UUA and CUC while others are members of only the CUC.
The Canadian Unitarian Universalist youth of the day disapproved of the 2002 change in relationship between the CUC and UUA. It is quite evident in the words of this statement, which was adopted by the attendees of the 2001 youth conference held at the Unitarian Church of Montreal: "We the youth of Canada are deeply concerned about the direction the CUC seems to be taking. As stewards of our faith, adults have a responsibility to take into consideration the concerns of youth. We are opposed to making this massive jump in our evolutionary progress."
Name of CUC and playful abbreviation of Unitarian Universalist.
While the name of the organization is the Canadian Unitarian Council, the CUC includes congregations with Unitarian, Universalist, Unitarian Universalist, and Universalist Unitarian in their names. Changing the name of the CUC has occasionally been debated, but there have been no successful motions. To recognize this diversity, some members of the CUC abbreviate Unitarian Universalist as U*U (and playfully read it as "You star, you"). Note, not all CUC members like this playful reading and so when these people write the abbreviation they leave out the star(*), just writing UU instead.

</doc>
<doc id="7668" url="https://en.wikipedia.org/wiki?curid=7668" title="Charles Mingus">
Charles Mingus

Charles Mingus Jr. (April 22, 1922 â January 5, 1979) was an American jazz double bassist, composer and bandleader. His compositions retained the hot and soulful feel of hard bop, drawing heavily from black gospel music and blues, while sometimes containing elements of Third Stream, free jazz, and classical music. He once cited Duke Ellington and church as his main influences.
Mingus espoused collective improvisation, similar to the old New Orleans jazz parades, paying particular attention to how each band member interacted with the group as a whole. In creating his bands, he looked not only at the skills of the available musicians, but also their personalities. Many musicians passed through his bands and later went on to impressive careers. He recruited talented and sometimes little-known artists, whom he utilized to assemble unconventional instrumental configurations. As a performer, Mingus was a pioneer in double bass technique, widely recognized as one of the instrument's most proficient players.
Nearly as well known as his ambitious music was Mingus's often fearsome temperament, which earned him the nickname "The Angry Man of Jazz". His refusal to compromise his musical integrity led to many onstage eruptions, exhortations to musicians, and dismissals. Because of his brilliant writing for midsize ensembles, and his catering to and emphasizing the strengths of the musicians in his groups, Mingus is often considered the heir of Duke Ellington, for whom he expressed great admiration. Indeed, Dizzy Gillespie had once claimed Mingus reminded him "of a young Duke", citing their shared "organizational genius".
Mingus' compositions continue to be played by contemporary musicians ranging from the repertory bands Mingus Big Band, Mingus Dynasty, and Mingus Orchestra, to the high school students who play the charts and compete in the Charles Mingus High School Competition.
Gunther Schuller has suggested that Mingus should be ranked among the most important American composers, jazz or otherwise. In 1988, a grant from the National Endowment for the Arts made possible the cataloging of Mingus compositions, which were then donated to the Music Division of the New York Public Library for public use. In 1993, The Library of Congress acquired Mingus's collected papersâincluding scores, sound recordings, correspondence and photosâin what they described as "the most important acquisition of a manuscript collection relating to jazz in the Library's history".
Biography.
Early life and career.
Charles Mingus was born in Nogales, Arizona. He was largely raised in the Watts area of Los Angeles. His maternal grandfather was a Chinese British subject from Hong Kong, his maternal grandmother was black. Charles Mingus Sr. was the illegitimate offspring of an African American farmhand and his employer's white/European granddaughter. Mingus was the third great-grandson of the family's founding partiarch who was, by most accounts, a German immigrant. His ancestors included German American, African American, British, Chinese, and Native American. In Mingus's autobiography "Beneath the Underdog" his mother was described as "the daughter of an Englishman and a Chinese woman", and his father was the son "of a black farm worker and a Swedish woman". Charles Mingus Sr. claims to have been raised by his mother and her husband as a white person until he was fourteen, when his mother revealed to her family that the child's true father was a black slave, after which he had to run away from his family and live on his own. The autobiography doesn't confirm whether Charles Mingus Sr. or Mingus himself believed this story was true, or whether it was merely an embellished version of the Mingus family's lineage.
His mother allowed only church-related music in their home, but Mingus developed an early love for other music, especially Duke Ellington. He studied trombone, and later cello, although he was unable to follow the cello professionally because, at the time, it was nearly impossible for a black musician to make a career of classical music, and the cello was not yet accepted as a jazz instrument. Despite this, Mingus was still attached to the cello; as he studied bass with Red Callender in the late 1930s, Callender even commented that the cello was still Mingus's main instrument. In "Beneath the Underdog", Mingus states that he did not actually start learning bass until Buddy Collette accepted him into his swing band under the stipulation that he be the band's bass player.
Due to a poor education, the young Mingus could not read musical notation quickly enough to join the local youth orchestra. This had a serious impact on his early musical experiences, leaving him feeling ostracized from the classical music world. These early experiences, in addition to his lifelong confrontations with racism, were reflected in his music, which often focused on themes of racism, discrimination and (in)justice. Much of the cello technique he learned was applicable to double bass when he took up the instrument in high school. He studied for five years with Herman Reinshagen, principal bassist of the New York Philharmonic, and compositional techniques with Lloyd Reese. Throughout much of his career, he played a bass made in 1927 by the German maker Ernst Heinrich Roth.
Beginning in his teen years, Mingus was writing quite advanced pieces; many are similar to Third Stream because they incorporate elements of classical music. A number of them were recorded in 1960 with conductor Gunther Schuller, and released as "Pre-Bird", referring to Charlie "Bird" Parker; Mingus was one of many musicians whose perspectives on music were altered by Parker into "pre- and post-Bird" eras.
Mingus gained a reputation as a bass prodigy. His first major professional job was playing with former Ellington clarinetist Barney Bigard. He toured with Louis Armstrong in 1943, and by early 1945 was recording in Los Angeles in a band led by Russell Jacquet, which also included Teddy Edwards, Maurice Simon, Bill Davis, and Chico Hamilton, and in May that year, in Hollywood, again with Teddy Edwards, in a band led by Howard McGhee. He then played with Lionel Hampton's band in the late 1940s; Hampton performed and recorded several of Mingus's pieces. A popular trio of Mingus, Red Norvo and Tal Farlow in 1950 and 1951 received considerable acclaim, but Mingus's race caused problems with club owners and he left the group. Mingus was briefly a member of Ellington's band in 1953, as a substitute for bassist Wendell Marshall. Mingus's notorious temper led to him being one of the few musicians personally fired by Ellington (Bubber Miley and drummer Bobby Durham are among the others), after an on-stage fight between Mingus and Juan Tizol.
Also in the early 1950s, before attaining commercial recognition as a bandleader, Mingus played gigs with Charlie Parker, whose compositions and improvisations greatly inspired and influenced him. Mingus considered Parker the greatest genius and innovator in jazz history, but he had a love-hate relationship with Parker's legacy. Mingus blamed the Parker mythology for a derivative crop of pretenders to Parker's throne. He was also conflicted and sometimes disgusted by Parker's self-destructive habits and the romanticized lure of drug addiction they offered to other jazz musicians. In response to the many sax players who imitated Parker, Mingus titled a song, "If Charlie Parker were a Gunslinger, There'd be a Whole Lot of Dead Copycats" (released on "Mingus Dynasty" as "Gunslinging Bird").
Based in New York.
In 1952 Mingus co-founded Debut Records with Max Roach so he could conduct his recording career as he saw fit. The name originated from his desire to document unrecorded young musicians. Despite this, the best-known recording the company issued was of the most prominent figures in bebop. On May 15, 1953, Mingus joined Dizzy Gillespie, Parker, Bud Powell, and Roach for a concert at Massey Hall in Toronto, which is the last recorded documentation of Gillespie and Parker playing together. After the event, Mingus chose to overdub his barely audible bass part back in New York; the original version was issued later. The two 10" albums of the Massey Hall concert (one featured the trio of Powell, Mingus and Roach) were among Debut Records' earliest releases. Mingus may have objected to the way the major record companies treated musicians, but Gillespie once commented that he did not receive any royalties "for years and years" for his Massey Hall appearance. The records though, are often regarded as among the finest live jazz recordings.
One story has it that Mingus was involved in a notorious incident while playing a 1955 club date billed as a "reunion" with Parker, Powell, and Roach. Powell, who suffered from alcoholism and mental illness (possibly exacerbated by a severe police beating and electroshock treatments), had to be helped from the stage, unable to play or speak coherently. As Powell's incapacitation became apparent, Parker stood in one spot at a microphone, chanting "Bud Powell...Bud Powell..." as if beseeching Powell's return. Allegedly, Parker continued this incantation for several minutes after Powell's departure, to his own amusement and Mingus's exasperation. Mingus took another microphone and announced to the crowd, "Ladies and Gentleman, please don't associate me with any of this. This is not jazz. These are sick people." This was Parker's last public performance; about a week later he died after years of substance abuse.
Mingus often worked with a mid-sized ensemble (around 8â10 members) of rotating musicians known as the Jazz Workshop. Mingus broke new ground, constantly demanding that his musicians be able to explore and develop their perceptions on the spot. Those who joined the Workshop (or Sweatshops as they were colorfully dubbed by the musicians) included Pepper Adams, Jaki Byard, Booker Ervin, John Handy, Jimmy Knepper, Charles McPherson and Horace Parlan. Mingus shaped these musicians into a cohesive improvisational machine that in many ways anticipated free jazz. Some musicians dubbed the workshop a "university" for jazz.
"Pithecanthropus Erectus" among other creations.
The decade that followed is generally regarded as Mingus's most productive and fertile period. Impressive new compositions and albums appeared at an astonishing rate: some "thirty" records in ten years, for a number of record labels (Atlantic Records, Candid, Columbia Records, Impulse! Records and others), a pace perhaps unmatched by any other musicians except Ellington. 
Mingus had already recorded around ten albums as a bandleader, but 1956 was a breakthrough year for him, with the release of "Pithecanthropus Erectus", arguably his first major work as both a bandleader and composer. Like Ellington, Mingus wrote songs with specific musicians in mind, and his band for "Erectus" included adventurous musicians: piano player Mal Waldron, alto saxophonist Jackie McLean and the Sonny Rollins-influenced tenor of J. R. Monterose. The title song is a ten-minute tone poem, depicting the rise of man from his hominid roots ("Pithecanthropus erectus") to an eventual downfall. A section of the piece was free improvisation, free of structure or theme.
Another album from this period, "The Clown" (1957 also on Atlantic Records), the title track of which features narration by humorist Jean Shepherd, was the first to feature drummer Dannie Richmond, who remained his preferred drummer until Mingus's death in 1979. The two men formed one of the most impressive and versatile rhythm sections in jazz. Both were accomplished performers seeking to stretch the boundaries of their music while staying true to its roots. When joined by pianist Jaki Byard, they were dubbed "The Almighty Three".
"Mingus Ah Um" and other works.
In 1959 Mingus and his jazz workshop musicians recorded one of his best-known albums, "Mingus Ah Um". Even in a year of standout masterpieces, including Dave Brubeck's "Time Out", Miles Davis's "Kind of Blue", and Ornette Coleman's prophetic "The Shape of Jazz to Come", this was a major achievement, featuring such classic Mingus compositions as "Goodbye Pork Pie Hat" (an elegy to Lester Young) and the vocal-less version of "Fables of Faubus" (a protest against segregationist Arkansas governor Orval E. Faubus that features double-time sections). Also during 1959, Mingus recorded the album "Blues & Roots", which was released the following year. As Mingus explained in his liner notes: "I was born swinging and clapped my hands in church as a little boy, but I've grown up and I like to do things other than just swing. But blues can do more than just swing."
Mingus witnessed Ornette Coleman's legendaryâand controversialâ1960 appearances at New York City's Five Spot jazz club. He initially expressed rather mixed feelings for Coleman's innovative music: "...if the free-form guys could play the same tune twice, then I would say they were playing something...Most of the time they use their fingers on the saxophone and they don't even know what's going to come out. They're experimenting." That same year, however, Mingus formed a quartet with Richmond, trumpeter Ted Curson and multi-instrumentalist Eric Dolphy. This ensemble featured the same instruments as Coleman's quartet, and is often regarded as Mingus rising to the challenging new standard established by Coleman. "Charles Mingus Presents Charles Mingus" was the quartet's only album. This album also features the version of "Fables of Faubus" with lyrics, aptly titled "Original Faubus Fables".
Only one misstep occurred in this era: 1962's "Town Hall Concert". An ambitious program, it was plagued with troubles from its inception. Mingus's vision, now known as "Epitaph", was finally realized by conductor Gunther Schuller in a concert in 1989, 10 years after Mingus's death.
"The Black Saint and the Sinner Lady" and other Impulse! albums.
In 1963, Mingus released "The Black Saint and the Sinner Lady", a sprawling, multi-section masterpiece, described as "one of the greatest achievements in orchestration by any composer in jazz history." The album was also unique in that Mingus asked his psychotherapist to provide notes for the record.
Mingus also released "Mingus Plays Piano", an unaccompanied album featuring some fully improvised pieces, in 1963.
In addition, 1963 saw the release of "Mingus Mingus Mingus Mingus Mingus", an album praised by critic Nat Hentoff.
In 1964 Mingus put together one of his best-known groups, a sextet including Dannie Richmond, Jaki Byard, Eric Dolphy, trumpeter Johnny Coles, and tenor saxophonist Clifford Jordan. The group was recorded frequently during its short existence; Coles fell ill and left during a European tour. Dolphy stayed in Europe after the tour ended, and died suddenly in Berlin on June 28, 1964. 1964 was also the year that Mingus met his future wife, Sue Graham Ungaro. The couple were married in 1966 by Allen Ginsberg. Facing financial hardship, Mingus was evicted from his New York home in 1966.
"Changes".
Mingus's pace slowed somewhat in the late 1960s and early 1970s. In 1974 he formed a quintet with Richmond, pianist Don Pullen, trumpeter Jack Walrath and saxophonist George Adams. They recorded two well-received albums, "Changes One" and "Changes Two". Mingus also played with Charles McPherson in many of his groups during this time. "Cumbia and Jazz Fusion" in 1976 sought to blend Colombian music (the "Cumbia" of the title) with more traditional jazz forms. In 1971, Mingus taught for a semester at the University at Buffalo, The State University of New York as the Slee Professor of Music.
Later career and death.
By the mid-1970s, Mingus was suffering from amyotrophic lateral sclerosis (ALS). His once formidable bass technique suffered, until he could no longer play the instrument. He continued composing, however, and supervised a number of recordings before his death. At the time of his death, he was working with Joni Mitchell on an album eventually titled "Mingus", which included lyrics added by Mitchell to his compositions, including "Goodbye Pork Pie Hat". The album featured the talents of Wayne Shorter, Herbie Hancock, and another influential bassist and composer, Jaco Pastorius.
Mingus died, aged 56, in Cuernavaca, Mexico, where he had traveled for treatment and convalescence. His ashes were scattered in the Ganges River.
Personality and temper.
As respected as Mingus was for his musical talents, he was sometimes feared for his occasionally violent onstage temper, which was at times directed at members of his band, and other times aimed at the audience. He was physically large, prone to obesity (especially in his later years), and was by all accounts often intimidating and frightening when expressing anger or displeasure. Mingus was prone to clinical depression. He tended to have brief periods of extreme creative activity, intermixed with fairly long periods of greatly decreased output.
When confronted with a nightclub audience talking and clinking ice in their glasses while he performed, Mingus stopped his band and loudly chastised the audience, stating "Isaac Stern doesn't have to put up with this shit." Mingus reportedly destroyed a $20,000 bass in response to audience heckling at New York's Five Spot.
Guitarist and singer Jackie Paris was a first-hand witness to Mingus's irascibility. Paris recalls his time in the Jazz Workshop: "He chased everybody off the stand except Paul Motian and me... The three of us just wailed on the blues for about an hour and a half before he called the other cats back."
On October 12, 1962, Mingus punched Jimmy Knepper in the mouth while the two men were working together at Mingus's apartment on a score for his upcoming concert at New York Town Hall and Knepper refused to take on more work. The blow from Mingus broke off a crowned tooth and its underlying stub. According to Knepper, this ruined his embouchure and resulted in the permanent loss of the top octave of his range on the trombone â a significant handicap for any professional trombonist. This attack temporarily ended their working relationship and Knepper was unable to perform at the concert. Charged with assault, Mingus appeared in court in January 1963 and was given a suspended sentence. Knepper did again work with Mingus in 1977 and played extensively with the Mingus Dynasty, formed after Mingus's death in 1979.
In 1966, Mingus was evicted from his apartment at 5 Great Jones Street in New York City for nonpayment of rent, captured in the 1968 documentary film "", directed by Thomas Reichman. The film also features Mingus performing in clubs and in the apartment, firing a shotgun, composing at the piano, playing with and taking care of his young daughter Caroline, and discussing love, art, politics, and the music school he had hoped to create.
Legacy.
The Mingus Big Band.
The music of Charles Mingus is currently being performed and reinterpreted by the Mingus Big Band, which, starting October 2008, plays every Monday at Jazz Standard in New York City, and often tours the rest of the U.S. and Europe. Elvis Costello has written lyrics for a few Mingus pieces. He had once sung lyrics for one piece, "Invisible Lady", being backed by the Mingus Big Band on the album, "Tonight at Noon: Three of Four Shades of Love".
In addition to the Mingus Big Band, there is the Mingus Orchestra and the Mingus Dynasty, each of which are managed by Jazz Workshop, Inc., and run by Mingus's widow Sue Graham Mingus.
"Epitaph".
"Epitaph" is considered one of Charles Mingus's masterpieces. The composition is 4,235 measures long, requires two hours to perform, and is one of the longest jazz pieces ever written. "Epitaph" was only completely discovered, by musicologist Andrew Homzy, during the cataloging process after his death. With the help of a grant from the Ford Foundation, the score and instrumental parts were copied, and the piece itself was premiered by a 30-piece orchestra, conducted by Gunther Schuller. This concert was produced by Mingus's widow, Sue Graham Mingus, at Alice Tully Hall on June 3, 1989, ten years after his death. It was performed again at several concerts in 2007. The performance at Walt Disney Concert Hall is available on NPR. The complete score was published in 2008 by Hal Leonard.
Autobiography.
Mingus wrote the sprawling, exaggerated, quasi-autobiography, "Beneath the Underdog: His World as Composed by Mingus", throughout the 1960s, and it was published in 1971. Its "stream of consciousness" style covered several aspects of his life that had previously been off-record. In addition to his musical and intellectual proliferation, Mingus goes into great detail about his perhaps overstated sexual exploits. He claims to have had more than 31 affairs in the course of his life (including 26 prostitutes in one sitting). This does not include any of his five wives (he claims to have been married to two of them simultaneously). In addition, he asserts that he held a brief career as a pimp. This has never been confirmed.
Mingus's autobiography also serves as an insight into his psyche, as well as his attitudes about race and society. It includes accounts of abuse at the hands of his father from an early age, being bullied as a child, his removal from a white musician's union, and grappling with disapproval while married to white women and other examples of the hardship and prejudice.
Cover versions.
Considering the number of compositions that Charles Mingus wrote, his works have not been recorded as often as comparable jazz composers. The only Mingus tribute albums recorded during his lifetime were baritone saxophonist Pepper Adams's album, "Pepper Adams Plays Charlie Mingus", in 1963, and Joni Mitchell's album "Mingus", in 1979. Of all his works, his elegant elegy for Lester Young, "Goodbye Pork Pie Hat" (from "Mingus Ah Um") has probably had the most recordings. Besides recordings from the expected jazz artists, the song has also been recorded by musicians as disparate as Jeff Beck, Andy Summers, Eugene Chadbourne, and Bert Jansch and John Renbourn with and without Pentangle. Joni Mitchell sang a version with lyrics that she wrote for the song.
Elvis Costello has recorded "Hora Decubitus" (from "Mingus Mingus Mingus Mingus Mingus") on "My Flame Burns Blue" (2006). "Better Git It in Your Soul" was covered by Davey Graham on his album "Folk, Blues, and Beyond." Trumpeter Ron Miles performs a version of "Pithecanthropus Erectus" on his EP "Witness." New York Ska Jazz Ensemble has done a cover of Mingus's "Haitian Fight Song", as have the British folk rock group Pentangle and others. Hal Willner's 1992 tribute album "Weird Nightmare: Meditations on Mingus" (Columbia Records) contains idiosyncratic renditions of Mingus's works involving numerous popular musicians including Chuck D, Keith Richards, Henry Rollins and Dr. John. The Italian band Quintorigo recorded an entire album devoted to Mingus's music, titled "Play Mingus".
Gunther Schuller's edition of Mingus's "Epitaph" which premiered at Lincoln Center in 1989 was subsequently released on Columbia/Sony Records.
One of the ultimate tributes to Mingus came on September 29, 1969, at a festival honoring him. Duke Ellington performed "The Clown" at the festival. Duke himself did Jean Shepherd's narration. As of this date, this recording has not been issued.

</doc>
<doc id="7669" url="https://en.wikipedia.org/wiki?curid=7669" title="Centimetre">
Centimetre

A centimetre (international spelling as used by the International Bureau of Weights and Measures; symbol cm) or centimeter (American spelling) is a unit of length in the metric system, equal to one hundredth of a metre, "centi" being the SI prefix for a factor of . The centimetre was the base unit of length in the now deprecated centimetreâgramâsecond (CGS) system of units.
Though for many physical quantities, SI prefixes for factors of 103âlike "milli-" and "kilo-"âare often preferred by technicians, the centimetre remains a practical unit of length for many everyday measurements. A centimetre is approximately the width of the fingernail of an average adult person.
Equivalence to other units of length.
One 1 millilitre is defined as one cubic centimetre, under the SI system of units.
Other uses.
In addition to its use in the measurement of length, the centimetre is used:
Unicode symbols.
For the purposes of compatibility with Chinese, Japanese and Korean (CJK) characters, Unicode has symbols for:
They are mostly used only with East Asian fixed-width CJK fonts, because they are equal in size to one Chinese character.

</doc>
<doc id="7670" url="https://en.wikipedia.org/wiki?curid=7670" title="Central Coast">
Central Coast

Central Coast may refer to:

</doc>
<doc id="7671" url="https://en.wikipedia.org/wiki?curid=7671" title="Committee on Data for Science and Technology">
Committee on Data for Science and Technology

The Committee on Data for Science and Technology (CODATA) was established in 1966 as an interdisciplinary committee of the International Council for Science. It seeks to improve the compilation, critical evaluation, storage, and retrieval of data of importance to science and technology.
CODATA sponsors the CODATA international conference every two years.
Task Group on Fundamental Constants.
CODATA is best known for (and sometimes confused with) its Task Group on Fundamental Constants. Established in 1969, its purpose is to periodically provide the international scientific and technological communities with an internationally accepted set of values of the fundamental physical constants and closely related conversion factors for use worldwide.
The first such CODATA set was published in 1973. Later versions are named based on the year of the data incorporated; the 1986 CODATA (published April 1987) used data up to 1 January 1986. All subsequent releases use data up to the "end" of the stated year, and are necessarily published a year or two later: 1998 (April 2000), 2002 (January 2005), 2006 (June 2008) and the sixth in 2010 (November 2012). The latest version is Version 7.0 called "2014 CODATA" published on 25 June 2015.
The CODATA recommended values of fundamental physical constants are published at the NIST Reference on Constants, Units, and Uncertainty.
Schedule.
Since 1998, the task group has produced a new version every four years, incorporating results published up to the end of the specified year.
In order to support the upcoming redefinition of the SI base units, expected to be adopted at the 26th General Conference on Weights and Measures in the fall of 2018, CODATA will make two special releases.
The first, incorporating all data up to 2017-07-01, will determine the final numerical values of "h", "e", "k", and "N"A that will be used for the new SI definitions.
A second, with a closing date of 2018-07-01 (6 months ahead of the usual schedule) will be used to produce a new 2018 CODATA simultaneously with the formal adoption of the new SI. This is necessary because the redefinitions have a significant (mostly beneficial) effect on the uncertainties and correlation coefficients reported by CODATA.

</doc>
<doc id="7672" url="https://en.wikipedia.org/wiki?curid=7672" title="Chuck Jones">
Chuck Jones

Charles Martin "Chuck" Jones (September 21, 1912Â â February 22, 2002) was an American animator, cartoon artist, screenwriter, producer, and director of animated films, most memorably of "Looney Tunes" and "Merrie Melodies" shorts for the Warner Bros. Cartoons studio. He directed many classic animated cartoon shorts starring Bugs Bunny, Daffy Duck, the Road Runner and Wile E. Coyote, PepÃ© Le Pew, Porky Pig and a slew of other Warner characters.
After his career at Warner Bros. ended in 1962, Jones started Sib Tower 12 Productions, and began producing cartoons for Metro-Goldwyn-Mayer, including a new series of "Tom and Jerry" shorts and the television adaptation of Dr. Seuss' "How the Grinch Stole Christmas!". He later started his own studio, Chuck Jones Enterprises, which created several one-shot specials, and periodically worked on "Looney Tunes" related works.
Jones was nominated for an Academy Award eight times and won three times, receiving awards for the cartoons "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line". He received an Honorary Academy Award in 1996 for his work in the animation industry. Film historian Leonard Maltin has praised Jones' work at Warner Bros., MGM and Chuck Jones Enterprises. He also said that the "feud" that there may have been between Jones and colleague Bob Clampett was mainly because they were so different from each other. In Jerry Beck's "The 50 Greatest Cartoons", ten of the entries were directed by Jones, with four out of the five top cartoons being Jones shorts.
Early life.
Jones was born on September 21, 1912 in Spokane, Washington, the son of Mabel McQuiddy (Martin) and Charles Adams Jones. He later moved with his parents and three siblings to the Los Angeles, California area.
In his autobiography, "Chuck Amuck", Jones credits his artistic bent to circumstances surrounding his father, who was an unsuccessful businessman in California in the 1920s. His father, Jones recounts, would start every new business venture by purchasing new stationery and new pencils with the company name on them. When the business failed, his father would quietly turn the huge stacks of useless stationery and pencils over to his children, requiring them to use up all the material as fast as possible. Armed with an endless supply of high-quality paper and pencils, the children drew constantly. Later, in one art school class, the professor gravely informed the students that they each had 100,000 bad drawings in them that they must first get past before they could possibly draw anything worthwhile. Jones recounted years later that this pronouncement came as a great relief to him, as he was well past the 200,000 mark, having used up all that stationery. Jones and several of his siblings went on to artistic careers.
During his artistic education, he worked part-time as a janitor. After graduating from Chouinard Art Institute, Jones got a phone call from a friend named Fred Kopietz, who had been hired by the Ub Iwerks studio and offered him a job. He worked his way up in the animation industry, starting as a cell washer; "then I moved up to become a painter in black and white, some color. Then I went on to take animator's drawings and traced them on to the celluloid. Then I became what they call an in-betweener, which is the guy that does the drawing between the drawings the animator makes". While at Iwerks, he met a cel painter named Dorothy Webster, who later became his first wife.
Warner Bros..
Jones joined Leon Schlesinger Productions, the independent studio that produced "Looney Tunes" and "Merrie Melodies" for Warner Bros., in 1933 as an assistant animator. In 1935, he was promoted to animator, and assigned to work with new Schlesinger director Tex Avery. There was no room for the new Avery unit in Schlesinger's small studio, so Avery, Jones, and fellow animators Bob Clampett, Virgil Ross, and Sid Sutherland were moved into a small adjacent building they dubbed "Termite Terrace". When Clampett was promoted to director in 1937, Jones was assigned to his unit; the Clampett unit was briefly assigned to work with Jones' old employer, Ub Iwerks, when Iwerks subcontracted four cartoons to Schlesinger in 1937. Jones became a director (or "supervisor", the original title for an animation director in the studio) himself in 1938 when Frank Tashlin left the studio.
He was actively involved in efforts to unionize the staff of Leon Schlesinger Studios. He was responsible for recruiting animators, layout men, and background people. Almost all animators joined, in reaction to salary cuts imposed by Leon Schlesinger. The Metro-Goldwyn-Mayer cartoon studio had already signed a union contract, encouraging their counterparts under Schlesinger. In a meeting with his staff, Schlesinger talked for a few minutes, then turned over the meeting to his attorney. His insulting manner had a unifying effect on the staff. Jones gave a pep talk at the union headquarters. As negotiations broke down, the staff decided to go on strike. Schlesinger locked them out of the studio for a few days, before agreeing to sign the contract. A Labor Management Committee was formed and Jones served as a moderator. Because of his role as a supervisor in the studio, he could not himself join the union. Jones created many of his lesser-known characters during this period, including Charlie Dog, Hubie and Bertie, and The Three Bears.
During World War II, Jones worked closely with Theodor Geisel, better known as Dr. Seuss, to create the "Private Snafu" series of Army educational cartoons. Jones later collaborated with Seuss on animated adaptations of Seuss' books, including "How the Grinch Stole Christmas!" in 1966. Jones directed such shorts as "The Weakly Reporter", a 1944 short that related to shortages and rationing on the home front. During the same year, he directed "Hell-Bent for Election", a campaign film for Franklin D. Roosevelt.
Jones created characters through the late 1940s and the 1950s, which include Claude Cat, Marc Antony and Pussyfoot, Charlie Dog, Michigan J. Frog, and his three most popular creations, Marvin the Martian, Pepe LePew, Wile E. Coyote and The Road Runner. Jones and writer Michael Maltese collaborated on the Road Runner cartoons, "Duck Amuck", "One Froggy Evening", and "What's Opera, Doc?". Other staff at Unit A that Jones collaborated with include layout artist, background designer, co-director Maurice Noble; animator and co-director Abe Levitow; and animators Ken Harris and Ben Washam.
Jones remained at Warner Bros. throughout the 1950s, except for a brief period in 1953 when Warner closed the animation studio. During this interim, Jones found employment at Walt Disney Productions, where he teamed with Ward Kimball for a four-month period of uncredited work on "Sleeping Beauty" (1959). Upon the reopening of the Warner animation department, Jones was rehired and reunited with most of his unit.
In the early 1960s, Jones and his wife Dorothy wrote the screenplay for the animated feature "Gay Purr-ee". The finished film would feature the voices of Judy Garland, Robert Goulet and Red Buttons as cats in Paris, France. The feature was produced by UPA, and directed by his former Warner Bros. collaborator, Abe Levitow.
Jones moonlighted to work on the film, since he had an exclusive contract with Warner Bros. UPA completed the film and made it available for distribution in 1962; it was picked up by Warner Bros. When Warner Bros. discovered that Jones had violated his exclusive contract with them, they terminated him. Jones' former animation unit was laid off after completing the final cartoon in their pipeline, "The Iceman Ducketh", and the rest of the Warner Bros. Cartoons studio was closed in early 1963.
Post-Warner Bros..
With business partner Les Goldman, Jones started an independent animation studio, Sib Tower 12 Productions, and brought on most of his unit from Warner Bros., including Maurice Noble and Michael Maltese. In 1963, Metro-Goldwyn-Mayer contracted with Sib Tower 12 to have Jones and his staff produce new "Tom and Jerry" cartoons as well as a television adaptation of all "Tom and Jerry" theatricals produced to that date. This included major editing, including writing out the African-American maid, Mammy Two-Shoes, and replacing her with one of Irish descent voiced by June Foray. In 1964, Sib Tower 12 was absorbed by MGM and was renamed MGM Animation/Visual Arts. His animated short film, "The Dot and the Line: A Romance in Lower Mathematics", won the 1965 Academy Award for Best Animated Short. Jones directed the classic animated short "The Bear That Wasn't".
As the "Tom and Jerry" series wound down (it was discontinued in 1967), Jones produced more for television. In 1966, he produced and directed the TV special "How the Grinch Stole Christmas!", featuring the voice and facial models based on the readings by Boris Karloff.
Jones continued to work on other TV specials such as "Horton Hears a Who!" (1970), but his main focus during this time was producing the feature film "The Phantom Tollbooth", which did lukewarm business when MGM released it in 1970. Jones co-directed 1969's "The Pogo Special Birthday Special", based on the Walt Kelly comic strip, and voiced the characters of Porky Pine and Bun Rab. It was at this point that he decided to start ST Incorporated.
MGM closed the animation division in 1970, and Jones once again started his own studio, Chuck Jones Enterprises. He produced a Saturday morning children's TV series for the American Broadcasting Company called "The Curiosity Shop" in 1971. In 1973, he produced an animated version of the George Selden book "The Cricket in Times Square", and would go on to produce two sequels.
Three of his works during this period were animated TV adaptations of short stories from Rudyard Kipling's "The Jungle Book": "Mowgli's Brothers", "The White Seal" and "Rikki-Tikki-Tavi". During this period, Jones began to experiment with more realistically designed characters, most of which having larger eyes, leaner bodies, and altered proportions, such as those of the Looney Tunes characters.
Jones resumed working with Warner Bros. in 1976 with the animated TV adaptation of "The Carnival of the Animals" with Bugs Bunny and Daffy Duck. Jones also produced the 1979 film "The Bugs Bunny/Road Runner Movie" which was a compilation of Jones' best theatrical shorts; Jones produced new Road Runner shorts for "The Electric Company" series and "Bugs Bunny's Looney Christmas Tales" (1979), and even newer shorts were made for "Bugs Bunny's Bustin' Out All Over" (1980).
From 1977â1978, Jones wrote and drew the newspaper comic strip "Crawford" (also known as "Crawford & Morgan") for the Chicago Tribune-NY News Syndicate. In 2011 IDW Publishing collected Jones' strip as part of their Library of American Comic Strips.
In 1978, Jones' wife Dorothy died; three years later, he married Marian Dern, the writer of the comic strip "Rick O'Shay".
Jones-Avery letter.
On December 11, 1975, shortly after the release of "Bugs Bunny Superstar", which prominently featured Bob Clampett, Jones wrote a letter to Tex Avery, accusing Clampett of taking credit for ideas that were not his, and for characters created by other directors (notably Jones's Sniffles and Friz Freleng's Yosemite Sam). Their correspondence was never published in the media. It was forwarded to Michael Barrier, who conducted the interview with Clampett and was distributed by Jones to multiple people concerned with animation over the years. Robert McKimson claimed in an interview that many animators but mostly Clampett contributed to the crazy personality of Bugs, while others like Chuck Jones concentrated more on the more calmed-down gags. As far as plagiarism is concerned, McKimson claimed the animators would always be looking at each other's sheets to see if they could borrow some punchlines and cracks.
Later years.
Through the 1980s and 1990s, Jones was painting cartoon and parody art, sold through animation galleries by his daughter's company, Linda Jones Enterprises. Jones was the creative consultant and character designer for two Raggedy Ann animated specials and the first "Alvin and the Chipmunks" Christmas special "A Chipmunk Christmas". He made a cameo appearance in the 1984 film "Gremlins" and directed the Bugs Bunny/Daffy Duck animated sequences that bookend "" (1990). Jones directed animated sequences for various features such as a lengthy sequence in the 1992 film "Stay Tuned" and a shorter one seen at the start of the 1993 film "Mrs. Doubtfire".
Jones' final Looney Tunes cartoon was "From Hare to Eternity" in 1996, which starred Bugs Bunny and Yosemite Sam, with Greg Burson voicing Bugs. The cartoon was dedicated to Friz Freleng, who had died in 1995. Jones' final animation project was a series of 13 shorts starring a timber wolf character he had designed in the 1960s named Thomas Timber Wolf. The series was released online by Warner Bros. in 2000. From 2001 until 2004, Cartoon Network aired "The Chuck Jones Show" which features shorts directed by him. The show won the Annie Award for Outstanding Achievement in an Animated Special Project.
Death.
Jones died of heart failure on February 22, 2002. He was cremated and his ashes were scattered at sea. After his death, the Looney Tunes cartoon "Daffy Duck for President", based on the book that Jones had written and using Jones' style for the characters, originally scheduled to be released in 2000, was released in 2004 as part of of the "" DVD set.
Accolades.
Jones was a historical authority as well as a major contributor to the development of animation throughout the 20th century. He received an honorary degree from Oglethorpe University in 1993. For his contribution to the motion picture industry, Jones has a star on the Hollywood Walk of Fame at 7011 Hollywood Blvd.
Jones, whose work had been nominated eight times over his career for an Oscar (winning the award three times: "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line"), received an Honorary Academy Award in 1996 by the Board of Governors of the Academy of Motion Picture Arts and Sciences, for "the creation of classic cartoons and cartoon characters whose animated lives have brought joy to our real ones for more than half a century." At that year's awards show, Robin Williams, a self-confessed "Jones-aholic," presented the Honorary award to Jones, calling him "The Orson Welles of cartoons.", and the audience gave Jones a standing ovation as he walked onto the stage. For himself, a flattered Jones wryly remarked in his acceptance speech, "Well, what can I say in the face of such humiliating evidence? I stand guilty before the world of directing over three hundred cartoons in the last fifty or sixty years. Hopefully this means you've forgiven me."
Jones' life and legacy were celebrated January 12, 2012, with the official grand opening of "The Chuck Jones Experience" at Circus Circus Las Vegas. Many of Jones' family welcomed celebrities, animation aficionados and visitors to the new attraction when they opened the attraction in an appropriate and unconventional way. Among those in attendance were Jones' widow, Marian Jones; daughter Linda Clough; and grandchildren Craig, Todd and Valerie Kausen.

</doc>
<doc id="7673" url="https://en.wikipedia.org/wiki?curid=7673" title="Costume">
Costume

Costume is the distinctive style of dress of an individual or group that reflects their class, gender, profession, ethnicity, nationality, activity or epoch.
The term also was traditionally used to describe typical appropriate clothing for certain activities, such as riding costume, swimming costume, dance costume, and evening costume. Appropriate and acceptable costume is subject to changes in fashion and local cultural norms.
This general usage has gradually been replaced by the terms "dress", "attire" or "wear" and usage of "costume" has become more limited to unusual or out-of-date clothing and to attire intended to evoke a change in identity, such as theatrical, Halloween, and mascot costumes.
Before the advent of ready-to-wear apparel, clothing was made by hand. When made for commercial sale it was made, as late as the beginning of the 20th century, by "costumiers", often women who ran businesses that met the demand for complicated or intimate female costume, including millinery and corsetry.
Etymology.
Costume comes from the same Italian word, inherited via French, which means fashion or custom.
National costume.
National costume or regional costume expresses local (or exiled) identity and emphasizes a culture's unique attributes. They are often a source of national pride. Examples include the Scottish kilt or Japanese kimono.
In Bhutan there is a traditional national dress prescribed for men and women, including the monarchy. These have been in vogue for thousands of years and have developed into a distinctive dress style. The dress worn by men is known as Gho which is a robe worn up to knee-length and is fastened at the waist by a band called the Kera. The front part of the dress which is formed like a pouch, in olden days was used to hold baskets of food and short dagger, but now it is used to keep cell phone, purse and the betel nut called "Doma". The dress worn by women consist of three pieces known as Kira, Tego and Wonju. The long dress which extends up to the ankle is Kira. The jacket worn above this is Tego which is provided with Wonju, the inner jacket. However, while visiting the Dzong or monastery a long scarf or stoll, called Kabney is worn by men across the shoulder, in colours appropriate to their ranks. Women also wear scarfs or stolls called Rachus, made of raw silk with embroidery, over their shoulder but not indicative of their rank.
Theatrical costume.
"Costume" often refers to a particular style of clothing worn to portray the wearer as a character or type of character at a social event in a theatrical performance on the stage or in film or television. In combination with other aspects of stagecraft, theatrical costumes can help actors portray characters' and their contexts as well as communicate information about the historical period/era, geographic location and time of day, season or weather of the theatrical performance. Some stylized theatrical costumes, such as Harlequin and Pantaloon in the Commedia dell'arte, exaggerate an aspect of a character.
Religious festivals.
The wearing of costumes is an important part of holidays developed from religious festivals such as Mardi Gras (in the lead up to Easter), and Halloween (related to All Hallow's Eve). Mardi Gras costumes usually take the form of jesters and other fantasy characters; Halloween costumes traditionally take the form of supernatural creatures such as ghosts, vampires, pop-culture icons and angels. In modern times. Christmas costumes typically portray characters such as Santa Claus (developed from Saint Nicholas). In Australia, the United Kingdom and the United States the American version of a Santa suit and beard is popular; in the Netherlands, the costume of Zwarte Piet is customary. Easter costumes are associated with the Easter Bunny or other animal costumes.
In Judaism, a common practice is to dress up on Purim. During this holiday, Jews celebrate the change of their destiny. They were delivered from being the victims of an evil decree against them and were instead allowed by the King to destroy their enemies. A quote from the Book of Esther, which says: "On the contrary" () is the reason that wearing a costume has become customary for this holiday.
Buddhist religious festivals in Tibet, Bhutan, Mongolia and Lhasa and Sikkim in India perform the Cham dance, which is a popular dance form utilising masks and costumes.
Parades and processions.
Parades and processions provide opportunities for people to dress up in historical or imaginative costumes. For example, in 1879 the artist Hans Makart designed costumes and scenery to celebrate the wedding anniversary of the Austro-Hungarian Emperor and Empress and led the people of Vienna in a costume parade that became a regular event until the mid-twentieth century. Uncle Sam costumes are worn on Independence Day in the United States. The Lion Dance, which is part of Chinese New Year celebrations, is performed in costume. Some costumes, such as the ones used in the Dragon Dance, need teams of people to create the required effect.
Sporting events and parties.
Public sporting events such as fun runs also provide opportunities for wearing costumes, as do private masquerade balls and fancy dress parties.
Mascots.
Costumes are popularly employed at sporting events, during which fans dress as their team's representative mascot to show their support. Businesses use mascot costumes to bring in people to their business either by placing their mascot in the street by their business or sending their mascot out to sporting events, festivals, national celebrations, fairs, and parades. Mascots appear at organizations wanting to raise awareness of their work. Children's Book authors create mascots from the main character to present at their book signings. Animal costumes that are visually very similar to mascot costumes are also popular among the members of the furry fandom, where the costumes are referred to as fursuits and match one's animal persona, or "fursona".
Children.
Costumes also serve as an avenue for children to explore and roleplay. For example, children may dress up like characters from history or fiction, like pirates, princesses or cowboys. They may also dress in uniforms use in common jobs, like nurses or police officers, or as zoo or farm animals. Overall, young boys tend to prefer costumes that reinforce stereotypical ideas of being male, and young girls tend to prefer costumes that reinforce stereotypical ideas of being female.
Cosplay.
Cosplay, a word of Japanese origin that's short for "costume play", is a performance art in which participants wear costumes and accessories to represent a specific character or idea that is usually always identified with a unique name (as opposed to a generic word). These costume wearers often interact to create a subculture centered on role play, so they can be seen most often in play groups, or at a gathering or convention. A significant number of these costumes are homemade and unique, and depend on the character, idea, or object the costume wearer is attempting to imitate or represent. The costumes themselves are often artistically judged to how well they represent the subject or object that the costume wearer is attempting to contrive.
Design.
Costume design is the envisioning of clothing and the overall appearance of a character or performer. Costume may refer to the style of dress particular to a nation, a class, or a period. In many cases, it may contribute to the fullness of the artistic, visual world that is unique to a particular theatrical or cinematic production. The most basic designs are produced to denote status, provide protection or modesty, or provide visual interest to a character. Costumes may be for, but not limited to, theater, cinema, or musical performances. Costume design should not be confused with costume coordination, which merely involves altering existing clothing, although both processes are used to create stage clothes.
Organizations.
The Costume Designers Guild's international membership includes motion picture, television, and commercial costume designers, assistant costume designers and costume illustrators, and totals over 750 members.
Publications.
"The Costume Designer" is a quarterly magazine devoted to the costume design industry.
Notable designers and awards.
Notable costume designers include recipients of the Academy Award for Best Costume Design, Tony Award for Best Costume Design, and Drama Desk Award for Outstanding Costume Design. Edith Head and Orry-Kelly, both of whom were born late in 1897, were two of Hollywood's most notable costume designers.
DIY and homemade costumes.
In the 20th century, contemporary fabric stores offered commercial patterns that could be bought and used to make a costume from raw materials. Some companies also began producing catalogs with great numbers of patterns.
More recently, and particularly with the advent of the Internet, the DIY movement has ushered in a new era of DIY costumes and pattern sharing. POPSUGAR is one such example, with several hundred designs readily available. YouTube, Pinterest, Mashable also feature many DIY costumes.
Industry.
Professional-grade costumes are typically designed and produced by artisan crafters, often specifically for a particular character or setting. Specialty shops may also include common costumes of this caliber.
Some high-end costumes may even be designed by the costume's wearer.
The costume industry includes vendors such the American company Spirit Halloween, which opens consumer-oriented stores seasonally for Halloween with pre-made costumes.

</doc>
<doc id="7674" url="https://en.wikipedia.org/wiki?curid=7674" title="Cable car (railway)">
Cable car (railway)

A cable car is a type of cable transportation used for mass transit where rail cars are hauled by a continuously moving cable running at a constant speed. Individual cars stop and start by releasing and gripping this cable as required. Cable cars are distinct from funiculars, where the cars are permanently attached to the cable, and cable railways, which are similar to funiculars, but where the rail vehicles are attached and detached manually.
History.
The first cable-operated railway, employing a moving rope that could be picked up or released by a grip on the cars was
the Fawdon railway (or wagonway) in 1826, a Colliery railway line. The London and Blackwall Railway, which opened for passengers in east London, England, in 1840 used such a system. The rope available at the time proved too susceptible to wear and the system was abandoned in favour of steam locomotives after eight years. In America, the first cable car installation in operation probably was the West Side and Yonkers Patent Railway in New York City, which ran from 1 July 1868 to 1870. The cable technology used in this elevated railway involved collar-equipped cables and claw-equipped cars, and proved cumbersome. The line was closed and rebuilt, and reopened with steam locomotives.
In 1869 P. G. T. Beauregard demonstrated Cable car at New Orleans and issued .
Other cable cars to use grips were those of the Clay Street Hill Railroad, which later became part of the San Francisco cable car system. The building of this line was promoted by Andrew Smith Hallidie with design work by William Eppelsheimer, and it was first tested in 1873. The success of these grips ensured that this line became the model for other cable car transit systems, and this model is often known as the "Hallidie Cable Car".
In 1881 the Dunedin cable tramway system opened in Dunedin, New Zealand and became the first such system outside San Francisco. For Dunedin, George Smith Duncan further developed the Hallidie model, introducing the pull curve and the slot brake; the former was a way to pull cars through a curve, since Dunedin's curves were too sharp to allow coasting, while the latter forced a wedge down into the cable slot to stop the car. Both of these innovations were generally adopted by other cities, including San Francisco.
In Australia the Melbourne cable tramway system operated from 1885 to 1940. It was one of the most extensive in the world with 1200 trams and trailers operating over 15 routes with 103Â km (64 miles) of track. Sydney also had a few cable tram routes.
Cable cars rapidly spread to other cities, although the major attraction for most was the ability to displace horsecar (or mule-drawn) systems rather than the ability to climb hills. Many people at the time viewed horse-drawn transit as unnecessarily cruel, and the fact that a typical horse could work only four or five hours per day necessitated the maintenance of large stables of draft animals that had to be fed, housed, groomed, medicated and rested. Thus, for a period, economics worked in favour of cable cars even in relatively flat cities.
For example, the Chicago City Railway, also designed by Eppelsheimer, opened in Chicago in 1882 and went on to become the largest and most profitable cable car system. As with many cities, the problem in flat Chicago was not one of grades but of transportation capacity. This caused a different approach to the combination of grip car and trailer. Rather than using a grip car and single trailer, as many cities did, or combining the grip and trailer into a single car, like San Francisco's "California Cars", Chicago used grip cars to pull trains of up to three trailers.
In 1883 the New York and Brooklyn Bridge Railway was opened, which had a most curious feature: though it was a cable car system, it used steam locomotives to get the cars into and out of the terminals. After 1896 the system was changed to one on which a motor car was added to each train to maneuver at the terminals, while en route, the trains were still propelled by the cable.
On 25 September 1883 a test of a cable car system was held by Liverpool United Tramways and Omnibus Company in Kirkdale, Liverpool. This would have been the first cable car system in Europe, but the company decided against implementing it. Instead the distinction went to the 1884 route from Archway to Highgate, north London, which used a continuous cable and grip system on the 1 in 11 (9%) climb of Highgate Hill. The installation was not reliable and was replaced by electric traction in 1909. Other cable car systems were implemented in Europe, though, among which was the Glasgow District Subway, the first underground cable car system, in 1896. (London's first deep-level tube railway, the City & South London Railway, had earlier also been built for cable haulage but had been converted to electric traction before opening in 1890.) A few more cable car systems were built in the United Kingdom, Portugal and France, but European cities, having many more curves in their streets, were less suitable for cable cars than American cities.
Though some new cable car systems were still being built, by 1890 the cheaper to construct and simpler to operate electrically-powered trolley or tram started to become the norm, and eventually started to replace existing cable car systems. For a while hybrid cable/electric systems operated, for example in Chicago where electric cars had to be pulled by grip cars through the loop area, due to the lack of trolley wires there. Eventually, San Francisco became the only street-running manually operated system to surviveâDunedin, the second city with such cars, was also the second-last city to operate them, closing down in 1957.
Recent Revival.
In the last decades of the 20th century cable traction in general has seen a limited revival as automatic people movers, used in resort areas, airports (for example, Toronto Airport), huge hospital centers and some urban settings. While many of these systems involve cars permanently attached to the cable, the Minimetro system from Poma/Leitner Group and the Cable Liner system from DCC Doppelmayr Cable Car both have variants that allow the cars to be automatically decoupled from the cable under computer control, and can thus be considered a modern interpretation of the cable car.
Operation.
The cable is itself powered by a stationary motor or engine situated in a cable house or power house. The speed at which it moves is relatively constant depending on the number of units gripping the cable at any given time.
The cable car begins moving when a clamping device attached to the car, called a "grip", applies pressure to ("grips") the moving cable. Conversely the car is stopped by releasing pressure on the cable (with or without completely detaching) and applying the brakes. This gripping and ungripping action may be manual, as was the case in all early cable car systems, or automatic, as is the case in some recent cable operated people mover type systems. Gripping must be an even and gradual process in order to avoid bringing the car to cable speed too quickly and unacceptably jarring the passengers.
In the case of manual systems, the grip resembles a very large pair of pliers, and considerable strength and skill are required to operate the car. As many early cable car operators discovered the hard way, if the grip is not applied properly, it can damage the cable, or even worse, become entangled in the cable. In the latter case, the cable car may not be able to stop and can wreak havoc along its route until the cable house realizes the mishap and halts the cable.
One apparent advantage of the cable car is its relative energy efficiency, because of the economy of centrally located power stations, and the ability of descending cars to transfer energy to ascending cars. However, this advantage is totally negated by the relatively large energy consumption required to simply move the cable over and under the numerous guide rollers and around the many sheaves. Approximately 95% of the tractive effort in the San Francisco system is expended in simply moving the four cables at 9.5 miles per hour. Electric cars with regenerative braking do offer the advantages, without the problem of moving a cable. In the case of steep grades, however, cable traction has the major advantage of not depending on adhesion between wheels and rails. There is also the obvious advantage that keeping the car gripped to the cable will also limit the downhill speed to that of the cable.
Because of the constant and relatively low speed, a cable car's potential to cause harm in an accident can be underestimated. Even with a cable car traveling at only 9 miles per hour, the mass of the cable car and the combined strength and speed of the cable can do quite a lot of damage in a collision.
Relation to Funiculars.
A cable car is superficially similar to a funicular, but differs from such a system in that its cars are not permanently attached to the cable and can stop independently, whereas a funicular has cars that are permanently attached to the propulsion cable, which is itself stopped and started. A cable car cannot climb as steep a grade as a funicular, but many more cars can be operated with a single cable, making it more flexible, and allowing a higher capacity. During the rush hour on San Francisco's Market Street Railway, a car would leave the terminal every 15 seconds.
A few funicular railways operate in street traffic, and because of this operation are often incorrectly described as cable cars. Examples of such operation, and the consequent confusion, are:
Even more confusingly, a hybrid cable car/funicular line once existed in the form of the original Wellington Cable Car, in the New Zealand city of Wellington. This line had both a continuous loop haulage cable that the cars gripped using a cable car gripper, and a balance cable permanently attached to both cars over an undriven pulley at the top of the line. The descending car gripped the haulage cable and was pulled downhill, in turn pulling the ascending car (which remained ungripped) uphill by the balance cable. This line was rebuilt in 1979 and is now a standard funicular, although it retains its old cable car name.
List of cable car systems.
Cities currently operating cable cars.
Traditional cable car systems.
The best known existing cable car system is the San Francisco cable car system in the city of San Francisco, California. San Francisco's cable cars constitute the oldest and largest such system in permanent operation, and it is the only one to still operate in the traditional manner with manually operated cars running in street traffic.
Modern cable car systems.
Several cities operate a modern version of the cable car system. These systems are fully automated and run on their own reserved right of way. They are commonly referred to as people movers, although that term is also applied to systems with other forms of propulsion, including funicular style cable propulsion.
These cities include:
Cities previously operating cable cars.
United States.
8th St. Tunnel in use (1887-1956)
External links.
Information
Patents

</doc>
<doc id="7676" url="https://en.wikipedia.org/wiki?curid=7676" title="Creaky voice">
Creaky voice

In linguistics, creaky voice (sometimes called laryngealisation, pulse phonation, vocal fry, or glottal fry) is a special kind of phonation in which the arytenoid cartilages in the larynx are drawn together; as a result, the vocal folds are compressed rather tightly, becoming relatively slack and compact. They vibrate irregularly at 20â50 pulses per second, about two octaves below the frequency of normal voicing, and the airflow through the glottis is very slow. Although creaky voice may occur with very low pitch, as at the end of a long intonation unit, it can occur with any pitch.
Creaky voice is prevalent as a peer-group affectation among young women in the United States. According to a 2012 study in "PLOS ONE", young women using creaky voice are viewed as less competent, less educated, less trustworthy, less attractive and less employable.
In some languages, such as Jalapa Mazatec, creaky voice has a phonemic status; that is, the presence or absence of creaky voice can change the meaning of a word. In the International Phonetic Alphabet, creaky voice of a phone is represented by a diacritical tilde , for example .
A slight degree of laryngealisation, occurring in some Korean consonants for example, is called "stiff voice". The Danish prosodic feature "stÃ¸d" is an example of a form of laryngealisation that has a phonemic function. 

</doc>
<doc id="7677" url="https://en.wikipedia.org/wiki?curid=7677" title="Computer monitor">
Computer monitor

A computer monitor or a computer display is an electronic visual display for computers. A monitor usually comprises the display device, circuitry, casing, and power supply. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) or a flat panel LED display, while older monitors used a cathode ray tubes (CRT). It can be connected to the computer via VGA, DVI, HDMI, DisplayPort, Thunderbolt, LVDS (Low-voltage differential signaling) or other proprietary connectors and signals.
Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and computer monitors, has changed from 4:3 to 16:10, to 16:9.
History.
Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information, and were very transient, they were rarely considered for programme output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the programme's operation.
As technology developed it was realized that the output of a CRT display was more flexible than a panel of light bulbs and eventually, by giving control of what was displayed to the programme itself, the monitor itself became a powerful output device in its own right.
Technologies.
Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.
Cathode ray tube.
The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for specialized military, industrial and scientific applications but they were far too costly for general use.
Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.
By the end of the 1980s color CRT monitors that could clearly display 1024 x 768 pixels were widely available and increasingly affordable. During the following decade maximum display resolutions gradually increased and prices continued to fall. CRT technology remained dominant in the PC monitor market into the new millennium partly because it was cheaper to produce and offered viewing angles close to 180 degrees. CRTs still offer some image quality advantages over LCDs but improvements to the latter have made them much less obvious. The dynamic range of early LCD panels was very poor, and although text and other motionless graphics were sharper than on a CRT, an LCD characteristic known as pixel lag caused moving graphics to appear noticeably smeared and blurry.
Liquid crystal display.
There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCDs justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.
TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.
The first standalone LCDs appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors. The main advantages of LCDs over CRT displays are that LCDs consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain. On the other hand, CRT monitors have superior contrast, have superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.
Organic light-emitting diode.
Organic light-emitting diode (OLED) monitors provide higher contrast and better viewing angles than LCDs but they require more power when displaying documents with white or bright backgrounds. In 2011, a OLED monitor cost $7500, but the prices are expected to drop.
Measurements of performance.
The performance of a monitor is measured by the following parameters:
Size.
On two-dimensional display devices such as computer monitors the display size or viewable image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.
The size of a display is usually by monitor manufacturers given by the diagonal, i.e. the distance between two opposite screen corners. This method of measurement is inherited from the method used for the first generation of CRT television, when picture tubes with circular faces were in common use. Being circular, it was the external diameter of the glass envelope that described their size. Since these circular tubes were used to display rectangular images, the diagonal measurement of the rectangular image was smaller than the diameter of the tube's face (due to the thickness of the glass). This method continued even when cathode ray tubes were manufactured as rounded rectangles; it had the advantage of being a single number specifying the size, and was not confusing when the aspect ratio was universally 4:3.
With the introduction of flat panel technology, the diagonal measurement became the actual diagonal of the visible display. This meant that an eighteen-inch LCD had a larger visible area than an eighteen-inch cathode ray tube.
The estimation of the monitor size by the distance between opposite corners does not take into account the display aspect ratio, so that for example a 16:9 widescreen display has less area, than a 4:3 screen. The 4:3 screen has dimensions of and area , while the widescreen is , .
Aspect ratio.
Until about 2003, most computer monitors had a aspect ratio and some had . Between 2003 and 2006, monitors with and mostly (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time. In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.
In 2010 the computer industry started to move over from to because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture.
In 2011 non-widescreen displays with 4:3 aspect ratios were only being manufactured in small quantities. According to Samsung this was because the "Demand for the old 'Square monitors' has decreased rapidly over the last couple of years," and "I predict that by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand."
Resolution.
The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 800x600 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080. Before 2013 top-end consumer products were limited to 2560x1600 at , excluding Apple products. Apple introduced 2880x1800 with Retina MacBook Pro at on June 12, 2012, and introduced a 5120x2880 Retina iMac at on October 16, 2014. By 2015 all major display manufacturers had released 3840x2160 resolution displays.
Additional features.
Power saving.
Most modern monitors will switch to a power-saving mode if no video-input signal is received. This allows modern operating systems to turn off a monitor after a specified period of inactivity. This also extends the monitor's service life.
Some monitors will also switch themselves off after a time period on standby.
Most modern laptops provide a method of screen dimming after periods of inactivity or when the battery is in use. This extends battery life and reduces wear.
Integrated accessories.
Many monitors have other accessories (or connections for them) integrated. This places standard ports within easy reach and eliminates the need for another separate hub, camera, microphone, or set of speakers. These monitors have advanced microprocessors which contain codec information, Windows Interface drivers and other small software which help in proper functioning of these functions.
Glossy screen.
Some displays, especially newer LCD monitors, replace the traditional anti-glare matte finish with a glossy one. This increases color saturation and sharpness but reflections from lights and windows are very visible. Anti-reflective coatings are sometimes applied to help reduce reflections, although this only mitigates the effect.
Curved designs.
In about 2009, NEC/Alienware together with Ostendo Technologies (based in Carlsbad, CA) were offering a curved (concave) monitor that allows better viewing angles near the edges, covering 75% of peripheral vision. This monitor had 2880x900 resolution, LED backlight and was marketed as suitable both for gaming and office work, while for $6499 it was rather expensive. While this particular monitor is no longer in production, most PC manufacturers now offer some sort of curved desktop display.
Directional screen.
Narrow viewing angle screens are used in some security conscious applications.
3D.
Newer monitors are able to display a different image for each eye, often with the help of special glasses, giving the perception of depth.
A directional screen which generates 3D images without headgear.
Touch screen.
These monitors use touching of the screen as an input method. Items can be selected or moved with a finger, and finger gestures may be used to convey commands. The screen will need frequent cleaning due to image degradation from fingerprints.
Tablet screens.
A combination of a monitor with a graphics tablet. Such devices are typically unresponsive to touch without the use of one or more special tools' pressure. Newer models however are now able to detect touch from any pressure and often have the ability to detect tilt and rotation as well.
Touch and tablet screens are used on LCDs as a substitute for the light pen, which can only work on CRTs.
Mounting.
Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.
Desktop.
A desktop monitor is typically provided with a stand from the manufacturer which lifts the monitor up to a more ergonomic viewing height. The stand may be attached to the monitor using a proprietary method or may use, or be adaptable to, a Video Electronics Standards Association, VESA, standard mount. Using a VESA standard mount allows the monitor to be used with an after-market stand once the original stand is removed. Stands may be fixed or offer a variety of features such as height adjustment, horizontal swivel, and landscape or portrait screen orientation.
VESA mount.
The Flat Display Mounting Interface (FDMI), also known as VESA Mounting Interface Standard (MIS) or colloquially as a VESA mount, is a family of standards defined by the Video Electronics Standards Association for mounting flat panel monitors, TVs, and other displays to stands or wall mounts. It is implemented on most modern flat-panel monitors and TVs.
For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.
Rack mount.
Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:
A fixed rack mount monitor is mounted directly to the rack with the LCD visible at all times. The height of the unit is measured in rack units (RU) and 8U or 9U are most common to fit 17-inch or 19-inch LCDs. The front sides of the unit are provided with flanges to mount to the rack, providing appropriately spaced holes or slots for the rack mounting screws. A 19-inch diagonal LCD is the largest size that will fit within the rails of a 19-inch rack. Larger LCDs may be accommodated but are 'mount-on-rack' and extend forward of the rack. There are smaller display units, typically used in broadcast environments, which fit multiple smaller LCDs side by side into one rack mount.
A stowable rack mount monitor is 1U, 2U or 3U high and is mounted on rack slides allowing the display to be folded down and the unit slid into the rack for storage. The display is visible only when the display is pulled out of the rack and deployed. These units may include only a display or may be equipped with a keyboard creating a KVM (Keyboard Video Monitor). Most common are systems with a single LCD but there are systems providing two or three displays in a single rack mount system.
Panel mount.
A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.
Open frame.
An open frame monitor provides the LCD monitor and enough supporting structure to hold associated electronics and to minimally support the LCD. Provision will be made for attaching the unit to some external structure for support and protection. Open frame LCDs are intended to be built in to some other piece of equipment. An arcade video game would be a good example with the display mounted inside the cabinet. There is usually an open frame display inside all end-use displays with the end-use display simply providing an attractive protective enclosure. Some rack mount LCD manufacturers will purchase desk-top displays, take them apart, and discard the outer plastic parts, keeping the inner open-frame LCD for inclusion into their product.
Security vulnerabilities.
According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.
Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.

</doc>
<doc id="7681" url="https://en.wikipedia.org/wiki?curid=7681" title="ClearType">
ClearType

ClearType is Microsoft's implementation of subpixel rendering technology in rendering text in a font system. ClearType attempts to improve the appearance of text on certain types of computer display screens by sacrificing color fidelity for additional intensity variation. This trade-off is asserted to work well on LCD flat panel monitors.
ClearType was first announced at the November 1998 COMDEX exhibition. The technology was first introduced in software in January 2000 as an always-on feature of Microsoft Reader, which was released to the public in August 2000.
ClearType was significantly changed with the introduction of DirectWrite in Windows 7.
Word 2013 stopped using ClearType, because "There is a problem with ClearType: it depends critically on the color of the background pixels."
Background.
Computer displays where the positions of individual pixels are permanently fixed such as most modern flat panel displays can show saw-tooth edges when displaying small, high-contrast graphic elements, such as text. ClearType uses spatial anti-aliasing at the subpixel level to reduce visible artifacts on such displays when text is rendered, making the text appear "smoother" and less jagged. ClearType also uses very heavy font hinting to force the font to fit into the pixel grid. This increases edge contrast and readability of small fonts at the expense of font rendering fidelity and has been criticized by graphic designers for making different fonts look similar.
Like most other types of subpixel rendering, ClearType involves a compromise, sacrificing one aspect of image quality (color or "chrominance" detail) for another (light and dark or "luminance" detail). The compromise can improve text appearance when luminance detail is more important than chrominance.
Only user and system applications render application of ClearType. ClearType does not alter other graphic display elements (including text already in bitmaps). For example, ClearType enhancement renders text on the screen in Microsoft Word, but text placed in a bitmapped image in a program such as Adobe Photoshop is not. In theory, the method (called "RGB Decimation" internally) can enhance the anti-aliasing of any digital image.
ClearType is not used when printing text. Most printers already use such small pixels that aliasing is rarely a problem, and they don't have the addressable fixed subpixels ClearType requires. Nor does ClearType affect text stored in files. ClearType only applies any processing to the text while it is being rendered onto the screen.
ClearType was invented in the Microsoft e-Books team by Bert Keely and Greg Hitchcock. It was then analyzed by researchers in the company, and signal processing expert John Platt designed an improved version of the algorithm. Dick Brass, a Vice President at Microsoft from 1997 to 2004, complained that the company was slow in moving ClearType to market in the portable computing field.
How ClearType works.
Normally, the software in a computer treats the computerâs display screen as a rectangular array of square, indivisible pixels, each of which has an intensity and color that are determined by the blending of three primary colors: red, green, and blue. However, actual display hardware usually implements each pixel as a group of three adjacent, independent "subpixels," each of which displays a different primary color. Thus, on a real computer display, each pixel is actually composed of separate red, green, and blue subpixels. For example, if a flat-panel display is examined under a magnifying glass, the pixels may appear as follows:
In the illustration above, there are nine pixels but 27 subpixels.
If the computer controlling the display knows the exact position and color of all the subpixels on the screen, it can take advantage of this to improve the apparent resolution in certain situations. If each pixel on the display actually contains three rectangular subpixels of red, green, and blue, in that fixed order, then things on the screen that are smaller than one full pixel in size can be rendered by lighting only one or two of the subpixels. For example, if a diagonal line with a width smaller than a full pixel must be rendered, then this can be done by lighting only the subpixels that the line actually touches. If the line passes through the leftmost portion of the pixel, only the red subpixel is lit; if it passes through the rightmost portion of the pixel, only the blue subpixel is lit. This effectively triples the horizontal resolution of the image at normal viewing distances; the drawback is that the line thus drawn will show color fringes (at some points it might look green, at other points it might look red or blue).
ClearType uses this method to improve the smoothness of text. When the elements of a type character are smaller than a full pixel, ClearType lights only the appropriate subpixels of each full pixel in order to more closely follow the outlines of that character. Text rendered with ClearType looks âsmootherâ than text rendered without it, provided that the pixel layout of the display screen exactly matches what ClearType expects.
The following picture shows a 4Ã enlargement of the word "Wikipedia" rendered using ClearType. The word was originally rendered using a Times New Roman 12 pt font.
In this magnified view, it becomes clear that, while the overall smoothness of the text seems to improve, there is also color fringing of the text.
An extreme close-up of a color display shows (a) text rendered without ClearType and (b) text rendered with ClearType. Note the changes in subpixel intensity that are used to increase effective resolution when ClearType is enabled without ClearType, all sub-pixels of a given pixel have the same intensity.
In the above lines of text, when the orange circle is shown, all the text in the frame is rendered using ClearType (RGB subpixel rendering); when the orange circle is absent all the text is rendered using normal (full pixel greyscale) anti-aliasing.
ClearType, human vision and cognition.
ClearType and similar technologies work on the theory that variations in intensity are more noticeable than variations in color.
Expert opinion.
In a MSDN article, Microsoft acknowledges that "that is rendered with ClearType can also appear significantly different when viewed by individuals with varying levels of color sensitivity. Some individuals can detect slight differences in color better than others." This opinion is shared by font designer Thomas Phinney (Vice President of FontLab and formerly with Adobe Systems): "There is also considerable variation between individuals in their sensitivity to color fringing. Some people just notice it and are bothered by it a lot more than others." Software developer Melissa Elliot has written about finding ClearType rendering uncomfortable to read, saying that "instead of seeing black text, I see blue text, and rendered over it but offset by a pixel or two, I see orange text, and someone reached into a bag of purple pixel glitter and just tossed it on...Iâm not the only person in the world with this problem, and yet, every time it comes up, people are quick to assure me it works for them as if thatâs supposed to make me feel better."
Hinting expert Beat Stamm, who worked on ClearType at Microsoft, agrees that ClearType may look blurry at 96 dpi, which was a typical resolution for LCDs in 2008, but adds that higher resolution displays improve on this aspect: "WPF Presentation Foundation uses method C with fractional pixel positioning, but few display devices have a sufficiently high resolution to make the potential blur a moot point for everybody.Â .Â .Â . Some people are ok with the blur in Method C, some arenât. Anecdotal evidence suggests that some people are fine with Method C when reading continuous text at 96 dpi (e.g. Times Reader, etc.) but not in UI scenarios. Many people are fine with the colors of ClearType, even at 96 dpi, but a few arenâtâ¦ To my eyes and at 96 dpi, Method C doesnât read as well as Method A. It reads âblurrilyâ to me. Conversely, at 144 dpi, I donât see a problem with Method C. It looks and reads just fine to me." One illustration of the potential problem is the following image:
In the above block of text, the same portion of text is shown in the upper half without and in the lower half with ClearType rendering (as opposed to Standard and ClearType in the previous image). This and the previous example with the orange circle demonstrate the blurring introduced. For many observers this blurring is beneficial; others do not find ClearType beneficial.
Empirical studies.
A 2001 study, conducted by researchers from Clemson University and The University of Pennsylvania on "18 users who spent 60 minutes reading fiction from each of three different displays" found that "When reading from an LCD display, users preferred text rendered with ClearTypeâ¢. ClearType also yielded higher readability judgments and lower ratings of mental fatigue." A 2002 study on 24 users conducted by the same researchers from Clemson University also found that "Participants were significantly more accurate at identifying words with ClearTypeâ¢ than without ClearTypeâ¢."
According to a 2006 study, at the University of Texas at Austin by Dillon et al., ClearType "may not be universally beneficial". The study notes that maximum benefit may be seen when the information worker is spending large proportions of their time reading text (which is not necessarily the case for the majority of computer users today). Additionally, over one third of the study participants experienced some disadvantage when using ClearType. Whether ClearType, or other rendering, should be used is very subjective and it must be the choice of the individual, with the report recommending "to allow users to disable if they find it produces effects other than improved performance".
Another 2007 empirical study, found that "while ClearType rendering does not improve text legibility, reading speed or comfort compared to perceptually-tuned grayscale rendering, subjects prefer text with moderate ClearType rendering to text with grayscale or higher-level ClearType contrast."
A 2007 survey, of the literature by Microsoft researcher Kevin Larson presented a different picture: "Peer-reviewed studies have consistently found that using ClearType boosts reading performance compared with other text-rendering systems. In a 2004 study, for instance, Lee Gugerty, a psychology professor at Clemson University, in South Carolina, measured a 17 percent improvement in word recognition accuracy with ClearType. Gugertyâs group also showed, in a sentence comprehension study, that ClearType boosted reading speed by 5 percent and comprehension by 2 percent. Those results were unusual because, typically, any gain in reading speed decreases comprehension. Similarly, in a study published last year, psychologist Andrew Dillon at the University of Texas at Austin found that when subjects were asked to scan a spreadsheet and pick out certain information, they did those tasks 7 percent faster with ClearType."
Display requirements.
ClearType and allied technologies require display hardware with fixed pixels and subpixels. More precisely, the positions of the pixels and subpixels on the screen must be exactly known to the computer to which it is connected. This is the case for flat-panel displays, on which the positions of the pixels are permanently fixed by the design of the screen itself. Almost all flat panels have a perfectly rectangular array of square pixels, each of which contains three rectangular subpixels in the three primary colors, with the normal ordering being red, green, and blue, arranged in vertical bands. ClearType assumes this arrangement of pixels when rendering text.
ClearType does not work properly with flat-panel displays that are operated at resolutions other than their ânativeâ resolutions, since only the native resolution corresponds exactly to the actual positions of pixels on the screen of the display.
If a display does not have the type of fixed pixels that ClearType expects, text rendered with ClearType enabled actually looks worse than type rendered without it. Some flat panels have unusual pixel arrangements, with the colors in a different order, or with the subpixels positioned differently (in three horizontal bands, or in other ways). ClearType needs to be manually tuned for use with such displays (see below).
ClearType will not work as intended on displays that have no fixed pixel positions, such as CRT displays, however it will still have some antialiasing effect and may be preferable to some users as compared to non-anti-aliased type.
Sensitivity to display orientation.
Because ClearType utilizes the physical layout of the red, green and blue pigments of the LCD screen, it is sensitive to the orientation of the display.
ClearType in Windows XP currently supports the RGB and BGR sub pixel structures. Rotated displays, in which the subpixels are arranged vertically rather than horizontally, are "not" currently supported. Using ClearType on these display configurations will actually reduce the display quality. The best option for users of Windows XP having rotated LCD displays (Tablet PCs or swivel-stand LCD displays) is using regular anti-aliasing, or switching off font-smoothing altogether.
The software developer documentation for Windows CE states that ClearType for rotated screens is supported on that platform.
Vertical sub pixel structures are not supported in Windows XP.
Implementations.
ClearType is also an integrated component of the Windows Presentation Foundation text-rendering engine.
ClearType in GDI.
ClearType can be globally enabled or disabled for GDI applications. A control panel applet is available to let the users tune the GDI ClearType settings. The GDI implementation of ClearType does not support sub-pixel positioning.
ClearType tuning.
Some versions of Microsoft Windows, as supplied, allow ClearType to be turned on or off, with no adjustment; other versions allow tuning of the ClearType parameters. A Microsoft ClearType tuner utility is available for free download for Windows versions lacking this facility. If ClearType is disabled in the operating system, applications with their own ClearType controls can still support it. Microsoft Reader (for e-books) has its own ClearType tuner.
ClearType in WPF.
All text in Windows Presentation Foundation is anti-aliased and rendered using ClearType. There are separate ClearType registry settings for GDI and WPF applications, but by default the WPF entries are absent, and the GDI values are used in their absence. WPF registry entries can be tuned using the instructions from the MSDN WPF Text Blog.
ClearType in WPF supports sub-pixel positioning, natural advance widths, Y-direction anti-aliasing and hardware acceleration. WPF supports aggressive caching of pre-rendered ClearType text in video memory. The extent to which this is supported is dependent on the video card. DirectX 10 cards will be able to cache the font glyphs in video memory, then perform the composition (assembling of character glyphs in the correct order, with the correct spacing), alpha blending (application of anti-aliasing), and RGB blending (ClearType's sub-pixel color calculations), entirely in hardware. This means that only the original glyphs need to be stored in video memory once per font (Microsoft estimates that this would require 2Â MB of video memory per font), and other operations such as the display of anti-aliased text on top of other graphics including video can also be done with no computation effort on the part of the CPU. DirectX 9 cards will only be able to cache the alpha-blended glyphs in memory, thus requiring the CPU to handle glyph composition and alpha-blending before passing this to the video card. Caching these partially rendered glyphs requires significantly more memory (Microsoft estimates 5Â MB per process). Cards that don't support DirectX 9 have no hardware-accelerated text rendering capabilities.
ClearType in DirectWrite.
The font rendering engine in DirectWrite supports an improved version of ClearType, as demonstrated at PDC 2008. The improved version is sometimes called "Natural ClearType". The improvements have been confirmed by independent sources, such as Firefox developers; they were particularly noticeable for OpenType fonts in Compact Font Format (CFF).
Despite these improvements, Word 2013 stopped using ClearType. The reasons invoked are, in the words of Murray Sargent: "There is a problem with ClearType: it depends critically on the color of the background pixels. This isnât a problem if you know a priori that those pixels are white, which is usually the case for text. But the general case involves calculating what the colors should be for an arbitrary background and that takes time. Meanwhile, Word 2013 enjoys cool animations and smooth zooming. Nothing jumps any more. Even the caret (the blinking vertical line at the text insertion point) glides from one position to the next as you type. Jerking movement just isnât considered cool any more. Well animations and zooms have to be faster than human response times in order to appear smooth. And that rules out ClearType in animated scenarios at least with present generation hardware. And in future scenarios, screens will have sufficiently high resolution that gray-scale anti-aliasing should suffice."
For the same reasons related to animation performance, the color-ware version of ClearType was abandoned in Metro and the Windows 8 (and 10) start menus.
Patents.
ClearType is a registered trademark and Microsoft claims protection under the following U.S. patents:
Other uses of the ClearType brand.
The ClearType name was also used to refer to the screens of Microsoft Surface tablets. ClearType HD Display includes a 1366Ã768 screen, while ClearType Full HD Display includes a 1920Ã1080 screen.

</doc>
<doc id="7682" url="https://en.wikipedia.org/wiki?curid=7682" title="Centriole">
Centriole

In cell biology a centriole ("centri-" + "-ole") is a cylindrical cell structure composed mainly of a protein called tubulin that is found in most eukaryotic cells. An associated pair of centrioles, surrounded by a shapeless mass of dense material, called the pericentriolar material, or PCM, makes up a compound structure called a centrosome.
Centrioles are present in the cells of most eukaryotes, for example those of animals. However, they are absent from conifers (pinophyta), flowering plants (angiosperms) and most fungi, and are only present in the male gametes of charophytes, bryophytes, seedless vascular plants, cycads, and ginkgo.
Most centrioles are made up of nine sets of microtubule triplets, arranged in a cylinder.
Deviations from this structure include crabs and "Drosophila melanogaster" embryos, with nine doublets, and "Caenorhabditis elegans" sperm cells and early embryos, with nine singlets.
Edouard van Beneden and Theodor Boveri made the first observation and identification of centrioles in 1883 and 1888 respectively, while the pattern of centriole duplication was first worked out independently by Etienne de Harven and Joseph G. Gall c. 1950 
The main function of centrioles is to produce aster and spindle during cell division.
Cell division.
Centrioles are involved in the organization of the mitotic spindle and in the completion of cytokinesis. Centrioles were previously thought to be required for the formation of a mitotic spindle in animal cells. However, more recent experiments have demonstrated that cells whose centrioles have been removed via laser ablation can still progress through the G1 stage of interphase before centrioles can be synthesized later in a de novo fashion. Additionally, mutant flies lacking centrioles develop normally, although the adult flies' cells lack flagella and cilia and as a result, they die shortly after birth.
The centrioles can self replicate during cell division.
Cellular organization.
Centrioles are a very important part of centrosomes, which are involved in organizing microtubules in the cytoplasm. The position of the centriole determines the position of the nucleus and plays a crucial role in the spatial arrangement of the cell.
Ciliogenesis.
In organisms with flagella and cilia, the position of these organelles is determined by the mother centriole, which becomes the basal body. An inability of cells to use centrioles to make functional cilia and flagella has been linked to a number of genetic and developmental diseases. In particular, the inability of centrioles to properly migrate prior to ciliary assembly has recently been linked to Meckel-Gruber syndrome.
Animal development.
Proper orientation of cilia via centriole positioning toward the posterior of embryonic node cells is critical for establishing leftâright asymmetry during mammalian development.
Centriole duplication.
Before DNA replication, cells contain two centrioles. The older of the two centrioles is termed the "mother centriole", the other the "daughter". During the cell division cycle, a new centriole grows from the side of each mother centriole. After duplication, the two centriole pairs will remain attached to each other orthogonally until mitosis. At that point the mother and daughter centrioles separate dependently on an enzyme called separase.
The two centrioles in the centrosome are tied to one another. The mother centriole has radiating appendages at the distal end of its long axis and is attached to its daughter at the proximal end. Each daughter cell formed after cell division will inherit one of these pairs. Centrioles start duplicating when DNA replicates.
Origin.
The last common ancestor of all eukaryotes was a ciliated cell with centrioles. Some lineages of eukaryotes, such as land plants, do not have centrioles except in their motile male gametes. Centrioles are completely absent from all cells of conifers and flowering plants, which do not have ciliate or flagellate gametes.
It is unclear if the last common ancestor had one or two cilia. Important genes required for centriole growth, like centrins, are only found in eukaryotes and not in bacteria or archaeans.
Etymology and pronunciation.
The word "centriole" () uses combining forms of "centri-" and "-ole", yielding "little central part", which describes a centriole's typical location near the center of the cell.

</doc>
<doc id="7683" url="https://en.wikipedia.org/wiki?curid=7683" title="Creation science">
Creation science

Creation science or scientific creationism is a branch of creationism that attempts to provide scientific support for the creation myth in the Book of Genesis and disprove or reinterpret the scientific facts, theories and scientific paradigms about geology, cosmology, biological evolution, archeology, history, and even linguistics.
The overwhelming consensus of the scientific community is that creation science is a religious, not a scientific view. It fails to qualify as a science because it lacks empirical support, supplies no tentative hypotheses, and resolves to describe natural history in terms of scientifically untestable supernatural causes. Creation science is a pseudoscientific attempt to map the Bible into scientific facts, and is viewed by professional biologists as unscholarly and, even, as a dishonest and misguided sham, with extremely harmful educational consequences.
Creation science began in the 1960s as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and nullify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. The main ideas in creation science are: the belief in "creation "ex nihilo"" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000â10,000 years; the belief that mankind and other life on Earth were created as distinct fixed "baraminological" "kinds"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists acknowledge are irreconcilable to the account in the Book of Genesis. Creation science proponents often refer to the theory of evolution as "Darwinism" or as "Darwinian evolution."
The creation science texts and curricula that first emerged in the 1960s focused upon concepts derived from a literal interpretation of the Bible and were overtly religious in nature, most notably linking Noah's flood in the Biblical Genesis account to the geological and fossil record in a system termed flood geology. These works attracted little notice beyond the schools and congregations of conservative fundamental and Evangelical Christians until the 1970s when its followers challenged the teaching of evolution in the public schools and other venues in the United States, bringing it to the attention of the public-at-large and the scientific community. Many school boards and lawmakers were persuaded to include the teaching of creation science alongside evolution in the science curriculum. Creation science texts and curricula used in churches and Christian schools were revised to eliminate their Biblical and theological references, and less explicitly sectarian versions of creation science education were introduced in public schools in Louisiana, Arkansas, and other regions in the United States.
The 1982 ruling in "McLean v. Arkansas" found that creation science fails to meet the essential characteristics of science and that its chief intent is to advance a particular religious view. The teaching of creation science in public schools in the United States effectively ended in 1987 following the United States Supreme Court decision in "Edwards v. Aguillard". The court affirmed that a statute requiring the teaching of creation science alongside evolution when evolution is taught in Louisiana public schools was unconstitutional because its sole true purpose was to advance a particular religious belief. In response to this ruling, drafts of the creation science school textbook "Of Pandas and People" were edited to change references of creation to intelligent design before its publication in 1989. The intelligent design movement promoted this version, then teaching intelligent design in public school science classes was found to be unconstitutional in the 2005 "Kitzmiller v. Dover Area School District" federal court case.
Beliefs and activities.
Religious basis.
Creation science is based largely upon chapters 1â11 of the Book of Genesis. These describe how God calls the world into existence through the power of speech ("And God said, Let there be light," etc.) in six days, calls all the animals and plants into existence, and molds the first man from clay and the first woman from a rib taken from the man's side; a world-wide flood destroys all life except for Noah and his family and representatives of the animals, and Noah becomes the ancestor of the 70 "nations" of the world; the nations live together until the incident of the Tower of Babel, when God disperses them and gives them their different languages. Creation science rarely goes beyond biblical stories in its study, and attempts to explain history and science within the span of Biblical chronology, which places the initial act of creation some six thousand years ago.
Modern religious affiliations.
Most creation science proponents hold fundamentalist or Evangelical Christian beliefs in Biblical literalism or Biblical inerrancy, as opposed to the higher criticism supported by Liberal Christianity in the FundamentalistâModernist Controversy. However, there are also examples of Islamic and Jewish scientific creationism that conform to the accounts of creation as recorded in their religious doctrines.
The Seventh-day Adventist Church has a history of support for creation science. This dates back to George McCready Price, an active Seventh-day Adventist who developed views of flood geology, which formed the basis of creation science. This work was continued by the Geoscience Research Institute, an official institute of the Seventh-day Adventist Church, located on its Loma Linda University campus in California.
Creation science is generally rejected by the Church of England as well as the Roman Catholic Church. The Pontifical Gregorian University has officially discussed intelligent design as a "cultural phenomenon" without scientific elements. The Church of England's official website cites Charles Darwin's local work assisting people in his religious parish.
Views on science.
Creation science rejects evolution's theory of the common descent of all living things on the Earth. Instead, it asserts that the field of evolutionary biology is itself pseudoscientific or even a religion. Creationists argue instead for a system called baraminology, which considers the living world to be descended from uniquely created kinds or "baramins."
Creation science incorporates the concept of catastrophism to reconcile current landforms and fossil distributions with Biblical interpretations, proposing the remains resulted from successive cataclysmic events, such as a world-wide flood and subsequent ice age. It rejects one of the fundamental principles of modern geology (and of modern science generally), uniformitarianism, which applies the same physical and geological laws observed on the Earth today to interpret the Earth's geological history.
Sometimes creationists attack other scientific concepts, like the Big Bang cosmological model or methods of scientific dating based upon radioactive decay. Young Earth creationists also reject current estimates of the age of the universe and the age of the Earth, arguing for creationist cosmologies with timescales much shorter than those determined by modern physical cosmology and geological science, typically less than 10,000 years.
The scientific community has overwhelmingly rejected the ideas put forth in creation science as lying outside the boundaries of a legitimate science. The foundational premises underlying scientific creationism disqualify it as a science because the answers to all inquiry therein are preordained to conform to Bible doctrine, and because that inquiry is constructed upon theories which are not empirically testable in nature. Scientists also deem creation science's attacks against biological evolution to be without scientific merit. Those views of the scientific community were accepted in two significant court decisions in the 1980s which found the field of creation science to be a religious mode of inquiry, not a scientific one.
History.
The teaching of evolution was gradually introduced into more and more public high school textbooks in the United States after 1900, but in the aftermath of the First World War the growth of fundamentalist Christianity gave rise to a creationist opposition to such teaching. Legislation prohibiting the teaching of evolution was passed in certain regions, most notably Tennessee's Butler Act of 1925. The Soviet Union's successful launch of "Sputnik 1" in 1957 sparked national concern that the science education in public schools was outdated. In 1958, the United States passed National Defense Education Act which introduced new education guidelines for science instruction. With federal grant funding, the Biological Sciences Curriculum Study (BSCS) drafted new standards for the public schools' science textbooks which included the teaching of evolution. Almost half the nation's high schools were using textbooks based on the guidelines of the BSCS soon after they were published in 1963. The Tennessee legislature did not repeal the Butler Act until 1967.
Creation science (dubbed "scientific creationism" at the time) emerged as an organized movement during the 1960s. It was strongly influenced by the earlier work of armchair geologist George McCready Price who wrote works such as "The New Geology" (1923) to advance what he termed "new catastrophism" and dispute the current geological time frames and explanations of geologic history. Price's work was cited at the Scopes Trial of 1925, yet although he frequently solicited feedback from geologists and other scientists, they consistently disparaged his work. Price's "new catastrophism" also went largely unnoticed by other creationists until its revival with the 1961 publication of "" by John C. Whitcomb and Henry M. Morris, a work which quickly became an important text on the issue to fundamentalist Christians and expanded the field of creation science beyond critiques of geology into biology and cosmology as well. Soon after its publication, a movement was underway to have the subject taught in United States' public schools.
Court determinations.
The various state laws prohibiting teaching of evolution were overturned in 1968 when the United States Supreme Court ruled in "Epperson v. Arkansas" such laws violated the Establishment Clause of the First Amendment to the United States Constitution. This ruling inspired a new creationist movement to promote laws requiring that schools give balanced treatment to creation science when evolution is taught. The 1981 Arkansas Act 590 was one such law that carefully detailed the principles of creation science that were to receive equal time in public schools alongside evolutionary principles. The act defined creation science as follows:
"'Creation-science' means the scientific evidences for creation and inferences from those evidences. Creation-science includes the scientific evidences and related inferences that indicate:
This legislation was examined in "McLean v. Arkansas", and the ruling handed down on January 5, 1982, concluded that creation-science as defined in the act "is simply not science". The judgement defined the following as essential characteristics of science:
The court ruled that creation science failed to meet these essential characteristics and identified specific reasons. After examining the key concepts from creation science, the court found:
The court further noted that no recognized scientific journal had published any article espousing the creation science theory as described in the Arkansas law, and stated that the testimony presented by defense attributing the absence to censorship was not credible.
In its ruling, the court wrote that for any theory to qualify as scientific, the theory must be tentative, and open to revision or abandonment as new facts come to light. It wrote that any methodology which begins with an immutable conclusion which cannot be revised or rejected, regardless of the evidence, is not a scientific theory. The court found that creation science does not culminate in conclusions formed from scientific inquiry, but instead begins with the conclusion, one taken from a literal wording of the Book of Genesis, and seeks only scientific evidence to support it.
The law in Arkansas adopted the same two-model approach as that put forward by the Institute for Creation Research, one allowing only two possible explanations for the origins of life and existence of man, plants and animals: it was either the work of a creator or it was not. Scientific evidence that failed to support the theory of evolution was posed as necessarily scientific evidence in support of creationism, but in its judgment the court ruled this approach to be no more than a "contrived dualism which has not scientific factual basis or legitimate educational purpose."
The judge concluded that "Act 590 is a religious crusade, coupled with a desire to conceal this fact," and that it violated the First Amendment's Establishment Clause.
The decision was not appealed to a higher court, but had a powerful influence on subsequent rulings. Louisiana's 1982 Balanced Treatment for Creation-Science and Evolution-Science Act, authored by State Senator Bill P. Keith, judged in the 1987 United States Supreme Court case "Edwards v. Aguillard", and was handed a similar ruling. It found the law to require the balanced teaching of creation science with evolution had a particular religious purpose and was therefore unconstitutional.
Intelligent design splits off.
In 1984, "The Mystery of Life's Origin" was first published. It was co-authored by chemist and creationist Charles B. Thaxton with Walter L. Bradley and Roger L. Olsen, the foreword written by Dean H. Kenyon, and sponsored by the Christian-based Foundation for Thought and Ethics (FTE). The work presented scientific arguments against current theories of abiogenesis and offered an hypothesis of special creation instead. While the focus of creation science had until that time centered primarily on the criticism of the fossil evidence for evolution and validation of the creation myth of the Bible, this new work posed the question whether science reveals that even the simplest living systems were far too complex to have developed by natural, unguided processes.
Kenyon later co-wrote with creationist Percival Davis a book intended as a "scientific brief for creationism" to use as a supplement to public high school biology textbooks. Thaxton was enlisted as the book's editor, and the book received publishing support from the FTE. Prior to its release, the 1987 Supreme Court ruling in "Edwards v. Aguillard" barred the teaching of creation science and creationism in public school classrooms. The book, originally titled "Biology and Creation" but renamed "Of Pandas and People", was released in 1989 and became the first published work to promote the anti-evolutionist design argument under the name intelligent design. The contents of the book later became a focus of evidence in the federal court case, "Kitzmiller v. Dover Area School District", when a group of parents filed suit to halt the teaching of intelligent design in Dover, Pennsylvania, public schools. School board officials there had attempted to include "Of Pandas and People" in their biology classrooms and testimony given during the trial revealed the book was originally written as a creationist text but following the adverse decision in the Supreme Court it underwent simple cosmetic editing to remove the explicit allusions to "creation" or "creator," and replace them instead with references to "design" or "designer."
By the mid-1990s, intelligent design had become a separate movement. The creation science movement is distinguished from the intelligent design movement, or neo-creationism, because most advocates of creation science accept scripture as a literal and inerrant historical account, and their primary goal is to corroborate the scriptural account through the use of science. In contrast, as a matter of principle, neo-creationism eschews references to scripture altogether in its polemics and stated goals (see Wedge strategy). By so doing, intelligent design proponents have attempted to succeed where creation science has failed in securing a place in public school science curricula. Carefully avoiding any reference to the identity of the intelligent designer as God in their public arguments, intelligent design proponents sought to reintroduce the creationist ideas into science classrooms while sidestepping the First Amendment's prohibition against religious infringement. However, the intelligent design curriculum was struck down as a violation of the Establishment Clause in "Kitzmiller v. Dover Area School District", the judge in the case ruling "that ID is nothing less than the progeny of creationism."
Today, creation science as an organized movement is primarily centered within the United States. Creation science organizations are also known in other countries, most notably Creation Ministries International which was founded (under the name Creation Science Foundation) in Australia.
Proponents are usually aligned with a Christian denomination, primarily with those characterized as evangelical, conservative, or fundamentalist. While creationist movements also exist in Islam and Judaism, these movements do not use the phrase "creation science" to describe their beliefs.
Issues.
Creation science has its roots in the work of young Earth creationist George McCready Price disputing modern science's account of natural history, focusing particularly on geology and its concept of uniformitarianism, and his efforts instead to furnish an alternative empirical explanation of observable phenomena which was compatible with strict Biblical literalism. Price's work was later discovered by civil engineer Henry M. Morris, who is now considered to be the father of creation science. Morris and later creationists expanded the scope with attacks against the broad spectrum scientific findings that point to the antiquity of the Universe and common ancestry among species, including growing body of evidence from the fossil record, absolute dating techniques, and cosmogony.
The proponents of creation science often say that they are concerned with religious and moral questions as well as natural observations and predictive hypotheses. Many state that their opposition to scientific evolution is primarily based on religion.
The overwhelming majority of scientists are in agreement that the claims of science are necessarily limited to those that develop from natural observations and experiments which can be replicated and substantiated by other scientists, and that claims made by creation science do not meet those criteria. Duane Gish, a prominent creation science proponent, has similarly claimed, "We do not know how the creator created, what processes He used, "for He used processes which are not now operating anywhere in the natural universe." This is why we refer to creation as special creation. We cannot discover by scientific investigation anything about the creative processes used by the Creator." But he also makes the same claim against science's evolutionary theory, maintaining that on the subject of origins, scientific evolution is a religious theory which cannot be validated by science.
Metaphysical assumptions.
Creation science makes the "a priori" metaphysical assumption that there exists a creator of the life whose origin is being examined. Christian creation science holds that the description of creation is given in the Bible, that the Bible is inerrant in this description (and elsewhere), and therefore empirical scientific evidence must correspond with that description. Creationists also view the preclusion of all supernatural explanations within the sciences as a doctrinaire commitment to exclude the supreme being and miracles. They claim this to be the motivating factor in science's acceptance of Darwinism, a term used in creation science to refer to evolutionary biology which is also often used as a disparagement. Critics argue that creation science is religious rather than scientific because it stems from faith in a religious text rather than by the application of the scientific method. The United States National Academy of Sciences (NAS) has stated unequivocally, "Evolution pervades all biological phenomena. To ignore that it occurred or to classify it as a form of dogma is to deprive the student of the most fundamental organizational concept in the biological sciences. No other biological concept has been more extensively tested and more thoroughly corroborated than the evolutionary history of organisms." Anthropologist Eugenie Scott has noted further, "Religious opposition to evolution propels antievolutionism. Although antievolutionists pay lip service to supposed scientific problems with evolution, what motivates them to battle its teaching is apprehension over the implications of evolution for religion."
Creation science advocates argue that scientific theories of the origins of the Universe, Earth, and life are rooted in "a priori" presumptions of methodological naturalism and uniformitarianism, each of which is disputed. In some areas of science such as chemistry, meteorology or medicine, creation science proponents do not challenge the application of naturalistic or uniformitarian assumptions. Traditionally, creation science advocates have singled out those scientific theories judged to be in conflict with held religious beliefs, and it is against those theories that they concentrate their efforts.
Religious criticism.
Some mainstream Christian churches criticize creation science on theological grounds, asserting either that religious faith alone should be a sufficient basis for belief in the truth of creation, or that efforts to prove the Genesis account of creation on scientific grounds are inherently futile because reason is subordinate to faith and cannot thus be used to prove it.
Many Christian theologies, including Liberal Christianity, consider the Genesis creation narrative to be a poetic and allegorical work rather than a literal history, and many Christian churchesâincluding the Roman Catholic, Anglican and the more liberal denominations of the Lutheran, Methodist, Congregationalist and Presbyterian faithsâhave either rejected creation science outright or are ambivalent to it. Belief in non-literal interpretations of Genesis is often cited as going back to Saint Augustine.
Theistic evolution and evolutionary creationism are theologies that reconcile belief in a creator with biological evolution. Each holds the view that there is a creator but that this creator has employed the natural force of evolution to unfold a divine plan. Religious representatives from faiths compatible with theistic evolution and evolutionary creationism have challenged the growing perception that belief in a creator is inconsistent with the acceptance of evolutionary theory. Spokespersons from the Catholic Church have specifically criticized biblical creationism for relying upon literal interpretations of biblical scripture as the basis for determining scientific fact.
Scientific criticism.
The National Academy of Sciences states that "the claims of creation science lack empirical support and cannot be meaningfully tested" and that "creation science is in fact not science and should not be presented as such in science classes." According to Joyce Arthur writing for "Skeptic" magazine, the "creation 'science' movement gains much of its strength through the use of distortion and scientifically unethical tactics" and "seriously misrepresents the theory of evolution."
Scientists have considered the hypotheses proposed by creation science and have rejected them because of a lack of evidence. Furthermore, the claims of creation science do not refer to natural causes and cannot be subject to meaningful tests, so they do not qualify as scientific hypotheses. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. Most mainline Christian denominations have concluded that the concept of evolution is not at odds with their descriptions of creation and human origins.
A summary of the objections to creation science by scientists follows:
By invoking claims of "abrupt appearance" of species as a miraculous act, creation science is unsuited for the tools and methods demanded by science, and it cannot be considered scientific in the way that the term "science" is currently defined. Scientists and science writers commonly characterize creation science as a pseudoscience.
Historical, philosophical, and sociological criticism.
Historically, the debate of whether creationism is compatible with science can be traced back to 1874, the year science historian John William Draper published his "History of the Conflict between Religion and Science". In it Draper portrayed the entire history of scientific development as a war against religion. This presentation of history was propagated further by followers such as Andrew Dickson White in his two-volume "A History of the Warfare of Science with Theology in Christendom" (1896). Their conclusions have been disputed.
In the United States, the principal focus of creation science advocates is on the government-supported public school systems, which are prohibited by the Establishment Clause from promoting specific religions. Historical communities have argued that Biblical translations contain many translation errors and errata, and therefore that the use of biblical literalism in creation science is self-contradictory.
Areas of study.
Subjects within creation science correspond to the scientific disciplines of biology, earth sciences and astronomy.
Creationist biology.
Creationist biology centers on an idea derived from Genesis that states that life was created by God, in a finite number of "created kinds," rather than through biological evolution from a common ancestor. Creationists consider that any observable speciation descends from these distinctly created kinds through inbreeding, deleterious mutations and other genetic mechanisms. Whereas evolutionary biologists and creationists share similar views of microevolution, creationists disagree that the process of macroevolution can explain common ancestry among organisms far beyond the level of common species. Creationists contend that there is no empirical evidence for new plant or animal species, and deny fossil evidence has ever been found documenting the process.
Popular arguments against evolution have changed since the publishing of Henry M. Morris' first book on the subject, "Scientific Creationism" (1974), but some consistent themes remain: that missing links or gaps in the fossil record are proof against evolution; that the increased complexity of organisms over time through evolution is not possible due to the law of increasing entropy; that it is impossible that the mechanism of natural selection could account for common ancestry; and that evolutionary theory is untestable. The origin of the human species is particularly hotly contested; the fossil remains of purported hominid ancestors are not considered by advocates of creation biology to be evidence for a speciation event involving "Homo sapiens". Creationists also assert that early hominids, are either apes, or humans.
Richard Dawkins has explained evolution as "a theory of gradual, incremental change over millions of years, which starts with something very simple and works up along slow, gradual gradients to greater complexity," and described the existing fossil record as entirely consistent with that process. Biologists emphasize that transitional gaps between those fossils recovered are to be expected, that the existence of any such gaps cannot be invoked to disprove evolution, and that instead the fossil evidence that could be used to disprove the theory would be those fossils which are found and which are entirely inconsistent with what can be predicted or anticipated by the evolutionary model. One example given by Dawkins was, "If there were a single hippo or rabbit in the Precambrian, that would completely blow evolution out of the water. None have ever been found."
Earth sciences and geophysics.
Flood geology.
Flood geology is a concept based on the belief that most of Earth's geological record was formed by the Great Flood described in the story of Noah's Ark. Fossils and fossil fuels are believed to have formed from animal and plant matter which was buried rapidly during this flood, while submarine canyons are explained as having formed during a rapid runoff from the continents at the end of the flood. Sedimentary strata are also claimed to have been predominantly laid down during or after Noah's flood and orogeny. Flood geology is a variant of catastrophism and is contrasted with geological science in that it rejects standard geological principles such as uniformitarianism and radiometric dating. For example, the Creation Research Society argues that "uniformitarianism is wishful thinking."
Geologists conclude that no evidence for such a flood is observed in the preserved rock layers and moreover that such a flood is physically impossible, given the current layout of land masses. For instance, since Mount Everest currently is approximately 8.8 kilometres in elevation and the Earth's surface area is 510,065,600Â km2, the volume of water required to cover Mount Everest to a depth of 15 cubits (6.8 m), as indicated by Genesis 7:20, would be 4.6 billion cubic kilometres. Measurements of the amount of precipitable water vapor in the atmosphere have yielded results indicating that condensing all water vapor in a column of atmosphere would produce liquid water with a depth ranging between zero and approximately 70mm, depending on the date and the location of the column. Nevertheless, there continue to be many adherents to flood geology, and in recent years new theories have been introduced such as catastrophic plate tectonics and catastrophic orogeny.
Radiometric dating.
Creationists point to experiments they have performed, which they claim demonstrate that 1.5 billion years of nuclear decay took place over a short period of time, from which they infer that "billion-fold speed-ups of nuclear decay" have occurred, a massive violation of the principle that radioisotope decay rates are constant, a core principle underlying nuclear physics generally, and radiometric dating in particular.
The scientific community points to numerous flaws in the creationists' experiments, to the fact that their results have not been accepted for publication by any peer-reviewed scientific journal, and to the fact that the creationist scientists conducting them were untrained in experimental geochronology.
The constancy of the decay rates of isotopes is well supported in science. Evidence for this constancy includes the correspondences of date estimates taken from different radioactive isotopes as well as correspondences with non-radiometric dating techniques such as dendrochronology, ice core dating, and historical records. Although scientists have noted slight increases in the decay rate for isotopes subject to extreme pressures, those differences were too small to significantly impact date estimates. The constancy of the decay rates is also governed by first principles in quantum mechanics, wherein any deviation in the rate would require a change in the fundamental constants. According to these principles, a change in the fundamental constants could not influence different elements uniformly, and a comparison between each of the elements' resulting unique chronological timescales would then give inconsistent time estimates.
In refutation of young Earth claims of inconstant decay rates affecting the reliability of radiometric dating, Roger C. Wiens, a physicist specializing in isotope dating states:
Radiohaloes.
In the 1970s, young Earth creationist Robert V. Gentry proposed that radiohaloes in certain granites represented evidence for the Earth being created instantaneously rather than gradually. This idea has been criticized by physicists and geologists on many grounds including that the rocks Gentry studied were not primordial and that the radionuclides in question need not have been in the rocks initially.
Thomas A. Baillieul, a geologist and retired senior environmental scientist with the United States Department of Energy, disputed Gentry's claims in an article entitled, "'Polonium Haloes' Refuted: A Review of 'Radioactive Halos in a Radio-Chronological and Cosmological Perspective' by Robert V. Gentry." Baillieul noted that Gentry was a physicist with no background in geology and given the absence of this background, Gentry had misrepresented the geological context from which the specimens were collected. Additionally, he noted that Gentry relied on research from the beginning of the 20th century, long before radioisotopes were thoroughly understood; that his assumption that a polonium isotope caused the rings was speculative; and that Gentry falsely argued that the half-life of radioactive elements varies with time. Gentry claimed that Baillieul could not publish his criticisms in a reputable scientific journal, although some of Baillieul's criticisms rested on work previously published in reputable scientific journals.
Astronomy and cosmology.
Creationist cosmologies.
Several attempts have been made by creationists to construct a cosmology consistent with a young Universe rather than the standard cosmological age of the universe, based on the belief that Genesis describes the creation of the Universe as well as the Earth. The primary challenge for young-universe cosmologies is that the accepted distances in the Universe require millions or billions of years for light to travel to Earth (the "starlight problem"). An older creationist idea, proposed by creationist astronomer Barry Setterfield, is that the speed of light has decayed in the history of the Universe. More recently, creationist physicist Russell Humphreys has proposed a hypothesis called "white hole cosmology" which suggests that the Universe expanded out of a white hole less than 10,000 years ago; the apparent age of the universe results from relativistic effects. Humphreys' theory is advocated by creationist organisations such as Answers in Genesis; however because the predictions of Humphreys' cosmology conflict with current observations, it is not accepted by the scientific community.
Planetology.
Various claims are made by creationists concerning alleged evidence that the age of the Solar System is of the order of thousands of years, in contrast to the scientifically accepted age of 4.6 billion years. It is commonly argued that the number of comets in the Solar System is much higher than would be expected given its supposed age. Creationist astronomers express scepticism about the existence of the Kuiper belt and Oort cloud. Creationists also argue that the recession of the Moon from the Earth is incompatible with either the Moon or the Earth being billions of years old. These claims have been refuted by planetologists.
In response to increasing evidence suggesting that Mars once possessed a wetter climate, some creationists have proposed that the global flood affected not only the Earth but also Mars and other planets. People who support this claim include creationist astronomer Wayne Spencer and Russell Humphreys.
An ongoing problem for creationists is the presence of impact craters on nearly all Solar System objects, which is consistent with scientific explanations of solar system origins but creates insuperable problems for young Earth claims. Creationists Harold Slusher and Richard Mandock, along with Glenn Morton (who later repudiated this claim) asserted that impact craters on the Moon are subject to rock flow, and so cannot be more than a few thousand years old. While some creationist astronomers assert that different phases of meteoritic bombardment of the Solar System occurred during creation week and during the subsequent Great Flood, others regard this as unsupported by the evidence and call for further research.
External links.
Notable creationist museums in the United States:

</doc>
<doc id="7685" url="https://en.wikipedia.org/wiki?curid=7685" title="List of cartographers">
List of cartographers

Cartography is the study of map making and cartographers are map makers.

</doc>
<doc id="7689" url="https://en.wikipedia.org/wiki?curid=7689" title="Cirth">
Cirth

The Cirth (; plural of certh , in Sindarin meaning runes) are a semi-artificial script, with letters shaped on those of actual runic alphabets, invented by J.R.R. Tolkien for the constructed languages he devised and used in his works. "Cirth" is written with a capital letter when referring to the writing system; the runes themselves can be called "cirth".
In the fictional history of Middle-earth, the original Certhas Daeron was created by the elf Daeron, and was later expanded into what was known as the Angerthas Daeron. Although the Cirth were later largely replaced by the Tengwar, they were adopted by Dwarves to write down their Khuzdul language (Angerthas Moria and Angerthas Erebor) because their straight lines were better suited to carving than the curved strokes of the Tengwar. Cirth was also adapted, in its oldest and simplest form, by various peoples as Men and even Orcs.
Earliest Cirth.
During the Chaining of Melkor, the Sindar of Beleriand began developing an alphabet for their language. Its letters were entirely made for carving on wood, stone or metal, hence their angular forms and straight lines. These letters were named "cirth" (sing. "certh"). The corresponding Quenya words are "certar" () and "certa" (). The assignment of values was unsystematic. The form of a certh consisted of a stem and a branch. The attachment of the branch was, if on one side only, usually made on the right side. The reverse was not infrequent, but had no phonetic significance.
Notes.
The known ancient cirth donât cover all the sounds of Sindarin, since we are missing "rh", "lh", "mh", "y", "Å". Perhaps they were used for the Old Sindarin tongue, and many of the above-mentioned sounds indeed didnât exist in that language. However still frequent sounds "w" and "a" are missing. This indicates that some ancient, unknown cirth could have existed, but didnât make it to the later systems; a fuller table therefore can't be reconstructed.<br>As for the vowel usage, perhaps the certh for "u" possibly was used for "w" (like in early Latin orthography). The certh for "a" canât be guessed, so maybe this sound was meant (like in some Tengwar Modes for Quenya). More possibly it was one of some other cirth that did not survive.<br>Long vowels were evidently indicated by doubling.<br>In its earliest form, the Cirth became known to many peoples of Middle-earth like Men, Dwarves or Orcs. The people of Dale and the Rohirrim maintained a simple form of these characters.
Certhas Daeron.
The elf Daeron, minstrel of king Thingol of Doriath reorganised the cirth and added new ones, being somehow inspired by FÃ«anor's Tengwar (therefore this mustn't have occurred before the return of the Noldor) and made the extension of the cirth known as Certhas Daeron (where "Certhas" means "runic alphabet"), used for inscribing names in Menegroth. The Dwarves working for Thingol liked them and adopted them, making them known also in the East.
Angerthas Daeron.
Daeron's alphabet was originally used by the Grey Elves (Sindar) in Beleriand. Later the Noldor in Eregion adopted the Cirth, added several more runes to the system and created the Angerthas Daeron (where "Angerthas" means "long rune-rows") sometimes also referred to as Angerthas Eregion. These additional letters were used to represent sounds not found in Sindarin, but present in the tongues of other peoples. The Angerthas Daeron was used primarily for carved inscriptions. For most other forms of written communication the Tengwar were used.<br>Here the Cirth are grouped according to their phonetic features:
Notes.
For the transliteration of this alphabet, meant to be used for more than one language (for Quenya and Sindarin, at least) and needing a bigger set of sounds, Tolkien thought up to a kind of "general Middle-earth languages phonetic transcription", here used.
Angerthas Moria.
Dwarves first came to know the runes of the Noldor during the beginning of the Second Age. They modified them to suit the specific needs of their language, Khuzdul. The Dwarves spread their revised alphabet to Moria, where it came to be known as Angerthas Moria. The Dwarves developed both carved and pen-written forms of the runes. Travelling for trading, they spread their alphabet throughout Middle-earth: as a result, variations of Angerthas Moria were employed by other races for their languages.
Many cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star (â).
Notes.
[[File:Balin zg2.PNG|thumb|right|300px|Runes in the upper inscription of Balin's tomb use Angerthas Moria, reading left-to-right:<br>
"Balin<br>
âul<br>
UzbadâKÊ°azaddÃ»mu"]]
Angerthas Erebor.
According to Tolkien's legendarium, after the Second Age, the Cirth were obsoleted by the Tengwar among the western races and remained in use only by Dwarves and Men. The Dwarves developed even pen-written cursive forms, since they used them exclusively in any form of writing communication, even in paper. At the beginning of the Third Age, the Dwarves were driven out of Moria. Some migrated to the Grey Mountains, some to the Iron Hills and ThrÃ¡in I came to Erebor. There he founded his Dwarf-kingdom. There the "Angerthas Moria" was modified further and some new cirth were added, but some reverted to their Elvish usage, thus creating the Angerthas Erebor variation. This mode was used in Westron by Dwarves.
Many cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star (â).
Combining diacritics occur in Angerthas Erebor as well: a circumflex accent used to denote long consonants, a macron below to indicate a long vowel sound, and an underdot to mark cirth used as numerals1.
Use for English.
Tolkien used Angerthas Erebor mode to write English with Cirth at least twice in "The Lord of the Rings":
The Book of Mazarbul shows that additional cirth were introduced in this mode (for a double "l" ligature, for the definite article and for the representation of six diphthongs). These were probably only to be used with English language:
Additionally, is used for English and for .
Other Middle-earth Runes.
The Cirth is not the only runic writing system devised by Tolkien for Middle Earth. In fact, he invented a great number of runic alphabets, of which only a few others have been published. Many of them were included in the "Appendix on Runes" in The History of Middle-Earth, vol. VII, The Treason of Isengard, edited by Christopher Tolkien.
Runes from "The Hobbit".
It is speculated that the runes used in "The Hobbit" are indeed the form of Cirth used by the Men of Dale, although Tolkien himself wrote that the letters used for Thror's Map are a form of our ancient runes used to transliterate actual Dwarf-runes.
These runes â used only to write in English â are indeed nearly identical to those of FuÃ¾orc but their sound may change according to their position, just as the Latin letters do. And, in fact, Tolkien's writing mode is mainly orthographic.<br>It has one rune for each letter, regardless of pronunciation (for example the rune "C" can sound in the word ""c"at" or in the word ""c"ellar" or even in the word "deli"c"ious" and in the digraph "CH").<br>A few sounds are instead written with the same rune regardless of the letter (e.g. the sound is always written with the rune "O" either if in English it is written "o" as in "n"o"rth", "a" as in "f"a"ll", or "oo" as in "d"oo"r"). The letters that are subject to this phonemic spelling are "a" and "o".<br>In addition, there are also some runes which stand for a particular English digraph or diphthong.
Here the runes used in "The Hobbit" are represented along with their English transliteration:
Two other runes, not attested in "The Hobbit", were added by Tolkien in order to represent additional English sounds:
Notes.
It must be noticed that Tolkien always wrote the English digraph "wh" (representing the sound , or , like in ""wh"ain") in runes as "HW".<br>There is no rune to transliterate "q": the digraph "qu" (representing the sound , like in ""qu"estion") is always rendered in runes as "CW".
This table could be helpful for the transcription of "a" and "o" in runes:
Gondolinic Runes.
Not all the runes mentioned in "The Hobbit" are Dwarf-runes. The swords found in the Trolls' cave (which were from the ancient kingdom of Gondolin) bore runes that Gandalf could not read. In fact, being the swords Glamdring and Orcrist forged in Gondolin, they bore a type of letters known as Gondolinic runes. They seem to have been obsoleted and forgotten by the Third Age, and this is supported by the fact that only Elrond could read the inscriptions of the swords.<br>
Tolkien devised this runic alphabet in a very early stage of his shaping of Middle-earth, but they are known to us only from a slip of paper written by J.R.R. Tolkien, a photocopy of which Christopher Tolkien sent to Paul Nolan Hyde in February 1992, who published it, together with an extensive analysis, in the 1992 Summer issue of Mythlore, no. 69.
The system provides sounds not found in the known Elven languages of the First Age, but perhaps it was designed for notating a variety of languages. However, the consonants seem to be, more or less, the same found in Welsh phonology, a theory supported by the fact that Tolkien was heavily influenced by Welsh when creating Elven languages.
Concept and creation.
Many letters have shapes also found in the historical FuÃ¾ark, but their sound values are only similar in a few of the vowels. Rather, the system of assignment of sound values is much more systematic in the Cirth than in the historical runes (e.g., voiced variants of a voiceless sound are expressed by an additional stroke). A similar system has been proposed for a few historical runes but is in any case much more obscure.
The division between the older Cirth of Daeron and their adaptation by Dwarves and Men has been interpreted as a parallel drawn by Tolkien to the development of the FuÃ¾orc to the Younger FuÃ¾ark. The original Elvish Cirth "as supposed products of a superior culture" are focused on logical arrangement and a close connection between form and value whereas the adaptations by mortal races introduced irregularities. Similar to the Germanic tribes who had no written literature and used only simple runes before their conversion to Christianity, the Sindar Elves of Beleriand with their Cirth were introduced to the more elaborate Tengwar of FÃ«anor when the Noldor Elves returned to Middle-earth from the lands of the divine Valar.
Unicode.
Equivalents for most but not all cirth can be found in the Runic block of Unicode.
Three J. R. R. Tolkien-specific letters were added in June, 2014 with the release of Unicode 7.0:
A formal Unicode proposal to encode Cirth as a separate script was made in September 1997 by Michael Everson.
No action was taken by the Unicode Technical Committee (UTC) but Cirth appears in the Roadmap to the SMP.
ConScript Unicode Registry.
Unicode Private Use Area layouts for Cirth are defined at the ConScript Unicode Registry (CSUR) and the Under-ConScript Unicode Registry (UCSUR).
Two different layouts are defined by the CSUR/UCSUR:
Without proper rendering support, you may see question marks, boxes, or other symbols below instead of Cirth.

</doc>
<doc id="7697" url="https://en.wikipedia.org/wiki?curid=7697" title="Lockheed C-130 Hercules">
Lockheed C-130 Hercules

The Lockheed C-130 Hercules is a four-engine turboprop military transport aircraft designed and built originally by Lockheed, now Lockheed Martin. Capable of using unprepared runways for takeoffs and landings, the C-130 was originally designed as a troop, medivac, and cargo transport aircraft. The versatile airframe has found uses in a variety of other roles, including as a gunship (AC-130), for airborne assault, search and rescue, scientific research support, weather reconnaissance, aerial refueling, maritime patrol, and aerial firefighting. It is now the main tactical airlifter for many military forces worldwide. Over forty models and variants of the Hercules, including a civilian one marketed as Lockheed L-100, operate in more than sixty nations.
The C-130 entered service with the U.S. in the 1950s, followed by Australia and others. During its years of service, the Hercules family has participated in numerous military, civilian and humanitarian aid operations. In 2007, the C-130 became the fifth aircraftâafter the English Electric Canberra, B-52 Stratofortress, Tu-95, and KC-135 Stratotankerâto mark 50 years of continuous service with its original primary customer, in this case, the United States Air Force. The C-130 Hercules is the longest continuously produced military aircraft at over 60 years, with the updated Lockheed Martin C-130J Super Hercules being produced today.
Design and development.
Background and requirements.
The Korean War, which began in June 1950, showed that World War II-era piston-engine transportsâFairchild C-119 Flying Boxcars, Douglas C-47 Skytrains and Curtiss C-46 Commandosâwere inadequate for modern warfare. Thus, on 2 February 1951, the United States Air Force issued a General Operating Requirement (GOR) for a new transport to Boeing, Douglas, Fairchild, Lockheed, Martin, Chase Aircraft, North American, Northrop, and Airlifts Inc. The new transport would have a capacity of 92 passengers, 72 combat troops or 64 paratroopers in a cargo compartment that was approximately long, high, and wide. Unlike transports derived from passenger airliners, it was to be designed from the ground-up as a combat transport with loading from a hinged loading ramp at the rear of the fuselage.
A key feature was the introduction of the Allison T56 turboprop powerplant, first developed specifically for the C-130. At the time, the turboprop was a new application of turbine engines that used exhaust gases to turn a propeller, which offered greater range at propeller-driven speeds compared to pure turbojets, which were faster but consumed more fuel. As was the case on helicopters of that era, such as the UH-1 Huey, turboshafts produced much more power for their weight than piston engines. Lockheed would subsequently use the same engines and technology in the Lockheed L-188 Electra. That aircraft failed financially in its civilian configuration but was successfully adapted into the Lockheed P-3 Orion maritime patrol and submarine attack aircraft where the efficiency and endurance of turboprops excelled.
Design phase.
The Hercules resembled a larger four-engine brother to the C-123 Provider with a similar wing and cargo ramp layout that evolved from the Chase XCG-20 Avitruc, which in turn, was first designed and flown as a cargo glider in 1947. The Boeing C-97 Stratofreighter also had a rear ramp, which made it possible to drive vehicles onto the plane (also possible with forward ramp on a C-124). The ramp on the Hercules was also used to airdrop cargo, which included low-altitude extraction for Sheridan tanks and even dropping large improvised "daisy cutter" bombs.
The new Lockheed cargo plane design possessed a range of , takeoff capability from short and unprepared strips, and the ability to fly with one engine shut down. Fairchild, North American, Martin, and Northrop declined to participate. The remaining five companies tendered a total of ten designs: Lockheed two, Boeing one, Chase three, Douglas three, and Airlifts Inc. one. The contest was a close affair between the lighter of the two Lockheed (preliminary project designation L-206) proposals and a four-turboprop Douglas design.
The Lockheed design team was led by Willis Hawkins, starting with a 130-page proposal for the "Lockheed L-206". Hall Hibbard, Lockheed vice president and chief engineer, saw the proposal and directed it to Kelly Johnson, who did not care for the low-speed, unarmed aircraft, and remarked, "If you sign that letter, you will destroy the Lockheed Company." Both Hibbard and Johnson signed the proposal and the company won the contract for the now-designated Model 82 on 2 July 1951.
The first flight of the "YC-130" prototype was made on 23 August 1954 from the Lockheed plant in Burbank, California. The aircraft, serial number "53-3397", was the second prototype, but the first of the two to fly. The YC-130 was piloted by Stanley Beltz and Roy Wimmer on its 61-minute flight to Edwards Air Force Base; Jack Real and Dick Stanton served as flight engineers. Kelly Johnson flew chase in a Lockheed P2V Neptune.
After the two prototypes were completed, production began in Marietta, Georgia, where over 2,300 C-130s have been built through 2009.
The initial production model, the "C-130A", was powered by Allison T56-A-9 turboprops with three-blade propellers and originally equipped with the blunt nose of the prototypes. Deliveries began in December 1956, continuing until the introduction of the "C-130B" model in 1959. Some A-models were equipped with skis and re-designated "C-130D". As the C-130A became operational with Tactical Air Command (TAC), the C-130's lack of range became apparent and additional fuel capacity was added in the form of external pylon-mounted tanks at the end of the wings.
Improved versions.
The C-130B model was developed to complement the A-models that had previously been delivered, and incorporated new features, particularly increased fuel capacity in the form of auxiliary tanks built into the center wing section and an AC electrical system. Four-bladed Hamilton Standard propellers replaced the Aeroproducts three-blade propellers that distinguished the earlier A-models. The C-130B had ailerons with increased boostâ3,000Â psi (21Â MPa) versus 2,050Â psi (14Â MPa)âas well as uprated engines and four-blade propellers that were standard until the J-model's introduction.
An electronic reconnaissance variant of the C-130B was designated C-130B-II. A total of 13 aircraft were converted. The C-130B-II was distinguished by its false external wing fuel tanks, which were disguised signals intelligence (SIGINT) receiver antennas. These pods were slightly larger than the standard wing tanks found on other C-130Bs. Most aircraft featured a swept blade antenna on the upper fuselage, as well as extra wire antennas between the vertical fin and upper fuselage not found on other C-130s. Radio call numbers on the tail of these aircraft were regularly changed so as to confuse observers and disguise their true mission.
The extended-range "C-130E" model entered service in 1962 after it was developed as an interim long-range transport for the Military Air Transport Service. Essentially a B-model, the new designation was the result of the installation of 1,360Â US gal (5,150Â L) "Sargent Fletcher" external fuel tanks under each wing's midsection and more powerful Allison T56-A-7A turboprops. The hydraulic boost pressure to the ailerons was reduced back to 2050 psi as a consequence of the external tanks' weight in the middle of the wingspan. The E model also featured structural improvements, avionics upgrades and a higher gross weight. Australia took delivery of 12 C130E Hercules during 1966â67 to supplement the 12 C-130A models already in service with the RAAF. Sweden and Spain fly the TP-84T version of the C-130E fitted for aerial refueling capability.
The "KC-130" tankers, originally "C-130F" procured for the US Marine Corps (USMC) in 1958 (under the designation "GV-1") are equipped with a removable 3,600Â US gal (13,626Â L) stainless steel fuel tank carried inside the cargo compartment. The two wing-mounted hose and drogue aerial refueling pods each transfer up to 300Â US gal per minute (19Â L per second) to two aircraft simultaneously, allowing for rapid cycle times of multiple-receiver aircraft formations, (a typical tanker formation of four aircraft in less than 30 minutes). The US Navy's "C-130G" has increased structural strength allowing higher gross weight operation.
More improvements.
The "C-130H" model has updated Allison T56-A-15 turboprops, a redesigned outer wing, updated avionics and other minor improvements. Later "H" models had a new, fatigue-life-improved, center wing that was retrofitted to many earlier H-models. For structural reasons, some models are required to land with certain amounts of fuel when carrying heavy cargo, reducing usable range. The H model remains in widespread use with the United States Air Force (USAF) and many foreign air forces. Initial deliveries began in 1964 (to the RNZAF), remaining in production until 1996. An improved C-130H was introduced in 1974, with Australia purchasing 12 of type in 1978 to replace the original 12 C-130A models, which had first entered RAAF Service in 1958.
The United States Coast Guard employs the HC-130H for long-range search and rescue, drug interdiction, illegal migrant patrols, homeland security, and logistics.
C-130H models produced from 1992 to 1996 were designated as C-130H3 by the USAF. The "3" denoting the third variation in design for the H series. Improvements included ring laser gyros for the INUs, GPS receivers, a partial glass cockpit (ADI and HSI instruments), a more capable APN-241 color radar, night vision device compatible instrument lighting, and an integrated radar and missile warning system. The electrical system upgrade included Generator Control Units (GCU) and Bus Switching units (BSU)to provide stable power to the more sensitive upgraded components.
The equivalent model for export to the UK is the "C-130K", known by the Royal Air Force (RAF) as the "Hercules C.1". The "C-130H-30" ("Hercules C.3" in RAF service) is a stretched version of the original Hercules, achieved by inserting a 100Â in (2.54Â m) plug aft of the cockpit and an 80Â in (2.03Â m) plug at the rear of the fuselage. A single C-130K was purchased by the Met Office for use by its Meteorological Research Flight, where it was classified as the "Hercules W.2". This aircraft was heavily modified (with its most prominent feature being the long red and white striped atmospheric probe on the nose and the move of the weather radar into a pod above the forward fuselage). This aircraft, named "Snoopy", was withdrawn in 2001 and was then modified by Marshall of Cambridge Aerospace as flight-testbed for the A400M turbine engine, the TP400. The C-130K is used by the RAF Falcons for parachute drops. Three C-130K (Hercules C Mk.1P) were upgraded and sold to the Austrian Air Force in 2002.
Later models.
The "MC-130E Combat Talon" was developed for the USAF during the Vietnam War to support special operations missions in Southeast Asia, and led to both the "MC-130H Combat Talon II" as well as a family of other special missions aircraft. 37 of the earliest models currently operating with the Air Force Special Operations Command (AFSOC) are scheduled to be replaced by new-production MC-130J versions. The EC-130 Commando Solo is another special missions variant within AFSOC, albeit operated solely by an AFSOC-gained wing in the Pennsylvania Air National Guard, and is a psychological operations/information operations (PSYOP/IO) platform equipped as an aerial radio station and television stations able to transmit messaging over commercial frequencies. Other versions of the EC-130, most notably the EC-130H Compass Call, are also special variants, but are assigned to the Air Combat Command (ACC). The AC-130 gunship was first developed during the Vietnam War to provide close air support and other ground-attack duties.
The "HC-130" is a family of long-range search and rescue variants used by the USAF and the U.S. Coast Guard. Equipped for deep deployment of Pararescuemen (PJs), survival equipment, and (in the case of USAF versions) aerial refueling of combat rescue helicopters, HC-130s are usually the on-scene command aircraft for combat SAR missions (USAF only) and non-combat SAR (USAF and USCG). Early USAF versions were also equipped with the Fulton surface-to-air recovery system, designed to pull a person off the ground using a wire strung from a helium balloon. The John Wayne movie "The Green Berets" features its use. The Fulton system was later removed when aerial refueling of helicopters proved safer and more versatile. The movie "The Perfect Storm" depicts a real life SAR mission involving aerial refueling of a New York Air National Guard HH-60G by a New York Air National Guard HC-130P.
The "C-130R" and "C-130T" are U.S. Navy and USMC models, both equipped with underwing external fuel tanks. The USN C-130T is similar, but has additional avionics improvements. In both models, aircraft are equipped with Allison T56-A-16 engines. The USMC versions are designated "KC-130R" or "KC-130T" when equipped with underwing refueling pods and pylons and are fully night vision system compatible.
The RC-130 is a reconnaissance version. A single example is used by the Islamic Republic of Iran Air Force, the aircraft having originally been sold to the former Imperial Iranian Air Force.
The "Lockheed L-100 (L-382)" is a civilian variant, equivalent to a C-130E model without military equipment. The L-100 also has two stretched versions.
Next generation.
In the 1970s, Lockheed proposed a C-130 variant with turbofan engines rather than turboprops, but the U.S. Air Force preferred the takeoff performance of the existing aircraft. In the 1980s, the C-130 was intended to be replaced by the Advanced Medium STOL Transport project. The project was canceled and the C-130 has remained in production.
Building on lessons learned, Lockheed Martin modified a commercial variant of the C-130 into a High Technology Test Bed (HTTB). This test aircraft set numerous short takeoff and landing performance records and significantly expanded the database for future derivatives of the C-130. Modifications made to the HTTB included extended chord ailerons, a long chord rudder, fast-acting double-slotted trailing edge flaps, a high-camber wing leading edge extension, a larger dorsal fin and dorsal fins, the addition of three spoiler panels to each wing upper surface, a long-stroke main and nose landing gear system, and changes to the flight controls and a change from direct mechanical linkages assisted by hydraulic boost, to fully powered controls, in which the mechanical linkages from the flight station controls operated only the hydraulic control valves of the appropriate boost unit. The HTTB first flew on 19 June 1984, with civil registration of N130X. After demonstrating many new technologies, some of which were applied to the C-130J, the HTTB was lost in a fatal accident on 3 February 1993, at Dobbins Air Reserve Base, in Marietta, Georgia. The crash was attributed to disengagement of the rudder fly-by-wire flight control system, resulting in a total loss of rudder control capability while conducting ground minimum control speed tests (Vmcg). The disengagement was a result of the inadequate design of the rudder's integrated actuator package by its manufacturer; the operator's insufficient system safety review failed to consider the consequences of the inadequate design to all operating regimes. A factor which contributed to the accident was the flight crew's lack of engineering flight test training.
In the 1990s, the improved C-130J Super Hercules was developed by Lockheed (later Lockheed Martin). This model is the newest version and the only model in production. Externally similar to the classic Hercules in general appearance, the J model has new turboprop engines, six-bladed propellers, digital avionics, and other new systems.
Upgrades and changes.
In 2000, Boeing was awarded a contract to develop an Avionics Modernization Program kit for the C-130. The program was beset with delays and cost overruns until project restructuring in 2007. On 2 September 2009, Bloomberg news reported that the planned Avionics Modernization Program (AMP) upgrade to the older C-130s would be dropped to provide more funds for the F-35, CV-22 and airborne tanker replacement programs. However, in June 2010, Department of Defense approved funding for the initial production of the AMP upgrade kits. Under the terms of this agreement, the USAF has cleared Boeing to begin low-rate initial production (LRIP) for the C-130 AMP. A total of 198 aircraft are expected to feature the AMP upgrade. The current cost per aircraft is although Boeing expects that this price will drop to US$7Â million for the 69th aircraft.
An engine enhancement program saving fuel and providing lower temperatures in the T56 engine has been approved, and the US Air Force expects to save $2 billion and extend the fleet life.
Replacement.
In October 2010, the Air Force released a capabilities request for information (CRFI) for the development of a new airlifter to replace the C-130. The new aircraft is to carry a 190 percent greater payload and assume the mission of mounted vertical maneuver (MVM). The greater payload and mission would enable it to carry medium-weight armored vehicles and drop them off at locations without long runways. Various options are being considered, including new or upgraded fixed-wing designs, rotorcraft, tiltrotors, or even an airship. Development could start in 2014, and become operational by 2024. The C-130 fleet of around 450 planes would be replaced by only 250 aircraft. The Air Force had attempted to replace the C-130 in the 1970s through the Advanced Medium STOL Transport project, which resulted in the C-17 Globemaster III that instead replaced the C-141 Starlifter. The Air Force Research Laboratory funded Lockheed and Boeing demonstrators for the Speed Agile concept, which had the goal of making a STOL aircraft that can take off and land at speeds as low as on airfields less than 2,000Â ft (610 m) long and cruise at Mach 0.8-plus. Boeing's design used upper-surface blowing from embedded engines on the inboard wing and blown flaps for circulation control on the outboard wing. Lockheed's design also used blown flaps outboard, but inboard used patented reversing ejector nozzles. Boeing's design completed over 2,000 hours of windtunnel tests in late 2009. It was a 5 percent-scale model of a narrowbody design with a payload. When the AFRL increased the payload requirement to , they tested a 5% scale model of a widebody design with a take-off gross weight and an "A400M-size" wide cargo box. It would be powered by four IAE V2533 turbofans. In August 2011, the AFRL released pictures of the Lockheed Speed Agile concept demonstrator. A 23% scale model went through wind tunnel tests to demonstrate its hybrid powered lift, which combines a low drag airframe with simple mechanical assembly to reduce weight and better aerodynamics. The model had four engines, including two Williams FJ44 turbofans. On 26 March 2013, Boeing was granted a patent for its swept-wing powered lift aircraft.
As of January 2014, Air Mobility Command, Air Force Materiel Command and the Air Force Research Lab are in the early stages of defining requirements for the C-X next generation airlifter program to replace both the C-130 and C-17. An aircraft would be produced from the early 2030s to the 2040s. If requirements are decided for operating in contested airspace, Air Force procurement of C-130s would end by the end of the decade to not have them serviceable by the 2030s and operated when they can't perform in that environment. Development of the airlifter depends heavily on the Army's "tactical and operational maneuver" plans. Two different cargo planes could still be created to separately perform tactical and strategic missions, but which course to pursue is to be decided before C-17s need to be retired.
Operational history.
Military.
The first production aircraft, C-130As were first delivered beginning in 1956 to the 463d Troop Carrier Wing at Ardmore AFB, Oklahoma and the 314th Troop Carrier Wing at Sewart AFB, Tennessee. Six additional squadrons were assigned to the 322d Air Division in Europe and the 315th Air Division in the Far East. Additional aircraft were modified for electronics intelligence work and assigned to Rhein-Main Air Base, Germany while modified RC-130As were assigned to the Military Air Transport Service (MATS) photo-mapping division.
In 1958, a U.S. reconnaissance C-130A-II of the 7406th Support Squadron was shot down over Armenia by MiG-17s.
Australia became the first non-American force to operate the C-130A Hercules with 12 examples being delivered from late 1958. These aircraft were fitted with AeroProducts three-blade, 15-foot diameter propellers. The Royal Canadian Air Force became another early user with the delivery of four B-models (Canadian designation C-130 Mk I) in October / November 1960.
In 1963, a Hercules achieved and still holds the record for the largest and heaviest aircraft to land on an aircraft carrier. During October and November that year, a USMC KC-130F (BuNo "149798"), loaned to the U.S. Naval Air Test Center, made 29 touch-and-go landings, 21 unarrested full-stop landings and 21 unassisted take-offs on at a number of different weights. The pilot, LT (later RADM) James H. Flatley III, USN, was awarded the Distinguished Flying Cross for his role in this test series. The tests were highly successful, but the idea was considered too risky for routine "Carrier Onboard Delivery" (COD) operations. Instead, the Grumman C-2 Greyhound was developed as a dedicated COD aircraft. The Hercules used in the test, most recently in service with Marine Aerial Refueler Squadron 352 (VMGR-352) until 2005, is now part of the collection of the National Museum of Naval Aviation at NAS Pensacola, Florida.
In 1964, C-130 crews from the 6315th Operations Group at Naha Air Base, Okinawa commenced forward air control (FAC; "Flare") missions over the Ho Chi Minh Trail in Laos supporting USAF strike aircraft. In April 1965 the mission was expanded to North Vietnam where C-130 crews led formations of B-57 bombers on night reconnaissance/strike missions against communist supply routes leading to South Vietnam. In early 1966 Project Blind Bat/Lamplighter was established at Ubon RTAFB, Thailand. After the move to Ubon the mission became a four-engine FAC mission with the C-130 crew searching for targets then calling in strike aircraft. Another little-known C-130 mission flown by Naha-based crews was Operation "Commando Scarf", which involved the delivery of chemicals onto sections of the Ho Chi Minh Trail in Laos that were designed to produce mud and landslides in hopes of making the truck routes impassable.
In November 1964, on the other side of the globe, C-130Es from the 464th Troop Carrier Wing but loaned to 322d Air Division in France, flew one of the most dramatic missions in history in the former Belgian Congo. After communist Simba rebels took white residents of the city of Stanleyville hostage, the U.S. and Belgium developed a joint rescue mission that used the C-130s to airlift and then drop and air-land a force of Belgian paratroopers to rescue the hostages. Two missions were flown, one over Stanleyville and another over Paulis during Thanksgiving weeks. The headline-making mission resulted in the first award of the prestigious MacKay Trophy to C-130 crews.
In the Indo-Pakistani War of 1965, as a desperate measure the transport No. 6 Squadron of the Pakistan Air Force modified its entire small fleet of C-130Bs for use as heavy bombers, capable of carrying up to 20,000Â lb (9,072Â kg) of bombs on pallets. These improvised bombers were used to hit Indian targets such as bridges, heavy artillery positions, tank formations and troop concentrations. Some C-130s even flew with anti-aircraft guns fitted on their ramp, apparently shooting down some 17 aircraft and damaging 16 others.
In October 1968, a C-130Bs from the 463rd Tactical Airlift Wing dropped a pair of M-121 10,000 pound bombs that had been developed for the massive B-36 bomber but had never been used. The U.S. Army and U.S. Air Force resurrected the huge weapons as a means of clearing landing zones for helicopters and in early 1969 the 463rd commenced Commando Vault missions. Although the stated purpose of COMMANDO VAULT was to clear LZs, they were also used on enemy base camps and other targets.
During the late 1960s, the U.S. was eager to get information on Chinese nuclear capabilities. After the failure of the Black Cat Squadron to plant operating sensor pods near the Lop Nur Nuclear Weapons Test Base using a Lockheed U-2, the CIA developed a plan, named "Heavy Tea", to deploy two battery-powered sensor pallets near the base. To deploy the pallets, a Black Bat Squadron crew was trained in the U.S. to fly the C-130 Hercules. The crew of 12, led by Col Sun Pei Zhen, took off from Takhli Royal Thai Air Force Base in an unmarked U.S. Air Force C-130E on 17 May 1969. Flying for six and a half hours at low altitude in the dark, they arrived over the target and the sensor pallets were dropped by parachute near Anxi in Gansu province. After another six and a half hours of low altitude flight, they arrived back at Takhli. The sensors worked and uploaded data to a U.S. intelligence satellite for six months, before their batteries wore out. The Chinese conducted two nuclear tests, on 22 September 1969 and 29 September 1969, during the operating life of the sensor pallets. Another mission to the area was planned as Operation "Golden Whip", but was called off in 1970. It is most likely that the aircraft used on this mission was either C-130E serial number 64-0506 or 64-0507 (cn 382-3990 and 382-3991). These two aircraft were delivered to Air America in 1964. After being returned to the U.S. Air Force sometime between 1966 and 1970, they were assigned the serial numbers of C-130s that had been destroyed in accidents. 64-0506 is now flying as 62-1843, a C-130E that crashed in Vietnam on 20 December 1965 and 64-0507 is now flying as 63-7785, a C-130E that had crashed in Vietnam on 17 June 1966.
The A-model continued in service through the Vietnam War, where the aircraft assigned to the four squadrons at Naha AB, Okinawa and one at Tachikawa Air Base, Japan performed yeoman's service, including operating highly classified special operations missions such as the BLIND BAT FAC/Flare mission and FACT SHEET leaflet mission over Laos and North Vietnam. The A-model was also provided to the South Vietnamese Air Force as part of the Vietnamization program at the end of the war, and equipped three squadrons based at Tan Son Nhut AFB. The last operator in the world is the Honduran Air Force, which is still flying one of five A model Hercules (FAH "558", c/n 3042) as of October 2009. As the Vietnam War wound down, the 463rd Troop Carrier/Tactical Airlift Wing B-models and A-models of the 374th Tactical Airlift Wing were transferred back to the United States where most were assigned to Air Force Reserve and Air National Guard units.
Another prominent role for the B model was with the United States Marine Corps, where Hercules initially designated as GV-1s replaced C-119s. After Air Force C-130Ds proved the type's usefulness in Antarctica, the U.S. Navy purchased a number of B-models equipped with skis that were designated as LC-130s. C-130B-II electronic reconnaissance aircraft were operated under the SUN VALLEY program name primarily from Yokota Air Base, Japan. All reverted to standard C-130B cargo aircraft after their replacement in the reconnaissance role by other aircraft.
The C-130 was also used in the 1976 Entebbe raid in which Israeli commando forces carried a surprise assault to rescue 103 passengers of an airliner hijacked by Palestinian and German terrorists at Entebbe Airport, Uganda. The rescue force â 200 soldiers, jeeps, and a black Mercedes-Benz (intended to resemble Ugandan Dictator Idi Amin's vehicle of state) â was flown over almost entirely at an altitude of less than from Israel to Entebbe by four Israeli Air Force (IAF) Hercules aircraft without mid-air refueling (on the way back, the planes refueled in Nairobi, Kenya).
During the Falklands War () of 1982, Argentine Air Force C-130s undertook highly dangerous, daily re-supply night flights as blockade runners to the Argentine garrison on the Falkland Islands. They also performed daylight maritime survey flights. One was lost during the war. Argentina also operated two KC-130 tankers during the war, and these refueled both the Douglas A-4 Skyhawks and Navy Dassault-Breguet Super Ãtendards; some C-130s were modified to operate as bombers with bomb-racks under their wings. The British also used RAF C-130s to support their logistical operations.
During the Gulf War of 1991 (Operation "Desert Storm"), the C-130 Hercules was used operationally by the U.S. Air Force, U.S. Navy and U.S. Marine Corps, along with the air forces of Australia, New Zealand, Saudi Arabia, South Korea and the UK. The MC-130 Combat Talon variant also made the first attacks using the largest conventional bombs in the world, the BLU-82 "Daisy Cutter" and GBU-43/B "Massive Ordnance Air Blast" bomb, (MOAB). Daisy Cutters were used to clear landing zones and to eliminate mine fields. The weight and size of the weapons make it impossible or impractical to load them on conventional bombers. The GBU-43/B MOAB is a successor to the BLU-82 and can perform the same function, as well as perform strike functions against hardened targets in a low air threat environment.
Since 1992, two successive C-130 aircraft named "Fat Albert" have served as the support aircraft for the U.S. Navy Blue Angels flight demonstration team. "Fat Albert I" was a TC-130G ("151891"), while "Fat Albert II" is a C-130T ("164763"). Although "Fat Albert" supports a Navy squadron, it is operated by the U.S. Marine Corps (USMC) and its crew consists solely of USMC personnel. At some air shows featuring the team, "Fat Albert" takes part, performing flyovers. Until 2009, it also demonstrated its rocket-assisted takeoff (RATO) capabilities; these ended due to dwindling supplies of rockets.
The AC-130 also holds the record for the longest sustained flight by a C-130. From 22 to 24 October 1997, two AC-130U gunships flew 36 hours nonstop from Hurlburt Field Florida to Taegu (Daegu), South Korea while being refueled seven times by KC-135 tanker aircraft. This record flight shattered the previous record longest flight by over 10 hours while the two gunships took on of fuel. The gunship has been used in every major U.S. combat operation since Vietnam, except for Operation "El Dorado Canyon", the 1986 attack on Libya.
During the invasion of Afghanistan in 2001 and the ongoing support of the International Security Assistance Force (Operation "Enduring Freedom"), the C-130 Hercules has been used operationally by Australia, Belgium, Canada, Denmark, France, Italy, the Netherlands, New Zealand, Norway, Portugal, South Korea, Spain, the UK and the United States.
During the 2003 invasion of Iraq (Operation "Iraqi Freedom"), the C-130 Hercules was used operationally by Australia, the UK and the United States. After the initial invasion, C-130 operators as part of the Multinational force in Iraq used their C-130s to support their forces in Iraq.
Since 2004, the Pakistan Air Force has employed C-130s in the War in North-West Pakistan. Some variants had forward looking infrared (FLIR Systems Star Safire III EO/IR) sensor balls, to enable close tracking of Islamist militants.
Civilian.
The U.S. Forest Service developed the Modular Airborne FireFighting System for the C-130 in the 1970s, which allows regular aircraft to be temporarily converted to an airtanker for fighting wildfires. In the late 1980s, 22 retired USAF C-130As were removed from storage at Davis-Monthan Air Force Base and transferred to the U.S. Forest Service who then sold them to six private companies to be converted into air tankers (see U.S. Forest Service airtanker scandal). After one of these aircraft crashed due to wing separation in flight as a result of fatigue stress cracking, the entire fleet of C-130A air tankers was permanently grounded in 2004 (see 2002 airtanker crashes). C-130s have been used to spread chemical dispersants onto the massive oil slick in the Gulf Coast in 2010.
A recent development of a C-130âbased airtanker is the Retardant Aerial Delivery System developed by Coulson Aviation USA. The system consists of a C-130H/Q retrofitted with an in-floor discharge system, combined with a removable 3,500- or 4,000-gallon water tank. The combined system is FAA certified.
Variants.
Significant military variants of the C-130 include:
Accidents.
The C-130 Hercules has had a low accident rate in general. The Royal Air Force recorded an accident rate of about one aircraft loss per 250,000 flying hours over the last 40 years, placing it behind Vickers VC10s and Lockheed TriStars with no flying losses. USAF C-130A/B/E-models had an overall attrition rate of 5% as of 1989 as compared to 1-2% for commercial airliners in the U.S., according to the NTSB, 10% for B-52 bombers, and 20% for fighters (F-4, F-111), trainers (T-37, T-38), and helicopters (H-3).
A total of 70 aircraft were lost by the U.S. Air Force and the U.S. Marine Corps during combat operations in the Vietnam War in Southeast Asia. By the nature of the Hercules' worldwide service, the pattern of losses provides an interesting barometer of the global hot spots over the past 50 years.
References.
Notes
Citations
Bibliography

</doc>
<doc id="7699" url="https://en.wikipedia.org/wiki?curid=7699" title="Commodore 1570">
Commodore 1570

The Commodore 1570 is a 5Â¼" floppy disk drive for the Commodore 128 home/personal computer. It is a single-sided, 170 kB version of the Commodore 1571, released as a stopgap measure when Commodore International was unable to provide large enough quantities of 1571s due to a shortage of double-sided drive mechanisms (supplied from an outside manufacturer). Like the 1571, it can read and write both GCR and MFM disk formats.
The 1570 utilizes a 1571 logic board in a cream-colored original-1541-like case with a drive mechanism similar to the 1541's except that it was equipped with track-zero detection. Like the 1571, its built-in DOS provides a data burst mode for transferring data to the C128 computer at a faster speed than a 1541 can. Its ROM also contains some DOS bug fixes that didn't appear in the 1571 until much later. The 1570 can read and write all single-sided CP/M-format disks that the 1571 can access.
Although the 1570 is compatible with the Commodore 64, the C64 isn't capable of taking advantage of the drive's higher-speed operation, and when used with the C64 it's little more than a pricier 1541. Also, many early buyers of the C128 chose to temporarily make do with a 1541 drive, perhaps owned as part of a previous C64 setup, until the 1571 became more widely available.
The drive uses the CPU MOS 6502, floppy controller WD1770 or WD1772, I/O controllers 2x MOS Technology 6522 and 1x MOS Technology 6526.

</doc>
<doc id="7700" url="https://en.wikipedia.org/wiki?curid=7700" title="Commodore 1571">
Commodore 1571

The Commodore 1571 is Commodore's high-end 5Â¼" floppy disk drive. With its double-sided drive mechanism, it has the ability to use double-sided, double-density (DS/DD) floppy disks natively. This is in contrast to its predecessors, the 1541 and 1570, which can fully read and write such disks only if the user manually flipped them over to access the second side. Because flipping the disk also reverses the direction of rotation, the two methods are not interchangeable; disks which had their back side created in a 1541 by flipping them over would have to be flipped in the 1571 too, and the back side of disks written in a 1571 using the native support for two-sided operation could not be read in a 1541.
Release & features.
The 1571 was released to match the Commodore 128, both design-wise and feature-wise. It was announced in the summer of 1985, at the same time as the C128, and became available in quantity later that year. The later C128"D" had a 1571-compatible drive integrated in the system unit. A double-sided disk on the 1571 would have a capacity of 340 kB (70 tracks, 1,360 disk blocks of 256 bytes each); as 8 kB are reserved for system use (directory and block availability information) and, under of each block serve as pointers to the next logical block, = 337,312 B or about were available for user data. (However, with a program organizing disk storage on its own, all space could be used, e.g. for data disks.)
The 1571 features a "burst mode" when used in conjunction with the C128 (although not when used with the Commodore 64 (without modifying hardware) or VIC-20). This mode replaced the slow bit-banging serial routines of the 1541 with a true serial shift register implemented in hardware, thus dramatically increasing the drive speed. Although this originally had been planned when Commodore first switched from the parallel IEEE-488 interface to a custom serial interface (CBM-488), hardware bugs in the VIC-20's 6522 VIA shift register prevented it from working properly.
For compatibility with copy-protected software, the 1571 could closely emulate the 1541. This mode was the default when the drive was used in conjunction with a C64; while always being able to read and write the 1541's of single-sided, in this mode it also would format disks single-sided and transfer data at 1541 speed. An undocumented command allowed the drive to format and use the second side of a disk, but only in single-sided mode.
The 1571 was noticeably quieter than its predecessor and tended to run cooler as well, even though, like the 1541, it had an internal power supply (later Commodore drives, like the 1541-II and the 3Â½" 1581, came with external power supplies). The 1541-II/1581 power supply makes mention of a 1571-II, hinting that Commodore may have intended to release a version of the 1571 with an external power supply. However, no 1571-IIs are known to exist. The embedded OS in the 1571 was an improvement over the 
Early 1571s had a bug in the ROM-based disk operating system that caused relative files to corrupt if they occupied both sides of the disk. A version 2 ROM was released, but though it cured the initial bug, it introduced some minor quirks of its own - particularly with the 1541 emulation. Curiously, it was also identified as V3.0.
As with the 1541, Commodore initially could not meet demand for the 1571, and that lack of availability and the drive's relatively high price (about US$300) presented an opportunity for cloners. Two 1571 clones appeared, one from Oceanic and one from Blue Chip, but legal action from Commodore quickly drove them from the market.
Commodore announced at the 1985 Consumer Electronics Show a dual-drive version of the 1571, to be called the Commodore 1572, but quickly canceled it, reportedly due to technical difficulties with the 1572 DOS. It would have had four times as much RAM as the 1571 (8 kB), and twice as much ROM (64 kB). The 1572 would have allowed for fast disk backups of non-copy-protected media, much like the old 4040, 8050, and 8250 dual drives.
The 1571 built into the European plastic-case C128 D computer is electronically identical to the stand-alone version, but 1571 version integrated into the later metal-case C128 D (often called C128 DCR, for D Cost-Reduced) differs a lot from the stand-alone 1571. It includes a newer DOS, version 3.1, replaces the MOS Technology CIA interface chip, of which only a few features were used by the 1571 DOS, with a very much simplified chip called 5710, and has some compatibility issues with the stand-alone drive. Because this internal 1571 does not have an unused 8-bit input/output port on any chip, unlike most other Commodore drives, it is not possible to install a parallel cable in this drive, such as that used by SpeedDOS, Dolphin DOS and some other fast third-party Commodore DOS replacements.
Technical design.
The drive detects the motor speed and generates an internal data sampling clock signal that matches with the motor speed.
The 1571 uses a saddle canceler when reading the data stream. A correction signal is generated when the raw data pattern on the disk consists of two consecutive zeros. With the GCR recording format a problem occurs in the read signal waveform. The worst case pattern 1001 may cause a saddle condition where a false data bit may occur. The original 1541 drives uses a one-shot to correct the condition. The 1571 uses a gate array to corrected this digitally.
The drive uses the CPU MOS 6502, floppy controller WD1770 or WD1772, I/O controllers 2x MOS Technology 6522 and 1x MOS Technology 6526.
Disk format.
Unlike the 1541, which was limited to GCR formatting, the 1571 could do both GCR and MFM disk formats. A C128 in CP/M mode equipped with a 1571 was capable of reading and writing floppy disks formatted for many CP/M computers; specifically, the following formats:
Other MFM formats were possible if their characteristics were added to the CP/M C128-specific source code (available from Commodore) and the CP/M operating system were re-assembled. However, booting CP/M was only supported from disks in the standard Commodore GCR format; the MFM formats could only be used once the system was running.
Depending on format, CP/M disks would format to with a mechanical maximum capacity of a format (as with generally). 
With additional software, it was possible to read and write to MS-DOS-formatted floppies as well. Numerous commercial and public-domain programs for this purpose became available, the best-known being SOGWAP's "Big Blue Reader". Although the C128 could not run any DOS-based software, this capability allowed data files to be exchanged with PC users. Reading or disks was possible as well with special software, but the standard format, which used FM rather than MFM encoding, could not be handled by the 1571 hardware without modifying the drive circuitry as the control line that determines if FM or MFM encoding is used by the disc controller chip was permanently wired to ground (MFM mode) rather than being under software control.
In the 1541 format, while 40 tracks are possible for a drive like the 154x/157x, only are used. Commodore chose not to use the upper five tracks by default (or at least to use more than 35) due to the bad quality of some of the drive mechanisms, which did not always work reliably on those tracks. By reducing the number of tracks used (and thus the capacity), Commodore could further reduce cost - in contrast to the double-density drives used e.g. in IBM PCs of the day which saved 180 kB on one side (by using a 40-track format).
For compatibility and ease of implementation, the 1571's double-sided format of one logical disk side with was created by putting together the lower 35 physical tracks on each of the physical sides of the disk rather than using two times even though there were no more quality problems with the mechanisms of the 1571 drives.

</doc>
