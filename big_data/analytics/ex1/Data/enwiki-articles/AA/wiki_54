<doc id="4077" url="https://en.wikipedia.org/wiki?curid=4077" title="Binary prefix">
Binary prefix

A binary prefix is a prefix attached before a unit symbol to multiply it by a power of 2. In computing, such a prefix is seen in combination with a unit of information (bit, byte, etc.), to indicate a power of 1024.
The computer industry has historically used the units "kilobyte", "megabyte", and "gigabyte", and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, "gigabyte" customarily means bytes. As this is the third power of 1024, and 1024 is a power of two (210), this usage is referred to as a binary prefix.
In most other contexts, the industry uses the multipliers "kilo", "mega", "giga", etc., in a manner consistent with their meaning in the International System of Units (SI), namely as powers of 1000. For example, a 500 gigabyte hard disk holds bytes, and a 1 Gbit/s (gigabit-per-second) Ethernet connection transfers data at bit/s. In contrast with the "binary prefix" usage, this use is described as a "decimal prefix", as 1000 is a power of 10 (103).
The use of the same unit prefixes with two different meanings has caused confusion. Starting around 1998, the International Electrotechnical Commission (IEC) and several other standards and trade organizations addressed the ambiguity by publishing standards and recommendations for a set of binary prefixes that refer exclusively to powers of 1024. Accordingly, the US National Institute of Standards and Technology (NIST) requires that SI prefixes only be used in the decimal sense: kilobyte and megabyte denote one thousand bytes and one million bytes respectively (consistent with SI), while new terms such as kibibyte, mebibyte and gibibyte, having the symbols KiB, MiB, and GiB, denote 1024 bytes, bytes, and bytes, respectively. In 2008, the IEC prefixes were incorporated into the IEC 80000-13 standard.
History.
Main memory.
Early computers used one of two addressing methods to access the system memory; binary (base 2) or decimal (base 10).
For example, the IBM 701 (1952) used binary and could address 2048 words of 36 bits each, while the IBM 702 (1953) used decimal and could address ten thousand 7-bit words.
By the mid-1960s, binary addressing had become the standard architecture in most computer designs, and main memory sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.
Early computer system documentation would specify the memory size with an exact number such as 4096, 8192, or 16384 words of storage. These are all powers of two, and furthermore are small multiples of 210, or 1024. As storage capacities increased, several different methods were developed to abbreviate these quantities.
The method most commonly used today uses prefixes such as kilo, mega, giga, and corresponding symbols K, M, and G, which the computer industry originally adopted from the metric system. The prefixes "kilo-" and "mega-", meaning 1000 and respectively, were commonly used in the electronics industry before World War II. 
Along with "giga-" or G-, meaning , they are now known as SI prefixes after the International System of Units (SI), introduced in 1960 to formalize aspects of the metric system. (Note that K is the SI unit for temperature (kelvin) and should not be confused with k, the SI prefix for kilo.)
The International System of Units does not define units for digital information but notes that the SI prefixes may be applied outside the contexts where base units or derived units would be used. But as computer main memory in a 
binary-addressed system is manufactured in sizes that were easily expressed as multiples of 1024, "kilobyte", when applied to computer memory, came to be used to mean 1024 bytes instead of 1000. This usage is not consistent with the SI. Compliance with the SI requires that the prefixes take their 1000-based meaning, and cannot be used as placeholders for other numbers, like 1024.
The use of K in the binary sense as in a "32K core" meaning words, i.e., words, can be found as early as 1959.
Gene Amdahl's seminal 1964 article on IBM System/360 used "1K" to mean 1024.
This style was used by other computer vendors, the CDC 7600 "System Description" (1968) made extensive use of K as 1024.
Thus the first binary prefix was born.
Another style was to truncate the last three digits and append K, essentially using K as a decimal prefix similar to SI, but always truncating to the next lower whole number instead of rounding to the nearest. The exact values words, words and words would then be described as "32K", "65K" and "131K".
This style was used from about 1965 to 1975.
These two styles (K = 1024 and truncation) were used loosely around the same time, sometimes by the same company. In discussions of binary-addressed memories, the exact size was evident from context. (For memory sizes of "41K" and below, there is no difference between the two styles.) The HP 21MX real-time computer (1974) denoted (which is 192×1024) as "196K" and as "1M",
while the HP 3000 business computer (1973) could have "64K", "96K", or "128K" bytes of memory.
The "truncation" method gradually waned. Capitalization of the letter K became the "de facto" standard for binary notation, although this could not be extended to higher powers, and use of the lowercase k did persist. Nevertheless, the practice of using the SI-inspired "kilo" to indicate 1024 was later extended to "megabyte" meaning 10242 () bytes, and later "gigabyte" for 10243 () bytes. For example, a "512 megabyte" RAM module is 512×10242 bytes (512 × , or ), rather than .
The symbols Kbit, Kbyte, Mbit and Mbyte started to be used as "binary units"—"bit" or "byte" with a multiplier that is a power of 1024—in the early 1970s.
For a time, memory capacities were often expressed in K, even when M could have been used: The IBM System/370 Model 158 brochure (1972) had the following: "Real storage capacity is available in 512K increments ranging from 512K to 2,048K bytes."
Megabyte was used to describe the 22-bit addressing of DEC PDP-11/70 (1975)
and gigabyte the 30-bit addressing DEC VAX-11/780 (1977).
In 1998, the International Electrotechnical Commission IEC introduced the binary prefixes kibi, mebi, gibi ... to mean 1024, 10242, 10243 etc., so that 1048576 bytes could be referred to unambiguously as 1 mebibyte. The IEC prefixes were defined for use alongside the International System of Quantities (ISQ) in 2009.
Disk drives.
The disk drive industry followed a different pattern. Industry practice, more thoroughly documented at Timeline of binary prefixes and continuing today, is to specify hard drives using SI prefixes and symbols in their SI or "decimal" interpretation. Unlike binary-addressed computer main memory, there is nothing in a disk drive that influences it to have a total capacity easily expressed using a power of 1024. The first commercially sold disk drive, the IBM 350, had 50 (not 32 or 64) physical disk "platters" containing a total of 50,000 sectors of 100 characters each, for a total quoted capacity of "5 million characters." It was introduced in September 1956.
In the 1960s most disk drives used IBM's variable block length format (called Count Key Data or "CKD").
Any block size could be specified up to the maximum track length. Since the block headers occupied space, the usable capacity of the drive was dependent on the block size. Blocks ("records" in IBM's terminology) of 88, 96, 880 and 960 were often used because they related to the fixed block size of punch cards. The drive capacity was usually stated under conditions of full track record blocking. For example, the 100-megabyte 3336 disk pack only achieved that capacity with a full track block size of 13,030 bytes.
Hard disk drive manufacturers used "megabytes" or "MB", meaning 106 bytes, to characterize their products as early as 1974. By 1977, in its first edition, Disk/Trend, a leading hard disk drive industry marketing consultancy segmented the industry according to MBs (decimal sense) of capacity.
One of the earliest hard disk drives in personal computing history,
the Seagate ST-412, was specified as "Formatted: 10.0 Megabytes". The specification of 4 heads or active surfaces (tracks per cylinder), 306 cylinders and when formatted with a sector size of 256 bytes and 32 sectors/track results in a capacity of . This drive was one of several types installed into the IBM PC/XT and extensively advertised and reported as a "10 MB" (formatted) hard disk drive.
Operating systems and programs using the customary binary prefixes show this as "9.5625 MB".
The hard drive industry continues to use decimal prefixes for drive capacity. Today, for example, a "300 GB" hard drive offers slightly more than 300×109, or , bytes, not (which would be about ). Operating systems such as Microsoft Windows that display hard drive sizes using the customary binary prefix "GB" (as it is used for RAM) would display this as "279.4 GB" (meaning bytes, or ). On the other hand, Mac OS X has since version 10.6 shown hard drive size using decimal prefixes (thus matching the drive makers' packaging). (Previous versions of Mac OS used binary prefixes.)
However, other usages still occur. For example, in one document, Seagate specifies data transfer rates of some of its hard drives in "both" IEC and decimal units. 
"Advanced Format" drives using 4096-byte sectors are described as having "4K sectors."
Information transfer and clock rates.
Like the hard drive, there is nothing in a computer clock circuit or data transfer path that demands or even encourages that things happen at rates easily expressed using powers of 1024, or even using powers of 2.
Computer clock frequencies are always quoted using SI prefixes in their decimal sense. For example, the internal clock frequency of the original IBM PC was 4.77 MHz, that is, . 
Similarly, digital information transfer rates are mostly quoted using decimal prefixes:
Standardization of dual definitions.
By the mid-1970s it was common to see K meaning 1024 and the occasional M meaning for words or bytes of main memory (RAM) while K and M were commonly used with their decimal meaning for disk storage. In the 1980s, as capacities of both types of devices increased, the SI prefix G, with SI meaning, was commonly applied to disk storage, while M in its binary meaning, became common for computer memory. In the 1990s, the prefix G, in its binary meaning, became commonly used for computer memory capacity. The first terabyte (SI prefix, bytes) hard disk drive was introduced in 2007.
The dual usage of the kilo, mega, and giga prefixes and their corresponding symbols K, M, and G as both powers of 1000 and powers of 1024 was recorded in standards and dictionaries. For example, the 1986 ANSI/IEEE Std 1084-1986
defined dual uses for kilo and mega.
Many dictionaries have noted the practice of using traditional prefixes to indicate binary multiples.
Oxford online dictionary defines, for example, megabyte as: "Computing: a unit of information equal to one million or (strictly) bytes."
The units Kbyte, Mbyte, and Gbyte are found in the trade press and in IEEE journals. Gigabyte was formally defined in IEEE Std 610.10-1994 as either or 230 bytes.
Kilobyte, Kbyte, and KB are equivalent units and all are defined in the obsolete standard, IEEE 100-2000.
Byte multiples using powers of 1000 up to yottabyte are given by the on-line computing dictionary FOLDOC (Free On-Line Dictionary of Computing).
The hardware industry has coped with the dual definitions because of relative consistency:
system memory (RAM) typically uses the binary meaning while magnetic disk storage uses the SI meaning.
There are, however, exceptions and special cases.
Diskettes use yet another "megabyte" equal to 1024×1000 bytes.
In optical disks, Compact Disks use MB to mean 10242 bytes while DVDs use GB to mean 10003 bytes.
Inconsistent use of units.
Deviation between powers of 1024 and powers of 1000.
Computer storage has become cheaper per unit and thereby larger, by many orders of magnitude since "K" was first used to mean 1024. 
Because both the SI and "binary" meanings of kilo, mega, etc., are based on powers of 1000 or 1024 rather than simple multiples, the difference between 1M "binary" and 1M "decimal" is proportionally larger than that between 1K "binary" and 1k "decimal," and so on up the scale.
The relative difference between the values in the binary and decimal interpretations increases, when using the SI prefixes as the base, from 2.4% for kilo to nearly 21% for the yotta prefix.
Consumer confusion.
In the early days of computers (roughly, prior to the advent of personal computers) there was little or no consumer confusion because of the technical sophistication of the buyers and their familiarity with the products. In addition, it was common for computer manufacturers to specify their products with capacities in full precision.
In the personal computing era, one source of consumer confusion is the difference in the way many operating systems display hard drive sizes, compared to the way hard drive manufacturers describe them. Hard drives are specified and sold using "GB" and "TB" in their decimal meaning: one billion and one trillion bytes. Many operating systems and other software, however, display hard drive and file sizes using "MB", "GB" or other SI-looking prefixes in their binary sense, just as they do for displays of RAM capacity. For example, many such systems display a hard drive marketed as "160 GB" as "149.05 GB". The earliest known presentation of hard disk drive capacity by an operating system using "KB" or "MB" in a binary sense is 1984; earlier operating systems generally presented the hard disk drive capacity as an exact number of bytes, with no prefix of any sort, for example, in the output of the MS-DOS or PC DOS CHKDSK command.
Legal disputes.
The different interpretations of disk size prefixes has led to three significant class action lawsuits against digital storage manufacturers.
One case involved flash memory and the other two involved hard disk drives.
Two of these were settled with the manufacturers admitting no wrongdoing but agreeing to clarify the storage capacity of their products on the consumer packaging.
Flash memory and hard disk manufacturers now have disclaimers on their packaging and web sites clarifying the formatted capacity of the devices
or defining MB as 1 million bytes and 1 GB as 1 billion bytes.
Willem Vroegh v. Eastman Kodak Company.
On 20 February 2004, Willem Vroegh filed a lawsuit against Lexar Media, Dane–Elec Memory, Fuji Photo Film USA, Eastman Kodak Company, Kingston Technology Company, Inc., Memorex Products, Inc.; PNY Technologies Inc., SanDisk Corporation, Verbatim Corporation, and Viking Interworks alleging that their descriptions of the capacity of their flash memory cards were false and misleading.
Vroegh claimed that a 256 MB Flash Memory Device had only 244 MB of accessible memory. "Plaintiffs allege that Defendants marketed the memory capacity of their products by assuming that one megabyte equals one million bytes and one gigabyte equals one billion bytes."
The plaintiffs wanted the defendants to use the traditional values of 10242 for megabyte and 10243 for gigabyte.
The plaintiffs acknowledged that the IEC and IEEE standards define a MB as one million bytes but stated that the industry has largely ignored the IEC standards.
The manufacturers agreed to clarify the flash memory card capacity on the packaging and web sites. The consumers could apply for "a discount of ten percent off a future online purchase from Defendants' Online Stores Flash Memory Device".
Orin Safier v. Western Digital Corporation.
On 7 July 2005, an action entitled "Orin Safier v. Western Digital Corporation, et al." was filed in the Superior Court for the City and County of San Francisco, Case No. CGC-05-442812.
The case was subsequently moved to the Northern District of California, Case No. 05-03353 BZ.
Although Western Digital maintained that their usage of units is consistent with "the indisputably correct industry standard for measuring and describing storage capacity", and that they "cannot be expected to reform the software industry", they agreed to settle in March 2006 with 14 June 2006 as the Final Approval hearing date.
Western Digital offered to compensate customers with a free download of backup and recovery software valued at US$30. They also paid $500,000 in fees and expenses to San Francisco lawyers Adam Gutride and Seth Safier, who filed the suit.
The settlement called for Western Digital to add a disclaimer to their later packaging and advertising.
Cho v. Seagate Technology (US) Holdings, Inc..
A lawsuit (Cho v. Seagate Technology (US) Holdings, Inc., San Francisco Superior Court, Case No. CGC-06-453195) was filed against Seagate Technology, alleging that Seagate overrepresented the amount of usable storage by 7% on hard drives sold between March 22, 2001 and September 26, 2007. The case was settled without Seagate admitting wrongdoing, but agreeing to supply those purchasers with free backup software or a 5% refund on the cost of the drives.
Unique binary prefixes.
Early suggestions.
While early computer scientists typically used k to mean 1000, some recognized the convenience that would result from working with multiples of 1024 and the confusion that resulted from using the same prefixes for two different meanings.
Several proposals for unique binary prefixes were made in 1968. Donald Morrison proposed to use the Greek letter kappa (κ) to denote 1024, κ2 to denote 1024×1024, and so on.
Wallace Givens responded with a proposal to use bK as an abbreviation for 1024 and bK2 or bK2 for 1024×1024, though he noted that neither the Greek letter nor lowercase letter b would be easy to reproduce on computer printers of the day.
Bruce Alan Martin of Brookhaven National Laboratory further proposed that the prefixes be abandoned altogether, and the letter B be used for base-2 exponents, similar to E in decimal scientific notation, to create shorthands like 3B20 for 3×220, a convention still used on some calculators to present binary floating point-numbers today.
None of these gained much acceptance, and capitalization of the letter K became the "de facto" standard for indicating a factor of 1024 instead of 1000, although this could not be extended to higher powers.
As the discrepancy between the two systems increased in the higher-order powers, more proposals for unique prefixes were made.
In 1996, Markus Kuhn proposed a system with "di" prefixes, like the "dikilobyte" (K₂B or K2B). Donald Knuth, who uses decimal notation like 1 MB = 1000 kB, expressed "astonishment" that the IEC proposal was adopted, calling them "funny-sounding" and opining that proponents were assuming "that standards are automatically adopted just because they are there." Knuth proposed that the powers of 1024 be designated as "large kilobytes" and "large megabytes" (abbreviated KKB and MMB, as "doubling the letter connotes both binary-ness and large-ness"). Double prefixes were already abolished from SI, however, having a multiplicative meaning ("MMB" would be equivalent to "TB"), and this proposed usage never gained any traction.
IEC prefixes.
The set of binary prefixes that were eventually adopted, now referred to as the "IEC prefixes", were first proposed by the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols (IDCNS) in 1995. At that time, it was proposed that the terms kilobyte and megabyte be used only for 103 bytes and 106 bytes, respectively. The new prefixes "kibi" (kilobinary), "mebi" (megabinary), "gibi" (gigabinary) and "tebi" (terabinary) were also proposed at the time, and the proposed symbols for the prefixes were kb, Mb, Gb and Tb respectively, rather than Ki, Mi, Gi and Ti. The proposal was not accepted at the time.
The Institute of Electrical and Electronic Engineers (IEEE) began to collaborate with the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) to find acceptable names for binary prefixes. IEC proposed "kibi", "mebi", "gibi" and "tebi", with the symbols Ki, Mi, Gi and Ti respectively, in 1996.
The names for the new prefixes are derived from the original SI prefixes combined with the term "binary", but contracted, by taking the first two letters of the SI prefix and "bi" from binary. The first letter of each such prefix is therefore identical to the corresponding SI prefixes, except for "K", which is used interchangeably with "k", whereas in SI, only the lower-case k represents 1000.
The IEEE decided that their standards would use the prefixes "kilo", etc. with their metric definitions, but allowed the binary definitions to be used in an interim period as long as such usage was explicitly pointed out on a case-by-case basis.
Adoption by IEC, NIST and ISO.
In January 1999, the IEC published the first international standard (IEC 60027-2 Amendment 2) with the new prefixes, extended up to "pebi" (Pi) and "exbi" (Ei).
The IEC 60027-2 Amendment 2 also states that the IEC position is the same as that of BIPM (the body that regulates the SI system); the SI prefixes retain their definitions in powers of 1000 and are never used to mean a power of 1024.
In usage, products and concepts typically described using powers of 1024 would continue to be, but with the new IEC prefixes. For example, a memory module of bytes () would be referred to as 512 MiB or 512 mebibytes instead of 512 MB or 512 megabytes. Conversely, since hard drives have historically been marketed using the SI convention that "giga" means , a "500 GB" hard drive would still be labeled as such. According to these recommendations, operating systems and other software would also use binary and SI prefixes in the same way, so the purchaser of a "500 GB" hard drive would find the operating system reporting either "500 GB" or "466 GiB", while bytes of RAM would be displayed as "512 MiB".
The second edition of the standard, published in 2000, defined them only up to "exbi", but in 2005, the third edition added prefixes "zebi" and "yobi", thus matching all SI prefixes with binary counterparts.
The harmonized ISO/IEC IEC 80000-13:2008 standard cancels and replaces subclauses 3.8 and 3.9 of IEC 60027-2:2005 (those defining prefixes for binary multiples). The only significant change is the addition of explicit definitions for some quantities. In 2009, the prefixes kibi-, mebi-, etc. were defined by ISO 80000-1 in their own right, independently of the kibibyte, mebibyte, and so on.
The BIPM standard JCGM 200:2012 "International vocabulary of metrology - Basic and general concepts and associated terms (VIM), 3rd edition" lists the IEC binary prefixes and states "SI prefixes refer strictly to powers of 10, and should not be used for powers of 2. For example, 1 kilobit should not be used to represent bits (210 bits), which is 1 kibibit." 
Other standards bodies and organizations.
The IEC standard binary prefixes are now supported by other standardization bodies and technical organizations.
The United States National Institute of Standards and Technology (NIST) supports the ISO/IEC standards for
"Prefixes for binary multiples" and has a web site documenting them, describing and justifying their use. NIST suggests that in English, the first syllable of the name of the binary-multiple prefix should be pronounced in the same way as the first syllable of the name of the corresponding SI prefix, and that the second syllable should be pronounced as "bee". NIST has stated the SI prefixes "refer strictly to powers of 10" and that the binary definitions "should not be used" for them.
The microelectronics industry standards body JEDEC describes the IEC prefixes in its online dictionary. The JEDEC standards for semiconductor memory use the customary prefix symbols K, M, G and T in the binary sense.
On 19 March 2005, the IEEE standard IEEE 1541-2002 ("Prefixes for Binary Multiples") was elevated to a full-use standard by the IEEE Standards Association after a two-year trial period. However, , the IEEE Publications division does not require the use of IEC prefixes in its major magazines such as "Spectrum" or "Computer". 
The International Bureau of Weights and Measures (BIPM), which maintains the International System of Units (SI), expressly prohibits the use of SI prefixes to denote binary multiples, and recommends the use of the IEC prefixes as an alternative since units of information are not included in SI.
The Society of Automotive Engineers (SAE) prohibits the use of SI prefixes with anything but a power-of-1000 meaning, but does not recommend or otherwise cite the IEC binary prefixes.
The European Committee for Electrotechnical Standardization (CENELEC) adopted the IEC-recommended binary prefixes via the harmonization document HD 60027-2:2003-03.
The European Union (EU) has required the use of the IEC binary prefixes since 2007.
Current practice.
Most computer hardware uses SI prefixes to state capacity and define other performance parameters such as data rate. Main and cache memories are notable exceptions.
Capacities of main memory and cache memory are usually expressed with customary binary prefixes
On the other hand, flash memory, like that found in solid state drives, mostly uses SI prefixes to state capacity.
Some operating systems and other software continue to use the customary binary prefixes in displays of memory, disk storage capacity, and file size, but SI prefixes in other areas such as network communication speeds and processor speeds.
In the following subsections, unless otherwise noted, examples are first given using the common prefixes used in each case, and then followed by interpretation using other notation where appropriate.
Operating systems.
Prior to the release of Macintosh System Software (1984), file sizes were typically reported by the operating system without any prefixes. Today, most operating systems report file sizes with prefixes.
Software.
, most software does not distinguish symbols for binary and decimal prefixes.
The IEC binary naming convention has been adopted by a few, but this is not used universally.
One of the stated goals of the introduction of the IEC prefixes was "to preserve the SI prefixes as unambiguous decimal multipliers." Programs such as fdisk/cfdisk, parted, and apt-get use SI prefixes with their decimal meaning.
Example of the use of IEC binary prefixes in the Linux operating system displaying traffic volume on a network interface in kibibytes (KiB) and mebibytes (MiB), as obtained with the ifconfig utility:
Software that uses standard SI prefixes for powers of 1000, but "not" IEC binary prefixes for powers of 1024, includes:
Software that uses IEC binary prefixes for powers of 1024 "and" uses standard SI prefixes for powers of 1000 includes:
Computer hardware.
Hardware types that use powers-of-1024 multipliers, such as memory, continue to be marketed with customary binary prefixes.
Computer memory.
Measurements of most types of electronic memory such as RAM and ROM are given using customary binary prefixes (kilo, mega, and giga). This includes some flash memory, like EEPROMs. For example, a "512-megabyte" memory module is 512×220 bytes (512 × , or ).
JEDEC Solid State Technology Association, the semiconductor engineering standardization body of the Electronic Industries Alliance (EIA), continues to include the customary binary definitions of kilo, mega and giga in their "Terms, Definitions, and Letter Symbols" document,
and uses those definitions in later memory standards
Many computer programming tasks reference memory in terms of powers of two because of the inherent binary design of current hardware addressing systems. For example, a 16-bit processor register can reference at most 65,536 items (bytes, words, or other objects); this is conveniently expressed as "64K" items. An operating system might map memory as 4096-byte pages, in which case exactly 8192 pages could be allocated within bytes of memory: "8K" (8192) pages of "4 kilobytes" (4096 bytes) each within "32 megabytes" (32 MiB) of memory.
Hard disk drives.
All hard disk drive manufacturers state capacity using SI prefixes.
Flash drives.
USB flash drives, flash-based memory cards like CompactFlash or Secure Digital, and flash-based SSDs use SI prefixes;
for example, a "256 MB" flash card provides at least 256 million bytes (), not 256×1024×1024 ().
The flash memory chips inside these devices contain considerably more than the quoted capacities, but much like a traditional hard drive, some space is reserved for internal functions of the flash drive. These include wear leveling, error correction, sparing, and metadata needed by the device's internal firmware.
Floppy drives.
Floppy disks have existed in numerous physical and logical formats, and have been sized inconsistently. In part, this is because the end user capacity of a particular disk is a function of the controller hardware, so that the same disk could be formatted to a variety of capacities. In many cases, the media are marketed without any indication of the end user capacity, as for example, DSDD, meaning double-sided double-density.
The last widely adopted diskette was the 3½-inch high density. This has a formatted capacity of bytes or 1440 KB (1440 × 1024, using "KB" in the customary binary sense). These are marketed as "HD", or "1.44 MB" or both. This usage creates a third definition of "megabyte" as 1000×1024 bytes.
Most operating systems display the capacity using "MB" in the customary binary sense, resulting in a display of "1.4 MB" (). Some users have noticed the missing 0.04 MB and both Apple and Microsoft have support bulletins referring to them as 1.4 MB.
The earlier "1200 KB" (1200×1024 bytes) 5¼-inch diskette sold with the IBM PC AT was marketed as "1.2 MB" (). The largest 8-inch diskette formats could contain more than a megabyte, and the capacities of those devices were often irregularly specified in megabytes, also without controversy.
Older and smaller diskette formats were usually identified as an accurate number of (binary) KB, for example the Apple Disk II described as "140KB" had a 140×1024-byte capacity, and the original "360KB" double sided, double density disk drive used on the IBM PC had a 360×1024-byte capacity.
In many cases diskette hardware was marketed based on unformatted capacity, and the overhead required to format sectors on the media would reduce the nominal capacity as well (and this overhead typically varied based on the size of the formatted sectors), leading to more irregularities.
Optical discs.
The capacities of most optical disc storage media like DVD, Blu-ray Disc, HD DVD and magneto-optical (MO) are given using SI decimal prefixes.
A "4.7 GB" DVD has a nominal capacity of about 4.38 GiB. However, CD capacities are always given using customary binary prefixes. Thus a "700-MB" (or "80-minute") CD has a nominal capacity of about 700 MiB (approx 730 MB).
Tape drives and media.
Tape drive and media manufacturers use SI decimal prefixes to identify capacity.
Data transmission and clock rates.
Certain units are always used with SI decimal prefixes even in computing contexts.
Two examples are hertz (Hz), which is used to measure the clock rates of electronic components, and bit/s, used to measure data transmission speed.
Bus clock speeds and therefore bandwidths are both quoted using SI decimal prefixes.
Use by industry.
IEC prefixes are used by Toshiba, The IBM Style Guide reads "To help avoid inaccuracy (especially with the larger prefixes) and potential ambiguity, the International Electrotechnical Commission (IEC) in 2000 adopted a set of prefixes specifically for binary multipliers (See IEC 60027-2). Their use is now supported by the United States National Institute of Standards and Technology (NIST) and incorporated into ISO 80000. They are also required by EU law and in certain contexts in the US.
However, most documentation and products in the industry continue to use SI prefixes when referring to binary multipliers. In product documentation, follow the same standard that is used in the product itself (for example, in the interface or firmware). Whether you choose to use IEC prefixes for powers of 2 and SI prefixes for powers of 10, or use SI prefixes for a dual purpose ... be consistent in your usage and explain to the user your adopted system." 

</doc>
<doc id="4078" url="https://en.wikipedia.org/wiki?curid=4078" title="National Baseball Hall of Fame and Museum">
National Baseball Hall of Fame and Museum

The National Baseball Hall of Fame and Museum is an American history museum and hall of fame, located at 25 Main Street in Cooperstown, New York, and operated by private interests. It serves as the central point for the study of the history of baseball in the United States and beyond, displays baseball-related artifacts and exhibits, and honors those who have excelled in playing, managing, and serving the sport. The Hall's motto is "Preserving History, Honoring Excellence, Connecting Generations."
The word Cooperstown is often used as shorthand (or a metonym) for the National Baseball Hall of Fame and Museum.
History.
The Hall of Fame was founded in 1939 by Stephen Carlton Clark, the owner of a local hotel. Clark had sought to bring tourists to a city hurt by the Great Depression, which reduced the local tourist trade, and Prohibition, which devastated the local hops industry. A new building was constructed, and the Hall of Fame was dedicated on June 12, 1939. (Clark's granddaughter, Jane Forbes Clark, is the current chairman of the Board of Directors.)
The erroneous claim that U.S. Civil War hero Abner Doubleday invented baseball in Cooperstown was instrumental in the early marketing of the Hall.
An expanded library and research facility opened in 1994. Dale Petroskey became the organization's president in 1999.
In 2002, the Hall launched "Baseball As America", a traveling exhibit that toured ten American museums over six years. The Hall of Fame has since also sponsored educational programming on the Internet to bring the Hall of Fame to schoolchildren who might not visit. The Hall and Museum completed a series of renovations in spring 2005. The Hall of Fame also presents an annual exhibit at FanFest at the Major League Baseball All-Star Game.
Jeff Idelson replaced Petroskey as president on April 16, 2008. He had been acting as president since March 25, 2008, when Petroskey was forced to resign for having "failed to exercise proper fiduciary responsibility" and making "judgments that were not in the best interest of the National Baseball Hall of Fame and Museum."
In 2012, Congress passed and President Barack Obama signed a law ordering the United States Mint to produce and sell commemorative, non-circulating coins to benefit the private, non-profit Hall. The bill, House Bill H.R. 2527, was introduced in the United States House of Representatives by Rep. Richard Hanna, a Republican from New York, and passed the House on October 26, 2011. The coins, which depict baseball gloves and balls, are the first concave designs produced by the Mint. The mintage included 50,000 gold coins, 400,000 silver coins, and 750,000 clad (Nickel-Copper) coins. The Mint released them on March 27, 2014, and the gold and silver editions quickly sold out. The Hall receives money from surcharges included in the sale price: a total of $9.5 million if all the coins are sold.
Inductees.
Among baseball fans, "Hall of Fame" means not only the museum and facility in Cooperstown, New York, but the pantheon of players, managers, umpires, executives, and pioneers who have been enshrined in the Hall. The first five men elected were Ty Cobb, Babe Ruth, Honus Wagner, Christy Mathewson and Walter Johnson, chosen in 1936; roughly 20 more were selected before the entire group was inducted at the Hall's 1939 opening. , 310 people had been elected to the Hall of Fame, including 215 former Major League Baseball players, 35 Negro league baseball players and executives, 22 managers, 10 umpires, and 28 pioneers, executives, and organizers. 114 members of the Hall of Fame have been inducted posthumously, including four who died after their selection was announced. Of the 35 Negro league members, 29 were inducted posthumously, including all 24 selected since the 1990s. The Hall of Fame includes one female member, Effa Manley.
The newest inductees, enshrined on July 26, , are players Craig Biggio, Randy Johnson, Pedro Martínez, and John Smoltz. In addition to honoring Hall of Fame inductees, the National Baseball Hall of Fame has presented 38 men with the Ford C. Frick Award for excellence in broadcasting, 65 with the J. G. Taylor Spink Award for excellence in baseball writing, and three with the Buck O'Neil Lifetime Achievement Award for contributions to baseball. While Frick and Spink Award honorees are not members of the Hall of Fame, they are recognized in an exhibit in the Hall of Fame's library. O'Neil Award honorees are also not Hall of Fame members, but are listed alongside a permanent statue of the award's namesake and first recipient, Buck O'Neil, that stands at the Hall.
Selection process.
Players are currently inducted into the Hall of Fame through election by either the Baseball Writers' Association of America (or BBWAA), or the Veterans Committee, which now consists of three subcommittees, each of which considers and votes for candidates from a separate era of baseball. Five years after retirement, any player with 10 years of major league experience who passes a screening committee (which removes from consideration players of clearly lesser qualification) is eligible to be elected by BBWAA members with 10 years' membership or more who also have been actively covering MLB at any time in the 10 years preceding the election (the latter requirement was added for the 2016 election). From a final ballot typically including 25–40 candidates, each writer may vote for up to 10 players; until the late 1950s, voters were advised to cast votes for the maximum 10 candidates. Any player named on 75% or more of all ballots cast is elected. A player who is named on fewer than 5% of ballots is dropped from future elections. In some instances, the screening committee had restored their names to later ballots, but in the mid-1990s, dropped players were made permanently ineligible for Hall of Fame consideration, even by the Veterans Committee. A 2001 change in the election procedures restored the eligibility of these dropped players; while their names will not appear on future BBWAA ballots, they may be considered by the Veterans Committee. Players receiving 5% or more of the votes but fewer than 75% are reconsidered annually until a maximum of ten years of eligibility (lowered from fifteen years for the 2015 election).
Under special circumstances, certain players may be deemed eligible for induction even though they have not met all requirements. Addie Joss was elected in 1978, despite only playing nine seasons before he died of meningitis. Additionally, if an otherwise eligible player dies before his fifth year of retirement, then that player may be placed on the ballot at the first election at least six months after his death. Roberto Clemente's induction in 1973 set the precedent when the writers chose to put him up for consideration after his death on New Year's Eve, 1972.
The five-year waiting period was established in 1954 after an evolutionary process. In 1936 all players were eligible, including active ones. From the 1937 election until the 1945 election, there was no waiting period, so any retired player was eligible, but writers were discouraged from voting for current major leaguers. Since there was no formal rule preventing a writer from casting a ballot for an active player, the scribes did not always comply with the informal guideline; Joe DiMaggio received a vote in 1945, for example. From the 1946 election until the 1954 election, an official one-year waiting period was in effect. (DiMaggio, for example, retired after the 1951 season and was first eligible in the 1953 election.) The modern rule establishing a wait of five years was passed in 1954, although an exception was made for Joe DiMaggio because of his high level of previous support, thus permitting him to be elected within four years of his retirement. Contrary to popular belief, no formal exception was made for Lou Gehrig, other than to hold a special one-man election for him. There was no waiting period at that time and Gehrig met all other qualifications, so he would have been eligible for the next regular election after he retired during the 1939 season, but the BBWAA decided to hold a special election at the 1939 Winter Meetings in Cincinnati, specifically to elect Gehrig (most likely because it was known that he was terminally ill, making it uncertain that he would live long enough to see another election). Nobody else was on that ballot, and the numerical results have never been made public. Since no elections were held in 1940 or 1941, the special election permitted Gehrig to enter the Hall while still alive.
If a player fails to be elected by the BBWAA within 20 years of his retirement from active play, he may be selected by the Veterans Committee. Following the most recent changes to the election process for that body made in 2010, it is now responsible for electing all otherwise eligible candidates who are not eligible for the BBWAA ballot—both long-retired players and non-playing personnel (managers, umpires, and executives). With these changes, each candidate can now be considered once every three years. A more complete discussion of the new process is available below.
From 2008 to 2010, following changes made by the Hall in July 2007, the main Veterans Committee, then made up of living Hall of Famers, voted only on players whose careers began in 1943 or later. These changes also established three separate committees to select other figures:
Players of the Negro Leagues have also been considered at various times, beginning in 1971. In 2005 the Hall completed a study on African American players between the late 19th century and the integration of the major leagues in 1947, and conducted a special election for such players in February 2006; seventeen figures from the Negro Leagues were chosen in that election, in addition to the eighteen previously selected. Following the 2010 changes, Negro Leagues figures will primarily be considered for induction alongside other figures from the 1871–1946 era, called the "Pre-Integration Era" by the Hall.
Predictably, the selection process catalyzes endless debate among baseball fans over the merits of various candidates. Even players elected years ago remain the subjects of discussions as to whether they deserved election. For example, Bill James' book "Whatever Happened to the Hall of Fame?" goes into detail about who he believes does and does not belong in the Hall of Fame.
Changes to Veterans Committee process.
The actions and composition of the Veterans Committee have been at times controversial, with occasional selections of contemporaries and teammates of the committee members over seemingly more worthy candidates.
In 2001, the Veterans Committee was reformed to comprise the living Hall of Fame members and other honorees. The revamped Committee held three election, in 2003 and 2007, for both players and non-players, and in 2005 for players only. No individual was elected in that time, sparking criticism among some observers who expressed doubt whether the new Veterans Committee would ever elect a player. The Committee members, most of whom were Hall members, were accused of being reluctant to elect new candidates in the hope of heightening the value of their own selection. After no one was selected for the third consecutive election in 2007, Hall of Famer Mike Schmidt noted, "The same thing happens every year. The current members want to preserve the prestige as much as possible, and are unwilling to open the doors." In 2007, the committee and its selection processes were again reorganized; the main committee then included all living members of the Hall, and voted on a reduced number of candidates from among players whose careers began in 1943 or later. Separate committees, including sportswriters and broadcasters, would select umpires, managers and executives, as well as players from earlier eras.
In the first election to be held under the 2007 revisions, two managers and three executives were elected in December 2007 as part of the 2008 election process. The next Veterans Committee elections for players were held in December 2008 as part of the 2009 election process; the main committee did not select a player, while the panel for pre–World War II players elected Joe Gordon in its first and ultimately only vote. The main committee voted as part of the election process for inductions in odd-numbered years, while the pre-World War II panel would vote every five years, and the panel for umpires, managers, and executives voted as part of the election process for inductions in even-numbered years.
Further changes to the Veterans Committee process were announced by the Hall on July 26, 2010, effective with the 2011 election.
All individuals eligible for induction but not eligible for BBWAA consideration are now considered on a single ballot, grouped by the following eras in which they made their greatest contributions:
The Hall is using the BBWAA's Historical Overview Committee to formulate the ballots for each era, consisting of 12 individuals for the Expansion Era and 10 for the other eras. The Hall's board of directors selects a committee of 16 voters for each era, made up of Hall of Famers, executives, baseball historians, and media members. Each committee meets and votes at the Baseball Winter Meetings once every three years. The Expansion Era committee held its first vote in 2010 for 2011 induction, with longtime general manager Pat Gillick becoming the first individual elected under the new procedure. The Golden Era committee voted in 2011 for the induction class of 2012, with Ron Santo becoming the first player elected under the new procedure. The Pre-Integration Era committee voted in 2012 for the induction class of 2013, electing three figures. Subsequent elections rotate among the three committees in that order.
Players and managers with multiple teams.
While the text on a player's or manager's plaque lists all teams for which the inductee was a member in that specific role, inductees are usually depicted wearing the cap of a specific team, though in a few cases, like umpires, they wear caps without logos. (Executives are not depicted wearing caps.) Additionally, as of 2015, inductee biographies on the Hall's website for all players and managers, and executives who were associated with specific teams, list a "primary team", which does not necessarily match the cap logo. The Hall selects the logo "based on where that player makes his most indelible mark."
Although the Hall always made the final decision on which logo was shown, until 2001 the Hall deferred to the wishes of players or managers whose careers were linked with multiple teams. Some examples of honorees associated with multiple teams are the following:
In all of the above cases, the "primary team" is the team for which the inductee spent the largest portion of his career except for Ryan, whose primary team is listed as the Angels despite playing one fewer season for that team than for the Astros.
In 2001, the Hall of Fame decided to change the policy on cap logo selection, as a result of rumors that some teams were offering compensation, such as number retirement, money, or organizational jobs, in exchange for the cap designation. (For example, though Wade Boggs denied the claims, some media reports had said that his contract with the Tampa Bay Devil Rays required him to request depiction in the Hall of Fame as a Devil Ray.) The Hall decided that it would no longer defer to the inductee, though the player's wishes would be considered, when deciding on the logo to appear on the plaque. Newly elected members affected by the change include the following:
The museum.
According to the Hall of Fame, approximately 300,000 visitors enter the museum each year, and the running total has surpassed 14 million. These visitors see only a fraction of its 38,000 artifacts, 2.6 million library items (such as newspaper clippings and photos) and 130,000 baseball cards.
The Hall has seen a noticeable decrease in attendance in recent years. A 2013 story on "ESPN.com" about the village of Cooperstown and its relation to the game partially linked the reduced attendance with Cooperstown Dreams Park, a youth baseball complex about 5 miles (8 km) away in the town of Hartwick. The 22 fields at Dreams Park currently draw 17,000 players each summer for a week of intensive play; while the complex includes housing for the players, their parents and grandparents must stay elsewhere. According to the story,Prior to Dreams Park, a room might be filled for a week by several sets of tourists. Now, that room will be taken by just one family for the week, and that family may only go into Cooperstown and the Hall of Fame once. While there are other contributing factors (the recession and high gas prices among them), the Hall's attendance has tumbled since Dreams Park opened. The Hall drew 383,000 visitors in 1999. It drew 262,000 last year.
Unauthorized sale of items in collection.
A controversy erupted in 1982, when it emerged that some historic items given to the Hall had been sold on the collectibles market. The items had been lent to the Baseball Commissioner's office, gotten mixed up with other property owned by the Commissioner's office and employees of the office, and moved to the garage of Joe Reichler, an assistant to Commissioner Bowie Kuhn, who sold the items to resolve his personal financial difficulties. Under pressure from the New York Attorney General, the Commissioner's Office made reparations, but the negative publicity damaged the Hall of Fame's reputation, and made it more difficult for it to solicit donations.
Non-induction of banned players.
Following the banning of Pete Rose from MLB, the selection rules for the Baseball Hall of Fame were modified to prevent the induction of anyone on Baseball's permanent suspension list, such as Rose or Shoeless Joe Jackson. Many others have been barred from participation in MLB, but none have Hall of Fame qualifications on the level of Jackson or Rose.
Jackson and Rose were both banned from MLB for life for actions related to gambling on their own teams—Jackson was determined to have cooperated with those who conspired to lose the 1919 World Series intentionally, and Rose voluntarily accepted a permanent spot on the ineligible list in return for MLB's promise to make no official finding in relation to alleged betting on the Cincinnati Reds when he was their manager in the 1980s. (Baseball's Rule 21, prominently posted in every clubhouse locker room, mandates permanent banishment from the MLB for having a gambling interest of any sort on a game in which a player or manager is directly involved.) Rose later admitted that he bet on the Reds in his 2004 autobiography. Baseball fans are deeply split on the issue of whether these two should remain banned or have their punishment revoked. Writer Bill James, though he advocates Rose eventually making it into the Hall of Fame, compared the people who want to put Jackson in the Hall of Fame to "those women who show up at murder trials wanting to marry the cute murderer".
References.
Notes

</doc>
<doc id="4079" url="https://en.wikipedia.org/wiki?curid=4079" title="BPP (complexity)">
BPP (complexity)

In computational complexity theory, BPP, which stands for bounded-error probabilistic polynomial time is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded away from 1/3 for all instances.
BPP is one of the largest "practical" classes of problems, meaning most problems of interest in BPP have efficient probabilistic algorithms that can be run quickly on real modern machines. BPP also contains P, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.
Informally, a problem is in BPP if there is an algorithm for it that has the following properties:
Definition.
A language "L" is in BPP if and only if there exists a probabilistic Turing machine "M", such that
Unlike the complexity class ZPP, the machine "M" is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.
Alternatively, BPP can be defined using only deterministic Turing machines. A language "L" is in BPP if and only if there exists a polynomial "p" and deterministic Turing machine "M", such that
In this definition, the string "y" corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.
In practice, an error probability of 1/3 might not be acceptable, however, the choice of 1/3 in the definition is arbitrary. It can be any constant between 0 and 1/2 (exclusive) and the set BPP will be unchanged. It does not even have to be constant: the same class of problems is defined by allowing error as high as 1/2 − "n"−"c" on the one hand, or requiring error as small as 2−"nc" on the other hand, where "c" is any positive constant, and "n" is the length of input. The idea is that there is a probability of error, but if the algorithm is run many times, the chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound. This makes it possible to create a highly accurate algorithm by merely running the algorithm several times and taking a "majority vote" of the answers. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most 1/2100, this would result in the same class of problems.
Problems.
Besides the problems in P, which are obviously in BPP, many problems were known to be in BPP but not known to be in P. The number of such problems is decreasing, and it is conjectured that P = BPP.
For a long time, one of the most famous problems that was known to be in BPP but not known to be in P was the problem of determining whether a given number is prime. However, in the 2002 paper "PRIMES is in P", Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in P.
An important example of a problem in BPP (in fact in co-RP) still not known to be in P is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial, when you have access to the value of the polynomial for any given input, but not to the coefficients. In other words, is there an assignment of values to the variables such that when a nonzero polynomial is evaluated on these values, the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least "d" values to achieve bounded error probability, where "d" is the total degree of the polynomial.
Related classes.
If the access to randomness is removed from the definition of BPP, we get the complexity class P. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class BQP.
Adding postselection to BPP, or allowing computation paths to have different lengths, gives the class BPPpath. BPPpath is known to contain NP, and it is contained in its quantum counterpart PostBQP.
A Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class BPP have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs "fail" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class ZPP. Alternatively, ZPP contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.
Complexity-theoretic properties.
It is known that BPP is closed under complement; that is, BPP = co-BPP. BPP is low for itself, meaning that a BPP machine with the power to solve BPP problems instantly (a BPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, BPPBPP = BPP.
The relationship between BPP and NP is unknown: it is not known whether BPP is a subset of NP, NP is a subset of BPP or neither. If NP is contained in BPP, which is considered unlikely since it would imply practical solutions for NP-complete problems, then NP = RP and PH ⊆ BPP.
It is known that RP is a subset of BPP, and BPP is a subset of PP. It is not known whether those two are strict subsets, since we don't even know if P is a strict subset of PSPACE. BPP is contained in the second level of the polynomial hierarchy and therefore it is contained in PH. More precisely, the Sipser–Lautemann theorem states that formula_1. As a result, P = NP leads to P = BPP since PH collapses to P in this case. Thus either P = BPP or P ≠ NP or both.
Adleman's theorem states that membership in any language in BPP can be determined by a family of polynomial-size Boolean circuits, which means BPP is contained in P/poly. Indeed, as a consequence of the proof of this fact, every BPP algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however.
Some weak separation results for Monte Carlo time classes were proven by , see also 
Closure properties.
The class BPP is closed under complementation, union and intersection.
Relativization.
Relative to oracles, we know that there exist oracles A and B, such that PA = BPPA and PB ≠ BPPB. Moreover, relative to a random oracle with probability 1, P = BPP and BPP is strictly contained in NP and co-NP.
There is even an oracle in which BPP=EXPNP (and hence P<NP<BPP=EXP=NEXP), which can be iteratively constructed as follows. For a fixed ENP (relativized) complete problem, the oracle will give correct answers with high probability if queried with the problem instance followed by a random string of length "kn" ("n" is instance length; "k" is an appropriate small constant). Start with "n"=1. For every instance of the problem of length "n" fix oracle answers (see lemma below) to fix the instance output. Next, provide the instance outputs for queries consisting of the instance followed by "kn"-length string, and then treat output for queries of length ≤("k"+1)"n" as fixed, and proceed with instances of length "n"+1.
Lemma: Given a problem (specifically, an oracle machine code and time constraint) in relativized ENP, for every partially constructed oracle and input of length "n", the output can be fixed by specifying 2"O"("n") oracle answers.
Proof: The machine is simulated, and the oracle answers (that are not already fixed) are fixed step-by-step. There is at most one oracle query per deterministic computation step. For the relativized NP oracle, if possible fix the output to be yes by choosing a computation path and fixing the answers of the base oracle; otherwise no fixing is necessary, and either way there is at most 1 answer of the base oracle per step. Since there are 2"O"("n") steps, the lemma follows.
The lemma ensures that (for a large enough "k"), it is possible to do the construction while leaving enough strings for the relativized ENP answers. Also, we can ensure that for the relativized ENP, linear time suffices, even for function problems (if given a function oracle and linear output size) and with exponentially small (with linear exponent) error probability. Also, this construction is effective in that given an arbitrary oracle A we can arrange the oracle B to have PA≤PB and EXPNPA=EXPNPB=BPPB. Also, for a ZPP=EXP oracle (and hence ZPP=BPP=EXP<NEXP), one would fix the answers in the relativized E computation to a special nonanswer, thus ensuring that no fake answers are given.
Derandomization.
The existence of certain strong pseudorandom number generators is conjectured by most experts of the field. This conjecture implies that randomness does not give additional computational power to polynomial time computation, that is, P = RP = BPP. Note that ordinary generators are not sufficient to show this result; any probabilistic algorithm implemented using a typical random number generator will always produce incorrect results on certain inputs irrespective of the seed (though these inputs might be rare).
László Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless EXPTIME collapses to MA, BPP is contained in 
The class i.o.-SUBEXP, which stands for infinitely often SUBEXP, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that P = BPP if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and E as EPH, collapses to E; however, note that the exponential-time hierarchy is usually conjectured "not" to collapse.
Russell Impagliazzo and Avi Wigderson showed that if any problem in E, where 
has circuit complexity 2Ω("n") then P = BPP.

</doc>
<doc id="4080" url="https://en.wikipedia.org/wiki?curid=4080" title="BQP">
BQP

In computational complexity theory, BQP (bounded error quantum polynomial time) is the class of decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances. It is the quantum analogue of the complexity class BPP.
In other words, there is an algorithm for a quantum computer (a quantum algorithm) that solves the decision problem with "high" probability and is guaranteed to run in polynomial time. On any given run of the algorithm, it has a probability of at most 1/3 that it will give the wrong answer.
Similarly to other "bounded error" probabilistic classes the choice of 1/3 in the definition is arbitrary. We can run the algorithm a constant number of times and take a majority vote to achieve any desired probability of correctness less than 1, using the Chernoff bound. Detailed analysis shows that the complexity class is unchanged by allowing error as high as 1/2 − "n"−"c" on the one hand, or requiring error as small as 2−"nc" on the other hand, where "c" is any positive constant, and "n" is the length of input.
Definition.
BQP can also be viewed as a bounded-error uniform family of quantum circuits. A language "L" is in BQP if and only if there exists a polynomial-time uniform family of quantum circuits formula_1, such that
Quantum computation.
The number of qubits in the computer is allowed to be a polynomial function of the instance size. For example, algorithms are known for factoring an "n"-bit integer using just over 2"n" qubits (Shor's algorithm).
Usually, computation on a quantum computer ends with a measurement. This leads to a collapse of quantum state to one of the basis states. It can be said that the quantum state is measured to be in the correct state with high probability.
Quantum computers have gained widespread interest because some problems of practical interest are known to be in BQP, but suspected to be outside P. Some prominent examples are:
Relationship to other complexity classes.
This class is defined for a quantum computer and its natural corresponding class for an ordinary computer (or a Turing machine plus a source of randomness) is . Just like and , is low for itself, which means = . Informally, this is true because polynomial time algorithms are closed under composition. If a polynomial time algorithm calls as a subroutine polynomially many polynomial time algorithms, the resulting algorithm is still polynomial time.
In fact, is low for , meaning that a machine achieves no benefit from being able to solve problems instantly, an indication of the possible difference in power between these similar classes.
As the problem of has not yet been solved, the proof of inequality between and classes mentioned above is supposed to be difficult. The relation between and is not known.
Adding postselection to results in the complexity class which is equal to .
External Links.
Complexity Zoo link to BQP

</doc>
<doc id="4086" url="https://en.wikipedia.org/wiki?curid=4086" title="Brainfuck">
Brainfuck

Brainfuck is an esoteric programming language created in 1993 by Urban Müller, and notable for its extreme minimalism. 
The language consists of only eight simple commands and an instruction pointer. While it is fully Turing-complete, it is not intended for practical use, but to challenge and amuse programmers. 
The language's name is a reference to the slang term "brain fuck", which refers to things so complicated or unusual that they exceed the limits of one's understanding.
History.
In 1992, Urban Müller, a Swiss physics student, took over a small online archive for Amiga software. The archive grew more popular, and was soon mirrored around the world. Today, it is the world's largest Amiga archive, known as Aminet. 
Müller designed Brainfuck with the goal of implementing it with the smallest possible compiler, inspired by the 1024-byte compiler for the FALSE programming language. Müller's original compiler was implemented in machine language and compiled to a binary with a size of 296 bytes. He uploaded the first Brainfuck compiler to Aminet in 1993. The program came with a "Readme" file, which briefly described the language, and challenged the reader "Who can program anything useful with it? :)". Müller also included an interpreter and some quite elaborate examples. A second version of the compiler used only 240 bytes.
As Aminet grew, the compiler became popular among the Amiga community, and in time it was implemented for other platforms. Several brainfuck compilers have been made smaller than 200 bytes, and one is only 100 bytes.
Language design.
The language consists of eight commands, listed below. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.
The brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as an array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).
Commands.
The eight language commands each consist of a single character:
codice_2 and codice_1 match as parentheses usually do: each codice_2 matches exactly one codice_1 and vice versa, the codice_2 comes first, and there can be no unmatched codice_2 or codice_1 between the two.
Brainfuck programs can be translated into C using the following substitutions, assuming codice_10 is of type codice_11 and has been initialized to point to an array of zeroed bytes:
As the name suggests, brainfuck programs tend to be difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands; partly it is because the program's text gives no direct indications of the program's state. These, as well as brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing-complete language, brainfuck is theoretically capable of computing any computable function or simulating any other computational model, if given access to an unlimited amount of memory. A variety of brainfuck programs have been written. Although brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for brainfuck in a more typical language such as C due to its simplicity. There even exists a brainfuck interpreter written in the brainfuck language itself.
Brainfuck is an example of a so-called Turing tarpit: It can be used to write "any" program, but it is not practical to do so, because Brainfuck provides so little abstraction that the programs get very long or complicated.
Brainfuck's formal "parent language".
Except for its two I/O commands, brainfuck is a minor variation of the formal programming language P′′ created by Corrado Böhm in 1964. In fact, using six symbols equivalent to the respective brainfuck commands codice_12, codice_13, codice_14, codice_15, codice_2, codice_1, Böhm provided an explicit program for each of the basic functions that together serve to compute any computable function. So the first "brainfuck" programs appear in Böhm's 1964 paper – and they were programs sufficient to prove Turing-completeness.
Examples.
Adding two values.
As a first, simple example, the following code snippet will add the current cell's value to the next cell: Each time the loop is executed, the current cell is decremented, the data pointer moves to the right, that next cell is incremented, and the data pointer moves left again. This sequence is repeated until the starting cell is 0.
Hello World!
The following program prints "Hello World!" and a newline to the screen:
For "readability", this code has been spread across many lines and blanks and comments have been added. Brainfuck ignores all characters except the eight commands codice_18 so no special syntax for comments is needed (as long as the comments don't contain the command characters). The code could just as well have been written as:
ROT13.
This program enciphers its input with the ROT13 cipher. To do this, it must map characters A-M (ASCII 65-77) to N-Z (78-90), and vice versa. Also it must map a-m (97-109) to n-z (110-122) and vice versa. It must map all other characters to themselves; it reads characters one at a time and outputs their enciphered equivalents until it reads an EOF (here assumed to be represented as either -1 or "no change"), at which point the program terminates.
The basic approach used is as follows. Calling the input character "x", divide "x"-1 by 32, keeping quotient and remainder. Unless the quotient is 2 or 3, just output "x", having kept a copy of it during the division. If the quotient is 2 or 3, divide the remainder (("x"-1) modulo 32) by 13; if the quotient here is 0, output "x"+13; if 1, output "x"-13; if 2, output "x".
Regarding the division algorithm, when dividing "y" by "z" to get a quotient "q" and remainder "r", there is an outer loop which sets "q" and "r" first to the quotient and remainder of 1/"z", then to those of 2/"z", and so on; after it has executed "y" times, this outer loop terminates, leaving "q" and "r" set to the quotient and remainder of "y"/"z". (The dividend "y" is used as a diminishing counter that controls how many times this loop is executed.) Within the loop, there is code to increment "r" and decrement "y", which is usually sufficient; however, every "z"th time through the outer loop, it is necessary to zero "r" and increment "q". This is done with a diminishing counter set to the divisor "z"; each time through the outer loop, this counter is decremented, and when it reaches zero, it is refilled by moving the value from "r" back into it.
Portability issues.
Partly because Urban Müller did not write a thorough language specification, the many subsequent brainfuck interpreters and compilers have come to use slightly different dialects of brainfuck.
Cell size.
In the classic distribution, the cells are of 8-bit size (cells are bytes), and this is still the most common size. However, to read non-textual data, a brainfuck program may need to distinguish an end-of-file condition from any possible byte value; thus 16-bit cells have also been used. Some implementations have used 32-bit cells, 64-bit cells, or bignum cells with practically unlimited range, but programs that use this extra range are likely to be slow, since storing the value "n" into a cell requires Ω("n") time as a cell's value may only be changed by incrementing and decrementing.
In all these variants, the codice_19 and codice_20 commands still read and write data in bytes. In most of them, the cells wrap around, i.e. incrementing a cell which holds its maximal value (with the codice_12 command) will bring it to its minimal value and vice versa. The exceptions are implementations which are distant from the underlying hardware, implementations that use bignums, and implementations that try to enforce portability.
Fortunately, it is usually easy to write brainfuck programs that do not ever cause integer wraparound or overflow, and therefore don't depend on cell size. Generally this means avoiding increment of +255 (unsigned 8-bit wraparound), or avoiding overstepping the boundaries of [-128, +127] (signed 8-bit wraparound) (since there are no comparison operators, a program cannot distinguish between a signed and unsigned two's complement fixed-bit-size cell and negativeness of numbers is a matter of interpretation). For more details on integer wraparound, see the Integer overflow article.
Array size.
In the classic distribution, the array has 30,000 cells, and the pointer begins at the leftmost cell. Even more cells are needed to store things like the millionth Fibonacci number, and the easiest way to make the language Turing-complete is to make the array unlimited on the right.
A few implementations extend the array to the left as well; this is an uncommon feature, and therefore portable brainfuck programs do not depend on it.
When the pointer moves outside the bounds of the array, some implementations will give an error message, some will try to extend the array dynamically, some will not notice and will produce undefined behavior, and a few will move the pointer to the opposite end of the array. Some tradeoffs are involved: expanding the array dynamically to the right is the most user-friendly approach and is good for memory-hungry programs, but it carries a speed penalty. If a fixed-size array is used it is helpful to make it very large, or better yet let the user set the size. Giving an error message for bounds violations is very useful for debugging but even that carries a speed penalty unless it can be handled by the operating system's memory protections.
End-of-line code.
Different operating systems (and sometimes different programming environments) use subtly different versions of ASCII. The most important difference is in the code used for the end of a line of text. MS-DOS and Microsoft Windows use a CRLF, i.e. a 13 followed by a 10, in most contexts. UNIX and its descendants (including Linux and Mac OS X) and Amigas use just 10, and older Macs use just 13. It would be unfortunate if brainfuck programs had to be rewritten for different operating systems. Fortunately, a unified standard is easy to find. Urban Müller's compiler and his example programs use 10, on both input and output; so do a large majority of existing brainfuck programs; and 10 is also more convenient to use than CRLF. Thus, brainfuck implementations should make sure that brainfuck programs that assume newline=10 will run properly; many do so, but some do not.
This assumption is also consistent with most of the world's sample code for C and other languages, in that they use '\n', or 10, for their newlines. On systems that use CRLF line endings, the C standard library transparently remaps "\n" to "\r\n" on output and "\r\n" to "\n" on input for streams not opened in binary mode.
End-of-file behavior.
The behavior of the "codice_19" command when an end-of-file condition has been encountered varies. Some implementations set the cell at the pointer to 0, some set it to the C constant EOF (in practice this is usually -1), some leave the cell's value unchanged. There is no real consensus; arguments for the three behaviors are as follows.
Setting the cell to 0 avoids the use of negative numbers, and makes it marginally more concise to write a loop that reads characters until EOF occurs. This is a language extension devised by Panu Kalliokoski.
Setting the cell to -1 allows EOF to be distinguished from any byte value (if the cells are larger than bytes), which is necessary for reading non-textual data; also, it is the behavior of the C translation of "codice_19" given in Müller's readme file. However, it is not obvious that those C translations are to be taken as normative.
Leaving the cell's value unchanged is the behavior of Urban Müller's brainfuck compiler. This behavior can easily coexist with either of the others; for instance, a program that assumes EOF=0 can set the cell to 0 before each "codice_19" command, and will then work correctly on implementations that do either EOF=0 or EOF="no change". It is so easy to accommodate the "no change" behavior that any brainfuck programmer interested in portability should do so.
Derivatives.
Many people have created brainfuck equivalents (languages with commands that directly map to brainfuck) or brainfuck derivatives (languages that extend its behavior or map it into new semantic territory).
Some examples:
However, there are also unnamed minor variants (or dialects), possibly formed as a result of inattention, of which some of the more common are:

</doc>
<doc id="4091" url="https://en.wikipedia.org/wiki?curid=4091" title="Bartolomeo Ammannati">
Bartolomeo Ammannati

Bartolomeo Ammannati (18 June 1511 – 13 April 1592) was an Italian architect and sculptor, born at Settignano, near Florence. He studied under Baccio Bandinelli and Jacopo Sansovino (assisting on the design of the Library of St. Mark's, the "Biblioteca Marciana", Venice) and closely imitated the style of Michelangelo.
He was more distinguished in architecture than in sculpture. He worked in Rome in collaboration with Vignola and Vasari), including designs for the Villa Giulia, but also for works and at Lucca. He labored during 1558–1570, in the refurbishment and enlargement of Pitti Palace, creating the courtyard consisting of three wings with rusticated facades, and one lower portico leading to the amphitheatre in the Boboli Gardens. His design mirrored the appearance of the main external façade of Pitti. He was also named "Consul" of Accademia delle Arti del Disegno of Florence, which had been founded by the Duke Cosimo I in 1563.
In 1569, Ammanati was commissioned to build the Ponte Santa Trinita, a bridge over the Arno River. The three arches are elliptic, and though very light and elegant, has survived, when floods had damaged other Arno bridges at different times. Santa Trinita was destroyed in 1944, during World War II, and rebuilt in 1957.
Ammannati designed what is considered a prototypic mannerist sculptural ensemble in the "Fountain of Neptune" ("Fontana del Nettuno"), prominently located in the Piazza della Signoria in the center of Florence. The assignment was originally given to the aged Bartolommeo Bandinelli; however when Bandinelli died, Ammannati's design, bested the submissions of Benvenuto Cellini and Vincenzo Danti, to gain the commission. From 1563 and 1565, Ammannati and his assistants, among them Giambologna, sculpted the block of marble that had been chosen by Bandinelli. He took Grand Duke Cosimo I as model for Neptune's face. The statue was meant to highlight Cosimo's goal of establishing a Florentine Naval force. When the work on the ungainly sea god was finished, and sited at the other corner of the Palazzo Vecchio of Michelangelo David statue, the then 87-year-old sculptor, is said to have scoffed at Ammannati that he had ruined a beautiful piece of marble, with the ditty: "Ammannati, Ammanato, che bel marmo hai rovinato!" Ammannati continued work on this fountain for a decade, adding around the perimeter a cornucopia of demigod figures: bronze reclining river gods, laughing satyrs and marble sea horses emerging from the water. 
In 1550 Ammannati married Laura Battiferri, an elegant poet and an accomplished woman. Later in his life he had a religious crisis, influenced by Counter-Reformation piety, which resulted in condemning his own works depicting nudity, and he left all his possessions to the Jesuits.
He died in Florence in 1592.

</doc>
<doc id="4092" url="https://en.wikipedia.org/wiki?curid=4092" title="Bishop">
Bishop

A bishop (English derivation from the New Testament of the Christian Bible Greek , "epískopos", "overseer", "guardian") is an ordained or consecrated member of the Christian clergy who is generally entrusted with a position of authority and oversight.
Within the Roman Catholic, Eastern Orthodox, Oriental Orthodox, Anglican, Old Catholic and Independent Catholic churches and in the Assyrian Church of the East, bishops claim apostolic succession, a direct historical lineage dating back to the original Twelve Apostles. Within these churches, bishops are seen as those who possess the full priesthood and can ordain clergy – including other bishops. Some Protestant churches including the Lutheran and Methodist churches have bishops serving similar functions as well, though not always understood to be within apostolic succession in the same way. One who has been ordained deacon, priest, and then bishop is understood to hold the fullness of the (ministerial) priesthood, given responsibility by Christ to govern, teach and sanctify the Body of Christ, members of the Faithful. Priests, deacons and lay ministers cooperate and assist their bishop(s) in shepherding a flock.
Term.
The term "epískopos" (), meaning "overseer" in Greek, the early language of the Christian Church, was not from the earliest times clearly distinguished from the term presbýteros (literally: "elder" or "senior", origin of the modern English word priest), but the term was already clearly used in the sense of the order or office of bishop, distinct from that of presbyter in the writings attributed to Ignatius of Antioch (died c. 110) but of contested authenticity, and sources from the middle of the 2nd century undoubtedly set forth that all the early centers of Christianity recognized and had the office of bishop, using a form of organization that remained universal until the Protestant Reformation.
History.
The earliest organization of the Church in Jerusalem was, according to most scholars, similar to that of Jewish synagogues, but it had a council or college of ordained presbyters ( "elders"). In ( and ), we see a collegiate system of government in Jerusalem chaired by James the Just, according to tradition the first bishop of the city. In (), the Apostle Paul ordains presbyters in churches in Anatolia.
Often, the word "presbyter" was not yet distinguished from "overseer" ( "episkopos", later used exclusively to mean "bishop"), as in (, and ) The earliest writings of the Apostolic Fathers, the Didache and the First Epistle of Clement, for example, show the church used two terms for local church offices—presbyters (seen by many as an interchangeable term with "episcopos" or overseer) and deacon.
According to the view held unanimously among the Catholic writers, this does not mean that the episcopate, in the sense of the holder of the order or office of bishop, must have developed only later, or have been plural, because in each church the college or presbyter-overseers (also called "presbyter-bishops") did not exercise an independent supreme power; it was subject to the Apostles or to their delegates. An explanation suggests that the delegates were bishops in the actual sense of the term, but that they did not possess fixed sees nor had they a special title. Since they were essentially itinerant, they confided to the care of some of the better educated and highly respected converts the fixed necessary functions relating to the daily life of the community.
In Timothy and Titus in the New Testament a more clearly defined episcopate can be seen. We are told that Paul had left Timothy in Ephesus and Titus in Crete to oversee the local church ( and ). Paul commands Titus to ordain presbyters/bishops and to exercise general oversight, telling him to "rebuke with all authority" ().
Early sources are unclear but various groups of Christian communities may have had the bishop surrounded by a group or college functioning as leaders of the local churches. Eventually the head or "monarchic" bishop came to rule more clearly, and all local churches would eventually follow the example of the other churches and structure themselves after the model of the others with the one bishop in clearer charge, though the role of the body of presbyters remained important.
Eventually, as Christendom grew, bishops no longer directly served individual congregations. Instead, the Metropolitan bishop (the bishop in a large city) appointed priests to minister each congregation, acting as the bishop's delegate.
Apostolic Fathers.
Around the end of the 1st century, the church's organization became clearer in historical documents. In the works of the Apostolic Fathers, and Ignatius of Antioch in particular, the role of the episkopos, or bishop, became more important or, rather, already was very important and being clearly defined. While Ignatius of Antioch offers the earliest clear description of monarchial bishops (a single bishop over all house churches in a city) he is an advocate of monepiscopal structure rather than describing an accepted reality. To the bishops and house churches to which he writes, he offers strategies on how to pressure house churches who don't recognize the bishop into compliance. Other contemporary Christian writers do not describe monarchial bishops, either continuing to equate them with the presbyters or speaking of episkopoi (bishops, plural) in a city.
"Plainly therefore we ought to regard the bishop as the Lord Himself" — Epistle of Ignatius to the Ephesians 6:1.
"your godly bishop" — Epistle of Ignatius to the Magnesians 2:1.
"the bishop presiding after the likeness of God and the presbyters after the likeness of the council of the Apostles, with the deacons also who are most dear to me, having been entrusted with the diaconate of Jesus Christ" — Epistle of Ignatius to the Magnesians 6:1.
"Therefore as the Lord did nothing without the Father, united with Him, either by Himself or by the Apostles, so neither do ye anything without the bishop and the presbyters." — Epistle of Ignatius to the Magnesians 7:1.
"Be obedient to the bishop and to one another, as Jesus Christ was to the Father to the flesh, and as the Apostles were to Christ and to the Father, that there may be union both of flesh and of spirit." — Epistle of Ignatius to the Magnesians 13:2.
"In like manner let all men respect the deacons as Jesus Christ, even as they should respect the bishop as being a type of the Father and the presbyters as the council of God and as the college of Apostles. Apart from these there is not even the name of a church." — Epistle of Ignatius to the Trallesians 3:1.
"follow your bishop, as Jesus Christ followed the Father, and the presbytery as the Apostles; and to the deacons pay respect, as to God's commandment" — Epistle of Ignatius to the Smyrnans 8:1.
"He that honoureth the bishop is honoured of God; he that doeth aught without the knowledge of the bishop rendereth service to the devil" — Epistle of Ignatius to the Smyrnans 9:1.
— Lightfoot translation.
As the Church continued to expand, new churches in important cities gained their own bishop. Churches in the regions outside an important city were served by Chorbishop, an official rank of bishops. However, soon, presbyters and deacons were sent from bishop of a city church. Gradually priests replaced the chorbishops. Thus, in time, the bishop changed from being the leader of a single church confined to an urban area to being the leader of the churches of a given geographical area.
Clement of Alexandria (end of the 2nd century) writes about the ordination of a certain Zachæus as bishop by the imposition of Simon Peter Bar-Jonah's hands. The words bishop and ordination are used in their technical meaning by the same Clement of Alexandria. The bishops in the 2nd century are defined also as the only clergy to whom the ordination to priesthood (presbyterate) and diaconate is entrusted: "a priest (presbyter) lays on hands, but does not ordain." ("cheirothetei ou cheirotonei")
At the beginning of the 3rd century, Hippolytus of Rome describes another feature of the ministry of a bishop, which is that of the ""Spiritum primatus sacerdotii habere potestatem dimittere peccata"": the primate of sacrificial priesthood and the power to forgive sins.
Bishops and civil government.
The efficient organization of the Roman Empire became the template for the organisation of the church in the 4th century, particularly after Constantine's Edict of Milan. As the church moved from the shadows of privacy into the public forum it acquired land for churches, burials and clergy. In 391, Theodosius I decreed that any land that had been confiscated from the church by Roman authorities be returned.
The most usual term for the geographic area of a bishop's authority and ministry, the diocese, began as part of the structure of the Roman Empire under Diocletian. As Roman authority began to fail in the western portion of the empire, the church took over much of the civil administration. This can be clearly seen in the ministry of two popes: Pope Leo I in the 5th century, and Pope Gregory I in the 6th century. Both of these men were statesmen and public administrators in addition to their role as Christian pastors, teachers and leaders. In the Eastern churches, latifundia entailed to a bishop's see were much less common, the state power did not collapse the way it did in the West, and thus the tendency of bishops acquiring secular power was much weaker than in the West. However, the role of Western bishops as civil authorities, often called prince bishops, continued throughout much of the Middle Ages.
Bishops holding political office.
As well as being arch chancellors of the Holy Roman Empire after the 9th century, bishops generally served as chancellors to medieval monarchs, acting as head of the "justiciary" and chief chaplain. The Lord Chancellor of England was almost always a bishop up until the dismissal of Cardinal Thomas Wolsey by Henry VIII. Similarly, the position of Kanclerz in the Polish kingdom was always a bishop until the 16th century. And today, the principality of Andorra is headed by two Co-Princes, one of whom is a Catholic Bishop (and the other, the President of France).
In France before the French Revolution, representatives of the clergy — in practice, bishops and abbots of the largest monasteries — comprised the First Estate of the Estates-General, until their role was abolished during the French Revolution.
In the 21st century, the more senior bishops of the Church of England continue to sit in the House of Lords of the Parliament of the United Kingdom, as representatives of the established church, and are known as Lords Spiritual. The Bishop of Sodor and Man, whose diocese lies outside of the United Kingdom, is an "ex officio" member of the Legislative Council of the Isle of Man. In the past, the Bishop of Durham, known as a prince bishop, had extensive viceregal powers within his northern diocese — the power to mint money, collect taxes and raise an army to defend against the Scots.
Eastern Orthodox bishops, along with all other members of the clergy, are canonically forbidden to hold political office. Occasional exceptions to this rule are tolerated when the alternative is political chaos. In the Ottoman Empire, the Patriarch of Constantinople, for example, had de facto administrative, fiscal, cultural and legal jurisdiction, as well as spiritual, over all the Christians of the empire. More recently, Archbishop Makarios III of Cyprus, served as President of the Republic of Cyprus from 1960 to 1977.
In 2001, Peter Hollingworth, AC, OBE – then the Anglican Archbishop of Brisbane – was controversially appointed Governor-General of Australia. Although Hollingworth gave up his episcopal position to accept the appointment, it still attracted considerable opposition in a country which maintains a formal separation between Church and State.
Episcopacy during the English Civil War.
During the period of the English Civil War, the role of bishops as wielders of political power and as upholders of the established church became a matter of heated political controversy. Indeed, Presbyterianism was the polity of most Reformed Churches in Europe, and had been favored by many in England since the English Reformation. Since in the primitive church the offices of "presbyter" and "episkopos" were identical, many Puritans held that this was the only form of government the church should have. The Anglican divine, Richard Hooker, objected to this claim in his famous work "Of the Laws of Ecclesiastic Polity" while, at the same time, defending Presbyterian ordination as valid (in particular Calvin's ordination of Beza). This was the official stance of the English Church until the Commonwealth, during which time, the views of Presbyterians and Independents (Congregationalists) were more freely expressed and practiced.
Churches.
Catholic Church, Orthodox churches and Anglican churches.
Bishops form the leadership in the Roman Catholic Church, the Eastern Orthodox Church, the Oriental Orthodox Churches, the Anglican Communion, the Lutheran Church, the Independent Catholic Churches, the Independent Anglican Churches, and certain other, smaller, denominations.
The traditional role of a bishop is as pastor of a diocese (also called a bishopric, synod, eparchy or see), and so to serve as a "diocesan bishop," or "eparch" as it is called in many Eastern Christian churches. Dioceses vary considerably in size, geographically and population-wise. Some dioceses around the Mediterranean Sea which were Christianised early are rather compact, whereas dioceses in areas of rapid modern growth in Christian commitment—as in some parts of Sub-Saharan Africa, South America and the Far East—are much larger and more populous.
As well as traditional diocesan bishops, many churches have a well-developed structure of church leadership that involves a number of layers of authority and responsibility.
Duties.
In Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, and Anglicanism, only a bishop can ordain other bishops, priests, and deacons.
In the Eastern liturgical tradition, a priest can celebrate the Divine Liturgy only with the blessing of a bishop. In Byzantine usage, an antimension signed by the bishop is kept on the altar partly as a reminder of whose altar it is and under whose omophorion the priest at a local parish is serving. In Syriac Church usage, a consecrated wooden block called a thabilitho is kept for the same reasons.
The pope, in addition to being the Bishop of Rome and spiritual head of the Catholic Church, is also the Patriarch of the Latin Rite. Each bishop within the Latin Rite is answerable directly to the Pope and not any other bishop except to metropolitans in certain oversight instances. The pope previously used the title "Patriarch of the West", but this title was dropped from use in 2006 a move which caused some concern within the Orthodox Communion as, to them, it implied wider papal jurisdiction.
In Catholic, Eastern Orthodox, Oriental Orthodox and Anglican cathedrals there is a special chair set aside for the exclusive use of the bishop. This is the bishop's "cathedra" and is often called the throne. In some Christian denominations, for example, the Anglican Communion, parish churches may maintain a chair for the use of the bishop when he visits; this is to signify the parish's union with the bishop.
The bishop is the ordinary minister of the sacrament of confirmation in the Latin Rite Catholic Church, and in the Anglican and Old Catholic communion only a bishop may administer this sacrament. However, in the Byzantine and other Eastern rites, whether Eastern or Oriental Orthodox or Eastern Catholic, chrismation is done immediately after baptism, and thus the priest is the one who confirms, using chrism blessed by a bishop.
Ordination of Catholic, Orthodox and Anglican bishops.
Bishops in all of these communions are ordained by other bishops through the laying on of hands. While traditional teaching maintains that any bishop with apostolic succession can validly perform the ordination of another bishop, some churches require two or three bishops participate, either to ensure sacramental validity or to conform with church law. Roman Catholic doctrine holds that one bishop can validly ordain another male (priest) as a bishop. Though a minimum of three bishops participating is desirable (there are usually several more) in order to demonstrate collegiality, canonically only one bishop is necessary. The practice of only one bishop ordaining was normal in countries where the Church was persecuted under Communist rule.
The title of archbishop or metropolitan may be granted to a senior bishop, usually one who is in charge of a large ecclesiastical jurisdiction. He may, or may not, have provincial oversight of suffragan bishops and may possibly have auxiliary bishops assisting him.
Ordination of a bishop, and thus continuation of apostolic succession, takes place through a ritual centred on the imposition of hands and prayer.
Apart from the ordination, which is always done by other bishops, there are different methods as to the actual selection of a candidate for ordination as bishop. In the Catholic Church the Congregation for Bishops oversees the selection of new bishops with the approval of the pope. The papal nuncio usually solicits names from the bishops of a country, and then selects three to be forwarded to the Holy See. Most Eastern Orthodox churches allow varying amounts of formalised laity and/or lower clergy influence on the choice of bishops. This also applies in those Eastern churches which are in union with the pope, though it is required that he give assent.
Catholic, Orthodox, Anglican, Old Catholic and some Lutheran bishops claim to be part of the continuous sequence of ordained bishops since the days of the apostles referred to as apostolic succession. Since Pope Leo XIII issued the bull "Apostolicae curae" in 1896, the Catholic Church has insisted that Anglican orders are invalid because of changes in the Anglican ordination rites of the 16th century and divergence in understanding of the theology of priesthood, episcopacy and Eucharist. However, since the 1930s, Utrecht Old Catholic bishops (recognised by the Holy See as validily ordained) have sometimes taken part in the ordination of Anglican bishops. According to the writer Timothy Dufort, by 1969, all Church of England bishops had acquired Old Catholic lines of apostolic succession recognised by the Holy See. This development has muddied the waters somewhat as it could be argued that the strain of apostolic succession has been re-introduced into Anglicanism, at least within the Church of England.
The Catholic Church does recognise as valid (though illicit) ordinations done by breakaway Catholic, Old Catholic or Oriental bishops, and groups descended from them; it also regards as both valid and licit those ordinations done by bishops of the Eastern churches, so long as those receiving the ordination conform to other canonical requirements (for example, is an adult male) and an orthodox rite of episcopal ordination, expressing the proper functions and sacramental status of a bishop, is used; this has given rise to the phenomenon of "episcopi vagantes" (for example, clergy of the Independent Catholic groups which claim apostolic succession, though this claim is rejected by both Orthodoxy and Catholicism).
The Orthodox Churches would not accept the validity of any ordinations performed by the Independent Catholic groups, as Orthodoxy considers to be spurious any consecration outside of the Church as a whole. Orthodoxy considers apostolic succession to exist only within the Universal Church, and not through any authority held by individual bishops; thus, if a bishop ordains someone to serve outside of the (Orthodox) Church, the ceremony is ineffectual, and no ordination has taken place regardless of the ritual used or the ordaining prelate's position within the Orthodox Churches.
The position of Roman Catholicism is slightly different. Whilst it does recognise the validity of the orders of certain groups which separated from communion with Holy See. The Holy See accepts as valid the ordinations of the Old Catholics in communion with Utrecht, as well as the Polish National Catholic Church (which received its orders directly from Utrecht, and was—until recently—part of that communion); but Roman Catholicism does not recognise the orders of any group whose teaching is at variance with what they consider the core tenets of Christianity; this is the case even though the clergy of the Independent Catholic groups may use the proper ordination ritual. There are also other reasons why the Holy See does not recognise the validity of the orders of the Independent clergy:
Whilst members of the Independent Catholic movement take seriously the issue of valid orders, it is highly significant that the relevant Vatican Congregations tend not to respond to petitions from Independent Catholic bishops and clergy who seek to be received into communion with the Holy See, hoping to continue in some sacramental role. In those instances where the pope does grant reconciliation, those deemed to be clerics within the Independent Old Catholic movement are invariably admitted as laity and not priests or bishops.
There is a mutual recognition of the validity of orders amongst Roman Catholic, Eastern Orthodox, Old Catholic, Oriental Orthodox and Assyrian Church of the East churches. 
Some provinces of the Anglican Communion have begun ordaining women as bishops in recent decades for example, the United States, New Zealand, Canada and Cuba. The first woman to be consecrated a bishop within Anglicanism was Barbara Harris, who was ordained in the United States in 1989. In 2006, Katharine Jefferts Schori, the Episcopal Bishop of Nevada, became the first woman to become the Presiding Bishop of the Episcopal Church.
Lutheranism.
In the Evangelical Lutheran Church in America (ELCA) and the Evangelical Lutheran Church in Canada (ELCIC), the largest Lutheran Church bodies in the United States and Canada respectively and roughly based on the Nordic Lutheran state churches (similar to that of the Church of England), bishops are elected by Synod Assemblies, consisting of both lay members and clergy, for a term of 6 years, which can be renewed, depending upon the local synod's "constitution" (which is mirrored on either the ELCA or ELCIC's national constitution). Since the implementation of concordats between the ELCA and the Episcopal Church of the United States and the ELCIC and the Anglican Church of Canada, all bishops, including the Presiding Bishop (ELCA) or the National Bishop (ELCIC), have been consecrated using the historic succession, with at least one Anglican bishop serving as co-consecrator.
Since going into ecumenical communion with their respective Anglican body, bishops in the ELCA or the ELCIC not only approve the "rostering" of all ordained pastors, diaconal ministers, and associates in ministry, but they serve as the principal celebrant of all pastoral ordination and installation ceremonies, diaconal consecration ceremonies, as well as serving as the "chief pastor" of the local synod, upholding the teachings of Martin Luther as well as the documentations of the Ninety-Five Theses and the Augsburg Confession. Unlike their counterparts in the United Methodist Church, ELCA and ELCIC synod bishops do not appoint pastors to local congregations (pastors, like their counterparts in the Episcopal Church, are called by local congregations). The Presiding Bishop of the ELCA and the National Bishop of the ELCIC, the national bishops of their respective bodies, is elected for a single 6-year term and may be elected to an additional term.
Although ELCA agreed with the Episcopal Church to limit ordination to the bishop "ordinarily", ELCA pastor-"ordinators" are given permission to perform the rites in "extraordinary" circumstance. In practice, "extraordinary" circumstance have included disagreeing with Episcopalian views of the episcopate, and as a result, ELCA pastors ordained by other pastors are not permitted to be deployed to Episcopal Churches (they can, however, serve in Presbyterian Church USA, United Methodist Church, Reformed Church in America, and Moravian Church congregations, as the ELCA is in full communion with these denominations). The Lutheran Church–Missouri Synod (LCMS) and the Wisconsin Evangelical Lutheran Synod (WELS), the second and third largest Lutheran bodies in the United States and the two largest Confessional Lutheran bodies in North America, do not follow an episcopal form of governance, settling instead on a form of quasi-congregationalism patterned off what they believe to be the practice of the early church. It should be noted that the second largest of the three predecessor bodies of the ELCA, the American Lutheran Church, was a congregationalist body, with national and synod presidents before they were re-titled as bishops (borrowing from the Lutheran churches in Germany) in the 1980s. It must also be noted that with regard to ecclesial discipline and oversight, national and synod presidents typically function similarly to bishops in episcopal bodies.
Methodism.
United Methodist Church.
In the United Methodist Church (the largest branch of Methodism in the world) bishops serve as administrative and pastoral superintendents of the church. They are elected for life from among the ordained elders (presbyters) by vote of the delegates in regional (called jurisdictional) conferences, and are consecrated by the other bishops present at the conference through the laying on of hands. In the United Methodist Church bishops remain members of the "Order of Elders" while being consecrated to the "Office of the Episcopacy". Within the United Methodist Church only bishops are empowered to consecrate bishops and ordain clergy. Among their most critical duties is the ordination and appointment of clergy to serve local churches as pastor, presiding at sessions of the Annual, Jurisdictional, and General Conferences, providing pastoral ministry for the clergy under their charge, and safeguarding the doctrine and discipline of the Church. Furthermore, individual bishops, or the Council of Bishops as a whole, often serve a prophetic role, making statements on important social issues and setting forth a vision for the denomination, though they have no legislative authority of their own. In all of these areas, bishops of the United Methodist Church function very much in the historic meaning of the term. According to the "Book of Discipline of the United Methodist Church", a bishop's responsibilities are 
In each Annual Conference, United Methodist bishops serve for four-year terms, and may serve up to three terms before either retirement or appointment to a new Conference. United Methodist bishops may be male or female, with Marjorie Matthews being the first woman to be consecrated a bishop in 1980.
The collegial expression of episcopal leadership in the United Methodist Church is known as the Council of Bishops. The Council of Bishops speaks to the Church and through the Church into the world and gives leadership in the quest for Christian unity and interreligious relationships. The "Conference of Methodist Bishops" includes the United Methodist "Council of Bishops" plus bishops from affiliated autonomous Methodist or United Churches.
John Wesley consecrated Thomas Coke a "General Superintendent," and directed that Francis Asbury also be consecrated for the United States of America in 1784, where the Methodist Episcopal Church first became a separate denomination apart from the Church of England. Coke soon returned to England, but Asbury was the primary builder of the new church. At first he did not call himself bishop, but eventually submitted to the usage by the denomination.
Notable bishops in United Methodist history include Coke, Asbury, Richard Whatcoat, Philip William Otterbein, Martin Boehm, Jacob Albright, John Seybert, Matthew Simpson, John S. Stamm, William Ragsdale Cannon, Marjorie Matthews, Leontine T. Kelly, William B. Oden, Ntambo Nkulu Ntanda, Joseph Sprague, William Henry Willimon, and Thomas Bickerton.
Christian Methodist Episcopal Church.
In the Christian Methodist Episcopal Church in the United States, bishops are administrative superintendents of the church; they are elected by "delegate" votes for as many years deemed until the age of 74, then he/she must retire. Among their duties, are responsibility for appointing clergy to serve local churches as pastor, for performing ordinations, and for safeguarding the doctrine and discipline of the Church. The General Conference, a meeting every four years, has an equal number of clergy and lay delegates. In each Annual Conference, CME bishops serve for four-year terms. CME Church bishops may be male or female.
The Church of Jesus Christ of Latter-day Saints.
In The Church of Jesus Christ of Latter-day Saints, the Bishop is the leader of a local congregation, called a ward. As with most LDS priesthood holders, the bishop is a part-time lay minister and earns a living through other employment; in all cases, he is a married man. As such, it is his duty to preside at services, call local leaders, and judge the worthiness of members for service. The bishop does not deliver sermons at every service (generally asking members to do so), but is expected to be a spiritual guide for his congregation. It is therefore believed that he has both the right and ability to receive divine inspiration (through the Holy Spirit) for the ward under his direction. Because it is a part-time position, all able members are expected to assist in the management of the ward by holding delegated lay positions (for example, women's and youth leaders, teachers) referred to as 'callings.' Although members are asked to confess serious sins to him, unlike the Roman Catholic Church, he is not the instrument of divine forgiveness, merely a guide through the repentance process (and a judge in case transgressions warrant excommunication or other official discipline). The bishop is also responsible for the physical welfare of the ward, and thus collects tithing and fast offerings and distributes financial assistance where needed.
A bishop is the president of the Aaronic priesthood in his ward (and is thus a form of Mormon Kohen; in fact, a literal descendant of Aaron has "legal right" to act as a Bishop after being found worthy and ordained by the First Presidency). In the absence of a literal descendant of Aaron, a High priest in the Melchizedek priesthood is called to be a Bishop. Each bishop is selected from resident members of the ward by the stake presidency with approval of the First Presidency, and chooses two "counselors" to form a "bishopric". In special circumstances (such as a ward consisting entirely of young university students), a bishop may be chosen from outside the ward. A bishop is typically released after about five years and a new bishop is called to the position. Although the former bishop is released from his duties, he continues to hold the Aaronic priesthood office of Bishop. Church members frequently refer to a former bishop as "Bishop" as a sign of respect and affection.
Latter-day Saint bishops do not wear any special clothing or insignia the way clergy in many other churches do, but are expected to dress and groom themselves neatly and conservatively per their local culture, especially when performing official duties. Bishops (as well as other members of the priesthood) can trace their line of authority back to Joseph Smith, who, according to church doctrine, was ordained to lead the Church in modern times by the ancient apostles Peter, James, and John, who were ordained to lead the Church by Jesus Christ.
The Presiding Bishop oversees the temporal affairs (buildings, properties, commercial corporations, and so on) of the worldwide Church, including the Church's massive global humanitarian aid and social welfare programs. The Presiding Bishop has two counselors; the three together form the Presiding Bishopric.
New Apostolic Church.
The New Apostolic Church (NAC) knows three classes of ministries: Deacons, Priests and Apostles. The Apostles, who are all included in the apostolate with the Chief Apostle as head, are the highest ministries.
Of the several kinds of priest...ministries, the bishop is the highest. Nearly all bishops are set in line directly from the chief apostle. They support and help their superior apostle.
Church of God (Cleveland, Tennessee).
In the polity of the Church of God (Cleveland, Tennessee), the international leader is the Presiding Bishop, and the members of the Executive Committee are Executive Bishops. Collectively, they supervise and appoint national and state leaders across the world. Leaders of individual states and regions are Administrative Bishops, who have jurisdiction over local churches in their respective states and are vested with appointment authority for local pastorates. All ministers are credentialed at one of three levels of licensure, the most senior of which is the rank of Ordained Bishop. To be eligible to serve in state, national, or international positions of authority, a minister must hold the rank of Ordained Bishop.
Pentecostal Church of God.
In 2002, the general convention of the Pentecostal Church of God came to a consensus to change the title of their overseer from General Superintendent to Bishop. The change was brought on because internationally, the term Bishop is more commonly related to religious leaders than the previous title.
The title Bishop is used for both the General (International leader) and the district (state) leaders. The title is sometimes used in conjunction with the previous thus becoming General (District) Superintendent/Bishop.
Seventh-day Adventists.
According to the Seventh-day Adventist understanding of the Doctrine of the Church:
"The "elders" (Greek, presbuteros) or "bishops" (episkopos) were the most important officers of the church. The term elder means older one, implying dignity and respect. His position was similar to that of the one who had supervision of the synagogue. The term bishop means "overseer." Paul used these terms interchangeably, equating elders with overseers or bishops (Acts 20:17,; Titus 1:5, 7).
"Those who held this position supervised the newly formed churches. Elder referred to the status or rank of the office, while bishop denoted the duty or responsibility of the office—"overseer." Since the apostles also called themselves elders (1 Peter 5:1; 2 John 1; 3 John 1), it is apparent that there were both local elders and itinerant elders, or elders at large. But both kinds of elder functioned as shepherds of the congregations."
The above understanding is part of the basis of Adventist organizational structure. The world wide Seventh-day Adventist church is organized into local districts, conferences or missions, union conferences or union missions, divisions, and finally at the top is the general conference. At each level (with exception to the local districts), there is an elder who is elected president and a group of elders who serve on the executive committee with the elected president. Those who have been elected president would in effect be the "bishop" while never actually carrying the title or ordained as such because the term is usually associated with the episcopal style of church governance most often found in Catholic, Anglican, Methodist and some Pentecostal/Charismatic circles.
Others.
Some Baptists have begun taking on the title of Bishop.
In some smaller Protestant denominations and independent churches, the term bishop is used in the same way as pastor, to refer to the leader of the local congregation, and may be male or female. This usage is especially common in African-American churches in the USA.
In the Church of Scotland, which has a Presbyterian church structure, the word "bishop" refers to an ordained person, usually a normal parish minister, who has temporary oversight of a trainee minister. In the Presbyterian Church (U.S.A.), the term bishop is an expressive name for a Minister of Word and Sacrament who serves a congregation and exercises "the oversight of the flock of Christ." The term is traceable to the 1789 Form of Government of the PC (U.S.A.) and the Presbyterian understanding of the pastoral office.
While not considered orthodox Christian, the Ecclesia Gnostica Catholica uses roles and titles derived from Christianity for its clerical hierarchy, including bishops who have much the same authority and responsibilities as in Roman Catholicism.
The Salvation Army does not have bishops but has appointed leaders of geographical areas, known as Divisional Commanders. Larger geographical areas, called Territories, are led by a Territorial Commander, who is the highest-ranking officer in that Territory.
Jehovah’s Witnesses do not use the title ‘Bishop’ within their organizational structure, but appoint elders to be overseers (to fulfill the role of oversight) within their congregations.
Dress and insignia.
Traditionally, a number of items are associated with the office of a bishop, most notably the mitre, crosier, and ecclesiastical ring. Other vestments and insignia vary between Eastern and Western Christianity.
In the Latin Rite of the Catholic Church, the choir dress of a bishop includes the purple cassock with amaranth trim, rochet, purple zucchetto (skull cap), purple biretta, and pectoral cross. The cappa magna may be worn, but only within the bishop's own diocese and on especially solemn occasions. The mitre, zuchetto, and stole are generally worn by bishops when presiding over liturgical functions. For liturgical functions other than the Mass the bishop typically wears the cope. Within his own diocese and when celebrating solemnly elsewhere with the consent of the local ordinary, he also uses the crosier. When celebrating Mass, a bishop, like a priest, wears the chasuble. The Caeremoniale Episcoporum recommends, but does not impose, that in solemn celebrations a bishop should also wear a dalmatic, which can always be white, beneath the chasuble, especially when administering the sacrament of holy orders, blessing an abbot or abbess, and dedicating a church or an altar. The Caeremoniale Episcoporum no longer makes mention of episcopal gloves, episcopal sandals, liturgical stockings (also known as buskins), or the accoutrements that it once prescribed for the bishop's horse. The coat of arms of a Latin Rite Catholic bishop usually displays a galero with a cross and crosier behind the escutcheon; the specifics differ by location and ecclesiastical rank (see Ecclesiastical heraldry).
Anglican bishops generally make use of the mitre, crosier, ecclesiastical ring, purple cassock, purple zucchetto, and pectoral cross. However, the traditional choir dress of Anglican bishops retains its late mediaeval form, and looks quite different from that of their Catholic counterparts; it consists of a long rochet which is worn with a chimere.
In the Eastern Churches (Eastern Orthodox, Eastern Rite Catholic) a bishop will wear the mandyas, panagia (and perhaps an enkolpion), sakkos, omophorion and an Eastern-style mitre. Eastern bishops do not normally wear an episcopal ring; the faithful kiss (or, alternatively, touch their forehead to) the bishop's hand. To seal official documents, he will usually use an inked stamp. An Eastern bishop's coat of arms will normally display an Eastern-style mitre, cross, eastern style crosier and a red and white (or red and gold) mantle. The arms of Oriental Orthodox bishops will display the episcopal insignia (mitre or turban) specific to their own liturgical traditions. Variations occur based upon jurisdiction and national customs.

</doc>
<doc id="4093" url="https://en.wikipedia.org/wiki?curid=4093" title="Bertrand Andrieu">
Bertrand Andrieu

__NOTOC__
Bertrand Andrieu (24 November 1761 – 6 December 1822) was a French engraver of medals. He was born in Bordeaux. In France, he was considered as the restorer of the art, which had declined after the time of Louis XIV. During the last twenty years of his life, the French government commissioned him to undertake every major work of importance.

</doc>
<doc id="4097" url="https://en.wikipedia.org/wiki?curid=4097" title="Bordeaux">
Bordeaux

Bordeaux (; Gascon: "Bordèu") is a port city on the Garonne River in the Gironde department in southwestern France.
The municipality (commune) of Bordeaux proper has a population of 243,626 (2012). Together with its suburbs and satellite towns Bordeaux is the center of the Bordeaux Métropole, which with 749,595 inhabitants () is the 5th largest in France. It is the capital of the Aquitaine-Limousin-Poitou-Charentes region, as well as the prefecture of the Gironde department. Its inhabitants are called "Bordelais" (for men) or "Bordelaises" (women). The term "Bordelais" may also refer to the city and its surrounding region.
The city's titles are "La perle d'Aquitaine" (The Pearl of Aquitaine), and "La Belle Endormie" (Sleeping Beauty) in reference to the old center which had black walls due to pollution. Nowadays, this is not the case. In fact, a part of the city, Le Port de La Lune, was almost completely renovated. Bordeaux is the city which has the highest number of preserved historical buildings in France, except for Paris.
Bordeaux is the world's major wine industry capital. It is home to the world's main wine fair, Vinexpo, while the wine economy in the metro area takes in 14.5 billion euros each year. Bordeaux wine has been produced in the region since the 8th century. The historic part of the city is on the UNESCO World Heritage List as "an outstanding urban and architectural ensemble" of the 18th century.
History.
In historical times, around 300 BCE it was the settlement of a Celtic tribe, the Bituriges Vivisci, who named the town "Burdigala", probably of Aquitanian origin. The name Bourde is still the name of a river south of the city.
In 107 BCE, the Battle of Burdigala was fought by the Romans who were defending the Allobroges, a Gallic tribe allied to Rome, and the Tigurini led by Divico. The Romans were defeated and their commander, the consul Lucius Cassius Longinus, was killed in the action.
The city fell under Roman rule around 60 BC, its importance lying in the commerce of tin and lead towards Rome. Later it became capital of Roman Aquitaine, flourishing especially during the Severan dynasty (3rd century). In 276 it was sacked by the Vandals. Further ravage was brought by the same Vandals in 409, the Visigoths in 414 and the Franks in 498, beginning a period of obscurity for the city.
In the late 6th century, the city re-emerged as the seat of a county and an archdiocese within the Merovingian kingdom of the Franks, but royal Frankish power was never strong. The city started to play a regional role as a major urban center on the fringes of the newly founded Frankish Duchy of Vasconia. Circa 585, a certain Gallactorius is cited as count of Bordeaux and fighting the Basques.
The city was plundered by the troops of Abd er Rahman in 732 after storming the fortified city and overwhelming the Aquitanian garrison. Duke Eudes mustered a force ready to engage the Umayyads outside Bordeaux, eventually taking on them in the Battle of the River Garonne somewhere near the river Dordogne, described as taking a heavy death toll. After duke Eudes's defeat, the Aquitanian duke could still save part of its troops and keep his grip on Aquitaine after the Battle of Poitiers.
In 735, the Aquitanian duke Hunald led a rebellion after his father Eudes's death, at which Charles responded by sending an expedition that captured and plundered Bordeaux again, but it was not retained for long. The following year, the Frankish commander descended again over Aquitaine, but clashed in battle with the Aquitanians and left to take on hostile Burgundian authorities and magnates. In 745, Aquitaine faced yet another expedition by Charles' sons Pepin and Carloman against Hunald, the Aquitanian "princeps" (or duke) strong in Bordeaux. Hunald was defeated, and his son Waifer replaced him, who in turn confirmed Bordeaux as the capital city (along with Bourges in the north).
During the last stage of the war against Aquitaine (760–768), it was one of Waifer's last important strongholds to fall to King Pepin the Short's troops. Next to Bordeaux, Charlemagne built the fortress of Fronsac ("Frontiacus", "Franciacus") on a hill across the border with the Basques ("Wascones"), where Basque commanders came over to vow loyalty to him (769).
In 778, Seguin (or Sihimin) was appointed count of Bordeaux, probably undermining the power of the Duke Lupo, and possibly leading to the Battle of Roncevaux Pass that very year. In 814, Seguin was made Duke of Vasconia, but he was deposed in 816 for failing to suppress or sympathise with a Basque rebellion. Under the Carolingians, sometimes the Counts of Bordeaux held the title concomitantly with that of Duke of Vasconia. They were meant to keep the Basques in check and defend the mouth of the Garonne from the Vikings when the latter appeared c. 844 in the region of Bordeaux. In Autumn 845, count Seguin II marched on the Vikings assaulting Bordeaux and Saintes, but was captured and put to death. There are no bishops mentioned during the whole 8th century and part of the 9th in Bordeaux.
From the 12th to the 15th century, Bordeaux regained importance following the marriage of Duchess Eléonore of Aquitaine with the French-speaking Count Henri Plantagenet, born in Le Mans, who became, within months of their wedding, King Henry II of England. The city flourished, primarily due to the wine trade, and the cathedral of St. André was built. It was also the capital of an independent state under Edward, the Black Prince (1362–1372), but in the end, after the Battle of Castillon (1453), it was annexed by France which extended its territory. The "Château Trompette" (Trumpet Castle) and the "Fort du Hâ", built by Charles VII of France, were the symbols of the new domination, which however deprived the city of its wealth by halting the wine commerce with England.
In 1462, Bordeaux obtained a parliament, but regained importance only in the 16th century when it became the center of the distribution of sugar and slaves from the West Indies along with the traditional wine.
Bordeaux adhered to the Fronde, being effectively annexed to the Kingdom of France only in 1653, when the army of Louis XIV entered the city.
The 18th century was the golden age of Bordeaux. Many downtown buildings (about 5,000), including those on the quays, are from this period. Victor Hugo found the town so beautiful he once said: "Take Versailles, add Antwerp, and you have Bordeaux". Baron Haussmann, a long-time prefect of Bordeaux, used Bordeaux's 18th-century large-scale rebuilding as a model when he was asked by Emperor Napoleon III to transform a then still quasi-medieval Paris into a "modern" capital that would make France proud.
In 1814, towards the end of the Peninsula war, the duke of Wellington sent William Beresford with two divisions, who seized Bordeaux without much resistance on 12 March. Bordeaux was largely anti-Bonapartist and had a majority that supported the Bourbons, so the British troops were treated as liberators.
In 1870, at the beginning of the Franco-Prussian war against Prussia, the French government temporary relocated to Bordeaux from Paris. This happened again during the First World War and again very briefly during the Second World War, when it became clear that Paris would soon fall into German hands. However, on the last of these occasions the French capital was soon moved again to Vichy. In May and June 1940, Bordeaux was the site of the life-saving actions of the Portuguese consul-general, Aristides de Sousa Mendes, who illegally granted thousands of Portuguese visas, which were needed to pass the Spanish border, to refugees fleeing the German Occupation.
From 1940 to 1943, the Italian Royal Navy ("Regia Marina Italiana") established BETASOM, a submarine base at Bordeaux. Italian submarines participated in the Battle of the Atlantic from this base, which was also a major base for German U-boats as headquarters of 12th U-boat Flotilla. The massive, reinforced concrete U-boat pens have proved impractical to demolish and are now partly used as a cultural center for exhibitions.
Geography.
Bordeaux is located close to the European Atlantic coast, in the southwest of France and in the north of the Aquitaine region. It is around southwest of Paris. The city is built on a bend of the river Garonne, and is divided into two parts: the right bank to the east and left bank in the west. Historically the left bank is more developed because when flowing outside the bend, the water makes a furrow of the required depth to allow the passing of merchant ships, which used to offload on this side of the river. In Bordeaux, the Garonne River is accessible to ocean liners. The right bank of the Garonne is a low-lying, often marshy plain.
Climate.
Bordeaux's climate is usually classified as an oceanic climate (Köppen climate classification "Cfb"); however, the summers tend to be warmer and the winters milder than most areas of similar classification. Substantial summer rainfall prevents its climate from being classified as Mediterranean.
Winters are cool because of the prevalence of westerly winds from the Atlantic. Summers are warm and long due to the influence from the Bay of Biscay (surface temperature reaches . The average seasonal winter temperature is , but recent winters have been warmer than this. Frosts in the winter are commonplace, occurring several times during a winter, but snowfall is very rare, occurring only once every three years. The average summer seasonal temperature is . The summer of 2003 set a record with an average temperature of .
Economy.
Bordeaux is a major centre for business in France as it has the 5th largest metropolitan population in France.
, the GDP of Bordeaux is €32.7 Billion.
Wine.
The vine was introduced to the Bordeaux region by the Romans, probably in the mid-1st century, to provide wine for local consumption, and wine production has been continuous in the region since then.
Bordeaux now has about of vineyards, 57 appellations, 10,000 wine-producing châteaux and 13,000 grape growers. With an annual production of approximately 960 million bottles, Bordeaux produces large quantities of everyday wine as well as some of the most expensive wines in the world. Included among the latter are the area's five "premier cru" (first growth) red wines (four from Médoc and one, Château Haut-Brion, from Graves), established by the Bordeaux Wine Official Classification of 1855:
The first growths are:
Both red and white wines are made in Bordeaux. Red Bordeaux is called claret in the United Kingdom. Red wines are generally made from a blend of grapes, and may be made from Cabernet Sauvignon, Merlot, Cabernet Franc, Petit verdot, Malbec, and, less commonly in recent years, Carménère. White Bordeaux is made from Sauvignon blanc, Sémillon, and Muscadelle. Sauternes is a subregion of Graves known for its intensely sweet, white, dessert wines such as Château d'Yquem.
Because of a wine glut (wine lake) in the generic production, the price squeeze induced by an increasingly strong international competition, and vine pull schemes, the number of growers has recently dropped from 14,000 and the area under vine has also decreased significantly. In the meanwhile however, the global demand for the first growths and the most famous labels markedly increased and their prices skyrocketed.
The Cité du Vin, a museum as well as a place of exhibitions, shows, movie projections and academic seminars on the theme of wine will open its doors on June 2016.
Others.
The Laser Mégajoule will be one of the most powerful lasers in the world, allowing fundamental research and the development of the laser and plasma technologies. This project, carried by the French Ministry of Defence, involves an investment of 2 billion euros. The "Road of the lasers", a major project of regional planning, promotes regional investment in optical and laser related industries leading to the Bordeaux area having the most important concentration of optical and laser expertise in Europe.
20,000 people work for the aeronautic industry in Bordeaux. The city has some of the biggest companies including Dassault, EADS Sogerma, Snecma, Thales, SNPE, and others. The Dassault Falcon private jets are built there as well as the military aircraft Rafale and Mirage 2000, the Airbus A380 cockpit, the boosters of Ariane 5, and the M51 SLBM missile.
Tourism, especially wine tourism, is a major industry.
Access to the port from the Atlantic is via the Gironde estuary. Almost 9 million tons of goods arrive and leave each year.
Population.
At the January 2011 census, there were 239,399 inhabitants in the city proper (commune) of Bordeaux. Bordeaux in its hey day had a population of 262,662 in 1968. The majority of the population is French, but there are sizable groups of Italians, Spaniards (Up to 20% of the Bordeaux population claim some degree of Spanish heritage), Portuguese, Turks, Germans..
The built-up area has grown for more than a century beyond the municipal borders of Bordeaux due to urban sprawl, so that by the January 2011 census there were 1,140,668 people living in the overall metropolitan area of Bordeaux, only a fifth of whom lived in the city proper.
Politics.
For the 2007 presidential election, the Bordelais give 31.37% of their votes to Ségolène Royal of the Socialist Party against 30.84% to Nicolas Sarkozy, president of the UMP. Then come Francois Bayrou with 22.01%, followed by Jean-Marie Le Pen which records 5.42%, none of the other candidates exceeding the 5% mark. Nationally, it is Nicolas Sarkozy who leads with 31.18% before Ségolène Royal with 25.87%, followed by François Bayrou with 18.57%. Comes after Jean-Marie Le Pen with 10.44%, none of the other candidates exceeding the 5% mark. In the second round, the city of Bordeaux place Segolene Royal 52.44% against head with 47.56% for Nicolas Sarkozy, the latter being elected President of the Republic with 53.06% against 46.94% for Ségolène Royal. The abstention amounts to Bordeaux in the first round and 14.52% in the second round to 15.90%. This is an earthquake in Bordeaux, a city deeply rooted right symbol for the latter city.
In the parliamentary elections of 2007, the left won eight constituencies against only three right. It should be added that after the partial 2008, the eighth district of Gironde will switch to the left, bringing to nine the account. In Bordeaux, the left is for the first time in its history the majority as it holds two three constituencies following the elections. In the first division of the Gironde, the outgoing UMP MP Chantal Bourragué well ahead with 44.81% against 25.39% for only the Socialist candidate Beatrice Desaigues. In the second round, it was Chantal Bourragué who was re-elected with 54.45% against 45.55% for his socialist opponent. In the second district of Gironde confront the UMP mayor and all new Minister of Ecology, Energy, Sustainable Development and the Sea Alain Juppe facing the General Counsel PS Michèle Delaunay. In the first round, Alain Juppe is well ahead with 43.73% against 31.36% for Michèle Delaunay. In the second round, it was finally Michèle Delaunay who won the election with 50.93% of the votes against 49.07% for Alain Juppé, the advance being only 670 votes. The defeat of the so-called constituency "Mayor" shows that Bordeaux rocking increasingly left. Finally, on the third constituency of the Gironde, Noël Mamère is well ahead with 39.82% against 28.42% for only the UMP candidate Elizabeth Vine. In the second round, it's Christmas Mamère is re-elected with 62.82% against 37.18% for his rival right.
In 2008 held the municipal elections of 2008 which saw the clash mayor of Bordeaux, Alain Juppe against the President of the Regional Council of Aquitaine Socialist Alain Rousset. The PS had put a Socialist heavyweight Gironde and had put great hopes in this election after the victory of Ségolène Royal and Michèle Delaunay in 2007. However, after a rather exciting campaign is Alain Juppé who was widely elected in the first round with 56.62%, far ahead of Alain Rousset who has managed to get 34.14%. At present, of the eight cantons that has Bordeaux, five and three are at the PS to the UMP, left munching at every election a little bit on the right electorate.
In the European elections of 2009, Bordeaux voters largely voted for the UMP list Dominique Baudis, who won 31.54% against 15.00% for only a list PS Kader Arif. The list of Europe Ecology José Bové has released its game arrived second with 22.34%. None of the other lists have reached the 10% mark. The 2009 European elections are like the previous ones in eight constituencies. Bordeaux is located in the district "Southwest", here are the results:
UMP list Dominique Baudis: 26.89%. His list gets four seats.
PS list Kader Arif: 17.79%. Two seats in the European Parliament.
Europe Ecology list Bove: 15.83%. Two seats were obtained.
MoDem list Robert Rochefort: 8.61%. Won a seat.
List Left Front of Jean-Luc Mélenchon: 8.16%. The last seat deserves.
At regional elections in 2010, the Socialist incumbent president Alain Rousset won the first round by totaling 35.19% in Bordeaux, but this score is lower than the Gironde and Aquitaine plan. Then arrived Xavier Darcos, Minister of Labour who got 28.40% of the votes, score above the regional and departmental average. Then come Monique De Marco, Green candidate which totals 13.40%, followed by the member of Pyrenees-Atlantiques and candidate of the Modem Jean Lassalle who registers a low 6.78% while it is qualified to the second round on the whole Aquitaine. It is closely followed by Jacques Colombier, candidate of the National Front, which gets 6.48%. Finally comes the candidate of the Left Front Gérard Boulanger which totals 5.64%, no other candidate from the 5% mark. In the second round, Alain Rousset is needed largely carried away by the wave as national totals rose 55.83%. If Xavier Darcos largely loses the election, he nevertheless achieves a score above the regional and departmental average obtaining 33.40%. Jean Lassalle, who qualified for the second round, passes the 10% mark by totaling 10.77%. The ballot is marked by abstention amounted to 55.51% in the first round and 53.59% in the second round.
"Only candidates obtaining more than 5% are listed"
Municipal Administration.
The Mayor of the city is Alain Juppe.
Bordeaux is the capital of five cantons and the Prefecture of the Gironde and Aquitaine.
The town is divided into three districts, the first three of Gironde. The headquarters of Urban Community of Bordeaux Mériadeck is located in the neighborhood and the city is at the head of the Chamber of Commerce and Industry that bears his name.
The number of inhabitants of Bordeaux is greater than 199,999 and less than 250,000 and so the number of municipal councilors is 61. They are divided according to the following composition:
Mayors of Bordeaux.
Since 1947, there have been 3 mayors of Bordeaux:
1947–1995: Jacques Chaban-Delmas (RPR)<br>
1995–2004: Alain Juppe (RPR/UMP/LR)<br>
2004–2006: Hugues Martin (UMP)<br>
2006–present: Alain Juppe (UMP/LR)
Education.
University.
The university was created by the archbishop Pey Berland in 1441 and was abolished in 1793, during the French Revolution, before reappearing in 1808 with Napoleon I. Bordeaux accommodates approximately 70,000 students on one of the largest campuses of Europe (235 ha).
The University of Bordeaux is divided into four:
Schools.
Bordeaux has numerous public and private schools offering undergraduate and postgraduate programs.
Engineering schools:
Business and management schools:
Other:
Weekend education.
The "École Compleméntaire Japonaise de Bordeaux" (ボルドー日本語補習授業校 "Borudō Nihongo Hoshū Jugyō Kō"), a part-time Japanese supplementary school, is held in the "Salle de L'Athenee Municipal" in Bordeaux.
Main sights.
Bordeaux is classified "City of Art and History". The city is home to 362 "monuments historiques" (only Paris has more in France) with some buildings dating back to Roman times. Bordeaux has been inscribed on UNESCO World Heritage List as ""an outstanding urban and architectural ensemble"".
Bordeaux is home to one of Europe's biggest 18th-century architectural urban areas, making it a sought-after destination for tourists and cinema production crews. It stands out as one of the first French cities, after Nancy, to have entered an era of urbanism and metropolitan big scale projects, with the team Gabriel father and son, architects for King Louis XV, under the supervision of two intendants (Governors), first Nicolas-François Dupré de Saint-Maur then the Marquis (Marquess) de Tourny.
Buildings.
Main sights include:
Saint-André Cathedral, Saint-Michel Basilica and Saint-Seurin Basilica are part of the World Heritage Sites of the Routes of Santiago de Compostela in France.
Parks and gardens.
"Le Jardin Public" is a park in the heart of the city.
Pont Jacques Chaban-Delmas.
Europe’s longest-span vertical-lift bridge, the Pont Jacques Chaban-Delmas, was opened in 2013 in Bordeaux, spanning the River Garonne. The central lift span is and can be lifted vertically up to to let tall ships pass underneath. The €160 million bridge was inaugurated by President François Hollande and Mayor Alain Juppé on 16 March 2013. The bridge was named after the late Jacques Chaban-Delmas, who was a former Prime Minister and Mayor of Bordeaux.
Shopping.
Bordeaux has many shopping options. In the heart of Bordeaux is "Rue Sainte-Catherine". This pedestrian-only shopping street has of shops, restaurants and cafés; it is also one of the longest shopping streets in Europe. "Rue Sainte-Catherine" starts at "Place de la Victoire" and ends at "Place de la Comédie" by the "Grand Théâtre". The shops become progressively more upmarket as one moves towards "Place de la Comédie" and the nearby "Cours de l'Intendance" is where one finds the more exclusive shops and boutiques.
Culture.
Bordeaux is also the first city in France to have created, in the 1980s, an architecture exhibition and research center, "Arc en rêve".
Bordeaux offers a large number of cinemas, theatres and is the home of the Opéra national de Bordeaux. There are many music venues of varying capacity. The city also offers several festivals throughout the year.
Transport.
Road.
Bordeaux is an important road and motorway junction. The city is connected to Paris by the A10 motorway, with Lyon by the A89, with Toulouse by the A62, and with Spain by the A63. There is a ring road called the "Rocade" which is often very busy. Another ring road is under consideration.
Bordeaux has five road bridges that cross the Garonne, the Pont de pierre built in the 1820s and three modern bridges built after 1960: the Pont Saint Jean, just south of the Pont de pierre (both located downtown), the Pont d'Aquitaine, a suspended bridge downstream from downtown, and the Pont François Mitterrand, located upstream of downtown. These two bridges are part of the ring road around Bordeaux. A fifth bridge, the Pont Jacques-Chaban-Delmas, was constructed in 2009–2012 and opened to traffic in March 2013. Located halfway between the Pont de pierre and the Pont d'Aquitaine and serving downtown rather than highway traffic, it is a vertical-lift bridge with a height comparable to the Pont de pierre in closed position, and to the Pont d'Aquitaine in open position. All five road bridges, including the two highway bridges, are open to cyclists and pedestrians as well.
Another bridge, the Pont Jean-Jacques Bosc, is to be built in 2018.
Lacking any steep hills, Bordeaux is relatively friendly to cyclists. Cycle paths (separate from the roadways) exist on the highway bridges, along the riverfront, on the university campuses, and incidentally elsewhere in the city. Cycle lanes and bus lanes that explicitly allow cyclists exist on many of the city's boulevards. A paid Bicycle sharing system with automated stations has been established in 2010.
Rail.
The main railway station, Gare de Bordeaux Saint-Jean, near the center of the city, has 4 million passengers a year. It is served by the French national (SNCF) railway's high speed train, the TGV, that gets to Paris in three hours, with connections to major European centers such as Lille, Brussels, Amsterdam, Cologne, Geneva and London. The TGV also serves Toulouse and Irun from Bordeaux. A regular train service is provided to Nantes, Nice, Marseille and Lyon. The Gare Saint-Jean is the major hub for regional trains (TER) operated by the SNCF to Arcachon, Limoges, Agen, Périgueux, Pau, Le Médoc, Angoulême and Bayonne.
Historically the train line used to terminate at a station on the right bank of the river Garonne near the Pont de Pierre, and passengers crossed the bridge to get into the city. Subsequently a double-track steel railway bridge was constructed in the 1850s, by Gustave Eiffel, to bring trains across the river direct into Gare de Bordeaux Saint-Jean. The old station was later converted and in 2010 comprised a cinema and restaurants.
The two-track Eiffel bridge with a speed limit of became a bottleneck and a new bridge was built, opening in 2009. The new bridge has 4 tracks and allows trains to pass at . During the planning there was much lobbying by the Eiffel family and other supporters to preserve the old bridge as a footbridge across the Garonne, with possibly a museum to document the history of the bridge and Gustave Eiffel's contribution. The decision was taken to save the bridge, but by early 2010 no plans had been announced as to its future use. The bridge remains intact, but unused and without any means of access.
Air.
Bordeaux is served by an international airport, Aéroport de Bordeaux Mérignac, located from the city center in the suburban city of Mérignac.
Trams, buses and boats.
Bordeaux has an important public transport system called Tram et Bus de la CUB (TBC). This company is run by the Keolis group. The network consists of:
This network is operated from 5 am to 1 am.
There had been several plans for a subway network to be set up, but they stalled for both geological and financial reasons. Work on the Tramway de Bordeaux system was started in the autumn of 2000, and services started in December 2003 connecting Bordeaux with its suburban areas. The tram system uses ground-level power supply technology (APS), a new cable-free technology developed by French company Alstom and designed to preserve the aesthetic environment by eliminating overhead cables in the historic city. Conventional overhead cables are used outside the city. The system was controversial for its considerable cost of installation, maintenance and also for the numerous initial technical problems that paralysed the network. Many streets and squares along the tramway route became pedestrian areas, with limited access for cars.
Taxi.
There are more than 400 taxicabs in Bordeaux.
Sport.
The 35,000-capacity Stade Chaban-Delmas is the largest stadium in Bordeaux. It was a venue for the FIFA World Cup in 1938 and 1998, as well as the 2007 Rugby World Cup. In the 1938 FIFA World Cup, it hosted a violent quarter-final known as the Battle of Bordeaux. The ground was formerly known as the "Stade du Parc Lescure" until 2001, when it was renamed in honour of the city's long-time mayor, Jacques Chaban-Delmas.
There are two major sport teams in Bordeaux, both playing at the Stade Chaban-Delmas. Girondins de Bordeaux is the football team, currently playing in Ligue 1 in the French football championship. Union Bordeaux Bègles is a rugby team in the Top 14 in the Ligue Nationale de Rugby.
Skateboarding, rollerblading, and BMX biking are activities enjoyed by many young inhabitants of the city. Bordeaux is home to a beautiful quai which runs along the Gironde river. On the Quai there is a skatepark divided into three sections. One section is for Vert tricks, one for street style tricks, and one for little action sports athletes with easier features and softer materials. The skatepark is very well maintained by the municipal workers.
A new stadium has been open in 2015 to replace the Chaban-Delmas stadium. It has a capacity of 42500 people.
Bordeaux is also the home of one of the strongest cricket teams in France and are the current champions of the South West League.
There is a wooden velodrome, Vélodrome du Lac, in Bordeaux which hosts international cycling competition in the form of UCI Track Cycling World Cup events.
People.
Bordeaux was the birthplace of:
International relationship.
Twin towns and sister cities.
Bordeaux is twinned with:

</doc>
<doc id="4098" url="https://en.wikipedia.org/wiki?curid=4098" title="Puzzle Bobble">
Puzzle Bobble

, also known as Bust-a-Move in North America, is a 1994 tile-matching arcade puzzle video game for one or two players created by Taito Corporation. It is based on Taito's popular 1986 arcade game "Bubble Bobble", featuring characters and themes from that game. Its characteristically cute Japanese animation and music, along with its play mechanics and level designs, made it successful as an arcade title and spawned several sequels and ports to home gaming systems.
Versions.
Two different versions of the original game were released. "Puzzle Bobble" was originally released in Japan only in June 1994 by Taito Corporation, running on Taito's B System hardware (with the preliminary title "Bubble Buster"). Then, 6 months later in December, the international Neo Geo version of "Puzzle Bobble" was released. It was almost identical aside from being in stereo and having some different sound effects and translated text.
When set to the US region, the Neo Geo version displays the alternative title "Bust a Move" and features anti-drugs and anti-littering messages in the title sequence. The Bust-a-Move title was used for all subsequent games in the series in the United States and Canada, as well as for some (non-Taito published) console releases in Europe.
Gameplay.
At the start of each round, the rectangular playing arena contains a prearranged pattern of coloured "bubbles". (These are actually referred to in the translation as "balls"; however, they were clearly intended to be bubbles, since they pop, and are taken from "Bubble Bobble".) At the bottom of the screen, the player controls a device called a "pointer", which aims and fires bubbles up the screen. The colour of bubbles fired is randomly generated and chosen from the colors of bubbles still left on the screen.
The fired bubbles travel in straight lines (possibly bouncing off the side walls of the arena), stopping when they touch other bubbles or reach the top of the arena. If a bubble touches identically-colored bubbles, forming a group of three or more, those bubbles—as well as any bubbles hanging from them—are removed from the field of play, and points are awarded.
After every few shots, the "ceiling" of the playing arena drops downwards slightly, along with all the bubbles stuck to it. The number of shots between each drop of the ceiling is influenced by the number of bubble colors remaining. The closer the bubbles get to the bottom of the screen, the faster the music plays and if they cross the line at the bottom then the game is over.
The objective of the game is to clear all the bubbles from the arena without any bubble crossing the bottom line. Bubbles will fire automatically if the player remains idle. After clearing the arena, the next round begins with a new pattern of bubbles to clear. The game consists of 32 levels.
Scoring system.
As with many popular arcade games, experienced players (who can complete the game relatively easily) become much more interested in the secondary challenge of obtaining a high score (which involves a lot more skill and strategy). "Puzzle Bobble" caters to this interest very well, featuring an exponential scoring system which allows extremely high scores to be achieved.
"Popped" bubbles (that is, bubbles of the same color which disappear) are worth 10 points each. However, "dropped" bubbles (that is, bubbles that were hanging from popped bubbles), are worth far more: one dropped bubble scores 20 points; two score 40; three score 80. This figure continues doubling for each bubble dropped, up to 17 or more bubbles which scores 1,310,720 points. It is possible to achieve this maximum on most rounds (sometimes twice or more), resulting in a potential total score of 30 million and beyond.
Bonus points are also awarded for completing a round quickly. The maximum 50,000-point bonus is awarded for clearing a round in 5 seconds or less; this bonus then drops down to zero over the next minute, after which no bonus is awarded.
Two player mode.
There are no rounds in the two player game. Both players have an arena each (both visible on screen) and an identical arrangement of colored bubbles in each arena. When a player removes a large group (four bubbles or more) some of those removed are transferred to the opponent's arena, usually delaying their efforts to remove all the bubbles from their individual arena. In some versions, the two player game can also be played by one player against a computer opponent.
Connections with Bubble Bobble.
The characters and theme of the game are based on the 1986 platform arcade game "Bubble Bobble". An arrangement of the original "Bubble Bobble" BGM is played in the game's end credits.
The two dinosaurs operating the pointer are called "Bub" and "Bob" (or "Bubblun" and "Bobblun" in Japan). Their graphics and animation are based directly on the original "Bubble Bobble", only larger (very similar to Bubble Symphony which was released less than a month later). Less obvious is the fact that "Puzzle Bobble" also features all the enemies from "Bubble Bobble", which are trapped inside the bubbles and fly out when the bubbles pop. Inspecting the bubbles closely, one can see the enemies twitching inside the bubbles.
Reception.
"Electronic Gaming Monthly" gave the Super NES version a 7.8 out of 10, calling it "a thoroughly enjoyable and incredibly addicting puzzle game". They considered the two player mode the highlight, but also said that the one player mode provides a solid challenge. "GamePro" gave it a generally negative review, saying it "starts out fun but ultimately lacks intricacy and longevity." They elaborated that in one player mode all the levels feel the same, and that two player matches are over too quickly to build up any excitement. They also criticized the lack of any 3D effects in the graphics.

</doc>
<doc id="4099" url="https://en.wikipedia.org/wiki?curid=4099" title="Bone">
Bone

A bone is a rigid organ that constitutes part of the vertebral skeleton. Bones support and protect the various organs of the body, produce red and white blood cells, store minerals and also enable mobility. Bone tissue is a type of dense connective tissue. Bones come in a variety of shapes and sizes and have a complex internal and external structure. They are lightweight yet strong and hard, and serve multiple functions. Mineralized osseous tissue, or bone tissue, is of two types, cortical and cancellous, and gives a bone rigidity and a coral-like three-dimensional internal structure. Other types of tissue found in bones include marrow, endosteum, periosteum, nerves, blood vessels and cartilage.
Bone is an active tissue composed of different types of bone cells. Osteoblasts are involved in the creation and mineralisation of bone; osteocytes and osteoclasts are involved in the reabsorption of bone tissue. The mineralised matrix of bone tissue has an organic component mainly of collagen and an inorganic component of bone mineral made up of various salts.
In the human body at birth, there are over 270 bones, but many of these fuse together during development, leaving a total of 206 separate bones in the adult, not counting numerous small sesamoid bones. The largest bone in the body is the thigh-bone (femur) and the smallest is the stapes in the middle ear.
Structure.
Bone is not a uniformly solid material, but is mostly a matrix. The primary tissue of bone, bone tissue (osseous tissue), is relatively hard and lightweight. Its matrix is mostly made up of a composite material incorporating the inorganic mineral calcium phosphate in the chemical arrangement termed calcium hydroxylapatite (this is the bone mineral that gives bones their rigidity) and collagen, an elastic protein which improves fracture resistance. Bone is formed by the hardening of this matrix around entrapped cells. When these cells become entrapped from osteoblasts they become osteocytes.
Layered structure.
Cortical bone.
The hard outer layer of bones is composed of cortical bone also called compact bone. Cortical referring to the outer (cortex) layer. The hard outer layer gives bone its smooth, white, and solid appearance, and accounts for 80% of the total bone mass of an adult human skeleton. However, that proportion may be much lower, especially in marine mammals and marine turtles, or in various Mesozoic marine reptiles, such as ichthyosaurs, among others.
Cortical bone consists of multiple microscopic columns, each called an osteon. Each column is multiple layers of osteoblasts and osteocytes around a central canal called the Haversian canal. Volkmann's canals at right angles connect the osteons together. The columns are metabolically active, and as bone is reabsorbed and created the nature and location of the cells within the osteon will change. Cortical bone is covered by a periosteum on its outer surface, and an endosteum on its inner surface. The endosteum is the boundary between the cortical bone and the cancellous bone. 
Cancellous bone.
Filling the interior of the bone is the cancellous bone also known as trabecular or spongy bone tissue. It is an open cell porous network. Thin formations of osteoblasts covered in endosteum create an irregular network of spaces. Within these spaces are bone marrow and hematopoietic stem cells that give rise to platelets, red blood cells and white blood cells. Trabecular marrow is composed of a network of rod- and plate-like elements that make the overall organ lighter and allow room for blood vessels and marrow. Trabecular bone accounts for the remaining 20% of total bone mass but has nearly ten times the surface area of compact bone.
Bone marrow.
Bone marrow, also known as myeloid tissue, can be found in almost any bone that holds cancellous tissue. In newborns, all such bones are filled exclusively with red marrow, but as the child ages it is mostly replaced by yellow, or "fatty" marrow. In adults, red marrow is mostly found in the bone marrow of the femur, the ribs, the vertebrae and pelvic bones.
Composition.
Cells.
Bone is a metabolically active tissue composed of several types of cells. These cells include osteoblasts, which are involved in the creation and mineralization of bone tissue, osteocytes, and osteoclasts, which are involved in the reabsorption of bone tissue. Osteoblasts and osteocytes are derived from osteoprogenitor cells, but osteoclasts are derived from the same cells that differentiate to form macrophages and monocytes. Within the marrow of the bone there are also hematopoietic stem cells. These cells give rise to other cells, including white blood cells, red blood cells, and platelets.
Extracellular.
Bones consist of living cells embedded in a mineralized organic matrix. This matrix consists of organic components, mainly collagen – "organic" referring to materials produced as a result of the human body – and inorganic components, primarily hydroxyapatite and other salts of calcium and phosphate. Above 30% of the acellular part of bone consists of the organic components, and 70% of salts. The strands of collagen give bone its tensile strength, and the interspersed crystals of hydroxyapatite give bone its compressional strength. These effects are synergistic.
The inorganic composition of bone (bone mineral) is primarily formed from salts of calcium and phosphate, the major salt being hydroxyapatite (Ca10(PO4)6(OH)2). The exact composition of the matrix may change over time and with nutrition, with the ratio of calcium to phosphate varying between 1.3 and 2.0 (per weight), and trace minerals such as magnesium, sodium, potassium and carbonate also being found.
The organic part of matrix is mainly composed of Type I collagen. Collagen composes 90–95% of the organic matrix, with remainder of the matrix being a homogenous liquid called ground substance consisting of proteoglycans such as hyaluronic acid and chondroitin sulfate. Collagen consists of strands of repeating units, which give bone tensile strength, and are arranged in an overlapping fashion that prevents sheer stress. The function of ground substance is not fully known. Two types of bone can be identified microscopically according to the arrangement of collagen:
Woven bone is produced when osteoblasts produce osteoid rapidly, which occurs initially in all fetal bones, but is later replaced by more resilient lamellar bone. In adults woven bone is created after fractures or in Paget's disease. Woven bone is weaker, with a smaller number of randomly oriented collagen fibers, but forms quickly; it is for this appearance of the fibrous matrix that the bone is termed "woven". It is soon replaced by lamellar bone, which is highly organized in concentric sheets with a much lower proportion of osteocytes to surrounding tissue. Lamellar bone, which makes its first appearance in humans in the fetus during the third trimester, is stronger and filled with many collagen fibers parallel to other fibers in the same layer (these parallel columns are called osteons). In cross-section, the fibers run in opposite directions in alternating layers, much like in plywood, assisting in the bone's ability to resist torsion forces. After a fracture, woven bone forms initially and is gradually replaced by lamellar bone during a process known as "bony substitution." Compared to woven bone, lamellar bone formation takes place more slowly. The orderly deposition of collagen fibers restricts the formation of osteoid to about 1 to 2 µm per day. Lamellar bone also requires a relatively flat surface to lay the collagen fibers in parallel or concentric layers.
Deposition.
The extracellular matrix of bone is laid down by osteoblasts, which secrete both collagen and ground substance. These synthesise collagen within the cell, and then secrete collagen fibrils. The collagen fibres rapidly polymerise to form collagen strands. At this stage they are not yet mineralised, and are called "osteoid". Around the strands calcium and phosphate precipitate on the surface of these strands, within a days to weeks becoming crystals of hydroxyapatite.
In order to mineralise the bone, the osteoblasts secrete vesicles containing alkaline phosphatase. This cleaves the phosphate groups and acts as the foci for calcium and phosphate deposition. The vesicles then rupture and act as a centre for crystals to grow on. More particularly, bone mineral is formed from globular and plate structures.
Types.
There are five types of bones in the human body: long, short, flat, irregular, and sesamoid.
Terminology.
In the study of anatomy, anatomists use a number of anatomical terms to describe the appearance, shape and function of bones. Other anatomical terms are also used to describe the location of bones. Like other anatomical terms, many of these derive from Latin and Greek. Some anatomists still use Latin to refer to bones. The term "osseous", and the prefix "osteo-", referring to things related to bone, are still used commonly today.
Some examples of terms used to describe bones include the term "foramen" to describe a hole through which something passes, and a "canal" or "meatus" to describe a tunnel-like structure. A protrusion from a bone can be called a number of terms, including a "condyle", "crest", "spine", "eminence", "tubercle" or "tuberosity", depending on the protrusion's shape and location. In general, long bones are said to have a "head", "neck", and "body".
When two bones join together, they are said to "articulate". If the two bones have a fibrous connection and are relatively immobile, then the joint is called a "suture".
Development.
The formation of bone is called ossification. During the fetal stage of development this occurs by two processes, Intramembranous ossification and endochondral ossification. Intramembranous ossification involves the creation of bone from connective tissue, whereas in the process of endochondral ossification bone is created from cartilage.
Intramembranous ossification.
Intramembranous ossification mainly occurs during formation of the flat bones of the skull but also the mandible, maxilla, and clavicles; the bone is formed from connective tissue such as mesenchyme tissue rather than from cartilage. The steps in intramembranous ossification are:
Endochondral ossification.
Endochondral ossification, on the other hand, occurs in long bones and most of the rest of the bones in the body; it involves an initial hyaline cartilage that continues to grow. The steps in endochondral ossification are:
Endochondral ossification begins with points in the cartilage called "primary ossification centers." They mostly appear during fetal development, though a few short bones begin their primary ossification after birth. They are responsible for the formation of the diaphyses of long bones, short bones and certain parts of irregular bones. Secondary ossification occurs after birth, and forms the epiphyses of long bones and the extremities of irregular and flat bones. The diaphysis and both epiphyses of a long bone are separated by a growing zone of cartilage (the epiphyseal plate). When the child reaches skeletal maturity (18 to 25 years of age), all of the cartilage is replaced by bone, fusing the diaphysis and both epiphyses together (epiphyseal closure). In the upper limbs, only the diaphyses of the long bones and scapula are ossified. The epiphyses, carpal bones, coracoid process, medial border of the scapula, and acromion are still cartilaginous.
The following steps are followed in the conversion of cartilage to bone:
Function.
Bones have a variety of functions:
Mechanical.
Bones serve a variety of mechanical functions. Together the bones in the body form the skeleton. They provide a frame to keep the body supported, and an attachment point for skeletal muscles, tendons, ligaments and joints, which function together to generate and transfer forces so that individual body parts or the whole body can be manipulated in three-dimensional space (the interaction between bone and muscle is studied in biomechanics).
Bones protect internal organs, such as the skull protecting the brain or the ribs protecting the heart and lungs. Because of the way that bone is formed, bone has a high compressive strength of about 170 MPa (1800 kgf/cm²), poor tensile strength of 104–121 MPa, and a very low shear stress strength (51.6 MPa). This means that bone resists pushing(compressional) stress well, resist pulling(tensional) stress less well, but only poorly resists shear stress (such as due to torsional loads). While bone is essentially brittle, bone does have a significant degree of elasticity, contributed chiefly by collagen.
Mechanically, bones also have a special role in hearing. The ossicles are three small bones in the middle ear which are involved in sound transduction.
Synthetic.
Cancellous bones contain bone marrow. Bone marrow produces blood cells in a process called hematopoiesis. Blood cells that are created in bone marrow include red blood cells, platelets and white blood cells. Progenitor cells such as the hematopoietic stem cell divide in a process called mitosis to produce precursor cells. These include precursors which eventually give rise to white blood cells, and erythroblasts which give rise to red blood cells. Unlike red and white blood cells, created by mitosis, platelets are shed from very large cells called megakaryocytes. This process of progressive differentiation occurs within the bone marrow. After the cells are matured, they enter the circulation. Every day, over 2.5 billion red blood cells and platelets, and 50–100 billion granulocytes are produced in this way.
As well as creating cells, bone marrow is also one of the major sites where defective or aged red blood cells are destroyed.
Remodeling.
Bone is constantly being created and replaced in a process known as remodeling. This ongoing turnover of bone is a process of resorption followed by replacement of bone with little change in shape. This is accomplished through osteoblasts and osteoclasts. Cells are stimulated by a variety of signals, and together referred to as a remodeling unit. Approximately 10% of the skeletal mass of an adult is remodelled each year. The purpose of remodeling is to regulate calcium homeostasis, repair microdamaged bones from everyday stress, and also to shape and sculpt the skeleton during growth.. Repeated stress, such as weight-bearing exercise or bone healing, results in the bone thickening at the points of maximum stress (Wolff's law). It has been hypothesized that this is a result of bone's piezoelectric properties, which cause bone to generate small electrical potentials under stress.
The action of osteoblasts and osteoclasts are controlled by a number of chemical enzymes that either promote or inhibit the activity of the bone remodeling cells, controlling the rate at which bone is made, destroyed, or changed in shape. The cells also use paracrine signalling to control the activity of each other. For example, the rate at which osteoclasts resorb bone is inhibited by calcitonin and osteoprotegerin. Calcitonin is produced by parafollicular cells in the thyroid gland, and can bind to receptors on osteoclasts to directly inhibit osteoclast activity. Osteoprotegerin is secreted by osteoblasts and is able to bind RANK-L, inhibiting osteoclast stimulation.
Osteoblasts can also be stimulated to increase bone mass through increased secretion of osteoid and by inhibiting the ability of osteoclasts to break down osseous tissue. Increased secretion of osteoid is stimulated by the secretion of growth hormone by the pituitary, thyroid hormone and the sex hormones (estrogens and androgens). These hormones also promote increased secretion of osteoprotegerin. Osteoblasts can also be induced to secrete a number of cytokines that promote reabsorbtion of bone by stimulating osteoclast activity and differentiation from progenitor cells. Vitamin D, parathyroid hormone and stimulation from osteocytes induce osteoblasts to increase secretion of RANK-ligand and interleukin 6, which cytokines then stimulate increased reabsorption of bone by osteoclasts. These same compounds also increase secretion of macrophage colony-stimulating factor by osteoblasts, which promotes the differentiation of progenitor cells into osteoclasts, and decrease secretion of osteoprotegerin.
Bone volume.
Bone volume is determined by the rates of bone formation and bone resorption. Recent research has suggested that certain growth factors may work to locally alter bone formation by increasing osteoblast activity. Numerous bone-derived growth factors have been isolated and classified via bone cultures. These factors include insulin-like growth factors I and II, transforming growth factor-beta, fibroblast growth factor, platelet-derived growth factor, and bone morphogenetic proteins. Evidence suggests that bone cells produce growth factors for extracellular storage in the bone matrix. The release of these growth factors from the bone matrix could cause the proliferation of osteoblast precursors. Essentially, bone growth factors may act as potential determinants of local bone formation. Research has suggested that trabecular bone volume in postemenopausal osteoporosis may be determined by the relationship between the total bone forming surface and the percent of surface resorption.
Clinical significance.
A number of diseases can affect bone, including arthritis, fractures, infections, osteoporosis and tumours. Conditions relating to bone can be managed by a variety of doctors, including rheumatologists for joints, and orthopedic surgeons, who may conduct surgery to fix broken bones. Other doctors, such as rehabilitation specialists may be involved in recovery, radiologists in interpreting the findings on imaging, and pathologists in investigating the cause of the disease, and family doctors may play a role in preventing complications of bone disease such as osteoporosis.
When a doctor sees a patient, a history and exam will be taken. Bones are then often imaged, called radiography. This might include ultrasound X-ray, CT scan, MRI scan and other imaging such as a Bone scan, which may be used to investigate cancer. Other tests such as a blood test for autoimmune markers may be taken, or a synovial fluid aspirate may be taken.
Fractures.
In normal bone, fractures occur when there is significant force applied, or repetitive trauma over a long time. Fractures can also occur when a bone is weakened, such as with osteoporosis, or when there is a structural problem, such as when the bone remodels excessively (such as Paget's disease) or is the site of the growth of cancer. Common fractures include wrist fractures and hip fractures, associated with osteoporosis, vertebral fractures associated with high-energy trauma and cancer, and fractures of long-bones. Not all fractures are painful. When serious, depending on the fractures type and location, complications may include flail chest, compartment syndromes or fat embolism.
Compound fractures involve the bone's penetration through the skin.
Fractures and their underlying causes can be investigated by X-rays, CT scans and MRIs. Fractures are described by their location and shape, and several classification systems exist, depending on the location of the fracture. Fractures in children are described with the Salter–Harris fracture. When fractures are managed, pain relief is often given, and the fractured area is often immobilised. This is to promote bone healing. In addition, surgical measures such as internal fixation may be used. Because of the immobilisation, people with fractures are often advised to undergo rehabilitation.
Tumours.
There are several types of tumour that can affect bone; examples of benign bone tumours include osteoma, osteoid osteoma, osteochondroma, osteoblastoma, enchondroma, giant cell tumor of bone, aneurysmal bone cyst, and fibrous dysplasia of bone.
Cancer.
Cancer can arise in bone tissue, and bones are also a common site for other cancers to spread (metastasise) to. Cancers that arise in bone are called "primary" cancers, although such cancers are rare. Metastases within bone are "secondary" cancers, with the most common being breast cancer, lung cancer, prostate cancer, thyroid cancer, and kidney cancer. Secondary cancers that affect bone can either destroy bone (called a "lytic" cancer) or create bone (a "sclerotic" cancer). Cancers of the bone marrow inside the bone can also affect bone tissue, examples including leukemia and multiple myeloma. Bone may also be affected by cancers in other parts of the body. Cancers in other parts of the body may release parathyroid hormone or parathyroid hormone-related peptide. This increases bone reabsorption, and can lead to bone fractures.
Bone tissue that is destroyed or altered as a result of cancers is distorted, weakened, and more prone to fracture. This may lead to compression of the spinal cord, destruction of the marrow resulting in bruising, bleeding and immunosuppression, and is one cause of bone pain. If the cancer is metastatic, then there might be other symptoms depending on the site of the original cancer. Some bone cancers can also be felt.
Cancers of the bone are managed according to their type, their stage, prognosis, and what symptoms they cause. Many primary cancers of bone are treated with radiotherapy. Cancers of bone marrow may be treated with chemotherapy, and other forms of targeted therapy such as immunotherapy may be used. Palliative care, which focuses on maximising a person's quality of life, may play a role in management, particularly if the likelihood of survival within five years is poor.
Osteoporosis.
Osteoporosis is a disease of bone where there is reduced bone mineral density, increasing the likelihood of fractures. Osteoporosis is defined by the World Health Organization in women as a bone mineral density 2.5 standard deviations below peak bone mass, relative to the age and sex-matched average, as measured by Dual energy X-ray absorptiometry, with the term "established osteoporosis" including the presence of a fragility fracture. Osteoporosis is most common in women after menopause, when it is called "postmenopausal osteoporosis", but may develop in men and premenopausal women in the presence of particular hormonal disorders and other chronic diseases or as a result of smoking and medications, specifically glucocorticoids. Osteoporosis usually has no symptoms until a fracture occurs. For this reason, DEXA scans are often done in people with one or more risk factors, who have developed osteoporosis and be at risk of fracture.
Osteoporosis treatment includes advice to stop smoking, decrease alcohol consumption, exercise regularly, and have a healthy diet. Calcium supplements may also be advised, as may Vitamin D. When medication is used, it may include bisphosphonates, Strontium ranelate, and osteoporosis may be one factor considered when commencing Hormone replacement therapy.
Osteology.
The study of bones and teeth is referred to as osteology. It is frequently used in anthropology, archeology and forensic science for a variety of tasks. This can include determining the nutritional, health, age or injury status of the individual the bones were taken from. Preparing fleshed bones for these types of studies can involve the process of maceration.
Typically anthropologists and archeologists study bone tools made by "Homo sapiens" and "Homo neanderthalensis". Bones can serve a number of uses such as projectile points or artistic pigments, and can also be made from external bones such as antlers.
Other animals.
Bird skeletons are very lightweight. Their bones are smaller and thinner, to aid flight. Among mammals, bats come closest to birds in terms of bone density, suggesting that small dense bones are a flight adaptation. Many bird bones have little marrow due to their being hollow.
A bird's beak is primarily made of bone as projections of the mandibles which are covered in keratin.
A deer's antlers are composed of bone which is an unusual example of bone being outside the body.
The extinct predatory fish "Dunkleosteus" had sharp edges of hard exposed bone along its jaws.
Many animals possess an exoskeleton that is not made of bone, These include insects and crustaceans.
Society and culture.
Bones from slaughtered animals have a number of uses. In prehistoric times, they have been used for making bone tools. They have further been used in bone carving, already important in prehistoric art, and also in modern time as crafting materials for buttons, beads, handles, bobbins, calculation aids, head nuts, dice, poker chips, pick-up sticks, ornaments, etc. A special genre is scrimshaw.
Bone glue can be made by prolonged boiling of ground or cracked bones, followed by filtering and evaporation to thicken the resulting fluid. Historically once important, bone glue and other animal glues today have only a few specialized uses, such as in antiques restoration. Essentially the same process, with further refinement, thickening and drying, is used to make gelatin.
Broth is made by simmering several ingredients for a long time, traditionally including bones.
Ground bones are used as an organic phosphorus-nitrogen fertilizer and as additive in animal feed. Bones, in particular after calcination to bone ash, are used as source of calcium phosphate for the production of bone china and previously also phosphorus chemicals.
Bone char, a porous, black, granular material primarily used for filtration and also as a black pigment, is produced by charring mammal bones.
Oracle bone script was a writing system used in Ancient china based on inscriptions in bones.
To point the bone at someone is considered bad luck in some cultures, such as Australian aborigines, such as by the Kurdaitcha.
Osteopathic medicine is a school of medical thought originally developed based on the idea of the link between the musculoskeletal system and overall health, but now very similar to mainstream medicine. , over 77,000 physicians in the United States are trained in Osteopathic medicine colleges.
The wishbones of fowl have been used for divination, and are still customarily used in a tradition to determine which one of two people pulling on either prong of the bone may make a wish.
Various cultures throughout history have adopted the custom of shaping an infant's head by the practice of artificial cranial deformation. A widely practised
custom in China was that of foot binding to limit the normal growth of the foot.

</doc>
<doc id="4100" url="https://en.wikipedia.org/wiki?curid=4100" title="Bretwalda">
Bretwalda

Bretwalda (also brytenwalda and bretenanwealda) is an Old English word, the first record of which comes from the late 9th century "Anglo-Saxon Chronicle". It is given to some of the rulers of Anglo-Saxon kingdoms from the 5th century onwards who had achieved overlordship of some or all of the other Anglo-Saxon kingdoms. It is unclear whether the word dates back to the 5th century and was used by the kings themselves, or whether it is a later, 9th-century, invention. The term "bretwalda" also appears in a charter of Æthelstan.
The rulers of Mercia were generally the most powerful of the Anglo-Saxon kings from the mid-7th to the early 9th centuries, but are not accorded the title of bretwalda by the "Chronicle", which is generally thought to be because of the anti-Mercian bias of the Chroniclers. The "Annals of Wales" continued to recognise the kings of Northumbria as 'Kings of the Saxons' until the death of Osred I of Northumbria in 716.
Etymology.
The first syllable of the term "bretwalda" may be related to 'Briton' or 'Britain' and would thus mean 'sovereign of Britain' or 'wielder of Britain'. The word may be a compound containing the Old English adjective "brytten" (from the verb "breotan" meaning 'to break' or 'to disperse'), an element also found in the terms "bryten rice" ('kingdom'), "bryten-grund" ('the wide expanse of the earth') and "bryten cyning" ('king whose authority was widely extended'). Though the origin is ambiguous, the draughtsman of the charter issued by Æthelstan used the term in a way that can only mean 'wide ruler'.
The latter etymology was first suggested by John Mitchell Kemble who alluded that "of six manuscripts in which this passage occurs, one only reads "Bretwalda": of the remaining five, four have "Bryten-walda" or "-wealda", and one "Breten-anweald", which is precisely synonymous with Brytenwealda"; that Æthelstan was called "brytenwealda ealles ðyses ealondes", which Kemble translates as "ruler of all these islands"; and that "bryten-" is a common prefix to words meaning 'wide or general dispersion' and that the similarity to the word "bretwealh" ('Briton') is "merely accidental".
Contemporary use.
The first recorded use of the term "Bretwalda" comes from a West Saxon chronicle of the late 9th century that applied the term to Ecgberht, who ruled Wessex from 802 to 839. The chronicler also wrote down the names of seven kings that Bede listed in his "Historia ecclesiastica gentis Anglorum" in 731. All subsequent manuscripts of the "Chronicle" use the term "Brytenwalda", which may have represented the original term or derived from a common error.
There is no evidence that the term was a title that had any practical use, with implications of formal rights, powers and office, or even that it had any existence before the 9th-century. Bede wrote in Latin and never used the term and his list of kings holding "imperium" should be treated with caution, not least in that he overlooks kings such as Penda of Mercia, who clearly held some kind of dominance during his reign. Similarly, in his list of bretwaldas, the West Saxon chronicler ignored such Mercian kings as Offa.
The use of the term "Bretwalda" was the attempt by a West Saxon chronicler to make some claim of West Saxon kings to the whole of Great Britain. The concept of the overlordship of the whole of Britain was at least recognised in the period, whatever was meant by the term. Quite possibly it was a survival of a Roman concept of "Britain": it is significant that, while the hyperbolic inscriptions on coins and titles in charters often included the title "rex Britanniae", when England was unified the title used was "rex Angulsaxonum", ('king of the Anglo-Saxons'.)
Modern interpretation by historians.
For some time the existence of the word "bretwalda" in the "Anglo-Saxon Chronicle", which was based in part on the list given by Bede in his "Historia Ecclesiastica", led historians to think that there was perhaps a 'title' held by Anglo-Saxon overlords. This was particularly attractive as it would lay the foundations for the establishment of an English monarchy. The 20th-century historian Frank Stenton said of the Anglo-Saxon chronicler that "his inaccuracy is more than compensated by his preservation of the English title applied to these outstanding kings". He argued that the term "bretwalda" "falls into line with the other evidence which points to the Germanic origin of the earliest English institutions".
Over the later 20th century this assumption was increasingly challenged. Patrick Wormald interpreted it as "less an objectively realized office than a subjectively perceived status" and emphasised the partiality of its usage in favour of Southumbrian rulers. In 1991, Steven Fanning argued that "it is unlikely that the term ever existed as a title or was in common usage in Anglo-Saxon England". The fact that Bede never mentioned a special title for the kings in his list implies that he was unaware of one. In 1995, Simon Keynes observed that "if Bede's concept of the Southumbrian overlord, and the chronicler's concept of the 'Bretwalda', are to be regarded as artificial constructs, which have no validity outside the context of the literary works in which they appear, we are released from the assumptions about political development which they seem to involve... we might ask whether kings in the eighth and ninth centuries were quite so obsessed with the establishment of a pan-Southumbrian state".
Modern interpretations view the concept of bretwaldaship as complex and an important indicator of how a 9th-century chronicler interpreted history and attempted to insert the increasingly powerful Saxon kings into that history.
Overlordship.
A complex array of dominance and subservience existed during the Anglo-Saxon period. A king who used charters to grant land in another kingdom indicated such a relationship. If a king held sway over a large kingdom, such as when the Mercians dominated the East Anglians, the relationship would have been more equal than in the case of the Mercian dominance of the Hwicce, which was a comparatively small kingdom. Mercia was arguably the most powerful Anglo-Saxon kingdom for much of the late 7th and 8th centuries, though Mercian kings are missing from the two main 'lists'. For Bede, Mercia was a traditional enemy of his native Northumbria and he regarded powerful kings such as the pagan Penda as standing in the way of the Christian conversion of the Anglo-Saxons. Bede omits them from his list, even though it is evident that Penda held a considerable degree of power. Similarly powerful Mercia kings such as Offa are missed out of the West Saxon "Anglo-Saxon Chronicle", which sought to demonstrate the legitimacy of their kings to rule over other Anglo-Saxon peoples.

</doc>
<doc id="4101" url="https://en.wikipedia.org/wiki?curid=4101" title="Brouwer fixed-point theorem">
Brouwer fixed-point theorem

Brouwer's fixed-point theorem is a fixed-point theorem in topology, named after Luitzen Brouwer. It states that for any continuous function "f" mapping a compact convex set into itself there is a point "x"0 such that "f"("x"0) = "x"0. The simplest forms of Brouwer's theorem are for continuous functions "f" from a closed interval "I" in the real numbers to itself or from a closed disk "D" to itself. A more general form than the latter is for continuous functions from a convex compact subset "K" of Euclidean space to itself.
Among hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics.
In its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem and the Borsuk–Ulam theorem.
This gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry.
It appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Kenneth Arrow and Gérard Debreu.
The theorem was first studied in view of work on differential equations by the French mathematicians around Poincaré and Picard.
Proving results such as the Poincaré–Bendixson theorem requires the use of topological methods.
This work at the end of the 19th century opened into several successive versions of the theorem. The general case was first proved in 1910 by Jacques Hadamard and by Luitzen Egbertus Jan Brouwer.
Statement.
The theorem has several formulations, depending on the context in which it is used and its degree of generalization.
The simplest is sometimes given as follows:
This can be generalized to an arbitrary finite dimension:
A slightly more general version is as follows:
An even more general form is better known under a different name:
Importance of the pre-conditions.
The theorem holds only for sets that are "compact", i.e. bounded and closed, and "convex". The following examples show why these three requirements are important.
Boundedness.
Consider the function
which is a continuous function from "R" to itself. As it shifts every point to the right, it cannot have a fixed point. Note that "R" is convex and closed, but not bounded.
Closedness.
Consider the function
which is a continuous function from the open interval (−1,1) to itself. In this interval, it shifts every point to the right, so it cannot have a fixed point. Note that (−1,1) is convex and bounded, but not closed. The function "f" "does" have a fixed point for the closed interval [−1,1], namely "f"("x") = "x" = 1.
Convexity.
Note that convexity is not strictly necessary for BFPT. Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, BFPT is equivalent to forms in which the domain is required to be a closed unit ball "D" "n". For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).
The following example shows that BFPT doesn't work for domains with holes. Consider the following function, defined in polar coordinates:
which is a continuous function from the unit circle to itself. It rotates every point on the unit circle 45 degrees counterclockwise, so it cannot have a fixed point. Note that the unit circle is closed and bounded, but it has a hole (and so it is not convex). The function "f" "does" have a fixed point for the unit disc, since it takes the origin to itself.
A formal generalization of BFPT for "hole-free" domains can be derived from the Lefschetz fixed-point theorem.
Notes.
The continuous function in this theorem is not required to be bijective or even surjective.
Illustrations.
The theorem has several "real world" illustrations. Here are some examples.
1. Take two sheets of graph paper of equal size with coordinate systems on them, lay one flat on the table and crumple up (without ripping or tearing) the other one and place it, in any fashion, on top of the first so that the crumpled paper does not reach outside the flat one. There will then be at least one point of the crumpled sheet that lies directly above its corresponding point (i.e. the point with the same coordinates) of the flat sheet. This is a consequence of the "n" = 2 case of Brouwer's theorem applied to the continuous map that assigns to the coordinates of every point of the crumpled sheet the coordinates of the point of the flat sheet immediately beneath it.
2. Take an ordinary map of a country, and suppose that that map is laid out on a table inside that country. There will always be a "You are Here" point on the map which represents that same point in the country.
3. In three dimensions the consequence of the Brouwer fixed-point theorem is that, no matter how much you stir a cocktail in a glass, when the liquid has come to rest some point in the liquid will end up in exactly the same place in the glass as before you took any action, assuming that the final position of each point is a continuous function of its original position, and that the liquid after stirring is contained within the space originally taken up by it.
Intuitive approach.
Explanations attributed to Brouwer.
The theorem is supposed to have originated from Brouwer's observation of a cup of coffee.
If one stirs to dissolve a lump of sugar, it appears there is always a point without motion.
He drew the conclusion that at any moment, there is a point on the surface that is not moving.
The fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.
The result is not intuitive, since the original fixed point may become mobile when another fixed point appears.
Brouwer is said to have added: "I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet."
Brouwer "flattens" his sheet as with a flat iron, without removing the folds and wrinkles. This example is better than the coffee cup one as it shows that uniqueness of the fixed point may fail. This distinguishes Brouwer's result from other fixed-point theorems, such as Banach's, that guarantee uniqueness.
One-dimensional case.
In one dimension, the result is intuitive and easy to prove. The continuous function "f" is defined on a closed interval ["a", "b"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval ["a", "b"] which maps "x" to "x" (light green).
Intuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. Proof: consider the function "g" which maps "x" to "f"("x") - "x". It is ≥ 0 on "a" and ≤ 0 on "b". By the intermediate value theorem, "g" has a zero in ["a", "b"]; this zero is a fixed point.
Brouwer is said to have expressed this as follows: "Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string."
History.
The Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case "n" = 3 first was proved by Piers Bohl in 1904 (published in "Journal für die reine und angewandte Mathematik"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Methods to construct (approximations to) fixed points guaranteed by Brouwer's theorem are now known.
Prehistory.
To understand the prehistory of Brouwer's fixed point theorem one needs to pass through differential equations. At the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.
Its solution required new methods. As noted by Henri Poincaré, who worked on the three-body problem, there is no hope to find an exact solution: "Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge."
He also noted that the search for an approximate solution is no more efficient: "the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision".
He studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincaré discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincaré went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval "t". If the area is a circular band, or if it is not closed, then this is not necessarily the case.
To understand differential equations better, a new branch of mathematics was born. Poincaré called it "analysis situs". The French Encyclopædia Universalis defines it as the branch which "treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing". In 1886, Poincaré proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincaré group. This method can be used for a very compact proof of the theorem under discussion.
Poincaré's method was analogous to that of Émile Picard, a contemporary mathematician who generalized the Cauchy–Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.
First proofs.
At the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.
It was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincaré. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Poincaré, Hadamard, and Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincaré had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincaré group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. Hans Freudenthal comments on the respective roles as follows: "Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator."
Brouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincaré and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.
Reception.
The theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.
Brouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.
Besides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk–Ulam theorem says that a continuous map from the "n"-dimensional sphere to Rn has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to multivalued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.
Other areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economics, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.
Brouwer's celebrity is not exclusively due to his topological work. The proofs of his great topological theorems are not constructive, and Brouwer's dissatisfaction with this is partly what led him to articulate the idea of constructivity. He became the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. The fixed-point theorem is, as he originally stated it, "false" in intuitionism, and Brouwer disavowed it, proposing instead alternative versions to be constructively proven.
Proof outlines.
A proof using degree.
Brouwer's original 1911 proof relied on the notion of the degree of a continuous mapping. Modern accounts of the proof can also be found in the literature.
Let formula_4 denote the closed unit ball in formula_5 centered at the origin. Suppose for simplicitly that formula_6 is continuously differentiable. A regular value of formula_7 is a point formula_8 such that the Jacobian of formula_7 is non-singular at every point of the preimage of formula_10. In particular, by the inverse function theorem, every point of the preimage of formula_7 lies in formula_12 (the interior of formula_13). The degree of formula_7 at a regular value formula_8 is defined as the sum of the signs of the Jacobian determinant of formula_7 over the preimages of formula_10 under formula_7:
The degree is, roughly speaking, the number of "sheets" of the preimage "f" lying over a small open set around "p", with sheets counted oppositely if they are oppositely oriented. This is thus a generalization of winding number to higher dimensions.
The degree satisfies the property of "homotopy invariance": let formula_7 and formula_21 be two continuously differentiable functions, and formula_22 for formula_23. Suppose that the point formula_10 is a regular value of formula_25 for all "t". Then formula_26.
If there is no fixed point of the boundary of formula_13, then the function 
is a homotopy from formula_29 to the identity function. The identity function has degree one at every point. In particular, the identity function has degree one at the origin, so formula_21 also has degree one at the origin. As a consequence, the preimage formula_31 is not empty. The elements of formula_31 are precisely the fixed points of the original function "f".
This requires some work to make fully general. The definition of degree must be extended to singular values of "f", and then to continuous functions. The more modern advent of homology theory simplifies the construction of the degree, and so has become a standard proof in the literature.
A proof using homology.
The proof uses the observation that the boundary of "D" "n" is "S" "n" − 1, the ("n" − 1)-sphere.
The argument proceeds by contradiction, supposing that a continuous function "f" : "D" "n" → "D" "n" has "no" fixed point, and then attempting to derive an inconsistency, which proves that the function must in fact have a fixed point. For each "x" in "D" "n", there is only one straight line that passes through "f"("x") and "x", because it must be the case that "f"("x") and "x" are distinct by hypothesis (recall that "f" having no fixed points means that "f"("x") ≠ "x"). Following this line from "f"("x") through "x" leads to a point on "S" "n" − 1, denoted by "F"("x"). This defines a continuous function "F" : "D" "n" → "S" "n" − 1, which is a special type of continuous function known as a retraction: every point of the codomain (in this case "S" "n" − 1) is a fixed point of the function.
Intuitively it seems unlikely that there could be a retraction of "D" "n" onto "S" "n" − 1, and in the case "n" = 1 it is obviously impossible because "S" 0 (i.e., the endpoints of the closed interval "D" 1) is not even connected. The case "n" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce an injective group homomorphism from the fundamental group of "S" 1 to that of "D" 2, but the first group is isomorphic to Z while the latter group is trivial, so this is impossible. The case "n" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.
For "n" > 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology "H""n" − 1("D" "n") is trivial, while "H""n" − 1("S" "n" − 1) is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.
A proof using Stokes's theorem.
To prove that a map has fixed points, one can assume that it is smooth, because if a map has no fixed points then convolving it with a smooth function of sufficiently small support produces a smooth function with no fixed points. As in the proof using homology, one is reduced to proving that there is no smooth retraction "f" from the ball "B" onto its boundary "∂B". If ω is a volume form on the boundary then by Stokes Theorem,
giving a contradiction.
More generally, this shows that there is no smooth retraction from any non-empty smooth orientable compact manifold onto its boundary. The proof using Stokes's theorem is closely related to the proof using homology (or rather cohomology), because the form ω generates the de Rham cohomology group "H""n"−1("∂B") used in the cohomology proof.
A combinatorial proof.
The BFPT can be proved based on Sperner's lemma. We now give an outline of the proof for the special case in which "f" is a function from the unit "n"-simplex to itself, i.e.: 
Where:
For every point formula_36, also formula_37. Hence the sum of their coordinates is equal:
Hence, by the pigeonhole principle, for every formula_36 there must be an index formula_40 such that the formula_41-th coordinate of formula_42 is weakly larger than the formula_41-th coordinate of its image under "f":
Moreover, if formula_42 lies on a "k"-dimensional sub-face of formula_46, then by the same argument, the index formula_41 can be selected from among the ("k"+1) coordinates which are not zero on this sub-face.
We now use this fact to construct a Sperner coloring. For every triangulation of formula_46, the color of every vertex formula_42 is an index formula_41 such that formula_44.
By construction, this is a Sperner coloring. Hence, by Sperner's lemma, there is an "n"-dimensional simplex whose vertices are colored with the entire set of ("n"+1) available colors.
Because "f" is continuous, this simplex can be made arbitrarily small by choosing an arbitrarily fine triangulation. Hence, there must be a point formula_42 which satisfies the labeling condition in all coordinates, i.e.:
Because the sum of the coordinates of formula_42 and formula_55 must be equal, all these inequalities must actually be equalities. But this means that:
I.e, formula_42 is a fixed point of formula_7.
The proof can be extended to every object which is homeomorphic to formula_46.
A proof by Hirsch.
There is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map "f" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem, for example. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball—which is impossible in a retraction.
Kellogg, Li, and Yorke turned Hirsch's proof into a constructive proof by observing that the retract is in fact defined everywhere except at the fixed points. For almost any point, q, on the boundary, (assuming it is not a fixed point) the one manifold with boundary mentioned above does exist and the only possibility is that it leads from q to a fixed point. It is an easy numerical task to follow such a path from q to the fixed point so the method is essentially constructive. Chow, Mallet-Paret, and Yorke gave a conceptually similar path-following version of the homotopy proof which extends to a wide variety of related problems.
A proof using the "oriented area".
A variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If "r" : "B"→∂"B"   is a smooth retraction, one considers the smooth deformation "gt(x) := t r(x) + (1-t)x," and the smooth function
Differentiating under the sign of integral it is not difficult to check that "φ′(t)=0" for all "t", so "φ" is a constant function, which is a contradiction because "φ(0)" is the "n"-dimensional volume of the ball, while "φ(1)" is zero. The geometric idea is that "φ(t)" is the oriented area of "gt(B)" (that is, the Lebesgue measure of the image of the ball via "gt", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter "t" passes form "0" to "1" the map "gt" transforms continuously from the identity map of the ball, to the retraction "r", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of "r" is necessarily "0", as its image is the boundary of the ball, a set of null measure.
A proof using the game hex.
A quite different proof given by David Gale is based on the game of Hex. The basic theorem about Hex is that no game can end in a draw. This is equivalent to the Brouwer fixed-point theorem for dimension 2. By considering "n"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.
A proof using the Lefschetz fixed-point theorem.
The Lefschetz fixed-point theorem says that if a continuous map "f" from a finite simplicial complex "B" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number
and in particular if the Lefschetz number is nonzero then "f" must have a fixed point. If "B" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero homology group is :formula_62, so "f" has a fixed point.
A proof in a weak logical system.
In reverse mathematics, Brouwer's theorem can be proved in the system WKL0, and conversely over the base system RCA0 Brouwer's theorem for a square implies the weak König's lemma, so this gives a precise description of the strength of Brouwer's theorem.
Generalizations.
The Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.
The straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space ℓ2 of square-summable real (or complex) sequences, consider the map "f" : ℓ2 → ℓ2 which sends a sequence ("x""n") from the closed unit ball of ℓ2 to the sequence ("y""n") defined by
It is not difficult to check that this map is continuous, has its image in the unit sphere of ℓ 2, but does not have a fixed point.
The generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and in addition also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.
There is also finite-dimensional generalization to a larger class of spaces: If formula_64 is a product of finitely many chainable continua, then every continuous function formula_65 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_66, such that formula_67 if and only if formula_68. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.
The Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R"n", but considers upper hemi-continuous correspondences (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.
The Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of "D" "n".

</doc>
<doc id="4106" url="https://en.wikipedia.org/wiki?curid=4106" title="Benzoic acid">
Benzoic acid

Benzoic acid , C7H6O2 (or C6H5COOH), is a colorless crystalline solid and a simple aromatic carboxylic acid. The name is derived from gum benzoin, which was for a long time its only known source. Benzoic acid occurs naturally in many plants and it serves as an intermediate in the biosynthesis of many secondary metabolites. Salts of benzoic acid are used as food preservatives and benzoic acid is an important precursor for the industrial synthesis of many other organic substances. The salts and esters of benzoic acid are known as benzoates .
History.
Benzoic acid was discovered in the sixteenth century. The dry distillation of gum benzoin was first described by Nostradamus (1556), and then by Alexius Pedemontanus (1560) and Blaise de Vigenère (1596).
Pioneer work in 1830 through a variety of experiences based on amygdalin, obtained from bitter almonds (the fruit of Prunus dulcis) oil by Pierre Robiquet and Antoine Boutron-Charlard, two French chemists, had produced benzaldehyde but they failed in working out a proper interpretation of the structure of amygdalin that would account for it, and thus missed the identification of the benzoyl radical C7H5O.
This last step was achieved some few months later (1832) by Justus von Liebig and Friedrich Wöhler, who determined the composition of benzoic acid. These latter also investigated how hippuric acid is related to benzoic acid.
In 1875 Salkowski discovered the antifungal abilities of benzoic acid, which was used for a long time in the preservation of benzoate-containing cloudberry fruits.
It is also one of the chemical compounds found in castoreum. This compound is gathered from the castor sacs of the North American Beaver.
Production.
Industrial preparations.
Benzoic acid is produced commercially by partial oxidation of toluene with oxygen. The process is catalyzed by cobalt or manganese naphthenates. The process uses cheap raw materials, and proceeds in high yield.
U.S. production capacity is estimated to be 126,000 tonnes per year (139,000 tons), much of which is consumed domestically to prepare other industrial chemicals.
Laboratory synthesis.
Benzoic acid is cheap and readily available, so the laboratory synthesis of benzoic acid is mainly practiced for its pedagogical value. It is a common undergraduate preparation.
Benzoic acid can be purified by recrystallization from water because of its high solubility in hot water and poor solubility in cold water. The avoidance of organic solvents for the recrystallization makes this experiment particularly safe. The solubility of benzoic acid in over 40 solvents with references to original sources can be found as part of the Open Notebook Science Challenge.
By hydrolysis.
Like other nitriles and amides, benzonitrile and benzamide can be hydrolyzed to benzoic acid or its conjugate base in acid or basic conditions.
From benzaldehyde.
The base-induced disproportionation of benzaldehyde, the Cannizzaro reaction, affords equal amounts of benzoate and benzyl alcohol; the latter can be removed by distillation.
From bromobenzene.
Bromobenzene can be converted to benzoic acid by "carbonation" of the intermediate phenylmagnesium bromide. This synthesis offers a convenient exercise for students to carry out a Grignard reaction, an important class of carbon–carbon bond forming reaction in organic chemistry.
From benzyl alcohol.
Benzyl alcohol is refluxed with potassium permanganate or other oxidizing reagents in water. The mixture is hot filtered to remove manganese dioxide and then allowed to cool to afford benzoic acid.
From benzyl chloride.
Benzoic acid can be prepared by oxidation of benzyl chloride in the presence of alkaline KMnO4:
Historical preparation.
The first industrial process involved the reaction of benzotrichloride (trichloromethyl benzene) with calcium hydroxide in water, using iron or iron salts as catalyst. The resulting calcium benzoate is converted to benzoic acid with hydrochloric acid. The product contains significant amounts of chlorinated benzoic acid derivatives. For this reason, benzoic acid for human consumption was obtained by dry distillation of gum benzoin. Food-grade benzoic acid is now produced synthetically.
Uses.
Benzoic acid is mainly consumed in the production of phenol by oxidative decarboxylation at 300−400 °C:
The temperature required can be lowered to 200 °C by the addition of catalytic amounts of copper(II) salts. The phenol can be converted to cyclohexanol, which is a starting material for nylon synthesis.
Precursor to plasticizers.
Benzoate plasticizers, such as the glycol-, diethylenegylcol-, and triethyleneglycol esters, are obtained by transesterification of methyl benzoate with the corresponding diol. Alternatively these species arise by treatment of benzoylchloride with the diol. These plasticizers are used similarly to those derived from terephthalic acid ester.
Precursor to sodium benzoate and related preservatives.
Benzoic acid and its salts are used as a food preservatives, represented by the E-numbers E210, E211, E212, and E213. Benzoic acid inhibits the growth of mold, yeast and some bacteria. It is either added directly or created from reactions with its sodium, potassium, or calcium salt. The mechanism starts with the absorption of benzoic acid into the cell. If the intracellular pH changes to 5 or lower, the anaerobic fermentation of glucose through phosphofructokinase is decreased by 95%. The efficacy of benzoic acid and benzoate is thus dependent on the pH of the food. Acidic food and beverage like fruit juice (citric acid), sparkling drinks (carbon dioxide), soft drinks (phosphoric acid), pickles (vinegar) or other acidified food are preserved with benzoic acid and benzoates.
Typical levels of use for benzoic acid as a preservative in food are between 0.05–0.1%. Foods in which benzoic acid may be used and maximum levels for its application are controlled by international food law.
Concern has been expressed that benzoic acid and its salts may react with ascorbic acid (vitamin C) in some soft drinks, forming small quantities of benzene.
Medicinal.
Benzoic acid is a constituent of Whitfield's ointment which is used for the treatment of fungal skin diseases such as tinea, ringworm, and athlete's foot. As the principal component of benzoin resin, benzoic acid is also a major ingredient in both tincture of benzoin and Friar's balsam. Such products have a long history of use as topical antiseptics and inhalant decongestants.
Benzoic acid was used as an expectorant, analgesic, and antiseptic in the early 20th century.
Benzoyl chloride.
Benzoic acid is a precursor to benzoyl chloride, C6H5C(O)Cl by treatment with thionyl chloride, phosgene or one of the chlorides of phosphorus. is an important starting material for several benzoic acid derivates like benzyl benzoate, which is used in artificial flavours and insect repellents.
Niche and laboratory uses.
In teaching laboratories, benzoic acid is a common standard for calibrating a bomb calorimeter.
Biology and health effects.
Benzoic acid is relatively nontoxic. It is excreted as hippuric acid. Benzoic acid is metabolized by butyrate-CoA ligase into an intermediate product, benzoyl-CoA, which is then metabolized by glycine N-acyltransferase into hippuric acid.
Benzoic acid occurs naturally as do its esters in many plant and animal species. Appreciable amounts have been found in most berries (around 0.05%). Ripe fruits of several "Vaccinium" species (e.g., cranberry, "V. vitis macrocarpon"; bilberry, "V. myrtillus") contain as much as 0.03–0.13% free benzoic acid. Benzoic acid is also formed in apples after infection with the fungus "Nectria galligena". Among animals, benzoic acid has been identified primarily in omnivorous or phytophageous species, e.g., in viscera and muscles of the Rock Ptarmigan ("Lagopus muta") as well as in gland secretions of male muskoxen ("Ovibos moschatus") or Asian bull elephants ("Elephas maximus").
Gum benzoin contains up to 20% of benzoic acid and 40% benzoic acid esters.
"Cryptanaerobacter phenolicus" is a bacterium species that produces benzoate from phenol via 4-hydroxybenzoate
Benzoic acid is present as part of hippuric acid ("N"-benzoylglycine) in urine of mammals, especially herbivores (Gr. "hippos" = horse; "ouron" = urine). Humans produce about 0.44 g/L hippuric acid per day in their urine, and if the person is exposed to toluene or benzoic acid, it can rise above that level.
For humans, the World Health Organization's International Programme on Chemical Safety (IPCS) suggests a provisional tolerable intake would be 5 mg/kg body weight per day. Cats have a significantly lower tolerance against benzoic acid and its salts than rats and mice. Lethal dose for cats can be as low as 300 mg/kg body weight. The oral for rats is 3040 mg/kg, for mice it is 1940–2263 mg/kg.
In Taipei, Taiwan, a city health survey in 2010 found that 30% of dried and pickled food products had too much benzoic acid, which may affect the liver and kidney, along with more serious issues like excessive cyclamate.
Reactions.
Reactions of benzoic acid can occur at either the aromatic ring or at the carboxyl group:
Aromatic ring.
Electrophilic aromatic substitution reaction will take place mainly in 3-position due to the electron-withdrawing carboxylic group; i.e. benzoic acid is "meta" directing.
The second substitution reaction (on the right) is slower because the first nitro group is deactivating. Conversely, if an activating group (electron-donating) was introduced (e.g., alkyl), a second substitution reaction would occur more readily than the first and the disubstituted product might accumulate to a significant extent.
Carboxyl group.
All the reactions mentioned for carboxylic acids are also possible for benzoic acid.

</doc>
<doc id="4107" url="https://en.wikipedia.org/wiki?curid=4107" title="Boltzmann distribution">
Boltzmann distribution

In statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution) is a probability distribution, probability measure, or frequency distribution of particles in a system over various possible states. The distribution is expressed in the form
formula_1
where formula_2 is state energy (which varies from state to state), and formula_3 (a constant of the distribution) is the product of Boltzmann's constant and thermodynamic temperature.
In statistical mechanics, the Boltzmann distribution is a probability distribution that gives the probability that a system will be in a certain state as a function of that state’s energy and the temperature of the system. It is given as
formula_4
where "pi" is the probability of state i, "εi" the energy of state i, "k" the Boltzmann constant, "T" the temperature of the system and "M" is the number of states accessible to the system. The sum is over all states accessible to the system of interest. The term system here has a very wide meaning; it can range from a single atom to a macroscopic system such as a natural gas storage tank. Because of this Boltzmann distribution can be used to solve a very wide variety of problems. The distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy.
The "ratio" of a Boltzmann distribution computed for two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference.
formula_5
The Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium.
The distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.
The Boltzmann distribution should not be confused with the Maxwell-Boltzmann distribution. The former gives the probability that a system will be in a certain state as a function of that state's energy. When applied to particles such as atoms or molecules, it gives the distribution of particles over energy states. The Maxwell-Boltzmann distribution is used to describe particle speeds in idealized gases.
The distribution.
The Boltzmann distribution is a probability distribution that gives the probability of a certain state as a function of that state’s energy and temperature of the system to which the distribution is applied. It is given as
formula_6
where "pi" is the probability of state i, "εi" the energy of state i, "k" the Boltzmann constant, "T" the temperature of the system and "M" is the number of all states accessible to the system. The sum is over all states accessible to the system of interest. The right hand side denominator of the equation above is also known as the canonical partition function, commonly denoted by Q (or by some authors by Z).
formula_7
Therefore, the Boltzmann distribution can also be written as
formula_8
The partition function can be calculated if we know the energies of the levels accessible to the system of interest. For atoms the partition function values can be found in the NIST Atomic Spectra Database.
The distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy. It can also give us the quantitative relationship between the probabilities of the two states being occupied. The ratio of probabilities for states i and j is given as
formula_9
where "pi" is the probability of state i, "pj" the probability of state j, and "εi" and "εi" are the energies of states i and j, respectively.
The Boltzmann distribution is often used to describe the distribution of particles, such as atoms or molecules, over energy states accessible to them. If we have a system consisted of many particles, the probability of a particle being in state i is practically the probability that, if we pick a random particle from that system and check what state it is in, we will find it is in state i. This probability is equal to the number of particles in state i divided by the total number of particles in the system, that is the fraction of particles that occupy state i.
formula_10
where "Ni" is the number of particles in state i and "N" is the total number of particles in the system. We may use the Boltzmann distribution to find this probability that is, as we have seen, equal to the fraction of particles that are in state i. So the equation that gives the fraction of particles in state i as a function of the energy of that state is 
formula_11
This equation is of great importance to spectroscopy. In spectroscopy we observe a spectral line if atoms or molecules that we are interested in go from one state to another. In order for this to be possible, there must be some particles in the first state to undergo the transition. We may find that this condition is fulfilled by finding the fraction of particles in the first state. If it is negligible, the transition is very likely not be observed at the temperature for which the calculation was done. In general, a larger fraction of molecules in the first state means a higher number of transitions to the second state. This gives a stronger spectral line. However, there are other factors that influence the intensity of a spectral line, such as whether it is caused by an allowed or a forbidden transition.
In statistical mechanics.
The Boltzmann distribution appears in statistical mechanics when considering isolated (or nearly-isolated) systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble, but also some special cases (derivable from the canonical ensemble) also show the Boltzmann distribution in different aspects:
Although these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:
In mathematics.
In more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning it is called a log-linear model.
In economics.
The Boltzmann distribution can be introduced to allocate permits in emissions trading. The new allocation method using the Boltzmann distribution can describe the most probable, natural, and unbiased distribution of emissions permits among multiple countries. Simple and versatile, this new method holds potential for many economic and environmental applications.

</doc>
<doc id="4109" url="https://en.wikipedia.org/wiki?curid=4109" title="Leg theory">
Leg theory

Leg theory is a bowling tactic in the sport of cricket. The term "leg theory" is somewhat archaic and seldom used any longer, but the basic tactic remains a play in modern cricket.
Simply put, leg theory involves concentrating the bowling attack at or near the line of leg stump. This may or may not be accompanied by a concentration of fielders on the leg side. The line of attack aims to cramp the batsman, making him play the ball with the bat close to the body. This makes it difficult to hit the ball freely and score runs, especially on the off side. Since a leg theory attack means the batsman is more likely to hit the ball on the leg side, additional fielders on that side of the field can be effective in preventing runs and taking catches.
Stifling the batsman in this manner can lead to impatience and frustration, resulting in rash play by the batsman which in turn can lead to a quick dismissal.
Leg theory can be a moderately successful tactic when used with both fast bowling and spin bowling, particularly leg spin to right-handed batsmen or off spin to left-handed batsmen. However, because it relies on lack of concentration or discipline by the batsman, it can be risky against patient and skilled players, especially batsmen who are strong on the leg side. The English opening bowlers Sydney Barnes and Frank Foster used leg theory with some success in Australia in 1911-12. In England, at around the same time Fred Root was one of the main proponents of the same tactic.
Concentrating attack on the leg stump is considered by many cricket fans and commentators to lead to boring play, as it stifles run scoring and encourages batsmen to play conservatively.
Fast leg theory.
In 1930, England captain Douglas Jardine, together with Nottinghamshire's captain Arthur Carr and his bowlers Harold Larwood and Bill Voce, developed a variant of leg theory in which the bowlers bowled fast, short-pitched balls that would rise into the batsman's body, together with a heavily stacked ring of close fielders on the leg side. The idea was that when the batsman defended against the ball, he would be likely to deflect the ball into the air for a catch.
Jardine called this modified form of the tactic "fast leg theory". On the 1932-33 English tour of Australia, Larwood and Voce bowled fast leg theory at the Australian batsmen. It turned out to be extremely dangerous, and most Australian players sustained injuries from being hit by the ball. Wicket-keeper Bert Oldfield's skull was fractured by a ball hitting his head (although the ball had first glanced off the bat and Larwood had an orthodox field), almost precipitating a riot by the Australian crowd.
The Australian press dubbed the tactic "Bodyline", and claimed it was a deliberate attempt by the English team to intimidate and injure the Australian players. Reports of the controversy reaching England at the time described the bowling as "fast leg theory", which sounded to many people to be a harmless and well-established tactic. This led to a serious misunderstanding amongst the English public and the Marylebone Cricket Club - the administrators of English cricket - of the dangers posed by Bodyline. The English press and cricket authorities declared the Australian protests to be a case of sore losing and "squealing".
It was only with the return of the English team and the subsequent use of Bodyline against English players in England by the touring West Indian cricket team in 1933 that demonstrated to the country the dangers it posed. The MCC subsequently revised the Laws of Cricket to prevent the use of "fast leg theory" tactics in future, also limiting the traditional tactic.

</doc>
<doc id="4110" url="https://en.wikipedia.org/wiki?curid=4110" title="Blythe Danner">
Blythe Danner

Blythe Katherine Danner (born February 3, 1943) is an American actress of film, television and stage. 
She is known for her role as Marilyn Truman, mother of Will, on the sitcom "Will & Grace", and for co-starring opposite Robert De Niro in the three "Meet the Parents" franchise films. She is the mother of actress Gwyneth Paltrow and director Jake Paltrow. In 1970 she won the Tony Award for Best Featured Actress in a Play for her performance in "Butterflies Are Free". She won two Primetime Emmy Awards for Best Supporting Actress in a Drama for her role as Izzy Huffstodt on "Huff".
Her other film work includes roles in "1776" (1972), "The Great Santini" (1979), "Mr. and Mrs. Bridge" (1990), "The Prince of Tides" (1991), "Husbands and Wives" (1992), and "I'll See You in My Dreams" (2015).
Early life.
Danner was born in Philadelphia, Pennsylvania, the daughter of Katharine (née Kile; 1909-2006) and Harry Earl Danner, a bank executive. She has a brother, opera singer/actor Harry Danner; a sister-in-law, performer-turned-director Dorothy (Dottie) Danner; and a half-brother, violin maker William Moennig. Danner has Pennsylvania Dutch (German), and some English and Irish, ancestry; her maternal grandmother was a German immigrant, and one of her paternal great-grandmothers was born in Barbados (to a family of European descent).
Danner graduated from George School, a Quaker high school located near Newtown, Bucks County, Pennsylvania in 1960. She began her Friends School experience in Kindergarten at Media-Providence Friends School, then known as Media Friends in Media, Pennsylvania.
Career.
A graduate of Bard College, Danner's first roles included the 1967 musical "Mata Hari" (closed out of town), and the 1968 off-Broadway production, "Summertree". Her early Broadway appearances included roles in "Cyrano de Bergerac" (1968) and "The Miser" (1969). She won a Best Supporting Actress Tony playing a free-spirited divorcee in "Butterflies Are Free" (1969).
In 1972, Danner portrayed Martha Jefferson opposite Ken Howard's Thomas Jefferson in the movie version of "1776". That same year, she played a wife whose husband has been unfaithful opposite Peter Falk and John Cassavetes in the "Columbo" episode "Etude in Black".
Her earliest starring film role was opposite Alan Alda in "To Kill a Clown" (1972). Danner appeared in the episode of "M*A*S*H" entitled "The More I See You", playing the love interest of Alda's character Hawkeye Pierce. She played lawyer Amanda Bonner in television's "Adam's Rib", also opposite Ken Howard as Adam Bonner. She played Zelda Fitzgerald in "F. Scott Fitzgerald and 'The Last of the Belles'" (1974). She was the eponymous heroine in the film "Lovin' Molly" (1974) (directed by Sidney Lumet). She appeared in "Futureworld", playing Tracy Ballard with co-star Peter Fonda (1976). In the 1982 TV movie "Inside the Third Reich", she played the wife of Albert Speer. In the film version of Neil Simon's semiautobiographical play "Brighton Beach Memoirs" (1986), she portrayed a middle-aged Jewish mother. She has appeared in two films based on the novels of Pat Conroy, "The Great Santini" (1979) and "The Prince of Tides" (1991), as well as two television movies adapted from books by Anne Tyler, "Saint Maybe" and "Back When We Were Grownups", both for the Hallmark Hall of Fame.
Danner appeared opposite Robert De Niro in the 2000 comedy hit "Meet the Parents", and its sequels, "Meet the Fockers" and "Little Fockers".
From 2001 to 2006, she regularly appeared on "Will & Grace" as Will Truman's mother Marilyn. From 2004 to 2006, she starred in the TV series "Huff". In 2005, she was nominated for three Emmy Awards: for her work on "Will & Grace", "Huff", and "Back When We Were Grownups". Emmy host Ellen DeGeneres poked fun at Danner during the awards ceremony, saying that Danner should not be nervous because she was almost certain to win at least one Emmy, which she did, for "Huff". In July 2006, she won a second consecutive Emmy award for "Huff". For 25 years, she has been a regular performer at the Williamstown Summer Theater Festival, where she also serves on the Board of Directors.
In 2006, Danner was awarded an inaugural Katharine Hepburn Medal by Bryn Mawr College's Katharine Houghton Hepburn Center.
In 2015, Danner was inducted into the American Theater Hall of Fame.
Environmental activism.
Danner has been involved in environmental issues such as recycling and conservation for over 30 years. She has been active with INFORM, Inc., is on the Board of Environmental Advocates of New York and the Board of Directors of the Environmental Media Association, and won the 2002 EMA Board of Directors Ongoing Commitment Award. In 2011, Danner joined Moms Clean Air Force, to help call on parents to join in the fight against toxic air pollution.
Health care activism.
After the death of her husband Bruce Paltrow from oral cancer, she became involved with the Oral Cancer Foundation, a national 501(c)3 nonprofit charity. In 2005, she filmed a public service announcement that played on TV stations around the country about the risks associated with oral cancer, and through that shared the personal pain associated with the loss of her husband publicly to further awareness of the disease and the need for early detection. She continues to donate her time to the foundation, and has appeared on morning talk shows, and has done interviews in high profile magazines such as "People" to further public awareness of the disease and its risk factors. Through The Bruce Paltrow Oral Cancer Fund, administered by the Oral Cancer Foundation, she continues to raise awareness and funding for oral cancer issues, particularly those involving communities in which disparities in health care exist. She appeared in commercials for Prolia, a brand of denosumab for injection.
Personal life.
Danner is the widow of producer/director Bruce Paltrow, who died from complications of pneumonia while battling oral cancer in 2002, and the mother of actress Gwyneth Paltrow and director Jake Paltrow. Danner first co-starred with her daughter in 1992 in the TV movie "Cruel Doubt" and then again in the 2003 film "Sylvia" playing Aurelia Plath, mother to Gwyneth Paltrow's title role as Sylvia Plath. Danner says: "I have found transcendental meditation very helpful and comforting. It centers me." 

</doc>
<doc id="4111" url="https://en.wikipedia.org/wiki?curid=4111" title="Bioleaching">
Bioleaching

Bioleaching is the extraction of metals from their ores through the use of living organisms. This is much cleaner than the traditional heap leaching using cyanide. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to recover copper, zinc, lead, arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.
Process.
Bioleaching can involve numerous ferrous iron and sulfur oxidizing bacteria, including "Acidithiobacillus ferrooxidans" (formerly known as "Thiobacillus ferrooxidans") and "Acidithiobacillus thiooxidans " (formerly known as "Thiobacillus thiooxidans"). As a general principle, Fe3+ ions are used to oxidize the ore. This step is entirely independent of microbes. The role of the bacteria is the further oxidation of the ore, but also the regeneration of the chemical oxidant Fe3+ from Fe2+. For example, bacteria catalyse the breakdown of the mineral pyrite (FeS2) by oxidising the sulfur and metal (in this case ferrous iron, (Fe2+)) using oxygen. This yields soluble products that can be further purified and refined to yield the desired metal.
Pyrite leaching (FeS2):
In the first step, disulfide is spontaneously oxidized to thiosulfate by ferric ion (Fe3+), which in turn is reduced to give ferrous ion (Fe2+):
The ferrous ion is then oxidized by bacteria using oxygen:
Thiosulfate is also oxidized by bacteria to give sulfate:
The ferric ion produced in reaction (2) oxidized more sulfide as in reaction (1), closing the cycle and given the net reaction:
The net products of the reaction are soluble ferrous sulfate and sulfuric acid.
The microbial oxidation process occurs at the cell membrane of the bacteria. The electrons pass into the cells and are used in biochemical processes to produce energy for the bacteria while reducing oxygen to water. The critical reaction is the oxidation of sulfide by ferric iron. The main role of the bacterial step is the regeneration of this reactant.
The process for copper is very similar, but the efficiency and kinetics depend on the copper mineralogy. The most efficient minerals are supergene minerals such as chalcocite, Cu2S and covellite, CuS. The main copper mineral chalcopyrite (CuFeS2) is not leached very efficiently, which is why the dominant copper-producing technology remains flotation, followed by smelting and refining. The leaching of CuFeS2 follows the two stages of being dissolved and then further oxidised, with Cu2+ ions being left in solution.
Chalcopyrite leaching:
net reaction:
In general, sulfides are first oxidized to elemental sulfur, whereas disulfides are oxidized to give thiosulfate, and the processes above can be applied to other sulfidic ores. Bioleaching of non-sulfidic ores such as pitchblende also uses ferric iron as an oxidant (e.g., UO2 + 2 Fe3+ ==> UO22+ + 2 Fe2+). In this case, the sole purpose of the bacterial step is the regeneration of Fe3+. Sulfidic iron ores can be added to speed up the process and provide a source of iron. Bioleaching of non-sulfidic ores by layering of waste sulfides and elemental sulfur, colonized by "Acidithiobacillus" spp., has been accomplished, which provides a strategy for accelerated leaching of materials that do not contain sulfide minerals.
Further processing.
The dissolved copper (Cu2+) ions are removed from the solution by ligand exchange solvent extraction, which leaves other ions in the solution. The copper is removed by bonding to a ligand, which is a large molecule consisting of a number of smaller groups, each possessing a lone electron pair. The ligand-copper complex is extracted from the solution using an organic solvent such as kerosene:
The ligand donates electrons to the copper, producing a complex - a central metal atom (copper) bonded to the ligand. Because this complex has no charge, it is no longer attracted to polar water molecules and dissolves in the kerosene, which is then easily separated from the solution. Because the initial reaction is reversible, it is determined by pH. Adding concentrated acid reverses the equation, and the copper ions go back into an aqueous solution.
Then the copper is passed through an electro-winning process to increase its purity: An electric current is passed through the resulting solution of copper ions. Because copper ions have a 2+ charge, they are attracted to the negative cathodes and collect there.
The copper can also be concentrated and separated by displacing the copper with Fe from scrap iron:
The electrons lost by the iron are taken up by the copper. Copper is the oxidising agent (it accepts electrons), and iron is the reducing agent (it loses electrons).
Traces of precious metals such as gold may be left in the original solution. Treating the mixture with sodium cyanide in the presence of free oxygen dissolves the gold. The gold is removed from the solution by adsorbing (taking it up on the surface) to charcoal.
Bioleaching with fungi.
Several species of fungi can be used for bioleaching. Fungi can be grown on many different substrates, such as electronic scrap, catalytic converters, and fly ash from municipal waste incineration. Experiments have shown that two fungal strains ("Aspergillus niger, Penicillium simplicissimum") were able to mobilize Cu and Sn by 65%, and Al, Ni, Pb, and Zn by more than 95%."Aspergillus niger" can produce some organic acids such as citric acid. This form of leaching does not rely on microbial oxidation of metal but rather uses microbial metabolism as source of acids that directly dissolve the metal.
Compared with other extraction techniques.
Extractions involve many expensive steps such as roasting and smelting, which require sufficient concentrations of elements in ores and are environmentally unfriendly. Low concentrations are not a problem for bacteria because they simply ignore the waste that surrounds the metals, attaining extraction yields of over 90% in some cases. These microorganisms actually gain energy by breaking down minerals into their constituent elements. The company simply collects the ions out of the solution after the bacteria have finished. There is a limited amount of ores.
Disadvantages of bioleaching.
At the current time, it is more economical to smelt copper ore rather than to use bioleaching, since the concentration of copper in its ore is in general quite high. The profit obtained from the speed and yield of smelting justifies its cost. Nonetheless, at the largest copper mine of the world, Escondida in Chile the process seems to be favorable.
However, the concentration of gold in its ore is in general very low. In this case, the lower cost of bacterial leaching outweighs the time it takes to extract the metal.
Economically it is also very expensive and many companies once started can not keep up with the demand and end up in debt. Projects like Finnish Talvivaara proved to be environmentally and economically disastrous.

</doc>
<doc id="4113" url="https://en.wikipedia.org/wiki?curid=4113" title="Bouldering">
Bouldering

Bouldering is a form of rock climbing that is performed without the use of ropes or harnesses. While it can be done without any equipment whatsoever, most climbers use climbing shoes to help secure footholds, chalk to keep their hands dry, and bouldering mats to prevent injuries from falls. Unlike free solo climbing, which is also performed without ropes, bouldering problems (the path that a climber takes in order to complete the climb) are usually less than 6 meters (20 ft.) tall. Artificial climbing walls allow boulderers to train indoors in areas without natural boulders. Bouldering competitions, which employ a variety of formats, take place in both indoor and outdoor settings.
The sport originated as a method of training for roped climbs and mountaineering. Bouldering enabled climbers to practice specific moves at a safe distance from the ground. Additionally, the sport served to build stamina and increase finger strength. Throughout the 1900s, bouldering evolved into a separate discipline. Individual problems are assigned ratings based on their difficulty. There have been many different rating systems used throughout the history of the sport, but modern problems usually use either the V-scale or the Fontainebleau scale.
The growing popularity of the sport has caused several environmental concerns, including soil erosion and trampled vegetation as climbers hike off-trail to reach bouldering sites. This has caused some landowners to restrict access or prohibit bouldering altogether.
Overview.
Bouldering is a form of rock climbing which takes place on boulders and other small rock formations, usually measuring less than from ground to top, but in some cases can measure up to 30+ ft. Unlike top rope climbing and lead climbing, no ropes are used to protect or aid the climber. Bouldering routes or "problems" require the climber to reach the top of a boulder, usually from a specified start position. Some boulder problems, known as "traverses," require the climber to climb horizontally from one position to another.
The characteristics of boulder problems depend largely on the type of rock being climbed. Granite, for example, often features long cracks and slabs. Sandstone rocks are known for their steep overhangs and frequent horizontal breaks. Other common bouldering rocks include limestone and volcanic rock.
There are many prominent bouldering areas throughout the United States, including Hueco Tanks in Texas, Mount Evans in Colorado, and The Buttermilks in Bishop, California. Squamish, British Columbia is one of the most popular bouldering areas in Canada. Europe also hosts a number of bouldering sites, such as Fontainebleau in France, Albarracín in Spain, and various mountains throughout Switzerland.
Indoor bouldering.
Artificial climbing walls are used to simulate boulder problems in an indoor environment, usually at climbing gyms. These walls are constructed with wooden panels, polymer cement panels, concrete shells, or precast molds of actual rock walls. Holds, usually made of plastic, are then bolted onto the wall to create problems. The walls often feature steep overhanging surfaces, forcing the climber to employ highly technical movements while supporting much of their weight with their upper body strength.
Climbing gyms often feature multiple problems within the same section of wall. In the US the most common method Routesetters use to designate the intended route for a particular problem is by placing colored tape next to each hold—for example, holds with red tape would indicate one bouldering problem, while green tape would be used to set off a different problem in the same area. Across much of the rest of the world problems and grades are usually designated by using a set color of plastic hold to indicate a particular problem. For example, green may be v0-v1, blue may be v2-v3 and so on. Setting via color has certain advantages, the most notable of which are that it makes it more obvious where the holds for a problem are, and that there is no chance of tape being accidentally kicked off of footholds. Smaller, resource-poor climbing gyms may prefer taped problems because large, expensive holds can be used in multiple routes simply by marking them with more than one color of tape.
Competitions.
Bouldering competitions occur in both indoor and outdoor settings. The International Federation of Sport Climbing (IFSC) employs an indoor format that breaks the competition into three rounds: qualifications, semi-finals, and finals. The rounds feature different sets of four or five boulder problems, and each competitor has a fixed amount of time to attempt each problem. At the end of each round, competitors are ranked by the number of completed problems, with ties settled by the total number of attempts taken to solve the problems.
There are several other formats used for bouldering competitions. Some competitions give climbers a fixed number of attempts at each problem with a timed rest period in between each attempt, unlike the IFSC format, in which competitors can use their allotted time however they choose. In an open-format competition, all climbers compete simultaneously, and are given a fixed amount of time to complete as many problems as possible. More points are awarded for more difficult problems, while points are deducted for multiple attempts on the same problem.
In 2012, the IFSC submitted a proposal to the International Olympic Committee (IOC) to include lead climbing in the 2020 Summer Olympics. The proposal was later revised to an "overall" competition, which would feature bouldering, lead climbing, and speed climbing. In May 2013, the IOC announced that climbing would not be added to the 2020 Olympic program.
History.
Rock climbing first emerged as a sport in the mid-1800s. Early records describe climbers engaging in what is now referred to as bouldering, not as a separate discipline, but as a form of training for larger ascents. In the early 20th century, the Fontainebleau area of France established itself as a prominent climbing area, where some of the first dedicated "bleausards" (or "boulderers") emerged. The specialized rock climbing shoe was invented by one such athlete, Pierre Allain.
In the 1960s, the sport was pushed forward by American mathematician John Gill, who contributed several important innovations. Gill's previous athletic pursuit was gymnastics, a sport which had an established scale of difficulty for particular movements and body positions. He applied this idea to bouldering, which shifted the focus from reaching a summit to navigating a specific sequence of holds. Gill developed a closed-ended rating system: B1 problems were as difficult as the most challenging roped routes of the time, B2 problems were more difficult, and B3 problems were those that had only been completed once.
Gill introduced chalk as a method of keeping the climber's hands dry. He also emphasized the importance of strength training to complement technical skill. Neither of these practices had been popular among climbers, but as Gill's ability level and influence grew, his ideas became the norm.
Two important training tools emerged in the 1980s: Bouldering mats and artificial climbing walls. The former, also referred to as "crash pads", prevented injuries from falling, and enabled boulderers to climb in areas that would have been too dangerous to attempt otherwise. Indoor climbing walls helped spread the sport to areas without outdoor climbing, and allowed serious climbers to train year-round regardless of weather conditions.
As the sport grew in popularity, new bouldering areas were developed throughout Europe and the United States, and more athletes began participating in bouldering competitions. The visibility of the sport greatly increased in the early 2000s, as YouTube videos and climbing blogs helped boulderers around the world to quickly learn techniques, find hard problems, and announce newly completed projects.
In early 2010, two American climbers claimed first ascents on boulder problems that have come to be regarded as the most difficult in the world: "The Game" near Boulder, Colorado, established by Daniel Woods; and "Lucid Dreaming" near Bishop, California, established by Paul Robinson. The following year, fellow American Carlo Traversi claimed the second ascent of "The Game" and in January 2014, American Daniel Woods completed the second ascent of "Lucid Dreaming." In 2011, Czech climber Adam Ondra claimed the second ascent of "Gioia", originally established three years earlier by Italian boulderer Christian Core, and suggested that it was among the world's most challenging boulder problems.
Equipment.
Unlike other climbing sports, bouldering can be performed safely and effectively with very little equipment, an aspect which makes the discipline highly appealing to many climbers. Bouldering pioneer John Sherman asserted that "The only gear really needed to go bouldering is boulders". Others suggest the use of climbing shoes and a chalkbag as the bare minimum, while more experienced boulderers typically bring multiple pairs of shoes, chalk, brushes, crash pads, and a skincare kit.
Of the aforementioned equipment, climbing shoes have the most direct impact on performance. Besides protecting the climber's feet from rough surfaces, climbing shoes are designed to help the climber secure and maintain footholds. Climbing shoes typically fit much tighter than other athletic footwear, and often curl the toes downwards to enable precise footwork. They are manufactured in a variety of different styles in order to perform well in different situations: High-top shoes, for example, provide better protection for the ankle, while low-top shoes provide greater flexibility and freedom of movement. Stiffer shoes excel at securing small edges, whereas softer shoes provide greater sensitivity. The front of the shoe, called the "toe box", can be asymmetric, which performs well on overhanging rocks, or symmetric, which is better suited for vertical problems and slabs. 
Most boulderers use gymnastics chalk on their hands to absorb sweat. It is stored in a small chalkbag which can be tied around the waist, allowing the climber to reapply chalk during the climb. Brushes are used to remove excess chalk and other debris from boulders in between climbs; they are often attached to the end of a stick, pipe, or other straight object in order to reach higher holds. Crash pads, also referred to as bouldering mats, are foam cushions placed on the ground to protect climbers from falls.
Safety.
Boulder problems are generally shorter than from ground to top. This makes the sport significantly safer than free solo climbing, which is also performed without ropes, but with no upper limit on the height of the climb. However, minor injuries are common in bouldering, particularly sprained ankles and wrists. Two factors contribute to the frequency of injuries in bouldering: first, boulder problems typically feature more difficult moves than other climbing disciplines, making falls more common. Second, without ropes to arrest the climber's descent, every fall will cause the climber to hit the ground.
To prevent injuries, boulderers position crash pads near the boulder to provide a softer landing, as well as one or more spotters to help redirect the climber towards the pads. Upon landing, boulderers employ falling techniques similar to those used in gymnastics: spreading the impact across the entire body to avoid bone fractures, and positioning limbs to allow joints to move freely throughout the impact.
Technique.
As with other forms of climbing, bouldering technique is largely centered around proper footwork. Leg muscles are significantly stronger than arm muscles; thus, proficient boulderers use their arms primarily to maintain balance and body positioning, relying on their legs to push them up the boulder. Boulderers also keep their arms straight whenever possible, allowing their bones to support their body weight rather than their muscles.
Bouldering movements are described as either "static" or "dynamic". Static movements are those that are performed slowly, with the climber's position controlled by maintaining contact on the boulder with the other three limbs. Dynamic movements use the climber's momentum to reach holds that would be difficult or impossible to secure statically, with an increased risk of falling if the movement is not performed accurately.
Grading.
Bouldering problems are assigned numerical difficulty ratings by routesetters and climbers. The two most widely used rating systems are the V-scale and the Fontainebleau system. 
The V-scale, which originated in the United States, is an open-ended rating system with higher numbers indicating a higher degree of difficulty. The V1 rating indicates that a problem can be completed by a novice climber in good physical condition after several attempts. The scale begins at V0, and as of 2013, the highest V rating that has been assigned to a bouldering problem is V16. Some climbing gyms also use a VB grade to indicate beginner problems.
The Fontainebleau scale follows a similar system, with each numerical grade divided into three ratings with the letters "a", "b", and "c". For example, Fontainebleau 7A roughly corresponds with V6, while Fontainebleau 7C+ is equivalent to V10. In both systems, grades are further differentiated by appending "+" to indicate a small increase in difficulty. Despite this level of specificity, ratings of individual problems are often controversial, as ability level is not the only factor that affects how difficult a problem will be for a particular climber. Height, arm length, flexibility, and other body characteristics can also be relevant.
Environmental impact.
Bouldering can damage vegetation that grows on rocks, such as mosses and lichens. This can occur as a result of the climber intentionally cleaning the boulder, or unintentionally from repeated use of handholds and footholds. Vegetation on the ground surrounding the boulder can also be damaged from overuse, particularly by climbers laying down crash pads. Soil erosion can occur when boulderers trample vegetation while hiking off of established trails, or when they unearth small rocks near the boulder in an effort to make the landing zone safer. Other environmental concerns include littering, improperly disposed feces, and graffiti. These issues have caused some land managers to prohibit bouldering, as was the case in Tea Garden, a popular bouldering area in Rocklands, South Africa.

</doc>
<doc id="4115" url="https://en.wikipedia.org/wiki?curid=4115" title="Boiling point">
Boiling point

The boiling point of a liquid varies depending upon the surrounding environmental pressure. A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure. A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For a given pressure, different liquids boil at different temperatures. 
The normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, 1 atmosphere. At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of 1 bar.
The heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).
Liquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.
Saturation temperature and pressure.
A "saturated liquid" contains as much thermal energy as it can without boiling (or conversely a "saturated vapor" contains as little thermal energy as it can without condensing).
Saturation temperature means "boiling point". The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.
If the pressure in a system remains constant (isobaric), a vapor at saturation temperature will begin to condense into its liquid phase as thermal energy (heat) is removed. Similarly, a liquid at saturation temperature and pressure will boil into its vapor phase as additional thermal energy is applied.
The boiling point corresponds to the temperature at which the vapor pressure of the liquid equals the surrounding environmental pressure. Thus, the boiling point is dependent on the pressure. Boiling points may be published with respect to the NIST, USA standard pressure of 101.325 kPa (or 1 atm), or the IUPAC standard pressure of 100.000 kPa. At higher elevations, where the atmospheric pressure is much lower, the boiling point is also lower. The boiling point increases with increased pressure up to the critical point, where the gas and liquid properties become identical. The boiling point cannot be increased beyond the critical point. Likewise, the boiling point decreases with decreasing pressure until the triple point is reached. The boiling point cannot be reduced below the triple point.
If the heat of vaporization and the vapor pressure of a liquid at a certain temperature is known, the boiling point can be calculated by using the Clausius–Clapeyron equation thus:
formula_1
Saturation pressure is the pressure for a corresponding saturation temperature at which a liquid boils into its vapor phase. Saturation pressure and saturation temperature have a direct relationship: as saturation pressure is increased so is saturation temperature.
If the temperature in a system remains constant (an "isothermal" system), vapor at saturation pressure and temperature will begin to condense into its liquid phase as the system pressure is increased. Similarly, a liquid at saturation pressure and temperature will tend to flash into its vapor phase as system pressure is decreased.
There are two conventions regarding the "standard boiling point of water": The "normal boiling point" is at a pressure of 1 atm (i.e., 101.325 kPa). The IUPAC recommended "standard boiling point of water" at a standard pressure of 100 kPa (1 bar) is . For comparison, on top of Mount Everest, at elevation, the pressure is about and the boiling point of water is .
The Celsius temperature scale was defined until 1954 by two points - 0 °C being defined by the water freezing point and 100 °C being defined by the water boiling point at standard atmospheric pressure.
Relation between the normal boiling point and the vapor pressure of liquids.
The higher the vapor pressure of a liquid at a given temperature, the lower the normal boiling point (i.e., the boiling point at atmospheric pressure) of the liquid.
The vapor pressure chart to the right has graphs of the vapor pressures versus temperatures for a variety of liquids. As can be seen in the chart, the liquids with the highest vapor pressures have the lowest normal boiling points.
For example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (−24.2 °C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.
The critical point of a liquid is the highest temperature (and pressure) it will actually boil at.
See also Vapour pressure of water.
Properties of the elements.
The element with the lowest boiling point is helium. Both the boiling points of rhenium and tungsten exceed 5000 K at standard pressure; because it is difficult to measure extreme temperatures precisely without bias, both have been cited in the literature as having the higher boiling point.
Boiling point as a reference property of a pure compound.
As can be seen from the above plot of the logarithm of the vapor pressure vs. the temperature for any given pure chemical compound, its normal boiling point can serve as an indication of that compound's overall volatility. A given pure compound has only one normal boiling point, if any, and a compound's normal boiling point and melting point can serve as characteristic physical properties for that compound, listed in reference books. The higher a compound's normal boiling point, the less volatile that compound is overall, and conversely, the lower a compound's normal boiling point, the more volatile that compound is overall. Some compounds decompose at higher temperatures before reaching their normal boiling point, or sometimes even their melting point. For a stable compound, the boiling point ranges from its triple point to its critical point, depending on the external pressure. Beyond its triple point, a compound's normal boiling point, if any, is higher than its melting point. Beyond the critical point, a compound's liquid and vapor phases merge into one phase, which may be called a superheated gas. At any given temperature, if a compound's normal boiling point is lower, then that compound will generally exist as a gas at atmospheric external pressure. If the compound's normal boiling point is higher, then that compound can exist as a liquid or solid at that given temperature at atmospheric external pressure, and will so exist in equilibrium with its vapor (if volatile) if its vapors are contained. If a compound's vapors are not contained, then some volatile compounds can eventually evaporate away in spite of their higher boiling points.
In general, compounds with ionic bonds have high normal boiling points, if they do not decompose before reaching such high temperatures. Many metals have high boiling points, but not all. Very generally—with other factors being equal—in compounds with covalently bonded molecules, as the size of the molecule (or molecular mass) increases, the normal boiling point increases. When the molecular size becomes that of a macromolecule, polymer, or otherwise very large, the compound often decomposes at high temperature before the boiling point is reached. Another factor that affects the normal boiling point of a compound is the polarity of its molecules. As the polarity of a compound's molecules increases, its normal boiling point increases, other factors being equal. Closely related is the ability of a molecule to form hydrogen bonds (in the liquid state), which makes it harder for molecules to leave the liquid state and thus increases the normal boiling point of the compound. Simple carboxylic acids dimerize by forming hydrogen bonds between molecules. A minor factor affecting boiling points is the shape of a molecule. Making the shape of a molecule more compact tends to lower the normal boiling point slightly compared to an equivalent molecule with more surface area.
Most volatile compounds (anywhere near ambient temperatures) go through an intermediate liquid phase while warming up from a solid phase to eventually transform to a vapor phase. By comparison to boiling, a sublimation is a physical transformation in which a solid turns directly into vapor, which happens in a few select cases such as with carbon dioxide at atmospheric pressure. For such compounds, a sublimation point is a temperature at which a solid turning directly into vapor has a vapor pressure equal to the external pressure.
Impurities and mixtures.
In the preceding section, boiling points of pure compounds were covered. Vapor pressures and boiling points of substances can be affected by the presence of dissolved impurities (solutes) or other miscible compounds, the degree of effect depending on the concentration of the impurities or other compounds. The presence of non-volatile impurities such as salts or compounds of a volatility far lower than the main component compound decreases its mole fraction and the solution's volatility, and thus raises the normal boiling point in proportion to the concentration of the solutes. This effect is called boiling point elevation. As a common example, salt water boils at a higher temperature than pure water.
In other mixtures of miscible compounds (components), there may be two or more components of varying volatility, each having its own pure component boiling point at any given pressure. The presence of other volatile components in a mixture affects the vapor pressures and thus boiling points and dew points of all the components in the mixture. The dew point is a temperature at which a vapor condenses into a liquid. Furthermore, at any given temperature, the composition of the vapor is different from the composition of the liquid in most such cases. In order to illustrate these effects between the volatile components in a mixture, a boiling point diagram is commonly used. Distillation is a process of boiling and condensation which takes advantage of these differences in composition between liquid and vapor phases.

</doc>
<doc id="4116" url="https://en.wikipedia.org/wiki?curid=4116" title="Big Bang">
Big Bang

The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. The model accounts for the fact that the universe expanded from a very high density and high temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background, large scale structure and Hubble's Law. If the known laws of physics are extrapolated beyond where they have been verified, there is a singularity. Some estimates place this moment at approximately 13.8 billion years ago, which is thus considered the age of the universe. After the initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies.
Since Georges Lemaître first noted, in 1927, that an expanding universe might be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. While the scientific community was once divided between supporters of two different expanding universe theories, the Big Bang and the Steady State theory, accumulated empirical evidence provides strong support for the former. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1965, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.
Overview.
American astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.
Large particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.
The first subatomic particles included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.
The Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the standard model of Big Bang cosmology, the simplest model that provides a reasonably good account of various observations about the universe.
Timeline.
Singularity.
Extrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity signals the breakdown of general relativity and thus, all the laws of physics. How closely this can be extrapolated toward the singularity is debated—certainly no closer than the end of the Planck epoch. This singularity is sometimes called "the Big Bang", but the term can also refer to the early hot, dense phase itself, which can be considered the "birth" of our universe. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the universe has an estimated age of 13.799 ± 0.021 billion years. The agreement of these three independent measurements strongly supports the ΛCDM model that describes in detail the contents of the universe.
Inflation and baryogenesis.
The earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10−37 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially. After inflation stopped, the universe consisted of a quark–gluon plasma, as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.
Cooling.
The universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10−11 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle physics experiments. At about 10−6 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 1010 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).
A few minutes into the expansion, when the temperature was about a billion (one thousand million; 109; SI prefix giga-) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei. As the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.
Structure formation.
Over a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available (from WMAP) show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an "extended model" which includes hot dark matter in the form of neutrinos, then if the "physical baryon density" Ωbh2 is estimated at about 0.023 (this is different from the 'baryon density' Ωb expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density Ωch2 is about 0.11, the corresponding neutrino density Ωvh2 is estimated to be less than 0.0062.
Cosmic acceleration.
Independent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate. Dark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both observationally and theoretically.
All of this cosmic evolution after the inflationary epoch can be rigorously described and modelled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10−15 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.
Underlying assumptions.
The Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.
These ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10−5. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.
If the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10−5 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.
Expansion of space.
General relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, themselves are specified using a coordinate chart or "grid" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). This metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system the grid expands along with the universe, and objects that are moving only because of the expansion of the universe remain at fixed points on the grid. While their "coordinate" distance (comoving distance) remains constant, the "physical" distance between two such comoving points expands proportionally with the scale factor of the universe.
The Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion "in space", but rather an expansion "of space". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.
Horizons.
An important feature of the Big Bang spacetime is the presence of horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a "past horizon" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never "catch up" to very distant objects. This defines a "future horizon", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe. Our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.
History.
Etymology.
English astronomer Fred Hoyle is credited with coining the term "Big Bang" during a 1949 BBC radio broadcast. It is popularly reported that Hoyle, who favored an alternative "steady state" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.
Development.
The Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a "spiral nebula" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were "island universes" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist and Roman Catholic priest, proposed that the inferred recession of the nebulae was due to the expansion of the universe.
In 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a "primeval atom" where and when the fabric of time and space came into existence.
Starting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.
In the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Monsignor Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, viz., that matter is eternal. A beginning in time was "repugnant" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.
During the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.
After World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the cosmic microwave background radiation (CMB). Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as "this "big bang" idea" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the cosmic microwave background radiation in 1965 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.
In 1968 and 1970, Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called "inflation". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe). In the mid-1990s observations of certain globular clusters appeared to indicate that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.
Significant progress in Big Bang cosmology have been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.
Observational evidence.
The earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the "four pillars" of the Big Bang theory.
Precise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.
Hubble's law and the expansion of space.
Observations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:
where
Hubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.
The theory requires the relation "v" = "HD" to hold at all times, where "D" is the comoving distance, "v" is the recessional velocity, and "v", "H", and "D" vary as the universe expands (hence we write "H"0 to denote the present-day Hubble "constant"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity "v". However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.
That space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.
Measurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the cosmic microwave background over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.
Cosmic microwave background radiation.
[[File:Ilc 9yr moll4096.png|thumb|9 year WMAP image of the cosmic microwave background radiation (2012). The radiation is isotropic to roughly one part in 100,000.|257x257px]]
In 1965, Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.
[[File:Cmbr.svg|thumb|left|The cosmic microwave background spectrum measured by the FIRAS instrument on the COBE satellite is the most-precisely measured black body spectrum in nature. The data points and error bars on this graph are obscured by the theoretical curve.]]
The "surface of last scattering" corresponding to emission of the CMB occurs shortly after "recombination", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.
In 1989 NASA launched the Cosmic Background Explorer satellite (COBE) which made two major advances: in 1990, high-precision spectrum measurements showed the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992 further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 105. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results. During the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.
In early 2003 the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.
Abundance of primordial elements.
Using the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for /, about 10−3 for /, about 10−4 for / and about 10−9 for /.
The measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for , and off by a factor of two for ; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to "tune" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than , and in constant ratios, too.
Galactic evolution and distribution.
Detailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then larger structures have been forming, such as galaxy clusters and superclusters. Populations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures agree well with Big Bang simulations of the formation of structure in the universe and are helping to complete details of the theory.
Primordial gas clouds.
In 2011 astronomers found what they believe to be pristine clouds of primordial gas, by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.
Other lines of evidence.
The age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.
The prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.
On 17 March 2014, astronomers at the Harvard-Smithsonian Center for Astrophysics announced the apparent detection of primordial gravitational waves, which, was shown to be due to galactic dust. On February 11, 2016, the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made first observation of gravitational waves, originating from a pair of merging black holes using the Advanced LIGO detectors.
Future observations.
Future gravitational waves observatories might see primordial gravitational waves, relics of the early universe, up to less than a second of the Big Bang. 
Problems and related issues in physics.
As with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.
Baryon asymmetry.
It is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot, it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effect is not strong enough to explain the present baryon asymmetry.
Dark energy.
Measurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed "dark energy". Dark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the "missing" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.
Negative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.
The dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem sometimes called the "most embarrassing problem in physics" results from the apparent discrepancy between the measured energy density of dark energy and the one naively predicted from Planck units.
Dark matter.
During the 1970s and 80s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.
Indirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.
Additionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter but instead modify the laws of gravity established by Newton and Einstein, but no alternative theory as been as successful as the cold dark matter proposal in explaining all extant observations.
Horizon problem.
The horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.
A resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.
Heisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.
If inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.
A related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.
Magnetic monopoles.
The magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.
Flatness problem.
The flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric. The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density, positive if greater, and zero at the critical density, in which case space is said to be "flat". The problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10−43 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes
(the time of nucleosynthesis), the universe density must have been within one part in 1014 of its critical value, or it would not exist as it does today.
Ultimate fate of the universe.
Before observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch. Alternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.
Modern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.
Speculations.
While the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. "(Also see Planck epoch.)"
One proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.
It is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.
Some proposals, each of which entails untested hypotheses, are:
Proposals in the last two categories, see the Big Bang as an event in either a much larger and older universe or in a multiverse.
Religious and philosophical interpretations.
As a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.

</doc>
<doc id="4119" url="https://en.wikipedia.org/wiki?curid=4119" title="Bock">
Bock

Bock is a strong lager of German origin. Several substyles exist, including maibock (helles bock, heller bock), a paler, more hopped version generally made for consumption at spring festivals; doppelbock (double bock), a stronger and maltier version; and eisbock, a much stronger version made by partially freezing the beer and removing the ice that forms.
Originally a dark beer, a modern bock can range from light copper to brown in colour. The style is very popular, with many examples brewed internationally.
History.
The style known now as "bock" was a dark, malty, lightly hopped ale first brewed in the 14th century by German brewers in the Hanseatic town of Einbeck. The style from Einbeck was later adopted by Munich brewers in the 17th century and adapted to the new lager style of brewing. Due to their Bavarian accent, citizens of Munich pronounced "Einbeck" as "ein Bock" ("a billy goat"), and thus the beer became known as "bock". To this day, as a visual pun, a goat often appears on bock labels.
Bock is historically associated with special occasions, often religious festivals such as Christmas, Easter or Lent (the latter as ""). Bocks have a long history of being brewed and consumed by Bavarian monks as a source of nutrition during times of fasting.
Bockfest, in Cincinnati, Ohio, is the longest running and largest bock beer festival in the world. The festival celebrates bock beer, the Cincinnati neighborhood of Over-the-Rhine, the city's German heritage, and the coming of spring.
The styles of bock.
Traditional bock.
Traditional bock is a sweet, relatively strong (6.3%–7.2% by volume), lightly hopped (20-27 IBUs) lager. The beer should be clear, and colour can range from light copper to brown, with a bountiful and persistent off-white head. The aroma should be malty and toasty, possibly with hints of alcohol, but no detectable hops or fruitiness. The mouthfeel is smooth, with low to moderate carbonation and no astringency. The taste is rich and toasty, sometimes with a bit of caramel. Again, hop presence is low to undetectable, providing just enough bitterness so that the sweetness is not cloying and the aftertaste is muted. The following commercial products are indicative of the style: Einbecker Ur-Bock Dunkel, Pennsylvania Brewing St. Nick Bock, Aass Bock, Great Lakes Rockefeller Bock, Stegmaier Brewhouse Bock.
Maibock.
The maibock style, also known as helles bock or heller bock, is a helles lager brewed to bock strength; therefore, still as strong as traditional bock, but lighter in colour and with more hop presence. It is a fairly recent development compared to other styles of bock beers, frequently associated with springtime and the month of May. Colour can range from deep gold to light amber with a large, creamy, persistent white head, and moderate to moderately high carbonation, while alcohol content ranges from 6.3% to 7.4% by volume. The flavour is typically less malty than a traditional bock, and may be drier, hoppier, and more bitter, but still with a relatively low hop flavour, with a mild spicy or peppery quality from the hops, increased carbonation and alcohol content. The following commercial products are indicative of the style: Ayinger Maibock, Mahr’s Bock, Hacker-Pschorr Hubertus Bock, Capital Maibock, Einbecker Mai-Urbock, Hofbräu Maibock, Victory St. Boisterous, Gordon Biersch Blonde Bock, Smuttynose Maibock, Old Dominion Brewing Company Big Thaw Bock, Brewery 85's Quittin' Time, Rogue Dead Guy Ale, Franconia Brewing Company Maibock Ale, Church Street maibock, and Tröegs Cultivator.
Doppelbock.
"Doppelbock" or "double bock" is a stronger version of traditional bock that was first brewed in Munich by the Paulaner Friars, a Franciscan order founded by St. Francis of Paula. Historically, doppelbock was high in alcohol and sweet, thus serving as "liquid bread" for the Friars during times of fasting, when solid food was not permitted. Today, doppelbock is still strong—ranging from 7%–12% or more by volume. It is clear, with colour ranging from dark gold, for the paler version, to dark brown with ruby highlights for darker version. It has a large, creamy, persistent head (although head retention may be impaired by alcohol in the stronger versions). The aroma is intensely malty, with some toasty notes, and possibly some alcohol presence as well; darker versions may have a chocolate-like or fruity aroma. The flavour is very rich and malty, with toasty notes and noticeable alcoholic strength, and little or no detectable hops (16–26 IBUs). Paler versions may have a drier finish. The monks who originally brewed doppelbock named their beer "Salvator" ("Savior"), which today is trademarked by Paulaner. Brewers of modern doppelbocks often add "-ator" to their beer's name as a signpost of the style; there are 200 "-ator" doppelbock names registered with the German patent office. The following are representative examples of the style: Predator, Paulaner Salvator, Ayinger Celebrator, Weihenstephaner Korbinian, Andechser Doppelbock Dunkel, Spaten Optimator, Augustiner Maximator, Tucher Bajuvator, Weltenburger Kloster Asam-Bock, Capital Autumnal Fire, EKU 28, Eggenberg Urbock 23º, Bell's Consecrator, Moretti La Rossa, Samuel Adams Double Bock, Troegs Troegenator Double Bock, Wasatch Brewery Devastator, Great Lakes Doppelrock, Abita Andygator, and Wolverine State Brewing Company Predator.
Eisbock.
Eisbock is a traditional specialty beer of the Kulmbach district of Germany that is made by partially freezing a doppelbock and removing the water ice to concentrate the flavour and alcohol content, which ranges from 9% to 13% by volume. It is clear, with a colour ranging from deep copper to dark brown in colour, often with ruby highlights. Although it can pour with a thin off-white head, head retention is frequently impaired by the higher alcohol content. The aroma is intense, with no hop presence, but frequently can contain fruity notes, especially of prunes, raisins, and plums. Mouthfeel is full and smooth, with significant alcohol, although this should not be hot or sharp. The flavour is rich and sweet, often with toasty notes, and sometimes hints of chocolate, always balanced by a significant alcohol presence. The following are representative examples of the style: Kulmbacher Reichelbräu Eisbock, Eggenberg, Schneider Aventinus Eisbock, Urbock Dunkel Eisbock, Franconia Brewing Company Ice Bock 17%.
The strongest ice-beer is being produced by a Franconian company as well - it is called Schorschbräu and is 57% (current world record).
International variations.
Europe.
In Austria, bockbier is traditionally brewed only around Christmas and Easter, when nearly every brewery brews its own bock.
A number of bock beers are produced, including Brasserie d'Achouffe Bok and "Leute Bok" from the Van Steenberge brewer, brewed since 1927. Belgium-based InBev produces Artois Bock, which is exported internationally and can be found in areas where bock is not traditionally available.
Zagorka Brewery produces "Stolichno Bock Beer", a 6.5% abv beer.
Bock is an unusual style in the UK, but a few examples exist. The Robert Cains brewery in Liverpool brews Cains Double Bock beer at 8% abv, and the Dark Star Brewery, West Sussex, produce a 5.6% abv Maibock.
Dreher Brewery sells a rather strong (7.3% ABV) bock beer. It is called Bak, the name for billy goat in Hungarian.
A variation of bock called 'bokbier' is also brewed extensively in the Netherlands and occasionally in Belgium. Most larger Dutch breweries, such as Heineken International, Grolsch, Amstel, Alfa Brouwerij, Brand and Dommelsch, market at least one variety. Most bokbiers tend to be seasonal beers (traditionally autumn, although there are currently also spring, summer and winter boks). They are among the only few specialty beers that existed besides lager for a long time. Microbreweries may prefer to seasonally brew a bokbier, such as the eco-beer biobok, made in autumn by Brouwerij 't IJ in Amsterdam. The consumers' organization PINT holds a bok festival every autumn at the Beurs van Berlage in Amsterdam.
Bocks are also brewed in Norway, where they are known as "bokkøl" (bockbeers) and available during the whole year. Notable examples of bock brands are Aass, Borg, Frydenlund and Mack.
Bocks are also brewed in Poland, where they are known as "Koźlak" and available during the whole year. Notable examples of bock brands are Koźlak Amber, Miłosław Koźlak, Cornelius Kożlak, Perła Kożlak.
One of the main beer brands in Portugal is Super Bock.
A Spanish brewery Damm produces Bock Damm (5,4%)
The countries biggest brewer, Laško Brewery, made double bock named Striptis. It is the part of the Special line, which is a response to the rising interest of craft beers.
Only one Bock beer is brewed in Sweden; Mariestad's Old Ox, with an alcoholic percentage of 6,9%.
South America.
There are several beers brewed in Argentina which are termed bock, including "Araucana Negra Bock", "Quilmes Bock", and "Blest Bock".
In Bolivia, the Cerveceria Boliviana Nacional brews a beer called simply "Cerveza Bock," advertised primarily for its 7% alcohol by volume strength.
In Brazil, Kaiser is one of the breweries that sells bock beer, called "Kaiser Bock". This beer is available only in the months of fall and winter (April to September). Usually Brazilian bocks are produced by local breweries or craft breweries, especially in the cities of German settlement in Paraná/Santa Catarina States and also in Petrópolis, state of Rio de Janeiro. Kaiser Bock is made in Ponta Grossa, from Curitiba, capital of Paraná.
Kunstmann Brewery from Valdivia produces a dark, bittersweet version of bock. Kross brewery from Curacavi is producing a maibock (6.3% abv)
Inducerv S.A.S. brews a Bock (6.0% vol) under their Apóstol brand with German ingredients. The brewery is located in the city of Sabaneta, near Medellín.
North America.
Bock is a popular style, made by breweries across the country, including:
Cayman Island Brewing (George Town, Cayman Islands): Ironshore Bock (7.5% abv)
Bock beer is produced in Mexico around Christmas season, under the Noche Buena label with 5.9% abv
Bock and its substyles are popular in all parts of the country. Shiner Bock was first brewed in Texas in 1913 and became a year round brew in 1973. Coors Brewing Company in Golden, Colorado was well known for its bock beers that it formerly brewed every spring. The city of Cincinnati, Ohio has hosted a celebration called Bockfest since 1992 that promotes its German-style brewing history and the German culture of its Over-the-Rhine neighbourhood. A short list of American bocks include:
Africa.
Bock beer is produced and distributed under the Urbock label by Namibian Breweries. Like other Namibian Breweries beers, it is available in some of the neighbouring countries in Southern Africa, especially South Africa. The brewery also produces a maibock sporadically.
Oceania.
"Smokin' Bishop"; a bock-style beer that is brewed at the Invercargill Brewery
Doppel-bock by Monteith's; 6% alcohol

</doc>
<doc id="4124" url="https://en.wikipedia.org/wiki?curid=4124" title="Bantu languages">
Bantu languages

The Bantu languages (), technically the Narrow Bantu languages (as opposed to "Wide Bantu", a loosely defined categorization which includes other Bantoid languages), constitute a traditional branch of the Niger–Congo languages. There are about 250 Bantu languages by the criterion of mutual intelligibility, though the distinction between language and dialect is often unclear, and "Ethnologue" counts 535 languages. Bantu languages are spoken largely east and south of present-day Cameroon, that is, in the regions commonly known as Central Africa, Southeast Africa, and Southern Africa. Parts of the Bantu area include languages from other language families (see map).
The Bantu language with the largest total number of speakers is Swahili; however, the majority of its speakers know it as a second language. According to Ethnologue, there are over 180 million L2 (second-language) speakers, but only about 2 million native speakers.
According to Ethnologue, Shona is the most widely spoken as a first language, with 10.8 million speakers (or 14.2 million if Manyika and Ndau are included), followed closely by Zulu, with 10.3 million. Ethnologue separates the largely mutually intelligible Kinyarwanda and Kirundi, but, if grouped together, they have 12.4 million speakers.
Estimates of number of speakers of most languages vary widely, due both to the lack of accurate statistics in most developing countries and the difficulty in defining exactly where the boundaries of a language lie, particularly in the presence of a dialect continuum.
Origin.
The Bantu languages descend from a common Proto-Bantu language, which is believed to have been spoken in what is now Cameroon in West Africa. An estimated 2,500–3,000 years ago (1000 BC to 500 BC), although other sources put the start of the Bantu Expansion closer to 3000 BC, speakers of the Proto-Bantu language began a series of migrations eastward and southward, carrying agriculture with them. This Bantu expansion came to dominate Sub-Saharan Africa east of Cameroon, an area where Bantu peoples now constitute nearly the entire population.
The technical term Bantu, meaning "human beings" or simply "people", was first used by Wilhelm Bleek (1827–1875), as this is reflected in many of the languages of this group. A common characteristic of Bantu languages is that they use words such as "muntu" or "mutu" for "human being" or in simplistic terms "person", and the plural prefix for human nouns starting with "mu-" (class 1) in most languages is "ba-" (class 2), thus giving "bantu" for "people". Bleek, and later Carl Meinhof, pursued extensive studies comparing the grammatical structures of Bantu languages.
Classification.
The term 'narrow Bantu' was coined by the "Benue–Congo Working Group" to distinguish Bantu as recognized by Malcolm Guthrie in his 1948 classification of the Bantu languages, from the Bantoid languages not recognized as Bantu by Guthrie (1948). In recent times, the distinctiveness of Narrow Bantu as opposed to the other Southern Bantoid groups has been called into doubt (cf. Piron 1995, Williamson & Blench 2000, Blench 2011), but the term is still widely used. A coherent classification of Narrow Bantu will likely need to exclude many of the Zone A and perhaps Zone B languages.
There is no true genealogical classification of the (Narrow) Bantu languages. Most attempted classifications are problematic in that they consider only languages which happen to fall within traditional Narrow Bantu, rather than South Bantoid, which has been established as a unit by the comparative method. The most widely used classification, the alphanumeric coding system developed by Guthrie, is mainly geographic. 
At a broader level, the family is commonly split in two depending on the reflexes of proto-Bantu tone patterns: Many Bantuists group together parts of zones A through D (the extent depending on the author) as "Northwest Bantu" or "Forest Bantu", and the remainder as "Central Bantu" or "Savanna Bantu". The two groups have been described as having mirror-image tone systems: Where Northwest Bantu has a high tone in a cognate, Central Bantu languages generally have a low tone, and vice versa. 
Northwest Bantu is more divergent internally than Central Bantu, and perhaps less conservative due to contact with non-Bantu Niger–Congo languages; Central Bantu is likely the innovative line cladistically. Northwest Bantu is clearly not a coherent family, but even for Central Bantu the evidence is lexical, with little evidence that it is a historically valid group.
The only attempt at a detailed genetic classification to replace the Guthrie system is the 1999 "Tervuren" proposal of Bastin, Coupez, and Mann. However, it relies on lexicostatistics, which, because of its reliance on similarity rather than shared innovations, may predict spurious groups of conservative languages which are not closely related. Meanwhile, "Ethnologue" has added languages to the Guthrie classification that Guthrie overlooked, while removing the Mbam languages (much of zone A), and shifting some languages between groups (much of zones D and E to a new zone J, for example, and part of zone L to K, and part of M to F) in an apparent effort at a semi-genetic, or at least semi-areal, classification. This has been criticized for sowing confusion in one of the few unambiguous ways to distinguish Bantu languages. Nurse & Philippson (2006) evaluate many proposals for low-level groups of Bantu languages, but the result is not a complete portrayal of the family. "Glottolog" has incorporated many of these into their classification.
Nonetheless, some version of zone S (Southern Bantu) does appear to be a coherent group. The languages which share Dahl's Law may also form a valid group, Northeast Bantu. The infobox at right lists these together with various low-level groups that are fairly uncontroversial, though they continue to be revised. The development of a rigorous genealogical classification of many branches of Niger–Congo, not just Bantu, is hampered by insufficient data.
Language structure.
Guthrie reconstructed both the phonemic inventory and the vocabulary of Proto-Bantu.
The most prominent grammatical characteristic of Bantu languages is the extensive use of affixes (see Sotho grammar and Ganda noun classes for detailed discussions of these affixes). Each noun belongs to a class, and each language may have several numbered classes, somewhat like genders in European languages. The class is indicated by a prefix that is part of the noun, as well as agreement markers on verb and qualificative roots connected with the noun. Plural is indicated by a change of class, with a resulting change of prefix.
The verb has a number of prefixes, though in the western languages these are often treated as independent words. In Swahili, for example, "Mtoto mdogo amekisoma" (also "Kamwana kadoko kariverenga" in Shona language) means 'The small child has read it book'. "Mtoto" 'child' governs the adjective prefix "m-" and the verb subject prefix "a-". Then comes perfect tense "-me-" and an object marker "-ki-" agreeing with implicit "kitabu" 'book' (from Arabic "kitabu"). Pluralizing to 'children' gives "Watoto wadogo wamekisoma" ("Vana vadoko vakaverenga" in Shona), and pluralizing to 'books' ("vitabu") gives "Watoto wadogo wamevisoma".
Bantu words are typically made up of open syllables of the type CV (consonant-vowel) with most languages having syllables exclusively of this type. The Bushong language recorded by Vansina, however, has final consonants, while slurring of the final syllable (though written) is reported as common among the Tonga of Malawi. The morphological shape of Bantu words is typically CV, VCV, CVCV, VCVCV, etc.; that is, any combination of CV (with possibly a V- syllable at the start). In other words, a strong claim for this language family is that almost all words end in a vowel, precisely because closed syllables (CVC) are not permissible in most of the documented languages, as far as is understood. 
This tendency to avoid consonant clusters in some positions is important when words are imported from English or other non-Bantu languages. An example from Chewa: the word "school", borrowed from English, and then transformed to fit the sound patterns of this language, is "sukulu". That is, "sk-" has been broken up by inserting an epenthetic "-u-"; "-u" has also been added at the end of the word. Another example is "buledi" for "bread". Similar effects are seen in loanwords for other non-African CV languages like Japanese. However, a clustering of sounds at the beginning of a syllable can be readily observed in such languages as Shona, and the Makua variants.
Reduplication.
Reduplication is a common morphological phenomenon in Bantu languages and is usually used to indicate frequency or intensity of the action signalled by the (unreduplicated) verb stem.
Well-known words and names that have reduplication include
Repetition emphasizes the repeated word in the context that it is used. For instance, "Mwenda pole hajikwai," while, "Pole pole ndio mwendo," has two to emphasize the consistency of slowness of the pace. The meaning of the former in translation is, "He who goes slowly doesn't trip," and that of the latter is, "A slow but steady pace wins the race." Haraka haraka would mean hurrying just for the sake of hurrying, reckless hurry, as in "Njoo! Haraka haraka" here! Hurry, hurry.
On the contrary to the above definition, there are some words in some of the languages in which reduplication has the opposite meaning. It usually denotes short durations, and or lower intensity of the action and also means a few repetitions or a little bit more.
Noun Class.
Here is an ongoing list of nominal classes in Bantu:
Notable Bantu languages.
Following are the principal Bantu languages of each country. Included are those languages that constitute at least 1% of the population and have at least 10% the number of speakers of the largest Bantu language in the country.
Most languages are best known in English without the class prefix ("Swahili", "Tswana", "Ndebele"), but are sometimes seen with the (language-specific) prefix ("Kiswahili", "Setswana", "Sindebele"). In a few cases prefixes are used to distinguish languages with the same root in their name, such as Tshiluba and Kiluba (both "Luba"), Umbundu and Kimbundu (both "Mbundu"). The bare (prefixless) form typically does not occur in the language itself, but is the basis for other words based on the ethnicity. So, in the country of Botswana the people are the "Batswana", one person is a "Motswana", and the language is "Setswana"; and in Uganda, centred on the kingdom of "Buganda", the dominant ethnicity are the "Baganda" (sg. "Muganda"), whose language is "Luganda".
Lingua franca
Angola
Botswana
Burundi
Cameroon
Central African Republic
Democratic Republic of the Congo
Equatorial Guinea
Gabon
Kenya
Lesotho
Malawi
Mozambique
Nigeria
Namibia
Republic of the Congo (Congo-Brazzaville)
Rwanda
South Africa
Swaziland
Tanzania
Uganda
Zambia
Zimbabwe
This list is incomplete; an attempt at a full list of Bantu languages (with various conflations and a puzzlingly diverse nomenclature) was found in "The Bantu Languages of Africa", 1959.
Bantu words popularised in western cultures.
Some words from various Bantu languages have been borrowed into western languages. These include: 
A case has been made out for borrowings of many place-names and even misremembered rhymes – chiefly from one of the Luba varieties – in the USA.
Writing systems.
Along with the Latin script and Arabic script orthographies, there are also some modern indigenous writing systems used for Bantu languages:

</doc>
<doc id="4127" url="https://en.wikipedia.org/wiki?curid=4127" title="Bearing">
Bearing

Bearing may refer to:

</doc>
<doc id="4130" url="https://en.wikipedia.org/wiki?curid=4130" title="CIM-10 Bomarc">
CIM-10 Bomarc

The Boeing CIM-10 Bomarc (IM-99 Weapon System prior to September 1962) was a supersonic long-range surface-to-air missile (SAM) used during the Cold War for the air defense of North America. In addition to being the first operational long-range SAM, it was the only SAM deployed by the US Air Force.
Stored horizontally in a launcher shelter with movable roof, the missile was erected, fired vertically using rocket boosters to high altitude, and then tipped over into a horizontal Mach 2.5 cruise powered by ramjet engines. This lofted trajectory allowed the missile to operate at a maximum range as great as . Controlled from the ground for most of its flight, when it reached the target area it was commanded to begin a dive, activating an onboard radar for terminal guidance. A radar proximity fuse detonated the warhead, either a large conventional explosive or the W40 nuclear warhead. 
The Air Force original planned for a total of 52 sites covering most of the major cities and industrial sites in the US. The US Army were deploying their own systems at the same time, and the two services fought constantly both in political circles and in the press. Development dragged on, and by the time it was ready for deployment in the late 1950s, the nuclear threat had moved from manned bombers to the intercontinental ballistic missile (ICBM), while the Army had successfully deployed their own system that filled any possible role in the 1960s, in spite of Air Force claims to the contrary.
As testing continued, the Air Force reduced its plans to sixteen sites, and then again to eight with an additional two sites in Canada. The first US site was declared operational in 1959, but with only a single working missile. Bringing the rest of the missiles into service took years, by which time the system was obsolete. Deactivations began in 1969 and by 1972 all Bomarc sites had been shut down. A small number were used as target drones, and only a few remain on display today.
Design and development.
In 1946, Boeing started to study surface-to-air guided missiles under the United States Army Air Forces project MX-606. By 1950, Boeing had launched more than 100 test rockets in various configurations, all under the designator XSAM-A-1 GAPA (Ground-to-Air Pilotless Aircraft). Because these tests were very promising, Boeing received a USAF contract in 1949 to develop a pilotless interceptor (a term then used by the USAF for air-defense guided missiles) under project MX-1599. The MX-1599 missile was to be a ramjet-powered, nuclear-armed long-range surface-to-air missile to defend the Continental United States from high-flying bombers. The Michigan Aerospace Research Center (MARC) was added to the project soon afterward, and this gave the new missile its name Bomarc (for Boeing and MARC). In 1951, the USAF decided to emphasize its point of view that missiles were nothing else than pilotless aircraft by assigning aircraft designators to its missile projects, and anti-aircraft missiles received F-for-Fighter designations. The Bomarc became the F-99.
Test flights of XF-99 test vehicles began in September 1952 and continued through early 1955. The XF-99 tested only the liquid-fueled booster rocket, which would accelerate the missile to ramjet ignition speed. In February 1955, tests of the XF-99A propulsion test vehicles began. These included live ramjets, but still had no guidance system or warhead. The designation YF-99A had been reserved for the operational test vehicles. In August 1955, the USAF discontinued the use of aircraft-like type designators for missiles, and the XF-99A and YF-99A became XIM-99A and YIM-99A, respectively. Originally the USAF had allocated the designation IM-69, but this was changed (possibly at Boeing's request to keep number 99) to IM-99 in October 1955. In October 1957, the first YIM-99A production-representative prototype flew with full guidance, and succeeded to pass the target within destructive range. In late 1957, Boeing received the production contract for the IM-99A Bomarc A interceptor missile, and in September 1959, the first IM-99A squadron became operational.
The IM-99A had an operational radius of and was designed to fly at Mach 2.5–2.8 at a cruising altitude of . It was long and weighed . Its armament was either a conventional warhead or a W40 nuclear warhead (7–10 kiloton yield). A liquid-fuel rocket engine boosted the Bomarc to Mach 2, when its Marquardt RJ43-MA-3 ramjet engines, fueled by 80-octane gasoline, would take over for the remainder of the flight. This was the same model of engine used to power both the Lockheed X-7, the Lockheed AQM-60 Kingfisher drone used to test air defenses, and the Lockheed D-21 launched off the back of an SR-71 .
The operational IM-99A missiles were based horizontally in semi-hardened shelters, nicknamed "coffins". After the launch order, the shelter's roof would slide open, and the missile raised to the vertical. After the missile was supplied with fuel for the booster rocket, it would be launched by the Aerojet General LR59-AJ-13 booster. After sufficient speed was reached, the Marquardt RJ43-MA-3 ramjets would ignite and propel the missile to its cruise speed and altitude of Mach 2.8 at .
When the Bomarc was within of the target, its own Westinghouse AN/DPN-34 radar guided the missile to the interception point. The maximum range of the IM-99A was , and it was fitted with either a conventional high-explosive or a 10 kiloton W-40 nuclear fission warhead.
The Bomarc relied on the Semi-Automatic Ground Environment (SAGE), an automated control system used by NORAD for detecting, tracking and intercepting enemy bomber aircraft. SAGE allowed for remote launching of the Bomarc missiles, which were housed in a constant combat-ready basis in individual launch shelters in remote areas. At the height of the program, there were 14 Bomarc sites located in the United States and two in Canada.
The liquid-fuel booster of the Bomarc A was no optimal solution. It took two minutes to fuel before launch, which could be a long time in high-speed intercepts, and its hypergolic propellants (hydrazine and nitric acid) were very dangerous to handle, leading to several serious accidents.
As soon as high-thrust solid-fuel rockets became a reality in the mid-1950s, the USAF began to develop a new solid-fueled Bomarc variant, the IM-99B Bomarc B. It used a Thiokol XM51 booster, and also had improved Marquardt RJ43-MA-7 (and finally the RJ43-MA-11) ramjets. The first IM-99B was launched in May 1959, but problems with the new propulsion system delayed the first fully successful flight until July 1960, when a supersonic KD2U-1/MQM-15A Regulus II drone was intercepted. Because the new booster took up less space in the missile, more ramjet fuel could be carried, increasing the range to . The terminal homing system was also improved, using the world's first pulse Doppler search radar, the Westinghouse AN/DPN-53. All Bomarc Bs were equipped with the W-40 nuclear warhead. In June 1961, the first IM-99B squadron became operational, and Bomarc B quickly replaced most Bomarc A missiles. On 23 March 1961, a Bomarc B successfully intercepted a Regulus II cruise missile flying at 100,000 ft, thus achieving the highest interception in the world up to that date.
Boeing built 570 Bomarc missiles between 1957 and 1964, 269 CIM-10A, 301 CIM-10B.
In September 1958 Air Research & Development Command decided to transfer the Bomarc program from its testing at Cape Canaveral Air Force Station to a new facility on Santa Rosa Island, immediately south of Eglin AFB Hurlburt Field on the Gulf of Mexico. To operate the facility and to provide training and operational evaluation in the missile program, Air Defense Command established the 4751st Air Defense Wing (Missile) (4751st ADW) on 15 January 1958. The first launch from Santa Rosa took place on 15 January 1959.
Operational history.
United States.
The first USAF operational Bomarc squadron was the 46th Air Defense Missile Squadron (ADMS), organized on 1 January 1959 and activated on 25 March. The 46th ADMS was assigned to the New York Air Defense Sector at McGuire Air Force Base, New Jersey. The training program, under the 4751st ADW used technicians acting as instructors and was established for a four-month duration. Training included missile maintenance; SAGE operations and launch procedures, including the launch of an unarmed missile at Eglin. In September 1959 the squadron assembled at their permanent station, the Bomarc site near McGuire AFB, and trained for operational readiness. The first Bomarc-A were used at McGuire on 19 September 1959 with Kincheloe AFB getting the first operational IM-99Bs. While several of the squadrons replicated earlier fighter interceptor unit numbers, they were all new organizations with no previous historical counterpart.
ADC's initial plans called for some 52 Bomarc sites around the United States with 120 missiles each but as defense budgets decreased during the 1950s the number of sites dropped substantially. Ongoing development and reliability problems didn't help, nor did Congressional debate over the missile's usefulness and necessity. In June 1959, the Air Force authorized 16 Bomarc sites with 56 missiles each; the initial five would get the IM-99A with the remainder getting the IM-99B. However, in March 1960, HQ USAF cut deployment to eight sites in the United States and two in Canada.
Bomarc incident.
Within a year of operations, a Bomarc-A with a nuclear warhead caught fire at McGuire AFB on 7 June 1960 after its on-board helium tank exploded. While the missile's explosives did not detonate, the heat melted the warhead and released plutonium, which the fire crews spread. The Air Force and the Atomic Energy Commission cleaned up the site and covered it with concrete. This was the only major incident involving the weapons system. The site remained in operation for several years following the fire. Since its closure in 1972, the area has remained off limits, primarily due to low levels of plutonium contamination. In 2002, the concrete at the site was removed and transported to Lakehurst Naval Air Station for transport by rail to a site for proper disposal.
Modification and inactivation.
In 1962, the US Air Force started using modified A-models as drones; following the October 1962 tri-service redesignation of aircraft and weapons systems they became CQM-10As. Otherwise the air defense missile squadrons maintained alert while making regular trips to Santa Rosa Island for training and firing practice. After the inactivation of the 4751st ADW(M) on 1 July 1962 and transfer of Hurlburt to Tactical Air Command for air commando operations the 4751st Air Defense Squadron (Missile) remained at Hurlburt and Santa Rosa Island for training purposes.
In 1964, the liquid-fueled Bomarc-A sites and squadrons began to be inactivated. The sites at Dow and Suffolk County closed first. The remainder continued to be operational for several more years while the government started dismantling the air defense missile network. Niagara Falls was the first BOMARC B installation to close, in December 1969; the others remained on alert through 1972. In April 1972, the last Bomarc B in U.S. Air Force service was retired at McGuire and the 46th ADMS inactivated and the base was deactivated.
In the era of the intercontinental ballistic missiles the Bomarc, designed to intercept relatively slow manned bombers, had become a useless asset. The remaining Bomarc missiles were used by all armed services as high-speed target drones for tests of other air-defense missiles. The Bomarc A and Bomarc B targets were designated as CQM-10A and CQM-10B, respectively.
Due to the accident, the McGuire complex has never been sold or converted to other uses and remains in Air Force ownership, making it the most intact site of the eight in the US. It has been nominated to the National Register of Historic Sites. Although a number of IM-99/CIM-10 Bomarcs have been placed on public display, because of concerns about the possible environmental hazards of the thoriated magnesium structure of the airframe several have been removed from public view.
Russ Sneddon, director of the Air Force Armament Museum, Eglin Air Force Base, Florida provided information about missing CIM-10 exhibit airframe serial 59-2016, one of the museum's original artifacts from its founding in 1975 and donated by the 4751st Air Defense Squadron at Hurlburt Field, Eglin Auxiliary Field 9, Eglin AFB. As of December 2006, the suspect missile was stored in a secure compound behind the Armaments Museum. In December 2010, the airframe was still on premises, but partially dismantled.
Canada.
The Bomarc Missile Program was highly controversial in Canada. The Progressive Conservative government of Prime Minister John Diefenbaker initially agreed to deploy the missiles, and shortly thereafter controversially scrapped the Avro Arrow, a supersonic manned interceptor aircraft, arguing that the missile program made the Arrow unnecessary.
Initially, it was unclear whether the missiles would be equipped with nuclear warheads. By 1960 it became known that the missiles were to have a nuclear payload, and a debate ensued about whether Canada should accept nuclear weapons. Ultimately, the Diefenbaker government decided that the Bomarcs should not be equipped with nuclear warheads. The dispute split the Diefenbaker Cabinet, and led to the collapse of the government in 1963. The Official Opposition and Liberal Party leader Lester "Mike" Pearson originally was against nuclear missiles, but reversed his personal position and argued in favor of accepting nuclear warheads. He won the 1963 election, largely on the basis of this issue, and his new Liberal government proceeded to accept nuclear-armed Bomarcs, with the first being deployed on 31 December 1963. When the nuclear warheads were deployed, Pearson's wife, Maryon, resigned her honorary membership in the anti-nuclear weapons group, Voice of Women.
Canadian operational deployment of the Bomarc involved the formation of two specialized Surface/Air Missile squadrons. The first to begin operations was No. 446 SAM Squadron at RCAF Station North Bay, Ontario which was the command and control center for both squadrons. With construction of the compound and related facilities completed in 1961, the squadron received its Bomarcs in 1961, without nuclear warheads. The squadron became fully operational from 31 December 1963, when the nuclear warheads arrived, until disbanding on 31 March 1972. All the warheads were stored separately and under control of Detachment 1 of the USAF 425th Munitions Maintenance Squadron. During operational service, the Bomarcs were maintained on stand-by, on a 24-hour basis, but were never fired, although the squadron test-fired the missiles at Eglin AFB, Florida on annual winter retreats.
No. 447 SAM Squadron operating out of RCAF Station La Macaza, Quebec was activated on 15 September 1962 although warheads were not delivered until late 1963. The squadron followed the same operational procedures as No. 446, its sister squadron. With the passage of time the operational capability of the 1950s-era Bomarc system no longer met modern requirements; the Department of National Defence deemed that the Bomarc missile defense was no longer a viable system, and ordered both squadrons to be stood down in 1972. The bunkers and ancillary facilities remain at both former sites.
Operators.
Locations under construction but not activated. Each site was programmed for 28 IM-99B missiles:
Reference for BOMARC units and locations:
Surviving missiles.
Below is a list of museums or sites which have a Bomarc missile on display:

</doc>
<doc id="4132" url="https://en.wikipedia.org/wiki?curid=4132" title="Branco River">
Branco River

The Branco River (; Engl: "White River") is the principal affluent of the Rio Negro from the north. It is enriched by many streams from the Tepui highlands which separate Venezuela and Guyana from Brazil. Its two upper main tributaries are the Uraricoera and the Takutu. The latter almost links its sources with those of the Essequibo.
The Branco flows nearly south, and finds its way into the Negro through several channels and a chain of lagoons similar to those of the latter river. It is long, up to its Uraricoera confluence. It has numerous islands, and, above its mouth, it is broken by a bad series of rapids.

</doc>
<doc id="4146" url="https://en.wikipedia.org/wiki?curid=4146" title="Bus">
Bus

A bus (archaically also omnibus, multibus, motorbus) is a road vehicle designed to carry many passengers. Buses can have a capacity as high as 300 passengers. The most common type of bus is the single-decker rigid bus, with larger loads carried by double-decker and articulated buses, and smaller loads carried by midibuses and minibuses; coaches are used for longer-distance services. Many types of buses, such as city transit buses and inter-city coaches, charge a fare. Other types, such as elementary or secondary school buses or shuttle buses within a post-secondary education campus do not charge a fare. In many jurisdictions, bus drivers require a special licence above and beyond a regular driver's licence.
Buses may be used for scheduled bus transport, scheduled coach transport, school transport, private hire, or tourism; promotional buses may be used for political campaigns and others are privately operated for a wide range of purposes, including rock and pop band tour vehicles.
Horse-drawn buses were used from the 1820s, followed by steam buses in the 1830s, and electric trolleybuses in 1882.The first internal combustion engine buses, or motor buses, were used in 1895. Recently, interest has been growing in hybrid electric buses, fuel cell buses, and electric buses, as well as ones powered by compressed natural gas or biodiesel. As of the 2010s, bus manufacturing is increasingly globalised, with the same designs appearing around the world.
Etymology.
Bus is a clipped form of the Latin word "omnibus". It appeared in Paris in 1819–20 as "(voiture) omnibus" meaning "(carriage) for all", and appeared in London in 1829. One etymology holds that "omnibus" is derived from a hatter's shop which was situated in front of one of the first bus stations in Nantes, France in 1823. "Omnes Omnibus" was a pun on the Latin-sounding name of that hatter Omnès: "omnes" meaning "all" and "omnibus" means "for all" in Latin. Nantes citizens soon gave the nickname Omnibus to the vehicle.
History.
Early history.
A short-lived early public bus line (known as a "carriage" at that time) was launched by Blaise Pascal in Paris in 1662; it was quite popular until fares were increased and access to the service was restricted to high-society members by regulation and law. Services ceased after 15 years and no further such services were known until the 1820s.
First omnibus services.
John Greenwood is said by some historians to have established the first modern omnibus service in 1824. As the keeper of a toll gate in Pendleton on the Manchester-to-Liverpool turnpike, he purchased a horse and a cart with several seats, and began an omnibus service between those two locations. His pioneering idea was to offer a service where, unlike with a stagecoach, no prior booking was necessary and the driver would pick up or set down passengers anywhere on request. Later on, he added daily services to Buxton, Chester, and Sheffield. His line immediately sparked fierce competition and a dense network of omnibus services quickly sprouted in the area, often acting as feeders to the railways. In 1865, Greenwood's company and its competitors amalgamated into the Manchester Carriage Company.
The Paris omnibus was started in 1828 by a businessman named Stanislas Baudry, who had begun the first French omnibus line in Nantes in 1826. The name was said to come from the station of the first line in Nantes, in front of the store of a hat-maker named Omnes, who had a large sign on his building saying "Omnes Omnibus" ("All for all" in Latin). Following success in Nantes, Baudry moved to Paris and founded the Enterprise des Omnibus on rue de Lancre, with workshops on the quai de Jemmapes. In 1827 he commissioned an English coach-maker, George Shillibeer, to design a vehicle that could be stable and carry a large number of passengers. Shillibeer's design worked, On 28 April 1828, the first Paris omnibus began service, running every fifteen minutes between La Madeleine and la Bastille. Before long, there were one hundred omnibuses in service, with eighteen different itineraries. A journey cost twenty-five centimes. The omnibuses circulated between seven in the morning and seven in the evening; each omnibus could carry between twelve and eighteen passengers. The busiest line was that along the Grand Boulevards; it ran from eight in the morning until midnight. 
The Paris omnibus service was an immediate popular success, with more than two and a half million passengers in the first six months. However, there was no reliable way to collect money from the passengers, or the fare collectors kept much of the money for themselves; In its first years the company was continually on the verge of bankruptcy, and in despair, Baudry committed suicide in February 1830. Baudry's partners reorganized the company and managed to keep it in business.
In September 1828, a competing company, les Dames-Blanches, had started running its own vehicles. In 1829 and the following years, more companies with poetic names entered the business; les Citadines, les Tricycles, les Orléanises, les Diligentes, les Écossaises, les Béarnaises, les Carolines, les Batignollaises, les Parisiennes, les Hirondelles, les Joséphines, les Excellentes, les Sylphides, les Constantines, les Dames-Françaises, les Algériennes, les Dames-Réunies, and les Gazelles. The omnibus had a profound effect on Parisian life, making it possible for Parisians to work and have a social life outside their own neighborhoods.
By 1845, there were thirteen companies in Paris operating twenty-twenty three omnibus lines. In 1855, Napoleon III had them combined into a single company, the Compagnie générale des omnibus, with a monopoly on Paris public transportation. Beginning in 1873, they were gradually replaced by tramways, and, beginning in 1906, by the "omnibus automobile", or motor bus. The last horse-drawn Paris omnibus ran on 11 January 1913, from Saint-Sulpice to La Villette. 
Shillibeer built another bus for the Quaker Newington Academy for Girls near London; this had a total of 25 seats, and entered history as the first school bus.
Shillibeer saw the success of the Paris omnibus in service and concluded that operating similar vehicles in London, for the fare-paying public with multiple stops, would be a paying enterprise, so he returned to his native city. His first London "Omnibus", using the same design and name as the Paris vehicle, took up service on 4 July 1829 on the route between Paddington (The Yorkshire Stingo) and "Bank" (Bank of England) via the "New Road" (now Marylebone Rd), Somers Town and City Road. Four services were provided in each direction daily. Shillibeer's success prompted many competitors to enter the market, and for a time buses were referred to as 'Shillibeers'.
Although passenger-carrying carriages had operated for many years, the new 'omnibus' pioneered a new service of picking up and setting down customers all along a particular route without the need to book in advance. Buses soon expanded their capacity, with additional seats for a few extra passengers provided alongside the driver. By 1845, passengers were being accommodated on the curved roofs, seated back to back in a configuration known as 'knife-board'. In 1852, Greenwood's in Manchester introduced the double-decker vehicle that could seat up to 42.
In Germany, the first bus service was established in Berlin in 1825, running from Brandenburger Tor to Charlottenburg. In 1850, Thomas Tilling started horse bus services in London, and in 1855, the London General Omnibus Company was founded to amalgamate and regulate the horse-drawn omnibus services then operating in London.
By the 1880s, bus services were a commonplace in England, continental Europe, and North America; one company in London was operating over 220 horse-buses. Horse-bus use declined with the advent of steam-buses and motor-buses; the last horse bus in London stopped operation in 1914.
Steam buses.
Regular intercity bus services by steam-powered buses were pioneered in England in the 1830s by Walter Hancock and by associates of Sir Goldsworthy Gurney, among others, running reliable services over road conditions which were too hazardous for horse-drawn transportation.
The first 'mechanically propelled omnibus appeared on the streets of London on 22 April 1833. Steam carriages were much less likely to overturn, they travelled faster than horse-drawn carriages, they were much cheaper to run, and caused much less damage to the road surface due to their wide tyres.
However, the heavy road tolls imposed by the turnpike trusts discouraged steam road vehicles and left the way clear for the horse bus companies, and from 1861 onwards, harsh legislation virtually eliminated mechanically propelled vehicles altogether from the roads of Great Britain for 30 years, the Locomotive Act of that year imposing restrictive speed limits on "road locomotives" of 5 mph in towns and cities, and 10 mph in the country.
Trolleybuses.
In parallel to the development of the bus was the invention of the electric trolleybus, typically fed through trolley poles by overhead wires. The Siemens brothers, William in England and Ernst Werner in Germany, collaborated on the development of the trolleybus concept. Sir William first proposed the idea in an article to the "Journal of the Society of Arts" in 1881 as an "...arrangement by which an ordinary omnibus...would have a suspender thrown at intervals from one side of the street to the other, and two wires hanging from these suspenders; allowing contact-rollers to run on these two wires, the current could be conveyed to the tram-car, and back again to the dynamo machine at the station, without the necessity of running upon rails at all."
The first such vehicle, the Electromote, was made by his brother Dr. Ernst Werner von Siemens and presented to the public in 1882 in Halensee, Germany. Although this experimental vehicle fulfilled all the technical criteria of a typical trolleybus, it was dismantled in the same year after the demonstration.
Max Schiemann opened a passenger-carrying trolleybus in 1901 near Dresden, in Germany. Although this system operated only until 1904, Schiemann had developed what is now the standard trolleybus current collection system. In the early days, a few other methods of current collection were used. Leeds and Bradford became the first cities to put trolleybuses into service in Great Britain on 20 June 1911.
Motor buses.
In Siegerland, Germany, two passenger bus lines ran briefly, but unprofitably, in 1895 using a six-passenger motor carriage developed from the 1893 Benz Viktoria. Another commercial bus line using the same model Benz omnibuses ran for a short time in 1898 in the rural area around Llandudno, Wales.
Daimler also produced one of the earliest motor-bus models in 1898, selling a double-decker bus to the Motor Traction Company for use on the streets of London. The vehicle had a maximum speed of 18 kph and accommodated up to 20 passengers, in an enclosed area below and on an open-air platform above. With the success and popularity of this bus, Daimler expanded production, selling more buses to companies in London and, in 1899, to Stockholm and Speyer. Daimler also entered into a partnership with the British company Milnes and developed a new double-decker in 1902 that became the market standard.
The first mass-produced bus model was the B-type double-decker bus, designed by Frank Searle and operated by the London General Omnibus Company – it entered service in 1910, and almost 3,000 had been built by the end of the decade. Hundreds saw military service on the Western Front during the First World War.
The Yellow Coach Manufacturing Company, which rapidly became a major manufacturer of buses in the US, was founded in Chicago in 1923 by John D. Hertz. General Motors purchased a majority stake in 1925 and changed its name to the Yellow Truck and Coach Manufacturing Company. They then purchased the balance of the shares in 1943 to form the GM Truck and Coach Division.
Models expanded in the 20th century, leading to the widespread introduction of the contemporary recognizable form of full-sized buses from the 1950s. The Routemaster, developed in the 1950s, was a pioneering design and remains an icon of London to this day. The innovative design used lightweight aluminium and techniques developed in aircraft production during World War II. As well as a novel weight-saving integral design, it also introduced for the first time on a bus independent front suspension, power steering, a fully automatic gearbox, and power-hydraulic braking.
Types.
Formats include single-decker bus, double-decker bus (both usually with a rigid chassis) and articulated bus (or 'bendy-bus') the prevalence of which varies from country to country. Bi-articulated buses are also manufactured, and passenger-carrying trailers—either towed behind a rigid bus (a bus trailer), or hauled as a trailer by a truck (a trailer bus). Smaller midibuses have a lower capacity and open-top buses are typically used for leisure purposes. In many new fleets, particularly in local transit systems, a shift to low-floor buses is occurring, primarily for easier accessibility. Coaches are designed for longer-distance travel and are typically fitted with individual high-backed reclining seats, seat belts, toilets, and audio-visual entertainment systems, and can operate at higher speeds with more capacity for luggage. Coaches may be single- or double-deckers, articulated, and often include a separate luggage compartment under the passenger floor. Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes.
Bus manufacturing may be by a single company (an integral manufacturer), or by one manufacturer's building a bus body over a chassis produced by another manufacturer.
Design.
Accessibility.
Transit buses used to be mainly high-floor vehicles. However, they are now increasingly of low-floor design and optionally also 'kneel' air suspension and have electrically or hydraulically extended under-floor ramps to provide level access for wheelchair users and people with baby carriages. Prior to more general use of such technology, these wheelchair users could only use specialist paratransit mobility buses.
Accessible vehicles also have wider entrances and interior gangways and space for wheelchairs. Interior fittings and destination displays may also be designed to be usable by the visually impaired. Coaches generally use wheelchair lifts instead of low-floor designs. In some countries, vehicles are required to have these features by disability discrimination laws.
Configuration.
Buses were initially configured with an engine in the front and an entrance at the rear. With the transition to one-man operation, many manufacturers moved to mid- or rear-engined designs, with a single door at the front or multiple doors. The move to the low-floor design has all but eliminated the mid-engined design, although some coaches still have mid-mounted engines. Front-engined buses still persist for niche markets such as American school buses, some minibuses, and buses in less developed countries, which may be derived from truck chassis, rather than purpose-built bus designs. Most buses have two axles, articulated buses have three.
Guidance.
Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes. Guidance can be mechanical, optical, or electromagnetic. Extensions of the guided technology include the Guided Light Transit and Translohr systems, although these are more often termed 'rubber-tyred trams' as they have limited or no mobility away from their guideways.
Liveries.
Transit buses are normally painted to identify the operator or a route, function, or to demarcate low-cost or premium service buses. Liveries may be painted onto the vehicle, applied using adhesive vinyl technologies, or using decals. Vehicles often also carry bus advertising or part or all of their visible surfaces (as mobile billboard). Campaign buses may be decorated with key campaign messages; these can be to promote an event or initiative.
Propulsion.
The most common power source since the 1920s has been the diesel engine. Early buses, known as trolleybuses, were powered by electricity supplied from overhead lines. Nowadays, electric buses often carry their own battery, which is sometimes recharged on stops/stations to keep the size of the battery small/lightweight. Currently, interest exists in hybrid electric buses, fuel cell buses, electric buses, and ones powered by compressed natural gas or biodiesel. Gyrobuses, which are powered by the momentum stored by a flywheel, were tried in the 1940s.
Dimensions.
United Kingdom
Maximum Length: Single rear axle 13.5 meters. Twin rear axle 15 meters.<br>
Maximum Width: 2.55 meters
United States
Maximum Length: None <br>
Maximum Width: 2.6 meters
Manufacture.
Early bus manufacturing grew out of carriage coachbuilding, and later out of automobile or truck manufacturers. Early buses were merely a bus body fitted to a truck chassis. This body+chassis approach has continued with modern specialist manufacturers, although there also exist integral designs such as the Leyland National where the two are practically inseparable. Specialist builders also exist and concentrate on building buses for special uses, or modifying standard buses into specialised products.
Integral designs have the advantages that they are have been well-tested for strength and stability, and also are off-the-shelf. However, two incentives cause use of the chassis+body model. First, it allows the buyer and manufacturer both to shop for the best deal for their needs, rather than having to settle on one fixed design—the buyer can choose the body and the chassis separately. Second, over the lifetime of a vehicle (in constant service and heavy traffic), it will likely get minor damage now and again, and being able easily to replace a body panel or window etc. can vastly increase its service life and save the cost and inconvenience of removing it from service.
As with the rest of the automotive industry, into the 20th century, bus manufacturing increasingly became globalized, with manufacturers producing buses far from their intended market to exploit labour and material cost advantages. As with the cars, new models are often exhibited by manufacturers at prestigious industry shows to gain new orders. A typical city bus costs almost US$450,000.
Uses.
Public transport.
Transit buses, used on public transport bus services, have utilitarian fittings designed for efficient movement of large numbers of people, and often have multiple doors. Coaches are used for longer-distance routes. High-capacity bus rapid transit services may use the bi-articulated bus or tram-style buses such as the Wright StreetCar and the Irisbus Civis.
Buses and coach services often operate to a predetermined published public transport timetable defining the route and the timing, but smaller vehicles may be used on more flexible demand responsive transport services.
Tourism.
Buses play a major part in the tourism industry. Tour buses around the world allow tourists to view local attractions or scenery. These are often open-top buses, but can also be by regular bus or coach.
In local sightseeing, City Sightseeing is the largest operator of local tour buses, operating on a franchised basis all over the world. Specialist tour buses are also often owned and operated by safari parks and other theme parks or resorts. Longer-distance tours are also carried out by bus, either on a turn up and go basis or through a tour operator, and usually allow disembarkation from the bus to allow touring of sites of interest on foot. These may be day trips or longer excursions incorporating hotel stays. Tour buses often carry a tour guide, although the driver or a recorded audio commentary may also perform this function. The tour operator may itself be a subsidiary of a company that operates buses and coaches for other uses, or an independent company that charters buses or coaches. Commuter transport operators may also use their coaches to conduct tours within the target city between the morning and evening commuter transport journey.
Buses and coaches are also a common component of the wider package holiday industry, providing private airport transfers (in addition to general airport buses) and organised tours and day trips for holidaymakers on the package.
Public long-distance coach networks are also often used as a low-cost method of travel by students or young people travelling the world. Some companies such as Topdeck Travel were set up to specifically use buses to drive the hippie trail or travel to places such as North Africa.
In many tourist or travel destinations, a bus is part of the tourist attraction, such as the North American tourist trolleys, London's Routemaster heritage routes, or the customised buses of Malta, Asia, and the Americas.
Student transport.
In some countries, particularly the USA and Canada, buses used to transport school children have evolved into a specific design with specified mandatory features. American states have also adopted laws regarding motorist conduct around school buses, including serious fines and the possibility of prison time for passing a stopped school bus in the process of offloading children passengers. These school buses feature things such as the school bus yellow livery and crossing guards. Other countries may mandate the use of seat belts. As a minimum, many countries require a bus carrying students to display a sign, and may also adopt yellow liveries. Student transport often uses older buses cascaded from service use, retrofitted with more seats and/or seatbelts. Student transport may be operated by local authorities or private contractors. Schools may also own and operate their own buses for other transport needs, such as class field trips, or transport to associated sports, music, or other school events.
Private charter.
Due to the costs involved in owning, operating, and driving buses and coaches, many bus and coach uses a private hire of vehicles from charter bus companies, either for a day or two, or a longer contract basis, where the charter company provides the vehicles and qualified drivers.
Charter bus operators may be completely independent businesses, or charter hire may be a subsidiary business of a public transport operator that might maintain a separate fleet or use surplus buses, coaches, and dual-purpose coach-seated buses. Many private taxicab companies also operate larger minibus vehicles to cater for group fares. Companies, private groups, and social clubs may hire buses or coaches as a cost-effective method of transporting a group to an event or site, such as a group meeting, racing event, or organised recreational activity such as a summer camp. Entertainment or event companies may also hire temporary shuttles buses for transport at events such as festivals or conferences. Party buses are used by companies in a similar manner to limousine hire, for luxury private transport to social events or as a touring experience. Sleeper buses are used by bands or other organisations that tour between entertainment venues and require mobile rest and recreation facilities. Some couples hire preserved buses for their wedding transport instead of the traditional car. Buses are often hired for parades or processions. Victory parades are often held for triumphant sports teams, who often tour their home town or city in an open-top bus. Sports teams may also contract out their transport to a team bus, for travel to away games, to a competition or to a final event. These buses are often specially decorated in a livery matching the team colours. Private companies often contract out private shuttle bus services, for transport of their customers or patrons, such as hotels, amusement parks, university campuses, or private airport transfer services. This shuttle usage can be as transport between locations, or to and from parking lots. High specification luxury coaches are often chartered by companies for executive or VIP transport. Charter buses may also be used in tourism and for promotion (See Tourism and Promotion sections)
Private ownership.
Many organisations, including the police, not for profit, social or charitable groups with a regular need for group transport may find it practical or cost-effective to own and operate a bus for their own needs. These are often minibuses for practical, tax and driver licensing reasons, although they can also be full size buses. Cadet or scout groups or other youth organizations may also own buses. Specific charities may exist to fund and operate bus transport, usually using specially modified mobility buses or otherwise accessible buses (See Accessibility section). Some use their contributions to buy vehicles, and provide volunteer drivers.
Airport operators make use of special airside airport buses for crew and passenger transport in the secure airside parts of an airport. Some public authorities, police forces and military forces make use of armoured buses where there is a special need to provide increased passenger protection. The United States Secret Service acquired two in 2010 for transporting dignitaries needing special protection. Police departments make use of police buses for a variety of reasons, such as prisoner transport, officer transport, temporary detention facilities, and as command and control vehicles. Some fire departments also use a converted bus as a command post, while those in cold climates might retain a bus as a heated shelter at fire scenes. Many are drawn from retired school or service buses.
Promotion.
Buses are often used for advertising, political campaigning, public information campaigns, public relations or promotional purposes. These may take the form of temporary charter hire of service buses, or the temporary or permanent conversion and operation of buses, usually of second-hand buses. Extreme examples include converting the bus with displays and decorations or awnings and fittings. Interiors may be fitted out for exhibition or information purposes with special equipment and/or audio visual devices.
Bus advertising takes many forms, often as interior and exterior adverts and all-over advertising liveries. The practice often extends into the exclusive private hire and use of a bus to promote a brand or product, appearing at large public events, or touring busy streets. The bus is sometimes staffed by promotions personnel, giving out free gifts. Campaign buses are often specially decorated for a political campaign or other social awareness information campaign, designed to bring a specific message to different areas, and/or used to transport campaign personnel to local areas/meetings. Exhibition buses are often sent to public events such as fairs and festivals for purposes such as recruitment campaigns, for example by private companies or the armed forces. Complex urban planning proposals may be organised into a mobile exhibition bus for the purposes of public consultation.
Around the world.
Historically, the types and features of buses have developed according to local needs. Buses were fitted with technology appropriate to the local climate or passenger needs, such as air conditioning in Asia, or cycle mounts on North American buses. The bus types in use around the world where there was little mass production were often sourced second hand from other countries, such as the Malta bus, and buses in use in Africa. Other countries such as Cuba required novel solutions to import restrictions, with the creation of the "camellos" (camel bus), a specially manufactured trailer bus.
After the Second World War, manufacturers in Europe and the Far East, such as Mercedes-Benz buses and Mitsubishi Fuso expanded into other continents influencing the use of buses previously served by local types. Use of buses around the world has also been influenced by colonial associations or political alliances between countries. Several of the Commonwealth nations followed the British lead and sourced buses from British manufacturers, leading to a prevalence of double-decker buses. Several Eastern Bloc countries adopted trolleybus systems, and their manufacturers such as Trolza exported trolleybuses to other friendly states. In the 1930s Italy designed the world's only triple decker bus for the busy route between Rome and Tivoli that could carry eighty-eight passengers. It was unique not only in being a triple decker but having a separate smoking compartment on the third level.
The buses to be found in countries around the world often reflect the quality of the local road network, with high floor resilient truck based designs prevalent in several less developed countries where buses are subject to tough operating conditions. Population density also has a major impact, where dense urbanisation such as in Japan and the far east has led to the adoption of high capacity long multi-axle buses, often double-deckers, while South America and China are implementing large numbers of articulated buses for bus rapid transit schemes.
Bus expositions.
Euro Bus Expo is a trade show, which is held bi-ennially at the UK's National Exhibition Centre in Birmingham. As the official show of the Confederation of Passenger Transport, the UK's trade association for the bus, coach and light rail industry, the three-day event offers visitors from Europe and beyond the chance to see and experience, at first hand, the very latest vehicles and product and service innovations right across the industry. The next show will be held in November 2016.
Busworld Kortrijk in Kortrijk, Belgium, is the leading bus trade fair in Europe. It is held bi-ennially, last time October 2013 and next time October 2015.
Use of retired buses.
Most public or private buses and coaches, once they have reached the end of their service with one or more operators, are sent to the wrecking yard for breaking up for scrap and spare parts. Some buses, while not economical to keep running as service buses, are often converted in some way for use by the operator, or another user, for purposes other than revenue earning transport. Much like old cars and trucks, buses often pass through a dealership where they can be bought for a price or at auction.
Bus operators will often find it economical to convert retired buses to use as permanent training buses for driver training, rather than taking a regular service bus out of use. Some large operators also converted retired buses into tow bus vehicles, to act as tow trucks. With the outsourcing of maintenance staff and facilities, the increase in company health and safety regulations, and the increasing curb weights of buses, many operators now contract their towing needs to a professional vehicle recovery company.
Some retired buses have been converted to static or mobile cafés, often using historic buses as a tourist attraction. Food is also provided from a catering bus, in which a bus is converted into a mobile canteen and break room. These are commonly seen at external filming locations to feed the cast and crew, and at other large events to feed staff. Another use is as an emergency vehicle, such as high-capacity ambulance bus or mobile command center.
Some organisations adapt and operate playbuses or learning buses to provide a playground or learning environments to children who might not have access to proper play areas. An ex-London Routemaster bus has been converted to a mobile theatre and catwalk fashion show.
Some buses meet a destructive end by being entered in banger races or at demolition derbys. A larger number of old retired buses have also been converted into mobile holiday homes and campers.
Bus preservation.
Rather than being scrapped or converted for other uses, sometimes retired buses are saved for preservation. This can be done by individuals, volunteer preservation groups or charitable trusts, museums, or sometimes by the operators themselves as part of a heritage fleet. These buses often need to undergo a degree of vehicle restoration to restore them to their original condition, and will have their livery and other details such as internal notices and rollsigns restored to be authentic to a specific time in the bus's actual history. Some buses that undergo preservation are rescued from a state of great disrepair, but others enter preservation with very little wrong with them. As with other historic vehicles, many preserved buses either in a working or static state form part of the collections of transport museums. Working buses will often be exhibited at rallies and events, and they are also used as charter buses. While many preserved buses are quite old or even vintage, in some cases relatively new examples of a bus type can enter restoration. In-service examples are still in use by other operators. This often happens when a change in design or operating practice, such as the switch to one person operation or low floor technology, renders some buses redundant while still relatively new.
Modification as railway vehicles.
Suitable buses to modify as passenger train cars are buses that are primarily used as school buses, factory buses, activity buses, military buses and police buses. Buses built by German manufacturers such as Mercedes-Benz and Setra, cannot be modified as railway vehicles. Buses and motor vehicles of other kinds are not allowed to travel along railway lines in Pakistan.
Country of origin.
Buses are built in many different countries such as Japan, China, South Korea, Sweden and more other countries. Right-hand drive buses can only be modified as railway vehicles for use in countries where motor vehicle drivers drive on the right side of the road and Left-hand drive buses can only be modified as railway vehicles in countries where motor vehicle drivers drive on the left side of the road.
Link doors.
Link doors enable people to walk along in the train in case of an emergency.
Front link doors.
The front link door has to be mounted on the suitable position on the front end of the bus.
A. Buses with driver's seat elevated high must have their front windscreens minimized upwards, making it possible to install the front link door at the bottom of the windscreen.
the railway coupler on the front of the bus.
B. Buses with driver's seat elevated low must be fitted a front link door over the driver's cockpit and a passageway to the front link door next to the aisle.
I) Buses with no passenger observation deck above driver's compartment must have the front link door installed in the suitable position.
1. Buses with no horizontally divided front windscreens must have the front link door installed in the suitable position.
2. Buses with front windscreen divided horizontally into two similar sections must have the front link door installed in the upper section of the front windscreen.
II) Buses with passenger observation deck above driver's compartment must have the front link door installed in the upper section of the front windscreen, either in the left, centered or in the right for both left-hand drive buses and right-hand drive buses.
C. Low-height single-deck commuter or transit buses must have the front link door installed over the driver's cockpit.
Rear link doors.
The rear link door must be installed in the suitable position in the rear end of the bus.
A. Buses with driver's seat elevated high must have their rear windows minimized upwards if the rear window of each bus is protruding downwards, to make easier to install the rear link door on the rear end of the bus.
I) Rear-engined buses must have the rear link door installed in the suitable position on the rear of the bus.
1. On each rear-engined bus, if the engine cooling system's radiator is mounted in the left or in the right, the rear link door must be installed at the bottom of the rear window.

</doc>
<doc id="4147" url="https://en.wikipedia.org/wiki?curid=4147" title="Bali">
Bali

Bali is an island and province of Indonesia. The province includes the island of Bali and a few smaller neighbouring islands, notably Nusa Penida, Nusa Lembongan, and Nusa Ceningan. It is located at the westernmost end of the Lesser Sunda Islands, between Java to the west and Lombok to the east. Its capital of Denpasar is located at the southern part of the island.
With a population of 3,890,757 in the 2010 census, and 4,225,000 as of January 2014, the island is home to most of Indonesia's Hindu minority. According to the 2010 Census, 83.5% of Bali's population adhered to Balinese Hinduism, followed by 13.4% Muslim, Christianity at 2.5%, and Buddhism 0.5%.
Bali is a popular tourist destination, which has seen a significant rise in tourists since the 1980s. It is renowned for its highly developed arts, including traditional and modern dance, sculpture, painting, leather, metalworking, and music. The Indonesian International Film Festival is held every year in Bali.
Bali is part of the Coral Triangle, the area with the highest biodiversity of marine species. In this area alone over 500 reef building coral species can be found. For comparison, this is about 7 times as many as in the entire Caribbean. Most recently, Bali was the host of the 2011 ASEAN Summit, 2013 APEC and Miss World 2013.
History.
Ancient.
Bali was inhabited around 2000 BC by Austronesian people who migrated originally from Southeast Asia and Oceania through Maritime Southeast Asia. Culturally and linguistically, the Balinese are closely related to the people of the Indonesian archipelago, Malaysia, the Philippines, and Oceania. Stone tools dating from this time have been found near the village of Cekik in the island's west.
In ancient Bali, nine Hindu sects existed, namely Pasupata, Bhairawa, Siwa Shidanta, Waisnawa, Bodha, Brahma, Resi, Sora and Ganapatya. Each sect revered a specific deity as its personal Godhead.
Inscriptions from 896 and 911 don't mention a king, until 914, when Sri Kesarivarma is mentioned. They also reveal an independent Bali, with a distinct dialect, where Buddhism and Sivaism were practiced simultaneously. Mpu Sindok's great granddaughter, Mahendradatta (Gunapriyadharmapatni), married the Bali king Udayana Warmadewa (Dharmodayanavarmadeva) around 989, giving birth to Airlangga around 1001. This marriage also brought more Hinduism and Javanese culture to Bali. Princess Sakalendukirana appeared in 1098. Suradhipa reigned from 1115 to 1119, and Jayasakti from 1146 until 1150. Jayapangus appears on inscriptions between 1178 and 1181, while Adikuntiketana and his son Paramesvara in 1204.
Balinese culture was strongly influenced by Indian, Chinese, and particularly Hindu culture, beginning around the 1st century AD. The name "Bali dwipa" ("Bali island") has been discovered from various inscriptions, including the Blanjong pillar inscription written by Sri Kesari Warmadewa in 914 AD and mentioning "Walidwipa". It was during this time that the people developed their complex irrigation system "subak" to grow rice in wet-field cultivation. Some religious and cultural traditions still practised today can be traced to this period.
The Hindu Majapahit Empire (1293–1520 AD) on eastern Java founded a Balinese colony in 1343. The uncle of Hayam Wuruk is mentioned in the charters of 1384-86. A mass Javanese emigration occurred in the next century.
Portuguese contacts.
The first known European contact with Bali is thought to have been made in 1512, when a Portuguese expedition led by Antonio Abreu and Francisco Serrão sighted its northern shores. It was the first expedition of a series of bi-annual fleets to the Moluccas, that throughout the 16th century usually traveled along the coasts of the Sunda Islands. Bali was also mapped in 1512, in the chart of Francisco Rodrigues, aboard the expedition. In 1585, a ship foundered off the Bukit Peninsula and left a few Portuguese in the service of Dewa Agung.
Dutch East India.
In 1597 the Dutch explorer Cornelis de Houtman arrived at Bali, and the Dutch East India Company was established in 1602. The Dutch government expanded its control across the Indonesian archipelago during the second half of the 19th century (see Dutch East Indies). Dutch political and economic control over Bali began in the 1840s on the island's north coast, when the Dutch pitted various competing Balinese realms against each other. In the late 1890s, struggles between Balinese kingdoms in the island's south were exploited by the Dutch to increase their control.
In June 1860 the famous Welsh naturalist, Alfred Russel Wallace, travelled to Bali from Singapore, landing at Buleleng on the northcoast of the island. Wallace's trip to Bali was instrumental in helping him devise his Wallace Line theory. The Wallace Line is a faunal boundary that runs through the strait between Bali and Lombok. It has been found to be a boundary between species of Asiatic origin west of the line and a mixture of Australian and Asian species to the east. In his travel memoir "The Malay Archipelago," Wallace wrote of his experience in Bali:
I was both astonished and delighted; for as my visit to Java was some years later, I had never beheld so beautiful and well-cultivated a district out of Europe. A slightly undulating plain extends from the seacoast about inland, where it is bounded by a fine range of wooded and cultivated hills. Houses and villages, marked out by dense clumps of coconut palms, tamarind and other fruit trees, are dotted about in every direction; while between them extend luxurious rice-grounds, watered by an elaborate system of irrigation that would be the pride of the best cultivated parts of Europe. 
The Dutch mounted large naval and ground assaults at the Sanur region in 1906 and were met by the thousands of members of the royal family and their followers who fought against the superior Dutch force in a suicidal "puputan" defensive assault rather than face the humiliation of surrender. Despite Dutch demands for surrender, an estimated 200 Balinese marched to their death against the invaders. In the Dutch intervention in Bali, a similar massacre occurred in the face of a Dutch assault in Klungkung. Afterward the Dutch governors exercised administrative control over the island, but local control over religion and culture generally remained intact. Dutch rule over Bali came later and was never as well established as in other parts of Indonesia such as Java and Maluku.
In the 1930s, anthropologists Margaret Mead and Gregory Bateson, artists Miguel Covarrubias and Walter Spies, and musicologist Colin McPhee all spent time here. Their accounts of the island and its peoples created a western image of Bali as "an enchanted land of aesthetes at peace with themselves and nature." Western tourists began to visit the island.
Imperial Japan occupied Bali during World War II. It was not originally a target in their Netherlands East Indies Campaign, but as the airfields on Borneo were inoperative due to heavy rains, the Imperial Japanese Army decided to occupy Bali, which did not suffer from comparable weather. The island had no regular Royal Netherlands East Indies Army (KNIL) troops. There was only a Native Auxiliary Corps "Prajoda" (Korps Prajoda) consisting of about 600 native soldiers and several Dutch KNIL officers under command of KNIL Lieutenant Colonel W.P. Roodenburg. On 19 February 1942 the Japanese forces landed near the town of Senoer . The island was quickly captured.
During the Japanese occupation, a Balinese military officer, Gusti Ngurah Rai, formed a Balinese 'freedom army'. The harshness of war requisitions made Japanese rule more resented than Dutch rule. Following Japan's Pacific surrender in August 1945, the Dutch returned to Indonesia, including Bali, to reinstate their pre-war colonial administration. This was resisted by the Balinese rebels, who now used recovered Japanese weapons. On 20 November 1946, the Battle of Marga was fought in Tabanan in central Bali. Colonel I Gusti Ngurah Rai, by then 29 years old, finally rallied his forces in east Bali at Marga Rana, where they made a suicide attack on the heavily armed Dutch. The Balinese battalion was entirely wiped out, breaking the last thread of Balinese military resistance.
Independence from the Dutch.
In 1946, the Dutch constituted Bali as one of the 13 administrative districts of the newly proclaimed State of East Indonesia, a rival state to the Republic of Indonesia, which was proclaimed and headed by Sukarno and Hatta. Bali was included in the "Republic of the United States of Indonesia" when the Netherlands recognised Indonesian independence on 29 December 1949.
Contemporary.
The 1963 eruption of Mount Agung killed thousands, created economic havoc and forced many displaced Balinese to be "transmigrated" to other parts of Indonesia. Mirroring the widening of social divisions across Indonesia in the 1950s and early 1960s, Bali saw conflict between supporters of the traditional caste system, and those rejecting this system. Politically, the opposition was represented by supporters of the Indonesian Communist Party (PKI) and the Indonesian Nationalist Party (PNI), with tensions and ill-feeling further increased by the PKI's land reform programs. An attempted coup in Jakarta was put down by forces led by General Suharto.
The army became the dominant power as it instigated a violent anti-communist purge, in which the army blamed the PKI for the coup. Most estimates suggest that at least 500,000 people were killed across Indonesia, with an estimated 80,000 killed in Bali, equivalent to 5% of the island's population. With no Islamic forces involved as in Java and Sumatra, upper-caste PNI landlords led the extermination of PKI members.
As a result of the 1965/66 upheavals, Suharto was able to manoeuvre Sukarno out of the presidency. His "New Order" government reestablished relations with western countries. The pre-War Bali as "paradise" was revived in a modern form. The resulting large growth in tourism has led to a dramatic increase in Balinese standards of living and significant foreign exchange earned for the country. A bombing in 2002 by militant Islamists in the tourist area of Kuta killed 202 people, mostly foreigners. This attack, and another in 2005, severely reduced tourism, producing much economic hardship to the island.
Geography.
The island of Bali lies 3.2 km (2 mi) east of Java, and is approximately 8 degrees south of the equator. Bali and Java are separated by the Bali Strait. East to west, the island is approximately 153 km (95 mi) wide and spans approximately 112 km (69 mi) north to south; administratively it covers 5,780 km2, or 5,577 km2 without Nusa Penida District, its population density is roughly 750 people/km2.
Bali's central mountains include several peaks over in elevation. The highest is Mount Agung (), known as the "mother mountain" which is an active volcano rated as one of the world's most likely sites for a massive eruption within the next 100 years. Mountains range from centre to the eastern side, with Mount Agung the easternmost peak. Bali's volcanic nature has contributed to its exceptional fertility and its tall mountain ranges provide the high rainfall that supports the highly productive agriculture sector. South of the mountains is a broad, steadily descending area where most of Bali's large rice crop is grown. The northern side of the mountains slopes more steeply to the sea and is the main coffee producing area of the island, along with rice, vegetables and cattle. The longest river, Ayung River, flows approximately 75 km.
The island is surrounded by coral reefs. Beaches in the south tend to have white sand while those in the north and west have black sand. Bali has no major waterways, although the Ho River is navigable by small "sampan" boats. Black sand beaches between Pasut and Klatingdukuh are being developed for tourism, but apart from the seaside temple of Tanah Lot, they are not yet used for significant tourism.
The largest city is the provincial capital, Denpasar, near the southern coast. Its population is around 491,500 (2002). Bali's second-largest city is the old colonial capital, Singaraja, which is located on the north coast and is home to around 100,000 people. Other important cities include the beach resort, Kuta, which is practically part of Denpasar's urban area, and Ubud, situated at the north of Denpasar, is the island's cultural centre.
Three small islands lie to the immediate south east and all are administratively part of the Klungkung regency of Bali: Nusa Penida, Nusa Lembongan and Nusa Ceningan. These islands are separated from Bali by the Badung Strait.
To the east, the Lombok Strait separates Bali from Lombok and marks the biogeographical division between the fauna of the Indomalayan ecozone and the distinctly different fauna of Australasia. The transition is known as the Wallace Line, named after Alfred Russel Wallace, who first proposed a transition zone between these two major biomes. When sea levels dropped during the Pleistocene ice age, Bali was connected to Java and Sumatra and to the mainland of Asia and shared the Asian fauna, but the deep water of the Lombok Strait continued to keep Lombok Island and the Lesser Sunda archipelago isolated.
Climate.
Being just 8 degrees south of the equator, Bali has a fairly even climate year round.
Day time temperatures at low elevations vary between 20-33⁰ C (68-91⁰ F), although it can be much cooler than that in the mountains. The west monsoon is in place from approximately October to April, and this can bring significant rain, particularly from December to March. Outside of the monsoon period, humidity is relatively low and any rain is unlikely in lowland areas.
Ecology.
Bali lies just to the west of the Wallace Line, and thus has a fauna that is Asian in character, with very little Australasian influence, and has more in common with Java than with Lombok. An exception is the yellow-crested cockatoo, a member of a primarily Australasian family. There are around 280 species of birds, including the critically endangered Bali myna, which is endemic. Others Include barn swallow, black-naped oriole, black racket-tailed treepie, crested serpent-eagle, crested treeswift, dollarbird, Java sparrow, lesser adjutant, long-tailed shrike, milky stork, Pacific swallow, red-rumped swallow, sacred kingfisher, sea eagle, woodswallow, savanna nightjar, stork-billed kingfisher, yellow-vented bulbul and great egret.
Until the early 20th century, Bali was home to several large mammals: the wild banteng, leopard and the endemic Bali tiger. The banteng still occurs in its domestic form, whereas leopards are found only in neighbouring Java, and the Bali tiger is extinct. The last definite record of a tiger on Bali dates from 1937, when one was shot, though the subspecies may have survived until the 1940s or 1950s. The relatively small size of the island, conflict with humans, poaching and habitat reduction drove the Bali tiger to extinction. This was the smallest and rarest of all tiger subspecies and was never caught on film or displayed in zoos, whereas few skins or bones remain in museums around the world. Today, the largest mammals are the Javan rusa deer and the wild boar. A second, smaller species of deer, the Indian muntjac, also occurs. Saltwater crocodiles were once present on the island, but became locally extinct sometime during the last century.
Squirrels are quite commonly encountered, less often is the Asian palm civet, which is also kept in coffee farms to produce Kopi Luwak. Bats are well represented, perhaps the most famous place to encounter them remaining the Goa Lawah (Temple of the Bats) where they are worshipped by the locals and also constitute a tourist attraction. They also occur in other cave temples, for instance at Gangga Beach. Two species of monkey occur. The crab-eating macaque, known locally as "kera", is quite common around human settlements and temples, where it becomes accustomed to being fed by humans, particularly in any of the three "monkey forest" temples, such as the popular one in the Ubud area. They are also quite often kept as pets by locals. The second monkey, endemic to Java and some surrounding islands such as Bali, is far rarer and more elusive is the Javan langur, locally known as "lutung". They occur in few places apart from the Bali Barat National Park. They are born an orange colour, though by their first year they would have already changed to a more blackish colouration. In Java however, there is more of a tendency for this species to retain its juvenile orange colour into adulthood, and so you can see a mixture of black and orange monkeys together as a family. Other rarer mammals include the leopard cat, Sunda pangolin and black giant squirrel.
Snakes include the king cobra and reticulated python. The water monitor can grow to at least in length and and can move quickly.
The rich coral reefs around the coast, particularly around popular diving spots such as Tulamben, Amed, Menjangan or neighbouring Nusa Penida, host a wide range of marine life, for instance hawksbill turtle, giant sunfish, giant manta ray, giant moray eel, bumphead parrotfish, hammerhead shark, reef shark, barracuda, and sea snakes. Dolphins are commonly encountered on the north coast near Singaraja and Lovina.
A team of scientists conducted a survey from 29 April 2011 to 11 May 2011 at 33 sea sites around Bali. They discovered 952 species of reef fish of which 8 were new discoveries at Pemuteran, Gilimanuk, Nusa Dua, Tulamben and Candidasa, and 393 coral species, including two new ones at Padangbai and between Padangbai and Amed.
The average coverage level of healthy coral was 36% (better than in Raja Ampat and Halmahera by 29% or in Fakfak and Kaimana by 25%) with the highest coverage found in Gili Selang and Gili Mimpang in Candidasa, Karangasem regency.
Many plants have been introduced by humans within the last centuries, particularly since the 20th century, making it sometimes hard to distinguish what plants are really native. Among the larger trees the most common are: banyan trees, jackfruit, coconuts, bamboo species, acacia trees and also endless rows of coconuts and banana species. Numerous flowers can be seen: hibiscus, frangipani, bougainvillea, poinsettia, oleander, jasmine, water lily, lotus, roses, begonias, orchids and hydrangeas exist. On higher grounds that receive more moisture, for instance around Kintamani, certain species of fern trees, mushrooms and even pine trees thrive well. Rice comes in many varieties. Other plants with agricultural value include: salak, mangosteen, corn, kintamani orange, coffee and water spinach.
Environment.
Some of the worst erosion has occurred in Lebih Beach, where up to of land is lost every year. Decades ago, this beach was used for holy pilgrimages with more than 10,000 people, but they have now moved to Masceti Beach.
From ranked third in previous review, in 2010 Bali got score 99.65 of Indonesia's environmental quality index and the highest of all the 33 provinces. The score measured 3 water quality parameters: the level of total suspended solids (TSS), dissolved oxygen (DO) and chemical oxygen demand (COD).
Because of over-exploitation by the tourist industry which covers a massive land area, 200 out of 400 rivers on the island have dried up and based on research, the southern part of Bali would face a water shortage up to 2,500 litres of clean water per second by 2015.
To ease the shortage, the central government plans to build a water catchment and processing facility at Petanu River in Gianyar. The 300 litres capacity of water per second will be channelled to Denpasar, Badung and Gianyar in 2013.
Administrative divisions.
The province is divided into eight regencies ("kabupaten") and one city ("kota"). These are:
Economy.
Three decades ago, the Balinese economy was largely agriculture-based in terms of both output and employment. Tourism is now the largest single industry in terms of income, and as a result, Bali is one of Indonesia's wealthiest regions. In 2003, around 80% of Bali's economy was tourism related. By end of June 2011, non-performing loan of all banks in Bali were 2.23%, lower than the average of Indonesian banking industry non-performing loan (about 5%). The economy, however, suffered significantly as a result of the terrorist bombings 2002 and 2005. The tourism industry has since recovered from these events.
Agriculture.
Although tourism produces the GDP's largest output, agriculture is still the island's biggest employer; most notably rice cultivation. Crops grown in smaller amounts include fruit, vegetables, "Coffea arabica" and other cash and subsistence crops. Fishing also provides a significant number of jobs. Bali is also famous for its artisans who produce a vast array of handicrafts, including batik and ikat cloth and clothing, wooden carvings, stone carvings, painted art and silverware. Notably, individual villages typically adopt a single product, such as wind chimes or wooden furniture.
The Arabica coffee production region is the highland region of Kintamani near Mount Batur. Generally, Balinese coffee is processed using the wet method. This results in a sweet, soft coffee with good consistency. Typical flavours include lemon and other citrus notes. Many coffee farmers in Kintamani are members of a traditional farming system called Subak Abian, which is based on the Hindu philosophy of "Tri Hita Karana". According to this philosophy, the three causes of happiness are good relations with God, other people and the environment. The Subak Abian system is ideally suited to the production of fair trade and organic coffee production. Arabica coffee from Kintamani is the first product in Indonesia to request a Geographical Indication.
Tourism.
The tourism industry is primarily focused in the south, while significant in the other parts of the island as well. The main tourist locations are the town of Kuta (with its beach), and its outer suburbs of Legian and Seminyak (which were once independent townships), the east coast town of Sanur (once the only tourist hub), in the center of the island Ubud, to the south of the Ngurah Rai International Airport, Jimbaran, and the newer development of Nusa Dua and Pecatu.
The American government lifted its travel warnings in 2008. The Australian government issued an advice on Friday, 4 May 2012. The overall level of the advice was lowered to 'Exercise a high degree of caution'. The Swedish government issued a new warning on Sunday, 10 June 2012 because of one more tourist who was killed by methanol poisoning. Australia last issued an advice on Monday, 5 January 2015 due to new terrorist threats.
In the last half of 2008, Indonesia's currency had dropped approximately 30% against the US dollar, providing many overseas visitors value for their currencies. Visitor arrivals for 2009 were forecast to drop 8% (which would be higher than 2007 levels), due to the worldwide economic crisis which has also affected the global tourist industry, but not due to any travel warnings.
Bali's tourism economy survived the terrorist bombings of 2002 and 2005, and the tourism industry has in fact slowly recovered and surpassed its pre-terrorist bombing levels; the longterm trend has been a steady increase of visitor arrivals. In 2010, Bali received 2.57 million foreign tourists, which surpassed the target of 2.0–2.3 million tourists. The average occupancy of starred hotels achieved 65%, so the island is still able to accommodate tourists for some years without any addition of new rooms/hotels, although at the peak season some of them are fully booked.
Bali received the Best Island award from Travel and Leisure in 2010. The island of Bali won because of its attractive surroundings (both mountain and coastal areas), diverse tourist attractions, excellent international and local restaurants, and the friendliness of the local people. According to BBC Travel released in 2011, Bali is one of the World's Best Islands, ranking second after Santorini, Greece.
In August 2010, the film "Eat Pray Love" was released in theatres. The movie was based on Elizabeth Gilbert's best-selling memoir "Eat, Pray, Love". It took place at Ubud and Padang-Padang Beach at Bali. The 2006 book, which spent 57 weeks at the No. 1 spot on the "New York Times" paperback nonfiction best-seller list, had already fuelled a boom in "Eat, Pray, Love"-related tourism in Ubud, the hill town and cultural and tourist center that was the focus of Gilbert's quest for balance through traditional spirituality and healing that leads to love.
In January 2016, after music icon David Bowie died, it was revealed that in his will, Bowie asked for his ashes to be scattered in Bali, conforming to Buddhist rituals. He had visited and performed in a number of Southeast Asian cities early in his career, including Bangkok and Singapore.
Since 2011, China has displaced Japan as the second-largest supplier of tourists to Bali, while Australia still tops the list. Chinese tourists increased by 17% from last year due to the impact of ACFTA and new direct flights to Bali.
In January 2012, Chinese tourists year on year (yoy) increased by 222.18% compared to January 2011, while Japanese tourists declined by 23.54% yoy.
Bali reported that it has 2.88 million foreign tourists and 5 million domestic tourists in 2012, marginally surpassing the expectations of 2.8 million foreign tourists. Forecasts for 2013 are at 3.1 million.
Based on Bank Indonesia survey in May 2013, 34.39 percent of tourists are upper-middle class with spending between $1,286 to $5,592 and dominated by Australia, France, China, Germany and the US with some China tourists move from low spending before to higher spending currently. While 30.26 percent are middle class with spending between $662 to $1,285.
Sex tourism.
In the twentieth century the incidence of tourism specifically for sex was regularly observed in the era of mass tourism in Indonesia In Bali, prostitution is conducted by both men and women. Bali in particular is notorious for its 'Kuta Cowboys', local gigolos targeting foreign female tourists.
Tens of thousands of single women throng the beaches of Bali in Indonesia every year. For decades, young Balinese men have taken advantage of the louche and laid-back atmosphere to find love and lucre from female tourists—Japanese, European and Australian for the most part—who by all accounts seem perfectly happy with the arrangement.
By 2013, Indonesia was reportedly the number one destination for Australian child sex tourists, mostly starting in Bali but also travelling to other parts of the country. The problem in Bali was highlighted by Luh Ketut Suryani, head of Psychiatry at Udayana University, as early as 2003. Surayani warned that a low level of awareness of paedophilia in Bali had made it the target of international paedophile organisations. On 19 February 2013, government officials announced measures to combat paedophilia in Bali.
Transportation.
The Ngurah Rai International Airport is located near Jimbaran, on the isthmus at the southernmost part of the island. Lt.Col. Wisnu Airfield is found in north-west Bali.
A coastal road circles the island, and three major two-lane arteries cross the central mountains at passes reaching to 1,750m in height (at Penelokan). The Ngurah Rai Bypass is a four-lane expressway that partly encircles Denpasar. Bali has no railway lines.
In December 2010 the Government of Indonesia invited investors to build a new Tanah Ampo Cruise Terminal at Karangasem, Bali with a projected worth of $30 million. On 17 July 2011 the first cruise ship (Sun Princess) anchored about away from the wharf of Tanah Ampo harbour. The current pier is only but will eventually be extended to to accommodate international cruise ships. The harbour here is safer than the existing facility at Benoa and has a scenic backdrop of east Bali mountains and green rice fields. The tender for improvement was subject to delays, and as of July 2013 the situation remained unclear with cruise line operators complaining and even refusing to use the existing facility at Tanah Ampo.
A Memorandum of Understanding has been signed by two ministers, Bali's Governor and Indonesian Train Company to build of railway along the coast around the island. As of July 2015, no details of this proposed railways have been released.
On 16 March 2011 (Tanjung) Benoa port received the "Best Port Welcome 2010" award from London's "Dream World Cruise Destination" magazine. Government plans to expand the role of Benoa port as export-import port to boost Bali's trade and industry sector. The Tourism and Creative Economy Ministry has confirmed that 306 cruise liners are heading for Indonesia in 2013 – an increase of 43 percent compared to the previous year.
In May 2011, an integrated Areal Traffic Control System (ATCS) was implemented to reduce traffic jams at four crossing points: Ngurah Rai statue, Dewa Ruci Kuta crossing, Jimbaran crossing and Sanur crossing. ATCS is an integrated system connecting all traffic lights, CCTVs and other traffic signals with a monitoring office at the police headquarters. It has successfully been implemented in other ASEAN countries and will be implemented at other crossings in Bali.
On 21 December 2011 construction started on the Nusa Dua-Benoa-Ngurah Rai International Airport toll road which will also provide a special lane for motorcycles. This has been done by seven state-owned enterprises led by PT Jasa Marga with 60% of shares. PT Jasa Marga Bali Tol will construct the toll road (totally with access road). The construction is estimated to cost Rp.2.49 trillion ($273.9 million). The project goes through of mangrove forest and through of beach, both within area. The elevated toll road is built over the mangrove forest on 18,000 concrete pillars which occupied 2 hectares of mangroves forest. It compensated by new planting of 300,000 mangrove trees along the road. On 21 December 2011 the Dewa Ruci underpass has also started on the busy Dewa Ruci junction near Bali Kuta Galeria with an estimated cost of Rp136 billion ($14.9 million) from the state budget. On 23 September 2013, the Bali Mandara Toll Road is opened and the Dewa Ruci Junction (Simpang Siur) underpass is opened before. Both are ease the heavy traffic congestion.
To solve chronic traffic problems, the province will also build a toll road connecting Serangan with Tohpati, a toll road connecting Kuta, Denpasar and Tohpati and a flyover connecting Kuta and Ngurah Rai Airport.
Demographics.
The population of Bali was 3,890,757 as of the 2010 Census; the latest estimate (for January 2014) is 4,225,384. There are an estimated 30,000 expatriates living in Bali.
Ethnic origins.
A DNA study in 2005 by Karafet et al. found that 12% of Balinese Y-chromosomes are of likely Indian origin, while 84% are of likely Austronesian origin, and 2% of likely Melanesian origin. The study does not correlate the DNA samples to the Balinese caste system.
Caste system.
Bali has a caste system based on the Indian Hindu model, with four castes:
Religion.
Unlike most of Muslim-majority Indonesia, about 83.5% of Bali's population adheres to Balinese Hinduism, formed as a combination of existing local beliefs and Hindu influences from mainland Southeast Asia and South Asia. Minority religions include Islam (13.3%), Christianity (1.7%), and Buddhism (0.5%). These figures do not include immigrants from other parts of Indonesia.
Balinese Hinduism is an amalgam in which gods and demigods are worshipped together with Buddhist heroes, the spirits of ancestors, indigenous agricultural deities and sacred places. Religion as it is practised in Bali is a composite belief system that embraces not only theology, philosophy, and mythology, but ancestor worship, animism and magic. It pervades nearly every aspect of traditional life. Caste is observed, though less strictly than in India. With an estimated 20,000 puras (temples) and shrines, Bali is known as the "Island of a Thousand Puras", or "Island of the Gods".
Balinese Hinduism has roots in Indian Hinduism and Buddhism, and adopted the animistic traditions of the indigenous people. This influence strengthened the belief that the gods and goddesses are present in all things. Every element of nature, therefore, possesses its own power, which reflects the power of the gods. A rock, tree, dagger, or woven cloth is a potential home for spirits whose energy can be directed for good or evil. Balinese Hinduism is deeply interwoven with art and ritual. Ritualizing states of self-control are a notable feature of religious expression among the people, who for this reason have become famous for their graceful and decorous behaviour.
Apart from the majority of Balinese Hindus, there also exist Chinese immigrants whose traditions have melded with that of the locals. As a result, these Sino-Balinese not only embrace their original religion, which is a mixture of Buddhism, Christianity, Taoism and Confucianism, but also find a way to harmonise it with the local traditions. Hence, it is not uncommon to find local Sino-Balinese during the local temple's "odalan". Moreover, Balinese Hindu priests are invited to perform rites alongside a Chinese priest in the event of the death of a Sino-Balinese. Nevertheless, the Sino-Balinese claim to embrace Buddhism for administrative purposes, such as their Identity Cards.
Language.
Balinese and Indonesian are the most widely spoken languages in Bali, and the vast majority of Balinese people are bilingual or trilingual. The most common spoken language around the tourist areas is Indonesian, as many people in the tourist sector are not solely Balinese, but migrants from Java, Lombok, Sumatra, and other parts of Indonesia. There are several indigenous Balinese languages, but most Balinese can also use the most widely spoken option: modern common Balinese. The usage of different Balinese languages was traditionally determined by the Balinese caste system and by clan membership, but this tradition is diminishing. Kawi and Sanskrit are also commonly used by some Hindu priests in Bali, for Hinduism literature was mostly written in Sanskrit.
English and Chinese are the next most common languages (and the primary foreign languages) of many Balinese, owing to the requirements of the tourism industry, as well as the English-speaking community and huge Chinese-Indonesian population. Other foreign languages, such as Japanese, Korean, French, Russian or German are often used in multilingual signs for foreign tourists.
Culture.
Bali is renowned for its diverse and sophisticated art forms, such as painting, sculpture, woodcarving, handcrafts, and performing arts. Balinese cuisine is also distinctive. Balinese percussion orchestra music, known as "gamelan", is highly developed and varied. Balinese performing arts often portray stories from Hindu epics such as the Ramayana but with heavy Balinese influence. Famous Balinese dances include "pendet", "legong", "baris", "topeng", "barong", "gong keybar", and "kecak" (the monkey dance). Bali boasts one of the most diverse and innovative performing arts cultures in the world, with paid performances at thousands of temple festivals, private ceremonies, or public shows.
The Hindu New Year, "Nyepi", is celebrated in the spring by a day of silence. On this day everyone stays at home and tourists are encouraged to remain in their hotels. On the day before New Year, large and colourful sculptures of "ogoh-ogoh" monsters are paraded and finally burned in the evening to drive away evil spirits. Other festivals throughout the year are specified by the Balinese "pawukon" calendrical system.
Celebrations are held for many occasions such as a tooth-filing (coming-of-age ritual), cremation or "odalan" (temple festival). One of the most important concepts that Balinese ceremonies have in common is that of "désa kala patra", which refers to how ritual performances must be appropriate in both the specific and general social context. Many of the ceremonial art forms such as "wayang kulit" and "topeng" are highly improvisatory, providing flexibility for the performer to adapt the performance to the current situation. Many celebrations call for a loud, boisterous atmosphere with lots of activity and the resulting aesthetic, "ramé", is distinctively Balinese. Often two or more "gamelan" ensembles will be performing well within earshot, and sometimes compete with each other to be heard. Likewise, the audience members talk amongst themselves, get up and walk around, or even cheer on the performance, which adds to the many layers of activity and the liveliness typical of "ramé".
"Kaja" and "kelod" are the Balinese equivalents of North and South, which refer to ones orientation between the island's largest mountain Gunung Agung ("kaja"), and the sea ("kelod"). In addition to spatial orientation, "kaja" and "kelod" have the connotation of good and evil; gods and ancestors are believed to live on the mountain whereas demons live in the sea. Buildings such as temples and residential homes are spatially oriented by having the most sacred spaces closest to the mountain and the unclean places nearest to the sea.
Most temples have an inner courtyard and an outer courtyard which are arranged with the inner courtyard furthest "kaja". These spaces serve as performance venues since most Balinese rituals are accompanied by any combination of music, dance and drama. The performances that take place in the inner courtyard are classified as "wali", the most sacred rituals which are offerings exclusively for the gods, while the outer courtyard is where "bebali" ceremonies are held, which are intended for gods and people. Lastly, performances meant solely for the entertainment of humans take place outside the walls of the temple and are called "bali-balihan". This three-tiered system of classification was standardised in 1971 by a committee of Balinese officials and artists to better protect the sanctity of the oldest and most sacred Balinese rituals from being performed for a paying audience.
Tourism, Bali's chief industry, has provided the island with a foreign audience that is eager to pay for entertainment, thus creating new performance opportunities and more demand for performers. The impact of tourism is controversial since before it became integrated into the economy, the Balinese performing arts did not exist as a capitalist venture, and were not performed for entertainment outside of their respective ritual context. Since the 1930s sacred rituals such as the "barong" dance have been performed both in their original contexts, as well as exclusively for paying tourists. This has led to new versions of many of these performances which have developed according to the preferences of foreign audiences; some villages have a "barong" mask specifically for non-ritual performances as well as an older mask which is only used for sacred performances.
Balinese society continues to revolve around each family's ancestral village, to which the cycle of life and religion is closely tied. Coercive aspects of traditional society, such as customary law sanctions imposed by traditional authorities such as village councils (including "kasepekang", or shunning) have risen in importance as a consequence of the democratisation and decentralisation of Indonesia since 1998.
Sports.
Bali is a major world surfing destination with popular breaks dotted across the southern coastline and around the offshore island of Nusa Lembongan.
As part of the Coral Triangle, Bali, including Nusa Penida, offers a wide range of dive sites with varying types of reefs.
Bali was the host of 2008 Asian Beach Games. It was the second time Indonesia hosted an Asia-level multi-sport event, after Jakarta held the 1962 Asian Games.
Heritage sites.
In June 2012, Subak, the irrigation system for paddy fields in Bali was enlisted as a UNESCO world heritage site.
Beauty pageant.
Bali was the host of Miss World 2013. It was the first time Indonesia hosted an international beauty pageant.

</doc>
<doc id="4149" url="https://en.wikipedia.org/wiki?curid=4149" title="Bulgarian language">
Bulgarian language

Bulgarian , (български език, ) is an Indo-European language, a member of the Southern branch of the Slavic language family. It is the language of the Bulgarians.
Bulgarian, along with the closely related Macedonian language (collectively forming the East South Slavic languages), has several characteristics that set it apart from all other Slavic languages: changes include the elimination of case declension, the development of a suffixed definite article (see Balkan language area), and the lack of a verb infinitive, but it retains and has further developed the Proto-Slavic verb system. Various evidential verb forms exist to express unwitnessed, retold, and doubtful action.
With the accession of Bulgaria to the European Union on 1 January 2007, Bulgarian became one of the official languages of the European Union.
History.
The development of the Bulgarian language may be divided into several periods.
"Bulgarian" was the first "Slavic" language attested in writing. As Slavic linguistic unity lasted into late antiquity, in the oldest manuscripts this language was initially referred to as языкъ словяньскъ, "the Slavic language". In the Middle Bulgarian period this name was gradually replaced by the name , the "Bulgarian language". In some cases, the name was used not only with regard to the contemporary Middle Bulgarian language of the copyist but also to the period of Old Bulgarian. A most notable example of anachronism is the Service of St. Cyril from Skopje (Скопски миней), a 13th-century Middle Bulgarian manuscript from northern Macedonia according to which St. Cyril preached with "Bulgarian" books among the Moravian Slavs. The first mention of the language as the "Bulgarian language" instead of the "Slavonic language" comes in the work of the Greek clergy of the Bulgarian Archbishopric of Ohrid in the 11th century, for example in the Greek hagiography of Saint Clement of Ohrid by Theophylact of Ohrid (late 11th century).
During the Middle Bulgarian period, the language underwent dramatic changes, losing the Slavonic case system, but preserving the rich verb system (while the development was exactly the opposite in other Slavic languages) and developing a definite article. It was influenced by its non-Slavic neighbors in the Balkan language area (mostly grammatically) and later also by Turkish, which was the official language of the Ottoman Empire, in the form of the Ottoman Turkish language, mostly lexically. As a national revival occurred toward the end of the period of Ottoman rule (mostly during the 19th century), a modern Bulgarian literary language gradually emerged that drew heavily on Church Slavonic/Old Bulgarian (and to some extent on literary Russian, which had preserved many lexical items from Church Slavonic) and later reduced the number of Turkish and other Balkan loans. Today one difference between Bulgarian dialects in the country and literary spoken Bulgarian is the significant presence of Old Bulgarian words and even word forms in the latter. Russian loans are distinguished from Old Bulgarian ones on the basis of the presence of specifically Russian phonetic changes, as in оборот (turnover, rev), непонятен (incomprehensible), ядро (nucleus) and others. As usual in such cases, many other loans from French, English and the classical languages have subsequently entered the language as well.
Modern Bulgarian was based essentially on the Eastern dialects of the language, but its pronunciation is in many respects a compromise between East and West Bulgarian (see especially the phonetic sections below). Following the efforts of some figures of the National awakening of Bulgaria (the most notable among them being Neofit Rilski and Ivan Bogorov), there had been many attempts to codify a standard Bulgarian language; however, there was much argument surrounding the choice of norms. Between 1835–1878 more than 25 proposals were put forward and "linguistic chaos" ensued. Eventually the eastern dialects prevailed, and in 1899 the Ministry of Education officially codified a standard Bulgarian language based on the Drinov-Ivanchev orthography.
Dialects.
The language is mainly split into two broad dialect areas, based on the different reflexes of the Common Slavic yat vowel (Ѣ). This split, which occurred at some point during the Middle Ages, led to the development of Bulgaria's:
The literary language norm, which is generally based on the Eastern dialects, also has the Eastern alternating reflex of "yat". However, it has not incorporated the general Eastern umlaut of "all" synchronic or even historic "ya" sounds into "e" before front vowels – e.g. поляна ("polyana") vs. полени ("poleni") "meadow – meadows" or even жаба ("zhaba") vs. жеби ("zhebi") "frog – frogs", even though it co-occurs with the yat alternation in almost all Eastern dialects that have it (except a few dialects along the yat border, e.g. in the Pleven region).
More examples of the "yat" umlaut in the literary language are:
Until 1945, Bulgarian orthography did not reveal this alternation and used the original Old Slavic Cyrillic letter "yat" (Ѣ), which was commonly called двойно е ("dvoyno e") at the time, to express the historical "yat" vowel or at least root vowels displaying the "ya – e" alternation. The letter was used in each occurrence of such a root, regardless of the actual pronunciation of the vowel: thus, both "mlyako" and "mlekar" were spelled with (Ѣ). Among other things, this was seen as a way to "reconcile" the Western and the Eastern dialects and maintain language unity at a time when much of Bulgaria's Western dialect area was controlled by Serbia and Greece, but there were still hopes and occasional attempts to recover it. With the 1945 orthographic reform, this letter was abolished and the present spelling was introduced, reflecting the alternation in pronunciation.
This had implications for some grammatical constructions:
Sometimes, with the changes, words began to be spelled as other words with different meanings, e.g.:
In spite of the literary norm regarding the yat vowel, many people living in Western Bulgaria, including the capital Sofia, will fail to observe its rules. While the norm requires the realizations "vidyal" vs. "videli" (he has seen; they have seen), some natives of Western Bulgaria will preserve their local dialect pronunciation with "e" for all instances of "yat" (e.g. "videl", "videli"). Others, attempting to adhere to the norm, will actually use the "ya" sound even in cases where the standard language has "e" (e.g. "vidyal", "vidyali"). The latter hypercorrection is called свръхякане ("svrah-yakane" ≈"over-"ya"-ing").
Bulgarian is the only Slavic language whose literary standard does not naturally contain the iotated sound (or its palatalized variant , except in non-Slavic foreign-loaned words). The sound is common in all modern Slavic languages (e.g. Czech "medvěd" "bear", Polish "pięć" "five", Serbo-Croatian "jelen" "deer", Ukrainian "немає" "there is not...", Macedonian "пишување" "writing", etc.), as well as some Western Bulgarian dialectal forms – e.g. "ора̀н’е" (standard Bulgarian: "оране" , "ploughing"), however it is not represented in standard Bulgarian speech or writing. Even where occurs in other Slavic words, in Standard Bulgarian it is usually transcribed and pronounced as pure – e.g. Boris Yeltsin is "Eltsin" (Борис Елцин), Yekaterinburg is "Ekaterinburg" (Екатеринбург) and Sarajevo is "Saraevo" (Сараево), although Jelena Janković is "Yelena" – Йелена Янкович.
Relationship to Macedonian.
Until the period immediately following the Second World War, all Bulgarian and the majority of foreign linguists referred to the South Slavic dialect continuum spanning the area of modern Bulgaria, the Republic of Macedonia and parts of Northern Greece as a group of Bulgarian dialects. In contrast, Serbian sources tended to label them "south Serbian" dialects. Some local naming conventions included "bolgarski", "bugarski" and so forth. The codifiers of the standard Bulgarian language, however, did not wish to make any allowances for a pluricentric "Bulgaro-Macedonian" compromise. After 1944 the People's Republic of Bulgaria and the Socialist Federal Republic of Yugoslavia began a policy of making Macedonia into the connecting link for the establishment of new Balkan Federative Republic and stimulating here a development of distinct Slav Macedonian consciousness. After 1958, when the pressure from Moscow decreased, Sofia reverted to the view that the Macedonian language did not exist as a separate language. Nowadays, Bulgarian and Greek linguists as well as some linguists from other countries still consider Macedonian dialects as Bulgarian. Outside Bulgaria and Greece, Macedonian is generally considered an autonomous language within the South Slavic dialect continuum. Sociolinguists agree that the question whether Macedonian is a dialect of Bulgarian or a language is a political one and cannot be resolved on a purely linguistic basis, because dialect continua do not allow for either-or judgments.
Alphabet.
In 886 AD, the Bulgarian Empire introduced the Glagolitic alphabet which was devised by the Saints Cyril and Methodius in the 850s. The Glagolitic alphabet was gradually superseded in later centuries by the Cyrillic script, developed around the Preslav Literary School, Bulgaria in the beginning of the 10th century.
Several Cyrillic alphabets with 28 to 44 letters were used in the beginning and the middle of the 19th century during the efforts on the codification of Modern Bulgarian until an alphabet with 32 letters, proposed by Marin Drinov, gained prominence in the 1870s. The alphabet of Marin Drinov was used until the orthographic reform of 1945, when the letters Ѣ, ѣ (called "ят" 'yat' or "двойно е"/"е-двойно" 'double e'), and Ѫ, ѫ (called "Голям юс" 'big yus', "голяма носовка" 'big nasal sign', "ъ кръстато" 'crossed yer' or "широко ъ" 'long yer'), were removed from its alphabet, reducing the number of letters to 30.
With the accession of Bulgaria to the European Union on 1 January 2007, Cyrillic became the third official script of the European Union, following the Latin and Greek scripts.
Phonology.
Bulgarian possesses a phonology similar to that of the rest of the South Slavic languages, notably lacking Serbo-Croatian's phonemic vowel length and tones and alveo-palatal affricates. Macedonian on the other side exhibits a phonology very similar to that of Bulgarian, which has spurred controversial debates regarding its status as a separate language. An interesting geographic pattern of dialectal distribution shows a tendency of western dialects to approach Serbo-Croatian's "hard" sound in contrast to the eastern dialect's "soft" sound due to pre-palatalization and rising of (similar to Russian) and ikanye (a merger of the two front vowels and ).
Bulgarian is typically analyzed as having six vowels, but at least two more reduced vowels can be encountered in everyday speech.
Grammar.
The parts of speech in Bulgarian are divided in 10 different types, which are categorized in two broad classes: mutable and immutable. The difference is that mutable parts of speech vary grammatically, whereas the immutable ones do not change, regardless of their use. The five classes of mutables are: "nouns", "adjectives", "numerals", "pronouns" and "verbs". Syntactically, the first four of these form the group of the noun or the nominal group. The immutables are: "adverbs", "prepositions", "conjunctions", "particles" and "interjections". Verbs and adverbs form the group of the verb or the verbal group.
Nominal morphology.
Nouns and adjectives have the categories grammatical gender, number, case (only vocative) and definiteness in Bulgarian. Adjectives and adjectival pronouns agree with nouns in number and gender. Pronouns have gender and number and retain (as in nearly all Indo-European languages) a more significant part of the case system.
Nominal inflection.
Gender.
There are three grammatical genders in Bulgarian: "masculine", "feminine" and "neuter". The gender of the noun can largely be inferred from its ending: nouns ending in a consonant ("zero ending") are generally masculine (for example, град 'city', син 'son', мъж 'man'; those ending in –а/–я (-a/-ya) (жена 'woman', дъщеря 'daughter', улица 'street') are normally feminine; and nouns ending in –е, –о are almost always neuter (дете 'child', езеро 'lake'), as are those rare words (usually loanwords) that end in –и, –у, and –ю (цунами 'tsunami', табу 'taboo', меню 'menu'). Perhaps the most significant exception from the above are the relatively numerous nouns that end in a consonant and yet are feminine: these comprise, firstly, a large group of nouns with zero ending expressing quality, degree or an abstraction, including all nouns ending on –ост/–ест -{ost/est} (мъдрост 'wisdom', низост 'vileness', прелест 'loveliness', болест 'sickness', любов 'love'), and secondly, a much smaller group of irregular nouns with zero ending which define tangible objects or concepts (кръв 'blood', кост 'bone', вечер 'evening', нощ 'night'). There are also some commonly used words that end in a vowel and yet are masculine: баща 'father', дядо 'grandfather', чичо / вуйчо 'uncle', and others.
The plural forms of the nouns do not express their gender as clearly as the singular ones, but may also provide some clues to it: the ending –и (-i) is more likely to be used with a masculine or feminine noun (факти 'facts', болести 'sicknesses'), while one in –а/–я belongs more often to a neuter noun (езера 'lakes'). Also, the plural ending –ове occurs only in masculine nouns.
Number.
Two numbers are distinguished in Bulgarian – singular and plural. A variety of plural suffixes is used, and the choice between them is partly determined by their ending in singular and partly influenced by gender; in addition, irregular declension and alternative plural forms are common. Words ending in –а/–я (which are usually feminine) generally have the plural ending –и, upon dropping of the singular ending. Of nouns ending in a consonant, the feminine ones also use –и, whereas the masculine ones usually have –и for polysyllables and –ове for monosyllables (however, exceptions are especially common in this group). Nouns ending in –о/–е (most of which are neuter) mostly use the suffixes –а, –я (both of which require the dropping of the singular endings) and –та.
With cardinal numbers and related words such as няколко ('several'), masculine nouns use a special count form in –а/–я, which stems from the Proto-Slavonic dual: два/три стола ('two/three chairs') versus тези столове ('these chairs'); cf. feminine две/три/тези книги ('two/three/these books') and neuter две/три/тези легла ('two/three/these beds'). However, a recently developed language norm requires that count forms should only be used with masculine nouns that do not denote persons. Thus, двама/трима ученици ('two/three students') is perceived as more correct than двама/трима ученика, while the distinction is retained in cases such as два/три молива ('two/three pencils') versus тези моливи ('these pencils').
Case.
Cases exist only in the personal and some other pronouns (as they do in many other modern Indo-European languages), with nominative, accusative, dative and vocative forms. Vestiges are present in a number of phraseological units and sayings. The major exception are vocative forms, which are still in use for masculine (with the endings -е, -о and -ю) and feminine nouns (-[ь/й]о and -е) in the singular.
Definiteness (article).
In modern Bulgarian, definiteness is expressed by a definite article which is postfixed to the noun, much like in the Scandinavian languages or Romanian (indefinite: човек, 'person'; definite: човекът, ""the" person") or to the first nominal constituent of definite noun phrases (indefinite: добър човек, 'a good person'; definite: добрият човек, ""the" good person"). There are four singular definite articles. Again, the choice between them is largely determined by the noun's ending in the singular. Nouns that end in a consonant and are masculine use –ът/–ят, when they are grammatical subjects, and –а/–я elsewhere. Nouns that end in a consonant and are feminine, as well as nouns that end in –а/–я (most of which are feminine, too) use –та. Nouns that end in –е/–о use –то.
The plural definite article is –те for all nouns except for those, whose plural form ends in –а/–я; these get –та instead. When postfixed to adjectives the definite articles are –ят/–я for masculine gender (again, with the longer form being reserved for grammatical subjects), –та for feminine gender, –то for neuter gender, and –те for plural.
In Bulgarian adjective-noun phrases, only the adjective takes a definite article ending –
Many of the English loanwords which have been adopted into the language since the end of communism, however, do not readily lend themselves to taking adjectival endings. This has caused an unprecedented shift in the language whereby, in certain cases, the adjective remains uninflected while the noun following it takes the grammatical ending. Examples include –
This type of combination is sometimes favoured even when the possibility of a traditional phrase structure exists, e.g. –
In this case, the brand name "btv" cannot be inflected and, being a brand, remains in Roman script within the sentence.
Adjective and numeral inflection.
Both groups agree in gender and number with the noun they are appended to. They may also take the definite article as explained above.
Pronouns.
Pronouns may vary in gender, number, definiteness and are the only parts of speech that have retained case inflections. Three cases are exhibited by some groups of pronouns – nominative, accusative and dative. The distinguishable types of pronouns include the following: personal, relative, reflexive, interrogative, negative, indefinitive, summative and possessive.
Verbal morphology and grammar.
The Bulgarian verb can take up to 3,000 distinct forms, as it varies in person, number, voice, aspect, mood, tense and even gender.
Finite verbal forms.
Finite verbal forms are "simple" or "compound" and agree with subjects in person (first, second and third) and number (singular, plural) in Bulgarian. In addition to that, past compound forms using participles vary in gender (masculine, feminine, neuter) and voice (active and passive) as well as aspect (perfective/aorist and imperfective).
Aspect.
Bulgarian verbs express lexical aspect: perfective verbs signify the completion of the action of the verb and form past perfective (aorist) forms; imperfective ones are neutral with regard to it and form past imperfective forms. Most Bulgarian verbs can be grouped in perfective-imperfective pairs (imperfective/perfective: идвам/дойда "come", пристигам/пристигна “arrive”). Perfective verbs can be usually formed from imperfective ones by suffixation or prefixation, but the resultant verb often deviates in meaning from the original. In the pair examples above, aspect is stem-specific and therefore there is no difference in meaning.
In Bulgarian, there is also grammatical aspect. Three grammatical aspects are distinguishable: neutral, perfect and pluperfect. The neutral aspect comprises the three simple tenses and the future tense. The pluperfect is manifest in tenses that use double or triple auxiliary "be" participles like the past pluperfect subjunctive. Perfect constructions use a single auxiliary "be".
Mood.
The traditional interpretation is that in addition to the four moods (наклонения ) shared by most other European languages – indicative (изявително, ) imperative (повелително ), subjunctive (подчинително ) and conditional (условно, ) – in Bulgarian there is one more to describe a general category of unwitnessed events – the inferential (преизказно ) mood. However, most contemporary Bulgarian linguists usually exclude the subjunctive mood and the inferential mood from the list of Bulgarian moods (thus placing the number of Bulgarian moods to a total of 3: indicative, imperative and conditional) and don't consider them to be moods but view them as verbial morphosyntactic constructs or separate gramemes of the verb class. The possible existence of a few other moods has been discussed in the literature. Most Bulgarian school grammars teach the traditional view of 4 Bulgarian moods (as described above, but excluding the subjunctive and including the inferential).
Tense.
There are three grammatically distinctive positions in time – present, past and future – which combine with aspect and mood to produce a number of formations. Normally, in grammar books these formations are viewed as separate tenses – i. e. "past imperfect" would mean that the verb is in past tense, in the imperfective aspect, and in the indicative mood (since no other mood is shown). There are more than 40 different tenses across Bulgarian's two aspects and five moods.
In the indicative mood, there are three simple tenses:
In the indicative there are also the following compound tenses:
The four perfect constructions above can vary in aspect depending on the aspect of the main-verb participle; they are in fact pairs of imperfective and perfective aspects. Verbs in forms using past participles also vary in voice and gender.
There is only one simple tense in the imperative mood, the present, and there are simple forms only for the second-person singular, -и/-й (-i, -y/i), and plural, -ете/-йте (-ete, -yte), e.g. уча ('to study'): учи , sg., учете , pl.; играя 'to play': играй , играйте . There are compound imperative forms for all persons and numbers in the present compound imperative (да играе, ), the present perfect compound imperative (да е играл, ) and the rarely used present pluperfect compound imperative (да е бил играл, ).
The conditional mood consists of five compound tenses, most of which are not grammatically distinguishable. The present, future and past conditional use a special past form of the stem би- (bi – "be") and the past participle (бих учил, , 'I would study'). The past future conditional and the past future perfect conditional coincide in form with the respective indicative tenses.
The subjunctive mood is rarely documented as a separate verb form in Bulgarian, (being, morphologically, a sub-instance of the quasi-infinitive construction with the particle да and a normal finite verb form), but nevertheless it is used regularly. The most common form, often mistaken for the present tense, is the present subjunctive ([по-добре] да отида , 'I had better go'). The difference between the present indicative and the present subjunctive tense is that the subjunctive can be formed by "both" perfective and imperfective verbs. It has completely replaced the infinitive and the supine from complex expressions (see below). It is also employed to express opinion about "possible" future events. The past perfect subjunctive ([по-добре] да бях отишъл , 'I'd had better be gone') refers to "possible" events in the past, which "did not" take place, and the present pluperfect subjunctive (да съм бил отишъл ), which may be used about both past and future events arousing feelings of incontinence, suspicion, etc. and has no perfect to English translation.
The inferential mood has five pure tenses. Two of them are simple – "past aorist inferential" and "past imperfect inferential" – and are formed by the past participles of perfective and imperfective verbs, respectively. There are also three compound tenses – "past future inferential", "past future perfect inferential" and "past perfect inferential". All these tenses' forms are gender-specific in the singular. There are also conditional and compound-imperative crossovers. The existence of inferential forms has been attributed to Turkic influences by most Bulgarian linguists. Morphologically, they are derived from the perfect.
Non-finite verbal forms.
Bulgarian has the following participles:
The participles are inflected by gender, number, and definiteness, and are coordinated with the subject when forming compound tenses (see tenses above). When used in attributive role the inflection attributes are coordinated with the noun that is being attributed.
Reflexive verbs.
Bulgarian uses reflexive verbal forms (i.e. actions which are performed by the agent onto him- or herself) which behave in a similar way as they do in many other Indo-European languages, such as French and Spanish. The reflexive is expressed by the invariable particle se, originally a clitic form of the accusative reflexive pronoun. Thus –
When the action is performed on others, other particles are used, just like in any normal verb, e.g. –
Sometimes, the reflexive verb form has a similar but not necessarily identical meaning to the non-reflexive verb –
In other cases, the reflexive verb has a completely different meaning from its non-reflexive counterpart –
When the action is performed on an indirect object, the particles change to si and its derivatives –
In some cases, the particle "si" is ambiguous between the indirect object and the possessive meaning –
The difference between transitive and intransitive verbs can lead to significant differences in meaning with minimal change, e.g. –
The particle "si" is often used to indicate a more personal relationship to the action, e.g. –
Adverbs.
The most productive way to form adverbs is to derive them from the neuter singular form of the corresponding adjective—e.g. бързо (fast), силно (hard), странно (strange)—but adjectives ending in -ки use the masculine singular form (i.e. ending in -ки), instead—e.g. юнашки (heroically), мъжки (bravely, like a man), майсторски (skillfully). The same pattern is used to form adverbs from the (adjective-like) ordinal numerals, e.g. първо (firstly), второ (secondly), трето (thirdly), and in some cases from (adjective-like) cardinal numerals, e.g. двойно (twice as/double), тройно (three times as), петорно (five times as).
The remaining adverbs are formed in ways that are no longer productive in the language. A small number are original (not derived from other words), for example: тук (here), там (there), вътре (inside), вън (outside), много (very/much) etc. The rest are mostly fossilized case forms, such as:
Adverbs can sometimes be reduplicated to emphasize the qualitative or quantitative properties of actions, moods or relations as performed by the subject of the sentence: "бавно-бавно" ("rather slowly"), "едва-едва" ("with great difficulty"), "съвсем-съвсем" ("quite", "thoroughly").
Syntax.
Bulgarian employs clitic doubling, mostly for emphatic purposes. For example, the following constructions are common in colloquial Bulgarian:
The phenomenon is practically obligatory in the spoken language in the case of inversion signalling information structure (in writing, clitic doubling may be skipped in such instances, with a somewhat bookish effect):
Sometimes, the doubling signals syntactic relations, thus:
This is contrasted with:
In this case, clitic doubling can be a colloquial alternative of the more formal or bookish passive voice, which would be constructed as follows:
Clitic doubling is also fully obligatory, both in the spoken and in the written norm, in clauses including several special expressions that use the short accusative and dative pronouns such as играе ми се (I feel like playing), студено ми е (I am cold), and боли ме ръката (my arm hurts):
Except the above examples, clitic doubling is considered inappropriate in a formal context. Bulgarian grammars usually do not treat this phenomenon extensively.
Other features.
Questions.
Questions in Bulgarian which do not use a question word (such as who? what? etc.) are formed with the particle ли after the verb; a subject is not necessary, as the verbal conjugation suggests who is performing the action:
While the particle "ли" generally goes after the verb, it can go after a noun or adjective if a contrast is needed:
A verb is not always necessary, e.g. when presenting a choice:
Rhetorical questions can be formed by adding ли to a question word, thus forming a "double interrogative" –
The same construction +не ('no') is an emphasised positive –
Significant verbs.
The verb съм – 'to be' is also used as an auxiliary for forming the perfect, the passive and the conditional:
Two alternate forms of съм exist:
The impersonal verb ще (lit. 'it wants') is used to for forming the (positive) future tense:
The negative future is formed with the invariable construction няма да (see няма below):
The past tense of this verb – щях is conjugated to form the past conditional ('would have' – again, with да, since it is "irrealis"):
Имам and нямам
The verbs имам ('to have') and нямам ('to not have'):
Diminutives and augmentatives.
Diminutive
Usually done by adding -че, -це or -(ч)ка. The first two of these change the gender to the neuter:
Affectionate Form
Sometimes proper nouns and words referring to friends or family members can have a diminutive ending added to show affection. These constructions are all referred to as ""na galeno"" (lit. "caressing" form):
Such words can be used "both" from parent to child, and vice versa, as can:
Personal names are shortened:
There is an interesting trend (which is comparatively modern, although it might well have deeper, dormant roots) where the feminine ending "-ka" and the definite suffix "-ta" ("the") are added to male names – note that this is affectionate and not at all insulting (in fact, the endings are not even really considered as being "feminine"):
The female equivalent would be to add the neuter ending "-to" to the diminutive form:
Augmentative
This is to present words to sound larger – usually by adding "-shte":
Some words only exist in an augmentative form – e.g.
Conjunctions and particles.
"But"
In Bulgarian, there are several conjunctions all translating into English as "but", which are all used in distinct situations. They are "но (no), ама (amà), а (a), ами (amì)", and "ала (alà)" (and "обаче (obache)" – "however", identical in use to "но").
While there is some overlapping between their uses, in many cases they are specific. For example, ami is used for a choice – "ne tova, ami onova" – "not this one, but that one" (comp. Spanish "sino"), while ama is often used to provide extra information or an opinion – "kazah go, ama sgreshih" – "I said it, but I was wrong". Meanwhile, a provides contrast between two situations, and in some sentences can even be translated as "although", "while" or even "and" – "az rabotya, a toy blee" – "I'm working, and he's daydreaming".
Very often, different words can be used to alter the emphasis of a sentence – e.g. while ""pusha, no ne tryabva"" and ""pusha, a ne tryabva"" both mean "I smoke, but I shouldn't", the first sounds more like a statement of fact ("...but I mustn't"), while the second feels more like a "judgement" ("...but I oughtn't"). Similarly, "az ne iskam, ama toy iska" and "az ne iskam, a toy iska" both mean "I don't want to, but he does", however the first emphasises the fact that "he" wants to, while the second emphasises the "wanting" rather than the person.
"Ala" is interesting in that, while it feels archaic, it is often used in poetry and frequently in children's stories, since it has quite a moral/ominous feel to it.
Some common expressions use these words, and some can be used alone as interjections:
Bulgarian has several abstract particles which are used to strengthen a statement. These have no precise translation in English. The particles are strictly informal and can even be considered rude by some people and in some situations. They are mostly used at the end of questions or instructions.
Modal Particles
These are "tagged" on to the beginning or end of a sentence to express the mood of the speaker in relation to the situation. They are mostly interrogative or slightly imperative in nature. There is no change in the grammatical mood when these are used (although they may be expressed through different grammatical moods in other languages).
These express intent or desire, perhaps even pleading. They can be seen as a sort of cohortative side to the language. (Since they can be used by themselves, they could even be considered as verbs in their own right.) They are also highly informal.
These particles can be combined with the vocative particles for greater effect, e.g. "ya da vidya, be" (let me see), or even exclusively in combinations with them, with no other elements, e.g. "haide, de!" (come on!); "nedey, de!" (I told you not to!).
Pronouns of Quality.
Bulgarian has several pronouns of quality which have no direct parallels in English – "kakuv" (what sort of); "takuv" (this sort of); "onakuv" (that sort of – colloq.); "nyakakuv" (some sort of); "nikakuv" (no sort of); "vsyakakuv" (every sort of); and the relative pronoun "kakuvto" (the sort of...that...). The adjective "ednakuv" ("the same") derives from the same radical.
Example phrases include:
An interesting phenomenon is that these can be strung along one after another in quite long constructions, e.g.
An extreme (colloquial) sentence, with almost no "physical" meaning in it whatsoever – yet which "does" have perfect meaning to the Bulgarian ear – would be :
—Note: the subject of the sentence is simply the pronoun "taya" (lit. "this one here"; colloq. "she").
Similar "meaningless" expressions are extremely common in spoken Bulgarian, especially when the speaker is finding it difficult to describe something.
Inflection and derivation.
Bulgarian has a rich set of inflectional and derivational processes. In the simplest terms, this can be seen in the way that most nouns and verbs are formed – namely by adding prefixes and suffixes to a rather limited number of roots, which creates almost a dozen new words, along with a couple of dozen derivatives thereof. Here are some examples using the root word "klyuch" (ключ) "key/switch":
Nouns:
Adjectives:
Verbs:
An extreme example using this root might be:
Adjectives can also take up to three endings that are added to the masculine root, for example:
Verbs can take several prefixes, thus expressing increasingly complex ideas. For example, the "bol–" root, which has to do with ailments ("bol-ka" – pain; "bol-est" – illness; "bol-i" – it hurts, etc.), can be used to express various different stages of falling ill:
Similarly, the root kri–, referring to hiding/discovery:
Lexis.
Most of the vocabulary of modern Bulgarian consists of derivations of some 2,000 words inherited from proto-Slavic through the mediation of Old and Middle Bulgarian. Thus, the native lexical terms in Bulgarian account for 70% to 75% of the lexicon.
The remaining 25% to 30% are loanwords from a number of languages, as well as derivations of such words. The languages which have contributed most to Bulgarian are Russian, French and to a lesser extent Turkish and English. Also Latin and Greek are the source of many words, used mostly in international terminology. Many of the numerous loanwords from Turkish (and, via Turkish, from Arabic and Persian) which were adopted into Bulgarian during the long period of Ottoman rule, have been replaced with native terms. In addition, both specialized (usually coming from the field of science) and commonplace English words (notably abstract, commodity/service-related or technical terms) have also penetrated Bulgarian since the second half of the 20th century, especially since 1989. A noteworthy portion of this English-derived terminology has attained some unique features in the process of its introduction to native speakers, and this has resulted in peculiar derivations that slightly set the newly formed loanwords apart from the original words (mainly in pronunciation), although many loanwords are completely identical to the source words. A growing number of international neologisms are also being widely adopted.
Borrowings.
Some very frequent expressions have been borrowed from other languages. Most of them are somewhat informal.
External links.
Linguistic reports
Dictionaries
Courses

</doc>
<doc id="4153" url="https://en.wikipedia.org/wiki?curid=4153" title="Bipyramid">
Bipyramid

An "n"-gonal bipyramid or dipyramid is a polyhedron formed by joining an "n"-gonal pyramid and its mirror image base-to-base. An "n"-gonal bipyramid has 2"n" triangle faces, 3"n" edges, and 2+"n" vertices.
The referenced "n"-gon in the name of the bipyramids is not an external face but an internal one, existing on the primary symmetry plane which connects the two pyramid halves.
Right, oblique and concave bipyramids.
A right bipyramid has two points above and below the centroid of its base. Nonright bipyramids are called oblique bipyramids. A regular bipyramid has a regular polygon internal face and is usually implied to be a "right bipyramid". A right bipyramid can be represented as { }+P for internal polygon P, and a regular n"-"bipyramid { } + {n}.
A concave bipyramid has a concave interior polygon.
The face-transitive regular bipyramids are the dual polyhedra of the uniform prisms and will generally have isosceles triangle faces.
A bipyramid can be projected on a sphere or globe as "n" equally spaced lines of longitude going from pole to pole, and bisected by a line around the equator.
Bipyramid faces, projected as spherical triangles, represent the fundamental domains in the dihedral symmetry Dnh.
Volume.
The volume of a bipyramid is formula_1 where "B" is the area of the base and "h" the height from the base to the apex. This works for any location of the apex, provided that "h" is measured as the perpendicular distance from the plane which contains the base.
The volume of a bipyramid whose base is a regular "n"-sided polygon with side length "s" and whose height is "h" is therefore:
Equilateral triangle bipyramids.
Only three kinds of bipyramids can have all edges of the same length (which implies that all faces are equilateral triangles, and thus the bipyramid is a deltahedron): the triangular, tetragonal, and pentagonal bipyramids. The tetragonal bipyramid with identical edges, or regular octahedron, counts among the Platonic solids, while the triangular and pentagonal bipyramids with identical edges count among the Johnson solids (J12 and J13).
Kalidescopic symmetry.
If the base is regular and the line through the apexes intersects the base at its center, the symmetry group of the "n"-agonal bipyramid has dihedral symmetry D"n"h of order 4"n", except in the case of a regular octahedron, which has the larger octahedral symmetry group Oh of order 48, which has three versions of D4h as subgroups. The rotation group is D"n" of order 2"n", except in the case of a regular octahedron, which has the larger symmetry group O of order 24, which has three versions of D4 as subgroups.
The digonal faces of a spherical "2n"-bipyramid represents the fundamental domains of dihedral symmetry in three dimensions: Dnh, [n,2], (*n22), order "4n". The reflection domains can be shown as alternately colored triangles as mirror images.
Scalenohedron.
A scalenohedron is topologically identical to a 2"n"-bipyramid, but contains congruent scalene triangles.
There are two types. In one type the 2"n" vertices around the center alternate in rings above and below the center. In the other type, the 2"n" vertices are on the same plane, but alternate in two radii.
The first has 2-fold rotation axes mid-edge around the sides, reflection planes through the vertices, and n-fold rotation symmetry on its axis, representing symmetry D"n"d, [2+,2"n"], (2*n), order 2"n". In crystallography, 8-sided and 12-sided scalenohedra exist. All of these forms are isohedrons.
The second has symmetry D"n", [2,"n"], (*nn2), order 2"n".
The smallest scalenohedron has 8 faces and is topologically identical to the regular octahedron. The second type is a "rhombic bipyramid". The first type has 6 vertices can be represented as (0,0,±1), (±1,0,"z"), (0,±1,-"z"), where "z" is a parameter between 0 and 1, creating a regular octahedron at "z"=0.0, and becomes a disphenoid with merged coplanar faces at "z"=1.0. Beyond 1, it becomes concave.
Star bipyramids.
Self-intersecting bipyramids exist with a star polygon central figure, defined by triangular faces connecting each polygon edge to these two points. A {p/q} bipyramid has Coxeter diagram .
isohedral even-sided stars can also be made with zig-zag offplane vertices, in-out isotoxal forms, or both, like this {8/3} form:
4-polytopes with bipyramid cells.
The dual of the rectification of each convex regular 4-polytopes is a cell-transitive 4-polytope with bipyramidal cells. In the following, the apex vertex of the bipyramid is A and an equator vertex is E. The distance between adjacent vertices on the equator EE=1, the apex to equator edge is AE and the distance between the apices is AA. The bipyramid 4-polytope will have VA vertices where the apices of NA bipyramids meet. It will have VE vertices where the type E vertices of NE bipyramids meet. NAE bipyramids meet along each type AE edge. NEE bipyramids meet along each type EE edge. CAE is the cosine of the dihedral angle along an AE edge. CEE is the cosine of the dihedral angle along an EE edge. As cells must fit around an edge,
NAA cos−1(CAA) ≤ 2π, NAE cos−1(CAE) ≤ 2π.
Higher dimensions.
In general, a "bipyramid" can be seen as an "n"-polytope constructed with a ("n"−1)-polytope in a hyperplane with two points in opposite directions, equal distance perpendicular from the hyperplane. If the ("n"−1)-polytope is a regular polytope, it will have identical pyramids facets. An example is the 16-cell, which is an octahedral bipyramid, and more generally an n-orthoplex is an (n-1)-orthoplex bypyramid.

</doc>
<doc id="4154" url="https://en.wikipedia.org/wiki?curid=4154" title="Beast of Bodmin">
Beast of Bodmin

The Beast of Bodmin, also known as the Beast of Bodmin Moor () is a phantom wild cat purported to live in Cornwall, England. Bodmin Moor became a centre of these sightings with occasional reports of mutilated slain livestock; the alleged panther-like cats of the same region came to be popularly known as the Beast of Bodmin Moor.
In general, scientists reject such claims because of the improbably large numbers necessary to maintain a breeding population and because climate and food supply issues would make such purported creatures' survival in reported habitats unlikely.
Investigation.
A long-held hypothesis suggests the possibility that alien big cats at large in the United Kingdom could have been imported as part of private collections or zoos, later escaped or set free. An escaped big cat would not be reported to the authorities due to the illegality of owning and importing the animals.
The Ministry of Agriculture, Fisheries and Food conducted an official investigation in 1995. The study found that there was "no verifiable evidence" of exotic felines loose in Britain, and that the mauled farm animals could have been attacked by common indigenous species. The report stated that "the investigation could not prove that a 'big cat' is not present".
Skull.
Less than a week after the government report, a boy was walking by the River Fowey when he discovered a large cat skull. Measuring about long by wide, the skull was lacking its lower jaw but possessed two sharp, prominent canines that suggested that it might have been a leopard. The story hit the national press at about the same time of the official denial of alien big cat evidence on Bodmin Moor.
The skull was sent to the Natural History Museum in London for verification. They determined that it was a genuine skull from a young male leopard, but also found that the cat had not died in Britain and that the skull had been imported as part of a leopard-skin rug. The back of the skull was cleanly cut off in a way that is commonly used to mount the head on a rug. There was an egg case inside the skull that had been laid by a tropical cockroach that could not possibly be found in Britain. There were also cut marks on the skull indicating the flesh had been scraped off with a knife, and the skull had begun to decompose only after a recent immersion in water.

</doc>
